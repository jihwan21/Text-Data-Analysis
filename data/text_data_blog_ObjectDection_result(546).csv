title,link,postdate,description
A Comparative Analysis of Object Detection Metrics ,https://blog.naver.com/mose1204/222529367142,20211007,"https://www.mdpi.com/2079-9292/10/3/279/htm A Comparative Analysis of Object Detection Metrics with a Companion Open-Source ToolkitRecent outstanding results of supervised object detection in competitions and challenges are often associated with specific metrics and datasets. The evaluation of such methods applied in different contexts have increased the demand for annotated datasets. Annotation tools represent the location and...www.mdpi.com 이 논문에서 필요한 부분만 공부​​object detection -> 3가지 특성으로 표현된다1. object의 class2. 그것에 대응되는 bounding box3. confidence score (보통 0~1사이의 값이며 detector의 예측에 대한 확률)​평가기준을 만들 때 고려해야할것1. ground truth bbox 집합은 특정 클래스의 object를 포함하는 이미지 안의 직사각형 영역임2. model에 의해 예측되는 detections 집합은 bbox, class, confidence level로 구성됨​기본적으로 모델이 bbox를 예측한다는 것은그 곳에 물체가 있다고 예측하는 것이다. (objectness)​B : bounding box p : predicted / gt : ground truth 예측된 bbox가 실제 bbox와 일치하면 IOU=1이 된다.​예측한 bbox가 맞았는지 틀렸는지를 판정하기 위한 기준으로metric 측정시에 IOU threshold를 설정하게 된다.일정 IOU를 넘으면 detection correct,넘지 못하면 detection incorrect로 count한다.(보통 IOU threshold는 50%나 75%)​1. Precision and RecallTrue : 올바르게 판단함Positive : 예측한 bbox가 ​True Positive (TP) : GT를 올바르게 찾은 predicted bbox들False Positive (FP) : 제대로 찾지 못한 predicted bbox들False Negative (FN) : 탐지되지 못한 GT bbox들​ ​​2. Average Precision (AP)​class 1개당 AP 1개씩 나온다.​예측된 bbox에는 confidence score가 같이 나옴 (박스안에 obj가 있을 확률)이 값과 실제 GT와의 IoU값을 비교해서 나온 (TP, FP, ...) 이런 것들을 같이 고려해보는 것이 값이 일정 threshold 이상이면 negative로 취급함 (bbox 예측하지 않았다) ​​이를 기반으로 그린 Precision-Recall 그래프는 지그재그 형태를 보인다. (not monotonic) AP는 위의 Precision-Recall 그래프에서 전처리를 통해 지그재그형태를 적절히 없앤 후에 계산한 면적이다.(자세한것은 논문 참고)​​3. Mean Average Precision (MAP)class들의 각각의 AP를 구한 후 평균을 낸다 참고) https://blog.naver.com/mose1204/222529312468 Mean Average Precision (MAP)https://www.youtube.com/watch?v=FppOzcDvaDI red : predictions green : targets score : 박...blog.naver.com 4. Average Recall (AR)COCO dataset에서의 정의각 IOU threshold에서의 최대 Recall 값들의 평균 5. MAR ​6. F1 scorePrecision과 Recall의 조화평균이며둘 중에 하나만 떨어져도 값이 확 떨어진다. 이외에 여러 metrics들이 있지만 여기서는 생략한다. (논문 참고) 참고)https://seongkyun.github.io/study/2019/01/15/map/ Mean Average Precision(mAP) on Object detection · Seongkyun Han's blogMean Average Precision(mAP) on Object detection mean average precision Mean Average Precision(mAP) on Object detection Object detection에서는 모델의 성능(정확도)을 주로 Mean average precision(mAP)를 통해 확인한다. mAP가 높을수록 정확하고, 작을수록 부정확하다. 그런데 보통 모델의 mAP의 측정 해당 데이터셋별(PASCAL VOC, MS COCO 등등..)로 잘 짜여진 코드가 많이 있어서 알고리즘적으로...seongkyun.github.io ​ "
[KT AIVLE] 7주차_Object Detection ,https://blog.naver.com/ksw64285/223046604270,20230316,"들어가기 전CNN 모델이 하는 일은 이미지를 classification하는 것이었다. 그래서 어떤 이미지가 들어오면 해당 이미지 내의 객체들이 무엇인지 알려주었다​Object detection에서는 classification 뿐만 아니라 localization이라는 개념도 포함되어 있다​Localization이란 객체라고 판단되는 곳에 Bounding box를 그려주는 것​​​Object DetectionObject detection이란 이미지 내에서 물체의 위치와 그 종류를 찾아내는 것이다​대표적으로 자율 주행을 위해서 차량이나 사람을 찾아내거나 얼굴 인식을 위해 사람 얼굴을 찾아내는 경우를 예시로 들 수 있으며, 그 외에도 다양한 방식으로 이용되고 있다​Object detection은 object의 class를 classification 할 뿐만 아니라 localization까지 함께 수행하는 작업​Localization이란 이미지 내에 하나의 object가 있을 때 그 object의 위치를 특정하는 것인데, Detection은 여러 개의 object가 존재할 때 각 object의 존재 여부를 파악하고 위치를 특정하여 classification을 수행하는 것입니다 위 사진을 보면 Object Detection에서 여러 개의 object를 찾고 box를 통해 위치를 특정했다는 것을 알 수 있다​기본적으로 object 한 개의 위치를 찾아내어 반복 과정을 거쳐 여러 object를 찾아낼 수 있다고 접근하면 좋다​ 단순히 이미지를 분류하는 것이 classification, 그리고 이미지 내에 car라는 object의 위치를 알아내는 것이 Classification with localization이고, localization을 통해 이미지에서 하나의 object가 아닌 여러 object를 인식하는 것을 Detection이라고 한다​​​Localization Localization은 위 사진의 경우, 이미지 안의 object가 어느 위치에 있는지 정보를 출력해주는 것이다​주로 Bounding box를 사용하고, 이 box의 좌표값의 정보를 뜻한다. 대신 box의 꼭지점 pixel 정보가 아닌 left top, right bottom 좌표를 출력한다​​​IoU (Intersection over Union)Bounding box가 있을 때 localization 모델이 인식한 결과를 평가하려면 지표(metric)를 사용해야 한다​각 좌표값의 차이를 L1이나 L2로 정의할 수도 있겠지만, 이는 박스가 크면 그 값이 커지고 작아지면 그 값이 작아져 크기에 따라 달라지는 문제가 생긴다​이렇게 면적의 절대적인 값에 영향을 받지 않도록 두 개 박스의 차이를 상대적으로 평가하기 위한 방법 중 하나가 IoU이다. 영문 그대로, 교차하는 영역을 합친 영역으로 나눈 값이다.​IoU는 Bounding box와 Ground Truth(정답) 영역으로 나눈 값인데, 만약 완전 동일하게 일치한다면 IoU의 값은 몇이 될까?​동일한 값을 서로 나누면 1이 되기 때문에 IoU는 1이 된다​​요약하자면, 실제값(Ground Truth)과 모델이 예측한 값이 얼마나 겹치는지를 나타내는 지표 실제 box와 예측한 box의 교집합/합집합을 의미이 값이 클수록 잘 예측했다고 판단할 수 있다​예를 들어, 두 이미지가 거의 겹친다면 맨 왼쪽 이미지처럼 IoU값이 클 것이고, 두 이미지가 아예 겹치지 않는다면 맨 오른쪽 이미지처럼 IoU값은 0이 된다 ​​​Confidence ScoreConfidence Score란 Object Detection 알고리즘마다 조금씩 차이는 있지만, 보편적으로 찾은 Bounding Box안에 물체가 있을 확률을 의미한다​아래 이미지에서 노란색 Box에는 물체가 없으므로 Confidence Score는 0이고, 빨간색 Box는 물체가 있으므로 1이 된다 이 값은 단순하게 물체가 있을 확률로 계산되기도 하고,물체가 있을 확률 * IoU로 계산되기도 하고,정확히 어떤 물체의 Class일 확률 * IoU가 되기도 한다​각 알고리즘마다 조금씩 차이는 있으나, 의미는 비슷하다​​​ NMS (Non-Maximum Suppression)NMS를 수행하는 이유는 동일한 물체를 가리키는 여러 박스의 중복을 제거하기 위함​Object Detection모델은 대체로 아래 왼쪽 이미지처럼 실제 물체 개수보다 훨씬 많은 Bounding Box를 예측한다​NMS는 이를 오른쪽 이미지처럼 각 물체별 가장 좋은 Box 한 개만 남기고 나머지는 다 지우는 역할을 한다 그 방법은 Confidence Score와 IoU를 이용하는데,1) 특정 Confidence Score 이하의 Bounding Box는 제거2) 남은 Bounding Box들을 Confidence Score 기준으로 내림차순 정렬3) 맨 앞 박스부터 기준으로, 이 박스와 IoU가 특정 Threshold 이상인 박스들은 모두 제거4) 그리고 2와 3을 반복한다(쉽게 2중 for 문을 생각하면되고, IoU가 일정 이상이면 두 박스는 서로 같은 물체를 가리키는 것이라고 판단하여 상대적으로 Confidence Score가 낮은 박스를 제거하는 것이다)​Confidence Threshold가 높을수록, IoU Threshold가 낮을수록 더 많은 박스가 제거된다​예를 들어, Object Detection 모델이 다음과 같은 Bounding Box들을 예측했다고 해보자 (Confidence Threshold보다 낮은 Box는 이미 제거된 상태) 검정 숫자가 Confidence Score이 Score를 기준으로 Box들을 내림차순 정렬하면 [0.9박스, 0.8박스, 0.7, 0.65, 0.6, 0.6]가 된다먼저, 0.9박스를 기준으로0.8박스와는 겹치지 않으므로 0.8박스는 (유지)0.7박스와는 일정 이상 겹치므로 0.9박스와 같은 물체를 가리킨다고 판단. (삭제)0.65, 0.6(왼쪽) 박스는 IoU가 0이므로 (유지)0.6(오른쪽) 박스는 IoU가 일정 이상 겹치므로 (삭제)=> 0.9, 0.8, 0.65, 0.6​이제 0.8 박스를 기준으로 위와 같이 진행최종적으로 흰색 박스가 남고 빨간색 박스는 삭제된다 ​​​정리Classification이미지가 주어졌을 때, Object가 한 개 있고, 그 물체가 무엇인지 맞추는 문제​Classification + Localization이미지에 한 개의 Object가 있을 때, 그 Object의 위치를 찾고, 무엇인지까지 맞추는 문제​Object Detection이미지에 한 개 이상의 Object가 있을 때, 각각의 Object에 대해 위치와 무엇인지까지 맞추는 문제​Instance Segmentation이미지에 한 개 이상의 Object가 있을 때, 각각의 Object에 픽셀단위 위치와 무엇인지까지 맞추는 문제​​​​​​ "
YOLOv8 Object Detection Basic ,https://blog.naver.com/beyondlegend/223039139249,20230309,"오늘은 이미지에서, 객체가 어디에 있고 그 객체가 무엇인지를 알려주는 Object Detection 프로젝트를 YOLO 최신버전인 YOLOv8 을 이용해서 진행해보도록 하겠습니다=========================================================================================== ​먼저 YOLO 히스토리와 버전을 간략히 알아보면,  YOLO가 등장하기 전까지는 Faster R-CNN 아키텍처가 많이 사용되었지만 최대 7 FPS 성능으로 실시간성이 부족해서 실용적이지 않았으나, 2015년에 평균 45 FPS 성능을 가진 YOLO의 등장으로 Object Detection 분야에 획기적인 발전이 이루어졌습니다.​이러한 YOLO는 YOLOv4까지는 C언어 기반의 Darknet 아키텍처가 사용되었지만, YOLOv5 부터는 PyTorch로 개발이 이루어져서 2023년 현재 다음과 같은 빠른 속도와 사용자 친화적인 파이썬 인터페이스를 제공하는  YOLOv8 아키텍처로 진화하고 있습니다=========================================================================================== ​YOLOv8은 Object Detection과 함께 이미지나 동영상의 Image Segmentation 또한 동일한 API를 이용하여 구현할 수 있습니다.  ​즉 YOLOv8 은 사전학습 모델종류와 데이터 위치를 나타내는 yaml 파일만 바꾸어 주면, 동일한 Python API 로 Object Detection 과 Image Segmentation 구현이 가능하다는 것을 알수 있습니다 =========================================================================================== ​YOLOv8 개발환경을 알아보면,  roboflow 나 인터넷 등에서 PC로 다운받은 학습데이터를 구글 코랩에 직접 업로드 하거나 데이터 용량이 큰 경우엔, 구글 드라이브를 Colab으로 마운트 시켜서 데이터를 업로드 한다는 것을 알수 있습니다​그리고 이러한 YOLOv8 프로젝트를 개발하기 위한 프로세스는 표에서 제시한 것처럼 먼저 [1] 학습 데이터를 만들고  [2] 시스템에 올린후에  [3] YOLOv8 인스톨 시키고  [4] 학습  [5] 예측하는 총 5 단계로 구성할수 있습니다=========================================================================================== ​우리가 사용할 데이터는, 보시는 것처럼 4개의 이미지인데, 예측에 사용되는 총 4장의 test image를 colab에 업로드 해서, test_image_dir 디렉토리에 저장되어 있는것을 확인 할 수 있습니다.=========================================================================================== ​그럼 이젠 pip install 을 이용해서 YOLOv8와 YOLOv8 실행에 필요한 라이브러리를 설치하고 MS COCO Dataset 사전학습된 yolov8n 모델을 로드합니다=========================================================================================== ​그럼 이제 코드에서처럼 YOLOv8 predict 함수를 이용해서 object detection을 실행해 보면, 테스트 이미지에 포함된 객체의 종류와 개수를 출력하면서, 예측 이미지로서 객체의 위치는 Boinding Box로 표시하고, 객체 종류는 0.89, 0.72, 0.96 등의 확률값으로 나타낸다는 것을 알 수 있습니다=========================================================================================== 또한 result.boxes.cls 값을 이용하면, 결과에서 처럼 각 객체를 나타내는 클래스 값과 그에 해당하는 객체이름 그리고 개수 까지도 자세히 출력해서 디버깅 하거나 사용자 친화적인 UI를 만들때 참고할수 있다는 것도 알아 두시면 좋을거 같습니다   "
(객체 탐지) Object Detection 개요 ,https://blog.naver.com/wooy0ng/222864481906,20220902,"개요객체 탐지 문제를 공부하기 이전에 우선 전통적인 이미지 분류 문제를 한번 생각해봐야한다.​ ​위 그림은 CNN Architecture를 사용하여 분류를 하는 과정을 간단하게 보인 것이다.CNN을 거치고 난 후의 Feature Map을 Flatten하여 생성되는 Output Vector로 N개의 클래스를 분류하는 것이 위 그림의 목표인데​​​​ ​""만약 CNN을 거치고 난 후의 Feature Map에서 위치 정보까지 근사시킬 수 있다면 어떻게 될까?""에서 시작된 것이 바로 Object Detection의 아이디어라고 생각하면 될 것 같다.​​​그럼 가중치 업데이트는 어떤 방식으로 진행을 하게 될까?​Object Detection의 결과값에는 분류 정보와 위치 정보가 모두 포함되어있다.바로 아래의 그림과 같이 말이다.  ​당연히 학습하기 전이기 때문에 위치 정보나 분류 정보가 이상하게 나올 것이다.이렇게되면 당연하게 우리의 목표는 우리가 원하는 값을 얻는 방향으로 가중치를 업데이트하는 것이 된다. (아래 그림과 같이)​ Object Detection에서는 Label이라는 단어를 쓰지 않고 Ground Truth이란 단어를 쓰게 된다.Label은 정답을 의미하며 Ground Truth는 원하는 값을 의미하는데, ​위치 정보는 변동성이 높은 목표 변수이기 때문에 언제든지 변할 수 있는 값이다.때문에 정답을 뜻하는 Label이라 부르지는 않고 Ground Truth이라 부른다.​​​아무튼 Object Detection은 예측값이 최대한 Ground Truth으로 근사하도록 학습하는 네트워크라고 생각하자.​​​​​​ 상세​Object Detection에서 탐지하는 객체가 무조건 1개인 것은 아니다. 위와 같이 탐지해야 할 객체가 여러 개라면 벡터 하나만으로는 모든 것을 다 표현할 수 없을 것이다.​​때문에 Object Detection에서는 아래의 그림과 같이 객체 별로 벡터가 1개씩 필요하다. 하지만 어떤 Object인지를 구하고 어느 위치에 해당 Object가 있는지, 그리고 Object 영역 크기를 구하는 문제들을 동시에 푸는 것은 매우 어려운 일이었다.​​​​​하지만 GPU 가속 연산의 탄생 이후로 대용량의 정보들을 빠르게 처리할 수 있게 되자Object Detection 분야는 급속도로 발전하기 시작하였다. 출처 : https://wikidocs.net/163640​Object Detection은 두 갈래로 나뉘어 발전해나갔다.Stage를 한 번만 거쳐서 객체를 탐지해주는 모델(One-stage)과Stage를 두 번 거쳐서 객체를 탐지해주는 모델(Two-stage)로 나뉘어졌는데​당연히 Stage를 한 번만 거치는 것이 속도 면에서는 빠르지만 정확도가 상대적으로 낮으며Stage를 두 번 거치는 것은 속도 면에서는 느리지만 정확도는 상대적으로 높다는 특징이 있다.​​​​​​​​ 공통 기술One-Stage와 Two-Stage는 상리공생하며 발전한 관계라서몇가지 기술은 공통적으로 공유하고 있다는 특징이 있다.​공통적으로 가지고 있는 기술은 총 4가지이며 다음과 같다. Region ProposalObject가 있을 만한 영역(Region)을 추천Non-Max Suppression겹치는 영역을 제거Bounding Box RegressionObject의 테두리(Boundary)를 미세 조정Classification영역 속의 Object가 무엇인지를 분류 ​​​​​​​ 결론지금까지 Object Detection의 탄생 배경과 기술에 대해서 간략하게 설명하였다.이 포스트의 내용은 객체 탐지의 기본 지식이기 때문에 미리 알아두면 좋을 것이다.​다음 포스트부터는 Object Detection의 네트워크와해당 네트워크의 동작 방식에 대해서 공부해보도록 하자.​​​RCNN (객체 탐지) Object Detection / RCNN 개념개요 객체 탐지의 기본 개념을 잘 모른다면 아래의 포스트를 참고하고 오길 바란다. RCNN (Regions wi...blog.naver.com ​​​​​​​​​​​​ "
"선박 충돌 회피 및 물체 탐지(Collision Avoidance and Object Detection Maritime) 시장 2028년 7억 2,209만 달러 예측 ",https://blog.naver.com/giikorea2/223057928446,20230328,"선박 충돌 회피 및 물체 탐지 시장은 2022년부터 2028년까지 연평균 복합 성장률(CAGR) 8.2%로 성장을 지속하여 2028년에는 7억 2,209만 달러에 이를 것으로 예측됩니다. ​​​​​​​​​​목차제 1장 서론제 2장 중요 포인트제 3장 조사 방법제 4장 선박 충돌 회피 및 물체 탐지 시장 구도     ㆍ 시장 개요     ㆍ PEST 분석(북미, 유럽, 아시아태평양, 중동 및 아프리카, 남미)     ㆍ 에코시스템 분석     ㆍ 전문가의 견해제 5장 선박 충돌 회피 및 물체 탐지 시장 - 주요 산업 역학     ㆍ 시장 성장 촉진요인            ㆍ 선박 및 보트수 증가            ㆍ 자율 주행 시스템에 관한 연구 증가     ㆍ 시장 성장 억제요인            ㆍ 레거시 시스템과의 통합     ㆍ 시장 기회            ㆍ 개인용 요트나 레저용 보트 수요 증가            ㆍ 무인 잠수기나 자율 수중 이동체의 군사적 응용에 대한 연구 증가     ㆍ 주요 시장 동향            ㆍ 인공지능(AI)과 머신러닝(ML)을 활용한 내비게이션 시스템의 급속한 발전            ㆍ 원정 크루즈의 인기 상승     ㆍ 성장 촉진요인과 촉진요인의 영향 분석제 6장 선박 충돌 회피 및 물체 탐지 시장 - 세계 시장 분석     ㆍ 충돌 회피와 물체 탐지 해사 시장 개요     ㆍ 선박 충돌 회피 및 물체 탐지 시장 전망과 분석     ㆍ 시장 평가 - 세계 주요 기업제 7장 선박 충돌 회피 및 물체 탐지 시장 분석 - 기술별     ㆍ 선박 충돌 회피 및 물체 탐지 시장 : 기술별(2021년, 2028년)     ㆍ LiDAR     ㆍ 컴퓨터 비전     ㆍ 레이더     ㆍ 기타제 8장 선박 충돌 회피 및 물체 탐지 세계 시장 분석 - 용도별     ㆍ 선박 충돌 회피 및 물체 탐지 시장, 용도별(2021년, 2028년)     ㆍ 사각지대 탐지     ㆍ 나이트 비전     ㆍ 기타제 9장 선박 충돌 회피 및 물체 탐지 세계 시장 분석 - 최종사용자별     ㆍ 선박 충돌 회피 및 물체 탐지 시장, 최종사용자별(2021년, 2028년)     ㆍ 무인수상차량     ㆍ 선박     ㆍ 자율수중로봇(AUV)제 10장 선박 충돌 회피 및 물체 탐지 시장 : 지역별 분석     ㆍ 북미(미국, 캐나다, 멕시코)     ㆍ 유럽(독일, 프랑스, 이탈리아, 영국, 러시아, 기타 유럽)     ㆍ 아시아태평양(호주, 중국, 인도, 일본, 한국, 기타 아시아태평양)     ㆍ 중동 및 아프리카(남아프리카공화국, 사우디아라비아, 아랍에미리트(UAE), 중동 및 아프리카(MEA) 기타 지역)     ㆍ 남미(브라질, 아르헨티나, 기타 남미)제 11장 선박 충돌 회피 및 물체 탐지 시장 - COVID-19의 영향 분석     ㆍ 북미     ㆍ 유럽     ㆍ 아시아태평양     ㆍ 중동 및 아프리카     ㆍ 남미제 12장 업계 상황제 13장 기업 개요제 14장 부록​​◈ 세계의 선박 충돌 회피 및 물체 탐지 시장 예측(-2028년) - COVID-19의 영향과 기술, 용도, 최종사용자별 분석 Collision Avoidance and Object Detection Maritime Market Forecast to 2028 - COVID-19 Impact and Global Analysis By Technology, Application, and End User⊙ 리서치사 : The Insight Partners       ⊙ 보고서 코드 : 1153751https://www.giikorea.co.kr/report/tip1153751-collision-avoidance-object-detection-maritime.html  ​ "
"[강의,세션] 카카오 dev 2022 object detection. 내용정리 ",https://blog.naver.com/ehdrndd/222961059236,20221220,"https://if.kakao.com/2022/session/15 if(kakao)dev2022함께 나아가는 더 나은 세상if.kakao.com  두 방법 모두 NMS로 중복된 box들을 처리해야 하는 Post processing과정 필요​NMS 없는 DETR(2020)이 나왔지만, attention 연산의 quadratic complexity 문제 때문에 multi scale feature를 사용할수 없었고, 이에 따라 작은 물체에 대한 검출 능력이 약했다.​이에따라 Deformable DETR(2021)이 모든 query가 모든 key를 attention 하여 계산비용이 linear complexity로 낮출수 있어서 multi scale feature를 사용할수 있었다. 그러나 encoder가 처리해야할 토큰의 개수가 약 20배 가량 증가해서 전체적인 계산량은 오히려 늘었다.​​코코데이터셋의 이미지는 약 30%정도가 물체가 있는 부분이였고 나머지는 배경이었다. 즉, 인코더로 들어오는 모든 토큰을 계산해야할 필요성을 못느꼈고, 소수의 토큰만 골라내어 인코더에 넘겨주는 Sparse DETR을 제시 계산량 줄였다. encoder의 계산량은 약 80% 줄였다.​  여기서 보통 이미지넷(classification task)로 학습한 네트워크를 초기화에 쓰는데, 레이블이 필요없는 SCRL(2021) 로 초기화하면 성능이 증가한다!​​현재 object detection 알고리즘의 한계는 학습때 미리 지정한 class만 가능하다는 것.아래 그림에서 학습되지 않은 오리인형은 감지 못함. 이걸 감지하려면 새로운 로짓을 배분하여, 다시 학습해야 함   아래 처럼 text를 clip에 넣어 임베딩을 하면, 학습하지 못한 단어(클래스)도 임베딩되게 함. 이를 위해 마지막 class 헤더를 제거하고, embedding 벡터를 예상하도록 함. ​현재 한계가 어느정도 있음. "
라즈베리파이 + NCS2 + Yolov5로 object detection ,https://blog.naver.com/zxcvb8842/222863071593,20220831,"졸업작품으로 장애물을 피해 주행하는 라즈베리파이 자동차를 만들고 있다. 장애물을 피하기 위해서는 우선 장애물을 detection 해야 하는데 우리는 yolov5를 사용했다.​ GitHub - ultralytics/yolov5: YOLOv5 🚀 in PyTorch > ONNX > CoreML > TFLiteYOLOv5 🚀 in PyTorch > ONNX > CoreML > TFLite. Contribute to ultralytics/yolov5 development by creating an account on GitHub.github.com 모델별로 성능과 추론 시간의 trade off 관계가 있어서. 적절한 모델을 선택해야 한다. 우리는 일단 s 모델을 사용했다. ​ ​roboflow라는 사이트를 통해 3개의 객체를 라벨링 했다. 최종적으로는 객체당 1만 장 정도 학습 시킬 계획이다. 코랩을 통해 학습결과로 custom model를 만들었다.​ ​그리고 라즈베리 파이에 custom 한 모델을 넣고, 추론을 해봤다. 추론 정확도는 나쁘지 않았는데 속도가 3초에서 최대 5초 넘게도 걸렸다. 자동차 특성상 움직이니 추론 시간이 더 빨라져야 한다. 그래서 몇 가지 방법이 제안됐다.​더 가벼운 모델로 변경사진 크기 줄이기다른 디바이스를 사용​먼저 NCS2라는 디바이스를 추가해서 추론해 보기로 했다. NCS2(Neural Compute Stick 2)는 이렇게 USB 형태로 연결해서 딥러닝 모델을 추론할 수 있는 디바이스이다.  ​NCS2는 Openvino라는 프레임 워크에서 동작시킬 수 있기 때문에 현재 pytorch에서 추론하는 것을 openvino에서 추론할 수 있도록 모델 변환이 필요했다. ​yolov5 공식 github를 보면 openvino로 변환할 수 있는 export 코드를 제공해 주지만. 이렇게 변환해서는 라즈베리파이에서 사용할 수 없다. yolov5에서 export로 변환해 주는 openvino의 버전과 현내 라즈베리파에서 사용할 수 있는 openvino 버전이 다르기 때문. 버전을 맞춰서 변환이 필요하다.​ GitHub - violet17/yolov5_demo: OpenVINO demo & Convert to OpenVINO IR ==完整又详细的Pytorch到OpenVINO转换流程 ><不点进来看看吗OpenVINO demo & Convert to OpenVINO IR ==完整又详细的Pytorch到OpenVINO转换流程 ><不点进来看看吗 - GitHub - violet17/yolov5_demo: OpenVINO demo & Convert to OpenVINO IR ==完整又详细的Pytorch到OpenVINO转换流程 >...github.com 위 github를 통해 해답을 찾을 수 있었다. 방법을 간단히 정리하면 ​yolov5에서 제공해 주는 export 코드를 통해. pt를. onnx로 변환한다.윈도우에 라즈베리파이와 동일한 openvino 버전을 설치하고. onnx를 IR file로 변환한다.변환한 IR file로 라즈베리파이에서 추론한다.​참고로 변환하는 과정에서 환경 변수 같은 것을 제대로 쓰지 않으면 추론이 잘 안된다. 보고 그대로 하는 게 좋음. 우리는 2021년 3월 버전을 윈도우와 라즈베리파이에 설치하고 테스트를 진행했다.​ [라즈베리파이] OpenVino 설치하기 (NCS2)OpenVino 설치 현재 Raspbian Buster(Linux 10), 32-bit 버전을 지원하는 빌드된 가장 최신 버전 확인 https://storage.openvinotoolkit.org/repositories/openvino/packages/2021.4.2/ 다음과 같이 다운로드 합..hbesthee.tistory.com 라즈베리파이에 openVino와 NCS 관련 환경 설정은 위 블로그를 참고했다. 그대로 따라 하면 된다. ​yolov5_demo_ov2021.3.py를 통해 추론을 해봤고. 0.5~1초의 물체 검출 속도를 얻을 수 있었다. ​ ​조금 더 속도를 높이기 위해 640px에서 320px으로 변경했고. 더 빠른. 속도 정도로 물체 검출을 할 수 있었다. 물체 인식만 한다고 했을 때는. 3fps에서 5fps 정도의 성능을 기대할 수 있다.​ ​하지만 자동차 특성상 물체만 인식하는 것이 아니라. 모터도 제어해야 하고, 센서 값들도 측정해야 해서 멀티 프로세스/스레드로 파이썬 코드를 작성하다 보니 조금 느려졌다. 그래도 물체 인식 속도가 이전보다 확실히 빨라졌다. ​현재 바운딩 박스와 라인 정보를 통해 보이는 물체까지의 대략적인 거리와 어느 차선에 있는지. l(왼쪽), c(가운데), r(오른쪽)에 대한 정보를 표시하고 있다. 조금 더 진전이 있으면 또 글을 써야겠다.  "
[Object Detection] Focal Loss ,https://blog.naver.com/gudrmsdl08/223079778875,20230420,"Object Detection 관련 논문을 읽다보면 Focal Loss를 접하게 되는데Focal Loss는 도대체 무엇이고 왜 등장하게 됐는지 조사해 보았다​​​기존 Cross-Entropy Loss의 경우 (Binary Cross Entropy Loss 로 가정) y_pred는 sigmoid를 통과하여 0~1 사이의 확률로 맵핑된 클래스 예측 값으로잘못 예측한 경우에 대해 가중치를 주게끔 되어있고 (true인데 pred값이 낮음 or False인데 pred가 높음)1에 가까운 확률이 나올 경우 Loss의 값은 감소하는데0에 가까워 질수록 상당히 큰 값으로 변화하게 된다​이게 언제 문제가 되는가 하면Object Detection 학습을 진행할 때 배경과 우리가 관심있는 객체에 대한 비율을 고려할 때배경에 대한 비율이 상당히 더 많고 이에 대한 Loss가 누적이 된다하면정작 우리가 관심있는 객체에 대한 학습에 지장을 미치게 된다는 것이다​즉 Class에 대한 balance를 고려하지 못한다는 치명적 단점이 존재한다추가로, easy negative (하늘, 산, 도로 등등)에 대한 비율을 고려하지 못하는 단점 또한 존재한다​​​이러한 단점들을 상쇄하기 위해 등장한 것이 바로 Category-Balanced Focal Loss이다 해당 class에 대한  정답이 H, 예측값이 \hat{H}라고하면객체일때 (H_xy^c=1) 예측 값이 작으면(어려우면) Loss가 커지고Ex) (1-0.2)^2*log(0.2) = 0.44배경일때 (H_xy^c!=1) 예측 값이 크면(쉬우면) Loss가 작아지게 된다Ex) (1-0.2)^4*(0.2)^2*log(0.8) = 0.002​​H_xy^c=1인 경우와 H_xy^c=0인 경우 CE Loss와 Focal Loss를 plot해보면 다음과 같다  그래프에서 볼 수 있듯 Cross-Entropy Loss는easy, hard에 따라 Loss값이 그래프가 상대적으로 완만하나Focal Loss의 경우 Easy, Hard 에 따라 Loss가 급격하게 증가함을 볼 수 있다 "
비전공자를 위한 알짜 인공지능 지식 - Object Detection  ,https://blog.naver.com/mins1216/222935823753,20221123,"​안녕하세요! 자스민지입니다.요즘 인공지능이 많이 떠오르고 활용되고 있습니다.그래서 이와 관련해서비전공자분들을 대상으로 인공지능 지식을 하나 전달해 드리려고 합니다.​​  오늘 알려드릴 내용은 바로 <Object Detection>입니다.​​Object Detection 이 뭘까~~~요?​​Object Detection은 인공지능과 관련한 여러 분야 중 컴퓨터 비전과 관련된 기술을 의미하는 용어입니다!​​ ​인공지능 관련 분야에는전문가 시스템 / 자연어 처리 / 데이터 마이닝 / 음성 인식 / 컴퓨터 비전 / 지능로봇이 있습니다.​그중에서 컴퓨터 비전은  컴퓨터와 시스템을 통해 디지털 이미지, 비디오 및 시각적 입력에서 의미 있는 정보를 추출한 다음, 이 정보를 바탕으로 작업을 실행하거나 추천하는""(용어 출처 : What is Computer Vision? | IBM)분야입니다.​​말이 좀 어렵죠..?ㅠㅠ​쉽게 말해서 ""카메라나 센서 등을 통해 이미지 정보를 얻고 그 이미지로 다양한 작업을 수행"" 하는 분야입니다!​예를 들면, 사용자의 얼굴을 인식하는 스마트폰과 자율주행 자동차부터 해서드론 배달, 의료 진단 개선, 식물 및 동물 건강 모니터링 등이 있습니다.  출처 : https://avinton.com/en/ai-object-detection/​IDC 시장조사기관에 의하면, 전 세계 컴퓨터 비전 기술 시장 규모는 2020년 7억 달러에서 올해 21달러로 성장했고, 2025년에는 72억 달러 규모에 도달할 것이라고 전망하고 있습니다.​Computer Vision이 적용될 수 있는 산업이 워낙 다양해서성장 가능성이 매우 높은 분야인 것 같습니다!​​  큼, 다시 object detection 이야기로 돌아와서요​object detection은 바로 이 컴퓨터 비전과 관련된,시각 정보를 기반으로 객체(object)를 인식(detection) 하는 기술입니다.​사진이나 영상을 보고 거기에 등장하는 사물의 위치와 종류를 파악합니다.​예를 들어 사진에 3가지 객체가 있고 그 객체가 각각 사람과 강아지, 차라고 해봅시다.이 사진을 object detection 하는 모델에 넣으면, 그 결과로 세 객체에 대한 종류(사람, 강아지, 차)와 바운딩 박스를 통해 위치를 알려줍니다. ​바로 이런 식으로요! 출처 : https://www.datasciencecentral.com/object-detection-technology-how-it-works-and-where-is-it-used/위 사진을 보시면, 사진 속에 있는 거의 모든 객체에 대해""바운딩박스"" 를 통해 위치를 표시하고 박스와 함께 사람인지, 가방인지, 빌딩인지 그 ""종류""를 알려주고 있습니다!​​object detection은 이런 식으로여러 물체 (Multiple objects)에 대해위치를 알려주고(localization), 클래스를 구분하는(classification) 기술입니다.​​이 object detection을 수행하는 네트워크 모델 중에 YOLO(You Only Look Once)라는 모델이 있습니다. 이 yolo 모델을 만든 개발자가 발표한 영상이 있는데 재밌어서 한 번 보여드릴게요 ㅎㅎ​https://www.youtube.com/watch?v=NM6lrxy0bxs&t=606s 복잡한 설명은 뒤로하고11분 19초부터 보시면 됩니당!​귀여운 물건 여러 개로 저렇게 인식하는 게 넘 재밌지 않나요 ㅋㅋ​​​아무튼! 오늘은 인공지능이 적용되는 computer vision 분야와object detection에 대해 간단히 말씀드려보았습니다.​​도움이 되셨길 바랍니다. 그럼 행복한 잠 주무세요~! "
[YOLO-NAS] A New Object Detection Foundation Model ,https://blog.naver.com/bottle_full/223094329087,20230505,"YOLO v8이 나온지 4개월이 지난 시점에서  새로운 YOLO 알고리즘이 발표 되었다  오브젝트 디텍션을 하고 있는데  그래도  최신 기술에 대한 내용은 소개라도 해야 하지 않을까'​YOLO-NAS를 발표한 그룹은   Deci ( https://deci.ai/ ) 라는 곳이다 ​우리가 가장 궁굼해하는 소스는 여기에 있다.  https://github.com/Deci-AI/super-gradients​벤치마킹 결과는 아래 사진과 같다  https://twitter.com/deci_ai/status/1653735234809270272/photo/1Deci에서 공개한 내용과 링크드인이나 다른 곳에서  기사화된 내용들을 읽어보고 자세한 학습법이나  뭔가 어떤 학습방법에 대한 내용을 제외하고, 특징과  함축적인 결론은 ""  Deci의 자체 기술인 Neural Architecture Search(NAS), 알고리즘으로   The Automated Neural Architecture Construction (AutoNAC) 엔진을 활용하여 YOLOv8을 능가하는 새로운 아키텍처를 발견했습니다.​NAS 프로세스 중에 모델 아키텍처에 정량화 인식 RepVGG 블록을 통합하여 모델 아키텍처가 Post-Training Quantization(PTQ)와 호환되도록 보장했습니다.​""요 말이 함축적인 내용인것으로 보인다  ​​YOLO-NAS 모델은 COCO, Objects365 및 Roboflow 100을 포함한 잘 알려진 데이터 세트에 대해 사전 교육을 받았으므로 프로덕션 환경에서 다운스트림 개체 탐지 작업에 매우 적합하고 Deci의 PyTorch 기반 오픈 소스 컴퓨터 비전 훈련 라이브러리인  SuperGradients 에서 연구용(비상업적)으로 사용할 수 있는 사전 훈련된 가중치를 가진 오픈 소스 라이센스로 제공하고 있다고 한다.​아래링크에 자세한 사용법에 대한 내용이 코랩으로 작성되어있다.https://colab.research.google.com/drive/1q0RmeVRzLwRXW-h9dPFSOchwJkThUy6d​SuperGradient를 사용하면 분산 데이터 병렬, 지수 이동 평균, 자동 혼합 정밀도 및 정량화 인식 교육과 같은 고급 내장 교육 기술을 활용하여 모델을 처음부터 교육하거나 기존 모델을 미세 조정할 수 있다고 한다.​아직은 어려운 작은 물체에 대한 디텍팅이  다른 버전의  욜로들과는 다르게 YOLO-NAS의 내세울수 있는 장점으로 소개 되고 있는 것으로 보인다.​한번 사용해보고  얼마나 잘 되는지 확인 해봐야겠다.​​============================================================작성하면서 참고했던 곳  원문이 궁굼하시면 이곳을 보시면 됩니다.1 : https://deci.ai/blog/YOLO-NAS-object-detection-foundation-model2: https://www.linkedin.com/pulse/introducing-yolo-nas-object-detection-foundation-model-generated?trk=organization_guest_main-feed-card_feed-article-content3: https://medium.com/augmented-startups/how-yolo-nas-is-leaving-yolov8-in-the-dust-and-why-you-need-to-know-about-it-87f67a844bcb4: https://www.kaggle.com/discussions/general/406701​ "
객체탐지(Object Detection)개념 및 정의 ,https://blog.naver.com/gayeon6423/223107710379,20230521,"객체탐지의 기본적인 개념에 대해서 정리해보았습니다.​1. Classification & Localization & Object Detection & Segmentation Classification : 특정 이미지가 어떤 객체인지, 예를들면 고양이인지, 강아지인지, 사람인지 객체의 클래스만을 분류합니다. 주로 CNN모델만을 사용합니다.Classification + Localization : Localization이란 단 1개의 객체 위치를 Bounding Box로 지정해 찾는 것을 의미합니다. 위 그림처럼 고양이 주변에 사각형으로 박스가 씌어져 있습니다.Object Detection : 여러개의 객체들에 대한 위치를 여러개의 박스로 지정해서 찾습니다. 이 때 동시에 여러개의 객체들이 무엇을 의미하는지 클래스 분류도 수행합니다.Segmentation : Pixel-level로 Object Detection을 하는 것을 의미합니다. 그래서 Object Detection일 때의 바운딩 박스와 비교하면서 시각적으로 차이점이 무엇인지 확인할 수 있습니다.​2. Classification(분류) + Localization(회귀)객체 탐지 : 이미지에서 객체의 종류를 식별함과 동시에 그 위치를 찾는 컴퓨터 비전 기술 => Classification(객체의 종류 식별) + Localization(객체의 위치를 파악)즉, 객체의 클래스를 '분류' + 바운딩 박스를 찾는 '회귀'문제를 함께 수행합니다. ​3. Region Proposal객체가 있을 만한 영역들의 후보군들을 여러개 추출해주는 것을 의미합니다.대부분의 이미지들은 모든 픽셀들이 객체를 설명하고 있지 않으므로 객체를 탐지하기 위해서 모든 배경의 픽셀들을 사용할 필요는 없습니다.따라서 객체들을 나타내는 픽셀들만 찾고 탐지하기 위해 Region Proposal이 객체가 '있을 법한'픽셀의 영역들을 대거 추출해 추천해줍니다.​1. Sliding Window 특정 모양의 윈도우를 이미지 왼쪼고 상단에서 오른쪽 하단으로 점진적으로 이동하면서 객체 가 있을법안 Region들을 Proposal(제안)해줍니다.특정한 모양으로 윈도우만으로 이용할 수 도 있고 각기 다른 모양의 윈도우를 가각 슬라이딩하거 탐지할 이미지 크기를 다양하게 스케일링해서 Region Proposal을 수행할 수도 있습니다.하지만 객체가 없는 지역도 슬라이딩을 하므로 시간이 오래 걸리고 객체를 잘 탐지할 확률이 낮아진다는 단점이 있습니다. ​2. Selective Search이미지 픽셀의 컬러, 무늬, 크기, 형태에 따라 유사한 Region을 계층적으로 그룹핑하는 방법입니다. 객체가 있을만한 여래개의 바운딩 박스를 만든 후 반복적으로 Selective Search를 사용하여 Region Proposal을 해줍니다. 왼쪽의 Input Image를 Selective Search를 수행하며 오른쪽처럼 구분된 구역으로 나누게 됩니다. 초기에 픽셀 개별로 Segementation된 부분을 유사도에 따라 Segmentation그룹핑을 계속 반복하게 되고 Selective Search는 위 그림에서 'After more iterations'에 해당하는 Region Proposal을 하게 됩니다.​4. Object Detection아키텍처 Regioon Proposal은 Object Detection Network(RCNN,SSP)에서 Feature Map과 매핑을 이루게 됩니다.​5. Detection성능 평가 MetricsIoU(Intersection Over Union)실제 객체가 있는 Bounding Box(Ground Truth)와 모델이 예측한 Bounding Box(Predicted)가 포함하고 있는 영역을 계산합니다.  IoU는 0~1의 값으로 나타나고 값이 1로 갈수록 객체 탐지 모델을 잘 예측했다고 할 수 있습니다.​2. NMS(Non-Max Suppression)여러개의 Bounding Box들 중 객체의 Confidence Score이 가장 높은 박스를 선정하고 이 Score보다 낮은 Box들을 삭제합니다. 그 후 Score이 가장 높은 박스와 다른 박스들간의 IoU를 계산하고 미리 지정한 IoU임곗값보다 큰 다른 박스들을 모두 제거하게 됩니다. ​이는 탐지 모델이 오브젝트를 정확하게 예측하도록 도와주는 기술이라고 할 수 있습니다.이를 통해서 특정 객체를 가장 잘 탐지한 바운딩 박스를 한개만 선정할 수 있게 되고, 다수의 객체를 탐지할 수 있도록 해줍니다.​3. mAP(mean-Average Precision)객체탐지에서의 Confusion Matrix는 다음과 같은 상황을 의미합니다. AP값은 Confidence Score임곗값을 조절하면서 얻은 Precision-Recall Curve그래프의 아래 면적을 나타냅니다. 하나의 AP값은 하나의 객체에 대한 탐지 성능을 의미하고 mAP는AP에 평균값을 취한 값입니다.즉, 여러개의 객체들이 담긴 이미지를 객체 탐지하거나 하나의 객체에 대한 AP값을 구하고 이미지에 존재하는 객체들의 수만큼 평균값을 취해준다고 생각하시면 됩니다. ​2021년 CNN(ConvolutionNeural Network, 합성곱 신경망)등장 : 이미지 분류2014년 R-CNN등장 : CNN을 활용한 객체 탐지​One-Stage Detector : Object Detection(분류)와 Region Proposal(지역화)를 동시에 수행Two-Stage Detector : Object Detection(분류)와 Region Proposal(지역화)를 순차적으로 수행 => 보통 Two-Stage Detector가 One-Stage Detector보다 상대적으로 수행 시간은 느리지만, 분류와 지역화의 정확도는 높습니다. 최근 객체탐지는 실시간 위험 요소 탐지를 위해 사용되므로 짧은 시간이 장점인 One-Stage Detector를 주로 사용합니다.​​ "
Bag of Freebies for Training Object Detection Neural Networks - 논문 리뷰 ,https://blog.naver.com/bshlab671/222933050781,20221119,"Object Detection Task는 서로 다른 작업에 대해 동일한 기법을 적용하지 못해서 발전이 느렸습니다논문의 저자는 모두 동일하게 적용될 수 있는 기법에 대해서 소개합니다.간단하게 요약하면 아래와 같습니다.​Mixup을 적용하면 처음보는 물체에 대한 인식률이 더 좋다.Label Smoothing 적용시 overgitting 가능성을 낮춰준다.one stage detector인 yolo에 random crop을 적용하면 공간변형을 통해 높은 성능을 얻을 수 있다.스케줄러로 consine  learning rate를 적용하는 것이 좋다.one stage detetor에 random resize를 하며 학습을 시켜 일반화 성능을 높인다.  논문은 어떤 구조나 수식에 대한 설명이 아닌 추가적인 계산 비용을 요구하지 않는 쉬운 성능 높이기에 대한 방법을 말하고있습니다. Bag of Freebies왜 이름을 Bag of Freebies이라고 지었을까요...?​Visually Coherent Image Mixup for Object DetectionMixup은 좋은성과를 보여주는 data augment기법입니다. ​아래는 간단한 Mixup예시 입니다. Mixup 파라미터는 베타분포에서 추출하는데 이 때 사용하는 파라미터는 최소 1이상으로 하라고 합니다.​아래 사진은 무작위로 코끼리 사진을 훈련데이터에 추가해 찾아내는 실험을 한 경우입니다.Mixup을 사용하면 물체에 대한 confidence는 줄어들지만 데이터에 대한 robust (강인함 or 일반화)가 더 잘되어있고 밀집된 물체의 객체탐지를 더 잘합니다. confidence가 줄어들어서 좋지 않아 보이지만 가장 높은 label은 같아서 결국 도출되는 결과는 같습니다. (의문점으로 cutmix논문에서 ImageNet데이터셋과 Pascal VOC 데이터셋에 대해 mAP가 오히려 낮아지는 결과가 나오던데 어떻게 된건지 궁금하네요. Mixup은 mAP나 confidence에 대해서는 신경쓰지 않고 모르는 데이터에 대해서도 잘 추론하기 위해 도입하는 것 같습니다. 근데 또 실험결과를 보면 성능이 높아져서 의문입니다...)​Classification Head Label Smoothingobject detection도 분류 헤드도 거의 softmax를 거치고 cross-entropy를 사용합니다. 여기서 잘못된 레이블은 0, 정답인 label은 1로 확신해서 사용합니다. 실제 분포 자체는 그렇지 않은데 훈련데이터를 통한 과한 확신을 하여 overfitting문제를 가져올 수 있다고 합니다.그래서 아래와 같이 Label Smoothing을 해줍니다. 이것도 이미 이전에 제안되었던 방법입니다. K는 모델이 예측한는 class 갯수이고, 엡실론은 작은 상수입니다.​Mixup과 같이 이용할때는 어떻게 사용하는지 나오지 않아서 직접 찾아봐야겠네요...​Data Preprocessing분류 문제에서는 기하학적 변환이 자유롭지만 Detection문제에서는 조금더 신중하게 적용할 필요가 있습니다.​논문은 아래의 메서드에 대해서만 검토 했다고 합니다.random crop, random expansion, random horizontal flip random resize, random color jittering(including brightness, hue, saturation, contrast)​여기서도 one stage와 two stage는 다르게 적용해야할 필요가 있습니다. two-stage detection인 Fast-R-CNN같은 경우에는 selective search를 통해 이미 구역을 자르는 작업을 하기 때문에 훈련시에 기하학적 증강(random crop)이 필요하지 않다고 합니다.​Training Schedule Revamping이제까지 StepLR을 쓰던 것을 Smooth Consine LR로 사용하면 전체 훈련 step에서 더 좋은 성능을 보인다고 합니다. StepLR은 중간에 급격한 학습전환 구간이 있어 optimizer가 안정화 되는 iteration구간이 필요합니다. 반면 코사인 LR은 천천히 부드럽게 줄어들기 때문에 그런 구간이 필요가 없습니다.​Warmup learning rate도 gradient explode를 피하기 위해 사용합니다. 초기에 주요 예측에 대해 0의 값을 가지고 negative example에 대해 우세한 기울기를 갖는 YOLOv3같은 모델의 학습 안정화를 위해 필요합니다. 이 논문에서는 자세히 나오지 않지만 Bag of Tricks for Image Classification with Convolution NN 논문에서 제시된 내용입니다. 간단하게 처음에는 기울기 안정화를 위해 작은 LR을 적용하다가 안정화가 되면 초기 LR값으로 돌아가 학습을 시작하는 방법입니다. 객체탐지에서는 이 방법이 중요하게 쓰인다고 합니다. 전체 학습에서 stepLR은 학습률이 바뀌기 전까지 score가 고립되는 현상을 겪는데 consine은 그런 현상이 없습니다. 그리고 앞에도 말했드시 전체적으로 좋은 성능을 나타냅니다.validation mAP achieved by applying cosine learning rate decay outperforms step learning rate schedule at all times in training. 논문에 위문장이 적혀있습니다만 그래프를 보면 후반부랑 초반부는 가늠하기가 힘드네요​Synchronized Batch Normalization동기화된 배치 정규화는 멀티 GPU를 사용해 큰 배치를 이용해 학습하는 것을 말하는 것 같습니다. 배치에 따른 성능의 차이도 있는 듯 하지만 실험적인 결과는 보이지 않습니다.​Random shapes training for single stage object detection networks보통 single stage detetor는 고정된 이미지로 훈련을 하지만 메모리가 허용한다면 오버피팅을 줄이고 일반화 성능을 높이기 위해 다양한 이미지 크기를 사용해 훈련하는 것이 좋습니다.​이제 나머지 내용을 실험에 관한 내용이고 그에 대한 결과입니다. 이 부분은 건너뛰겠습니다.​결론으로는 추가 오버헤드를 쓰지 않고 위 방법을 모두 누적하며 사용해 모델 성능을 개선했는데 향후 학습에 대해서도 폭 넓은 학습 파이프라인을 채택할 것을 제안하고 있습니다.(현재까지 나온 기법들은 모두 추론시간에 영향을 미치지 않았습니다)  조금 더 살펴볼 내용이 있습니다.​detection network의 두 부분에 따로 mixup을 적용해보는 실험도 있었는데 두 네트워크 모두 적용하면 제일 좋은 결과를 얻을 수 있었습니다. 논문의 저자는 두 단계 모두 mixup을 적용하면 freeze시키고 각각 mixup을 통한 학습을 하는 것 보다 low layer가 통계적으로 유사한 입력을 받아 혼란이 줄어들어서라고 말합니다.​논문 링크: https://arxiv.org/pdf/1902.04103.pdf "
"[Object Detection] You Only Look Once:Unified, Real-Time Object Detection (YOLO) ",https://blog.naver.com/kona419/223008240111,20230207,"논문 : https://ieeexplore.ieee.org/document/7780460 You Only Look Once: Unified, Real-Time Object DetectionWe present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bound...ieeexplore.ieee.org Abstract 본 논문에서는 객체탐지를 회귀 문제로 봄.하나의 neural network가 한 번의 계산만으로 bounding box와 클래스 확률을 예측함.+ bounding box란 객체의 위치를 알려주기 위해 객체의 둘레를 감싼 직사각형 박스를 말한다. 클래스 확률이란 bounding box로 둘러싸인 객체가 어떤 클래스에 해당하는지에 관한 확률을 의미한다.파이프라인이 하나의 신경망으로 구성되어 있으므로 end-to-end 형식.YOLO는 매우 빠르다.그러나 localization error가 더 생김. but, 배경에서 false positive를 예측할 가능성이 적음.​Introduction  객체 탐지의 이미지 픽셀에서 bounding box의 위치와 클래스 확률까지를 하나의 회귀 문제로 재정의한다.YOLO는 엄청 빠름.예측할 때 이미지 전체를 봄.물체의 일반적인 부분을 학습함. 그러나 최신 모델들에 비해 정확도가 다소 떨어진다는 단점이 있음. 빠르지만, 정확도는 낮음.​Unified Detection YOLO는 한개의 neural network만 사용.이미지의 모든 특징을 사용.논문의 network가 전체 이미지와 이미지의 모든 객체에 대해 전체적인 추론을 함.YOLO는 높은 정확도를 유지하면서 end-to-end training과 real-time speed를 가능하게 함. 논문에서는 이미지를 정방향 그리드로 나눈다. 이 grid 셀 안에 object가 있으면 --> 탐지!각각의 grid cell은 Bbox와 각 box의 신뢰도를 예측한다. confidence score은 모델에서 box가 object를 포함하는 것을 얼마나 신뢰하는지와 box가 얼마나 정확히 예측했는지를 반영한다. --> 수식 : Pr(Object) ∗ IOU(※ IOU는 두 영역의 교집합인 intersection 영역의 넓이를 두 영역의 합집합인 union 영역으로 나누어준 값입니다. 이를 통해 찾고자 하는 물건의 절대적인 면적과 상관없이, 영역을 정확하게 잘 찾아내었는지의 상대적인 비율을 구할 수 있으므로 모델이 영역을 잘 찾았는지 비교하는 지표이다)cell에 object가 없다면 IOU는 0.우리는 confidence score = predicted box와 ground truth사이의 IOU  이길 원함.각각의 Bbox는 5개의 prediction을 가진다.  x, y, w, h, confidencegrid cell 중에서 객체의 중앙과 가장 가까운 cell이 객체 탐지를 함.각 grid cell은 class의 확률인 C를 예측함. 식 : Pr(Class_i|Object)class probabilities와 각 box의 신뢰도를 곱하면 class 별 Confidence Score가 나온다.​ - Network Design GooLeNet 모델에 영감을 받아 해당 network를 설계함.GooLeNet은 inception module을 사용했지만, YOLO는 단순한 1X1 reduction 레이어를 사용함.24개 Conv layers, 2개 FC layers로 네트워크가 구성됨.1X1 layer는 이전 레이어의 feature space를 줄여줌.​ - Training 사전에 GooLeNet을 이용해 conv layer를 학습함. 정확도 88%마지막 레이어는 class probabilities와 bounding box의 위치를 예측함.bounding box의 weight와 height를 0~1 사이의 값으로 정규화시킴.마지막 레이어에는 linear activation function을 사용했고, 나머지 계층에는 Leaky ReLU를 사용함. loss function으로 sum-squared error를 사용하면 최적화하기 쉽지만 몇가지 문제가 있음.문제 1. 우리가 원하는만큼 평균 정확도를 최대화하지 못함.문제 2. 모든 grid cell이 객체를 포함하는 것이 아님.문제 3. error의 가중치를 작은 box든, small box든 같게 함.​<해결책>bounding box 위치 예측할 때 loss를 증가시키고, 객체를 포함하지 않는 box를 예측할 때는 loss를 줄임.weight와 height 대신 bounding box의 weight, height의 제곱근을 예측함.YOLO는 하나의 grid cell 당 여러 개의 bounding box를 예측함. 이 여러개 중 하나의 bounding box를 선택해야함. -> 선택 기준 : 현재에서 객체를 감싸는 ground-truth boudning box와의 IOU가 가장 큰 것을 선택.​ - Inference YOLO는 하나의 네트워크만 사용하기 때문에 테스트 단계에서 속도가 엄청 빠름.grid 디자인은 하나의 객체를 여러 grid cell이 동시에 검출할 가능성이 있다는 단점이 있음. --> 해결책 : non-maximal suppression​ - Limitations of YOLO 공간 제약 : 하나의 grid cell마다 2개의 bounding box를 예측하는데 검출은 하나만 할 수 있음.train 때 학습하지 못한 새로운 비율을 만나면 고전할 수 밖에 없음.큰 bounding box와 작은 bounding box의 loss 가중치가 동일함. 작은 bounding box는 위치가 조금만 달라져도 성능에 큰 영향을 미침. IOU 변화가 더 심하기 때문==> incorrect localization​Conclusion YOLO는 detection 성능에 해당하는 loss function에 대해 direct로 학습하고 전체 모델은 공동으로 학습함.실시간 객체 탐지에 대해 매우 성능이 좋으며 새로운 domain에도 잘 작동함.​​참고 ​https://bkshin.tistory.com/entry/%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0-YOLOYou-Only-Look-Once 논문 리뷰 - YOLO(You Only Look Once) 톺아보기본 글은 YOLO 논문 전체를 번역 및 설명해놓은 글입니다. 크게 중요하지 않은 부분을 제외하고는 대부분의 글을 번역했고 필요하다면 부가적인 설명도 추가했습니다. 내용이 긴 섹션 끝에는 요약도 추가했습니다. 번역이 이상하다거나 틀린 내용이 있다면 피드백 부탁드립니다. Abstract YOLO 연구진은 객체 검출(object detection)에 새로운 접근방식을 적용했습니다. 기존의 multi-task 문제를 하나의 회귀(regression) 문제로 재정의했습니다. YOLO는 이미지 전체에 대해서 하나의 신경망(a single ne...bkshin.tistory.com https://velog.io/@cha-suyeon/%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0-You-Only-Look-Once-YOLO-v1-v2-v3 [논문 리뷰] You Only Look Once - YOLO v1 (+ v2, v3)논문 출처: 링크, PDF이번에 리뷰할 논문은 2016년에 발표된 'You Only Look Once:Unified, Real-Time Object Detection'입니다.첫 논문 리뷰라서 떨리네요.😉 퍼실님께 논문 스터디 방법을 여쭤봤습니다. 팁을 주셨는데요. 제velog.io ​ "
딥러닝 Object Detection 모델 살펴보기(1) : R-CNN (RCNN) 논문 리뷰 ,https://blog.naver.com/baek2sm/222782537693,20220623,"이번 포스팅에서는 딥러닝을 이용한 object detection 연구의 출발점이라 할 수 있는 R-CNN [1] 논문을 함께 살펴보도록 하겠습니다. 이 논문은 2013년에 게재되었는데, 당시까지 딥러닝을 사용하지 않던 기존의 object detection 연구에 비해 획기적인 성능 향상을 이룸으로써 딥러닝 기반 모델의 성능을 입증했다는 점에서 object detection 연구 흐름에 중요한 역할을 한 논문입니다. RCNN논문: https://arxiv.org/abs/1311.2524딥러닝 기반의 object detection 방법은 region proposal과 classification을 따로 수행하는지, 합쳐서 한 번에 수행하는지에 따라 각각 two-stage 방법과 one-stage 방법으로 나뉘는데요. R-CNN은 그 중 two-stage 방법에 속합니다.​R-CNN은 two-stage 방법을 사용하는만큼 region proposal과 classification 과정이 분리되어 있습니다. 이 중 region proposal의 경우 seletive search를 통해 2,000개의 bounding box 후보를 생성하게 되는데요. selective search에 관한 더 자세한 내용은 제가 따로 포스팅해둔 다음 링크를 참고해주세요.​Region proposal : Selective search 설명 및 실습 : https://blog.naver.com/baek2sm/222778686809 Region proposal : Selective search 설명 및 실습딥러닝 기반의 object detection은 region proposal과 classification을 따로 수행하는 two-stage 방법과, ...blog.naver.com 이렇게 2,000개 영역에 대한 bounding box 후보를 얻은 후에는 CNN 모델을 이용해서 feature extraction을 하고, bounding box 영역에 대한 최종 클래스 예측은 support vector machine(SVM)을 통해 수행합니다. 즉, 딥러닝 모델을 사용하긴 했지만 딥러닝 모델은 feature extractor로서만 활용된 셈입니다. R-CNN의 전체적인 흐름은 다음 그림과 같습니다. Overview of RCNN, 출처: RCNN 논문 [1]다시 한 번 네가지 스텝을 정리하면 다음과 같습니다.​이미지를 입력합니다.이미지에서 selective search를 통해 2,000개의 bounding box 후보를 생성합니다.bounding box 후보 영역의 이미지 크기를 resize하여 CNN 모델에 넣고 feature를 추출합니다.CNN 모델에서 얻은 feature를 기반으로 SVM을 통해 해당 영역에 대한 예측을 수행합니다.​간단히만 설명드리면, 여기서 마무리지을 수 있을 것 같아요. 그러나 제대로 이해하려면 조금 더 알아야 할 부분이 있습니다. 먼저 다음의 성능 비교표를 살펴본 뒤 더 자세히 설명드릴게요. PASCAL VOC 데이터셋을 기준으로, 기존의 object detection 모델과 R-CNN 모델의 성능 비교. 출처: R-CNN 논문 [1]object detection 모델의 성능 비교표를 보면, 기존의 모델에 비해 R-CNN 모델의 mAP(mean average precision)이 압도적으로 높은 것을 알 수 있습니다. 그 중에서도, R-CNN BB 라는 모델의 성능이 기본 R-CNN 모델보다 성능이 더 뛰어난데요. R-CNN BB 모델은 CNN을 사용할 때 단순히 feature extraction만 수행하는 것이 아니라, 다음 그림과 같이 bounding box regression을 함께 수행함으로써 bounding box 영역에 대한 정확도를 높였습니다. R-CNN BB 모델의 구성, 출처: 직접 그림Bounding box regression에 대해 조금 더 생각해보면, selective search를 통해 이미 bounding box 영역을 추려서 CNN 모델에 넣었는데 CNN 모델에서 bounding box regression을 하는 것이 조금 의아할 수 있습니다. ​CNN 모델에서 수행하는 bounding box regression은 bounding box 영역에 대한 ""보정"" 작업이라고 생각하시면 편할 것 같아요. selective search를 통해 얻은 bounding box 영역이 부정확할 수 있으므로, CNN 모델에서 주어진 이미지의 중심이 어디일지, 크기와 높이는 몇일지를 다시 예측하는 과정입니다. 실제로 selective search로 영역 후보를 얻어보면 부정확한 bounding box가 무수히 많아서, 이 과정이 꼭 필요한 것 같습니다.​마지막으로, R-CNN 모델의 학습 과정에 대해 알아봅시다. R-CNN 모델은 보시다시피 마지막 부분 classification 예측 부분에서 fully connected layer가 아닌 SVM을 사용합니다. 그럼 backpropagation은 어떻게 이뤄질까요..? 이 구조로 바로 학습하는 것은 불가능하고 feature extractor 역할을 하는 CNN 모델을 먼저 따로 학습시킨 뒤 SVM을 학습시킵니다. CNN 모델을 학습시키는 방법만 따로 정리하며 포스팅을 마치겠습니다.​먼저, ImageNet dataset을 통해 CNN 모델을 학습시킵니다.그 다음, fully connected layer 부분만 바꿔서 PASCAL VOC dataset을 이용해 finetuning합니다. 이 때, selective search로 예측된 bounding box 영역이 모델 학습에 사용되며 ground truth와 IOU를 계산해 0.5 이상이면 해당 클래스로, 0.5 미만이면 background 영역으로 분류하도록 학습시킵니다.이렇게 학습된 CNN 모델을 feature extractor로 사용합니다. 딥러닝 Object Detection 모델 살펴보기 시리즈딥러닝 Object Detection 모델 살펴보기(1) : R-CNN 논문 리뷰 (현재 포스팅)딥러닝 Object Detection 모델 살펴보기(2) : Fast R-CNN 논문 리뷰: https://blog.naver.com/baek2sm/222783238067 딥러닝 Object Detection 모델 살펴보기(2) : Fast R-CNN (Fast RCNN) 논문 리뷰이번 포스팅에서는 R-CNN [1]의 처리 속도를 획기적으로 개선하고, mAP(mean average precision)도 ...blog.naver.com 딥러닝 Object Detection 모델 살펴보기(3) : Faster R-CNN 논문 리뷰: https://blog.naver.com/baek2sm/222784719619 딥러닝 Object Detection 모델 살펴보기(3) : Faster R-CNN (Faster RCNN) 논문 리뷰이번 포스팅에서는 딥러닝 기반의 object detection 모델 중 최초로 end-to-end로 구현된 Faster R-CN...blog.naver.com 딥러닝 Object Detection 모델 살펴보기(4) : YOLO : You Only Look Once 논문 리뷰 (포스팅 후 링크 연결 예정) 참고 문헌references[1] Rich feature hierarchies for accurate object detection and semantic segmentation : https://arxiv.org/abs/1311.2524​ "
"Image Classification 과 Object Detection의 차이 + Tiny ML, FOMO 관련 이야기 ",https://blog.naver.com/kjy6135/222842650228,20220808,"출처 : https://medium.com/analytics-vidhya/image-classification-vs-object-detection-vs-image-segmentation-f36db85fe81 Image Classification vs Object Detection vs Image SegmentationThe difference between Image Classification, Object Detection and Image Segmentation in the context of Computer Visionmedium.com ​​흔히 AI 영상처리 분야에서 Image Classification과 Object Detection이라는 용어를 자주 볼 수 있다.​하지만 막상 예제를 보면 카메라를 통해 영상을 획득하고 해당 영상을 미리 학습시킨 모델에 input으로 넣어 어떤 것인지 판단을 하는 동일한 방식으로 진행되는 것을 알 수 있다.​이 둘의 차이점은 뭘까?​ 간단하다. image classification은 ""단일 객체"" 에 대해 이 이미지가 어떤 것인지 분류하는 것이고Object Detection은 다중 객체에 대한 분류이다. 따라서 하나의 객체가 가진 위치정보가 필연적으로 필요할 수 밖에 없다.​​​이를 임베디드 AI, 혹은 Tiny ML에 적용시킨다고 생각해보자.​일반적으로 단일객체와 다중객체, 어떤 기능이 더 리소스를 많이 잡아먹을까?바로 다중객체, 즉 Object Detection이 리소스를 더 많이 잡아먹을 것이다.연산능력이 한정된 임베디드 시스템에서 Object Detection을 사용하려면최소한 어느정도 수준의 인프라가 구축이 되어야 할 것이다.​https://docs.edgeimpulse.com/docs/tutorials/detect-objects-using-fomo Detect objects with centroidsdocs.edgeimpulse.com ​edge impulse 사는 임베디드AI 관련 여러 정보를 제공하고 실제 서비스를 하고있는 회사이다. ​해당 회사의 Tutorial에서 Object Detection 튜토리얼을 따라갈 수 있는 보드들의 목록을 알려준다.​위 튜토리얼은 FOMO (Fater Objects, More Objects) 에 대한 내용을 다루며 테스트를 해보면 나쁘지 않을 것 같다. "
AI 기반의 도면 이미지 내 객체 탐지(Object Detection)를 통한 적산 캐드 프로그램 개발 ,https://blog.naver.com/cadiancad/223073879872,20230413,"본 내용은 국산캐드 개발사인 인텔리코리아가 보도자료로 준비한 내용입니다.​                                            [ 인공지능 기반의 적산 시스템]​​AI 기반 도면 이미지 내 객체 탐지(Object Detection)를 통한 적산 시스템 개발 국산캐드(CAD) 프로그램을 개발·공급하는 인텔리코리아(대표 박승훈)는 전통 목조건축 AI-CAD (CADian TWArch)에 적용된 인공지능 인지 기술을 바탕으로 전통 건축에서 머무르지 않고, 그 영역을 현대 건축으로 확대하여 이미지 도면, 스케치 이미지, CAD 도면 이미지 등에서 노이즈 제거, 구조 객체(벽, 기둥, 보 등), 건축 객체(문, 창, 가구, 가전 등)를 탐지하여 견적용, 검토용, 정산용 적산을 수행하는 시스템으로 확대 개발하였다고 4월13일 밝혔다.​인텔리코리아는 지난 달 17일 문화재청의 문화유산 보존·관리 기술 수준 향상과 문화재 산업 분야 발전을 위해 추진 중인 문화유산 스마트 보존‧활용 기술 개발의 하나인 ‘AI 기반의 전통건축 손도면 캐드 도면화 기술 개발’의 1단계 사업을 성공적으로 수행했다고 밝힌 바 있다.​ [AI 부재 인식을 통한 전통 건축 도면화 과정] 이번 인텔리코리아가 밝힌 CADian TWArch(전통 목조건축 AI-CAD)는 한국전자통신연구원(이하 ETRI), 인텔리코리아, 고려대학교 건축문화유산연구실 등이 공동으로 개발하고 있는 솔루션으로, 전통 목조 건축의 핵심 부분인 ‘공포계(지붕 하중 지지 부)’의 옛 손도면 이미지를 2D/3D로 도면화 하는 설계 툴이다.  [고려대 건축학도를 대상으로 CADian TWARCH 교육] 본 도구에는 AI 기반 객체 탐지 및 부재 라이브러리 고속화, AI 기반 노이즈 제거, 도면 수정‧편집, AI 기반 객체 벡터화 및 프로세스 제어, 디지털 도면 데이터 관리 기술 등이 유기적으로 연결하도록 구성되었다. 관계자에 따르면 객체 탐지 성능 93% 이상, 탐지된 객체 기반 CAD 도면화 비율이 91% 이상의 성능을 보임에 따라 전통 목조 건축업계의 비상한 관심을 모으고 있는 것으로 알려지고 있다.                                       [AI 이미지 인지 기반 도면 적산 솔루션 세부 기술과 적용 범위] 인텔리코리아는 전통 건축에 적용했던 AI 이미지 인지 기술을 현대 건축에 접목함으로써 기초 도면을 기반으로 공사 물량을 산출하는 입찰용 적산, 공사 완료 후 도면을 기반으로 발생하는 정산용 적산, 또는 개보수 등 2차 공사(소방/방범, 전기 포설/배선, 시스템에어콘, 인테리어 등)에 필요한 물량 산출 및 신규 도면 생성 등을 원활하게 지원할 수 있게 되었으며, 이로 인하여 지금까지 발생했던 과/소 적산, 물량 산출의 비용 및 시간 소요 등의 문제를 해결하게 되었다고 밝혔다. 또한 도면 내 객체 인지 기술의 적용 범위를 확대하여 기존 2D도면 내 요소 및 구조를 인지하여 3차원 데이터를 생성하는 자동화 도면 자원 생성, 관리 부분으로의 영역 확장도 진행 중이라고 밝혔다.또한 인텔리코리아 연구소에서 AI 솔루션 사업 본부를 총괄하는 한명기 이사는 도면 내 AI 기반 객체 인지 기술은 도면 이미지의 특성 즉, 기존 이미지들과 달리 모든 형상이 흰색 배경 위에 실선으로만 구성되어 있는 특수한 이미지들을 인지하여야 하므로 기존의 이미지 인지 기술보다 높은 난이도를 요구한다고 설명했다. 또한 이미 개발된 전통 건축물에 대한 이미지 인지는 단순 형상을 기반으로 추출했다면 이번 현대 건축에 적용된 이미지 인지는 도면 내 노이즈 제거, 한글 인지, 치수 인지, 구조체 인지, 건축 객체 인지 등을 모두 포함하는 포괄적 인지이며, 인지된 데이터에 대한 적산을 위하여 통합 추론하는 알고리즘을 포함하고 있으므로 기존 시스템 보다 정확한 적산을 진행할 수 있도록 구성되었다고 밝혔다. 또한 이미지 뿐만 아니라 CAD 도면 데이터에서 형상 기반으로도 데이터를 인지할 수 있도록 도면 스케일 변환 및 환원 솔루션을 개발하여 적용 가능하게 했다고 설명했다.현재 인텔리코리아는 AI 인지를 통한 도면 작성 및 적산을 각 산업 분야로 확대하고 있으며, 추후 각 산업별로 축적된 데이터를 기반으로 설계 최적화 패턴을 학습시켜 설계자의 의도를 예측하는 최적의 솔루션을 제안하는 AI 기반 자동화 설계 프로그램 개발을 최종 목표로 하고 있다. 1년 이내에 설계자의 의도를 거의 완벽하게 구현할 수 있는 AI-CAD의 구현이 기대되고 있다.  [AI 이미지 인지 기반의 도면 적산 솔루션 개요도]   수작업으로 설계된 도면 파일, 캐드로 설계했을 지라도 디지털 파일은 없어지고 종이도면만 남은 경우에..본 프로그램은 상당한 위력을 발휘합니다. 2023년말 이내에 상업화 할 예정입니다.  ​인텔리코리아 인텔리코리아-사업소개-인공지능 CAD 솔루션 (cadian.com)문의 ; 070-4610-2340   "
YOLOv8 TOD(Thermal Object Detection) 1 ,https://blog.naver.com/itiv-ai-flir/223064356427,20230404,"YOLO 역사 속으로객체 검출(Object Detection) 분야 관련 AI 모델의 대표적인 접근법 중인 하나인 YOLO(You Only Look Once, 욜로)가 있습니다. 객체 검출이란 이미지가 주어지면 배경과 사물을 구분하고, 어떤 사물(Object)인지 인지하는 것을 말합니다. 다시 말해, YOLO는 이미지 내의 객체 위치를 예측할 수 있는 알고리즘입니다. YOLO는 2016년 처음 등장한 이후 계속 업데이트되었으며, YOLOv1, YOLOv2와 같이 버전명을 붙여서 YOLOv(버전, Version)이라고 부릅니다.​ ​YOLO 버전별 특징을 간략하게 살펴보겠습니다.​2016년에 최초로 발표된 YOLOv1 버전의 실시간 객체 검출을 위한 딥러닝 기반 네트워크 구조는 이미지 분류를 위한 24개의 컨볼루션 계층과 2개의 완전히 연결된 계층으로 구성되어 있습니다.​2017년에 발표된 두 번째 버전은 YOLOv1보다 성능을 개선하고자 일괄 정규화 계층을 추가하여 속도를 높인 것이 특징이고, 대량의 분류 데이터를 활용하기 위해 설계되었습니다.​2018년에 발표된 YOLOv3는 네트워크 구조와 학습 방법을 개선하여 객체 검출의 정확도와 속도를 모두 개선했으며, YOLOv4 버전(2020년 4월)은 이전 버전에서 발생했던 작은 객체 검출 문제를 해결하고자 최신 객체 감지 학습법(전처리, 후처리)를 도입 & SPP와 PAN 기술을 적용하여 YOLOv3보다 정확한 객체 검출과 더 높은 속도를 제공했습니다.​ YOLOv3 - YOLOv4 비교 (출처: https://yong0810.tistory.com/30)​그리고 2개월 후 YOLOv5가 발표되었다. (2020년 6월) YOLOv4와 비교하여 객체 검출 정확도가 10% 이상 향상되었으며, 더 빠른 속도와 다양한 크기(Small, Medium, Large, X-large) 모델로 나누어 사용할 수 있게 되었습니다.​ 출처: https://velog.io/@katinon/YOLOv5-%EA%B0%9C%EB%B0%9C%ED%99%98%EA%B2%BD-%EA%B5%AC%EC%84%B1-%EB%B0%8F-%EC%83%98%ED%94%8C-%EB%8D%B0%EC%9D%B4%ED%84%B0-%ED%9B%88%EB%A0%A8​그리고 2022년 7월에 발표된 YOLOv7에서 GPU Device를 타깃으로 Real-time object detection이 가능해졌으며, 훈련 과정 최적화에 집중하여 훈련 cost의 강화 없이 네트워크 성능을 향상하기 위한 trainable bag-of-freebies가 제안되었습니다.​2022년 9월에 발표된 YOLOv6 버전은 특이하게도 YOLOv7보다 늦게 출시되었습니다. YOLOv6는 여러 방법을 이용하여 알고리즘의 효율을 높이고, 특히 시스템에 탑재하기 위한 Quantization과 distillation 방식도 일부 도입되어 성능이 향상되었습니다.​​ YOLOv1부터 YOLOv7까지 Object Detector Architecture​​그리고 최근, 2023년 1월에 발표된 YOLOv8의 주요 기능은 확장성이며, YOLO 모델을 위한 완전히 새로운 리포지토리를 출시하여 객체 감지, 인스턴스 세분화 및 이미지 분류 모델을 트레이닝하기 위한 통합 프레임워크로 구축되어 다른 버전과의 성능 비교가 용이해졌습니다. YOLOv8은 '연결'이라는 특성으로 전처리된 이미지와 취득한 이미지를 학습에 이용하고, CPU에서 GPU에 이르기까지 다양한 플랫폼을 실행할 수 있어 프레임이 끊기지 않고 부드럽게 객체를 인식할 수 있을 것으로 보입니다. ​이처럼 YOLOv8은 높은 정확도와 함께 앞서 말씀드린 여러 장점들이 있기 때문에 앞으로 포스팅할 자료에는 YOLOv8을 활용하여 열화상 데이터가 AI 모델에서 어떻게 적용되고 사용할 수 있는지 알려 드릴 예정입니다 ​ YOLOv5부터 YOLOv8까지 비교 (출처: https://velog.io/@qtly_u/n4ptcz54#yolov8)​​[YOLO 관련 논문]* YOLOv1 : CVPR 2016 Open Access Repository (cv-foundation.org)* YOLOV2 : 1612.08242.pdf (arxiv.org)* YOLOv3 : [1804.02767] YOLOv3: An Incremental Improvement (arxiv.org)* YOLOv4 : [2004.10934] YOLOv4: Optimal Speed and Accuracy of Object Detection (arxiv.org)[1] You Only Look Once: Unified, Real-Time Object Detection, 2016[2] YOLO9000: Better, Faster, Stronger, 2017[3] YOLOv3: An Incremental Improvement, 2018[4] Rich feature hierarchies for accurate object detection and semantic segmentation, 2013[5] Abnormal Water Quality Monitoring Based on Visual Sensing of Three-Dimensional Motion Behavior of Fish[6] Non-max suppression ​​ "
Yolov8 2023년 1월 새 버전 발표 - 더 쉽고 더 빨라진 Yolo 객체감지(Object Detection) ,https://blog.naver.com/jcosmoss/222982060932,20230112,"Yolov3부터 시작해서 v5 둘러 보다가 더 높은 버전인  v7 살펴 봐야 하나 하고 있는데 엥~ Yolov8이 발표 되었다고 한다. 얼마전 2023년 1월 발표 되었다. 기능도 더 추가되고 기존 버전에 비해 속도와 성능이 더 발전 했다는 것 같다. 가만히 보니 버전에 따라 개발 히스토리가 좀 다른듯 하다. 자세한 것은 아래 사이트를 참조 하고 실제로 돌려 보기로 하자. ​GitHub - ultralytics/ultralytics: YOLOv8 🚀 in PyTorch > ONNX > CoreML > TFLite <사이트 발췌 / 다른 버전에 비해 성능이 뛰어남>설치는 더 간단하다. 파이썬 3.7이상 설치 되어 있는 라즈베리파이, 윈도우등에  다양한 환경에 설치가 가능하다. ​3가지 대표적인 기능을 제공하고 있다. Object DetectionInstance SegmentationImage Classification​pip install ultralytics ​ultralytics  패키지를 설치 하면 필요한 것들이 모두 자동 설치 된다. Github에서 패키지를 다운로드 받아서 설치도 가능하다. 설치가 완료되면 yolo 명령은 어디에서나 실행이 가능하도록 패스가 걸린다.​제공되는 학습된 모델Yolov8nYolov8sYolov8mYolov8lYolov8x​뒤로 갈수록 디테일하게 감지하지만  사이즈가 커지고 처리가 느리게 된다. 자신의 컴퓨터 사양에 맞은 모델을 선택하자. 이제 프로젝트 폴더를 하나 만들고 실행을 해 보자. CLI 즉 커맨드 라인으로 실행하면 된다. 파이썬 코드 필요 없다. ​yolo task=detect mode=predict model=yolov8n.pt source=""sample.jpg"" ​목적에 따라 여러가지 argument를 설정해 줄 수 있다. ​task= detect / classify / segment mode= train / predict / val / export model= yolov8n.pt / yolov8n-cls.yaml / yolov8-seg.yaml ​yolo task=segment mode=predict model=yolov8n-seg.pt source=""https://cdn.pixabay.com/photo/2022/02/25/04/11/traffic-7033509_960_720.jpg""​ Segment 기능은 위 그림처럼 감진된 영역을 배경과 분리하여 표시하는 기능이다. 신기하다잉~하드의 이미지가 아니라 온라인 이미지도 즉시 샘플로 사용이 가능하다. ​yolo task=segment mode=predict model=yolov8n-seg.pt source=""sample02.mp4"" show=True​https://youtu.be/dzT8iM87j5Q 동영상도 마찬가지로 적용할 수 있다. 명령어 라인에 show=True 인자를 주면 영상을 보여주면서 작동된다. ​다 좋은데 이걸 어떻게 어디다가 응용하지???? 파이썬 코드에 Yolov8을 넣어서 사용하는 방법에 대해서는 더 찾아 보기로 하자. 나온지 얼마 되지 않아서 튜토리얼이 별로 없다. ㅎ​​​ "
[Object Detection] SSD(Single Shot MultiBox Detector) ,https://blog.naver.com/ongbbb/222808647489,20220712,"문제 제기1. 기존 Object Detection 시스템들의 전반적 플로우는""BBox Proposals"" → ""각 BBox에 대한 픽셀이나 피처들을 resampling"" → ""좋은 퀄리티의 분류기 적용"" 이지만 정확도가 높은 반면, real-time application에서 속도가 너무 느리다는 단점이 있음​2. 기존 Yolo의 단점은 맨 마지막 피쳐맵만 사용하기 때문에 작은 물체 detection에서 정확도가 하락하는 한계​ 이 논문의 제안1. BBox 제안 과정 그리고 그다음의 픽셀이나 피처들 resampling 과정 제거- 각 Conv layer를 통과할 때마다 detection과 classification을 진행(기존 마지막 feature map만을 사용해 디테일한 정보들은 고려되지 못한 문제점 해결하고자)  - 각 층별 feature map들을 가져와 Object Detection을 수행, 이 결과들로 Loss function을 적용해 학습시킴. ​2. 다른 비율을 가진 BBox(default box 혹은 anchor)들을 생성​그 결과, 더 빠르고 정확한 모델을 만들었다​​​ Architecture전반적인 구조는 VGG16을 base로 가져오고, feature 추출을 위한 conv layers 추가했다test단계에서 ​ 어떤 conv layer를 통과해 나온 feature map (b), (c)가 있다​왜 다양한 사이즈의 feature map을 사용?feature map 사이즈가 크면 그만큼 원래 이미지에 대한 정보를 더 담고 있으므로 작은 물체 detection을 잘할 것이며사이즈가 작을 경우 큰 이미지에 대한 detection에 적합하다​SSD는 이런 점을 고려해 큰 사이즈부터 1x1 feature map들의 정보를 마지막 layer에 전달한다​ 1. 여러 사이즈의 Feature map의 정보를 받아와 세부적인 정보를 함께 고려하며   2. default box을 활용해 다양한 형태의 object들을 detect​ ​​이 중  conv4_3, conv7, conv8_2, conv9_2, conv10_2, conv11_2 에서는 추가 conv를 통해 loc, conf 값을 출력loc = [x, y, w, h]conf = 클래스 개수 + background 1개​ conv layer 하나를 자세히 들여다보면 위의 그림과 같다*k = 3, c = 20, size = 5 x5 1. 먼저 feature map을 대상으로 default box를 생성한다(Yolo에서 각 그리드가 BBox를 가졌던 것처럼) → 5x5xk​2. feature map에 3x3 conv를 적용해 BBox regression 계산 → [x, y, w, h] ​​    따라서 k개의 default box 가 있다면 output feature size는 5x5x(4*k) ​3. 각 default box 마다 classification을 진행하는데 클래스 개수가 c개라면최종 output feature  map size는 5*5*((c+1)*k))​​각 층별 피쳐 맵들을 가져와 Object Detection을 수행한다 → 총 6개의 레이어에서 나온 default boxes  = 8732개이 중  gound truth와 jaccard overlap(IoU) 계산, threshold 0.5이상인 것을 남김 결과들을 모두 합하여 로스를 구한 다음, 전체 네트워크를 학습시키는 방식으로 1 Step end-to-end Object Detection 모델을 구성​​​​ Loss function​(1)을 보면 loc, conf 두개로 구성되어있으며 가중치를 달리한다(그대로 더하는 것은 타당하지 않으므로)(2)를 보면 smoothL1으로 loss를 계산하는데 이 부분은 아래에서 더 자세히 설명하려고 한다.(3)을 보면 Loss conf가 YOLO와 다른 점을 알 수 있다. 이부분도 아래에서 자세히!​​​ 해결의 논리가 실험으로써 증명됐는지, 어떻게 증명할 수 있는지 그에 상응하는지새롭게 디자인한 실험이 있다면 ,어떻게 , 의미/ 1. VOC2007test 이미지 데이터 등의 데이터로 실험- Person: person Animal: bird, cat, cow, dog, horse, sheep Vehicle: aeroplane, bicycle, boat, bus, car, motorbike, train Indoor: bottle, chair, dining table, potted plant, sofa, tv/monitor로 Fast 그리고 Faster  R-CNN과 비교했음​2. data augmentation, box ratio, use atrous 요인의 영향을 위한 실험도 진행 *atrous는 아래에서 자세히 설명​3. 추가로 해본 것들sampling strategy, similar to YOLO - 다른 R-CNN방법들은 pooling을 사용해서 sampling strategy의 benefit을 덜 받았음​​4. SSD는 input size가 클 때 더 나은 성능을  보임 → 작은 사물 detection에서 정확도가 떨어지므로 input 사이즈를 늘림​**만약 YOLO의 단점인 small size object detection에 대한 성능 측정을 하고자 한다면? (다양한 사이즈의 object를 가진 이미지이며 우리는 작은 사이즈 object에 대한 성능만 측정하고 싶다, 답변은 내 생각)이미지에 padding을 해서 전체 사이즈를 키우고 큰 이미지도 작게 만들어버려서 input특정 사이즈 이상의 BBox는 그냥 제외시켜서 성능 측정​​ smooth L1 - L1 Loss의 경우 미분 불가능한 지점 존재,  but L2에 비해 이상치에 대한 영향은 적음- L2 Loss의 경우 모든 지점 미분 가능하지만, 이상치에 취약하다는 단점 존재(제곱하므로 그 영향이 더 커진다는 의미), 수렴하기 어렵다​→ smooth L1은 모든 지점 미분가능하며 이상치에 강함​**왜 [-1, 1] 구간에서 L2를 쓰고 L1을 쓰는 게 나왔는가?- gradient update 관점에서 보면 pred이 real에 가까워진 시점에 L1을 쓰게 되면 gradient가 1혹은-1로 업데이트 되는데 이러면 수렴하지 않고 겉도는 시간이 많아질 수 있음​​​​​ atrous Conv중간에 0을 채워넣어서 동일한 해상도에서 일반적인 conv보다 더 큰 receptive field를 가져갈 수 있다feature가 더 dense하게 그리고 두드러지게 추출됨 왜 더 큰 receptive field가 필요한가?일반적 분류 문제에서는 대상의 여부에 대한 축약된 정보만 필요하지만(detail < global information)semantic segmentation이나 object detection에서는 더욱 세밀한 정보가 필요하므로 pooling layer를없애고 atrous conv를 이용해 receptive field를 확장​​​​​​​​참고자료 출처 https://velog.io/@gjtang/Huber-Loss%EB%9E%80https://better-tomorrow.tistory.com/entry/Atrous-Convolution Atrous ConvolutionAtrous Convolution 1. 일반적인 convolution 2. Atrous convolution(dilated convolution) 위 두 이미지를 한 번 살펴보자 일반적인 convolution과 달리 atrous convolution의 경우 kernel 사이가 한 칸씩 띄워..better-tomorrow.tistory.com https://csm-kr.tistory.com/4 [Object Detection] SSD 논문리뷰 (ECCV2016)안녕하세요 pulluper 입니다 :) 오늘은 eccv 2016 에 발표된 one-stage obejct detection 인 ssd 에 대하여 알아보겠습니다. SSD: Single Shot MultiBox Detector 는 YOLO 와 함께 real-time one stage object det..csm-kr.tistory.com https://jungnamgyu.tistory.com/57 [Computer Vision] Single Shot Multi box Detection (SSD)앞에서의 이론들을 바탕으로 Object Detection 모델인 Single Shot Multi box Detection(SSD)를 구성할 수 있습니다. SSD 설명에 앞서 Object Detection에는 크게 2가지 종류가 있습니다. 1-stage Detector : Loca..jungnamgyu.tistory.com 논문: https://arxiv.org/pdf/1512.02325.pdf "
"Object Detection in 20 Years - 01, Introduction ",https://blog.naver.com/tory0405/222837305284,20220802,"우연히 영상 객체 인식에 관련하여 좋은 논문을 읽게 되었는데  중요한 기술적 개선과 문제점, 앞으로의 방향성까지 두루 설명하고 있어 도움이 될까 하여 정리해 볼까 한다.   [ 논문 원본] 첨부파일1905.05055.pdf파일 다운로드 논문의 제목은 ""Object Detection in 20 Years"" , 말 그대로  20년 동안의 객체 인식이다. ​논문의 초록만 보더라도 흥미로움이 가득 찬  내용이다.   ""컴퓨터 비전에서 가장 기본적이고 도전적인 문제 중 하나가 최근 몇 년 동안 큰 주목을 받은 object dectection이다. ​해당 논문은 25년 동안(1990년대에서 2019년까지) object detection에 관련된 400개 이상의 논문을 광범위하게 검토하였고  milestone detectors in history, detection datasets, metrics, fundamental building blocks of the detection system, speed up techniques,  the recent state of the art detection methods 등을 포함하고 있다. 그뿐만 아니라 후반부에는 pedestrian detection, face detection,  text detection과 같이 중요한 detection application들을 포함하고 있고 최근 몇 년 동안의 기술적 개선과 과제에 대해 심층 분석하고 있다.  소개Object Detection은 컴퓨터 비전에서 중요한 작업이다. 즉, 디지털 이미지에서 특정 class( 인간, 자동차, 동물 등)의 시각적 개체 인스턴스를 감지하는 작업으로 계산모델과 기술을 개발하는 것을 의미한다. ​응용프로그램 관점에서는 ""일반 물체 감지"", ""감지 응용"" 두 가지 주제로 그룹화할 수 있고 전자는 시각과 인지를 의미하고 후자는 보행자 감지(pedestrian detection), 얼굴 감지(face detection), 테스트 감지(text detection) 등을 예로 들 수 있다. ​최근 몇 년 동안 딥러닝 기술을 통해 급속한 발전을 이루었고 이를 통해 자율주행, 로봇비전, 비디오 감지 등과 같은 실제 응용 프로그램으로 점차 확대되고 있고 관련된 출판물도 증가하는 추세이다.  ​최근의 object detection system은 “multiscale detection”, “hard negative mining”, “bounding box regression""와 같은 많은 기술과 통합되었지만 이런 정교한 기술의 특징을 이해하는데 어려움이 있었고 이러한 어려운 부분에 대한 심층 분석을 제공한다. ​또한 object detection의 처리 속도는 오랫동안 중요했지만 어려운 작업이었기에 “detection pipeline”,  “detection backbone”, “numerical computation” 과 같은 topic도 다룰 예정이다. ​객체 감지 로드맵은 지난 20년 동안 객체 감지의 발전을 볼 수 있는 그림으로 일반적으로 전통적인 방식과 딥러닝 기반의 기간을 잘 설명하고 있다.  ​  1. https://blog.naver.com/tory0405/222837305284 Object Detection in 20 Years - 01, Introduction우연히 영상 객체 인식에 관련하여 좋은 논문을 읽게 되었는데 중요한 기술적 개선과 문제점, 앞으로의 방...blog.naver.com 2. https://blog.naver.com/tory0405/222843224099 Object Detection in 20 Years - 02, Traditional Detection / Deep Learning based Detection앞장에서 설명한 것처럼 Object Detection은 크게 전통적인 방식(Traditional Detection)과 딥러닝 기...blog.naver.com 3. https://blog.naver.com/tory0405/222844508351 Object Detection in 20 Years - 03, Object Detection Datasets and Metrics[논문 원본] DataSet 더 작은 bias을 이용하여 더 큰 dataset을 구축하는 것은 고급 컴퓨터 비전 알고리...blog.naver.com 4. https://blog.naver.com/tory0405/222845416501 Object Detection in 20 Years - 04, Technical Evolution in Object Detection[논문원본] 탐지 시스템의 몇 가지 주요한 빌딩블록과 지난 20년 동안의 기술적 발전을 알아보자. Early ...blog.naver.com 5. https://blog.naver.com/tory0405/222851516420 Object Detection in 20 Years - 05, Speed-Up of Detection[논문 원본] 물체 감지 속도는 오랫동안 중요했지만 어려운 도전 과제이기도 하다. 지난 20년 동안 “speed...blog.naver.com 6. https://blog.naver.com/tory0405/222852484625 토리삼촌 : 네이버 블로그SW 케렌시아blog.naver.com 7. https://blog.naver.com/tory0405/222857584818 토리삼촌 : 네이버 블로그SW 케렌시아blog.naver.com ​ "
Object detection(6) _One-stage 2D object detectors: YoLo[CVPR'16] ,https://blog.naver.com/summa911/222994791220,20230126,"YoLo = You Only Look Once하나의 CNN만 학습시켜도 하나의 이미지에서 여러개의 객체를 찾는데 충분하다는 아이디어로 만들어짐.한번의 CNN 학습으로 classification 뿐만 아니라 98개의 box proposal도 가능하게 한 모델.정확도가 떨어진다는 단점이 있다. 1) Detection Flow (=inference flow)​-Box&Score이미지를 grid cell로 나눔각 셀마다 여러개의 boundin box를 proposalbounding box는 grid cell의 center point를 포함해야함.각 cell은 무조건 어떤 class에 속해있다.  BB 1개당 5개의 요소가 필요하다.​-Pred Metric각 cell에서, proposed Bounding Box갯수를 B또, cell하나당 class probability를 C개 도출해야 한다고 가정하면각 cell 은 5*B+C 의 크기를 가지는 벡터가 각 셀마다 제안되어야 한다. 전체 이미지를 S*S 개의 grid로 만들었다면, S*S*(5*B +C) 크기의 텐서가 도출됨.​​-Refiningconfidence가 낮은 BB는 지운다Non-max suppression 사용= one box for one object ​​2) YoLo - CNN architectureOne CNN (24 conv layers and 2 fc layers)​input = 전체 이미지(bounding box 1개가 인풋은 아님.)last layer 가 box proposal과 classification 둘 다 함. 마지막 2개의 fc layer가 전체 이미지를 다 섞어서 이해할 수 있게 해준다. 다른 층들은 전체 이미지를 분석하면서 feature vector를 뽑아냄.​​3)YoLo-Training step 1) classification을 위해서 앞쪽 20개의 layer를 pretraining.classification용 data set을 이용해서 pre-train하고, 해당 layer를 fix함.​step2)  convert the model for detection( transfer learning)모델 초반의 classification용 20개 layer conv는 고정하고,transger learning 을 위해서 마지막 average pooling과 fc layer를 제거그리고, 4개의 새로운 conv layer와, 2개의 fc layer를 random weight를 가지게 학습.6개의 층은 classification 과  detecting이 둘 다 되어야 하기 때문에 multi-task loss function 을 사용해서 학습시킨다.여러개의 proposed box중에 제일 IoU가 높은 하나의 박스만 object 를 위해서 선택된다.-->애매 다시생각해보기​4) YoLo - Loss FunctionLoss function이  왜 이렇게 되는지 다시 듣고 생각해보기 5) YoLo - PerformanceR-CNN 은 이미지 하나당 40s가 걸렸다면, YoLo는 22ms가 걸릴 정도로 빠르다.Fast R-CNN과 비교하면, mAP(정확도)sms Fast R-CNN이 70.0 정도라면, YoLo는 63.4 정도로 성능이 약간 떨어진다. 하지만, FPS(처리 속도)가 YoLo는 45, Fast R-CNN은 0.5로 90배나 차이가 난다. YoLo를 이용하면 정확도를 조금 손해보고, 엄청 빠른 속도로 처리가 가능하다.​6) YoLo 의 문제점YoLo는 Localization이 조금 부정확하다.YoLo는 sliding window 방법처럼 전체 이미지를 하나하나 훑는 것도 아니고, selective search같은 방법으로 따로 localization을 하는 것도 아니다.전체 이미지를 훑으면서 데이터를 통해서 box를 proposal하는 것이기 때문에 정확도가 떨어진다.그리고 새롭거나 unusual한 비율의 박스크기는 그려내기 어렵다.또, square root를 loss function에 써서 width와 height에러를 잡으려고 했지만 부족함이 있었다. 그리고 각 셀은 하나의 클래스만 제안하기 때문에 작은 object들이 무리 지어 있을 때는 detecting 하기 어렵다. gird갯수(49개)만큼만  box proposal할 수 있다.​​[출처] https://youtu.be/MRPsDN7dRJY [출처] J Redomn et al. ""You only look once: Unified, real-time object detection""https://arxiv.org/abs/1506.02640 You Only Look Once: Unified, Real-Time Object DetectionWe present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bound...arxiv.org "
[Object Detection 시리즈] Two-Stage Detection(RCNN review) ,https://blog.naver.com/koreadeep/222654824872,20220222,"이번 포스팅에서는 Two stage Detection 접근법의 시초 격인 RCNN 알고리즘에 대해 리뷰해 보도록 하겠습니다.이 알고리즘을 잘 이해함으로써 이후에 파생되는 FastRCNN, FasterRCNN 등의 이해에도 크게 도움이 될 수 있을 것입니다.​ Object detection Milestones먼저 아래 그림을 보도록 하겠습니다.Object detection field의 milestone인데요2006년도를 보면 지난번 리뷰했던  HOG descriptor 논문이 보입니다.당시에만 해도 딥러닝 접근보다 Feature를 Signal processing 기법을 통해 추출하고,SVM과 같은 ML(Machine Learning) 모델의 입력으로 넣어 학습을 진행하는 접근이 주를 이루었습니다. ​그러던 중 2012년을 기점으로 AlexNet이라는 모델의 등장과 함께Featuer Extraction 파트를 Machine에게 맡겨 학습이 이뤄질 수 있도록 하는 형태인딥러닝 기반 CNN 네트워크가 대세가 되었습니다.이후 VGG, GoogleNet, ResNet 등 유명한 ConvNet 들이 등장하는 계기가 되었죠.​어느 정도 Image Classification 분야의 성능이 궤도에 오르자,Object detection이라는 보다 어렵고 복잡한 새로운 문제를 풀기 위한 연구가 시작되었습니다.이 문제를 풀기 위한 접근 방법으로는 크게 두 가지 접근 방법이 있습니다.하나는 2014년 RCNN 모델을 필두로 하는 Two-stage detector이고,다른 하나는 2016년 YOLO 모델을 필두로 하는 One-stage detector 가 바로 그것입니다.앞서 말씀드렸듯이 본 포스팅에서는 RCNN 모델의 이해에 초점을 맞춰서 진행해 보도록 하겠습니다. ​ RCNN Model Architecture아래 그림은 RCNN 모델의 Flow입니다.한번 살펴보도록 하죠.입력 이미지가 들어오고 (1번)여기서 Object 가 있을 법한 영역을 약 2000여 개 정도 추출합니다(2번)이후 이 영역을 Warping 하여 고정 크기의 입력 자료 구조 형태로 만들고,이것을 CNN 네트워크에 입력으로 넣어줍니다(3번)그러면 각 영역별로 해당 영역이 담고 있는 object가 무엇인지 판별해서 예측하는 것입니다(4번)지극히 상식적인 과정이죠? 조금 더 디테일하게 살펴보도록 하겠습니다.아래에서 위쪽 방향으로 진행하는 형태로 표현되어 있습니다.약 2000개의 region을 추출하고, warping 한 후ConvNet을 통과시켜서 Feature map을 만들어 냅니다.그 이후 이것을 Bbox reg 와 SVMs 을 진행합니다. Bbox reg는 Bound box regression의 약어로, 추출한 region의 좌표를 추정하는 모델입니다.object가 판별이 된다면, 그 object가 original 이미지 상에서 어디에 존재하는지 파악해서 bound box를 그리기 위함입니다.*SVM은 영역 내에 존재하는 object의 카테고리를 판별하는 분류 모델입니다. ​RCNN 논문에서 CNN (AlexNet) 모델을 기반으로RCNN 모델을 디자인한 과정을 알아보도록 하겠습니다.먼저 AlexNet 은 ImageNet에 대해 충분히 학습된 모델입니다.동작은 입력 이미지를 가져와서 Convolution 및 Pooling 과정을 통해 정보를 추상화하여,특징 맵 (Feature map)을 만들어 냅니다.그리고 이 정보를 바탕으로 1000개의 카테고리를 구분하기 위한 Classifier 네트워크인Fully connected layer를 이어 붙이고 마지막에는 1000개의 출력을 softmax 활성함수를 통해 출력하게 되어있습니다. ​이때,RCNN이 학습할 Object detection 용 데이터는 VOC2007 데이터로 21가지의 클래스를 갖고 있는 데이터 셋입니다.따라서 뒷단을 21개의 확률 값으로 출력하게끔 변경해 줍니다. ​Convolution & Pooling 과정 중 다섯 번째 pooling layer의 결과를 별도로 저장합니다.Bound box regression을 하기 위함입니다.​ Bound box regressionBound box regression 이란 예측한 bound box의 좌표와 실제 정답(ground truth) 좌표들 간의 수치의 차이에 대해 학습함으로써, 예측값을 보정하는 것을 말합니다.  ​예를 들어,학습에 사용된 image region에 따라 보정값 정보들에 대해 알아보겠습니다.아래 그림의 첫 번째 이미지의 경우 고양이가 정중앙에 잘 위치해 있습니다.이것의 feature map을 생성하여 bound box regression 알고리즘을 통해 보정값을 예측해 보면dx, dy, dw, dh 가 모두 (0,0,0,0)인 것을 확인할 수 있습니다.즉 어느 방향으로도 보정해 줄 필요 없다는 의미입니다.한편 가운데 고양이 이미지를 보면 우측으로 쏠려있는 것을 확인할 수 있습니다.따라서 우측 방향으로 보정하게끔 dx 방향에 해당하는 값이 있습니다.마지막 이미지의 경우에도 고양이가 zoom out 되어 있기 때문에dw 보정값이 존재합니다. 자 그러면 이 값들이 어떤 원리로 정해지는 것인지 살펴보겠습니다.먼저 좌표를 정의해 보겠습니다.P는 예측되는 bound box의 좌표, G는 정답(ground truth) 좌표입니다.그리고 이 둘 좌표 간의 관계식을 정의합니다.x, y 좌표는 평행이동의 관계, w, h는 exponential 함수를 이용하여 식을 정의했습니다. 그리고 P 좌표와 G 좌표의 차이 t를 정의합니다.scalability를 고려하여 predicted box의 width height 값으로 나눠주었습니다.이때, delta(p)는 5번째 pooling layer의 결과에 가중치를 곱한 것으로 정의됩니다. 결과적으로 손실 함수는 아래와 같이 정의됩니다.예측값과 실제 값의 차(t)와 델타(p)의 차의 MSE로 관계식을 정의했습니다. ​논문에서는 왜 5번째 pooling layer를 선택하게 된 이유로실험을 통해 확인해 본 결과 5번째 layer가 가장 일반화된 Feature를 갖고 있다고 판단되었기 때문이라고 설명하고 있습니다.   Performance앞선 포스팅에서 다룬 object detection 성능지표 mAP에 따르면기존 모델인 DPM , Regionlets 대비하여 높은 수치를 달성한 것을 확인할 수 있습니다.추가적으로 backbone을 VGG로 바꾸고, bound box regression 모델을 반영하여추가적인 성능 개선을 달성했다고 설명하고 있습니다. ​이후 제안되는 모델들이 RCNN Fast, Faster.. 인 것에서 유추해 볼 수 있듯이RCNN은 속도가 느리다는 치명적인 결함을 갖고 있습니다.약 2천여 개의 region을 추출해서 각각 convnet을 거치고 또 각각 bound box regression , SVM classifier를 진행해야 최종적인 예측값(좌표, 카테고리 정보)을 출력할 수 있기 때문에,이 모든 과정을 수행하는 데 소모되는 리소스(연산량)가 굉장히 많은 것이 inference time을 지연하는 주된 이유라고 할 수 있습니다. (VGG16을 backbone으로 사용한 모델을 기준으로, 학습에는 총 84시간이 소요되었고, inference time에는 무려 47초가 걸렸다고 합니다)이러한 defect가 있다는 것을 유념하고, 앞으로 전개될 포스팅에서는이러한 단점을 어떻게 극복해 내는지에 주안점을 두고 Fast / Faster RCNN 모델을 이해해 보도록 하겠습니다. ​그럼 이상으로 포스팅을 마치도록 하겠습니다.감사합니다:)​  한국딥러닝(주)는 인공지능, 머신러닝, 딥러닝 솔루션을 개발하여 세상이 해결할 수 없는 문제를 해결하고 제안하고 있습니다. 끊임없이 발생되는 새로운 빅데이터와 응용 기술의 홍수 속에서 최신 인공지능 기술 동향을 예측하고 빠르게 반영하는 한국딥러닝을 만나보세요.​ ​ "
"물체 탐지 2D, 3D object detection point pillar ",https://blog.naver.com/wjrurtn517/222849210231,20220815," 2D Object Detection에서 one-stage detector 방식 중 하나인 centernet는 객체를 검출할 때 미리 지정된 이미지anchor가 아닌 하나의 이미지를 통해 객체를 검출한다. 이것은 2D image에 대한 빠른 추론 속도를 가진다.​​​(A) Backbone: 단계에서는 2D Image로부터 특징값을 추출한다.(B) Downsampling: Backbone에서 추출된 feature map의 해상도를 줄임으로 계산을 절약한다.(C) Head: 본격적인 객체의 검출한다. 3D Object Detection은 클러스터링 과정을 거친다. PointCloud(input)에서 지면 값을 제거한다. 이후 클러스터링을 거쳐 클래스 분류하고 클래스 클러스터 (class cluster)로 마무리된다.  One-stage 방식의 Point Pillars는 Point Cloud의 voxel data(공간 정보)를 기반으로 2D Image로 변환하는 network의 복합 구성이다. 이는 정확도 및 속도에 대한 이득을 모두 취할 수 있다. 기둥 Pillars(voxel data)들마다 특징을 구하여 객체의 형상, 위치, 방향을 구하는 알고리즘이다. 각 단계에서 Point Cloud의 각 point는 [x, y, z, Intensity]의 특징값들을 가진다. 이를 XY-grid처럼 구역화하여 각 points를 해당 영역에 배치한 후, 해당 구역의 Pillar로 재구성되며 Pillar의 한 point가 가지는 정보는 D=[x, y, z, Intesity, Xc, Yc, Zc, Xp, Yp]로 구성된다.  ​생성된 Pillars는 Stacked Pillars라는 단계를 통해 3차원 데이터로 구성하게 된다. D는 기존 Pillar의 특징인 [x, y, z, Intensity, Xc, Yc, Zc, Xp, Yp]를 의미하며 P는 Xy-grid의 Pillar index를 의미, Pillar 내부의 포인트 수를 N으로 표현하며 Pillar의 포인트 수가 희소할 경우, zero padding을 통해 값을 채우고, 반대로 Pillar의 포인트 수가 많을 경우, 초과하는 부분에 대해 무작위 샘플링을 진행한다. 이렇게 Pillar의 제한을 적용함으로써 Stacked Pillars 단계에서는 밀도가 높은 (D, P, N)으로 구성될 수 있다. Stacked Pillars들의 각 Point들은 우선 1*1 Conv layer[차원 축소 과정]을 통해 [C, P, N]으로 차원이 변경된다. 이후 BatchNorm layer[정규화 과정], ReLU layer[비선형성 부여] 및 Max Pooling layer(최대 특징값 추출) 과정을 통해 [C, P]로 차원이 변경된 후, 원래 Pillar의 위치로 특징값들을 돌려놓아서 수도 이미지 [C, H, W]를 생성하게 된다. 본 논문에서는 채널의 수 C는 64개, LiDAR 전방 범위 H는 432[(0m ~ 69.12m) / 0.16m], LiDAR 측면 범위 W는 496[(-39.68m ~ 39.68)m / 0.16m]로 지정했다. Backbone network에서는 두 개의 서브 네트워크로 구성되어 있다. 우선, Pseudo image는 Top-down 네트워크를 통해 공간적 정보는 적으나 함축 정보가 높은 특징 맵을 추출한 후, 업샘플링 레이어(Deconv)를 통해 공간적 정보를 추출한다. 이러한 Feature Pyramid Network 구조를 통해 얻은 Feature map들을 모두 연결함으로써 함축 및 공간 정보를 모두 취하였다. ​Backbone network에서 얻은 Feature map을 통해 객체의 크기, 위치, 방향, 각도, 클래스를 최종적으로 출력한다. "
[논문리뷰] PointPillars: Fast Encoders for Object Detection from Point Clouds ,https://blog.naver.com/einstephener/223065354532,20230405,"Point Cloud Object Detection 계열에서 안정된 성능으로 오랫동안 사용되고 있는 기술. Lidar Only Detection. 아이디어 역시 깔끔함.​- 아이디어: PointNet 으로 가상의 Voxel 형태 인, Pseudo Image 를 만들어 내고, 이런 가상 이미지에 기존 SSD Object Detection 방식을 붙여서 BBox 탐지.  ​- 특징: Point 기반의 표현 방식과 Voxel 기반의 표현 방식의 장점을 모두 살림.​- Data Augmentation: Rotation, Mirroring, Flip, Scaling, Translation ​ ​ "
3D Object Detection Survey - 2. Sensors ,https://blog.naver.com/mikangel/222476284902,20210819,"우리 인간은 운전할 때 실제 세계를 인식하기 위해 시각 및 청각 시스템을 활용합니다. 그렇다면 자율 주행 차량은 어떻습니까? 그들이 인간처럼 운전한다면 도로에서 끊임없이 보이는 것을 식별하는 것이 가야 할 길입니다. 이를 위해서는 센서가 중요합니다. 장애물 인식, 추월, 자동 비상 제동, 충돌 회피, 승차 공유, 신호등 및 보행자 감지 등 차량에 일련의 능력을 부여하는 센서입니다. 일반적으로 가장 일반적으로 사용되는 센서는 두 가지 범주로 나눌 수 있습니다. 수동 센서 및 능동 센서. 업계 전문가들 사이에서 진행 중인 논쟁은 차량에 카메라 시스템(LiDAR 없음)을 장착할지 아니면 온보드 카메라 시스템과 함께 LiDAR를 배포할지 여부입니다. 현재 웨이모(Waymo), 우버(Uber), 벨로다인(Velodyne)은 라이다(LiDAR)를 지원하는 반면 테슬라(Tesla)는 카메라 시스템을 선호하면서 수적으로 앞섰습니다. 카메라가 패시브 센서의 대표적인 대표자 중 하나로 간주되고 LiDAR가 능동 센서의 대표자로 간주된다는 점을 감안할 때 먼저 패시브 센서와 능동 센서의 기본 개념을 소개한 다음 카메라와 LiDAR를 예로 들어 논의합니다. 그들이 어떻게 자율 주행 시스템을 제공하는지, 장단점은 표 II 에 나와 있습니다. ​A. 수동형 센서 ​수동형 센서는 지구 표면과 대기 모두에서 자연적으로 방출되는 빛을 받는 것입니다. 이러한 자연 방출은 자연광 또는 적외선일 수 있습니다. 예를 들어, 카메라는 렌즈의 광학 장치에서 많은 색상 포인트를 직접 잡고 장면 이해를 위한 디지털 신호라고 하는 이미지 어레이로 정렬합니다. 이미지 신호를 분석하기 위한 컴퓨터 비전 알고리즘의 최근 발전에 영감을 받아 2D/3D 물체 감지가 크게 발전했습니다. 지금까지 Tesla는 카메라 시스템(LiDAR 없음)을 성공적으로 활용하여 자율 주행 차량에 대한 360도 뷰를 확보해 왔습니다(2021년 6월 본문 기준). 주로 단안 카메라는 유익한 색상 및 질감 속성, 도로 표지판의 텍스트에 대한 더 나은 시각적 인식, 무시할 수 있는 비용으로 높은 프레임 속도 등에 적합합니다. 반면에 현실 세계에서의 정확한 위치를 추정하는 깊이 정보는 부족합니다. 이를 극복하기 위해 스테레오 카메라는 매칭 알고리즘을 사용하여 깊이 복구를 위해 왼쪽 및 오른쪽 이미지의 대응을 정렬합니다. 카메라는 신뢰할 수 있는 비전 시스템으로서 잠재력을 보여주었지만 독립형 시스템으로는 충분하지 않습니다. 특히 야간이나 우천 시 조도가 낮은 경우 카메라의 정확도가 떨어지는 경향이 있습니다. 결과적으로 Tesla는 카메라 시스템이 오작동하거나 연결이 끊어지는 경우에 대비하여 보조 센서를 사용해야 합니다.​B. 능동 센서 ​능동 센서는 센서에 의해 전송되는 반사 신호를 측정해야 하며, 이 신호는 지표면이나 대기에 의해 반사됩니다. 일반적으로 LiDAR(Light Detection And Ranging)는 렌즈, 레이저 및 검출기의 세 가지 기본 구성 요소가 있는 바로 보고 찍는 장치입니다. "" 포인트 클라우드”는 높은 희소성와 불규칙성, 텍스쳐 속성의 부재는 이미지 어레이와 잘 구별되는 LiDAR의 주요 특징이다. 우리는 빛이 얼마나 빨리 이동하는지 이미 알고 있기 때문에 장애물의 거리는 노력 없이 결정할 수 있습니다. LiDAR 시스템은 제공되는 차량 주변의 360도 보기와 함께 초당 원을 그리며 회전하는 수천 개의 펄스를 방출합니다. 예를 들어 Velodyne HDL-64L은 10Hz 프레임 속도로 프레임당 120,000포인트를 생성합니다. 분명히 LiDAR는 자체적으로 광 펄스를 방출하기 때문에 외부 조명 조건(예: 야간)의 영향을 덜 받습니다. LiDAR 시스템은 카메라 시스템에 비해 높은 정확도와 신뢰성으로 호평을 받았지만 항상 그런 것은 아닙니다. 특히, LiDAR의 파장 안정성은 온도 변화에 취약한 반면 악천후(예: 눈 또는 안개)는 LiDAR 감지기의 SNR(Signal-to-Noise Ratio)이 좋지 않은 경향이 있습니다. LiDAR의 또 다른 문제는 높은 비용입니다. Velodyne에 따르면 지금까지 보수적으로 추정한 금액은 약 $70,000입니다. LiDAR의 가까운 미래에 비용을 줄이는 방법과 해상도와 범위를 높이는 방법은 전체 커뮤니티가 앞으로 나아가야 할 부분입니다. 전자의 경우 Solid State LiDAR의 출현은 고정된 시야를 따라 광 펄스를 방출하는 여러 고정 레이저의 도움으로 이러한 비용 감소 문제를 해결할 것으로 예상됩니다. 후자의 경우 128개의 레이저 펄스와 300m 반경 범위를 특징으로 하는 새로 발표된 Velodyne VLS-128이 판매되었습니다. 이는 공공 안전 측면에서 더 나은 인식 및 추적을 크게 촉진할 것입니다.​​C. 두 센서간 논의 ​자율주행차에서 발생하는 사망자는 이미 안전에 대한 사회의 심각한 우려를 증가시켰습니다. 자율주행차가 합법적으로 도로를 주행하려면 최소한 높은 정확도, 높은 확실성, 높은 신뢰성이라는 세 가지 기본 특성을 충족해야 합니다. 이를 위해서는 두 세계(카메라 vs. LiDAR)의 장점을 통합한 센서 융합이 필요할 것이다. 센서 관점에서 LiDAR는 선형 오류에 가까운 깊이 정보를 높은 수준의 정확도로 제공하지만 악천후(예: 눈 또는 안개)에 취약합니다. 색상이나 질감 속성을 사용할 수 있는 경우 카메라가 직관적으로 시각적 인식에서 훨씬 더 우수하지만(그림 참조)  앞서 언급한 것처럼 독립형 시스템으로는 충분하지 않습니다. 확실성은 여전히 ​​중요하지만 크게 탐구되지 않은 문제입니다. LiDAR와 카메라의 조합은 탐지 정확도를 보장하고 예측 확실성을 향상시킬 것으로 예상됩니다. 신뢰성과 관련하여 센서 보정과 시스템 이중화라는 두 가지 측면을 고려해야 합니다. 센서 교정은 의심할 여지 없이 배포의 어려움을 증가시키고 전체 시스템의 신뢰성에 직접적인 영향을 미칩니다. 연구[48-50]는 시간 경과에 따른 드리프트를 피하기 위해 센서를 보정하는 방법을 조사했습니다. 시스템 이중화는 오작동 또는 정전이 발생한 경우에 대비할 보조 센서를 갖는 것입니다. 커뮤니티는 단일 센서에 대한 과도한 의존의 안전 위험을 예리하게 인식해야 합니다. 더 자세한 내용은 섹션 IV-C에서 논의될 것입니다.​#3D #ObjectDetection #LiDAR​ "
[Object D.] Rich feature hierarchies for accurate object detection and semantic segmentation (R-CNN) ,https://blog.naver.com/kona419/223044705706,20230314,"논문 : https://arxiv.org/abs/1311.2524 Rich feature hierarchies for accurate object detection and semantic segmentationObject detection performance, as measured on the canonical PASCAL VOC dataset, has plateaued in the last few years. The best-performing methods are complex ensemble systems that typically combine multiple low-level image features with high-level context. In this paper, we propose a simple and scalab...arxiv.org Abstract 본 논문에서는 매우 간단하고 객체 탐지에서 잘 쓰일 수 있는 알고리즘을 소개함. 객체 localize와 분할을 위해 bottom-up region proposal에 CNN을 적용함사전 훈련과 fine-tuning을 적용해 라벨링된 데이터가 부족한 상황에서 성능을 올림.​Introduction본 논문은 CNN의 localization 문제를 ""recognition using regions""로 해결함. 입력 이미지를 받음.2000개의 후보 영역을 제안함.CNN을 이용해 각 feature 연산을 진행. SVM을 이용해 각 영역에 대해 분류 작업을 수행함.​CNN의 또다른 문제인 라벨링된 데이터가 부족한 문제를 해결하기 위해서 unsupervised pre-training을 활용함.큰 데이터 셋으로 supervised pre-training을 진행하고 작은 데이터셋으로 원하는 도메인에 맞게 fine-tuning을 진행함.  --> 성능 good​Object detection with R-CNNR-CNN은 세가지 모듈로 구성됨.각 클래스별로 영역을 제안하는 모듈.각 region에서 고정된 길이의 feature 벡터를 추출하는 큰 CNN 모듈선형 SVM 모듈.​ - Module designRegion proposals :   - Selective Search 기법을 이용해 이미지에서 object 위치를 추출함.  - 물체가 있을 법한 영역을 찾는 방법.​ - Feature extractionregion의 이미지 데이터를 CNN과 호환되도록 이미지의 크기를 고정시켜야함. (227x227) -> 구조를 단순하게 만들기 위해데이터를 고정 크기로 바꾸는 작업 -> warping 이미지 데이터를 고정크기로 바꾼 결과Test time detectiontest 단계에서 각 테스트 이미지 별로 selective search를 적용해 2000개의 후보 영역을 추출함.CNN으로 forward propagation을 수행해서 후보 영역을 warping하고 feature를 구함.이 feature를 활용해 SVM으로 각 클래스별 점수를 계산함.점수에 따라 non-maximum suppression을 수행함.점수가 높은 후보 영역 bounding box를 기준으로 IoU가 특정 임계값을 넘는 다른 bounding box는 모두 제거함. --> 그러면 꽤 많이 사라짐.​ - Run-time analysisCNN 파라미터들은 모든 카테고리에서 서로 공유됨.CNN에 의해 계산되는 feature vector는 다른 방식들과 비교했을 때 저차원임.R-CNN은 수천개의 object class들을 스케일링 할 수 있음. --> 계산이 빠르다.​Training큰 데이터 셋에서 CNN을 사전 훈련했음.사전훈련한 CNN 모델을 새로운 object detection에 적용하기 위해 해당 도메인에 맞게 fine-tuning함.object detection을 진행할 때 만약 애매한 bounding box가 있다면? IoU 임계값으로 해결. 0.3일때 가장 성능이 좋았음.fine-tuning할때는 임계값을 0.5로 SVM을 훈련할 땐 임계값을 0.3으로 설정. (같게하면 오히려 성능이 떨어졌음)feature extraction을 한 후에는 선형 SVM으로 클래스를 분류함.데이터가 너무 커서 hard negative mining으로 해결함.​Results on PASCAL VOC 2010-12 ​Results on ILSVRC2013 detection 두 데이터셋 모두 R-CNN BB가 가장 성능이 좋게 나왔음.R-CNN BB는 경계 박스 회귀를 사용한 R-CNN임.​Network architectures 본 논문에서는 AlexNet을 CNN 네트워크 구조로 사용함.AlexNet보다 VGG16를 사용하는게 성능이 훨씬 좋아지지만 시간이 너무 오래걸려서 본 논문에서는 AlexNet을 선택함.​Conclusion본 논문은 간단하고 확장성이 좋은 객체 탐지 알고리즘을 제안함.본 논문 알고리즘의 핵심은 객체를 localization 하기 위해 CNN을 활용해 영역을 추정한 것과 훈련 데이터가 부족한 상황에 대비하려고 pre-training과 fine-tuning을 적용한 것이다.​​​참고 : https://bkshin.tistory.com/entry/%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0-R-CNN-%ED%86%BA%EC%95%84%EB%B3%B4%EA%B8%B0 논문 리뷰 - R-CNN 톺아보기최초로 객체 탐지에 CNN을 적용한 알고리즘이 R-CNN입니다. 본 글에서 주요 내용 위주로 R-CNN 논문을 번역/정리했습니다. 글 중간에 로 부연 설명을 달아놓기도 했습니다. 틀린 내용이 있으면 피드백 부탁드립니다. 논문 제목: Rich feature hierarchies for accurate object detection and semantic segmentation 저자: Ross Girshick, Jeff Donahue, Trevor Darrell, Jitendra Malik 기관: UC Berkeley 개정 발표: 20...bkshin.tistory.com ​ "
"목동코딩학원, 인공지능 사물인식 영상(Object Detection) ",https://blog.naver.com/appnet/222993667033,20230125,"목동코딩학원: 인공지능수업​요즘 인공지능 활용을 위해 Custom dataset을 만드는 재미에 푹 빠져있다.프로젝트를 하면서, 직접 나만의 Cusotom Detection이 필요하다 보니 유튜브에서 많은 영상을 보며 공부했다.그결과 이제는 내가 원하는 데로 학습시키고 , 인식 하는데  많이 수월해 졌다.jpg, mp4, rtsp방식등 여러번 테스트를 하면서 다양한 수요에 준비를 하는 중이다.요즘 요청 받는 프로젝트는 거의가 인공지능( A.I )가 필수이다.그만큼 인공지능이 보편화 되어가는것 같다.​아래 영상은 유튜브영상을 Object Detection한 영상이다( 상업적 목적이 없습니다. 혹시 저작권등에 문제가  된다면 지우겠습니다 )​참고로 공부한 영상을 올려본다.    ​ "
[졸업작품] 라즈베리 파이로 Object Detection 하기 ,https://blog.naver.com/drunken_thispath/222571055515,20211117,"중간 점검이다.​ 뭔 범죄자 마냥 사진이 찍혔다.​​기본적인 골자는, Object detection 하여, 이의 개수를 알림으로 주는 역할이다.​​ ​README도 열심히 작성했다.​​라즈베리 파이에서 영상을 받아오는 것은 파이썬의 라이브러리 imagezmq를 이용해 받아오는데,​같이 작업하는 친구가 사실 거의 다 해놔서.. 난 별로 한 게 없다.​yolo 최고. "
[논문 리뷰2] A Survey of Deep Learning-Based Object Detection Methods and Datasets for Overhead Imagery  ,https://blog.naver.com/ziippy/222799785317,20220704,"Received January 2, 2022, accepted January 25, 2022, date of publication February 4, 2022, date of current version February 25, 2022.Digital Object Identifier 10.1109/ACCESS.2022.3149052https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9703336​이 논문에 대해 1. INTRODUCTION2. APPLYING DEEP LEARNING-BASED METHODS FOR OVERHEAD IMAGERY IN CHALLENGING ENVIRONMENTS에 대해서는 지난 블로그를 확인 하기 바란다. https://blog.naver.com/ziippy/222784099136 [논문 리뷰] A Survey of Deep Learning-Based Object Detection Methods and Datasets for Overhead ImageryReceived January 2, 2022, accepted January 25, 2022, date of publication February 4, 2022, date ...blog.naver.com 여기서부터는 이후 내용에 대해 리뷰한다.​​3. DATASETS이번 섹션에서는 이미지 센서 소스를 기반으로 가장 인기있고 공개적으로 사용 가능한 satellite imagery 데이터 셋을 설명한다.​A. EO Satellite Imagery DatasetsEO 는 Electro Optical 의 약어다.EO 위성 영상은 다른 영상에 비해 low-resolution 이고 모으기 어렵지만, UAV 나 비행기에 의해 넓은 영역을 커버하는 장점이 있다.EO 위성 영상 이미지들은 다음과 같다. HRSC2016, SpaceNet 1, SpaceNet 2, xView, SpaceNet MVOI GSD 는 Ground Sampling Distance 다. 이는 픽셀간 거리를 말한다. (바로 옆에 pixel 은 GSD 만큼 떨어져 있다는 걸 의미)각각의 샘플 이미지는 아래 그림과 같다. HRSC2016High-Resolution Ship Collec- tion 2016 (HRSC2016) dataset to promote research on optical remote sensing ship detection and recognition. rotated ship 에 대한 검출 성능을 평가하고자 공개된 데이터rotated 정보와 horizontal bounding boxes coordinations 제공선박의 종류에 대해 hierachical 한 클래스 제공하지만 대부분이 항구 배경으로 제공되므로, 더 나은 품질을 위해 바다를 분리하던지 하는 과정이 필요할 것 같다. SpaceNet Challenge 1 and 2SpaceNet 이라고 부르는 large satellite 데이터을 공개하였다.extract building footprints 를 위해 lower resolution 을 가진 8-band 다중 스펙트럼 이미지를 포함하고 있다. 왜냐하면, 빌딩들의 발자취가 polygon 포맷으로 제공되기 때문이다.  SpaceNet 1 SpaceNet 2xView60 개 클래스에 대해 1만개 이상의 object 가 있는 걸 고려해서 만든 데이터셋이다.WorldView-3 위성 기반으로 30cm GSD 로 촬영했다.다른 satellite dataset 들과 달리 xView 는 high geographic diversity 와 various class categories 를 제공했다.하지만 단일 소스에서 수집했기 때문에, 다양한 소스의 이미지에 대한 감지 성능 평가시에는 적합하지 않다.추가적으로 worker, supervisory, and expert 가 라벨링을 검수/보정하여 인력 오류를 최소화하고 데이터 세트의 일관성을 달성했다. SpaceNet MVOI앞서 언급한 데이터들은 제약이 있었다.바로 다양한 viewpoint 를 표현할 수 없다는 것이다.그래서 SpaceNet Multi-View Overhead Imagery (MVOI) 를 소개한다.SpaceNet MVOI 는 같은 지역에 대해 여러 각의 이미지를 획득했다. ​B. SAR Satellite Imagery DatasetsSAR 는 Synthetic Aperture Radar 의 약어다.SAR 영상은 speckle 노이즈를 많이 포함하고 있지만, 구름이나 빛과 같은 장애물에 관계없이 영상을 제공할 수 있는 독특한 특성으로 인해 중요한 연구 분야이다.특히 현존하는 데이터은 ship detection 용으로 구성되어 있고, 다음과 같다.SSDD, OpenSARShip, OpenSARShip 2.0, SAR-Ship-Dataset, HRSID, LS-SSDD-v1.0 SSDDSAR Ship Detection Dataset (SSDD) 는 SAR ship detection 연구에 널리 사용되고 있다.다양한 센서 타입과 해상도를 가지고 있다.ship 의 최소 사이즈는 3 pixel 이다.하지만 object 수가 상대적으로 적어서, 이미 95% mAP 이상을 성취한 데이터셋이다. OpenSARShip 2.0OpenSARShip 2.0 데이터셋은 OpenSARShip 을 업데이트한 버전이다.Sentinel-1 위성으로부터 2가지 다른 타입의 이미지를 수집했다.Single look complex 와 Ground range detected productOpenSARShip 과 비교했을 때 OpenSARShip 2.0 은 AIS (automatic identification system) 이라고 부르는 추가 정보와 Marine Traffic information (ship type, 위도, 경도를 포함하고 있는) 를 제공한다.cropped ship 이미지들로 annotation 되어 있어서 object detection 보다는 classification task 에 적합하다. SAR-Ship Dataset항구나 섬 근처의 복잡한 배경을 포함하는 것에 집중한 데이터셋이다.육지나 해상 상관없이 ship detection 탐지 모델의 성능을 높이는 것이 목표다. HRSIDHigh-Resolution SAR Image Dataset (HRSID) 이다.SAR 영상에서 ship detection 과 instance segmentation 을 위해 연구되었다.136 개 raw 이미지들을 모았고, 25% 가 겹치는 방식으로 fixed-size crop 을 했다.구글 earch 의 optical imagery 를 사용하여 annotation 오류를 최소화하였다.비교적 큰 이미지를 제공하므로 물체 감지 방법을 평가하는데 유용하며, 인접 선박을 식별하는 데 더 효과적이다. LS-SSDD-v1.0전에 릴리즈했던 SSDD 와 달리 Large-Scale SSDD 이다.24000 x 16000 pixel 해상도를 제공하고, 800x800 으로 분할한 이미지도 함께 제공한다.서브 이미지 패치들이 제공되며, 그 안에 객체가 있는지 없는지까지 함께 제공되므로 object detection 모델 연구에 있어 도움이 될 것이다.  ​C. Aerial Imagery Datasets비행기, 드론과 같은 passive optical sensor 들로부터 얻은 이미지들로 구성된 데이터셋이다.센서 사양이 general 하지 않기 때문에, 자세한 센서 정보를 지정하기가 어렵다.초기 데이터셋은 차량 탐지에 집중하였다.그러나, 최근에는 여러 객체들을 찾는 것으로 focus 되고 있다.여기서 설명할 리스트는 다음과 같다.OIRDS, DLR-MVDA, VEDAI, COWC, CARPK, VisDrone, UAVDT OIRDS저작권이 없는 Overhead Imagery Research Data Set (OIRDS)1800 가량 vehicle target 이 있고, 이는 car, pick-up truck, unknown 으로 구성되어 있다.그럭저럭 괜찮은 화질로 reality 를 제공하지만, 상대적으로 데이터 양이 적어서 일반화하기엔 많이 부족하다.DLR-MVDAGerman Aerospace Center (DLR) 에서는 DLR 3K 카메라로 항공 영상을 모으고 이걸 DLR-MVDA 로 제공했다.7 가지 vehicle 에서 2가지를 찾는것에 집중했다.1000미터 상공에서 촬영했으며, angel 정보까지 포함하여 annotation 되어 있다.VEDAIVehicle Detection in Aerial Imagery (VEDAI) 데이터셋2가지 서로 다른 이미지로 구성됨. 컬러 또는 적외선 이미지 유형.큰 원본 이미지로부터 작은 이미지로 분할되어 diversity 를 증대시켰다.9가지 vehicle 클래스가 정의되어 있고, 추가로 2가지 meta 클래스도 정의되어 있다.COWCCar Overhead with Context (COWC) 라고 부르는 large contextural dataset 이다.동일 지역 또는 동일 센서를 이용해서 데이터를 구축한 기존 데이터들과 달리, COWC 는 6개 region 을 커버했다. (Toronto Cannada, Selwyn New Zealand, Potsdam, Vaihingen Ger- many, Columbus, and Utah)Vaihingen and Columbus 는 grayscale 이고 나머진 color 이미지이다.다른 데이터셋들에 비해 diverse range 가 넓어졌다.CARPK드론으로부터 얻은 이미지들로 주차되어 있는 차들을 detect 하기 위한 것이다.다양한 viewpoint 로 4가지 다른 지역의 89,777 개 이미지를 포함하고 있다.OIRDS, VEDAI, COWC 와 달리 이 데이터셋은 higher resolution 이미지를 제공하고 있다.주차장을 대상으로 이미지를 수집하였으므로, 이미지의 많은 부분이 다른 객체로 채워져 있다. 왜냐하면, CARPK 의 이미지는 고해상도이고 근접한 식별 가능한 개체가 포함되어 있기 때문이다.VisDrone2018년과 2019년에 있던 Vis-Drone Challenge 에서 사용된 데이터셋이다.drone platform 을 위한 computer vision 연구를 위해 조사된 데이터이고, 중국에서 263개 비디오 클립 (179,264 frames) 들로부터 10,209 장의 이미지를 캡쳐한 것이다.또한, overhead 이미지들의 특성을 얻기 위해 occlusion 과 truncation ratio 정보도 포함되어 있다.다른 작은 오브젝트들도 일부 포함하고 있어 (vehicle, pedestrians, bicycle) 다양한 오브젝트 디텍션 목적으로 사용되곤 한다.UAVDTUAV platform 에서 vehicles 를 detection 하기 위한 데이터셋이다.그래서 이름이 UAV Detection and Tracking (UAVDT) 이다.그래서 weather condition, flying altitude, camer view, vehicle occlusion, out-ot-view 과 같은 유용한 annotation attribute 를 포함하고 있다.이로 인해 모델의 generalization 을 높일 수 있다.​D. Satellite and Aerial Imagery Datasets마지막으로 위성과 항공기로부터 구축된 데이터셋들에 대해 알아보겠다.이로 인해 모델의 일반화 성능을 높일 수 있다.이는 다음과 같다.TAS dataset, SZTAKI-INRIA, NWPU VHR-10, DOTA-v1.0, DOTA-v1.5, DOATA-v2.0 TAS구글 Earth 로부터 얻은 데이터Brussels 과 Belgium 외곽지역에 대해 792 x 636 pixel 크기의 30개 컬러 이미지총 1319 개의 cars 들이 labeling 되어 있다.TAS 데이터셋은 최초 항공 영상 view 의 vehicle 데이터셋이라는 점에 의미가 있다.하지만 다양성의 부족으로 인해 딥러닝 기반 detection 방법을 적용하기에는 아쉬운 점이 있다.SZTAKI-INRIASZTAKI-INRIA 는 Building Detection Benchmark 를 위한 데이터셋이다.총 9장의 이미지에, 665개의 빌딩이 labeling 되어 있다.9장 중에서 2장은 항공기에서 나머지들은 위성이나 Google Earth 플랫폼을 통해 얻었다.TAS 데이터셋과 유사하게 부족한 데이터양이다.NWPU VHR-10총 800 장의 이미지로 Google Earth 와 Vaihingen data 로부터 얻었다.10가지의 다른 클래스로 labeling 되어 있다. (비행기, 배, 저장탱크, 그 외)각각의 오브젝트들의 크기는 최대 418 x 418 에서 최소 33 x 33 로 되어 있다.이미지 해상도는 0.08m 부터 2m 까지로 다양하며, 적용 목적에 따라 데이터셋을 사용하기 위해 4가지로 분리했다.negative image set, positive image set, optimizing set, testing setDOTA처음으로 소개된 large-scale Dataset for Object deTection in Aerial images (DOTA)international object detection challenge 를 위한 목적성 데이터DOTA v1.0 이후에 v1.5 (2019년), v2.0 (2021년) 이 나왔다.DOTA v1.0 에서는 10 pixel 미만의 small object 에 대한 annotation 은 포함되어 있지 않다.DOTA v1.5 는 DOTA v1.0 과 같은 이미지지만, small object 에대한 annotation 이 추가되었다. 이후 DOTA v2.0 에서는 다양한 소스(Google Earth, GF-2, JL-1 satellite) 로부터의 이미지가 추가되었다. 추가로 오브젝트 카테고리도 15개에서 18개로 늘었다. 왜냐하면, 실제 세계에서는 임의의 방향이 존재하기 때문에 DOTA dataset 에서도 oriented bounding box 정보를 제공하는 것이다.​​4. Future Research Directions이번 장에서 우리는 2가지 유망한 research direction 을 제안하고자 한다.이는 딥러닝 기반의 방법론과 overhead image dataset 에 대한 연구 방향이다.​A. Accuracy vs. Efficiency일반적으로 accuracy 와 efficiency 간에는 trade-off 가 있으므로, 작고 imbalanced  한 오브젝트에 대한 detection 성능을 높이는 것은 efficiency 과 밀접한 관계가 있다.실제 세계의 이미지들은 더 다양하다.detection accuracy 에 집중한 모델들은 high computational load 를 필요로 하여 efficiency 측면에서는 손해가 있다.그러므로, research 의 목표는 accuracy 와 efficiency 를 둘 다 향상시키는 걸로 해야 한다.이게 첫번째 primary research direction 이다.​B. Fusioin of other domain Data또 다른 방향은 overhead 이미지에 대해 다른 도메인의 추가 정보를 활용하는 것이다.대부분의 경우 다양한 비전 작업에서 충분히 레이블된 데이터를 얻는 건 실질적으로 어렵다.그러므로 soft teacher 방법론과 같은 연구들이 이뤄지고 있다.soft teacher 방법 모델에서는 teacher 와 student 로 구성되어 있다. teacher 네트워크는 레이블 되지 않은 bounding box 에 대해 점수를 매기고, 이 과정을 통해서 pseudo label 의 정확도가 점차 향상된다.overhead 이미지에 대한 객체 감지에 대한 부족한 데이터 문제도 다른 도메인 데이터와의 융합을 통해 극복할 수 있다.따라서, overheag imagery 와 다른 도메인의 정보를 함께 고려하여 모델을 연구하는 방향이 research direction 중 한 가지다.​​5. Conclusionoverhead imagery 에 대한 object detection 은 흥미로운 연구 분야 중 하나이다.그러나 실제 overhead 이미지에서 unique 한 특징을 찾는 이슈는 여전히 존재한다.현재 있는 state-of-the-art 방법들을 실제 overhead 이미지에 직접 적용하기란 어려움이 있다.그러므로 많은 방법들이 소개되고 있다.​우리의 survey pager 는 위성 항공 영상 기반 object detection 방법론에 대한 최신 기법을 탐구하고, 포괄적인 비교 검토를 제시함으로써 이 분야에 대한 추가 연구를 촉직하는 것을 목표로 한다.많은 연구 논문들을 6가지로 카테고리화 했다. 또한 공개적으로 사용가능한 데이터셋을 비교했다.본 논문이 보다 발전된 딥러닝 기반 접근 방식을 개발하고, 향후 연구 방향을 이해하고 논의하는데 도움이 되길 바란다.​​​[끝]​ "
딥러닝 Object Detection 모델 살펴보기(2) : Fast R-CNN (Fast RCNN) 논문 리뷰 ,https://blog.naver.com/baek2sm/222783238067,20220627,"이번 포스팅에서는 R-CNN [1]의 처리 속도를 획기적으로 개선하고, mAP(mean average precision)도 향상시킨 Fast R-CNN [2] 논문을 리뷰해보겠습니다. R-CNN은 딥러닝을 사용하지 않던 기존의 object detection 기법 대비 mAP를 대폭 향상시켰지만, 처리 속도가 매우 느리다는 단점이 있었습니다. Fast R-CNN은 그 이름처럼 R-CNN 대비 속도가 매우 개선된 object detection 모델입니다. Fast R-CNN은 R-CNN의 이런 문제를 어떻게 개선했는지를 중점적으로 살펴보도록 하겠습니다. Fast R-CNN논문: https://arxiv.org/abs/1504.08083R-CNN은 selective search를 통해 2,000개의 bounding box 후보 영역(이하 ROI, Region Of Interest)을 추려내고, 2,000개의 ROI 영역을 모두 resize해서 CNN 모델을 통해 각각 inference를 수행합니다(이 부분이 헷갈리신 분들은 이전 포스팅, R-CNN 논문 리뷰를 먼저 읽어주세요).​즉, R-CNN은 1장의 이미지에서 object detection을 수행하는데, CNN 모델의 inference를 2,000번이나 수행해야 합니다. 속도가 빠를 수가 없겠죠? ​SPP(Spatial Pyramid Pooling) Network (이하 SPPNet) [3]는 input image를 바로 CNN 모델에 넣고 feature map을 얻은 뒤, selective search를 통해 얻은 ROI 영역을 input image로부터 얻은 feature map에 대응시키는 방식으로 CNN 모델을 단 1번만 사용해서 ROI 영역에 대한 feature map을 얻을 수 있도록 R-CNN을 개선했습니다. ​대신 SPPNet에서 ROI 영역에 대응되는 feature map의 크기는 ROI 영역의 크기에 따라 달라졌고, R-CNN의 모델 구조 상 feature의 크기가 매번 달라지면 학습을 진행할 수 없었습니다(fully connected layer와 support vector machine의 학습을 생각해보세요!). 따라서 SPPNet은 feature map의 크기에 상관없이 고정된 feature bin을 얻을 수 있는 spatial pyramid pooling(SPP) layer를 제안합니다. 본 포스팅에서는 SPP에 대한 자세한 설명은 생략하겠습니다. SPPNet의 Spatial Pyramid Pooling Overview, 출처: SPPNet [3]Fast R-CNN은 SPPNet이 제안한 방법과 유사한 방법을 차용했습니다. 다만 SPP 대신 ROI pooling이라는, 일종의 adaptive max pooling을 제안합니다. 여기까지 과정을 차례로 정리해보면 다음과 같습니다.​R-CNN과 마찬가지로, selective search를 통해 ROI 영역 2,000개를 선택합니다.단, CNN 모델은 input image 원본을 1번만 사용해서 feature map을 추출합니다.input image 원본으로부터 얻은 feature map에서 ROI 영역(2,000개)에 대응되는 부분만 잘라냅니다.이렇게 잘라낸 2,000개의 영역을 ROI pooling을 통해 일정한 크기의 feature map으로 변환합니다. 즉, CNN 모델은 1번만 사용해서 2,000개의 ROI 영역에 대한 고정된 크기의 feature map을 얻을 수 있습니다.​여기에 더해 Fast R-CNN은 R-CNN, SPPNet과 달리 support vector machine 대신 fully connected layer와 softmax를 사용한 분류를 수행하는 구조를 채택했다는 점이 가장 큰 차이점입니다. 이 덕분에 모델 학습 시 classification과 bounding box regression에 대한 LOSS를 하나로 합쳐 한 번에 학습이 가능해졌습니다. Fast R-CNN의 구조, 출처: Fast R-CNN 논문 [2]다음은 PASCAL VOC 2012 dataset에서 R-CNN 모델과 성능을 비교한 결과입니다. 똑같이 VOC 2012 데이터셋만으로 학습시켰을 때는 R-CNN BB의 mAP가 62.4, Fast R-CNN의 mAP가 65.7로 mAP가 개선된 것을 알 수 있습니다. PASCAL VOC 2012 데이터셋에서 R-CNN과 Fast R-CNN의 mAP 비교, 출처: Fast R-CNN 논문[2]마지막으로, Fast R-CNN의 속도가 얼마나 개선되었는지 확인하고 본 포스팅을 마치도록 하겠습니다. 다음 표를 보면, R-CNN 대비 모델의 학습 속도는 8.8배~18.3배까지 개선되었고, 테스트 속도는 80배~146배까지 개선된 것을 확인할 수 있습니다. 실사용 속도가 80배 이상 개선되어 기존의 R-CNN 모델과 비교해 실용성이 압도적으로 좋아졌다고 할 수 있겠네요 :) R-CNN과 Fast R-CNN의 속도 비교, 출처: Fast R-CNN 논문[2]딥러닝 Object Detection 모델 살펴보기 시리즈딥러닝 Object Detection 모델 살펴보기(1) : R-CNN 논문 리뷰 : https://blog.naver.com/baek2sm/222782537693 딥러닝 Object Detection 모델 살펴보기(1) : R-CNN 논문 리뷰이번 포스팅에서는 딥러닝을 이용한 object detection 연구의 출발점이라 할 수 있는 R-CNN [1] 논문을...blog.naver.com 딥러닝 Object Detection 모델 살펴보기(2) : Fast R-CNN 논문 리뷰 (현재 포스팅)딥러닝 Object Detection 모델 살펴보기(3) : Faster R-CNN 논문 리뷰: https://blog.naver.com/baek2sm/222784719619 딥러닝 Object Detection 모델 살펴보기(3) : Faster R-CNN (Faster RCNN) 논문 리뷰이번 포스팅에서는 딥러닝 기반의 object detection 모델 중 최초로 end-to-end로 구현된 Faster R-CN...blog.naver.com 딥러닝 Object Detection 모델 살펴보기(4) : YOLO : You Only Look Once 논문 리뷰 (포스팅 후 링크 연결 예정) 참고 문헌references[1] Rich feature hierarchies for accurate object detection and semantic segmentation : https://arxiv.org/abs/1311.2524[2] Fast R-CNN : https://arxiv.org/abs/1504.08083[3] Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition : https://arxiv.org/abs/1406.4729 "
tensorflow object detection api 를 이용한 object detection ,https://blog.naver.com/rlatjdduq12/223078226605,20230418,"ML에 거의 무지한 상태로 이것저것 예제 실행해보면서 떠돌다가.. Object detection에 흥미가 생겨 내가 원하는 label set 으로 custom model을 생성하고 싶어 구글에서 제공하는 api를 이용해보았습니다. ​클라우드에서 해보려다 개인적으로 테스트해보기에는 비용 문제가 있고, 가상머신이나 도커에서 테스트해보려하니 또 하나의 뎁스가 있어서 포기하고 윈도우 환경에서 테스트해보았습니다. 그러다보니 윈도우 환경이라서 그런지 여러 블로그에서 소개하고 있는 사용 방법으로는 여러 오류를 맞닥드려 찾아보면서 겨우 실행에는 성공하여 기록으로 남깁니다.​1. 우선 tensorflow api를 다운로드 합니다. git clone https://github.com/tensorflow/models.git 여기에서 우리는 models/research 경로만 사용합니다.​2. 다음으로 전이 학습에 사용할 이미지셋을 다운로드 합니다. ​이를 위해 여기에서는 크롬 익스텐션의 Image Downloader를 사용하였습니다. 원하는 이미지 종류를 구글에서 검색한 후, 검색 결과의 이미지 탭에서 Image Downloader를 실행해 이미지들을 내려받습니다.그리고 내려받은 이미지들을 models/research/object_detection/images에 저장합니다.​​※ 혹은 아래 경로에서 이미지 셋과, 분류 csv를 미리 다운받을 수도 있으나, 너무 많은 이미지와 분류를 포함하기 때문에 학습용으로는 적합하지 않아 배제하였습니다.https://public.roboflow.com/object-detection/microsoft-coco-subset/2 Microsoft COCO 2017 Object Detection Dataset - rawDownload 120362 free images labeled with bounding boxes for object detection.public.roboflow.com 3. 이미지 라벨링이미지 라벨링에는 labelImg를 사용합니다.  pip install labelImg파이썬 설치경로/Lib/site-packages/labelImg/labelImg.py를 파이썬으로 실행하면 실행됩니다. 다운받은 이미지들을 labelImg로 labeling 해줍니다. (이미지에서 라벨링할 영역을 Create rectBox를 통해 지정한 후 xml로 저장)​모두 labeling 해준 후, 분류 리스트를 아래와 같은 텍스트로, images 폴더에 class-names.txt로 저장합니다.​강아지고양이펭귄곰​그리고 labeling한 이미지들을 images에 train, test로 하위 경로를 생성하여 8:2 정도의 비율로 넣어줍니다.​4. 다음으로 labeling된 xml을 학습에 필요한 csv로 변환하기 위해 아래 단계를 거칩니다.​4-1 제너레이터 다운 git clone https://github.com/hojihun5516/object_detection_setting.git   다운받은 파일들 중 README.md를 제외하고 object_detection 경로에 넣어줍니다.​4-2 xml_to_csv python xml_to_csv.py  여기까지 실행하면 images 폴더에 test_labels.csv, train_labels.csv가 생성됩니다.​​4-3 generate tf record # 트레이닝 이미지python generate_tfrecord.py --csv_input=images/train_labels.csv --image_dir=images/train --output_path=train.record# 테스트 이미지python generate_tfrecord.py --csv_input=images/test_labels.csv --image_dir=images/test --output_path=test.record 여기까지 잘 실행이 되었다면 object_detection폴더에  train.record, test.record 파일들이 생성됩니다.​4-4 generate labelmap #object_detection 폴더#라벨맵 python generate_labelmap.py 잘 실행이 되었다면 images 폴더에 labelmap.pbtxt 파일이 생성됩니다.​5. pre-trained 모델 다운로드https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md 에서 모델을 골라서 다운로드합니다.여기에서는 EfficientDet D0 512x512 으로 다운로드 하겠습니다. models/tf2_detection_zoo.md at master · tensorflow/modelsModels and examples built with TensorFlow. Contribute to tensorflow/models development by creating an account on GitHub.github.com ​6. config 수정object_detection/configs/tf2 경로에 있는 ssd_efficientdet_d0_512x512_coco17_tpu-8.config 파일을 복사해서 images 폴더에 붙여넣기 해줍니다. 방금 복사한 파일을 images 폴더에서 편집합니다.​ 1. num_classes -> 원하는 Object Detection 클래스 수model {  ssd {    inplace_batchnorm_update: true    freeze_batchnorm: false    num_classes: 4    ...2. train_config -> 방금 받은 pertrained 모델의 체크포인트 경로를 적으시고(object_detection 폴더 기준)-> detection으로 변경, batch_size는 컴퓨터가 좋다면 더 높여도 괜찮습니다.train_config: {  fine_tune_checkpoint: ""efficientdet_d0_coco17_tpu-32/checkpoint/ckpt-0""  fine_tune_checkpoint_version: V2  fine_tune_checkpoint_type: ""detection""  batch_size: 4  ...3. train_config - optimizer -> learning rate를 낮춰줌, total_steps를 10000이나 그 이하로 낮춰줌(개인 PC 사양으로 인해 너무 오래걸립니다..)optimizer {    momentum_optimizer: {      learning_rate: {        cosine_decay_learning_rate {          learning_rate_base: 8e-3          total_steps: 10000           warmup_learning_rate: .0001          warmup_steps: 2500        }      }4. train_input_reader -> 전에 생성한 labelmap의 경로와 train.record 입력train_input_reader: {  label_map_path: ""images/labelmap.pbtxt""  tf_record_input_reader {    input_path: ""train.record""  }}5. eval_input_reader -> 전에 생성한 labelmap의 경로와 test.record 입력eval_input_reader: {  label_map_path: ""images/labelmap.pbtxt""  shuffle: false  num_epochs: 1  tf_record_input_reader {    input_path: ""test.record""  }} 7. training 폴더 생성​training 폴더 생성 후, images 폴더의 labelmap.pbtxt, ssd_effi~~~.confg(모델 config 파일) 을 복사해서 붙여넣습니다.​8. 이미지 학습colab에서 실행하면 좋겠지만 저는 무지하므로 로컬환경에서 그냥 실행하겠습니다.​ # package install!pip install tf-models-official!pip install tf_slim!pip install lvis!python model_main_tf2.py --pipeline_config_path=training/ssd_efficientdet_d0_512x512_coco17_tpu-8.config --model_dir=training --alsologtostderr ​9. 학습 완료 훙 모델 체크포인트 저장​ !python exporter_main_v2.py --trained_checkpoint_dir=training --pipeline_config_path=training/ssd_efficientdet_d0_512x512_coco17_tpu-8.config --output_directory inference_graph 학습이 완료된 후, 모델을 저장합니다.​여기까지 완료하면 object_detection 경로에 inference_graph 경로가 생성되고, 거기에 checkpoint, saved_model 폴더가 생깁니다. 내 데이터들로 학습된 모델이 생성되었습니다!​10. 모델 테스트​10-1. 테스트해볼 이미지를 test 경로에 다운로드합니다. 여기에서는 내가 object detection 학습 시 사용한 label로 labeling 될 수 있는 이미지들을 사용합니다.​10-2. jupyter notebook을 실행하여 colab_tutorials에 있는 object_detection_tutorial.ipynb 파일을 실행하면 되지만, 몇 가지 문제가 있어 아래 코드로 실행했습니다.​ # -*- coding: utf-8 -*-import numpy as npimport osimport six.moves.urllib as urllibimport sysimport tarfileimport tensorflow as tfimport zipfileimport pathlibfrom collections import defaultdictfrom io import StringIOfrom matplotlib import pyplot as pltfrom PIL import Imagefrom IPython.display import displayfrom object_detection.utils import ops as utils_opsfrom object_detection.utils import label_map_utilfrom object_detection.utils import visualization_utils as vis_utilimport random# patch tf1 into `utils.ops`utils_ops.tf = tf.compat.v1# Patch the location of gfiletf.gfile = tf.io.gfiledef load_model(model_name):  base_url = 'http://download.tensorflow.org/models/object_detection/'  model_file = model_name + '.tar.gz'  model_dir = tf.keras.utils.get_file(    fname=model_name,     origin=base_url + model_file,    untar=True)  model_dir = pathlib.Path(model_dir)/""saved_model""  model = tf.saved_model.load(str(model_dir))  return model  # List of the strings that is used to add correct label for each box.PATH_TO_LABELS = './images/labelmap.pbtxt'category_index = label_map_util.create_category_index_from_labelmap(PATH_TO_LABELS, use_display_name=True)# If you want to test the code with your images, just add path to the images to the TEST_IMAGE_PATHS.PATH_TO_TEST_IMAGES_DIR = pathlib.Path('test') #테스트 이미지 경로print(PATH_TO_TEST_IMAGES_DIR)TEST_IMAGE_PATHS = sorted(list(PATH_TO_TEST_IMAGES_DIR.glob(""*.jpg"")))TEST_IMAGE_PATHSprint(TEST_IMAGE_PATHS)detection_model = tf.saved_model.load('./inference_graph/saved_model')#print(detection_model.signatures['serving_default'].inputs)detection_model.signatures['serving_default'].output_dtypesdetection_model.signatures['serving_default'].output_shapesdef run_inference_for_single_image(model, image):  image = np.asarray(image)  # The input needs to be a tensor, convert it using `tf.convert_to_tensor`.  input_tensor = tf.convert_to_tensor(image)  # The model expects a batch of images, so add an axis with `tf.newaxis`.  input_tensor = input_tensor[tf.newaxis,...]  # Run inference  model_fn = model.signatures['serving_default']  output_dict = model_fn(input_tensor)  # All outputs are batches tensors.  # Convert to numpy arrays, and take index [0] to remove the batch dimension.  # We're only interested in the first num_detections.  num_detections = int(output_dict.pop('num_detections'))  output_dict = {key:value[0, :num_detections].numpy()                  for key,value in output_dict.items()}  output_dict['num_detections'] = num_detections  # detection_classes should be ints.  output_dict['detection_classes'] = output_dict['detection_classes'].astype(np.int64)     # Handle models with masks:  if 'detection_masks' in output_dict:    # Reframe the the bbox mask to the image size.    detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(              output_dict['detection_masks'], output_dict['detection_boxes'],               image.shape[0], image.shape[1])          detection_masks_reframed = tf.cast(detection_masks_reframed > 0.5,                                       tf.uint8)    output_dict['detection_masks_reframed'] = detection_masks_reframed.numpy()      return output_dict  def show_inference(model, image_path):  # the array based representation of the image will be used later in order to prepare the  # result image with boxes and labels on it.  image_np = np.array(Image.open(image_path))  # Actual detection.  output_dict = run_inference_for_single_image(model, image_np)  # Visualization of the results of a detection.  vis_util.visualize_boxes_and_labels_on_image_array(      image_np,      output_dict['detection_boxes'],      output_dict['detection_classes'],      output_dict['detection_scores'],      category_index,      instance_masks=output_dict.get('detection_masks_reframed', None),      use_normalized_coordinates=True,      line_thickness=8)  #display(Image.fromarray(image_np))  #print(Image.fromarray(image_np))  #object detection 결과 이미지 저장  im1 = Image.fromarray(image_np)  im1 = im1.save(random.choice('abcdefghijklmnopqrstuvwxyz0123456789') + "".jpg"")  for image_path in TEST_IMAGE_PATHS:  show_inference(detection_model, image_path)   ​​이렇게하면 현재 경로에 object detection으로 labeling 된 이미지들이 저장됩니다!  (저장 경로와 파일명 등은 따로 설정해주시는 것이 좋습니다.)​※ 해당 코드 실행할 때 lib\site-packages\tensorflow\python\lib\io\file_io.py 경로에서 아래 오류를 뱉어내는데 이유는 검색해봐도 나오지가 않네요.. 파일 읽는 데 사용하는 라이브러리는 바이너리로 되어 있어 수정이 불가능합니다.​ self._read_buf = _pywrap_file_io.BufferedInputStream(UnicodeDecodeError: 'utf-8' codec can't decode byte 0xc1 in position 70: invalid start byte 하여 utils/label_map_util.py 의 load_labelmap 함수를 아래와 같이 변경했습니다. def load_labelmap(path, validator=None):    with open(path, 'r') as fid: #tf util이 아닌 open으로 변경    label_map_string = fid.read()    print(label_map_string)    label_map = string_int_label_map_pb2.StringIntLabelMap()    try:      text_format.Merge(label_map_string, label_map)    except text_format.ParseError:      label_map.ParseFromString(label_map_string)  if validator is None:    validator = _validate_label_map  validator(label_map)  return label_map ​​ "
[논문 리뷰] YOLOv4: Optimal Speed and Accuracy of Object Detection ,https://blog.naver.com/tnsgh9603/222686094642,20220329,"YOLO의 각 버전들을 짧게 복습하고 넘어가자면, 우선 첫 번째 버전은 당시는 주로 region proposal 기반의 기법들이 주를 이뤘었는데, 한 번에 bounding box에서 coordinate도 regression도 하고 classification도 하는 1-stage 기법을 제안하여, 상대적으로 빠른 처리 속도로 동작해서 큰 주목을 받았다.​ V2는 V1에 다양한 테크닉을 적용해서 더 빠르고 좋은 모델을 제안하였습니다.  V3에서도 마찬가지로 V2에 다양한 아이디어를 추가하여 성능을 끌어올렸다. V4에 추가된 기법에는 크게 Bag of Freebies(프리비스), Bag of specials가 있다.​논문의 초반부에 object detection 관련한 연구들을 한 눈에 정리하였는데요. 다양한 backbone들이 사용되었고, 그림 가운데 Neck 부분에서는 대표적으로 Feature Pyramid Network 줄여서 FPN이 사용되었습니다.​Bag of Freebies는 전처리나 학습 단계를 의미합니다.augmentation에는 input에서 임의의 영역에 bbox을 치고, 해당 영역을 0~255의 랜덤한 값으로 바꿔주는 random erase, 0~255가 아닌 0으로 채우는  cutout, 두 이미지랑 라벨을 섞어서 새로운 이미지와 라벨을 만들어내는 mixup, 두 아이디어를 합친 cutmix, 그리고 style transfer GAN을 augmentation 기법으로 사용했다.​Regularization 기법으로는 대표적으로 Dropout을 사용하고, Dropout의 후속 연구인 DropPath, Spatial Droputout, Dropblock을 사용했다고 합니다.​Loss function으로는 MSE, IoU, Genarlized IoU, Complete IoU, Distance IoU가 있습니다.  다음으로는 아키텍쳐 관점에서의 기법들, 그리고 후처리, inference에 관련한 기법들을 Bag of Specials라고 부릅니다.​Receptive Field를 높여주기 위해서 Speical Pyramid Pooling, Atrous Spatial Pyramid Pooling(ASPP), Receptive Field Block(RFB)를 사용하였습니다.​Feature를 합쳐주는 방법에는 대표적으로 Skip-connection, Feature Pyramid Network, SFAM, ASFF, BiFPN 등 엄청 다양한 시도를 하였음을 알 수 있습니다.​Activation function으로도 보통 ReLU를 많이 사용하는데, ReLU의 변형 버전인 Leaky ReLU와 Paramaetic ReLU, ReLU6도 포함시켰고, autoML로 찾은 Swish, 그리고 Swish를 사람이 조금 개선시킨 Mish를 사용했다고 합니다.​Squeeze-and-excitation이나 Spatial Attention Module 같은 attention module로도 실험을 해봤고, Normalization 기법으로는 일반적으로 많이 사용하는 Batch normalization가 있습니다. 멀티 GPU 에서 많이 사용하는 Cross-GPU Batch norm에 CGBN과 SyncBN이 있는데, YOLO v4는 핵심은 single-GPU로만 동작하게 하는 것이므로 SyncBN은 쓰지 않았다고 합니다. 그리고 제가 처음 들어보는 FRN, CBM도 사용했다고 합니다.  backbone을 결정할 때는 위에 3가지를 고려했다고 합니다.​1번째는 YOLO의 고질적인 문제인 small-object에 취약하다는 점을 해결하기 위해 input resolution을 크게 가져갔다는 점입니다.(YOLOv4에서는 512x512를 사용)​2번째는 Receptive Field를 물리적으로 높여주기 위해 더 많은 layer를 쌓았다는 점입니다. ​3번째는 여러 크기의 object를 찾게 하기 위해서 network의 표현력을 높여줘야 하기 때문에 파라미터 수를 키워줬다. 다른 모델보다 파라미터수도 많고, Flop 수도 가장 많은데, 놀랍게도 RTX 2070에서 Frame Rate가 가장 높습니다.그 이유는 CSPNet 논문에 나와있습니다.inference 속도가 computation을 많이 잡아먹기 때문에 Cross Stage Partial Network 구조를 제안하였습니다.해당 논문에 중요한 점만 읽어보았는데요, Input Feature map을 두 파트로 나누어, 한 파트는 아무런 연산을 하지 않고, 나머지 파트로는 연산을 한 다음 합쳐주는 방식을 제안하였습니다.해당 논문에 따르면 정확도는 거의 손실되지 않고, 처리 속도는 빨라진다고 합니다. 제가 이해한 바로는 두 파트로 나눠지면서 Gradient flow가 쪼개지게 되기 때문에 학습에 좋은 영향을 줘서 정확도 손실이 적다로 표현이 되어 있었습니다. 확실한 건, 표처럼 실제로 Flop 수도 크고, 파라미터수도 많더라도 실제로 FPS가 66으로 빠르기 때문에, 높은 input resolution을 썼음에도 거의 실시간으로 동작이 가능하다고 합니다.​ YOLOv4를 요약하면 다음과 같습니다. Backbone은 CSPDarknet53을 사용하였고, Neck은 SPP에 + Path Aggregation Network라는 PAN 기법을 추가로 사용하였습니다. 이에 다가, Bag of Freebies(프리비스), Bag of specials을 사용하였습니다.​ BoF, BoS에 있는 모든 기법들을 실험해본 것은 아니라고 합니다.PReLU와 SELU는 학습이 잘 안되서 제외했다고 하고, ReLU6는 quantization network를 위해 제안된 방법이기 때문에 제외하고 실험했다고 합니다.​본 논문에서 Dropblock 빼고는 다른 regularization method를 실험해보지 않았다고 하는데, 그 이유는 DropBlock의 저자가 DropBlock이 우수함을 보이기 위해 YOLO에 자신의 방법을 적용한 뒤 레포트를 했다고 합니다. 왜 남에게 실험을 하게 한건지는 잘 모르겠습니다.​그리고 Single GPU만 사용할 것이기 때문에 SyncBN은 제외했다고 합니다.​저자는 남의 것만이 아닌, 저자만의 추가적인 기법도 사용했습니다.저자는 모자이크 augmentation을 제안했습니다. 모자이크 augmentation이란 4개의 image를 하나의 image로 합쳐주는 것을 의미합니다. 저자에 따르면 한 개의 input으로 4개의 image를 배우게 되므로, 어떻게 보면 batch size가 4배가 되는 효과를 볼 수 있다고 합니다. 그래서 모자이크 augmentation을 사용하면, batch size를 적게 사용해도, batch size를 크게 가져간 것 같은 효과를 볼 수 있다고 합니다. 사실 이 말이 엄청 reasonable 하지는 않은데 제가 생각하기에는 이렇게 모자이크 augmentation으로 이미지들을 섞어주는 과정에서 큰 object가 작은 object로 변하기 때문에, small object에 대한 성능이 좋아진 것 때문이 아닐까라고 추측하고 있습니다. 또한, Self-Adversarial Training(SAT)이라는 걸 자기네들이 제안했는데 논문에는 내용이 많이 부실했습니다. SAT는 input image에 노이즈를 가해서 network가 image를 틀리게 예측하도록 만든 다음에 원래 이미지의 Ground truth와 다시 학습시켜서 input image의 좀 더 디테일한 부분에 집중하도록 하여 모델의 robustness를 높이는 기법이라고 합니다. 아이러니 하게도 논문에 해당 실험 결과가 없어서 왜 넣었는지 사실 모르겠습니다.​  실험에 대한 구체적인 셋팅은 다음과 같습니다.​ 우선 classification에 대한 실험 결과 입니다. cutmix, 모자이크 augmentation, label smoothing을 사용하여 성능 향상이 이루어졌고, 나머지 swish나 bluring은 큰 재미를 보지 못했다고 나와있습니다.mish activation function을 사용하여 성능 향상이 이루어졌다고 나와있습니다.​ objection detection에 대한 실험 결과 입니다.빨간색 부분은 오히려 성능이 떨어지게 되는 기법들입니다.​ backbone 쪽에서는 Path Aggregation Network와 Spatial Pyramid Pooling, Spatial Attention Module을 적용 했을 때 Table 5와 같이 성능이 개선되었습니다.그리고 Receptive Field Block, Gaussian Yolo(G), ASFF CSPResNext이 classification task에는 CSPDarknet 보다 좀 더 좋은 성능을 보이지만, Detection task에서는 CSPDarknet이 CSPResNext보다 성능이 더 좋았다. ​ 다른 batch-size를 사용했을 때의 실험 결과입니다.CSPRexNext는 batch-size를 줄이자 AP가 많이 떨어지는 것을 확인할 수 있는데, Darknet53에 저자들이 제안한 BoF와 BoS를 적용하면 batch-size를 줄여도 성능이 크게 떨어지지 않는 것을 보여주고 있습니다. batch-size가 2나 1일때의 성능도 보여줬으면 좋겠는데 그게 없어서 조금 아쉬었습니다.  Conclusion​학회에서 유명한 다양한 기법들, BoF와 BoS를 YOLO에 적용하여 더 빠르고 정확한 detector를 만들 수 있었다. 하나의 GPU만 사용할 수 있게 만든점이 이 논문의 가장 큰 contribution인 것 같습니다. Cross Stage Partial Network을 제안하여 다른 모델보다 파라미터수도 많고, Flop 수도 가장 많음에도 불구하고 Frame Rate를 높일 수 있었다.  Reference​​ "
Object Detection 해보기 ,https://blog.naver.com/cocolo001/222908032973,20221023,"최근에 사물검출(object detection)을 배웠다​그래서 이것저것 시도해보고 있는데생각보다 학습이 쉽게 안된다​사진 라벨링 작업도엄청난 노가다이고..100장 학습하는데도한시간이나 걸렸다​  ​야구공 탐지를 잘 할지 봤는데역시나 제대로 못하는 모습…​​  ​학습양을 늘리고영상을 좀 더 식별할 만한걸로 바꿔보니 조금은 식별했다…그래도 아직은 멍충 그 잡채,, ㅠ ㅠ  ​​​​그래서하면서 알게된건!!실제로 검출할 영상 또는 센서이미지에 식별되는 형태로학습 시켜야될 것 같다는 것…​예를 들어 야구공이면야구공 크게 확대된 건학습에 크게 도움이 안 될 것 같다…​그래도 검출이 여러 상황에 사용될 수도 있으니,,,결론적으로는라벨링 데이터가 많이 필요할 것 같다……​라벨링 알바가 왜 필요한지뼈저리게 깨달았다​​앞으로 해볼 것:1. 하나의 이미지에 대해여러 영역을 라벨링했을때 detection 성능이올라가는지 확인해보기!​2. 여러 인물 detection 도전!​ "
[복습장] CNN_07 Object_Detection_2 ,https://blog.naver.com/dlehdgnsa/223046872272,20230316,"#AI #CNN ​저번의 이미지 데이터에 대해 Object Detection에 이어서, 이번에는 Video Detection을 실행해 보겠습니다.  https://github.com/ultralytics/yolov3#quick-start-examples GitHub - ultralytics/yolov3: YOLOv3 in PyTorch > ONNX > CoreML > TFLiteYOLOv3 in PyTorch > ONNX > CoreML > TFLite. Contribute to ultralytics/yolov3 development by creating an account on GitHub.github.com !git clone https://github.com/ultralytics/yolov3.git !cd yolov3: pip install -r requirements.txt 저번 이미지 Object Detection과 마찬가지로, 다음과 같은 코드를 작성하여 모델을 불러옵니다. from google.colab import drivedrive.mount('/content/drive') 이번에도 구글 드라이브의 짧은 쇼츠 영상을 사용하겠습니다. !wget -O /content/yolov3/pretrained/yolov3-tiny.pt https://github.com/ultralytics/yolov3/releases/download/v9.6.0/yolov3-tiny.pt 미리 학습된 가중치를 가져옵니다. !cd yolov3; python detect.py \    --weights '/content/yolov3/pretrained/yolov3-tiny.pt' \    --source '/content/drive/MyDrive/my_data/video/' \    --project '/content/yolov3/detected' \    --name 'shorts' \    --img 640 \    --conf-thres 0.5 \    --iou-thres 0.4 \    --line-thickness 2 \    --exist-ok \    --device CPU 그 후, 파일 경로, 저장 경로등을 설정한 후, Detection을 실행해 주겠습니다. 이렇게 하면, 위와 같이 객체가 추출되는 동영상 파일을 만들 수 있습니다.  ​ "
[제8회 금요명품세미나] 객체 탐지(Object Detection): R-CNN과 YOLO ,https://blog.naver.com/wedatalab/223079159670,20230419,"안녕하세요^^ 연구하고 강의하고 책쓰고 개발하는 위데이터랩입니다.​이번 금요명품세미나는 '객체 탐지(Object Detection): R-CNN과 YOLO'이라는 주제로, 위데이터랩의 강승우 부사장님이 진행합니다. 글로벌 소프트웨어 턱시도와 웹로직 개발에 참여했고, GNN, XAI, 잠재공간, VAE, 3D image, adversarial attack 등을 강의하는 국내 최고의 인공지능 전문가, 강승우 부사장님이 대표적인 객체 탐지 알고리즘인 R-CNN과 YOLO의 원리, 동작방식, 장단점 등을 이해하고 활용하는 방법을 알려드립니다. 대면 강의이므로 선착순 30명으로 제한됩니다. 서둘러 신청하세요~~​📣내용:- R-CNN과 YOLO의 원리 및 동작방식- R-CNN과 YOLO의 장단점- R-CNN과 YOLO의 활용​📣일시: 4월 28일(금) 오후2~5시📣장소: 인사동 수운회관 13층 위데이터랩 교육장📣비용: 무료!!!📣신청기간: 4월 18일(화)~4월 26일(수)📣신청방법: 구글폼 작성 (https://forms.gle/kmaqXYwZnw4Fj6af7)📣모집인원: 30명 (선착순)​ ※ 본 세미나는 CNN에 대한 기본 지식을 필요로 합니다.  위데이터랩서울특별시 종로구 삼일대로 457 수운회관 1303호 🌐문의 | 02-355-1016, 010-4298-8254, help@wedatalab.com🌐EZIS IT 학교 오픈채팅방 | https://open.kakao.com/o/gE7U095d (패스워드: ezisit22)💡오픈채팅방에서는 위데이터랩에서 진행하는 다양한 무료 강의 정보들을 확인하실 수 있습니다.​​#DBMS #인공지능 #AI #클라우드 #모니터링솔루션 #DPM #APM #머신러닝 #딥러닝 #데이터 #위데이터랩 #코딩 #프로그래밍 "
"미르기술, 이물검사 Foreign Object Detection (FOD) ",https://blog.naver.com/marketingmirtec/222961029051,20221220,"​ 오는 2023년 1월 24일 미국 샌디에이고에서 개최하는 IPC APEX EXPO 2023에서 미르기술 SPI 장비 대표 모델인 MS-11가 전시 예정입니다. 하단에서 설명할 이물 검사는 자사 SPI 장비의 대표 특징 중 하나로 미르기술 고객들이 직면한 다양한 작업 환경에서 일어날 수 있는 문제를 해결하기 위한 대표 방안 중 하나입니다.  미르기술의 이물 검사(Foreign Object Detection, FOD) 기능은 다양한 종류의 이물을 감지해 검사 오류를 최소한으로 줄임으로써 더욱 정확하고 정교한 검사 결과를 보장하고 있습니다. ​상기 사진에서 보이는 바와 같이 이물 검사(Foreign Object Detection, FOD) 기능은 검사 환경에서 빈번히 발생하는 플라스틱 조각, 금속 조각, 슬러지(Sludge), 머리카락, 먼지, 오염(얼룩) 등을 포함하는 다양한 크기 및 형태의 이물을 찾아낼 수 있습니다.​특히SPI 장비의 렌즈별 최소 검출 가능 사이즈가 다르기 때문에 이물 검사(Foreign Object Detection, FOD) 기능에 관심이 있으신 고객분들께서는 자세한 정보를 위해 미르기술로 연락해 주시기를 바랍니다.   미르기술은 미르기술을 이용하시는 모든 고객에게 더욱 정확하고 정교한 검사 결과를 제공하기 위해 항상 최선의 노력을 다하고 있습니다. 미르기술의 다양한 최신 기술과 제품에 대해 궁금하신 점이 있으시면 아래로 연락해 주시기를 바랍니다.​​미르기술 정보 및 장비 문의📲 1544-1062 💻 marketing@mirtec.com  💻 미르기술 홈페이지​ 미르기술Technology 실력이 뒷받침되는 정성의 철학으로 끊임없이 기술을 개발하고 최상의 서비스를 제공합니다. Smart Factory Solutions 스마트팩토리 솔루션 AOI 자동 광학 검사기 SPI 납 도포 검사기 SEMI / LED 반도체 / LED 검사기 Smart Factory Solutions 미르기술의 소프트웨어 솔루션 Intellisys®는 검사 데이터를 장기간 누적 취합하여 빅데이터를 구성하고, 통계적 방법론을 통해 해석하여 불량의 근본 원인을 추적합니다. 또한 원격지에서 장비를 관리ㆍ제어하여 문제를 해결함으로써 공...mirtec.com MirtecUSA - Distributors of Automated Optical Inspection sytems including Desktop AOI, Inline AOI Equipment.Welcome to MIRTEC With over 17,000 systems installed throughout the world and having received a total of 43 Industry Awards thus far for its products and services, MIRTEC has earned a solid reputation as one of the most progressive and dynamic suppliers of Automated Optical Inspection equipment to t...mirtecusa.com MIRTECTechnology Wir entwickeln ständig Technologien und bieten den besten Service auf der Basis unserer Philosophie des Fleißes, der durch unsere Fähigkeit unterstützt wird. Smart Factory Solutions Intelligente Fabriklösungen AOI Automatische optische Inspektionsgeräte SPI Lötzinnauftrags-Prüfungsgeräte ...mirtec.com 日本ミルテック株式会社｜プリント基板実装と半導体の外観検査装置専業メーカー｜東京都中央区--> 2022.3.28 2022年6月15日～17日に、東京ビックサイトで開催される 電子機器トータルソリューション展2022 に出展します。 詳細は追って掲載させていただきます。 2020.10.29 2021年1月20日～22日に、東京ビックサイトで開催される エレクトロテストジャパン に出展します。 詳細は追って掲載させていただきます。 2019.12.3 2019年12月より新大阪駅から徒歩8分の第8新大阪ビルに大阪営業所を開所しました。 2019.12.2 2020年1月15日～17日に、東京ビックサイトで開催される エレクトロテストジャパン に弊社装置を出展します。 出展ブース...mirtec-j.com ​ "
컴퓨터 비전(Computer Vision) | Object Detection 개념 ,https://blog.naver.com/bosongmoon/222840427911,20220805,"​ CV인간의 시각관 관련된 부분(영상, 이미지 등)을 컴퓨터 알고리즘을 이용해서 연구하는 분야대표적으로, Image Classification, Semantic Image Segmentation, Object Detection 가 있음​​ Object Detection물체가 있는 영역의 위치정보를 Bounding Box로 찾고, 해당 Bounding Box 내에 존재하는 object의 라벨(label)을 분류하는 문제 영역​ ​​ Object Detection 출력값x_min, y_min, x_max, y_max, class, confidence하나의 Bounding Box 내에 6개의 벡터가 mappingx_min : 물체의 Bounding Box의 왼쪽 위 (Left-Top) x 좌표y_min : 물체의 Bounding Box의 왼쪽 위 (Left-Top) y 좌표x_max : 물체의 Bounding Box의 오른쪽 아래 (Right-Bottom) x 좌표y_max : 물체의 Bounding Box의 오른쪽 아래 (Right-Bottom) y 좌표class : Bounding Box에 속한 물체의 classconfidence : Bounding Box에 실체 물체가 있을 것이라고 확신하는 정도(0 ~ 1.0 사이의 값)하나의 Bounding Box에 다음과 같은 벡터가 대응될 수 있다. (15, 5, 240, 297, 'person', 0.92)위의 값들은 절대 좌표인데, 상대좌표로도 표현 가능하다상대좌표는 각각의 x, y 좌표를 width와 height 로 나누면 된다x / widthy / height ​ "
[Object Detection] Fast R-CNN ,https://blog.naver.com/kona419/223052804934,20230322,"논문 : https://arxiv.org/abs/1504.08083 Fast R-CNNThis paper proposes a Fast Region-based Convolutional Network method (Fast R-CNN) for object detection. Fast R-CNN builds on previous work to efficiently classify object proposals using deep convolutional networks. Compared to previous work, Fast R-CNN employs several innovations to improve training...arxiv.org Abstract  본 논문은 Fast R-CNN을 제안.Fast R-CNN은 이전 작업을 기반으로 하며 deep conv. network를 사용함.train, test 시간을 단축했고, 동시에 탐지 정확도도 높임.​Introduction 본 논문은 train과정을 간소화시켰음.single-stage training algorithm을 제안함.object proposal을 분류하고 공간위치를 세분화하는 방법을 공동으로 학습함.기존 R-CNN, SPPnet의 문제점training이 mutli-stage 파이프라인 사용학습에 공간과 시간이 많이 소요객체 탐지가 느림. 등등​Fast R-CNN의 장점detection 퀄리티가 좋음single-stage에서 학습, muti-task loss 사용학습하면서 모든 네트워크의 레이어를 업데이트 할 수 있음disk storage 필요 없음.​Architecture and training input : 이미지 전체이미지 전체를 몇 개의 conv과 max pooling 레이어를 사용해 처리함. --> conv feature map 생성.feature map에서 RoI pooling 레이어가 fixed-length feature vector를 뽑아냄.각각의 feature vector는 FC layer로 들어감.FC layer는 두 개의 output으로 나뉘어짐.첫째는 softmax probability estimates를 생산하는 레이어둘째는 4가지의 real-valued number를 출력하는 레이어​The RoI pooling layer모든 유효한 관심 영역 내의 feature들을 작은 feature map으로 바꾸기 위해 max pooling 사용.본 논문에서 RoI는 conv feature map으로 들어가는 직사각형이다.​Pre-trained networks마지막 max pooling layer를 RoI pooling layer로 바꿈네트워크의 마지막 FC layer와 softmax를 두 개의 layer로 바꿈.네트워크는 input을 두 개 받음. 하나는 이미지 리스트, 하나는 그 이미지의 RoI 리스트.​Fine-tuning for detection모든 네트워크의 가중치를 back propagtion으로 학습하는 것은 Fast R-CNN에서 매우 중요함.본 논문에서는 더욱 효과적인 학습 방법을 소개함.훈련하는 동안 feature를 공유함.SGD mini-batch들을 계층적으로 샘플링함. 처음엔 N 이미지를 샘플링하고 각 이미지에서 R/N RoI를 샘플링함.같은 이미지에서 뽑은 RoI는 순전파, 역전파를 진행하면서 연산 결과와 메모리를 공유함.​Multi-task lossFast R-CNN은 output layer를 두개 가짐.RoI 마다 k+1개 확률값을 출력bounding-box 좌표 출력 multi-task loss 수식Scale invariance스케일이 바뀌지 않는 객체 탐지를 수행하기 위해 두가지를 실험함.1. brute force 학습 -- 각각의 이미지는 training과 testing하는 동안 pre-defined pixel size에서 진행됨. -- 네트워크는 반드시 training 데이터에서 scale-invariant object detection을 학습해야 함.2. image pyramid 방식 -- 대략적인 scale-invariance를 네트워크에 제공. -- test : 이미지 피라미드는 object proposal 각각에 대략적인 scale-normalize하는데 사용. -- train : 피라미드 스케일을 randomly sample함.​Fast R-CNN detection객체 탐지에서는 처리해야할 RoI가 많아서 FC layer에서 많은 시간을 소요함.해결책 : Truncated SVD 사용.​Main Results 다른 방법들에 비해 정확도 올라가고 학습 속도도 빠름.​참고 : https://bkshin.tistory.com/entry/%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0-Fast-R-CNN-%ED%86%BA%EC%95%84%EB%B3%B4%EA%B8%B0 논문 리뷰 - Fast R-CNN 톺아보기본 글에서 주요 내용 위주로 Fast R-CNN 논문을 번역/정리했습니다. 글 중간에 로 부연 설명을 달아놓기도 했습니다. 틀린 내용이 있으면 피드백 부탁드립니다. 논문 제목: Fast R-CNN 저자: Ross Girshick 기관: Microsoft Research 개정 발표: 2015년 9월 (첫 발표: 2015년 4월) Abstract 본 논문에서는 '빠른 공간 기반 합성곱 신경망 모델(Fast Region-based Convolutional Network method, Fast R-CNN)'을 소개합니다. Fast R-CN...bkshin.tistory.com ​ "
Object detection(1) _ Two-stage 2D object detectors ,https://blog.naver.com/summa911/222987303974,20230117,"Object Classification 여러 분류 카테고리 중 하나의 물체에 해당하는지(1) 해당하지 않는지(0) 밝히는 것. 모델 학습 결과 0/1의 명확한 데이터가 얻어짐.​Object DetectionObject classification의 한 기법으로, 물체의 분류 뿐만 아니라 전체 이미지 내부의 특정 물체의 위치를 찾아내는 기법.two-stage detection과 one-stage detection 등의 방법이 있다. ​Two-stage detection1 stage) box(region) proposal2 stage) Bounding box 내부 object classification​ ​Object Classification with Localization : 해당 물체의 카테고리를 분류하는 것에 추가해서 위치 정보를 얻어내야 한다. 일반 object classification 에서는 배경을 포함해서 전체 물체의 카테고리를 분류한 반면,위치 정보까지 파악이 필요할때는, Bounding box을 최대한 물체에 가까운 선을 그어야 하기 때문에 regression의 방법을 써주어야 한다. Loss = 마지막 layer의 벡터값, output벡터의 값의 차​​한편, Loss에서 주의해야 할 것은배경만 존재하는 이미지의 Loss와 물체가 존재하는 이미지의 Loss와 다르다.배경만 존재하는 이미지의 objectness=0이므로, 위치 정보가 필요없게 된다. 그래서 Loss에 포함하지도 않고, 오로지 object 값이 0과 가까운지 아닌지만 Loss값으로 고려해서 학습을 시킨다.  배경만 존재한는 이미지의 Loss는 물체가 존재하는 이미지의 Loss와 다르다. ​​​​또, 이미지가 여러개일 경우, box를 어떻게 쳐야하는지 문제가 될 수 있다. ​딥러닝 이전에 사용되던 방법이 Sliding Windows Detection 방법이다. 기존 CNN은 배경을 포함한 이미지의 정보를 카테고리별로 classification하도록 고안된 아키텍처이다. Sliding window detection은 사진을 물체로 꽉 찬 이미지로 자른 후, object로 가득 찬 사진으로 CNN을 트레이닝 하는 방법이다. 기존 CNN과 방법이 유사하지만, 배경을 제외하고 트레이닝을 하느냐 아니냐의 차이이다.  ​object로 가득찬 box를 찾기 위해서 일단 임의로 이미지를 훑으며(sliding) 적절한 box를 찾는다. 하나의 box를 분석하는 시간이 짧다면 sliding window방식은 괜찮은 방식이다. 전체 이미지를 다 훑으며, 다양한 box를 proposal 할 수 있기 때문이다. 머신러닝 알고리즘에서는 sliding window detection은 가능한 연산이 었지만,딥러닝이 도입된 이후 해당 방법으로는 연산량이 너무 많아지게 되어 비효율적(computation overhead)이었다.​​그래서 고안된 방법이 Selective Search이다. ​​Selective Searchshift와 같이 이미지 각 부분의 특성을 대표하는 feature값이 있는데, 비슷한 값을 가지는 feature를 기준으로 묶어서 bounding을 하는 방식이다.selective search를 이용해서 하늘과 같이 object가 아닌 배경이미지를 통으로 bounding 해주면, 연산량을 줄이는데 효과적이다.boundary를 단순화 할 수록, bounding box의 갯수가 줄어든다.  ​출처 : JRR. Uijlings et al., ""Selective search for object recognition"" https://link.springer.com/article/10.1007/s11263-013-0620-5 Selective Search for Object Recognition - International Journal of Computer VisionThis paper addresses the problem of generating possible object locations for use in object recognition. We introduce selective search which combines the strength of both an exhaustive search and segmentation. Like segmentation, we use the image structure to guide our sampling process. Like exhaustiv...link.springer.com 출처 : https://youtu.be/yGZVibqaPJY ​ "
YOLOv8 TOD(Thermal Object Detection) 2 ,https://blog.naver.com/itiv-ai-flir/223104505309,20230517,"열화상 카메라는 열을 감지할 수 있기 때문에 공장의 품질 관리 수준을 개선하고, 수리 공정을 간소화시켜줄 수 있는 제품입니다. 열화상 카메라를 활용하면 첨단 운전자 보조 시스템을 보다 효과적으로 운용할 수 있기 때문에 열화상 카메라를 이용한 다양한 솔루션이 개발되고 있습니다. 특히 안개 눈부심 현상이 심한 경우, 야간에도 먼 곳까지 육안으로 확인할 수 있어 운전자를 효과적으로 지원하고, 자율주행차량 시스템에 큰 도움을 줍니다.​ADAS(Advanced Driver Assistance System)는 운전자가 안전하고 편리하게 주행할 수 있도록 도와주는 모든 기능을 말한다. FLIR에서는 열화상 기술을 이용한 스마트한 주행을 위해 열 데이터 세트를 알고리즘 개발자에게 무료로 제공합니다.  제공되는 데이터 세트는 사람, 개, 자동차, 자전거 등으로 분류된다.​ 출처: FLIR ADAS DATA 세트 ​FLIR에서 제공된 데이터 세트를 사용하여 최근에 나온 Yolo 성능 확인을 위해 Train을 진행했습니다. 2023년 최근에 나온 Yolo는 개체 감지, 인스턴스 및 이미지 분류 모델을 Train하기 위한 통합 프레임워크가 구축되었으며, YOLOv8은 기능도 추가되고 기존 버전 YOLOv5보다 높은 성능을 보입니다.​2023년 1월, YOLOv8이 출시되어 속도와 정확도, 성능을 확인하고자 ADAS 이미지를 활용하여 테스트를 진행했습니다.​​[테스트 환경] 분   류내        용PC 운영체제Windows10 64bitPC CPU12th Gen Intel(R) Core(TM) i7-12700H 2.30 GHzPC RAM16GB 이상PC Video해상도: 1920X1080PythonPython 3.8 이상 YOLO는 3가지 (Detection, Segmentation, Classification) 대표적인 기능을 제공하며, YOLO 모델을 사용하기 위해 YOLO 저장소를 다운로드하고, 해당 디렉토리에서 코드를 실행해야 합니다. 다음 명령을 실행하여 설치할 수 있습니다. Ultralysis 패키지를 설치하면 모두 자동으로 설치됩니다.​​[Install YOLOv8] 출처: 【Python】 YOLOv8をWindows10に導入する方法（画像付き）｜星杜なぎさ｜note​  [YOLOv8 모델 불러오기]대부분 학습된 모델의 경우, Yolo 명령어 실행 시 자동으로 다운로드됩니다.YOLOv8은 YOLOv5와 같은 이전 제품보다 매개 변수가 더 많지만 YOLOv6보다 매개 변수가 작습니다. n 크기 모델에 대해 약 33% 더 높은 mAP(정확도)를 제공하며, 모든 YOLO 모델 중에서 제일 빠른 inference 시간을 관찰할 수 있습니다.​YOLOv8 내에는 yolov8- n - nano, s - small, m - medium, l - large 및 x - extra large와 같은 다양한 모델 크기가 있습니다.​ 출처: 【Python】 YOLOv8をWindows10に導入する方法（画像付き）｜星杜なぎさ｜note​큰 모델은 더 높은 mAP(정확도)로 개체를 감지하기 위해 더 많은 inference 시간을 소요합니다.작은 모델은 더 빠른 inference 시간을 갖지만 상대적으로 더 낮은 mAP(정확도)를 가지고 있어 데이터가 적을수록 mAP(정확도)가 좋은 큰 모델을 이용하는 게 좋습니다. 또한 공간이 작을수록 더 작은 모델이 효율적입니다.​​[Object Detection을 위한 데이터 주석 Yolo 형식으로 변환하기]Object Detection에서는 이미지에 있는 다양한 클래스를 식별하고, 정확한 위치를 감지해야 합니다. ADAS에서 제공하는 데이터 세트들의 주석 JSON 파일을 Yolo 형식에 맞춰 변화할 수 있습니다.​""bbox"": [    <absolute_x_top_left>,    <absolute_y_top_left>,    <absolute_width>,    <absolute_height>]​YOLO 형식의 Train 세트 구조는 다음과 같습니다.​ ​각 Images 폴더 안의 이미지에는 .txt 파일이 있으며, 이 txt 파일은 다음과 같이 이미지의 개체 및 bbox 정보를 포함하고 있습니다. (동일한 디렉토리에 같은 이름을 사용하지만 확장자는 .txt임)​예를 들어 img1.jpg의 경우, 다음과 같은 것을 포함하는 img1.txt가 있어야 합니다.​ ​​[데이터 세트에서 학습시키기 위한 명령] yolo task=detect mode=train model=yolov8n.pt imgsz=640 data=custom_data.yaml epochs=10 batch=8 name=yolov8n_custom task: 학습 분류 [detect, segment, classify]mode: 학습 모드 [train, val, predict]model: 학습된 YOLOv8 모델명imgsz: 이미지 크기 (기본 해상도: 640)data: 데이터 세트 YAML 파일의 경로epochs: 학습 횟수batch: 데이터 로더의 배치 크기/ GPU 메모리 가용성에 따라 늘리거나 줄일 수 있음name: 결과 디렉터리 경로 [.runs/detect]​train 함수를 이용해서 Yolov8을 학습할 수 있는데 yaml 파일에서 반드시 data 파라미터로 지정되어 있어야 합니다.​​[사용자 데이터셋 Train 결과] ​Object 학습이 끝나면 생성된 모델을 사용하여 아래의 명령어를 입력하여 추론합니다. yolo task = detect, mode=predict, model=/runs/detect/train/weights/best.pt,conf=0.25, source=/test/images  ​[그림 결과 이미지]ADAS 데이터 세트에서 YOLOv8 모델을 학습시키는 과정을 진행했으며, 지정된 디렉토리에 있는 이미지에 대해 Object Detection 결과 화면을 확인할 수 있습니다.​​​​ "
[논문리뷰]Class-agnostic Object Detection with Multi-modal Transformer (ECCV 2022) ,https://blog.naver.com/sounghyn105/222954961854,20221213,"Main contribution:다양한 상황에서 일반적으로 적용이 가능한 Multi-modal Transformer (MAVL)을 개발 Class-agnostic Detection으로, objectness 자체를 detect를 하는 모듈(일반적으로 적용이 가능)멀티모달 트랜스포머로, query-based image-feature와 이미지를 설명하는 text를 late fusion을 통해 정보를 통합하고, late fusion transformer block을 거쳐 vision-language fusion을 이룬다.​ a, b는 기존에 있던 multi-modal 방식이고, MAVL은 이 논문에서 제시한 모델이다.Text는 ""all objects"", ""all tall objects"", ""all obscure objects""... 와 같이 단서 형태로 주어진다. MAVL이 기존 모델과 다른점은 두가지로, multi-scale feature maps와 late-fusion technique다.Multi-scale keypoints는 sparse한 key와 쿼리를 바탕으로 여러가지 scale에서 detection을 진행할 수 있다.Late fusion technique는 이미지와 text를 각각 따로 처리를 하고 나중에 합친다.​Experiment 다양한 dataset에서의 Class agnostic task 결과다. 맨 밑의 섹션은 위에서 설명한 멀티모달 모듈인데, 그중에서 가장 좋은 성능을 보여주었다.​이 뿐만 아니라 다양한 task와 환경에서 일정한 변형을 통해 적용이 가능함을 보여주었고 applicable함을 입증하였다.뛰어난 성능을 보여준 taskclass agnostic에서  자주 보여지지 않는 물체의 탐지율언어와의 상호작용(interactability) -> 텍스트로 어떻게 인풋을 주느냐에 따라 성능을 더욱 향상 시킬 수 있다.(ex long objects, all little objects)학습시 나오지 않은 물건까지 알아차리는 open-world object detection -> MViT와 ORE(RPN기반)을 비교Self-supervised learning : 방대한 양의 object를 라벨링 없이 detect할 수 있다는 장점이 있음. 하지만 class agnoistic 특성상 local feature에 집중하게 되는 현상으로 global feature를 놓치는 문제점이 있었는데 MAVL을 통해 psudo-labeling을 만듦​MAVL은 pseudo-label을 만드는데 최적화 되어있는 모델! "
딥러닝 Object Detection 모델 살펴보기(3) : Faster R-CNN (Faster RCNN) 논문 리뷰 ,https://blog.naver.com/baek2sm/222784719619,20220630,"이번 포스팅에서는 딥러닝 기반의 object detection 모델 중 최초로 end-to-end로 구현된 Faster R-CNN[1] 논문을 리뷰해보도록 하겠습니다. Faster R-CNN은 Fast R-CNN의 아키텍처를 대부분 그대로 따르되, region proposal 부분을 selective search에서 딥러닝 기반의 region proposal network로 대체함으로써 end-to-end 모델 구조를 완성했습니다. ​저도 video 기반의 action recognition 관련 연구를 진행하면서 video마다 frame의 길이가 다르다는 문제 때문에 key frames을 extraction하기 위한 별도의 전처리 과정을 사용했는데, 조금 더 신중히 고민해서 비디오 기반의 end-to-end 방법을 제안했으면 좋았을걸.. 하는 아쉬움이 남네요. 개인적인 얘긴 뒤로하고, Faster R-CNN을 살펴보도록 하죠! 지난 Fast R-CNN 논문 리뷰 포스팅에서 설명한 것과 중복되는 부분들은 간단히 설명하겠습니다. Faster R-CNN논문: https://arxiv.org/abs/1506.01497R-CNN[2]은 selective search를 통해 region proposal을 하고, CNN을 통해 feature extraction을 한 뒤, SVM(Support Vector Machine)을 통해 최종 예측을 수행했습니다. 다만, region proposal을 통해 얻은 2,000개의 bounding box 영역에 대해 CNN 모델 inference를 2,000번 수행하므로 속도가 매우 느리다는 단점이 있었습니다. Overview of R-CNN, 출처: R-CNN 논문[2]Fast R-CNN[3]은 R-CNN과 마찬가지로 selective search를 통해 region proposal을 했지만, ROI(Region Of Interest) pooling을 제안해 region proposal을 통해 얻은 2,000개의 bounding box 영역에 대해 CNN 모델inference를 1번만 할 수 있도록 개선하여 속도를 획기적으로 향상시켰습니다. 또한 모델의 output 부분을 SVM에서 fully connected layer와 softmax를 사용하는 딥러닝 기반의 방법으로 개선했습니다. Overview of Fast R-CNN, 출처: Fast R-CNN 논문[3]Faster R-CNN은 Fast R-CNN의 region proposal 영역까지 딥러닝 기반의 RPN(Region Proposal Network)로 대체해 최초로 높은 정확도를 가진 end-to-end 방식의 object detection 모델을 제안했습니다. Faster R-CNN 모델이 object detection을 수행하는 과정은 다음과 같습니다.​CNN 모델을 통해 feature map을 얻습니다(논문에서 CNN backbone 모델은 ZF, VGG를 사용합니다).feature maps은 병렬 구조로, region proposal을 위한 RPN 네트워크의 입력으로도 함께 사용합니다.RPN 네트워크에서 얻은 bounding box 후보 영역(ROI 영역)을 feature map에서 잘라내서, ROI pooling을 수행합니다.ROI pooling으로 얻은 최종 feature maps을 이용해 classification과 bounding box regression을 수행합니다.​이 과정을 나타낸 Faster R-CNN의 구조는 다음 그림과 같습니다. Overview of Faster R-CNN, 출처: Faster R-CNN 논문[1]Faster R-CNN에서 중요한 핵심 부분은 RPN입니다. RPN은 각 픽셀을 중심으로 k개의 anchor boxes를 미리 정의하고, 1x1 CNN 연산을 통해 k개의 anchor boxes 영역 중 객체가 있을 확률과 bounding box 영역을 예측합니다. k개의 anchor boxes는 1:1, 2:1, 1:2 등 여러 비율의 anchor box로 구성되며 Faster R-CNN에서는 9개의 anchor boxes를 이용해서 모델을 학습시킵니다. RPN(Region Proposal Network), 출처: Faster R-CNN 논문 [1]anchor boxes가 이미 미리 정의되어 있는데, bounding box를 다시 예측하는 것이 의아할 수 있는데요. 사전에 정의된 k개의 anchor box로 object의 bounding box를 정확하게 예측하는 것은 불가능합니다. 따라서 bounding box regression은 bounding box 영역을 보정하는 역할을 한다고 생각하시면 될 것 같습니다.​조금 더 구체적으로 설명하자면, feature map의 크기는 NxM이고 256개의 채널로 구성되어 있습니다. 여기에 1x1 convolutional layer를 이용해서 각 픽셀의 영역의 anchor에 오브젝트가 존재하는지 유무를 classification하고, 또 다른 1x1 convolutional layer를 이용해서 bounding box 영역을 보정하기 위한 값을 regression합니다.​즉, k개의 anchor를 사용한다고 했을 때 오브젝트가 존재하는지 유무를 classification하는 부분은 2k개의 채널을 갖도록 해서, 각 픽셀 영역에서 k개의 anchor에 오브젝트가 존재하는지/존재하지 않는지를 분류하도록(softmax) 학습시킵니다.​그리고 bounding box는 각 픽셀 영역에서 k개의 anchor에 해당하는 x, y, width, height 값을 보정해야 하므로 4k개의 채널을 갖도록 해서 bounding box 영역을 보정할 값을 regression합니다.​이렇게 RPN을 통해 오브젝트가 있을만한 위치에서 bounding box 영역을 추려낸 뒤 모델의 후반부 구조는 Fast R-CNN과 같습니다. 마지막으로 한가지 더 알아둘만한 내용은, 같은 object에 대한 여러 bounding box 후보가 나타날 수 있기 때문에 복잡성을 낮추기 위해 RPN의 classification score를 통해 Non-Maximum Suppression(NMS)을 수행하여 영역이 많이 겹치는 bounding box를 제거했다는 것도 알아두세요! 딥러닝 Object Detection 모델 살펴보기 시리즈딥러닝 Object Detection 모델 살펴보기(1) : R-CNN 논문 리뷰: https://blog.naver.com/baek2sm/222782537693 딥러닝 Object Detection 모델 살펴보기(1) : R-CNN 논문 리뷰이번 포스팅에서는 딥러닝을 이용한 object detection 연구의 출발점이라 할 수 있는 R-CNN [1] 논문을...blog.naver.com 딥러닝 Object Detection 모델 살펴보기(2) : Fast R-CNN 논문 리뷰: https://blog.naver.com/baek2sm/222783238067 딥러닝 Object Detection 모델 살펴보기(2) : Fast R-CNN (Fast RCNN) 논문 리뷰이번 포스팅에서는 R-CNN [1]의 처리 속도를 획기적으로 개선하고, mAP(mean average precision)도 ...blog.naver.com 딥러닝 Object Detection 모델 살펴보기(3) : Faster R-CNN 논문 리뷰 (현재 포스팅)딥러닝 Object Detection 모델 살펴보기(4) : YOLO : You Only Look Once 논문 리뷰 (포스팅 후 링크 연결 예정) 참고 문헌references[1] Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks : https://arxiv.org/abs/1506.01497[2] Rich feature hierarchies for accurate object detection and semantic segmentation : https://arxiv.org/abs/1311.2524[3] Fast R-CNN : https://arxiv.org/abs/1504.08083 "
'Smart Factory Safety System Using Artificial Intelligence Object Detection' 논문 리뷰 ,https://blog.naver.com/gayeon6423/223106176937,20230519,"[요약 및 주제]인공지능 객체 탐지 기술을 활용하여 공장 내 작업자와 위험물체의 실시간 위치를 판별하고, 작업자와 위험물체의 위치에 따라 실시간으로 변동하는 위험지역을 계산했습니다. 움직이는 물체만 탐지하는 알고리즘을 사용하여 충돌 위험을 감지했습니다.실시간 객체 탐지를 위해 YOLO 알고리즘을 사용하였고 실제 공장 환경에 CCTV를 설치하여 데이터를 수집하고 안전 시스템을 구축하였습니다.​[키워드]스마트 공장, 지능형 영상 분석, 객체 추적, 객체 탐지, YOLOv5, DeepSORT​[1. 서론]공장 내 안전사고는 작업자의 생명 및 공장 전체 설비와 직결되므로 공장의 운영에 큰 영향을 미치게 됩니다. 공장에서 사고가 발생하게 되면 '작업중지명령'으로 인해 공장의 가동이 중지될 수 있기 때문입니다.예기치 못한 안전사고를 예방하기 위해서 지능형 영상 분석이 안전, 보안 시스템의 다양한 방면에서 사용되고 있습니다. 현재의 지능형 영상 분석 시스템만으는 실시간으로 변하는 공장의 안전사고를 완전히 예방하는데 한계가 있습니다. 안전지대와 위험지역을 지능형 형상 분석의 인공지능 모델이 스스로 판별하고 위험 상황을 인지하는 시스템은 없기 때문입니다.본 논문에서는 인공지능 객체 탐지를 활용하여 공장 내 작업자와 위험물체의 실시간 위치를 판별하고, 작업자와 위험물체의 위치에 따라 실시간으로 변동하는 위험 지역을 계산합니다. 이를 기반으로 작업자 및 위험물체의 위치와 위험지역을 모니터링 할 수 있습니다.​[2. 관련 연구][2.1 스마트 팩토리의 안전 시스템]대부분의 공장에서는 작업자들에게 안전 교육을 필수적으로 이행하고 있습니다. 가상현실(VR)을 사용한 안전교육, 증강현실(AR)기술을 사용한 스마트 글라스와 애플리케이션을 통한 안전 점검 보조 등과 같이 스마트 정보 기술을 안전 교육에 적용하고 있습니다.​풀프루프(Fool-Proof)설계는 작업자가 실수하고 싶어도 실수하지 못하게 하거나, 실수가 발생하였다 하더라도 경보가 발생하여 그 피해를 최소로 하거나 아예 없도록 하는 시스템이나 장치를 뜻합니다. ​기존 풀프루프 : 끼임 사고 방지를 위해 회전기나 절단기에 가드 설치, 센서와 잠금장치, 안전 레버나 스위치​인공지능을 활용한 풀프루프 : 1. 센서에 의한 상황 모니터링 제어 방법(IOT센서를 활용하여 공장 설비의 위험 물질 누출 모니터링 + 디지털화된 개인 보호장비에 RFID칩과 카메라, 주변환경 파악할 수 있는 센서 부착 => 모니터터링 결과 위험 발생하면 설비 중단 or 작업자에게 알림을 줘서 사고 예방 => 사고 발생 부분 저장하고 이를 기반으로 사고 당시 상황 기록 => 사고 예방을 위해 인공지능 모델 학습)2. CCTV에 딥러닝 적용하여 보호장구 착용 확인, 위험지역에서 작업자의 행동 패턴 분석하여 이상한 행동 감지되면 작업 현장에서 경고, 작업자들의 안저너 교육 및 모니터링 시스템에활용 가능​[2.2 인공지능 객체 탐지][2.1.1 인공지능 객체 탐지란?]객체 탐지 : 이미지에서 객체의 종류를 식별함과 동시에 그 위치를 찾는 컴퓨터 비전 기술 => Classification(객체의 종류 식별) + Localization(객체의 위치를 파악)즉, 객체의 클래스를 '분류' + 바운딩 박스를 찾는 '회귀'문제를 함께 수행합니다. ​​2012년 CNN(ConvolutionNeural Network, 합성곱 신경망)등장 : 이미지 분류2014년 R-CNN등장 : CNN을 활용한 객체 탐지 처음 등장​One-Stage Detector : Object Detection(분류)와 Region Proposal(지역화)를 동시에 수행Two-Stage Detector : Object Detection(분류)와 Region Proposal(지역화)를 순차적으로 수행 => 보통 Two-Stage Detector가 One-Stage Detector보다 상대적으로 수행 시간은 느리지만, 분류와 지역화의 정확도는 높습니다. 최근 객체탐지는 실시간 위험 요소 탐지를 위해 사용되므로 짧은 시간이 장점인 One-Stage Detector를 주로 사용합니다.​[2.2.2 YOLO 알고리즘] 대표적인 One-Stage Detector방식의 객체 탐지 알고리즘으로 객체 탐지를 하나의 이미지로부터 Bounding Box의 위치, 클래스 확률을 구하는 과정을 단일 회귀 문제로 판단합니다. YOLO에서는 객체 탐지를 하기 위해 이미지를 7x7개의 그리드(49개의 Grid Cell)로 분할합니다.객체의 중심점이 각각의 그리드 셀 안에 위치하면, 해당하는 그리드 셀이 그 객체를 검출합니다.각각의 그리드 셀은 2개의 Bounding Box와 class에 속할 확률인 20개의 Pr로 구성되어 있습니다.Bounding Box는 정 중앙 좌표인 x, y좌표, 바운딩 박스의 너비 w, 높이를 전체 이미지로 나눠 노멀라이즈한 h, 박스 내부에 물체가 존재할 확률인 Confidence Score인 pc까지 5가지로 구성되어 있습니다.​ 첫 번째 bounding box의 confidence score와 각 conditional class probability를 곱하면 첫번째 bounding box의 class specific confidence score이 나오게 됩니다.두번째 bounding box의 confidence score와 각 conditional class probability를 곱하면 두번째 bounding box의 class specific confidence socre이 나오게 됩니다. 이를 통해 총 98개의 class specific confidence score를 얻을 수 있게 됩니다.평가 지표로는 IoU를 사용합니다. 이는 객체의 실제 Bounding Box와 예측 Bounding Box의 합집합 면적과 교집합 면적의 비율로, 예측한 Bounding Box가 실제와의 차이가 얼마나 있는지를 나타내는 지표입니다.=>본 논문에서는 YOLO v5를 사용했습니다.​[2.2.3 DeepSORT 알고리즘] DeepSORT : SORT(Simple Online and Realtime Tracking)알고리즘 + 딥러닝 알고리즘SORT : 작동 원리가 간단하고 연산량에 큰 영향을 미치지 않는 실시간 추적을 위한 다중 객체 추적 알고리즘입니다. 칼만 필터 + 헝가리안 알고리즘 조합으로 구성됩니다.칼만필터는 이전의 정보를 사용하여 새로운 측정값에 잡음을 제거하여 최적의 추정값을 찾는 알고리즘으로 이를 객체 탐지에 도입함으로써 객체 탐지의 잡음을 처리할 수 있습니다. 또한, 객체 위치를 예측할 수 있어서 객체를 추적하기에 적합합니다. 하지만 SORT의 폐색이나 다른 ID를 할당하는 문제가 발생하는데 이를 해결하기 위해 외형 정보를 사용하는 딥러닝을 통해 SORT알고리즘의 문제를 해결하였습니다.=> 본 논문에서는 객체 추적 알고리즘으로 DeepSORT를 사용하였습니다.​[3. 제안 알고리즘][3.1 공장환경]선박 배기가스와 선박용 스크러버를 제조하는 공장으로 전체 작업 구역 내에 다수의 작업자와 위험물체가 분포됨​[3.2 제안 안전 시스템]1) 작업구역을 결정하고 위험지역을 판별하는 세부구역을 설정하기 위해 그리드로 분할2) 객체 탐지를 진행하여 작업자와 위험물체 판별(움직이는 객체만 탐지)3) 그리드 셀마다 작업자와 위험 물체의 존재 여부 파악 후 안전지대와 위험지대 판별4) 그리드와 탐지된 객체, 위험지대를 합하여 결과 화면 표시(위험지대는 다른 색)​[3.2.1 그리드 설정]CCTV는 항상 동일한 시점에서 운용되도록 고정하고 이를 기준으로 전체 구역을 그리드 형태로 분할그리드의 각 셀은 동적으로 안전 상태, 경고 상태, 위험 상태 중 하나로 설정 직관적이고 수정이 쉬운 CSV포맷을 사용하여 그리드 데이터를 저장해서 사용했습니다. CSV파일에는 그리드를 이루는 셀들의 꼭짓점을 저장각 꼭짓점은 영상의 왼쪽 위(0,0)부터 오른쪽 아래(w,h)까지의 좌표에서 절대 좌표(x,y)로 저장됩니다. 꼭짓점의 좌표는 CSV파일에서 한 꼭짓점마다 한 줄로 저장​[3.2.2 객체 탐지]입력 영상을 실시간으로 처리하여 위험물체와 작업자를 탐지하는 YOLO알고리즘 사용일YOLO로 탐지된 객체 => DeepSORT를 통한 탐지 결과 보정 및 ID부여해 물체 추적N_INIT : 새로운 객체가 탐지될 때, 최소 몇 프레임동안 객체 탐지가 유지되어야 탐지된 것으로 인정하는지에 대한 값 => N_INT프레임동안 객체 탐지가 유지되면 해당 객체 탐지 결과로 추가되고 새로운 ID부여MAX_AGE : 각 ID의 물체가 소실되어도 해당 ID를 몇 프레임동안 유지할지 설정 => 일시적으로 객체 탐지가 되지 않아도 MAX_AGE프레임동안 해당 ID와 객체의 Bounding Box저장 => 같은 물체에 새로운 ID부여되는 문제 해결​본 시스템에서는 동일한 ID를 가지는 물체마다 직전 프레임과 현재 프레임의 Bounding BOX를 비교하여 IoU값이 설정값 이하가 되면 움직이는 상태로 간주합니다.​특정 프레임의 Bounding Box가 잘못 그려지거나 다른 물체와 일시적으로 겹침으로 인해 Bounding Box가 순간적으로 변하게 됩니다. 이 때 두가지 문제가 발생합니다.1) 움직이지 않는 물체를 움직인다고 판단하는 경우움직이지 않는 물체의 Bounding Box가 변하여 IoU가 설정한 값 이해가 된 경우입니다. => 이를 방지하기 위해 최소 N_INIT프레임동안 연속적으로 움직이는 상태가 지속될 떄만 최종적으로 물체가 이동중인 것으로 판단합니다. 이를 통해 물체가 겹쳐서 Bounding Box가 변화되는 경우는 해결 할 수 있습니다.​2) 움직이는 물체가 움직이지 않는다고 판단하는 경우움직이는 물체가 일시적으로 정지하거나 미미하게 움직인 경우입니다. => 이를 방지하기 위해 이동중으로 MAX_AGE프레임동안 단 한 반도 움직이는 상태가 되지 않았을 때만 정지 상태로 판단하면서 이 문제를 해결했습니다.​​[3.2.3 위험지대 판별]안전 상태 : 세부구역이 안전지대(구역 내 위험물체 존재X)경고 상태 : 세부구역이 위험지대 & 작업자와 충돌 가능성↓ (구역 내 위험물체 존재 O, 작업자 존재X)위험 상태 : 세부구역이 위험지대 & 작업자와 충동 가능성↑ (구역 내 위험물체 존재O, 작업자 존재O)​경고상태에서 작업자 진입 => 위험상태 돌입감지된 작업자와 위험물체 기반으로 그리드 셀에 위험지대 계산배열의 각 요소는 해당 그리드 셀에 작업자와 위험물체가 존재하는지 표시해당 프레임에서 탐지된 모든 객체의 Bounding box가 각각의 그리드 셀과 겹치는지 계산그리드 셀미다 [작업자 존재여부, 위험물체 존재여부] 표시(존재하면1, 없으면0) 안전상태 : [1,0], [0,0] /  경고상태 : [0,1]=> 노란색 표시 / 위험상태 : [1,1] => 빨간색 표시​=>OpenCV를 사용하여 CCTV화면에 그리드, 객체 탐지, 위험지대를 그립니다. 그리드는반투명 직선으로, 탐지된 객체와 위험지대는 사각형과 색으로 반투명으로 채워진 사각형으로 표시합니다.​[4. 실험 및 결과][4.0 공장 환경 및 데이터 수집]선박 배기가스와 선박용 스크러버를 제조하는 공장에 CCTV를 설치하여 영상 수집공장의 작업 시간대인 평일 08시~16시에 녹화, 6개월간 녹화진행, 1분 간격으로 나누어 사진 파일 형태로 저장 실험 공장 환경에 설치한 CCTV 화면의 작업구역[4.1 그리드 결정]정해진 작업구역의 위험지대 계산을 위해 가로1.5m간격, 세로 3m 간격의 그리드로 분할 작업구역에 설정하는 그리드와 최종 결과 [4.2 위험물체 선정] 실험 공장에서 탐지할 객체의 종류worker : 공장의 작업자vehicle : 승용차, 트럭, 화물차, 지게차 등 공장 내에서 움직이는 탈 것scrubber : 해당 공장에서 주력 생산하는 품목으로 선박에 들어가는 거대 탱크box : scrubber운반하기 위해 포장한 목재 상자w_scrubber : 주황색의 방수천으로 덮어둔 scrubbercover : 육면체의 box의 한 면을 이루는 나무판자​[4.3 학습 데이터셋 구축]CCTV로 수집한 사진 중 무작위로 선별된 911장의 사진 선정 => 각각의 파일에 대해 전체 객체에 대한 라벨링 진행총 11,448개의 객체가 라벨링 라벨링 파일에서 각 필드에 대한 설명 / 클래스별 라벨링 개수 ​[4.4 모델 학습]YOLOv5모델의 네트워크별 구분 : YOLOv5n, YOLOv5s, YOLOv5m, YOLOv5l, YOLOv5x YOLOv5의 모델별 성능과 속도실험 PC에서는 최고 크기의 모델인 YOLOv5x를 사용해도 실시간에 해당하는 30FPS를 훨씬 초과 => 모델로 선택해서 사용YOLOv5의 기본 파라미터 사용학습과 검증 데이터셋 비율 7:3 설정평균적으로 100~200epochs에서 best모델 생성​[4.5 실험 결과]안전지대와 위험지역의 정확도 판별은 실시간 객체 탐지의 정확도와 비례 => 객체 탐지의 정확(AP)도를 평가 지표로 선정  객체 탐지에서 정답과 오답은 실제 라벨링한 Bounding box와 모델에서 탐지한 Bounding box의 IoU가 기준값 이상인지로 결정합니다. 기준값이 0.5 : IoU >= 0.5이면 TP, IoU < 0.5이면 FP정밀도-재현율 그래프 : 객체의 예측 신뢰수준에 따라 정밀도와 재현율을 그래프로 나타낸 것AP : 정밀도-재현율 그래프를 적분하여 단일 숫자로 표현함으로써 점수화한 값mAP : 모든 클래스의 AP의 평균  학습 모델의 epoch-mAP_0.5그래프YOLOv5모델의 epoch에 따른 IoU가 0.5일때 mAP의 값을 나타낸 그래프우상향 추세의 그래프를 그리면서 마지막 500epoch에서 0.9853의 최곳값 IoU가 0.5~0.95로 0.05씩 값을 변경하면서 계산된 mAP의 평균의 값을 나타낸 그래프283epoch에서 최곳값인 0.9272를 가진 후 하향=> YOLOv5에서 best모델 : mAP_0.5와 mAP_0.5:0.95의 값을 1:9의 비율로 합산했을 때 가장 높은 값을 가지는 epoch사용 => 283epoch의 모델​ best모델의 정밀도-재현율 그래프와 AP,mAPworker의 AP : 0.982, vehicle의 AP : 0.977, scrubber의 AP : 0.955, box의 AP : 0.991, w_scrubber의 AP : 0.990, cover의 AP : 0.976 => mAP : 0.985 ​(객체 추적으로 움직이는 객체를 판별하는 알고리즘에 대한 실험)그림12,14,16은 객체 추적이 적용되기 전의 결과물 영상의 스크린샷그림 13,15,17은 객체 추적이 적용된 결과물 영상의 스크린샷  왼쪽은 움직이지 않는 위험물체도 탐지하면서 위험지역 설정 오른쪽은 움직이는 사람 3명에 대한 객체 추적만 적용  왼쪽은 움직이지 않는 사람도 탐지하면서 위험지역 설정오른쪽은 움직이지 않는 사람을 제외하고 우측 box위의 움직이는 사람 3명에 대해서만 객체 추정  ​왼쪽은 움직이지 않는 물체까지도 탐지오른쪽은 움직이는 사람 3명에 대해서만 객체 추적, 위험물체의 객체 추정과 동시에 위험지역 표시​=> 객체 추정을 통해서 움직이는 물체만 탐지할 수 있게 됩니다.​[5. 결론 및 향후 연구 방향]실시간으로 이동하는 작업자와 위험물체에 따른 위험지역의 변경을 고려한 안전 시스템을 제안제안시스템 : 객체탐지를 위한 YOLOv5 알고리즘 + DeepSORT를 사용하여 탐지 결과 보정현재 프레임에서 탐지된 객체를 직전 프레임에서 탐지된 객체와 비교하여 움직이는 물체 판별움직이는 위험물체와 작업자를 기반으로 하여 3가지 상태의 위험지대 판별 + 결과 모니터링 시스템 구축제안 안전 시스템을 평가하기 위해 실제 공장 환경에 CCTV를 설치하여 실험 수행실험을 진행하기 위해 관리자가 선정한 5가지의 위험물체와 작업자에 대한 라벨링 진행공장의 환경에 맞춰 작업구역 정하고 세부구역 분할최소 0.976의 AP, 0.985의 높은 mAP 성능움직이는 객체를 추정하는 알고리즘을 통해 충돌 위험이 있는 위험지대 판별​한계점 : 객체가 작업구역의 지면과 접하는 부분은 Bounding box는 정확한 판단이 힘들게 됩니다.=>Bounding box형태가 아닌 마스크와 같은 다른 방법이 필요할 것으로 보인다고 언급하며 마무리됩니다.​​[References][논문 출처]https://www.dbpia.co.kr/journal/articleDetail?nodeId=NODE11149401[참고한 사이트]https://durian9s-coding-tree.tistory.com/136https://www.dbpia.co.kr/journal/articleDetail?nodeId=NODE09012398https://curt-park.github.io/2017-03-26/yolo/​ "
Object Detection API에서 checkpoint 옵션 변경 ,https://blog.naver.com/kyoungseop/222701840649,20220415,"Object Detection API에서 checkpoint 옵션 변경을 변경하려면​run_config.py 파일에서 적당히 바꿔준다.​@estimator_export('estimator.RunConfig')class RunConfig(object): """"""This class specifies the configurations for an `Estimator` run.""""""   def __init__(self,               model_dir=None,               tf_random_seed=None,               save_summary_steps=100,               save_checkpoints_steps=_USE_DEFAULT,               save_checkpoints_secs=_USE_DEFAULT,               session_config=None,               keep_checkpoint_max=5,               keep_checkpoint_every_n_hours=10000,               log_step_count_steps=100,               train_distribute=None,               device_fn=None,               protocol=None,               eval_distribute=None,               experimental_distribute=None,               experimental_max_worker_delay_secs=None,               session_creation_timeout_secs=7200,               checkpoint_save_graph_def=True):​훈련을 시키려고 하면 step이란게 나오는데  그 의미를 아래와 같이 설명한다.A step is one operation to update the weights of the model. So the number of steps is exactly the number of times the weights will be updated by the optimizer그래서 만일 70,000 training images 가 있고  batch size of 32 라면 epoch당 weight의 업데이트는 t 70000/32 대략 2188 이루어진다.n epoch를 훈련 시키고 싶으면 (이미지수 / batch size) * n 의 step만큼으로 환산이 된다.​TF 2.x에서는model_main_tf2.py에서    with strategy.scope():      model_lib_v2.train_loop(          pipeline_config_path=FLAGS.pipeline_config_path,          model_dir=FLAGS.model_dir,          train_steps=FLAGS.num_train_steps,          use_tpu=FLAGS.use_tpu,          checkpoint_every_n=FLAGS.checkpoint_every_n,          record_summaries=FLAGS.record_summaries,          checkpoint_max_to_keep=500)​로 수정하여 준다.​​​​​​​ "
FCOS (object detection) ,https://blog.naver.com/mose1204/222508880459,20210917,"https://github.com/rosinality/fcos-pytorch GitHub - rosinality/fcos-pytorch: Re-implementation of FCOS for personal studyRe-implementation of FCOS for personal study . Contribute to rosinality/fcos-pytorch development by creating an account on GitHub.github.com FCOS : Fully Convolutional One-stage Object Detection코드 + 논문 리뷰​내가 이해한 논문의 아이디어•각 feature map의 location(x,y)들을 원래 이미지에서의 좌표들로 대응시킨 후그 점들에서 bbox를 예측 ( l, r, t, b 회귀시킴)•이 때, centerness라는 개념을 도입원래 이미지에서의 좌표, 그리고 그 좌표를 포함하는 bbox가 있을 때좌표가 그 bbox의 center에서 멀수록 페널티를 받음 (centerness)​ ​ "
"image classification, Segmentation, Object detection의 각각 분류하는 단위 ",https://blog.naver.com/taeyang95/222996140509,20230127,"image classification, Segmentation, Object detection의 각각 분류하는 단위image classification의 분류단위는 일반적으로 이미지 전체이다.Semantic Segmentation의 분류 단위는 픽셀단위이다.object detection 분류 단위는 일반적으로 bounding box라고도 불리는 이미지의 직사각형 영역(객체)이다.  Semantic Segmentation & Instance SegmentationSemantic Segmentation 문제에서 입력은 이미지이며 출력으로 이미지의 모든 픽셀에 카테고리를 정한다.예를 들어 위 슬라이드의 예제, 고양이 이미지 파일을 입력한다고 가정한다.출력은 모든 픽셀에 대하여 그 픽셀이 고양이, 잔디, 하늘, 나무, 배경인지를 결정한다.Semantic Segmentation에서도 Classification 처럼 카테고리가 있다. 하지만 Classification처럼 이미지 전체에 카테고리 하나가 아니라 모든 픽셀에 카테고리가 매겨진다.Semantic Segmentation은 개별 객체를 구별하지 않는다. 위 슬라이드의 Semantic Segmentation 결과를 살펴보면 픽셀의 카테고리만 구분해 준다.즉, 오른쪽 슬라이드의 결과를 보면 소 2마리가 있는데 2마리 각각을 구분할 수는 없다.이는 Semantic Segmentation의 단점이며 Instance Segmentation에서 이 문제를 해결할 수 있다.  Classification + Localization이미지 분석을 하다 보면 이미지가 어떤 카테고리에 속하는지 뿐만 아니라 실제 객체가 어디에 있는지를 알고 싶을 수 있다.Classification + Localization은 이미지를 “cat”에 분류하는 것 뿐만 아니라 이미지 내에 cat이 어디에 있는지 네모 박스를 그리는 것이다.Classification + Localization 문제는 Object Detection 문제와는 구분이 된다. Localization 문제에서는 이미지 내에서 관심있는 객체는 오직 하나라고 가정하기 때문이다.기본적으로 이미지 내에 객체 하나만 찾아서 레이블을 매기고 위치를 찾는 문제이다.  Object detectionobject detection는 입력 이미지가 주어지면 이미지에 나타나는 객체들의 bounding box에 해당하는 카테고리를 예측한다. 즉, 앞에서 다룬 classification + localization과는 조금 다르다.이렇게 하는 이유는 예측하야 하는 Bbox의 수가 입력 이미지에 따라 달라지기 때문이다.. 각 이미지에 객체가 몇 개나 있을 지는 알 수 없다.위 슬라이드를 보면 첫 번째 사진에는 고양이 한마리에 대한 Bbox의 정보가 있어야 한다.두 번째 사진은 개 2마리, 고양이 1마리에 대한 Bbox 정보가 있어야 한다.마지막 사진에는 오리 여러 마리에 대한 Bbox가 있어야 한다.  Segmentation와 Object Detection 적용 사례Segmentation의료 영상: 인체 이미지에서 특정 구조 또는 관심 영역을 식별하고 분할이 가능하다.이미지 편집: 이미지의 전경과 배경을 분리하거나 이미지 내의 특정 개체를 분리할 수 있다.Object Detection물건 세기: 이미지 또는 비디오에 있는 물건 수를 셀 수 있다.감시 및 보안: 실시간 비디오 영상에서 사람, 차량 또는 기타 물체를 감지하고 추적할 수 있다.농업: 밭이나 과수원의 이미지에서 식물, 과일, 채소와 같은 물체를 감지 및 관리한다.제품검사: 제품 또는 기계 이미지의 결함 또는 이상 징후를 감지하고 찾아낼 수 있다.  https://gaussian37.github.io/vision-cs231n-detection-and-segmentation/https://www.youtube.com/watch?v=nDPWywWRIRo&t=2019s​ "
[제8회 금요명품세미나] 객체 탐지(Object Detection): R-CNN과 YOLO_강의후기 ,https://blog.naver.com/wedatalab/223087963277,20230428,"​ 안녕하세요. 연구하고 강의하고 책쓰고 개발하는 위데이터랩입니다. 😊 ​4월 28일 금요일, 강승우 부사장님의 금요 명품 세미나를 성황리에 마쳤습니다! 🎉 ​이번 세미나의 주제는 '객체 탐지(Object Detection): R-CNN과 YOLO' 이였으며, 이론 설명과 실습을 위주로 약 3시간 정도 강의가 진행되었습니다. ​대표적인 객체 탐지 알고리즘인 R-CNN과 YOLO의 원리, 동작방식, 장단점 등을 이해하고 활용하는 방법을 알려드렸습니다. ​앞으로도 위데이터랩에서는 데이터베이스, 빅데이터, 인공지능 관련 유익한 강의들을 제공할 예정입니다. 많은 관심과 참여 부탁드립니다! 🤗 ​위데이터랩의 강의정보를 알 수 있는, EZIS IT 학교 오픈채팅방 🔽🔽🔽 https://open.kakao.com/o/gE7U095d (패스워드: ezisit22)​  ​  위데이터랩서울특별시 종로구 삼일대로 457 수운회관 1303호 ​ #인공지능 #AI #클라우드 #모니터링솔루션 #DPM #APM #머신러닝 #딥러닝 #데이터 #위데이터랩 #코딩 #프로그래밍 "
Yolov5를 활용한 Object Detection[객체 검출] ,https://blog.naver.com/qkrdnjsrl0628/222857327064,20220824,"*Roboflow에는 다양한 Open Source 형태의 Dataset이 있습니다.   활용하면, 재밌는 프로젝트를 하는 것이 가능합니다.​* 코드 참고 바랍니당!https://github.com/kalelpark/TodayAnalysis/tree/main/Object%20Detection TodayAnalysis/Object Detection at main · kalelpark/TodayAnalysisIt's TodayAnalysis. Contribute to kalelpark/TodayAnalysis development by creating an account on GitHub.github.com ​*  별이 가장 많은  Pistols Dataset을 활용하여,  Detection Model을 만들어보도록 하겠습니다.https://universe.roboflow.com/ Roboflow Universe: Open Source Computer Vision CommunityDownload free, open source datasets and pre-trained computer vision machine learning models.universe.roboflow.com * YoloV5를 활용해보도록 하겠습니다.  Load Dataset and Model !curl -L ""https://universe.roboflow.com/ds/hzf5yklOnF?key=qAUnSECvfR"" > roboflow.zip; unzip roboflow.zip; rm roboflow.zip%cd /content!git clone https://github.com/ultralytics/yolov5.git * Yolov5에 존재하는 Package 및 Pip들의 Version을 맞춰줍니다! %cd /content/yolov5/!pip install -r requirements.txt// 경로 수정%cat /content/dataset/data.yaml Image 모두 가져오기 및 Dataset 나누기     - Dataset을 Yolov5 Pretrained_model에 알맞게 사용하게끔 변경하기 위함이다. # root로 변환%cd / from glob import globimg_list = glob('/content/dataset/export/images/*.jpg')print(len(img_list))from sklearn.model_selection import train_test_splittrain_img_list, val_img_list = train_test_split(img_list, test_size = 0.2, random_state = 2000)print(len(train_img_list), len(val_img_list)) data.yaml에 저장하기 위해, Image경로, 새로운 txt파일로 생성하기 with open('/content/dataset/train.txt', 'w') as f:    f.write('\n'.join(train_img_list) + '\n')with open('/content/dataset/val.txt', 'w') as f:    f.write('\n'.join(val_img_list) + '\n') yaml에 Dataset 경로 저장 import yamlwith open('/content/dataset/data.yaml', 'r') as f:    data = yaml.safe_load(f)print(data)data['train'] = '/content/dataset/train.txt'data['val'] = '/content/dataset/val.txt'with open('/content/dataset/data.yaml', 'w') as f:    yaml.dump(data, f)print(data) Yolov5 학습하기    - Github에 활용 방법과 관련하여, 상세하게 작성되어 있으니 참고하시기 바랍니다. %cd /content/yolov5/!python train.py --img 416 --batch 16 --epochs 50 --data /content/dataset/data.yaml --cfg ./models/yolov5s.yaml --weights yolov5s.pt --name gun_yolov5s_results 모델이 학습되는 것을 볼 수 있습니다! 또한, train을 통하여, 지속적으로 학습이 어떻게 되는지, log로 표현되어, 저장되어있는 것을 알 수 있습니다. * train batch에는 학습되는 과정에서의 기록들이 남는데, 확인하면 이미지들이 잘 학습되는 것을 보실 수 있습니다. * Model이 학습을 완료하면, weights를 저장하는 것을 보실 수 있습니다. * 학습이 잘되었다는 것을 확인할 수 있습니다. %load_ext tensorboard%tensorboard --logdir /content/yolov5/runs/ TEST Data 예측하기 from IPython.display import Imageimport osval_img_path = val_img_list[0]!python detect.py --weights /content/yolov5/runs/train/gun_yolov5s_results/weights/best.pt --img 416 --conf 0.5 --source ""{val_img_path}"" 경로를 통하여, 예측한 데이터를 확인할 수 있습니다.  Image(os.path.join('/content/yolov5/runs/detect/exp', os.path.basename(val_img_path))) ​ "
논문 | Overview of 3D Object Detection ,https://blog.naver.com/kimsamuel351/222717497951,20220501,"https://arxiv.org/abs/2010.15614​저자 : Yilin Wang, Jiayi Ye발행 시기 : 29 Oct 2020​3D object detection의 개요에 대해 간단하게 알 수 있는 논문​0. Abstract1. Introduction2. Data Format  - Data Set, Preprocessing3. 2D Object Detection  - 전통적인 방법, 딥러닝 방법4. 3D Object Detection  - using RGB image, using Point Cloud, Object detection combined RGB images with Point Cloud  0. Abstract Point cloud 3D object detection은 최근 많은 관심을 받고 있고, 3D computer vision 분야에서 활발히 연구되고 있다.  그러나 LiDAR 3D 객체 인식은 여전히 point clouds의 복잡성이라는 문제가 남아있다. 보행자, 자전거, traffic cone 같은 객체는 point가 적게 표현되어, 단지 point cloud 만으로 탐지가 어렵다. 이 프로젝트에서 우리는 다중 객체 인식에서 RGB와 point cloud data를 사용하는 framework를 제안한다. 우리는 ROI를 찾는데 기존의 2D detection models을 사용한다. 그리고 point cloud를 mapping 하고, 2D bounding box를 3D 공간으로 이동시킨다. 우리는 최근 발표된 nuScenes를 데이터 셋으로 사용한다. ​# 3D Object Recognition, Machin Learning, LiDAR Point Cloud​​1. Introduction  이 논문에서 최신 object detection을 요약하고, 데이터셋, 전처리 대한 설명에 이어 전통적인 방법과 딥러닝 방법의 2D object detection 소개와 3D object detection 주제에 대해 포괄적인 개요를 이야기한다.​​2. Data Format2-A. Data Set  - Depth Map : 물체의 표면 거리 정보를 포함하는 사진   - RGB-D format data set : Pascal VOC, COCO, ImageNet  - Radar data : 물체에 radio 파동을 보내고 반사된 data로, 물체의 속도와 거리를 정보를 포함한다.  - Point cloud data : X, Y, Z로 표현되는 3차원 벡터 data set 이면서 각 point cloud는 RGB color pixel, gray value, depth 정보를 포함한다. 대부분 point cloud data는 Lidar(2D/3D), STEREO 카메라, time-of-flight 카메라와 같은 3D scanning 장비로 정보가 수집된다. 대표적인 데이터셋은 KITTI, nuScenes, Waymo Open이 있다.  2-B. Preprocessing  - defogging algorithms : gamma correction, guided filtering  - 눈금 (calibration)  - 클래스 불균형 문제 -> 새로운 데이터 셋​​3. 2D Object Detection3-A. 전통적인 방법  - Histogram of Oriented Gradient (HOG) : Target detection algorithms  - Hough transform  : geometric shapes recognition  - Random Transform : medical image processing  - Yin : nose tracking  - etc​3-B. 딥러닝 방법 One stageTwo stagesbounding box를 하고, 분류하는 방법 (=Region-based method)이를 한단계에 하는 방법(end-to-end regression)(resizing, centering)정확성이 높음속도가 빠름 R-CNN, Fast R-CNN, Faster R-CNN, region-based fully convolutional network (R-FCN)Multi-Box, YOLO, Single Shot MultiBox Detector (SSD) ​​4. 3D Object Detection4-A. Object detection using RGB image   - 3D-GCK  - 2D bounding box를 예측하고, 뉴럴 네트워크를 사용하여 없는 depth 정보를 추측한다. 그리고 2D bounding box를 3D 공간으로 이동  - RGB-D sensor : 센서로 얻은 RGB 이미지를 gray scale 이미지로 변환하고, 배경을 분리한다. 후에 noise를 제거하고 5가지 분류 모델에 적용되어 feature extraction과 class 분류 예측을 한다.​​4-B. Object detection using Point Cloud  - 방법1. 직접 3차원 point cloud data를 사용하는 방법 : information loss가 적지만 컴퓨팅 자원이 많이 든다.  - 방법2. point cloud를 2차원 data로 처리하여 계산양을 줄이는 방법      ㄴex. Jansen 방법 : 64개의 다른 각도로 이미지 처리   - BirdNet : Object detection framework. LiDAR data.   - Deep Point Cloud Mapping Network (DPC-MN) : 처음부터 끝까지 point cloud를 설정하고, 비지도학습된 딥러닝을 이용하여 object recognition 하는 방법​​4-C. Object detection combined RGB images with Point Cloud  - Frustum PoinNets : 2D object detector로 찾을 공간을 줄이고, 찾은 공간에 대해 3D object instance segmentation을 실행  - MV3D : 높고, 강하고, 밀도있는 새의 관점에서 3D point cloud하고 전방을 투사한다.  ​ "
[논문 리뷰] A Survey of Deep Learning-Based Object Detection Methods and Datasets for Overhead Imagery ,https://blog.naver.com/ziippy/222784099136,20220622,"Received January 2, 2022, accepted January 25, 2022, date of publication February 4, 2022, date of current version February 25, 2022.Digital Object Identifier 10.1109/ACCESS.2022.3149052https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9703336​​Abstract최근 컴퓨터 비전 연구의 상당한 발전과 발전으로 드론, 비행기 및 위성의 다양한 소스에서 얻은 고해상도 오버헤드 이미지에서 다양한 물체를 보다 효과적으로 처리할 수 있다.많이 발전했지만, 여전히 어려운 대상이다.이유는, 상당한 이미지 볼륨, 일관되지 않은 이미지 해상도, 작은 크기의 객체, 매우 복잡한 배경 및 불균일한 객체 클래스 때문이다.딥 러닝 기반 object detection 에 대한 광범위한 연구는 놀라운 성능과 성공을 달성했지만, overhead 이미지의 근본적인 어려움으로 인해 여전히 낮은 감지 성능을 가지고 있다.​따라서 overhead 영상에서 고성능 물체 검출은 이러한 어려움을 극복하기 위한 활발히 연구되고 있는 분야이다.이 survey 문서는 overhead 이미지에서 최신 딥 러닝 기반 객체 감지에 대한 포괄적인 개요 및 비교 검토를 제공한다.특히, overhead 이미지에서 객체 감지 방법 중 가장 최근 추세를 다루고, 이전에 포괄적으로 조사되지 않은 overhead 데이터 세트의 도입을 밝힐 수 있다.​​1. INTRODUCTIONfocus on reviewing the recent advancements in remote sensing for satellite and aerial-imagery-based object detection.이걸 통해서 우리는 overhead imagery 에 대한 object detection 의 general overview 를 제공하고, 더 나아가 연구가 활발히 촉진되길 바란다.​그러므로, 이 서베이 논문은 multi-class dataset 에 대한 satellite 나 aerial 이미지에 대한 object detection 방식에 대해 논의하는 것을 목표로 한다.또한, 이 서베이 논문은 다양한 범위의 overhead image 들 (satellite images (EO, Electro-Optical), SAR(Synthetic Aperture Radar) 를 포함하는)에 대한 포괄적인 방법론도 다루는 걸 목표로 한다.​이 논문의 contributions 은 다음과 같이 요약할 수 있다:위성 이미지(SAR 과 EO)와 항공 이미지를 기반으로 object detection 을 하는 method 들에 대해 지난 6년간 90편 이상의 논문 리뷰를 기반으로 서베이함​우리는 6가지 주요 영역을 정의하고 overhead 이미지의 문제를 해결하기 위해 분류 체계를 구성하고 이에 따라 기존 연구를 광범위하게 분석 및 분류한다.이러한 조사에 기반하여, 우리는 최신 method 들과 dataset 들 간의 비교 연구를 제공하고, 현재의 방식의 한계와 향후 연구 방향에 대해 논의한다.​​2. APPLYING DEEP LEARNING-BASED METHODS FOR OVERHEAD IMAGERY IN CHALLENGING ENVIRONMENTS Backbone Network 은 image 로부터 feature extracting 하고Head Network 은 feature 를 이용하여 localize 하거나 classify 한다.​Backbone Network 는 CNN-based 모델이다.한편, ViT-FRCNN[18], ViT-YOLO[19], Swin Transformer[20]와 같은 트랜스포머 기반 네트워크와 self attention 메커니즘을 통합한 방법들이 최근 고성능을 보여주고 있다.하지만 new backbone 을 개발한다는 것은 massive computation 과 pre-training on large-scale data 가 필요하므로 어려운 task 이다.​이런 한계를 극복하고자 Liang 은 CBNetV2 를 제안했다. pre-trained 된 ResNet50, ResNet152, Res2Net50 을 이용하는 것이다.CBNetV2 는 여러 pre-trained idential backbone 과 lead backbone 을 composing 해서 performance 를 improving 시켰다.CBNetV2 는 CNN-based 와 transformer-based backbone 을 함께 apply 할 수 있다.​많은 methods 들이 보다 정확한 feature 를 extracting 하는 데 중점을 두고 있지만, 최근에 Dai 는 Dynamic head 라고 부르는 새로운 head network structure 를 제안했다.Dynamic head 의 input 은 backbone network 의 output data 이다.head network 는 scale, location, representation of object 로 이뤄진 3-dimensional tensor 가 input data 인 것이다.게다가 각각의 dimension 에 attention mechanism 이 적용되었다.그러므로 full self-attention mechanism 을 적용한 것에 비해 디텍션 성능이 improved 되었다.(아래 그림은 https://arxiv.org/pdf/2106.08322.pdf 에서 발췌) 그러나, 위성 및 항공 이미지의 고유한 기본 특성으로 인해 위에서 언급한 최신 기술을 overhead 이미지에 적용하는 것은 어렵다.​따라서 우리는 Faster-RCNN, YOLO, SSD 와 같은 일반적인 딥러닝 기반 방식 대신 overhead imagery domain 에 사용되는 다양한 방법에 focus 할 것이다.​이에 우리는 아래 그림 처럼  overhead imagery 관련하여 6가지 카테고리로 object detection 을 논의할 것이다. A. Efficient Detection효율성은 개체 탐지 작업의 중요한 성능 메트릭 중 하나이다.모델의 size, 이미지의 해상도, 복잡성, 크기가 증가함에 따라 최근 효율성의 중요성이 중요해지고 있다.Swin Transformer V2 나 Focal Transformer 가 제안되었다.Swin Transformer V2Swin Transformer 의 scaling uppost normalization, scaled cosine attention, log-spaced continuous position bias 과 같은 specific techniques 를 적용함high performance in object detection tasksFocal Trasnformerfocal self-attention method 를 적용해서 self-attention 의 computational overhead 를 극복함​최근 몇 년 동안 지구 관측 기술의 발전으로 인해, 엄청난 양의 고해상도 overhead 사진들이 거의 실시간으로 생성되고 있다.​따라서 효율성 연구 영역은 overhead 및 satellite 이미지 영역에서도 상당히 관심받고 있다.overhead imagery 에서의 효율적인 object detection 는 다음 그림과 같이 2가지 방식으로 제안되었다. 1) Reducing ComputationZhang et al. 이 제안한 SlimYOLOv3 은 channel pruning method 를 이용해서 모델이 더 lighter 하고 더 efficient 하다.계산 프로세스의 비용을 줄이기 위해 network slimming approach 에 영감을 받아 제안된 방법이다.original YOLOv3 에 Spatial Pyramid Pooling module 을 더했다.less informative 한 CNN 채널을 제거하여 탐지 정확도를 개선하고 parameters 의 size 를 줄임으로써 FLOPS(floating-point operations, 부동소수점연산)  을 줄였다.​high resolution 을 학습하기 위해서는 high computational cost 가 필요하다.Uzkent et al. 은 이를 줄이기 위해 강화학습(RL, Reinforcement Learning) 을 적용해서 high resolution image 의 사용을 최소화했다.​reinforcement learning 의 agent 는 저해상도 이미지가 object detect 하기에 충분한지 또는 고해상도 이미지가 필요한지 결정한다.이를 통해 high-resolution images 가 필요로 하는 개수를 줄임으로써 runtime efficiency 를 increase 시킨다.하지만, 이 방식을 low-resolution 과 high-resolution image가 쌍으로 필요하다. 이렇게 pairing 된 이미지 없이 high-resolution 이미지만을 가지고는 적용하지 못한다.​2) Reducing Search Area전체 영역이 아닌 object detection 을 위해 이미지의 검색 영역을 줄이는 것을 제안했다.​Han et al. 은 cropped images 를 생성하기 위해 Faster R-CNN 에 RLN(Region Locating Network)을 적용했다.RLN 의 구조는 Faster-RCNN 에 있는 RPN(Region Proposal Network)와 동일하다.그러나 RLN 은 RPN 과 달리 original overhead images 로부터 물체 위치가 가능한 영역을 예측할 수 있다.또한, 예측 영역의 cropped image 가 original image 의 전체 area 보다 훨씬 작기 때문에, 특정 객체를 탐지함에 있어 효율성 측면에서 상당한 개선을 보였다.​추가로 Sommer et al. 도 RLN 과 유사한 검색영역축소(SAR, Search Area Reduction) 모듈을 제안했다.이미지를 patch 단위로 잘게 자르고, 그 모듈이 각각의 이미지 패치에 대해 number of object contained 에 기반하여 score 를 예측한다. 특히 SAR 모듈이 RLN 과 달리 구별되는 특징은 2가지 있다.다양한 size 로 generate 되는 RLN 과 달리 SAR 은 specific sized image 로 generate 된다.RLN 은 region 을 찾기 위해 별도 network 를 이용하나, SAR 는 Faster R-CNN 과 통합되어 network 를 share 한다.이런 통합 방식이 inference time 을 reduce 시킨다.​또한 Yang et al 은 cluster proposal sub-network (CPNet) 과 scale estimation sub-network(ScaleNet) 과 dedicated detection network(DetecNet) 의 조합으로 된 Clustered Detection(ClusDet) network 를 제안했다.CPNet 은 feature extraction backbone network 에 연결되어 high-level feature maps 를 얻는다.feature map 정보를 기반으로 CPNet 은 입력 이미지 클러스터의 위치 및 규모를 예측한다.그런 다음 ScaleNet 은 클러스터 칩의 크기를 재조정하기 위해 개체의 크기 오프셋을 예측한다.마침내 이런 클러스터 칩에 대한 DetecNet 의 결과와 전체 이미지를 결합하여 최종 결과를 생성한다.이 방법을 통해 high runtime efficiency  를 달성한다. 또한 작은 물체에 대한 detection performance 을 improve 시켰다.​​B. Small Object Detection또한 small-sized object detection 에 대해 한계가 있다는 점도 overhead imagery 관련 또 다른 어려운 문제이다.이미지의 해상도가 떨어지면 작은 물체를 감지하는 능력도 급격히 떨어진다.최근에는 다음 그림과 같이 small object detection 에 대해 더 나은 성능을 달성하기 위해 몇 가지 방법이 제안되었다. 1) Fine-grained Model Architecturesmall-sized object detection 문제를 해결하기 위한 직관적이고 직접적인 접근 방식은, 모델 parameter 를 조정하여 source image 로부터 fine-grained feature 를 추출하는 것이다.​Sommer et al. 은 항공 이미지에서 차량 감지를 위한 딥러닝 기반 감지 방법의 효과를 입증했다.Faster R-CNN 기반으로 하며 number of layers, kernel size, anchor scale 등 파라미터들을 optimize 했다.이는 overhead imagery 에 대해 general object detection 이 적용 가능하다는 것만 보여줬을 뿐, 새로운 방법론은 제시하지 못했다.​YOLO 와 같은 one-stage object detection model 도 테스트했다.Pham et al. 은 YOLO-fine 을 제안해서 YOLOv3 에서도 인식하지 못한 8 pixel 이하 크기의 객체도 인식할 수 있게 하였다.그리드 서치를 통해 인접 객체의 식별 기능을 개선또한 small object detection 에 도움이 되지 않는 마지막 2개 convolutional block 을 제거하여 원본 YOLOv3 에 비해 parameter 수도 줄였다.​2) Multi-scale Learningparameter optimization 과 달리 multi-scale approach 방식을 제안하는 쪽도 있다.Van Etten 은 YOLO 에서 영감을 얻어 YOLT(You Only Look Twice)를 제안했다.YOLT 는 concurrently 한 multi-scale training approach 을 적용했다. (왜냐면 fine-grained model 이 high false-positive 이슈를 제공하기 때문)large object 의 결과와 small object 의 결과를 combine 해서 final object detection result 로 결정한다.​YOLT 의 영감을 받아 MOD-YOLT 가 제안되었다. MOD 는 Multi-scale Object Detection 의 약어. (아래 그림 참고) 3가지 타입의 다른 size 로 정의하고 → 이를 Multi-YOLT Network (MYN) 으로 학습시킨다.​Zhou et al. 은 Faster R-CNN 구조에 multi-scale network 를 적용하였다.왜냐하면, cnn 의 depth 는 feature level 과 연관있으므로, multi-scale network 는 multiple level of feature 를 사용할 수 있게 하기 때문이다.그러므로 multi-scale network 는 SAR imagery 에서 ship 을 찾는 것과 같은 small object 를 detect 하는데 beneficial 하다.SSD 나 RetinaNet 에 비해 mAP 가 증가하였다.​또다른 방법은 multi-scale training 을 source image 에서 crop 한 이미지에 적용하는 것이다.clustering method 또는 density map 을 기반으로 다양한 크기의 cropped 이미지는 생성된다.​Li et al 은 Density-Map 가이드로 Density-Map guided object detection Network (DMNet) 을 제안했다. density-module 에 영감을 받아 Multi-column CNN(MCNN) 이 제안되었다. cropped image 가 object detector 에 주입되고, 그 결과를 융합(fused)하여 small object 에 대한 detection 성능을 높였다.​​C. Oriented Object Detection또한 회전된 object 들이 미분류되는 경우가 있어 → 이게 곧 object detection module 의 성능을 감소시킬 수 있다.그래서 다음 그림과 같은 방식들이 제안되었다. 1) Detecting Horizontal Bounding Boxcenter point, width, height 를 사용oriented object 의 detection accuracy 를 높이는 가장 직관적인 방법은 data augmentation 이다.Cheng et al 은 data augmentation 전략과 rotation invariance 를 얻기위한 new function 을 제안했다.이는 AlexNet 을 확장한 것으로, 마지막 분류 계층을 rotation-invariant CNN(RICNN) 과 softmax 계층으로 교체하는 것이었다.RICNN 의 unique 한 손실 함수를 통해 회전 전후의 이미지에서 유사한 특징을 얻을 수 있었다.하지만 이 방법도 fine-tuning 이 필요했고, oriented bounding box 형식으로 레이블링 된 데이터세트에는 적용할 수 없었다.​2) Detecting Oriented Bounding BoxLiu et al 은 rotated region-based CNN (RR-CNN) 을 제안했다. 이는 rotated RoI (RRoI) pooling layer 를 가지고 있다.5개 tuple 로 feature 가 있다. x-axis, y-axis, width, height, rotation angle이를 통해서 더 robust 하고 정확한 feature 를 pooling 할 수 있다는 장점이 있고, two-stage detector 에 붙이기 쉽다는 장점이 있다.​Lie et al 은 이 개념을 이용해서 Faster R-CNN 실험 시 RR-CNN 을 이용했다.비록 RRoI 가 rotated feature 를 정확히 추출할 수 있지만, 이를 위한 computational cost 가 너무 많이 든다.​이 문제를 해결해보고자 Ding et al 은 RRoI leaner 와 Rotated Position Sensitive(RPS) RoI alignment module 로 구성된 RoI transformer 를 제안했다.RRoI leaner 는 Horizontal RoI로부터 transformation 을 learn 하는 걸 학습하고, RPS RoI alignment module 을 이용해서 rotation-invariant feature 를 추출하는 걸 하였다.무시할 수 있는 정도의 computational cost 증가 에도 불구하고, RoI transformer 를 base 로 한 방식은 oriented object 의 detetion 성능을 증가시켰다.​추가로 Yi et al 은 box boundary aware vectors (BBAVectors) 를 소개하며 oriented bounding box 를 detect 하고 predict 하였다.특징에서 예측된 각도 값을 사용하는 대신, BBAVectors 는 데카르트 좌표계(Cartesian coordinate system)를 사용했다.그리고 model 은 center key point 를 detect 하고, bounding box 의 position 을 지정했다.이러한 전체 model architecture 는 anchor-free 한 one-stage 한 detector 로 구현되어 모델이 더 빠르게 inference 할 수 있게 하였다.​한편 Han et al 은 single-shot alignment network (S^2-A-Net) 으로 불리는 one-stage detection module 을 제안했다.여기서 Feature Alignment Module (FAM) 과 Oriented Detection Module (ODM) 이 소개되었다.FAM 은 Anchor Refinement Network (ARN) 과 Alignment Convolution Layer (ACL) 로 구성된다.ARN 은 rotated anchors 를 생성하고, ACL 은 anchor prediction map 을 decoding 하여 oriented bounding box 를 생성하여, alignment convoluetion (AlignConv) 를 이용해 정렬된 특징을 추출한다.ODM 은 active rotating filters (ARF) 를 적용하여 orientation-sensitive 한 feature 와 orientation-invariant 한 feature 를 추출한다.이렇게 추출된 feature 를 이용하여 2개의 sub-networks 를 통과시켜서 bounding box 와 classify 를 예측한다.이는 DOTA 및 HSRC2016 데이터 세트에서 state-of-the-art 성능을 달성했으며, 이러한 데이터셋은 oriented object detection 연구 분야에 널리 활용되고 있다.​​D. Augmentation & Super Resolution향후 detection 의 성능을 높이기 위해서는 preprocessing 단계에서 data augmentation 과 super-resolution 이 적용되어야 한다.이 2가지의 서로 다른 preprocessing 기법은 다음 그림에서 볼 수 있다. 1) Image AugmentationChou et al. 은 항공 이미지에서 가오리를 탐지하기 위해 Conditional GLO (C-GLO) 라고 부르는 흥미로운 generative approach 을 제안했다.이 방식은 Generative Latent Optimization (GLO) 에서 동기부여가 된 것이다.original GLO 와 달리 C-GLO 는 선택된 이미지의 배경과 혼합된 객체를 생성한다.이렇게 생성된 이미지들로 학습된 baseline model 은 performance 측면에서 상당한 향상을 보였다.​참고로, GLO 는 simple reconstruction loss 를 이용하여 deep convolutional generator 를 학습시키는 프레임워크다.(아래 그림은 GLO 논문에서 발췌) 참고로, C-GLO 는 Mixed Bg-Fg Synthesis 를 이용하여 이미지를 생성한다. 그리고 C-GLO 의 접근 방식은 다음 그림과 같다. ​또한, Chen et al 은 AdaResampling 이라고 불리는 adaptive augmentation 방식을 적용하였다.regular augmentation method 에는 배경 및 scale 의 불일치라는 중요한 이슈가 있다.이를 극복하고자 AdaResampling 은 증간 단계에서 사전 훈련된 segmentation network 를 적용하여 segmented road map 을 생성했다.segmented road map 에서는 object 의 위치 정보를 사용한다.또한, 간단한 linear function 을 사용하여 개체 크기를 조정하는 scale factor 를 계산했다.이렇게 해서 생성된 이미지들은 re-regression module 을 가지고 있는 RRNet 이라는 hybrid detector 로 주입된다.그러면 re-regression module 은 거친(coarse) bounding box 가 있는 feature map 을 입력으로 사용하고 final bounding box 를 출력으로 예측한다. ​2) Super-Resolution또 다른 접근방식은 super-resolution 이미지를 생성하는 것이다.​Shermeye 와 Van 은 overhead imagery 에 대한 object detection 성능이 이미지의 다양한 해상도에 의해 영향받는다는 것을 분석했다.Super-Resolution Forest (SRF) 의 확장판이라고 할 수 있는 Very Deep Super-Resolution (VDSR) 이나 Random-Forest Super-Resolution(RFSR) 에서 각자의 실험을 위해 super-resolution images 를 생성했다.이러한 super-resolution 방식들로 인해 detection 성능이 향상됨을 보여줬다.​image 의 resolution 이 30cm 에서 15cm 로 증가했을 때, mAP 성능은 13% 에서 36% 로 증가했다.반대로 resolution 이 30cm 에서 120cm 로 감소하면, mAP 성능이 27% 에서 22%로 감소했다.즉, 이미지의 해상도가 detection 의 성능과 밀접한 관련이 있음을 보여주었다.​따라서 일반적인 super-resolution method 는 전체적인 object detection 성능을 향상시킬 수 있다.유사한 방향으로, Rabbi et al 은 Edge-Enhanced Super-Resolution GAN (EESRGAN) 방식을 소개했다.이는 Edge Enhanced GAN (EEGAN) 과 Enhanced Super-Resolution GAN (ESRGAN) 의 영감을 받았다.EESRGAN 은 Faster R-CNN 이나 SSD 와 같은 end-to-end network struture 로 구성된다.특히 EESRGAN 은 풍부한 edge information 으로 super-resolution 이미지를 생성하였으며, 기본 탐지 네트워크는 (Faster R-CNN 또는 SSD) 이 이미지를 이용하여 향상된 정확도를 달성했다.​​E. Multimodal Object Detection또 하나의 도전적인 연구 영역은 다양한 해상도, view points, data type 과 같은 멀티모달 기반 object detection 이다.여기서 우리는 object detection 을 위한 multimodal data 를 사용하는 방법에 대해 설명하겠다.다시말해 더 robust 하고 정확한 detection 성능을 얻기 위해서는, 다양한 타입의 data 를 사용할 수 있어야 한다. 첫번째로, 기본적인 접근 방식은 각각의 센서에서 얻은 서로 다른 해상도의 이미지를 사용하는 것이다.Cao et al. 은 low-resolution 위성 이미지와 high-resolution 항공 이미지를 동시에 사용하는 detection framework 를 소개했다.결합된 사전 학습을 적용하여 detection framework 를 위해 augmented 된 feature 를 얻은 다음에, E-SVM 을 사용하여 다양한 이미지 해상도에서 더 강력한 모델을 만들었다.multi-scale 학습 이나 super-resolution 이미지 생성하는 방식과 달리, 이 기법은 서로 다른 도메인에서 서로 다른 해상도별 데이터를 획득했다. ​multi-해상도 정보를 얻는 것처럼 Wegner et al. 은 multiple view (street 나 overhead view) 로부터 정보를 얻었다.Faster R-CNN 은 각 street view 에서 객체를 감지하기 위한 기본 감지 모델로 활용되었다. 그 결과와 geographic coordination 을 결합하여 multi-view proposal 점수를 계산하였고, 이게 입력 영역에 대한 최종 detection 결과이다.​이렇게 제안된 모델은 simple overhead images 에 대해 Faster R-CNN 보다 더 높은 mAP 점수를 보였다.​다양한 유형의 이미지를 활용한 접근 방식과 달리 Wu et al. 은 Nuisance Disentangled Feature Transform(NDFT) 를 제안했다. 이는 domain rebust 한 feature 를 얻기 위해 이미지와 메타-데이터를 함께 사용했다.제안한 이 방식은 domain-invariant 한 feature 를 학습하여 다양한 도메인에서 robust 한 모델을 학습할 수 있도록 하였다. ​F. Imbalanced Objects Detection오브젝트의 불균형 역시 overhead imagery 연구에 있어 도전적인 영역이다.RetinaNet 에서 소개된 focal loss 에 대한 더 많은 연구로 인해 performance 가 향상되었다. 특히 Yan et al. 은 Faster R-CNN 모델의 region proposal network (RPN) 에 focal loss 를 적용하는 것과, classifier module 에도 focal loss 를 적용하는 double focal loss CNN(DFL-CNN) 을 제안하였다.cross entropy loss 대신에 focal loss 를 적용함으로써 RPN 은 RoI 를 밝혀냄에 있어 class imbalance 문제를 고려하고, classifier 가 학습과정에서 hard negative data 를 다룰 수 있게 하였다. 추가적으로, shallow layer 로부터 deeper layer 로의 skip connection 도 제안하였다.​(DFL-CNN 에 대한 구조. 논문에서 발췌) ​이 방식은 Faster R-CNN 모델 기반으로 ITCVD dataset 에 대해 detection performance 를 향상시켰음을 보여줬다.ITCVD dataset 은 large-scale 의 well annotated 된 vehicle detection 데이터셋이다.​Sergievskiy 와 Ponamarev 는 original focal loss 를 수정한 reduced focal loss 로 데이터 불균형 이슈를 해결했다.positive sample 의 최소 weights 값을 유지하여 의도하지 않은 recall 의 drop 을 방지하기 위해 임계값이 적용되었다.이 방식은 xView dataset 에 대한 실험을 하였고 DIUx xView 2018 Detection Challenge 에서 1위를 차지하였다.​이와 달리 Zhang et al. 은 Difficult Region Estimation Network (DREN) 을 제안했다.DREN 은  difficult-to-detect region 에 대한 cropped image 를 생성하도록 훈련되었으며, original image 와 cropped image 를 함께 detector 로 전달한다. 이 network 은 Libra R-CNN 의 balanced L1 loss 를 사용한다. 그런 다음 balanced L1 loss 는 high loss 값을 가진 샘플에 의해 기울기가 억제된다.outlier 로부터 최대 gradient 로 자르기 위해 정확한 샘플에서 balanced regress 를 수행한다.​​​3. DATASETS이번 섹션에서는 이미지 센서 소스를 기반으로 가장 인기있고 공개적으로 사용 가능한 satellite imagery 데이터 셋을 설명한다.​​글이 너무 길어서~다음 블로그 게시글에 계속해서 작성하겠다. "
"Object Detection with ImageAI YOLOV3 and Calculate count of cars and Persons with Colab Google, Yolo ",https://blog.naver.com/aitutor21/223107291293,20230520,"Object Detection with ImageAI YOLOV3 and Calculate count of cars and Persons with Colab Google, Yolo Object Detection in Google Colab [Full Tutorial], Object Tracking with YOLOv8: Vehicles Tracking,  http://aitutor21.com/bbs/board.php?bo_table=aistudy&wr_id=393http://aitutor21.com/bbs/board.php?bo_table=aistudy&wr_id=393 Object Detection with ImageAI YOLOV3 and Calculate count of cars and Persons with Colab Google, Yolo Object Detection in Google Colab [Full Tutorial], ﻿Object Tracking with YOLOv8: Vehicles Tracking,https://cafe.naver.com/2018startup/2472Object Detection with ImageAI YOLOV3 and Calculate count of cars and Persons with Colab Google https://www.youtube.com/watch?v=CA21SLbMx3IYolo Object Detection i…aitutor21.com ​ "
"Object Detection in 20 Years - 03, Object Detection Datasets and Metrics ",https://blog.naver.com/tory0405/222844508351,20220810,"[논문 원본] 첨부파일1905.05055.pdf파일 다운로드  DataSet​더 작은 bias을 이용하여 더 큰 dataset을 구축하는 것은 고급 컴퓨터 비전 알고리즘을 개발하는데 매우 중요하다. PASCAL VOC Challenges , ImageNet Large Scale Visual Recognition Challenge, MS-COCO Detection Challenge 등에서 사용된 dataset은 그림에서 볼 수 있듯이 정확도에 많은 영향을 끼치고 있음을 보여 주고 있다.     왼쪽 그림은 2008년부터 2018년까지 VOC07, VOC12 및 MS-COCO dataset에 대한 탐지 정확도의 개선 사항을 보여 주고 있고 오른쪽 그림은 실제 사용되는  open image dataset 샘플이다. ​Pascal VOCPASCAL Visual Object Classes (VOC) Challenges는 2005년부터 2012ㄴ녀까지 개최되었고 초기 컴퓨터 비전 커뮤니티에서 가장 중요한 challenge였다. Pascal VOC에서는 image classification , object detection,  semantic segmentation, action detection 등이 포함되어 있다. Pascal VOC의 두 가지 버전은 대부분 object detection이었는데 전자는 이미지 12000개 이상이 포함된 5k tr로 구성되어 있고 후자의 경우 27000개 이상의 11k tr로 구성되어 있다.  두 가지 dataset 모두 일상생활에서 흔히 볼 수 있는 20가지 ((사람:  사람,  동물:  새,  고양이,  소,  개,  말,  양,  차량:  비행기,  자전거,  보트,  버스,  자동차,  오토바이,  기차,  실내:  병,  의자,  식탁,  화분,  소파,  TV/모니터) class의 객체가 주석 처리되어 있다. 최근 몇 년 동안  ILSVRC 와  MS-COCO와 같은 더 큰 dataset가 출시되면서 Pascal VOC는 점차 유행에서 벗어났고 대부분 새로운 감지에 대한 테스트 베드로 사용되고 있다. ​ ILSVRC ImageNet Large Scale Visual Recognition Challenge (ILSVRC)는 2010년부터 2017년까지 ImageNet의 200가지 class가 포함된 dataset을 이용하여 매년 개최되면서 일반 객체 감지 분야에 기술 향상에 크게 기여하였다. ​MS-COCO2015년부터 시작하여 현재 사용 가능한 가장 까다로운 물체 감지 dataset로 COCO ‑17 경우 80개 범주와 164000개 이미지에  897000개의 주석이 포함되어 있다. MS-COCO의 가장 큰 발전은 bounding box annotation을 제외하고 각 개체 인스턴스별  segmentation을 사용하여 추가 레이블을 지정하도록 하여 정확한 위치 지점을 계산할 수 있다는 것이다. 또한 VOC나 ILSVRC보다 더 작은 객체(영역의 1% 미만)를 탐지할 수 있어 객체 감지 커뮤 미티에서 사실상 표준으로 자리 잡게 되었다. ​Open Images2018년 MS-COCO 이어 전례 없는 OID(Open Image Detection) Challenge가 개최되었는데  2가지 영역으로 나눌 수 있다. 첫 번째는 standard object detection이고 두 번째는  특정 관계에서 쌍을 이루는 물체를 감지하는 시각적 관계 감지로  객체 감지 작업의 경우 dataset은  600개의 객체 범주로 분류되고  15440000의 주석이 달린 bounding box를 가진 1910000 이미지로 구성된다.​Datasets of Other Detection Tasks일반적인 물체 감지 외에도 지난 20년 동안  prosperity of detection,  face detection, text detection, traffic sign/light detection, remote sensing target detection 을 위한 Challenge가 개최되었고 아래 표가 대중적인 객체 감지를 위한 dataset이라고 할 수 있다.   Metrics​물체 감지기의 효율성을 평가하기 위한 도구로 사용되는데 초기에는 감지기 성능 평가에 널리 수용되는 평가 기준이 존재하지 않았다. 예를 들어 pedestrian detection에 대해 초창기에는 miss rate 또는 false positives per-window (FPPW)가 일반적으로 metric으로 사용되었다. 그러나 FPPW는 특정한 경우에 전체 이미지에 대한 성능을 예측하지 못하는 경우가 있었기에 2009년에 FPPW에서 FPPI로 평가하게 되었다. ​최근 몇 년 동안 가장 많이 사용되는 물체 감지 평가는  VOC2007에 포함된 “Average Precision (AP)”이다. AP는 다양한 리콜에서 average detection precision으로 정의되었고 모든 객체 범주에 대한 성능을 비교하기 위해 모든 객체 범주에 걸쳐 평균화된 평균 AP(mAP)가 일반적으로 성능의 최종 메트릭으로 사용되었다. ​2014년 이후 MS-COCO dataset의 인기로 인해  bounding box  위치의 정확성에 더 많은 관심을 기울이기 시작했고 고정된 IoU 임곗값을 사용하는 대신 MS-COCO AP는 0.5에서 0.95 사이의 여러 IoU 임곗값에 걸쳐 평균화하였는데 이러한 변화가 보다 정확한 객체 현지화를 촉진하는데 이바지했다. ​최근에는 Open Image dataset에서는  group-of boxes과  non-exhaustive image-level category hierarchies 을 고려하여  “localization recall precision”과 같은 대안적인 metric을 제안하기도 하였다. ​​ "
Monocular 3D object detection에서의 방향 추정 ,https://blog.naver.com/revan2426/222966010338,20221226,"모노 카메라에서 방향을 추정하려면 두 가지 개념을 숙지해야함​ Egocentric vs Allocentric orientation Egocentric은 말 그대로 카메라(자신)의 위치에 따라서 다른 모든 객체의 위치가 결정되는 것이고(Global orientation)Allocentric은 다른 객체의 위치에 따라 한 객체의 위치가 결정된다. (Local orientation)​Egocentric: 소화기가 북쪽에 멈춰있다. 자전거가 오른쪽으로 이동하고있다 등등Allocentric: 기준물체가 소화기라면, 자전거는 소화기의 왼쪽에서 오른쪽으로 이동하고 있다. 자동차는 소화기의 아래쪽으로 이동한다. 등등​egocentric orientation을 바꾼다는 것 = 나의 시점을 바꾼다 내가 다른곳으로 이동한다allocentric orientation을 바꾼다는 것 = 고정된 view에서 지켜볼 기준 물체를 바꾼다. coordinate이 카메라의 ray와 align됨​ (a)에서 차의 global orientation은 고정적으로 오른쪽이다. 그러나, 움직일때마다 local orientation과 appearance가 바뀐다.(b)에서 차의 global orientation은 계속 바뀌지만, camera coordinate에서의 local orientation과 appearance는 일정하다.​즉, monocular image에서의 object의 appearance는 local orientation에 따라 달라진다. 따라서 우리는 object의 appearance에 따라서 local orientation을 예측해볼 수 있다.Global orientation은 전반적인 배경 context를 통해 예측할 수 있다. 위 사진처럼, local orientation은 객체 patch로만 예측할 수 있지만, global orientation은 도로의 lane이 뻗은 정도에 따라 예측해 볼 수 있다.​​ KITTI에서는 3D orientation(roll, pitch, yaw) 중에서 roll과 pitch는 0으로 가정하고, 오직 yaw로만 방향을 정의한다.(자동차는 땅에 붙어있기 때문)​​Local yaw를 Gloal yaw로 변환하려면 어떻게 할까?​ Global orientation : thetaLocal orientation : theta_l​ray direction인 theta_ray는 box position과 camera intrinsics를 통해 예측할 수 있다. box의 위치는 4가지 종류 중에 선택할 수 있다.Detector boxes: 박스 중심Amodal boxes: occluded/truncated된 부분까지 표현한 박스의 중심3D b-box를 image에 projection했을떄의 박스의 중심2D b-box의 bottom center(차가 땅에 붙어있다고 가정했을떄의 높이를 가짐)​KITTI에서의 alpha는 object의 observation angle, 즉 local orientation을 말하고, rotation_y는 카메라 coordinate에서의 y축 기준 회전정도, 즉 global orientation을 말한다.​​  출처​https://towardsdatascience.com/orientation-estimation-in-monocular-3d-object-detection-f850ace91411https://www.nmr.mgh.harvard.edu/mkozhevnlab/?page_id=308 Allocentric vs. Egocentric Spatial ProcessingAllocentric vs. Egocentric Spatial Processing Our research on allocentric-egocentric spatial processing includes three main directions: Development of allocentric and egocentric spatial assessments Spatial Navigation and Individual Differences in Environmental Representations Spatial Updating This...www.nmr.mgh.harvard.edu ​ "
[논문리뷰]Class-Agnostic Object Detection (WACV 2021) ,https://blog.naver.com/sounghyn105/222970818973,20221231,"https://openaccess.thecvf.com/content/WACV2021/papers/Jaiswal_Class-Agnostic_Object_Detection_WACV_2021_paper.pdf​Class agnostic upstream 패키지 : two-stage / one-stage​Adversarial Object-type Discriminator -> object type까지 구별할 수 있음 .test할 때에는 3번 discriminator를 빼주었다. 결국에는 class를 구분해주는 agnostic 모델이기 때문이다. 3번을 통해 class specific 모듈로 training을 진행한 이유는 다른 타입의 물체를 명확히 구분해 주는 것을 학습시킨다.​Update strategy : Discriminator와 다른 모듈의 업데이트를 번갈아가면서 함.Discriminator 모듈 학습시 : Categorical Cross-entropy -> object의 종류까지 specific하게 구분한다.Upstream 모듈 학습시 : Binary cross entropy(object인가 background인가) + smooth l1 + negative entropy(⍺ entopy)negative entropy는 upstream 모듈 학습시에 추가된 loss로, discriminator에서 object type까지 구분하여 학습시키는 효과를 감쇄하기 위해서 추가하였다고 한다. ​negative entropy란? : 어떤 분포랑 평균과 분산이 같은 정규분포와의 KL Divergence,KL(P||N)이라고 표현해도 될 것같다. 항상 양수이다. https://en.wikipedia.org/wiki/Negentropy Negentropy - WikipediaNegentropy From Wikipedia, the free encyclopedia Not to be confused with Negative entropy . ""Syntropy"" redirects here. For other uses, see Syntropy (software) . In information theory and statistics , negentropy is used as a measure of distance to normality. The concept and phrase "" negative entropy...en.wikipedia.org Finetuning:기존의 object detection module인 Faster R-CNN과 SSD을 파인튜닝하여 upstream module을 만든다. 본래는 multi class를 classification하는 모듈이었는데, 이것을 파인튜닝을 통해 binary classification으로 바꾸었다.​Evaluation실험을 통해 주요하게 봐야할 것은 아무래도 finetuning을 한 class agnostic 모델의 성능과 이 논문의 main contribution이라고 할 수 있는 discriminator 유무의 따른 성능 변화이다. Metric은 Average Recall (AR)을 사용하였다. AR@k는 k개의 box에 대한 AR을 의미한다.실험에서는 VOC dataset을 사용하였고, 20개의 classes중 17개의 seen class와 3개의 unseen class로 구분하였다. 이때 3개의 unseen objects는 SSD 모델로부터 얻은 f1 score를 토대로 easy medium hard class로 나누었다.또한 COCO dataset은 object의 사이즈에 따라 small medium large로 나누었다. 실험 결과 adv 모듈을 추가한 모델이 AR- MEDIUM을 제외하고 가장 좋은 성능을 보여주었다. 또한, adv 모듈이 없고 파인 튜닝을 진행한 모델도 baseline에 비해 좋은 결과를 얻었다. 개인 적으로는 discriminator가 있는 모델과 없는 모델의 차이가 그 .05 안팎으로 그닥 차이가 안난다는 것이 아쉬웠다. "
객체 인식(Object detection) 알고리즘 : 합성곱 신경망(CNN)과 YOLO(You Only Look Once) ,https://blog.naver.com/whyplay/222978388090,20230108,"​■ 합성곱 신경망(CNN, Convolutional Neural Network)생명체가 물체를 인식하는 과정을 알아내는 것은 인류의 오랜 관심사였습니다. 그것은 1959년 데이빗 허블(David H. Hubel)과 토르스튼 위즐(Torsten N. Wiesel)의 고양이 실험을 통해 동물의 시각 피질은 여러 개로 분해된 선형 빛에 자극을 받는다는 주장으로부터 심화되었습니다. 이후 1980년, 쿠니히코 후쿠시마(Kunihiko Fukushima)는 객체의 특징을 추출하여 단순한 시각 패턴으로 인식하는 네오코그니트론(Neocognitron)모형을 제안합니다. 이것이 현대 딥러닝 알고리즘인 합성곱 신경망(CNN, Convolutional Neural Network)의 시초입니다.​ 사진1. 합성곱 신경망(CNN, Convolutional Neural Network)의 구조© MathWorks사람들은 관심 있는 것만 봅니다. 예를 들어 문서를 읽을 때 모든 정보를 이해하려면  오랜 시간이 걸리고, 도리어 자신이 원했던 정보를 잊어버릴 수 있기 때문입니다. 합성곱 신경망(CNN)도 마찬가지입니다. 컴퓨터에 사진 정보를 입력하면 하나부터 열까지의 모든 픽셀을 저장하는 것이 아니라, 사진이 가지는 특징만을 추출하여 학습합니다. 이것이 사진1의 특징 추출(Feature learning/extraction)입니다. 이 과정을 반복하여 약간의 오차를 허용하고 학습 모델이 과적합(Overfitting) 되는 것을 방지합니다. 그렇지 않으면 사용자가 컴퓨터에 다른 자동차 사진을 보여주었을 때, 차 문이 열려있다는 이유로 융통성 없게 자동차가 아니라고 판단할 수 있기 때문입니다.​특징 추출 덕분에, 컴퓨터가 수행해야 할 연산이 줄었습니다. 이제 완전연결층(Fully connected layer)으로 구성된 분류(Classification) 단계에서 해당 특징이 어떤 객체를 의미하는지 파악합니다. 이것은  인간, 강아지, 고양이 등의 일반적인 객체가 될 수 있습니다.​​■ YOLO(You Only Look Once)시각 지능은 두 번의 과정을 거칩니다. 첫째, 객체의 위치를 인식하고(Localization) 둘째, 그 객체가 무엇인지 분류(Classification)하는 것입니다. CNN 네트워크 계열은 두 과정을 차례로 수행하기 때문에 연산 속력이 느립니다. 따라서 이에 대한 대안으로 개발된 알고리즘이 YOLO(You Only Look Once)입니다.​ 그림2. YOLO(You Only Look Once)의 객체 인식 과정© Joseph Redmon​YOLO 알고리즘은 인식과 분류를 동시에 수행합니다. 인간이 사진을 한 번만 보고 물체를 인식하는 것처럼, YOLO도 과정을 단순화하여 실시간 인식을 가능케 한 것입니다. 그림2처럼 사진을 S X S 개의 격자로 나눈 후 객체가 있을 가능성이 큰 셀을 찾으면서(Bounding boxes + confidence) 객체의 종류를 예상(Class probability)합니다. 마지막으로 신뢰도가 가장 높은 결과를 출력합니다.​​■ YOLO(You Only Look Once) 실습Pytorch 기반의 YOLO 5버전을 실습하겠습니다. 제가 다니는 설계 회사에서 쫓겨나면 개발자로 일해도 괜찮을 것 같습니다. GitHub - ultralytics/yolov5: YOLOv5 🚀 in PyTorch > ONNX > CoreML > TFLiteYOLOv5 🚀 in PyTorch > ONNX > CoreML > TFLite. Contribute to ultralytics/yolov5 development by creating an account on GitHub.github.com 윈도우 환경에서  구동이 가능하나, 리눅스(Linux) 환경에서 테스트하는 것을 추천드립니다. 저는 우분투(Ubuntu) 20.04.4 LTS 버전을 사용했습니다. 더불어, GPU 아키텍처인 CUDA와 심층 신경망 가속기인 cuDNN(CUDA Deep Neural Network)를 설치하여 더 빠른 연산 속도를 기대할 수 있습니다. 저는 엔비디아 MX150(노트북) GPU를 사용하여 CUDA 10.2.0 버전과 cuDNN 8.2.1 버전을 사용했습니다. git clone https://github.com/ultralytics/yolov5 △ 첫째, YOLOv5를 다운로드합니다. cd yolov5 둘째, 생성된 YOLOv5 폴더에 진입합니다. pip install -r requirements.txt 셋째, YOLOv5를 실행하기 위한 기본 라이브러리를 설치합니다. python3 detect.py --weights yolov5s.pt --source 0 마지막으로, 파이썬으로 detect.py를 실행합니다. --weights는 학습 모델을, --source는 분석할 영상 매체를 선택하는 옵션입니다. '0'이 노트북에 연결된 웹캠을 의미합니다. 학습 모델에 관련해서는 앞서 첨부된 링크의 'Tips for Best Training Results' 중 'Model Selection'문서를 참고하십시오. 그림3. YOLOv5 구동 모습(3~5Books, 1 Person, 1 Cell phone 인식됨)화질을 낮추니 제가 잘 생겨졌습니다. 아무튼, YOLO 알고리즘은 저와(1 Person) 스마트폰(1 Cell Phone), 그리고 책꽂이의 책들(3~5 Books)을 인식했습니다. 한 프레임 당 연산 시간은 0.027초로써 실시간 분석이 가능한 수준입니다.  이로써 자동차의 자율주행, 건설현장의 안전관리 등에 활용되고 있습니다. 어쩌면, 눈이 달린 로봇을 만드는 엔지니어라면 누구나 열광할 기술일 겁니다. 본 본문을 통해 합성곱 신경망(CNN)과 YOLO(You Only Look Once) 알고리즘을 이해하였고, YOLO 5버전 실습을 수행하였습니다.​[참고문헌]1. Joseph Redmon, Santosh Divvala, Ross Girshick, Ali Farhadi. 2016. ​You Only Look Once: Unified, Real-Time Object Detection. https://pjreddie.com/darknet/yolo/2.Gu Jiuxiang, Wang Zhenhua, Kuen Jason, Ma Lianyang, Shahroudy Amir, Shuai Bing, Liu Ting, Wang Xingxing, Wang Gang, Cai Jianfei, Chen Tsuhan. 2018. Recent advances in convolutional neural networks.​ Pattern Recognition. 77. pp.354-377.3.K. Fukushima, S. Miyake. 1982. Neocognitron: a selforganizing neural network model for a mechanism of visual pattern recognition. Competition and Cooperation in Neural Nets. pp.267–285.4. 최미형. 2021. 딥러닝 기반의 객체 인식 기술을 이용한 보도 블럭 파손등급 검출 및 인식. 석사 학위논문. 부산대학교 대학원 ICT융합학과. 부산.​[참고문서]1. MathWorks. Convolutional Neural Network. https://www.mathworks.com/discovery/convolutional-neural-network-matlab.html?s_tid=srchtitle_Convolutional%20Neural%20Network_12. 고양이 미로. 2021. CNN(Convolutional Neural Network) 설명. https://rubber-tree.tistory.com/entry/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EB%AA%A8%EB%8D%B8-CNN-Convolutional-Neural-Network-%EC%84%A4%EB%AA%85  ​ "
Object detection(2) _Two-stage 2D object detectors: R-CNN[CVPR'14] ,https://blog.naver.com/summa911/222987794591,20230118,"why? R-CNNselective searching 으로 적절한 box를 proposal 했다면, box를 분석해서 어떤 object인지 classification해야하고, naive한 경계를 가진 box를 refining해주어야 한다.R-CNN 모델은 object classification과 bounding box fine tuning 을 잘 수행한다.but 느리다는 점이 단점.​ Step 1 ) Region(Box) Proposal stageSelective search 방식으로 최적화된 parameter를 통해서 2000개 정도의 bounding box를 선택한다.Selective search방법을 이용하면, Sliding window방법을 이용했을 때보다 훨씬 적은 개수의 bounding box를 선택할 수 있다.이 때, 선택된 bounding box의 크기는 제각각이기 때문에  CNN방법을 이용해서 classification 하기 부적절하다.​Step2) Re-size(wrapping) R-CNN은 CNN  architecture 중에서 AlexNet을 이용했다.AlexNet의 input size는 227*227*3이었기 때문에, R-CNN은 227*227의 input size값을 가져야 했다.임의로 잘린 bounding box들을 227*227의 크기를 가지도록 되도록  re-sizing해주어야 했다.​사실, Convolutional layer나 pooling layer자체는 filter size, stride, padding만 hyperparameter로 지정해주면 되기 때문에 input size가 상관이 없다.하지만 CNN의 마지막 layer가 fully connected layer이기 때문에 고정된 사이즈의 Input 벡터가 필요하다.​Step3) 전이학습(Transfre Learning) for CNNAlexNet은 classification을 위해 고안되었지, object detection을 위해서 고안되지는 않았다.즉, 배경과 객체가 함께 있는 상태로 학습하는 방법이지 객체로 가득찬 box를 찾아서 학습시키는 CNN방법은 아니다. 기존 데이터로 AlexNet을 object detection 용도로 사용하기에는 훈련 데이터가 너무 적어서, 분류학습을 위해 만들어진 데이터셋인 ImageNet을 이용해서 classification을 잘 하도록 pretraining 시킨다.이후, AlexNet을 전이학습(마지막 layer만 변형)해서 R-CNN에 이용하였다.  AlexNet과 20개의 분류 class를 가진 PASCAL을 이용해서 전이학습을 고안한다.1000가지로 분류되던 softmax classifier를 21개로 분류되는 softmax classifier로 바꿔준다.(20개의 객체 분류 class에 배경만 존재하는 1개의 class가 합해져서 21개이다.)마지막으로 VoC(VoC Pascal) data set을 이용해서 Fine tune 해준다.​그런데,총 bounding box 중, positive한 bounding box의 갯수가 현저히 적을 수 있다.예를 들어, bounding box는 2000개 정도 생겼을 때, 그 중에 object 에 맞는 box는 2개 남짓 일 것이다.object 에 fitting한 bounding box는 object 갯수만큼 정해지기 때문이다.이렇듯 positive한 데이터가 너무 작아서 모델이 overfitting되는 것을 막기위해 IoU(Intersection over Union)라는 값을 도입하였다. IoU는 ""교집합 영역/합집합 영역"" 이다. 출처: pyimagesearchIoU값이 0.5보다 클 때 positive 라고 분류해준다면, positive data가 너무 작아서 생기는 부작용 해소를 위한 방법이다. 해당 방법으로 positive한 값을 분류 해준다면, positive value가 기존 방법보다 30배 이상 많아지고, data 부족으로 인해 발생되는 overfitting을 막을 수 있다.​또,learning rate를 0.001정도로 작게 해준다.AlexNet을 학습시킬 때는 lr =0.01정도로 크게 해줘서 값을 빠르게 LOSS함수의 최솟값을 찾을 수 있도록 해주었었다. 하지만 R-CNN에서 bounding box를 학습시킬 때에는 파라미터 값이 이미 최적화 근처의 값을 가지기 때문에 learning rate을 크게 해주면 핑퐁현상이 일어나고 오히려 정확히 찾기 힘들어진다.​또,Mini-batch gradient decent를 해준다.논문에서는 128개의 bounding box 중에, 32개는 positive sample(valid object), 96개는 background만 존재하는 sample로 구성해주었다. IoU로 positive sample의 양을 늘이고도 배치의 구성에 background sample의 수를 더 많이 구성하는 이유는, 배경을 배경으로 정확히 파악해 내는것도 object를 구별하는 것 만큼이나 중요하기 때문이다. 보통 object보다 background가 데이터가 많다. 이렇듯 모델의 성능을 최적화하기 위해서는 positive sample과  background 샘플을 어떤 비율로 batch구성을 해줘야 할 지 고민을 해야 한다.​ step4) SVM(Support Vector Machine) classificationsoftmax함수로 최종 분류 하지 않고, SVM classification을 진행했다.SVM은 20개의 최종 class별로 각각 따로 training을 해주었다.2000개의 bounding box 각각을 전부 20개의 SVM을 트레이닝 시켜주었기 때문에, 2000*20개의 최종 분류된score가 생긴다. ​SVM training최종 분류 함수로 SVM을 쓴 이유는, 모델의 accuracy를 올리고자 함이다. CNN 과정까지는 기존 AlexNet과 동일하게 진행하되, 마지막 FC를 통과한 feature vector를 SVM의 input으로 사용했다. ​feature vector는 proposal box마다 CNN을 거쳐서 만들어 졌고, 그것을 SVM을 학습시키기 위해서 따로 저장해두어야 하기 때문에 disk용량을 많이 차지한다. 이 부분이 R-CNN의 가장 큰 단점이다. SVM은 학습시켜야 할 파라미터 수가 얼마 안되는 머신러닝 기법이기 때문에 과적합에 대한 위험성이 낮다.그래서 deep learning 일 때와 다르게, 데이터가 작다고 과적합을 걱정하지 않아도 된다. 그래서 거의 확실하게 object라고 판단이 들 때만 즉 IoU가 1일 때만, positive값을 가지게 하고 나머지는 negative값을 가지도록 처리한다.이 과정에서 positive라고 분류 된 데이터의 갯수가 30배 줄어들게 된다. 또, 확실하게 배경이라고 판단이 들 때에만 negative라고 판단해주기 위해서 IoU가 0.3 이하 일때만, negative라고 판단하게 했다. 나머지 값들은 SVM을 헷갈리게하는 애매한 값들이기 때문에 무시해준다.이렇게 트레이닝을 했더니 최종 분류 함수로 softmax를 썼을 때 보다 좀 더 좋은 성능을 가진 모델로 학습이 되었다. step5) Bounding Box Regreassion(BBReg) by linear regressionclassification이 끝난 이후에, object에 맞게 bounding box를 좀 더 최적화 시키기 위한 과정.Bounding Box Regression 은 줄여서 BBReg라고도 하는데, 개별 class마다 학습을 시킨다.  classification이 끝난 최종 score를 20개의 BBReg를 학습시키기 위한 input data로 사용한다.​BBReg trainingCNN feature vector를 SVM을 사용해서 분류 한 output을 input으로 사용한다.IoU>0.6 으로 ground-truth box 주변에 있다고 여겨지는 데이터만 BBReg training에 사용하였다.또,CNN feature vector(before FC layers)도 input data로 사용한다.FC를 통과하면, location정보가 다 날아가버린다. SVM을 트레이닝 할 때는 FC를 통과한 데이터를 사용했었던 반면, BBReg 트레이닝에는 위치 정보가 남아있어야 하기 때문에,fully connected layer를 통과하기 전 정보를 저정해뒀다가 사용한다. Step 6) Non-max suppression수많은 BB 후보 중에서 Best box로 추려가는 과정이다.max값을 제외한 box값들, 특히 best box랑 많이 겹쳐있는 박스는 모두 지운다.Non-max suppression 은 box별로 처리 하는 것이 아니라 class별로 처리를 한다.첫째, 가장 높은 score를 가진 box를 선택한다. 둘째, 첫번째에서 선택한 box와 IoU가 높은 box를 제거한다. ​R-CNN 모델은 처음 성공적으로 object detection을 수행했다는 의의가 있다.​R-CNN의 한계  CNN은 이미지 하나당 한번만 모델을 통과하면 최적화가 되었지만, R-CNN은 연산이 많아서 아주 느리게 수행된다. R-CNN은 이미지 하나당, CNN,SVM,BBReg를 2000번씩 트레이닝 시켜야 한다. Selective search 등이 CPU를 통해서 수행되어야 해서, GPU를 활용 못한다. CNN을 사용하기 위해서 warping하는 과정에서 데이터 위치 정보가 왜곡된다. ​​​​​​​[출처] R. Girshick et al., ""Rich feature hierarchies for accurate object detection and semantic segmentation.""https://openaccess.thecvf.com/content_cvpr_2014/html/Girshick_Rich_Feature_Hierarchies_2014_CVPR_paper.html CVPR 2014 Open Access RepositoryCVPR 2014 open access These CVPR 2014 papers are the Open Access versions, provided by the Computer Vision Foundation. Except for the watermark, they are identical to the accepted versions; the final published version of the proceedings is available on IEEE Xplore. This material is presented to ensu...openaccess.thecvf.com [출처] https://youtu.be/yGZVibqaPJY "
윈도우에 Yolov5 설치하고 다양한 영상소스를 쉽고 간단하게 객체검출(Object Detection) 하기 ,https://blog.naver.com/jcosmoss/222977168681,20230106,"이번에는 Yolov5를 설치하고 실행해 보자. 이전 버전보다 더 빠르고 사용이 간편하다고 한다. 빠른 성능을 위해서 CUDA를 설치해서 사용하는 것 같은데 내 똥컴은 그런거 모른다. 패스 하자.​1. Github 사이트 링크 GitHub - ultralytics/yolov5: YOLOv5 🚀 in PyTorch > ONNX > CoreML > TFLitezip파일로 다운로드 하고 압축을 풀어 준다. ​2. 파이썬 3.10설치최신 3.11말고 3.10을 설치 하자. 버전이 아직 지원 하지 않는다는 것 같다.설치시 path를 체크하고 설치 진행​3. 필요한 패키지 일괄 설치하기앞에서 다운로드 받은 폴더에 들어 가면 requirments.txt 파일이 있다. 실행에 필요한 패키지들을 자동으로 설치해 준다. pip install -r requirements.txt​4. 웹캠등 다양한 소스로 실행 테스트--source 와 --weights 옵션을 이용해서 웹캡, 이미지, 유튜브, 스트리밍 영상들 다양한 소소를 간단히 이용 가능하다. weights 옵션으로 모델의 크기나 디테일을 선택하여 실행 시킬 수도 있다. ​python detect.py --source 0 이렇게 실행하면 웹캠을 영상 소스로 객체 감지를 하는 것을 확인 할 수 있다.​python .\detect.py --weights yolov5m.pt --source https://youtu.be/KP5xnC8V5ew​ 유튜브 영상을 소스로 사용하기 가능하다.​python .\detect.py --source http://esp32-cam-ip:81/stream 마찬가지로 ESP32-Cam 에서 스트리밍 되는 영상도 감지가 가능하다. 앞서 해 봤던 Yolov3보다 훨씬 간편하고 속도도 빠른듯 하다. ​다음에는 내가 원하는 이미지를 트레이닝 시켜서 적용하는 방법에 대해 알아 보기로 하자. ​​​ "
Object Detection ,https://blog.naver.com/hasbro425/222900136620,20221014,"Object Detection ApplicationAutonomous drivingOptical Character Recognition​Two-stage detectorGradient-based detector Selective searchOver-segmentationIteratively merging similar regionsExtracting candidate boxes from all remaining segmentations ​​R-CNNDirectly leverage image classification networks for object detection ​Fast R-CNNRecycle a pre-computed feature for multiple object detection1. Conv. featrue map from the original image2. RoI feature extraction from the feature map through RoI pooling3. Class and box prediction for each RoI  ​Faster R-CNNEnd-to-end object detection by neural region proposalIntersection over Union Anchor boxesA set of pre-defined bounding boxesIoU with GT > 0.7 : positive sampleIoU with GT < 0.3 : negative sample ​Region Proposal Network (RPN) Non-Maximum Suppression (MMS) ​Summary of the R-CNN family ​​Single-stage detectorOne-stage vs. two-stageNo expilcit RoI pooling ​You only look once (YOLO) architecture ​​Single Shot MultiBox Dectector (SSD) architecture ​​Two-stage detector vs. One-stage detectorFocal lossClass imbalance problem ​RetinaNetone-stage network ​​Detection with TransformerDETR (DEtection TRansformer) by Facebook ​FurtherDetecting objects as pointsBounding box can be represented by other ways (left-top, right-bottom, centroid & size)Idea : Detect objects using corresponding pointsCornerNet/CenterNet ​ "
[ Object Detection 논문 리뷰] Fast R-CNN ,https://blog.naver.com/ongbbb/222824240300,20220722,"Abstract이번 모델은 이전보다 training, testing 속도를 높이고 detection accuracy도 향상시켰다R-CNN보다 mAP성능은 높이면서 train에서는 9배, test에서는 213배 빠르다SPPnet보다도 3배 빠름​​ 문제제기병목발생수많은 proposals이 처리되어야함이 proposal들은 정확한 위치를 찾는 과정이 필요한 rough한 localization이다이를 해결하기 위한 처리들로 속도, 정확성, 단순성은 떨어지는 경향이 있음​2. 기존 RCNN의 단점RCNN의 pipeline은 fine-tunig > SVM > BBox regression, 즉 multi-stage pipeline각 propsal들의 feature가 추출돼 디스크에 저장되는데 VGG16처럼 very deep network의 경우 5k이미지에 대해 2.5GPU 소요되며 이 feature 들 저장을 위해서는 수백 GB의 저장공간 필요느리다, 테스트타임에서 GPU 기반임에도 한 이미지당 47초가 걸림​3. 기존 SPPnet의 단점RCNN 속도 문제를 개선하려했던 SPPnet의 경우 RCNN보다 10~100배 테스트 타임 속도, training에서는 3배 향상을 이뤄냄(faster proposal feature extraction을 통해)하지만 multi-stage pipeline이라는 점, feature들이 disk에 기록되어야한다는 단점이 있음또한 RCNN과 달리 앞에 있는 Conv layer 업데이트가 불가능해서 정확도를 향상시키는데 제한이 있다​​ 이 논문의 제안1. 기존 2-stage였던 RCNN과 달리 1-stage로 구현(jointly learn to classify object proposals & refine spatial locations)2. Fast RCNN 장점RCNN, SPNet 보다 더 나은 detection 성능(mAP)Multi-task loss로 single stage로 학습모든 layers에 대한 가중치 업데이트 > feature sharing 통해서feature caching에 대한 디스크 저장공간 불필요 end-to-end with a multi-task loss ArchitectureImagenet network pretrained 모델을 활용 1. input image를 conv layers에 통과해 feature map 추출, 그리고 동시에 input image에 대한 region proposals r개 생성2.  ROI는 featuremap이 아닌, input 이미지에 대한 것이므로 feature map에 ROI를 projection-> ROI projection 3. max pooling(FC에 넘겨주려면 고정된 사이즈여야하기 때문에) 적용(each RoI is pooled into a ﬁxed-size feature map and then mapped to a feature vector by fully connected layers )  ROI projectionimage size = 800x 800, feature map = 8x8  → sub samplilng ratio = 0.01, 0.01따라서 feature map에 projection해 나타나는 proposal size = (0.01* 500) x(0.01*700)max pooling 한후 얻어지는 sub-window 크기는 2x2로 고정했으므로 이에 맞게 영역을 나누어 수행이후 FC로 각 proposal에 해당하는 feature vector를 추출​3. classification, localization 동시에 수행  (softmax probabilities and per-class bounding-box regression offsets) proposals 중 25 % 사용하며 최소 IoU 0.5인 것multi-task loss로 end-to-end 방식 수행​​ Loss functionLoss function에 localization, classification 텀이 함께 들어가있다(classification) p는 각 proposal이 갖는 k개의 클래스에 대한 확률(background까지 총 k+1), background의 u는 0, Loc 무시,(localization) t의 k는 각 object의 클래스를 나타냄, v는 ground truth ​  Lcls(p, u) = -logpu  ​​ROI pooling xi: 피처맵의 인덱스가 i인 곳 , r : r번째 RoI , J : RoI pooling의 아웃풋의 인덱스가 j인 곳y: output https://blahblahlab.tistory.com/104 Fast RCNN 논문 리뷰이 논문을 읽기 전 무조건 rcnn과 sppnet을 공부하고 오면 편하다. 0. Abstract 기존 object detection에서 쓰이던 rcnn이나 sppnet보다 빠른 fast rcnn에 대해 소개한다. 이전 네트워크들에 비해 training, test..blahblahlab.tistory.com ​​​​ 실험 결과VOC07, 2010, 2012 에 따른 mAP 비교 데이터를 늘렸음에도 좋은 성능을 보임을 증명​​2. SPP net, RCNN과 Fast RCNN 의 성능 비교- 모두 pretrianed 모델을 사용하는데 가장 큰 차이점은 pretrained 모델에서 가져온 convlayer들의 파라미터 update 가능 여부이다S, M, L : 모델의 deep한 정도(CaffeNet or AlexNet < VGG_CNN_M_1024 < VGG16) L 모델 사용 시 시간이 많이 늘어나긴 하지만 다른 모델과 비교했을 때 시간, 성능 측면에서 더 낫다SVD 썼을 때 성능이 좀 떨어지긴 하지만 시간은 훨씬 빨라진다​​​​​​3. VGG16의 conv layer들을 fine-tuning에 따른 mAP 향상 어떤 레이어를 fine-tunning 했을 때 성능향상을 이루는지 보여줌​​4. SVM이 진짜 좋은가, softmax 와 비교(VOC07 mAP) Softmax가 성능이 훨씬 뛰어나다 는 아니지만 softmax를 활용하면 굳이 multi-task로 할 필요가 없고 one-shot으로 끝낼수 있어서 softmax가 더 나은듯​​​​5. proposal이 많을수록 좋나?  proposal이 많아질수록 map가 높아지는 것은 아님을 알 수 있음​​​ Pooling왜 필요?더 높은 정확도를 위해서는 많은 filter가 있어야하지만 feature map이 늘어나기 때문에 파라미터 수도 그만큼 증가한다→ overfitting 가능성 높아짐, 따라서 차원을 줄여줄 수 있는 방법인 pooling을 사용​Max pooling 단점공간 관계 정보를 잃게 됨상대적인 위치, 방향과 상관없이 특징이 추출되고, 방향이나 비율이 달라지면 서로 다른 객체로 인식하는 단점.  feature sharing Fast RCNN의 장점은 네트워크 내 모든 가중치 업데이트가 가능하다는 점 > feature sharing을 통해 가능SGD의 mini batch의 데이터 개수가 N개, ROI가 R개면하나의 이미지는 R/N개의 ROI를 생성​효과) RoIs from the same image share computation and memory in the forward and backward passes​​​​ Scale invatiance: 이미지 크기에 상관없이 그 성질이 유지되는 것을 의미​broute forceMulti-scale approach(image pyramid 활용)​The network must directly learn scale-invariant object detection from the training data. ​ Truncated SVD(특이값 분해)SVD란 기존 행렬에서 근사행렬을 구하는 것proposals 너무 많기 때문에 FC layer에서 많은 시간이 소요됨→ truncated SVD 사용​효과) u*v개의 파라미터 개수를 t(u+v) 로 줄일 수 있다는 장점실험 증명) 사용했을 때 detection time을 30%줄임(but mAP 0.3% 감소) ​​​ 궁금했던 것scale invariance 가 필요한 이유각 이미지의 나무가 하나씩 있다고 생각해보자. 사이즈가 다를 경우 특정 사이즈로 맞춰주기 위해서는 해상도가 떨어지게 된다. 이때문에 정보 손실이 발생할 수도 있기에 성능에도 영향을 미칠 수 있다​2. Fast RCNN에서 급격하게 속도가 빨라졌다. 그 이유는?기존 proposals들을 디스크에 저장하는 것이 아닌 end - to -end 방식으로 한 이미지를 가져오면 max - pooling을 수행 바로 그 다음 레이어로 넘겨주기 때문에 이 과정에서 속도가 매우 빨라졌을 것추가로 SVD 등의 방법론들을 적용한 것도 이유가 될 수 있지만 무엇보다 디스크 저장이 필요 없다는 것이 가장 큰 이득이 아닐까 생각한다​3. Fast RCNN은 기존 SVM 대신 softmax로 분류를 수행했다. softmax가 더 빠른가?train 단계에서는 더 빠를 수 있음. 하지만 test에서는 복잡한 계산이나 Loss function을 통한 가중치 업데이트 등의 과정이 없으므로 비슷할 것이라 생각됨​​​​​​​​​​​​​https://noru-jumping-in-the-mountains.tistory.com/14 [논문 리뷰] Fast R-CNN1. Abstract & Introduction 이번에는 Fast R-CNN에 대한 논문 리뷰를 해볼 것이다. Fast R-CNN(Fast Region-based Convolutional Network method)는 object detection에서 주로 사용한다. 기존 R-CNN보다 trainin..noru-jumping-in-the-mountains.tistory.com ​ "
[Object Detection] Yolo(You Only Look Once) ,https://blog.naver.com/ongbbb/222803345378,20220707,"*핑크색은 논문에 명시하지 않은 내용 Yolo(You only look once)전반적 flow:BBox 예측과 Classification을 동시에 학습하며 이로 인해 다른 detection 알고리즘보다 훨씬 빠르게 학습​이미지를 SxS gird cell로 분할 및→ 각 cell마다 B개의 BBox 생성 → BBox 예측, Classfication 동시 학습​​ BBox 예측, Classification 을 위한 변수들1. BBox 예측   중심점(x,y) width, height, confidence → BBox 단위    *이때 x, y, w, h는 정규화한 후 사용​2. classification  Pr(class|object) → cell 단위​ → 따라서 하나의 cell 은 (B*5 + class종류)개를 예측.  →  즉, S xS x (B*5 + class종류) tensor → test time에 BBox의 confidence prediction과 Pr을 곱해 계산 ​*confidence score = P(object)*IOU(truth, pred)따라서 test time 에서 P(Class|object) x confidence score를 의미여기서 P(object)를 논문에서 명시해주지 않았는데 아마 BBox안에 obj 있으면 1 없으면 0인 것 같다​​​​ Architecture24개의 Convolutional layers 와 2개의 fully connected layers로 이루어져 있다input size= 448x448x3       *논문에서는 224x224의 이미지를 448x448로 만들어 사용output size = SxSx(B*5+C)​​​ Loss function모델의 목표는 class prob 와 BBox의 coordinates 두개 모두 예측하는 것이므로 loss function에 모두 포함되어야함 ※ 왜 obj, noobj로 나누고 가중치를 부여했는가?1. localization과 classfication error를 동등하게 loss function에 반영하는 것은 이상적이지 x2. obj 포함하지 않은 BBox는 confidence 값이 0수렴할 것이고, 이는 obj포함하는 cell의 gradient를 억제하게 됨​​※ 왜 width와 height텀에 squared root를 적용했는가?예를 들어 1번 BBox의 크기가 1000000이고 real width는 9999999, 2번 BBox의 크기가 9이고 real width는 8이라고 해보자.동일하게 1이 차이나지만 BBox의 크기를 고려한다면 사이즈가 작은 2번 BBox에서 더 크게 반영되어야함. 따라서 size에 따라 error를 가중시켜주기 위해 squared root값 사용​​※ 각 cell은 B개의 BBox를 가짐. 하지만 각 object에 대한 하나의 BBox를 갖기를 원함 training 에서는 ground truth와 비교하여 IOU 값을 계산해 가장 높은 값을 가지는 BBox를 선정실제 예측에서는 NMS(클래스 예측 확률과 예측 BBox들간 IOU값을 사용해 최적 BBox를 선정하는 방법론)을 활용​​​​​ 궁금한 점이미지를 SxS로 나누고 B개의 BBox를 갖게 되는데 물체 여러개가 밀집 & B개가 충분치 않는 경우는 어떻게..?→ Yolo의 한계점​s의 개수가 결과에 어떤 영향을 미칠까..? 큰 영향이 있을지?​Loss function의 가중치 설정이 타당한 것인가→ noobj의 가중치를 가진 텀들 중에x, y, w,h가 없었는데 그 이유는 BBox내에 obj 없으면 real BBox도 없으므로 존재하지 않게 됨, 타당!​결국 목표는 최적 BBox 예측과 분류인데 각 BBox가 해당 클래스일 확률은 어떻게 구하는것인가..?→ 논문에 나와있지 않음, 아마 해당 그리드와 BBox가 겹치는 넓이를 반영해 weighted sum 하지 않을까 하는 생각​​ 더 고민해봐야 할것- 왜 Yolo가 다른 알고리즘들보다 extremely fast한지- BBox 생성시 제약사항은 없는지(그리드보다 더 작아선 안된다 혹은 그리드 내 중심점을 반드시 포함해야한다 등, 논문에는 명시되어있지 않음)​​​​​​원본: https://arxiv.org/pdf/1506.02640.pdf "
객체탐지 (Object Detection) 강의 ,https://blog.naver.com/einstephener/223072160887,20230412,​1) Object Detection 소개https://www.youtube.com/watch?v=Hesxx-gSEgshttps://www.youtube.com/watch?v=H-i7ziJZHo8https://www.youtube.com/watch?v=7XMVbgo0C8c​2) 2 Stage Detectorhttps://www.youtube.com/watch?v=cFUnFiDWeZQhttps://www.youtube.com/watch?v=uEOzp94Op5c​3) 1 Stage Detectorhttps://www.youtube.com/watch?v=lKCGaa2wQr4https://www.youtube.com/watch?v=souWs8dmhwshttps://www.youtube.com/watch?v=mgUawHx6Zeohttps://www.youtube.com/watch?v=ctGcivzIvjY​ 
Improved DeepLab v3+ with metadata extraction for small object detection... ,https://blog.naver.com/ckrgksgudxo/222952763052,20221211,"Heungmin Oh, Minjung Lee, Hyungtae Kim, and Joonki Paik, ""Improved DeepLab v3+ with metadata extraction for small object detection in intelligent visual surveillance systems"", IEIE Trans. Smart Processing and Computing, Vol. 10, No. 03, pp. 209-218, June 2021.  "
[object detection 논문 리뷰] Faster RCNN ,https://blog.naver.com/ongbbb/222829407704,20220726,"문제 제기- SPPnet, Fast RCNN proposal간 공유 convolution덕분에 cost를 확 줄여 속도 문제를 개선하고자 했지만 여전히 region porposal에서 많은 시간을 소요- Fast RCNN의 경우 GPU의 장점을 활용했지만, region proposal 자체는 CPU 에서 수행.​​​ 제안 내용기존 region proposal 방법보다 시간을 줄인 Region proposal network 제안(detection network와 전체 이미지에 대한 feature를 공유하는 RPN과 Fast R-CNN을 합쳐 single network로 만들었고, feature를 공유함이를 통해 GPU에서 5fps 속도를 가지며, 한 이미지당 300개의 proposal 만으로 다른 모델만큼의 정확도를 보임 - RPN과 Fast RCNN의 detection network을 unify 위해  region proposal에 대한 fintuning과 object detection에 대한 fine-tuning을 번갈아하는 training schem(4-stage)제안​​​ 구조feature map 얻기RPN통해 적절한 region proposal 생성region proposal과 featrue map을 통해 ROI pooling을 수행해 고정된 크기의 feature map을 얻음기존 Fast-RCNN 모델에 3의 feature map을 넣어주고 BBox regression, classification 수행​Loss function ​ RPN - 기존에는 reigon proposal로 Selective search, Edge boxes 사용했는데 여전히 detection network만큼의 시간을 소요함​​역할regress region bound > localization 각 위치에서 사물이 있는지 없는지에 대한 objectness socre > classification​​​구현​​ ​input image에 대한 feature map 생성(pretrained model - VGG)feature map에 대해 3x3 conv연산 수행, ratio proposal k개 생성3. class) feature map에 1x1 conv 적용  → channel 수 2 x k    regress) 1x1 conv 적용 → channel 수 4 x k​​k = 9,  w = 8, h = 9 anchor box 는 8*8*9 개 생성 → 3의 feature map 통해 class score, Bbox regression 계산 → class 점수에 따라 상위 N개의 proposal 추출 → NMS 적용해 최적 proposal 만을 Fast-RCNN 전달​*이렇게 conv layer를 생성해 region proposal 과정도 GPU에서 할수 있게 됨​​학습방법-  랜덤으로 256anchor를 샘플링해 minibach단위로 loss function을 계산(negative:positive = 1:1 비율)- 우리는 표준 편차가 0.01인 제로 평균 가우스 분포에서 가중치를 그려 모든 새로운 계층을 무작위로 초기화한다. - 다른 모든 계층(즉, 공유 컨볼루션 계층)은 표준 관행[5]과 같이 ImageNet 분류[36]에 대한 모델을 사전 교육하여 초기화된다.- 메모리 절약 위해 ZF net의 모든 레이어와 VGG 네트를 위해 conv3_1 이상의 레이어를 튜닝  Fast RCNN 에서 region proposal자체도 CPU 에서 동작됨> region proposal으 ㄹ하는 네트워크 따로 만들어서 수행> but 어떻게 sharing computation을 할거냐? 알고리즘 측면에서의 변화를 통해 nearly cost -free 구현알고리듬 변화(심층 컨볼루션 신경망으로 제안을 계산)> detection network와 featuremap 자체를 공유하기때문에 성능, 속도 좋아짐​​​   sharing features for RPN and Fast R-CNNRPN과 Fast=RCNN은 각각 학습을 진행하는데 별개로 학습하는 것보다는 두 network들 사이 conv layers공유하도록 했음​Alternating TrainingRPN을 먼저 훈련 시킨 후 생성한 proposals로 Fast -RCNN 을 학습2. Fast -RCNN에 의해 튜닝된 network로 RPN을 초기화 시킴3. 위과정을 반복​​4- step Alternating trainigImagenet pretrained model로 end-to-end 로 초기화단계 RPN에 의해 생성된 proposal 사용하여 Fast R-CNN의 detection 네트워크를 훈련한다. 이 탐지 네트워크는 또한 ImageNet 사전 훈련된 모델에 의해 초기화된다. 이 시점에서 두 네트워크는 컨볼루션 레이어를 공유하지 않는다.  detection 네트워크를 사용하여 RPN 훈련을 초기화히거 공유 컨볼루션 레이어를 고정한 후.  RPN에 고유한 레이어만 미세 조정한다. 이제 두 네트워크는 컨볼루션 레이어를 공유한다. 공유 컨볼루션 레이어를 고정 상태로 유지하여 Fast R-CNN의 고유한 레이어를 미세 조정한다. ​따라서 두 네트워크는 동일한 컨볼루션 레이어를 공유하고 통합 네트워크를 형성한다. 유사한 교대 훈련을 더 많은 반복에 대해 실행할 수 있지만, 우리는 대수롭지 않은 개선을 관찰했다.​​​ 실험 및 검증detector는 고정한 채 진행(We fix this detector and evaluate the detection mAP by changing the proposal regions used at test-time.) → 이렇게 해도 타당한가​RPN+ZF가 다른 방법들보다 더 성능이 좋음conv layers 공유할 때 더 좋은 성능no NMS 는 상위 6000 RPN proposals로 성능평가 > NMS 가 성능에 도움이 된다no cls 의 경우 proposals 개수가 100이 되면 성능이 크게 떨어짐 no reg를 봤을 때도 성능 떨어지는 걸보면 좋은 퀄리티의 proposal들은 regression 으로 나옴을 알수있음(단순히 multiple scales, aspect ratios로는 부족하다는 의미)VGG16 을 pretrained 로 썼을 때 좀더 좋은 성능​​​​ VGG로 pretrained 모델 활용했을 때 전반적으로 성능 굿SS 경우 1-2초 정도 걸리지만Faster RCNN은 proposal, detection 모두 해서 198ms ​​​​​ anchor box ratio, scale에 따른 성능람다는 민감하지 않은 하이퍼 파라미터임을 보임​​​​ IoU 값에 따른 영역 추정 경계 박스의 recallone-stage, two stage 성능 비교​​​​Precision: 예측 Positive 중 TP → 박스 친 애들 중 제대로 클래스까지 잘 예측한 비율 recall : 실제 Positive 중 TP → 실제 object들을 빠뜨리지 않고 잘 detection 했는지​​​​​※ 실험 중 training region proposal 은 SS로 고정하고 test 는 RPN 으로 수행한 것이 있음. 이렇게 해도 돠나? 타당한가?해당 실험은 detector 를 고정, 즉 같은 detector 를 사용하면서 test 단계에서의 RPN 성능&제안한 모듈의 효과를 보여주려는 것이기 때문에 training 에서 region proposal 을 고정한 것이 적절함어떤 방법론을 적용하냐는 크게 문제되지 않지만, 더욱 타당성을 높이기 위해서는 RPN 으로 고정시킨 것이 더 좋았을 듯. 왜냐면 어떤 trainjng의 region proposal 방법론을 적용했냐에 따라 test에서의 RPN과 모듈들의 영향도가 달라질 수 있기 때문이다!!​​※모든 과정을 GPU에서 수행할 수 있게 된 이유region proposal 과정에서 proposals 들은 CPU에 저장되어야 했고, 이를 detection network에 넘겨주는 것임하지만 Faster-RCNN의 RPN은 conv layers 로 region proposal를 구현해 end-to-end로 CPU  사용 필요 없이 GPU에서 모두 연산가능해짐​*이렇게 conv layer를 생성해 region proposal 과정도 GPU에서 할수 있게 됨 "
[Tensorflow / TF] Object Detection API 사용하기 (2) ,https://blog.naver.com/bdh0727/221537759295,20190515,"거의 1년 가까이 지나서야 두번째 세션을 작성한다.1회성 실습인줄 알았는데 올해도 진행을 하게 되어서 새로 코드 정리한 김에 미뤄두었던 기록 시작​  Intro지난 글에서는 Object Detection API를 사용하기 위한 환경을 셋업하고 모델을 실행시키는 방법에 대해 다루었다.▶ Object Detection API 사용하기 (1) - https://blog.naver.com/bdh0727/221341342386 [Tensorflow / TF] Object Detection API 사용하기 (1)랩실에서 진행했던 실습 자료 정리Object detection (객체 탐지) 실습을 맡았었는데 그냥 실습 한두번 하...blog.naver.com ​아래 설명에서 사용한 코드와 데이터 등은 다음 링크에서 다운받을 수 있습니다.▶ 실습코드(Github) - https://github.com/Daehyun-Bae/Object_Detection_API_utils▶ 학습 모델 및 데이터셋(Google drive)  - https://drive.google.com/open?id=1mwz5Lk-CfPM7q4Yb3X8jThlZzUYsH9by​  Fine tuning 실습지난번 실습 때 사용했던 모델은 COCO 라는 데이터셋에 기 학습된 (pre-trained) 모델이다.다른 데이터셋에 있는 객체를 검출하려면 어떻게 해야할까?COCO 데이터셋이 워낙 많은 클래스에 대해 학습을 했기 때문에 웬만한 물체는 찾을 수 있겠지만 그 한계가 있을 것이다.이 때 사용자가 원하는 데이터셋에 추가 학습을 진행하는 것이 Fine-tuning 이다. ​   Cat 대신 고양이 종류와 얼굴 부분만 검출하고 싶다..! ​실습에서는 Oxford-IIIT Pets 라는 데이터셋을 이용하여 기존 모델을 fine tuning 하는 방법을 진행하였다.총 37 종류의 강아지와 고양이 사진이 각 클래스 별로 약 200장씩 구성되어 있는 데이터셋이다.실습에 사용되는 파일은 다음과 같다.​   train.py 코드를 통해 추가 학습을 진행할 수 있다.코드를 수정할 부분은 다음과 같다. # train.pyTRAIN_DIR = 'train_res_pet' # path to save training result PIPELINE_CONFIG_PATH = 'pipeline_pet.config' # path to training configuration file TRAIN_DIR 에는 학습 결과를 저장할 경로를 지정해 주고,PIPELINE_CONFIG_PATH에는 학습에 필요한 정보가 담겨있는 .config 파일 경로를 지정해 준다.​pipeline_pet.config 파일을 열어서 학습에 필요한 parameter를 수정한다. model {  faster_rcnn {    num_classes: 37    image_resizer {      keep_aspect_ratio_resizer {        min_dimension: 600        max_dimension: 1024      }    }    feature_extractor {      type: 'faster_rcnn_resnet101'      first_stage_features_stride: 16    }... model { ... } 은 학습을 수행할 모델에 대한 내용이 정의되어있다.크게 손 볼 부분은 없다.​ train_config: {  batch_size: 1  optimizer {    momentum_optimizer: {      learning_rate: {        manual_step_learning_rate {          initial_learning_rate: 0.0003          schedule {            step: 7000            learning_rate: .00003          }        }      }      momentum_optimizer_value: 0.9    }    use_moving_average: false  }  gradient_clipping_by_norm: 10.0  fine_tune_checkpoint: ""faster_rcnn_resnet101/model.ckpt""  from_detection_checkpoint: true  num_steps: 20000  data_augmentation_options {    random_horizontal_flip {    }  }} ​다음은 training과 관련된 내용을 정의하는 부분이다.batch size를 비롯해 optimizer나 learning rate scheduling도 가능하다.※ batch size을 1보다 크게 사용할 때는 위의 모델 정의에서 keep_aspect_ratio_resizer 대신에 fixed_shape_resizer 를 사용해야한다. fixed_shape_resizer {        height: 600        width: 1024      } learning rate scheduling은 다음과 같이 설정할 수 있다. initial_learning_rate: 0.0003          schedule {            step: 7000            learning_rate: .00003          } 위의 코드는 시작 learning_rate 를 0.0003으로 하고, step 7000부터는 0.00003으로 바꾸겠다는 의미이다.마지막으로 pre-trained model weight path와 training step 수를 지정해 준다. fine_tune_checkpoint: ""faster_rcnn_resnet101/model.ckpt""from_detection_checkpoint: truenum_steps: 20000 fine_tune_checkpoint에는 pre-trained model의 체크포인트 파일 경로를 작성한다.num_step 은 학습 횟수를 의미한다.​fine tuning이라고는 하지만 제대로 성능을 낼 수 있을 정도로 학습시켜보니 약 70,000 회는 되어야한다.실습 시간에 처음부터 7만번이나 학습하면 그 날 수업은 그대로 끝이기 때문에 미리 어느 정도 학습을 시켜놓은 모델에 추가 학습을 진행하였다.(약 5,000 회에 30분 정도니까 약 7시간...)​50,000 번 학습한 모델은 pet_dataset_pre_trained 폴더에 저장되어있다. (google drive에서 다운받을 수 있습니다.)​마지막으로 학습 데이터에 대한 경로를 지정해 준다. train_input_reader: {  tf_record_input_reader {    input_path: ""pet_dataset_pre_trained/pet_train.record""  }  label_map_path: ""label_map/pet_label_map.pbtxt""} input_path 에는 학습 데이터 경로를 작성한다.※ TFrecord라는 데이터형을 사용한다. 이 TFrecord를 생성하는 실습은 다음 번으로...※ google drive의 pet_pre_trained_dataset.zip 에 포함되어있다.​그리고 코드를 실행하여 학습을 진행한다.​학습이 완료되면 사전에 설정해 놓은 경로에 다음과 같은 파일이 생긴다.   ​학습 중간중간에 저장한 checkpoint 파일들과 configuration 파일, 학습 로그를 기록한 event. 파일 등이 보인다.​  Export Inference Graph학습이 끝난 후 실제 실행 결과를 보기 위해 Inferece graph라는 것을 추출하는 과정이 필요하다.graph로 표현되어 있는 모델에 checkpoint 파일의 weight를 덮어씌워 주는 과정이라고 생각하면 된다.export_inference_graph.py 의 코드를 다음과 같이 수정하고 실행한다. # export_inference_graph.pyPIPELINE_CONFIG_PATH = 'pipeline_pet.config'OUTPUT_DIR = 'export_pet'CKPT_PREFIX = 'pet_dataset_pre_trained/model.ckpt-50000' PIPELINE_CONFIG_PATH 에는 학습에 사용했던 configuration 파일 경로를 지정한다.OUTPUT_DIR 에는 추출한 Inference graph를 저장할 경로를 지정한다.CKPT_PREFIX 에는 학습된 모델의 checkpoint 파일 경로를 지정해 준다. ​실행 후에는 다음과 같은 파일이 생성된다.   Inference graph exporting result ​​  Test이제 새로 생성된 frozen_inference_graph.pb​ 파일을 이용하여 다시 테스트 사진에 적용해보자.object_detection_run.py를 사용한다.이 때 mscoco가 아닌 pet dataset에 대해 학습하였기 때문에 label map도 바꿔 주어야한다. # object_detection_run.py# Pre trained graph model PATHMODEL_NAME = 'export_pet'PATH_TO_CKPT = os.path.join(MODEL_NAME, 'frozen_inference_graph.pb')# Path to label mapPATH_TO_LABELS = os.path.join('label_map', 'pet_label_map.pbtxt')NUM_CLASSES = 37PATH_TO_TEST_IMAGES_DIR = 'test_images/test_pet'TEST_IMAGE_PATHS = [os.path.join(PATH_TO_TEST_IMAGES_DIR, img) for img in os.listdir(PATH_TO_TEST_IMAGES_DIR)]TEST_SAVE_PATH = 'test_result/{}'.format(MODEL_NAME) ​실행결과는 다음과 같다.​  Inference Results ​아까 전의 고양이는 벵갈 고양이었다.   ReferenceOxford-IIIT Pets dataset http://www.robots.ox.ac.uk/~vgg/data/pets/​Tensorflow Object Detection API Home: https://github.com/tensorflow/models/tree/master/research/object_detectionTensorflow Object Detection API Install Guide: https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/installation.md조대협님의 블로그: http://bcho.tistory.com/1192솔라리스의 인공지능 연구실: http://solarisailab.com/archives/2422 "
[Tensorflow / TF] Object Detection API 사용하기 (1) ,https://blog.naver.com/bdh0727/221341342386,20180818,"랩실에서 진행했던 실습 자료 정리Object detection (객체 탐지) 실습을 맡았었는데 그냥 실습 한두번 하고 버리긴 아까워서,그리고 혹시 Object detection API를 사용하실 분들에게 조금이나마 도움이 되었으면 해서 삽질 기록 환경 구성실습 환경OS: 우분투 16.04CPU: i7-7700KGPU: NVIDIA GeForce GTX 1080다음과 같은 파이썬 라이브러리 설치를 요구한다.- Protobuf- Pillow- lxml- tensorflow ( >1.4 )- Cython그 밖에도 evaluation이나 visualization을 위해 추가로 필요로하는 라이브러리가 있을 수 있다.​실습에 사용했던 코드는 공식으로 제공해주는 예제 파일을 실행 시키기 쉽게 약간 변형 하였다.아래 구글 드라이브 링크를 통해 받을 수 있다.https://drive.google.com/open?id=1Xohw-Qd84TE-So1EYYdby1m54NcUbNnr​※이번에 실습 코드를 다시 정리하였습니다.혹시 코드가 필요하신 분들은 아래 깃허브를 참고하시면 됩니다.https://github.com/Daehyun-Bae/Object_Detection_API_utils Daehyun-Bae/Object_Detection_API_utilsCustomized codes when using Object Detection API from Tensorflow - Daehyun-Bae/Object_Detection_API_utilsgithub.com ​Pre-trained model이나 실습에 사용한 VOC2007, Pet dataset은 아래 드라이브 링크를 통하여 받을 수 있습니다.https://drive.google.com/open?id=1mwz5Lk-CfPM7q4Yb3X8jThlZzUYsH9by data - Google 드라이브드라이브 로그인 drive.google.com ​먼저 google에서 제공하는 github repository를 다운 받는다. > git clone http://github.com/tensorflow/models 그리고 다음의 경로로 이동한다. // Change directory to /models/research/> cd models/research Object detection API에서는 프로토콜 버퍼를 이용한다. (구글에서 개발한 데이터 직렬화 방식이라는데 자세히는 잘 모르겠다.)protoc 명령어로 다음 경로에 있는 proto 파일들을 컴파일 해준다.  // From models/research/> protoc object_detection/protos/*.proto --python_out=. 그리고 PYTHONPATH 환경변수를 설정해준다. // From models/research/> export PYTHONPATH=$PYTHONPATH:`pwd`:`pwd`/slim 작은따옴표( ' ) 가 아니라 물결표 밑에 있는 악센트 부호( ` )... 처음엔 이것도 헷갈렸었다.다음 명령어로 제대로 설치가 되었는지 확인할 수 있다. // From models/research/> python3 object_detection/builders/model_builder_test.py   ※ 실제로 실행을 시켜보려고 했는데 몇가지 오류가 발생해서 삽질을 많이 했다.그 중 하나로 learning_schedules.py라는 파일에서 발생하는 오류가 있었는데 코드 한 줄만 고치면 되는 오류였다.올려둔 파일 중 modified_api/learning_schedules.py를 ../models/research/object_detection/utils/ 폴더에 덮어쓰기를 한다.최근 진행 했던 실습 (18.07.24)에서는 아직 고쳐지지 않았었다.※ 이번 실습 (19.05.09)에서는 오류가 발생하지 않아서 여기는 건너 뛰어도 됩니다.  modified_api/learning_schedules.py를 ../models/research/object_detection/utils/ 에 덮어 씌우기 Pre-trained model 가져오기 및 실행Google에서는 이미 다양한 dataset에 대해 학습이 완료된 pre-trained model을 제공하고 있다.   tensorflow/modelsmodels - Models and examples built with TensorFlowgithub.com 여기서는 MSCOCO dataset에 대해 학습한 resnet101기반의 faster-RCNN 모델인faster_rcnn_resnet101_coco 모델을 사용하였다.링크를 눌러서 다운 받은 뒤 압축을 풀면 몇가지 파일들이 나온다.  faster_rcnn_resnet101_coco_10_06_2017 Demo 실행에 필요한 파일은 frozen_inference_graph.pb 파일 하나뿐이다. model.ckpt 파일은 나중에 여기에 fine-tuning을 할 때 사용한다.​pre-trained model 실행에 필요한 파일· frozen_inference_graph.pb: 학습이 완료된 모델의 그래프를 export한 파일· mscoco_label_map.pbtxt: MSCOCO dataset에 대한 label 정보 (label_map/mscoco_label_map.pbtxt)· object_detection_run.py: pre-trained model을 실행 시키는 코드 (tensorflow github에서 제공하는 예제코드를 조금 변형)object_detection_run.py 에서 실행에 필요한 각종 경로와 값을 설정하고 실행한다.  object_detection_run.py · MODEL_NAME = pre-trained model (frozen_inference_graph.pb)이 저장 된 폴더명· PATH_TO_CKPT = frozen_inference_graph.pb 파일 경로· PATH_TO_LABELS = Label map 파일 (mscoco_label_map.pbtxt) 경로· NUM_CLASSES = Class 개수 (MSCOCO에서는 90개)​· PATH_TO_TEST_IMAGES_DIR = Detection test에 사용 될 이미지 Directory· TEST_IMAGE_PATHS = test image들의 경로· TEST_SAVE_PATH = test 결과를 저장할 directory 실행 결과실행을 하면 지정했던 경로에 테스트 결과를 저장할 폴더가 생성되고 테스트 결과 이미지가 저장된다.  Input  Output ​ 참고 사이트Tensorflow Object Detection API Home: https://github.com/tensorflow/models/tree/master/research/object_detectionTensorflow Object Detection API Install Guide: https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/installation.md조대협님의 블로그: http://bcho.tistory.com/1192솔라리스의 인공지능 연구실: http://solarisailab.com/archives/2422 "
"‘Object Detection, 그 실전 노하우를 익혀갑니다.’_2기 수강생 양영규님 ",https://blog.naver.com/fastcampus/221453877358,20190129,"   최근 논문을 통해 Object Detection의 발전 흐름과 방향성을 파악할 수 있었고, 성능 향상을 위한 기법들을 익힐 수 있었습니다.안녕하세요. 용인 흥덕 IT 밸리에서 카드 OCR 업무를 담당하고 있는 양영규라고 합니다. 저는 실제 업무에서 Object Detection을 사용하고 있는데요. 더 높은 성능 향상을 위한 지식이나 팁들을 배운다면 업무에 활용할 수 있는 폭이 넓어질 것 같아 [Object Detection 실전 Workshop] 수강을 결정했습니다.​​ 패스트캠퍼스의 [Object Detection 실전 Workshop]에서 가장 인상 깊었던 점은 ‘최근 논문을 다룬다’는 점이었습니다. 최근 논문을 통해 Object Detection의 발전 흐름과 방향성을 파악할 수 있었고, 성능 향상을 위한 기법들을 익힐 수 있었기 때문이죠. 또한, 강사님께서 강의를 시작하실 때 항상 이전 시간의 내용을 복습해주시는 점이 좋았습니다. 이전 내용을 리마인드하고 넘어가니까  본 수업 내용을 이해하기가 훨씬 수월하더라구요.​​ 이번 강의를 통해 Object Detection의 이론과 실무를 모두 다뤄볼 수 있어 정말 좋았는데요. 정말 꼭 필요한 내용만을 압축시켰다고해도 강사님들께서 전해주시려는 노하우가 워낙 많아서, 가끔은 강의 시간이 빠듯하다고 느껴질 때가 있었습니다. 하지만 그만큼 실무에서 활용하면 좋을 팁들을 꼼꼼하게 알려주시기 때문에, 예습과 복습을 통해 자신의 것으로 만드는 것은 온전히 자신의 몫이라고 생각해요.저도 강의에서 배웠던 코드를 그냥 흘려버리지 않으려고, 개인적으로 시간을 내서 저만의 코드를 다시 짜 볼 예정입니다.​​ 본 강의를 수강하면서, 실무에 적용해 볼만한 것들을 고민해 본다는 것 자체가 저에게는 큰 의미가 있었던 것 같습니다. 앞으로도 시간이 날 때 마다, 최신 논문을 살펴보는 시간을 가져야 겠다는 일종의 동기부여가 되기도 했구요. 아, 만약 이미지나 Object Detection 분야의 챌린지가 열린다면 한 번 참가해보고 싶습니다. 상위권 팀들의 기법들을 배우고 시도해보고 싶어졌거든요.​​ 마지막으로, 본 강의에 관심을 가지고 있으시거나 수강을 고민하고 계시는 분들께 한 말씀 드리고 싶은데요. 본 강의의 강사님들은 Object Detection 관련 블로그나 커뮤니티에서 활발히 활동하는 분들이시기 때문에, 굉장히 적극적이시고 강의 진행력 또한 뛰어나신 분들입니다. 그렇기 때문에 현업에서 본인이 마주하고 있는 문제점들이나 의문점들에 대해 질문을 많이 하면 할수록, 여러분이 기대하고 있는 그 이상을 얻어가실 수 있습니다. 단순히 지식을 쌓는 수준이 아닌, 본인의 현업에서 Object Detection의 적용을 고려하고 계신 분들이라면, 본 강의가 큰 도움이 될 것이라 생각해요!  Detection의 실무 적용 사례를 다루고,실제 데이터에 적용하는 법을 익히는 실습형 강의[Object Detection 실전 Workshop]   ​   "
Object Detection에서 쓰는 Image Augmentation Code ,https://blog.naver.com/mikangel/222154363385,20201125,"Imgaug 라이브러리를 이용한 이미지 데이터 확장 예제Object Detection에 맞게 변형.. #'''imgaug를 사용하면 Annotation을 포함한 이미지 증폭을 수행할 수 있다. 예를 들어, 이미지를 증가시키는 동안 이미지가 회전하면 라이브러리는 그에 따라 모든 객체의 Bounding-box를 함께 회전 시킨다.설치법conda config --add channels conda-forgeconda install imgaugpip install pascal-voc-writer'''from os import listdirimport cv2import numpy as npimport xml.etree.ElementTree as ETimport imgaug as iafrom imgaug import augmenters as iaa# from files import *from pascal_voc_writer import Writerdir = '/home/user1/DLCV/data/bottle/images/''''xml모듈을 이용하여 xml 구조를 파싱하는 코드이다. 한 이미지에 대한 Annotation(PASCAL VOC)으로부터 해당하는 이미지 filename과 이미지 내의 모든 Bounding-Box 정보를 읽어서 반환한다.'''def read_annotation(xml_file: str):    tree = ET.parse(xml_file)    root = tree.getroot()    bounding_box_list = []    file_name = root.find('filename').text    for obj in root.iter('object'):        object_label = obj.find(""name"").text        for box in obj.findall(""bndbox""):            x_min = int(box.find(""xmin"").text)            y_min = int(box.find(""ymin"").text)            x_max = int(box.find(""xmax"").text)            y_max = int(box.find(""ymax"").text)        bounding_box = [object_label, x_min, y_min, x_max, y_max]        bounding_box_list.append(bounding_box)    return bounding_box_list, file_name'''read_train_dataset 함수는 image 파일들과 각 이미지의 annotation xml 파일들이 함께 있는 디렉토리의 경로를 인자로 전달했을 때, 4차원(N:이미지 수, W:이미지 너비, H:이미지 높이, D:RGB) nparray로 변환된 이미지들과 3개의 항(bounding-box 리스트, xml 파일명, 관련 이미지 파일명)을 갖는 Annotation 투플의 리스트를 반환한다.'''def read_train_dataset(dir):    images = []    annotations = []    for file in listdir(dir):        if 'jpg' in file.lower() or 'png' in file.lower():            images.append(cv2.imread(dir + file, 1))            annotation_file = file.replace(file.split('.')[-1], 'xml')            annotation_file = annotation_file.replace('images', 'annotations')            # print(annotation_file)            bounding_box_list, file_name = read_annotation('/home/user1/DLCV/data/bottle/annotations/' + annotation_file)            annotations.append((bounding_box_list, annotation_file, file_name))    images = np.array(images)    return images, annotationsia.seed(1)images, annotations = read_train_dataset(dir)for idx in range(len(images)):    image = images[idx]    boxes = annotations[idx][0]    ia_bounding_boxes = []    for box in boxes:        ia_bounding_boxes.append(ia.BoundingBox(x1=box[1], y1=box[2], x2=box[3], y2=box[4]))    bbs = ia.BoundingBoxesOnImage(ia_bounding_boxes, shape=image.shape)    seq = iaa.Sequential([        iaa.Multiply((1.2, 1.5)),        iaa.Affine(            translate_px={""x"": 40, ""y"": 60},            scale=(0.5, 0.7),            rotate=45                            # 이미지 회전 추가        )    ])    seq_det = seq.to_deterministic()    image_aug = seq_det.augment_images([image])[0]    bbs_aug = seq_det.augment_bounding_boxes([bbs])[0]    new_image_file = dir + 'after_' + annotations[idx][2]    cv2.imwrite(new_image_file, image_aug)    print('augmentation image write file name = ', new_image_file)    h, w = np.shape(image_aug)[0:2]    voc_writer = Writer(new_image_file, w, h)    for i in range(len(bbs_aug.bounding_boxes)):        bb_box = bbs_aug.bounding_boxes[i]        voc_writer.addObject(boxes[i][0], int(bb_box.x1), int(bb_box.y1), int(bb_box.x2), int(bb_box.y2))    voc_writer.save('/home/user1/DLCV/data/bottle/annotations/' + 'after_' + annotations[idx][1]) 축소 후 회전, BBox 도 맞게 형성 됨 "
Object Detection ,https://blog.naver.com/wpdls6012/222031730756,20200715,"캐글의 Global Wheat Head Detection에 참가중이다!​오브젝트 디텍션 관련 자료를 스터디 중이다.​Fast RCNN논문 링크 ; https://arxiv.org/pdf/1504.08083.pdf 갈아먹는 Object Detection [3] Fast R-CNN지난 글 갈아먹는 Object Detection [1] R-CNN 갈아먹는 Object Detection [2] Spatial Pyramid Pooling Network 들어가며 지난 시간 SPPNet에 이어서 오늘은 Fast R-CNN[1]을 리뷰해보도록 하겠습니다. 저 역..yeomko.tistory.com Faster RCNN 부터 보다가, 이해가 가지 않는 몇 가지 부분이 있어서 Fast RCNN으로 되돌아 옴...Faster RCNN의 뒷 부분에 Fast RCNN을 그대로 갖다 쓰고 있기 때문에 꼭 짚고 넘어가야 한다.​참고로 fast rcnn은기존의 r-cnn 이 각각의 object-proposals(selective-search에 의해 생성)에 대해 convNet을 거치는 부분에서 느려지는 단점이 있었고,이 부분에서 convNet을 한 번만 거치고, 그 결과를 재사용(공유)할 수 있도록 개선한 모델이다.  ​faster rcnn에 나오는ROI pooling layer가 뭐하는 건지 막연하게만 알고 있었는데,제대로 짚어보면 다음과 같다.​유효한 ROI 영역 안에 들어오는 feature 들을 small faeture-map 으로 변형하는 레이어이다.H, W라는 하이퍼파라미터가 주어지는데, (ex. 7x7)conv feature map 안에 위치한 ROI 영역(r, c, h, w)을 HxW 영역으로 먼저 나누고, (h/H x w/W)각 sub window 내의 값에 대해서 max-pooling을 적용하여 값을 추출한다.​학습시 Sampling 방법도 눈여겨 볼만 한데,기존의 r-cnn이나 SPPnet 같은 경우, 각각의 학습 샘플은 서로 다른 이미지로부터 만들어졌다.하지만, 좀 더 효율적인 학습을 위해서 fast-rcnn에서는,SGD의 mini-batch의 샘플링 방식을 구조적(hierarchically)으로 적용한다.N개의 이미지와, R개의 ROI 개수가 파라미터로 주어질 때 (N, R)N개의 이미지에 대해 각각 R/N 개의 ROI를 샘플링해서,ROI끼리는 feature를 공유할 수 있도록 했다!=> 같은 이미지 내의 ROI 끼리는 corelated 되어 있어서 수렴 속도가 느려질 수 있는데, 실제로는 별 문제가 되지 않았다고 함.​그리고, Faster RCNN을 읽으면서 이해가 잘 가지 않았던 마지막 sibling layers에 대해서 보면, 마지막 sibling output layers는 각각- 하나는 softmax 를 거쳐서 K+1 category의 확률 분포를 만드는 레이어- 다른 하나는 k 개 클래스 각각에 대해 bounding box를 regression 하는 레이어로 구성된다.​(작성중) "
"""이미지 딥러닝 실무자라면, 워크샵으로 '진짜 기술'을 업그레이드해 보세요"" by Object Detection 실전 Workshop 수강생 양영규님 ",https://blog.naver.com/fastcampus/221493124596,20190320,"  ​ 카드 OCR에도 딥러닝이 사용된다?  ​안녕하세요. 저는 용인 흥덕 IT밸리에서 카드 OCR 관련 일을 하고 있는 양영규라고 합니다. 저는 현재 일을 하면서 Object detection을 실제로 사용하고 있구요. 업무의 질 향상, 그리고 제가 가지고 있는 관련 지식의 범위를 넓히기 위해 패스트캠퍼스의 Object detection 워크샵을 수강하게 되었습니다.​​ Object detection, 딥러닝의 다른 분야를 알기 위한 도전. ​딥러닝의 다른 분야는 한번씩 해봐서 알고 있었지만 Object detection에 대해서는 잘 알지 못했습니다. 일을 하다보니 자연스럽게 Object detection의 필요성에 대해서 알게 되었는데요. 기존에는 API 위주로 적용했을 때에는 실제 데이터로 원하는 결과값을 뽑아내기 어려웠습니다. 아무래도 제가 종사하고 있는 OCR 판독 관련하여 좀 더 실무에 활용가능한 방법론이 필요했었죠. 그래서 이 분야의 기초 지식과 현업에서 적용가능한 팁을 전수받기 위해서 워크샵의 문을 두드렸습니다.​​ 워크샵이기 때문에 얻을 수 있는 강점 '실습을 통한 최신 트렌드와 노하우 습득' ​일반 강의가 아니라 워크샵이기 때문에 이론보다는 실습 위주로 진행되었고 강사님이 수강생들에게 object detection의 동향, 실용적인 팁과 노하우를 아낌없이 알려주시는 점이 정말 좋았습니다. 아무래도 이론의 비중이 높아지면 이 부분에 대해서는 조금 얻어가는게 적을 수도 있거든요. 항상 수업 시작할 때 이전 시간에 했던 내용들에 대해서 꼬박꼬박 짚어주신 점도 이해를 높이는데 도움이 되었습니다. 모르는 점이 있으면 한명 한명의 궁금증을 풀어주셨습니다.​또한, 강사님들이 딥러닝 기반 솔루션 업체 실무진이시다 보니 관련 동향에 대해서 정통하게 알고 계시기 때문에 최근 논문들의 흐름이나 방향성에 대해서 자세히 알려 주신 점이 특히 마음에 들었습니다. object detection의 발전 흐름, 성능 향상을 위한 이전 기법들과 최신 유행 기법들 등에 대한 내용을 들으며 저 스스로 실무에 적용해 볼만한 아이디어 및 고민을 해 볼 수 있었구요.​​ '끊임없는 자기계발과 복습을 통해, 나만의 코드를 만들어볼 예정' ​저는 이 워크샵을 듣고 바로 현업에서 적용해 볼만한 기법들(Multi Scale Testing)을 실제로 시도해보았습니다. 공부를 하다 보니 최신 논문들도 시간이 된다면 꾸준히 읽어봐야겠다고 느꼈구요. 이미지나 object detection 분야의 챌린지가 열린다면 한번 참가해보고 싶고, 뛰어난 실력을 가진 팀들의 기법들을 배우고 시도해 보고 싶습니다. 워크샵에서 얻은 팁과 노하우를 기반으로 추가적인 시간을 내서 저만의 코드로 다시 짜볼 예정입니다.​​ 강의를 듣는 것만으로는 절대 본인의 것으로 만들 수 없다 ​강사님들은 Object detection에 관심이 있거나 자료를 찾다보면 알게 되는 커뮤니티, 블로그에서 활발하게 활동을 하시는 분들입니다. 그래서 아마 워크샵을 듣기 전에 강사님들에 대해서 알고 계시는 분들도 많으실 거예요. 쉽지 않은 분야인 만큼, 개인적으로 공부하거나 업무에 적용하다가 막히는 부분이 있으면 반드시 질문하고 해답을 구하셔야 합니다.​이쪽 분야에서 유명한 강사님들이 직접 가르쳐주시기 때문에, 강의의 내용은 훌륭합니다. 딥러닝 모델에 대한 기본적인 이해가 있는 분들이라면 이론부터 실습까지 해보며 논문부터 코드까지 다양한 방법으로 object detection 기법을 이해할 수 있습니다.  하지만 강의를 한번 듣는다고 해서 저절로 내것이 되지는 않습니다. 단순히 지식을 쌓는 수준이 아닌 본인의 현업에서의 적용을 고려하고 계신 분들이라면 본인이 마주하고 있는 문제점들이나 의문점들을 정리하여 강사님들에게 질문을 많이 한다면 강의 그 이상으로 얻어 가실 수 있을 것 같습니다.  이미지 딥러닝 전문가의 정점을 꿈꾼다면,실습 중심의 Object detection 실전 Workshop에서 실제 데이터에 적용할 수 있는 최강 스킬을 익혀보세요​Object detection 실전 Workshop예제가 아닌 실제 데이터 적용법, 하드트레이닝 시작해봅시다.​     "
Real-time 3D object detection on mobile devices with mediapipe ,https://blog.naver.com/talesoff/222233111151,20210205,"원문: https://ai.googleblog.com/2020/03/real-time-3d-object-detection-on-mobile.html Real-Time 3D Object Detection on Mobile Devices with MediaPipePosted by Adel Ahmadyan and Tingbo Hou, Software Engineers, Google Research Object detection is an extensively studied computer vision p...ai.googleblog.com 객체 탐지는 컴퓨터 비젼에서 아주 오래전부터 조사해온 분야이나, 대부분의 연구들이 2D object prediction에 초점을 맞추고 있다. 2D prediction은 단지 2D의 바운딩 박스만을 제공하는데, 이 prediction을 3차원으로 확장해보면, 사용자가 객체의 크기, 위치, 방위를 알아낼 수 있다. 이렇게 알아낸 정보들은 로보틱스나, 자율주행, 이미지 분석, AR등 다양한 어플리케이션에 사용될 수 있다. 2D 객체 탐지는 기술적으로 어느정도 성숙한 상태이고, 이미 업계에서 널리 사용되고 있다. 하지만 3D 객체 탐지를 2D 이미지로부터 수행하는 것은 데이터의 부족이나, 하나의 카테고리 내에서도 객체의 shape와 appearances가 다양하게 나오기 때문에 쉽지 않은 이슈이다.​오늘날, 우리는 MediaPipe Objectron의 릴리즈를 공포한다. MediaPipe Objectron은 모든 객체를 위한 모바일 실시간 3D 객체 탐지 파이프라인이다. 이 파이프라인은 3D 이미지 내의 객체를 탐지하고, 그 객체의 자세와 크기를 머신러닝 모델을 이용하여 추정한다. 이 머신러닝 모델은 새롭게 생성된 3D data set을 이용하여 훈련된다. 다양한 모달리티들의 perceptual data를 프로세스하는 파이프라인을 생성하기 위한 오픈소스 크로스 플랫폼 프레임워크인 Mediapipe 내에 구현된 Objectron은 모바일 디바이스 상에서 실시간의 객체를 둘러싸고있는 상자의 방위를 계산해낸다.  ​ 실세계의 3D 훈련 데이터 취득 ​거리의 장면들에 대한 3D 데이터는 손쉽게 취득할 수 있다. 그 이유는 자율차와 관련된 연구의 대부분이 lidar와 같은 3D capture sensor에 의존하기 때문에, 필요한 정보를 미리미리 생성해두었기 때문이다. 하지만 매일매일 접하는 객체들의 3D 정보를 담은 데이터는 매우 제한적이다.  이 문제를 극복하기 위해, 우리는 모바일 AR 세션 데이터를 이용한 data pipeline을 개발했다. ARCore와 ARKit 덕분에 수십억대의 스마트폰은 현재 AR 기능을 갖추고 있으며, AR 세션 도중 카메라 포즈, 3D point cloud, light 정보, 평면 정보를 포함하는 추가적인 정보를 얻어올 수 있다. ​실제 데이터에 label 하기 위해 우리는 자동 annotation tool을 생성했다. 이 툴은 AR session data와 함께 사용되며, 자동으로 객체의 3D bounding box를 생성해준다. 이 툴은 split-screen view를 사용하여 한쪽에서는 2D 비디오 프레임에 3D 바운딩 박스를 오버레이해서 보여주고, 다른 편에서는 3D point cloud와 카메라 위치, 그리고 탐지된 평면을 보여준다. annotator는 3D view에 3D 바운딩 박스를 그려주고, 그 결과를 2D video frame에 투사한 후 위치를 평가하는 방식으로 검증한다. 움직이지 않는 객체에 대해서는 하나의 프레임에 대해서만 annotate 작업을 해주고, 다른 프레임에 대해서는 카메라의 움직임 정보를 기준으로 위치를 잡아주는 형태로 계산량을 줄인다. ​ AR 합성 데이터 생성 실세계의 데이터를 합성 데이터로 보완하는 것은 정확도를 높이기 위해 적용되는 아주 흔한 방법이다. 그러나 이 작업을 잘못 수행하게 되면, 오히려 더 안좋은 결과를 얻을 수도 있으며, 그 과정 또한 매우 어려울 수 있다. 우리가 AR Synthetic Data Generation 로 부르는 방식은, AR 세션 데이터를 가지고 있는 씬에 가상 객체를 삽입한다. 이 방법으로 우리는 카메라 포즈를 움직일 수도 있고, 평면을 탐지할 수도 있으며, 물건을 배치했을 때 적합한 라이팅을 수행할 수 있게 한다. 이 방법은 결과적으로 고품질의 합성 데이터를 생성하였으며, 최종적으로 10% 정도의 정확도 향상을 이룰 수 있었다.​ 3D 객체 탐지를 위한 머신러닝 파이프라인 ​​ "
[Object Detection] YOLO V5 파인튜닝하기 with pytorch ,https://blog.naver.com/baek2sm/222241561517,20210214,"안녕하세요, 동네코더입니다. 오늘은 유명한 Object detection 네트워크인 Yolo의 최신 버전인 Yolo v5를 파인튜닝(finetuning)하는 방법을 정리해보겠습니다. ​먼저 Yolo v5의 공식 깃허브 주소는 아래과 같습니다. 코드는 파이토치로 작성되었습니다.Yolo v5 github: https://github.com/ultralytics/yolov5 ultralytics/yolov5YOLOv5 in PyTorch > ONNX > CoreML > TFLite. Contribute to ultralytics/yolov5 development by creating an account on GitHub.github.com Yolo v5는 네트워크의 크기에 따라 다시 네 종류로 나뉩니다. v5s, v5m, v5l, v5x 네 종류의 네트워크가 순서대로 더 많은 가중치를 가지며, 성능도 이에 비례합니다. 출처: https://github.com/ultralytics/yolov5이 포스팅에서는 Yolov5x 네트워크를 기준으로 파인튜닝해보도록 하겠습니다. 다른 네트워크도 방법은 동일합니다. 환경 세팅도커를 이용해서 환경 세팅을 마친 컨테이너를 열 수도 있고, requirements를 설치해서 환경을 직접 세팅할 수도 있습니다. 순서대로 알아보겠습니다.​먼저 github에서 프로젝트를 clone합니다. 여기까지는 공통입니다. git clone https://github.com/ultralytics/yolov5 Docker로 환경 세팅하기먼저 Docker를 이용하는 경우입니다. 공식 repository에서 DockerFile을 제공하기 때문에 Image로 빌드해서 컨테이너를 시작해서 간단히 환경 세팅을 마칠 수 있습니다.​저는 아래와 같이 yolo라는 이름으로 이미지를 빌드했습니다.  sudo docker build --tag yolo:v5 . 여기서 잠깐.. 도커 명령어가 헷갈리신 분들은 다음 포스팅을 참고해주세요.https://blog.naver.com/baek2sm/221742329605 도커(Docker) 명령어 한 번에 정리하기이번 포스팅에서는 도커 활용에 필수적인도커 이미지 관련 명령어와 컨테이너 관련 명령어를 정리해보겠습...blog.naver.com 이제 빌드한 이미지로 컨테이너를 실행하겠습니다. 참고로 저는 ipc 옵션을 설정해야 메모리 문제가 발생하지 않았습니다. 그리고 jupyter notebook에서 작업하기위해 8888 포트를 포워딩해주었습니다. sudo docker run -it --gpus all --ipc=host --name yolov5 -p 8888:8888 yolo:v5 requirements로 환경 세팅하기구글 코랩(Colab) 등에서 코드를 실행할 때는 requirements 파일을 이용해서 환경을 세팅하면 간단하게 세팅을 마칠 수 있습니다. 다음 명령어 한 줄이면 됩니다. pip install -qr requirements.txt 데이터 준비하기먼저 학습을 위해 이미지와 레이블이 필요하겠죠? 같은 이름으로 이미지 파일과 txt파일을 준비해주시면 됩니다. 레이블 파일은 멀티클래스 형태로 구성할 수 있으며 여러 클래스 정보가 있을 때는 줄 단위로 구분해서 입력하면 됩니다. 각 줄은 클래스, 중심 X좌표, 중심 Y좌표, 너비, 높이 순서로 공백으로 구분해서 정보를 입력하며 좌표와 너비는 0~1 사이의 범위로 입력하면 됩니다.  ex) 0 0.5 0.5 0.2 0.2  # 0번 클래스에 해당하는 오브젝트 정보 입력2 0.5 0.5 0.2 0.2  # 2번 클래스에 해당하는 오브젝트 정보 입력 데이터 경로는 다음과 같이 이미지와 레이블이 쌍을 이루도록 준비해주세요. 여러분의 데이터 이름이mydb라면.. 다음과 같은 형태로 디렉토리를 구성합니다. train 폴더에 실제 이미지 파일과 레이블 txt 파일이 위치하도록 해주세요.  ﻿- mydb/images/train- mydb/labels/train 검증 데이터는 다음과 같이 구성하겠습니다. val 폴더에 이미지와 레이블을 넣어주시면 됩니다. - mydb_val/images/val- mydb_val/labels/val﻿ 이제 데이터 정보를 yaml 파일에 작성합니다. mydb.yaml 파일을 만들고 다음과 같이 코드를 작성해주세요. dog, cat 두 개의 클래스가 있다고 가정한 예시입니다. # 학습, 검증 데이터의 이미지 경로 지정train: ./mydb/images/trainval: ./mydb_val/images/val# 클래스의 수nc: 2# 클래스 이름names:['dog', 'cat'] 위 예시와 같이 학습, 검증 세트의 이미지 데이터 경로를 지정합니다. 정해진 방식대로 데이터 디렉토리를 구성하면 labels 폴더는 알아서 찾기 때문에 레이블 경로는 지정하지 않아도 됩니다. 그리고 전체 클래스의 수와 각 클래스의 이름을 지정해줍니다. 이제 데이터 준비는 끝났습니다. 모델 학습시키기데이터를 준비하고, 데이터 정보를 입력한 yaml 파일이 준비됐으면 모델을 학습시킬 시간입니다. 파인튜닝할 것이기 때문에 먼저 다음 링크에서 pre-trained 모델을 다운로드합니다. v5 안에서도 다시 세부 버전이 나뉘는데, 이 글을 포스팅하는 21년 2월을 기준으로 v4.0이 최신 모델입니다. 저는 inference 속도보다는 정확도가 중요한 상황이었기 때문에 yolov5x.pt 파일을 다운로드했습니다.​https://github.com/ultralytics/yolov5/releases/tag/v4.0 Release v4.0 - nn.SiLU() activations, Weights & Biases logging, PyTorch Hub integration · ultralytics/yolov5This release implements two architecture changes to YOLOv5, as well as various bug fixes and performance improvements. Breaking Changes nn.SiLU() activations replace nn.LeakyReLU(0.1) and nn.Hards...github.com 이제 모델을 학습시켜봅시다. 다음 명령어 한 줄이면 됩니다. python train.py --img 640 --project ./result/ --cfg ./models/yolov5x.yaml --weights ./yolov5x.pt --data ./mydb.yaml --epochs 5 --batch 12 --img: 이미지 크기 지정--project: 학습 결과가 저장될 경로 지정--cfg: yolo 아키텍처 정보를 담은 yaml 파일 경로(여기서는 yolov5x 사용)--weights: finetuning하는 경우 pre-trained 모델의 가중치 파일 경로--data: 데이터 세트 정보를 담은 yaml 파일 경로--epoch: 에포크 수--batch: 배치 수 학습된 모델로 detection하기detect.py 스크립트를 이용해서 학습된 모델로 Object Detection을 할 수 있습니다. inference할 이미지는 특별한 규칙 없이 그냥 한 폴더에 이미지를 준비하면 됩니다. 다음 명령어 한 줄로 Object Detection을 실행할 수 있습니다.  python3 detect.py --source 이미지폴더경로 --weights 모델파일경로 --conf 0.4 --source: inference할 이미지 폴더 경로를 지정합니다--weights: 학습된 모델 경로를 지정합니다.--conf: confidence가 몇 이상인 경우 유효한 object로 간주할지 threshold 값을 설정합니다.​Detection 결과는 ./runs 폴더 하위 경로에 저장됩니다.​만약 이미지 말고 텍스트로 결과 저장하고 싶으면 save-txt 플래그를 설정하고,confidence 값도 함께 저장하고 싶으면 save-conf 플래그도 함께 설정하면 됩니다.​결과 이미지는 아래와 같은 형식으로 저장됩니다. Yolo v5 Object Detection 결과, 출처: https://github.com/ultralytics/yolov5​ "
Opencv : TensorFlow Object Detection ,https://blog.naver.com/codingteacher/222979994323,20230110,​https://github.com/opencv/opencv/wiki/TensorFlow-Object-Detection-API TensorFlow Object Detection API · opencv/opencv WikiOpen Source Computer Vision Library. Contribute to opencv/opencv development by creating an account on GitHub.github.com https://github.com/opencv/opencv_extra/tree/master/testdata/dnn opencv_extra/testdata/dnn at master · opencv/opencv_extraOpenCV extra data. Contribute to opencv/opencv_extra development by creating an account on GitHub.github.com https://github.com/tensorflow/models/tree/master/research/object_detection/samples/configs models/research/object_detection/samples/configs at master · tensorflow/modelsModels and examples built with TensorFlow. Contribute to tensorflow/models development by creating an account on GitHub.github.com ssd_mobilenet_v3_small_coco_2020_01_14​https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf1_detection_zoo.md#user-content-fn-1-ed5448667102f3feb9db29870747f3d8 models/tf1_detection_zoo.md at master · tensorflow/modelsModels and examples built with TensorFlow. Contribute to tensorflow/models development by creating an account on GitHub.github.com https://medium.com/analytics-vidhya/training-a-model-for-custom-object-detection-tf-2-x-on-google-colab-4507f2cc6b80 Training a model for custom object detection (TF 2.x) on Google ColabMy Youtube Video on this!medium.com 나만의 object detecthttps://towardsdatascience.com/custom-object-detection-using-tensorflow-from-scratch-e61da2e10087 Custom Object Detection using TensorFlow from ScratchCustom Dataset Training for Object Detection using TensorFlow | Dog Detection in Real time Videos | Perfect Guide for Object Detectiontowardsdatascience.com ​ 
object detection  ,https://blog.naver.com/petraeye/222753266612,20220531,"#object #detection​Object detection은 image 안의 여러 object (물체)들을 감지하고, 그 물체가 어떤 것인지 (사람인지, 나무인지) classification 하는 작업이다. ​Object detection 은 아래 예제와 같이, class만 예측하던 image classification task와 달리 bounding box 의 위치도 detection해야 한다. 1) bounding box 위치: object가 위치하는 영역 (좌측 상단, 우측 하단 좌표 또는 좌측 상단 좌표, Witdh, Height)2) bounding box내 물체의 종류: classification 사람인지, 자동차인지.. ​ 출처: Object Detection in 20 Years: A Survey ( https://arxiv.org/pdf/1905.05055.pdf  )2012년 AlexNet과 GPU의 발전으로 Object Detection 은 deep learning을 기반으로 하여 발전하였다. ​Deep learning based Detection methods는 크게 One-stage detector와 Two-stage detector로 나뉜다. ​bounding box의 위치를 먼저 찾고 (Region Proposal Network), 그 위치내 feature map을 가지고 classification 을 수행하여 두단계로 진행하는 것이 Tow-Stage detector이고 RCNN, Fast-RCNN, Faster-RCNN이 있다. ​여기서 좀더 inference 시간을 단축시키기 위해 bounding box의 좌표와 classification을 한 Stage에서 예측하는 방법론이 ONe Stage detector이고 여기에는 YOLO, SSD, RetinaNet 등이 있다. ​​​ "
Mask DINO : Towards a Unified Transformer-based Framework for Object Detection and Segmentation ,https://blog.naver.com/sounghyn105/222963435250,20221223,"기존 DINO모델에다가 segmentation task를 위해 mask prediction branch를 더한 모델쿼리 + 고해상도의 feature 결합 -> mask detection 용이panoptic segmentation, instance segmentation, semantic segmentation에서 우수한 성능을 보여줌cf) panoptic segmentation : 인스턴스 id와 class를 모두 구별함 (id + class)​​​기존 method의 장 단점 분석장점- DINO : global information 잘 뽑아냄- Mask2Former: 픽셀 단위의 로컬 정보 잘 뽑아냄​Mask2Former가 detection이 안좋은 이유:- content 쿼리랑 positional 쿼리랑 융합이 안됨- masked attention 떄문에 다양한 scale의 attention을 잡는데 실패함​DETR가 detection이 안좋은 이유 DETR head에서 small feature를 곱해주기 때문​DINO가 detection이 안좋은 이유:denoising, query를 만들고 선택하는데 탁월한 성능을 보이고, region-level에서 feature를 잘 뽑지만 segmentation에 그닥 중요한 것은 아니다. object detection은 region-level이어도 커버가 충분히 되지만, segmentation은 pixel 단위까지 나아가야하기 때문에 더 까다로운 task다.​ 위의 표를 보았을때, MaskFormer는 segmentation task, DINO/DETR은 detection task에 최적화 되있음을 알 수 있다. 또한, 좀 더 general한 모델을 만드려고 기존 모델을 변경하여 따로 head를 구성하는 방법은 성능을 급격히 저하시켰다.​​​전체적인 모델은 위의 그림과 같다. DINO와 달라진 점은 (빨간 색 부분)2x upsample (기존 이미지의 1/8 수준의 feature)와 backbone을 합쳐서 여러 scale의 feature가 들어가도록 하였다.기존보다 고차원의 featuremap을 쿼리에 곱한다. 또한, anchor 쿼리 뿐만 아니라 content 쿼리를 포함하였다. 이렇게 되면 region별 정보가 디코더에 잘 전달 될 수 있을 것 같다.  (cf. anchor point 생성 -> box를 생성)Box head를 담당하는 것과 Segmentation을 담당하는 branch를 따로 만들었다. Box를 prediction한 다음에 이것을 토대로 segmentation을 하지만, 모델에서 두가지를 담당하는 branch는 따로 만들었다. Segmetation에서는 box를 segmentation에서의 노이즈로 보고, 여기서 segment를 추출하는 과정(Unified Denoising)을 학습시킨것이다.Hybrid matching을 통해 Box branch와 segmentation을 만드는 브랜치의 결과물들을 종합해야한다. 이 매칭을 제대로 시키기 위해서 box와 mask match loss를 따로 만들었다. 'Stuff' category는 box prediction loss에서 뺌 -> Stuff category는 하늘, 벽과 같이 이미지 전체를 커버하는 큰 부분을 뜻한다. 모델을 통해 box를 예측하지만, 이를 loss에 반영하지는 않는다. 박스 크기가 불규칙적이고, Box가 너무 크면 이것이 loss값을 크게 변형시키기 떄문인 것 같다, 저자는 이것을 decoupled box prediction이라 하였다.​Experiments Instance/Semantic/Panoptic Segmentation에서 모두 좋은 성능을 보여주었다.Backbone은 Swin Transformer나 Resnet을 이용하였다. 기존의 SOTA 모델보다 더 우수한 성능을 보여주었다.  또한 쿼리 수를 변형하여 detection과 segmentation 성능을 측정하였다. 또한, FPS와 GFLOPS를 측정하여 모델의 정확도와 속도까지 측정하였다. 데이터셋은 COCO dataset을 이용하였다. 여기서 Resnet은 4 scale feature, SWin은 5scale feature를 이용했다고 하는데, 이것은 위의 모델에서 봤듯이 1/8, 1/16, 1/32 ... 와 같이 5개의 feature map을 추출하여 multi-scale attention을 만든 것을 의미하는 것 같다. "
Real-time Object Detection Without Machine Learning ,https://blog.naver.com/nanan75/221808232685,20200214,"https://towardsdatascience.com/real-time-object-detection-without-machine-learning-5139b399ee7d Real-time object detection without machine learningEarlier this year Nick Bourdakos, a software developer at IBM posted a series of videos demoing real-time object detection in a web…towardsdatascience.com   demo웹캠으로 테스트 해본건데인식률이 좋아요..​이것 저것 테스트 해봤는데..kinnect, AR이랑은 또 다른 재미가..ㅎㅎ..​진직할걸..​관련 프로젝트 꼭 한번 해 보고싶다..​  ​ "
Object Detection in 20 Years: A Survey ,https://blog.naver.com/kangdy1997/222684421675,20220327,"Zhengxia Zou, Zhenwei Shi, Member, IEEE, Yuhong Guo, and Jieping Ye, Senior Member, IEEE From:https://arxiv.org/pdf/1905.05055v2.pdf​​I always posted object detection algorithms but have never introduced how object detection technology is develped​ The  number of how many books are published for few years​so in this time I introduce the tendency of the object detection technology​Introduction1. The meaning of the objcect detection is detecting what is this object and where is it​2. It use various methods (for ex) instance segmentation, image captioning, object tracking)​3. Object Detection has been developted for many years in various ways​4. It especially use in the automonous driving, robot vision and computer visions.​ This picture shows how object detection technology has been developted for 20 years​* After DPM method object detection usually use deep learning based methods​*One-stage detector is finding localization and classification in a same time(localization: find the object's location, classification: detect what the object is)​*Two-stage detector is finding localization and classification sequentially​*One-stage is faster than two-stage but accuracy is lower than two-stage​*Representatively one-stage detectors are YOLO and SSD lines and two-stage is R-CNN lines​ (from:hoya012 github)PerformanceThe table above is the performance table but it's not accurate cuz every computer specs different​ The type of datasetsIn object detection usually use PASCAL VOC datasets, ImageNet datasets, COCO datasets, and Open Images datasets​​​After 2019 object detection flows*EfficientDet(2019.11) use EfficientNet for backbone Network​*YOLOv4 is lower performance than EfficientDet but better than v3​*YOLOv5 made AP and FPS higher than v4​*Scaled YOLOv4's performance got better than EfficientDet's AP and FPSSo both FPS and accuracy got better model is appeared ​*YOLOR(2021.5) is no performance improving but it's good trying to represent scaled YOLOv4 ​*End-to-end object detection with Transformers(DERT)(2020.3) is the first trying for use Transformer method which is always used in NLP​*Deformable DERT(2020.10) use deformable attention module so that is improved performance​*ViT(2020.10) tried replace CNN backbone to Transformer but model is way too big so We should custom data really big​*DeiT(Data-efficient Image Transformer)(2020.12)Using different training methos with CNN methods while training vision transformer lines​Optimization, augmentation and generalization datasets  using knowledge distillation methodso that we don't need to make the datasets bigger and also performance is better than EfficientDet​*YOLOS(just name is yolo not related with yolo lines)(2021.6)Using vision transformer by backbone but it's failed performance is lower than others​*Since 2021 tranformer lines are adopted rather than CNN lines​*Swin transformer(2021.3) (Read my post)​*Soft Teacher+Swin-L(2021.6)Based on Pseudo label and Mix with Semi-Supervised Learning methods​*Dyhead(2021.6)Swin transformer based is little bit lower performance than Soft Teahcer+Swin-L​*Florence-CoSwin-H(2021.11)This model's AP is 62.4 and it is SOTA model​ This picutre above is the recently object detection's performance with COCO datasets "
[object detection 시리즈] YOLOv3 리뷰! ,https://blog.naver.com/koreadeep/222686018494,20220329,"안녕하세요. 이번 포스팅에서는 YOLO 시리즈 중에서도 가장 유명한 YOLOv3에 대해 알아보도록 하겠습니다.​ YOLO v3 ArchitectureYOLOv3 모델을 제안하는 논문에서 모델 architecture에 대해 설명하는 부분부터 한번 살펴보겠습니다. The first detection is made by the 82nd layer. For the first 81 layers, the image is down sampled by the network, such that the 81st layer has a stride of 32. If we have an image of 416 x 416, the resultant feature map would be of size 13 x 13.One detection is made here using the 1 x 1 detection kernel,giving us a detection feature map of 13 x 13 x 255YOLOv3 논문82번째 layer에서 나온 결과로부터 첫번째 detection을 진행하며,81번째 layer에서 이미지는 네트워크에 의해 stride 32로 down sampling 된 결과를 얻는다고 설명하고 있습니다.예를들어 416x416 크기의 입력 이미지 주어진다면, feature map 의 크기는 13x13 이 되게 됩니다.​채널의 수는 data가 80개의 클래스로 이루어진 coco data인 경우, bound box의 속성 5가지(x좌표, y좌표, width, height, confidence score) 와 class수(80개) 의 합에 bound box의 개수(3)을 곱하여 (5+80)x3 = 255 가 됩니다.  Then, the feature map from layer 79 is subjected to a few convolutional layersbefore being up-sampled by 2x to dimensions of 26 x 26. This feature map is then depth concatenated with the feature map from layer 61. Then the combined feature maps is again subjected a few 1 x 1 convolutional layers to fuse the featuresfrom the earlier layer (61).Then, the second detection is made by the 94th layer yielding a detection feature map of 26 x 26 x 255.YOLOv3 논문두번째 detection은 94번째 레이어에서 진행된다고 합니다.91번째 layer에서 나온 feature map과 36번째 feature map을 concatenate(결합)하고2배 upsampling을 진행하여 크기가 가로/세로로 각각 2배씩 확대되어26x26크기 feature map이 된다고 합니다.마찬가지로 1x1 convolution을 통해 dimension reduction하여 255 채널 크기로 맞춰준다고 합니다.  A similar procedure is followed again, where the feature map from layer 91 is subjected to fewconvolutional layers before being depth concatenated with a feature map from layer 36. Like before, a few 1 x 1 convolutional layers follow to fuse the information from the previous layer (36).Then, make the final of the 3 at 106th layer yielding feature map of size 52 x 52 x 255YOLOv3 논문마지막으로 세번째 detection은 106번째 layer에서 진행됩니다.52x52크기의 feature map에서 detection을 진행한다고 합니다.​이해를 돕기 위해 아래의 시각자료를 통해 yolo v3 network architecture에 대해 좀 더 자세히 살펴보겠습니다. 79번째 layer까지 convolution 연산을 진행한 후 branch하여3회 convolution을 더 진행하여 1/32배 크기로 줄어든(크기가 가장 많이 줄어든) feature map에 대해 detection을 진행하므로 large object를 검출하는데 강점이 있다고 할 수 있습니다.한편 branch 이전 feature map을 2배 up-sampling하여 동일한 크기를 가진 feature map인 61번째 layer의 feature map과 concatenate을 하게 됩니다.​Up-sampling 한 결과와 이전 layer를 결합함으로써 작은 object를 찾아내기위한 fine grained feature를 preserve할 수 있다고 설명하고 있습니다.​ The up-sampled layers concatenated with the previous layers help preserve the fine grained features which help in detecting small objectsYOLOv3 논문그리고 convolution 연산을 지속한 후, 91번째 layer에서 두번째 branch를 하게 됩니다.91번째 feature map을 기준으로 3회 convolution을 진행하여 1/16배 크기로 줄어든 feature map에서 medium 크기의 object를 검출하는데 사용됩니다. ​마찬가지로 91번째 layer의 feature map을 2배 up-sampling하여 52x52 크기의 36번째 layer 와 concatenate를 진행하게 됩니다. The 13 x 13 layer is responsible for detecting large objects,whereas the 52 x 52 layer detects the smaller objects,with the 26 x 26 layer detecting medium objects.YOLOv3 논문FPN (Feature Pyramid Network) structure이러한 구조에 영향을 준 아이디어는 FPN(Feature pyramid network) 구조 입니다.다양한 크기의 물체를 검출할 수 있도록 CNN의 여러 레벨에서 추출된 특징들을 종합하는 multi-layer feature fusion기법 중 하나입니다.FPN에서는 level 3~7의 feature 들을 더 상위 레이어에 있는 작은 크기의 feature들을 더 작은 level에 있는 feature 크기만큼 확대하는 resize를 진행한 뒤 ,둘을 더하는 top-down방식으로 feature들을 합치는 것입니다.​아래 그림을 보면P7이라는 입력이 convolution 연산을 거쳐 p7 out이 나오고,이것을 resize하여 p6의 입력과 결합하여 p6 out을 얻는 방식으로 진행되게 됩니다. ​​ None Softmax또 다른 특징으로는 soft max activation 함수를 사용하지 않았다는 점 입니다.Softmax는 특성상 각 class들이 서로 exclusive하다는 가정을 하고 사용됩니다.마지막 layer에서 독립적으로 연산 되어 얻어지는 확률 값들을 Exponential 함수를 사용하여 전체 합에 대한 상대적 비중으로 표현하는 것이기 때문입니다. (아래 수식 참조) 그러나 object detecion을 하나의object가 다른 object에 포함 되는 경우도 있습니다.예를 들면, woman이라는 object가 있고 동시에 human이라는 object도 존재하기도 합니다. 이럴 경우 두 가지 모두 해당 된다고 할 수 있기 때문입니다.​대신 각 class score는 logistic regression을 통해 예측되고, threshold를 기준으로 filtering하여 다수의 object를 prediction하게 됩니다.​ Number of Bound boxesYolov2는 5개의 bound box를 사용했지만, yolo v3는 feature map당 3개의 bound box를 예측합니다.Feature map당 bound box수는 더 적지만, feature map이 3종류인데다가, feature의 크기도 다양하기 때문에 훨씬 더 많은 bound box를 예측하게 됩니다.​예를들어 416크기의 입력이 주어질 경우Yolov2의 경우에는 feature map의 크기 13x13와 각 grid 당 bound box 수 5를 곱한 수 만큼 13x13x5=845 개의 boudn box를 예측하나,Yolov3는 각각 첫번째 detection 에서 13x13x3 = 507 , 두번째detection에서 26x26x3 = 2,028 ,세번째 detection에서 52x52x3=8,112 으로총 10,647개의 bound box를 예측하는 것입니다.Yolo v2대비하여 약 10배이상의 bound box를 예측합니다​ Darknet 53Yolov3에서는 yolov2에서 사용한 darknet19 back bone 성능을 개선하여Darknet53을 사용했습니다.Darknet 53은 이름에서도 유추해볼 수 있듯이, 53개의 convolution layer를 포함하고 있습니다.Yolov2에 비해 yolov3의 inference time이 느린 주요한 이유이기도 합니다.​Darknet53의 구조는 아래와 같습니다. 네모 박스로 표시된 부분은 residual block입니다.앞에 곱해진 숫자만큼 연속 수행된다고 이해하시면 됩니다.예를들어 입력 이미지가 주어지면 가장 먼저 2회의 convolution을 거친 후,1회의 Residual block 을 거치게 됩니다이 residual block안에서 2회의 convolution연산을 수행 및 short cut connection을통해 residual 학습을 진행하게 되는 것입니다.이후 convolution을 1회 수행 후2회 연속으로 residual block을 거치는 방식으로 진행됩니다.​성능적인 측면에서 보면 Classification 모델로써 darknet 53은 resnet101에 비해 더 나은 성능을 보이면서도,약 1.5배 빠르다는 특징이 있습니다.(resnet152와도 유사한 성능을 보이면서도 2배 정도 빠릅니다) Conclusion결론적으로 yolo v3에서는 다양한 크기의 object를 검출하기 위해 이러한 구조로 설계를 하였고, yolo v1,v2에서 주요 문제로 지적되었던 small object를 잘 detection하지 못하는 문제를 개선하게 되었습니다.아래 그림에서 보시는 것처럼 전반적으로 높은 mAP수치를 보이면서도 압도적인 inference time을 보여 주고 있습니다 ​​그럼 이상으로 포스팅을 마치도록 하겠습니다:) 감사합니다.​  한국딥러닝(주)는 인공지능, 머신러닝, 딥러닝 솔루션을 개발하여 세상이 해결할 수 없는 문제를 해결하고 제안하고 있습니다. 끊임없이 발생되는 새로운 빅데이터와 응용 기술의 홍수 속에서 최신 인공지능 기술 동향을 예측하고 빠르게 반영하는 한국딥러닝을 만나보세요.​ ​ "
[논문 리뷰] Rich feature hierarchies for accurate object detection and semantic segmentation ,https://blog.naver.com/tnsgh9603/222679677695,20220322,"Background 이미지가 주어졌을 때, 이미지에 해당하는 object가 무엇인지 판별하는 task가 classification추가로 object가 어디에 위치해 있는지 적절한 bounding box를 예측해주는 task가 localization​여러 가지 객체의 적절한 bounding box와 object의 classifiation을 하는 task가 object detectionbounding box가 아닌 object의 boundary를 예측하는 task가 instance segmentation​R-CNN의 경우 selective search 기법을 사용하여 여러가지 후보 지역을 만든다. 고정된 길이의 output을 얻기 위해, 만들어진 각각의 후보 지역 위치에 대해 특정 사이즈로 warp 시켜 입력층을 같게 맞춰줍니다. 그 뒤, CNN 구조를 거치도록 하고, 그 결과로 분류를 하게 되는 모델이다.  CNN이 본격적으로 visual recognition task에 사용되기 이전에는 sift나 Hog 다른 기법으로 feature를 추출하여 classification 문제를 해결했다.2012년 AlexNet을 기반으로 ILSVRC에서 큰 성능 향상을 달성ILSVRC의 classification result를 PASCAL VOC Challenge의 object detection task에 확장하고자 연구가 진행됨본 논문에서는 CNN을 사용하여 기존 시스템에 비해 PASCAL VOC에서 좋은 성능을 보임본 논문에서 집중한 두 문제 상황에는1. deep network를 통한 localization2. 적은 양의 data로 모델을 학습시키는 것​파스칼 VOC 챌린지는 컴퓨터 비전 분야의 기술 중 물체 클래스 인식(object class recognition) 기술을 겨루는 국제대회​ region proposal algorithm(selective serach)을 기반으로 한 region에 대해, pre-training된 CNN 구조를 통해 고정된 길이의 feature를 추출하여 linear SVM(Soft Vector Machine)에 학습(classification 문제) + bounding box regression 학습(localization 문제)- class 별로 분류기가 존재- bounding box regression은 localization 성능 향상을 위한 것이다ILSVRC 데이터 중 classifcation에 대한 문제를 먼저 학습함으로써 성능 향상을 이끌어냄PASCAL VOC의 object detection task를 위한 fine-tuning을 따로 진행하여 매우 큰 성능 향상을 이끌어냈다.  Selective Search 1. 모든 영역에 대해 다양한 scale의 region 후보들을 만들어낸다.2. region들에 대해 color, texture, size, fill의 값을 계산한다.(fill은 특정 box와 box가 합쳐졌을 때 만들어지는 큰 후보 box가 있을 때, 큰 후보 box와 각 box간의 차이들을 모두 합한 값들입니다)3. 이웃하는 두 region 사이의 유사도를 구한다.4. 유사도가 높은 것부터 차례대로 merge하여 2000개를 구성한다. ​CNN은 일정한 크기의 input이 필요하므로 2000개의 후보 지역을 일정한 크기의 이미지로 변형시킨다.-> 이 과정에서 정보들이 왜곡될 수 있기 때문에 성능이 저하될 수 있다.이미 학습된 CNN 구조를 통해 고정된 길이의 feature vector을 추출한다.(classification layer 이전의 결과를 활용 : 4096차원 벡터)이 과정을 feature extraction module 이라고 한다.​ feature vector를 linear SVM으로 class별 score를 계산하여 classification하는 부분feature vector를 통해 bounding box를 예측하는 regressor 부분-> regressor bounding box와 실제 ground truth bounding box와의 거리를 최소화하도록 학습​proposed box : 네트워크에서 제시한 boxT는 G와 P의 차이를 나타냅니다. 우선, 둘의 차이를 다양하게 거리와 비율개념으로 얻어 낸 후, 이를 최소화 하는 방향으로 w를 학습시킵니다.-> 즉, W가 box를 만들어내는 가중치이기 때문에, 실제 box와 유사한 box를 propose할 수 있도록 regressor를 학습시킵니다.  Fine-tuning​PASCAL VOC의 object detection task에 맞게 classification layer를 object class 개수 + background로 만들어 줍니다.(object가 아무것도 없는 이미지에서 이를 background로 판별하기 위한 과정)positive sample : SS로 만들어낸 region과 할당된 object의 실제 box 사이의 IoU 값이 0.5 이상인 것positive sample : negative sample을 1:3의 비율로 SGD 학습 IoU는 교집합/합집합의 비율이라고 생각하면 됩니다.​Object catagory classifiers- training data가 너무 크기 때문에 class 별로 linear svm을 구성해서 학습을 진행- class 별로 training 하기 위해 앞에서와 다르게 positive sample과 negative sample을 선정- positive sample은 : class별 object의 ground-truth box- negative sample은 : SS로 만들어낸 region과 할당된 object의 실제 box 사이의 IoU 값이 0.3미만인 것위와 같이 준비한 sample을 앞 페이지에서 구성한 fine-tuned CNN으로 얻은 feature vector로 학습한다.  RCNN이 SS를 사용하는 UVA나 Regionlets보다 더 좋은 성능을 보인다.그리고 bounding box regression을 활용하면서 학습한 모델이 더 좋은 성능을 보이고 있다.​ 오른쪽 그림은 class별로 얻어낸, precision value의 분포를 나타내는 box poltRCNN의 box가 조금 더 위에 위치해있다.​Visualizing learned features CNN이 어떤 것을 학습했는지 시각화하는 것이 목적 : classification layer 이전 단에서 feature map에 대해 특정 element를 기준으로 내림차순 정리하여 region들을 시각화​첫 번째 줄을 보면, CNN의 구조가 사람의 얼굴을 기준으로 classification을 하는 방향으로 학습되었다는 것을 알 수 있다.​ Fine-tuning 여부에 따른 비교와 bounding box regression 학습 여부에 따른 성능 차이를 나타냈다.baseline인 DPM과 비교하였을 때, 표의 수치인 average precision 값이 매우 우월했고, fine-tuning을 할수록 좋은 성능을, 그리고 bounding box regression 학습을 할수록 좋은 성능을 나타냈다.​ ablation study,  https://blog.naver.com/ballade8/222082617131​ O-net이 VggNet, T-net이 AlexNet을 의미하는 것 같았다.O-net 즉, VggNet에 적용한 결과가 더 우월했다.그 이유는 VggNet의 복잡도가 T-Net인 AlexNet보다 크기 때문에, representational power가 좋기 때문이다.​ segmentation에 맞는 feature를 계산하기 위한 3가지 전략1. region의 shape를 무시하고 warped된 형태에 바로 feature를 계산해내는 것(사각형태만 고려) -> full2. region의 배경을, 형태가 알 수 없도록 input의 mean으로 처리해주는 것 -> fg3. 위의 두 가지를 적절히 섞은 것 : full + fg​baseline인 Regions and parts 방법과 second-order pooling 방법에 비해, 대부분의 class에서 좋은 성능을 보이고 있다.(항상 좋은 건 아님)  Conclusions​R-CNN의 장점ㅇ 연산 속도면에서는 후에 나오는 overfeat에게 밀리지만, detection task에 있어서 2배 이상 성능이 향상 되었다.- region proposal algorithm으로부터 얻은 결과에 represntational power가 좋은 CNN을 적용했기 때문- pre-training된 모델을 사용했기 때문- detection task라는 domain에 특화시킨 fine-tuning을 했기 때문​R-CNN의 단점- 고정된 size의 image가 input으로 들어가면서 정보가 손상된다.- 200개의 region에 대해 각각 CNN을 수행하기에 실제 학습시간이 너무 길다  Reference​https://www.youtube.com/watch?v=X4TxIPuYu0E ​https://arxiv.org/abs/1311.2524 Rich feature hierarchies for accurate object detection and semantic segmentationObject detection performance, as measured on the canonical PASCAL VOC dataset, has plateaued in the last few years. The best-performing methods are complex ensemble systems that typically combine multiple low-level image features with high-level context. In this paper, we propose a simple and scalab...arxiv.org ​ "
"[CV] Object Detection Metric - IoU, mAP ",https://blog.naver.com/dign7984/222884868067,20220926,"컴퓨터 비전(computer vision)cv는 인간의 시각과 관련된 부분을 컴퓨터 알고리즘을 이용해서 구현하는 방법을 연구하는 분야이미지 분류(image classification, segmantic image segmentation, object detection 등의 컴퓨터 비전의 대표적인 문제컴퓨터 비전 문제를 풀기 위해선 딥러닝 여러 구조 중 CNN이 많이 사용됨.​컴퓨터 비전 문제 영역 - object Detectionobject Detection : 물체가 있는 영역의 위치 정보를 bounding box로 찾고 bounding box내에 존재하는 사물의 라벨(label)을 분류하는 문제 영역​Object Dectection 문제 영역의 출력값 - x_min, y_min, x_max, y_max, class, confidencex_min : 물체의 bounding box의 왼쪽 위(left-top) x좌표y_min : 물체의 bounding box의 왼쪽 위(left-top) y좌표x_max : 물체의 bounding box의 오른쪽 아래(Right-Bottom) x좌표y_max : 물체의 bounding box의 오른쪽 아래(Right-Bottom) y좌표class : bounding box에 속한 물체의 classconfidence : bounding box에 실체 물체가 있을 것이라고 확신하는 정도를 나타내는 값(0.0~1.0 사이의 값)​Open Image Dataset Ground TruthObject Detection 문제 영역의 Ground Truth 데이터는 사람이 지정한 Bounding Box와 Class Label 정답 Ground Truth 데이터와 비교했을 때, 예측 값의 성능을 비교하기 위한 metric이 필요​Metric 1 - Intersection over Union(IoU) MetricIoU은 1개의 Bounding Box와 1개의 Bounding Box가 얼마나 일치하는지를 0.0~1.1사이 값으로 표현2개의 bounding box가 일치할 수록 1.0에 가까운 값이 되고, 일치하지 않ㅇ르수록 0.0에 가까운 값이 나오게 됨.  Metric 2 - Precision, Recall, F1Precision : 정밀도(Precision)는 검색된 결과들 중 관련 있는 것으로 분류된 결과물의 비율Recall : 재현율(Recall)은 관련 있는 것으로 분류된 항목들 중 실제 검색된 항목들의 비율 F1 : Precision과 Recall의 조화평균. Precision과 Recall을 한번에 비교 가능  f1 score가 높은 모델일 수록 결과가 좋다고 할 수 있다.  Metric 2 - Average Precision(AP)Positive 판단 기준 : 일정한 임계치의 IoU(예를 들어, Pascal VOC 데이터셋의 경우, 0.5)를 넘기면 맞춘 것으로 간주 Average Precison(AP) : Recall별 Precision의 평균(confidence가 높은 예측결과 순으로 정렬했을 때, 몇번째 이미지까지를 비교대상으로 삼을 것인가?)Mean Average Precision(mAP) : class들의 Average Precision의 평균 ​​참고 : https://jonathan-hui.medium.com/map-mean-average-precision-for-object-detection-45c121a31173 mAP (mean Average Precision) for Object DetectionAP (Average precision) is a popular metric in measuring the accuracy of object detectors like Faster R-CNN, SSD, etc. Average precision…jonathan-hui.medium.com ​ "
[object_detection] 잘 쓴 마스크와 잘못 쓴 마스크 판별하기 ,https://blog.naver.com/yhjo029/222641505825,20220207,"지난 포스팅에 잘 쓴 마스크와 잘못 쓴 마스크 판별하는 모델을 만들어 분류를 해보았는데요.https://blog.naver.com/yhjo029/222627052867 [머신러닝] 잘 쓴 마스크와 잘못 쓴 마스크 판별하기오늘은 잘 쓴 마스크와 잘못 쓴 마스크를 분류해볼게요 GitHub - cabani/MaskedFace-Net: MaskedFac...blog.naver.com 사실 지난번에 한 문제는 classfication에 관한 문제였습니다.무슨 말이냐면 locating이 된 얼굴 이미지 데이터들을 가지고 마스크 상태를 판별했기 때문에단지 마스크를 잘 썼다, 잘못 썼다라고만 분류하면 됐었죠.​​하지만 일반 사진에 여러 사람들이 실제 마스크를 썼는지 안썼는지 판단하려면각각 사람들의 얼굴에 bounding box를 하기 위한 locating 과정을 먼저 거쳐야 합니다.​오늘은 yolo v5를 통해 locating 과정을 거치고 classfication 문제를 해결해보도록 하겠습니다.​데이터는 https://models.roboflow.com/object-detection Open Source Computer Vision Object Detection ModelsState of the art object detection models to localize subjects in images. From YOLOv5 to MobileNet, we have the most popular models in easy to use formats.models.roboflow.com  이 사이트에 가셔서 데이터 파일을 yolov5 pytorch로 다운받으시면 됩니다.저는 마스크 데이터를 다운받았습니다.​​데이터를 다운받으시면 dataset에 image와 labels를 볼 수 있습니다. 훈련 데이터 1라벨링된 txt​fine tunning을 하기 위해선 훈련할 사진과 이를 labeling한 데이터가 필요합니다.라벨링된 .txt 첫줄을 보시면0 0.0515 0.025678650036683785 0.071 0.049889948642699924라고 되어 있습니다. 띄어쓰기 기준 스플릿해서 보시면0 //  0.0515 // 0.025678650036683785 // 0.071 // 0.049889948642699924총 5개의 데이터가 있는데요. ​먼저 1번째 데이터는 class를 분류하는 것입니다.(classfication)제 모델은 마스크를 썼는지 안썼는지 분류하기 때문에 0, 1로 이를 판단하였습니다.​2번째 3번째 4번째 5번째 데이터는 boung box에 관한 정보로 x, y, width, hegith 입니다.(locating)​이 bounding box와 class의 정보가 라벨 txt에 들어가 있습니다.이는 정답 데이터로 ground truth라고도 하죠​이미지와 label.txt의 여러 파일을 통해 학습을 진행할 수 있습니다.​​https://github.com/ultralytics/yolov5 GitHub - ultralytics/yolov5: YOLOv5 🚀 in PyTorch > ONNX > CoreML > TFLiteYOLOv5 🚀 in PyTorch > ONNX > CoreML > TFLite. Contribute to ultralytics/yolov5 development by creating an account on GitHub.github.com 다음 yolov5 github 사이트에 들어가셔서 설명을 참고하며(train custom data)train.py로 방금 다운받은 데이터의 학습을 진행합니다.​ 저도 이미지와 label.txt를 통해 학습을 진행하였습니다.참고로 mask를 쓰고있는 사진 위주이며 데이터가 총 84장이여서no_mask mAP0.5가 0.164로 낮았습니다.​이제 val data들로 detect.py파일에 훈련에 저장된 파라미터를 사용하여 마스크 판별 object detection을 수행해보겠습니다.  bounding box와 class분류가 잘 수행된 것을 볼 수 있습니다.​​https://www.xportsnews.com/?ac=article_view&entry_id=783615마지막으로 위 링크에 방탄 소년단 사진으로 테스트 해보겠습니다. 적은 데이터이지만 좋은 성능을 보이며, 또한 처리가 가벼운 yolo v5로 실제 구현해보았습니다.​다음 시간에는 코드 구현을 통해 같이 해보겠습니다. "
Object detection(5) _Two-stage 2D object detectors: Faster R-CNN[NIPS'15] ,https://blog.naver.com/summa911/222994186468,20230125,"R- CNN의 한계점​1) 이미지 하나당 2000번의 CNN 과정이 필요하다. 2) input data를 warping 해야한다. +3)Selective search 방법은 CPU를 써야해서 , GPU를 통한 빠른 연산이 불가능하다.​--> CNN을 한 번만 학습시킬 수 있다면 시간을 줄일 수 있음. --> 임의 사이즈 데이터를 input 할 수 있다면 wrapping을 안해도 됨. --> accuracy증가​--> GPU를 이용한 fast region proposal 고안. ​Faster R-CNN​Fast R-CNN 에서, region proposal 방법을 selective search가 아니라 별도의 딥뉴런 모델을 만들어냄.​​1) Region proposal network(RPN) 2) 위치 연산 CPU -> GPU / sliding window instead of selective search​selective search는 original image에서 위치 정보를 찾지만, Conv feature map을 다 뽑은 상태에서 box proposal하도록 training하면 더 좋을 것이라는 아이디어.Conv feature map을 다 통과하고 나면 이미 input이 많이 작아진 상태이므로 네트워크가 무겁지 않아도 됨​Conv feature map는 크지 않기 때문에 sliding window(3*3 size)로 한 번에 9개의 각기 다른 ratio를 가진 box를 만들어보고, box들을 regression 하면서 fine tuning해준다. 마지막엔 FC layer 2000개가 아니라 300개 박스만 만들어봐도 모델 학습이 가능했다는 장점.각 box마다 coordinate와 objectness probability를 예측하게 되었음.​ Faster R-CNN10x faster inference than Fast R-CNN [출처] Shaoqing Ren et al., ""Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks""https://arxiv.org/abs/1506.01497 Faster R-CNN: Towards Real-Time Object Detection with Region Proposal NetworksState-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet and Fast R-CNN have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Pr...arxiv.org [출처] https://youtu.be/yGZVibqaPJY ​ "
컴퓨터 비전의 꽃 - Object Detection (물체 검출) ,https://blog.naver.com/aischool_ai/222849708820,20220816,"컴퓨터 비전(Computer Vision)은 컴퓨터를 이용해서 인간의 시각 기능을 모방하는 방법을 연구하는 분야를 지칭하는 용어입니다. 예를 들어, 자율주행차에서 지나가는 사람을 인식하는 기술이나, 사진에 적힌 글씨가 어떤 내용인지를 인식하는 OCR 기술등이 대표적인 컴퓨터 비전 기술의 예시입니다.​ 컴퓨터 비전(Computer Vision) 기술은 컴퓨터를 이용해서 인간의 시각 기능을 모방하는 방법을 연구하는 분야입니다.조금더 자세히 들어가면, 컴퓨터 비전의 대표적인 세부 연구 분야는 다음과 같습니다.​ Image Classification (이미지 분류) Image Classification (이미지 분류)는 컴퓨터 비전의 가장 기본적이고 간단한 연구 주제중 하나입니다. 컴퓨터가 이미지를 입력으로 받으면, 전체 이미지가 미리 정의된 레이블(예를 들어, 개, 고양이, 호랑이) 중에서 어떤 레이블(Label)에 속하는지를 분류하는 문제입니다. Image Classification (이미지 분류)는 이미지가 미리 정의된 레이블(예를 들어, 개, 고양이, 호랑이) 중에서 어떤 레이블에 속하는지분류합니다.​ Face Detection (얼굴 검출) Face Detection (얼굴 검출) 은 이미지 내에 얼굴이 있는 위치 영역의 좌표를 바운딩 박스(Bounding Box)를 통해 찾아내는 문제입니다. 즉, 이미지 내에 얼굴이 있을 경우, 얼굴이 있는 위치를 찾아내는 문제입니다. Face Detection (얼굴 검출)은 이미지 내에 얼굴이 있을 경우 얼굴이 있는 위치를 찾아내는 문제입니다.​ Face Recognition (얼굴 인식) Face Recognition (얼굴 인식) 은 이미지 내에 있는 얼굴이 누구인지 신원을 식별하는 문제입니다. 보통 Face Detection (얼굴 검출) 기술과 결합해서 먼저 이미지 내에 얼굴이 있는 위치를 찾고 Face Recognition (얼굴 인식) 기술을 이용해서 해당 위치에 있는 얼굴의 신원이 누구인지를 식별합니다. Face Recognition (얼굴 인식)은 이미지 내에 있는 얼굴이 누구인지 신원을 식별하는 문제입니다.이외에도 Text Detection, Text Recognition(OCR), Semantic Image Segmentation 등 다양한 컴퓨터 비전 문제영역이 존재합니다.​ Object Detection (물체 검출) Object Detection (물체 검출) 은 이미지 내에 물체가 있는 위치의 좌표값을 바운딩 박스(Bounding box)로 찾고, 해당 위치 영역에 있는 물체가 미리 정의된 레이블(예를 들어, 사람, 자동차, 가로등) 중에 어떤 레이블(Label)에 속하는지를 분류하는 문제입니다.즉, 이미지 내에 물체의 위치를 찾고, 그 물체가 무엇인지를 식별하는 문제입니다. Object Detection은 다양한 컴퓨터 비전 문제 영역 중에서 가장 실용적이고, 다양한 분야에 응용될 수 있는 문제영역 중 하나입니다. Object Detection (물체 검출)은 이미지 내에서 물체의 위치를 검출하고, 해당 위치에 물체가 어떤것인지를 식별합니다. ​ Object Detection 기술의 활용 사례 Object Detection 기술의 대표적인 활용 사례는 다음과 같습니다.​자율 주행차 (AutoPilot)Object Detection 기술을 활용하면 차량내 카메라를 탑재하고 카메라 입력으로 들어온 이미지들에 존재하는 사람, 차량, 신호등 등을 파악하고 이에 기반한 판단을 통해 자율 주행차(AutoPilot)를 구현할 수 있습니다. Object Detection을 이용한 Tesla의 자율 주행 기술 [1]​불량 검출 (Defect Detection)Defect Detection (불량 검출)은 공장에서 공정 과정내에 불량이 있는지 판별하는 기술입니다. Object Detection 기술을 이용하면 사람이 판별하기 힘든 미세한 불량도 빠르고 자동화된 형태로 검출해서 공정과정을 효율화 할 수 있습니다. Object Detection 기술을 이용해서 공정과정 상에 불량(Defect)을 자동화된 형태로 빠르고 정확하게 발견할 수 있습니다.​무인화된 매장 (Self-Checkout)Object Detection 기술을 이용하면 매장에서 결제를 진행해주는 캐셔를 대체하고, 소비자가 구매한 물건이 무엇이고 해당 물건이 얼마인지를 식별해서 자동으로 비용이 청구되도록하는 무인화된 매장(Self-Checkout)을 구현할 수 있습니다.Amazon Go, Standard Cognition 등이 무인화된 매장을 위한 솔루션을 구현하고 있는 대표적인 기업들입니다.  Object Detection 기술을 이용하면 캐셔가 수행하던 물품 결제 업무를 자동화할 수 있습니다. [2] 위성 영상 분석 (Satellite Image Analysis)Object Detection 기술을 이용하면 위성 영상 분석 (Satellite Image Analysis)을 통해 위성 영상으로 촬영된 이미지를 분석할 수 있습니다.예를 들어, 주차장 내에 존재하는 차량의 대수를 분석해서 해당 매장의 고객수를 예상하고 해당 매장이 잘 운영되고 있는지를 파악할 수 있습니다. Object Detection 기술을 이용한 위성 영상 분석 (Satellite Image Analysis)을 통해 주차장내 차량의 대수를 파악하고, 이를 통해 매장의 고객수를 예측할 수 있습니다. [3]​매장 고객 분석Object Detection 기술을 이용해서 매장내 고객들을 분석하고, 매장을 방문한 고객들이 나이대별로 성별별로 어떤 물품을 많이 구매하는지, 어떤 경로로 이동하는지를 분석해서 매장내 상품들의 배치를 최적화 할 수 있습니다. Object Detection 기술을 이용해서 매장내 고객들을 분석하고, 판매를 최대화하기 위한 물품 배치 방식을 찾아낼 수 있습니다. [4]이처럼 Object Detection 기술은 다양한 분야에 실용적으로 활용될 수 있는 컴퓨터 비전 분야의 꽃과 같은 기술입니다.​구체적으로 Object Detection 기술을 구현하기 위해서 Faster R-CNN, YOLO(You Only Look Once), SSD(Single Shot MultiBox Detector), RetinaNet, CenterNet 등의 딥러닝 모델이 주로 활용됩니다.​ References [1] https://www.tesla.com/ko_KR/autopilot?redirect=no[2] https://www.technologyreview.com/2017/09/06/149308/i-tried-shoplifting-in-a-store-without-cashiers-and-heres-what-happened/[3] https://techcrunch.com/2017/01/05/crowdai/[4] https://kr.linkedin.com/company/auravisionlabs "
[AIFFEL / SeSAC] 06. 해커톤 - 시작 ~ Object Detection ,https://blog.naver.com/alsth819/222664990495,20220306," 이번 포스팅을 포함한 3개의 포스팅은 해커톤에 대하여 작성하려고 합니다.먼저 이번 포스팅은 팀빌딩부터 딥러닝 모델 학습까지의 일을 담고 있습니다. 팀 빌딩우리 팀 팀장님의 영업에 넘어가서 이 팀에 들어오게 되었음...팀장님 영업왕 - 임베딩 경험자, 인공지능 경험자 등 다양하게 섭외해옴 GCP 삽질해커톤 딥러닝 학습용으로 GCP (Google Cloud Platform)을 제공해줌하지만, cuDNN 설치 실패로 (메뉴얼 인지에 너무 많은 시간이 들까봐...) 포기(지금 알게된 사실... cuDNN 다설치된 옵션으로 인스턴스 생성할 수 있다는 것!... 공지 좀 잘해주시지...) Colab pro plus & Google Drive원활한 해커톤 진행을 위해 두가지 Tool을 추가로 지원해주심Colab pro plus : 우리가 사용하는 딥러닝 모델인 Yolov5 학습에 충분한 스펙Google Drive : 데이터셋 및 학습 결과를 저장하기 위하여 200GB 두달간 구독 Google Colaboratorycolab.research.google.com 업무/가정용 Cloud Storage - Google Drive클라우드에서 사진, 동영상, 파일 등을 안전하게 보관하고 공유하세요. Google 계정 하나당 15GB의 저장용량이 무료로 제공됩니다.www.google.com 데이터셋 제작 - 라벨링labelImg 프로그램 (https://github.com/tzutalin/labelImg)을 이용하여 직접 데이터셋을 제작하였음​ labelImg 사용 화면GitHub - tzutalin/labelImg: 🖍️ LabelImg is a graphical image annotation tool and label object bounding boxes in images🖍️ LabelImg is a graphical image annotation tool and label object bounding boxes in images - GitHub - tzutalin/labelImg: 🖍️ LabelImg is a graphical image annotation tool and label object bounding b...github.com 학습 진행Object Detection 모델로는 Yolov5 (https://github.com/ultralytics/yolov5)를 선정하였고, 원제작자 코드를 그대로 사용하였음학습은 Colab Pro+를 이용하였음 (Inference도^^*) YOLOV5 학습을 위한 초기 설정제작한 데이터셋을 이용한 YOLOV5 학습학습한 가중치로 Inference 코드 실행GitHub - ultralytics/yolov5: YOLOv5 🚀 in PyTorch > ONNX > CoreML > TFLiteYOLOv5 🚀 in PyTorch > ONNX > CoreML > TFLite. Contribute to ultralytics/yolov5 development by creating an account on GitHub.github.com 여기까지 딥러닝 기반의 Object Detection 진행까지의 해커톤 내용을 정리해보았습니다.다음 포스팅에서는 임베딩과 관련된 내용을 작성해보겠습니다.  ​#AIFFEL #아이펠 #SeSAC #모두의연구소 #해커톤 #삽질 #인공지능프로젝트 "
[Object detection networks] ,https://blog.naver.com/kje_9912/223049277840,20230319,"기본적인 딥러닝 기반의 객체 검출 네트워크 간단하게 정리​1. Faster R-CNN​대표적인 two-stage object detection network로, 기존 slide window 알고리즘으로 localization 후 classification 하던 Fast R-CNN 에 RPN을 제안​RPN: Region proposal networkregression loss: WxHx9개의 anchor box 이용하여 각 위치에서 anchor box와 GT간 IoU 계산 후, IoU >= 0.7: positiveIoU < 0.3  : negativeelse            : None (훈련에 사용되지 않음)이용하여 객체의 위치 (x, y, w, h) 예측​classification loss: 객체의 유무 판단 위해 훈련됨즉, 위의 IoU >= 0.7일 경우 positive anchor로 판단될 경우 regression loss: multi scale, aspect ratio를 가지는 9개의 anchor box 이용하여 위치(x,y,w,h) 예측​--- 이후 시간 있을 때 정리할 예정 ..​2. YOLO​​3. CenterNet "
object detection2 (nanodet) ,https://blog.naver.com/kimsjpk/222930209533,20221116,목표는 yolov5s 보다 속도가 빠르고 성능이 높은 object detection 모델을 학습시키는 것이다​detr real time 으로 검색하다가 연관 검색어에 nanodet을 보았고 github에 들어가 보았다​https://github.com/RangiLyu/nanodet GitHub - RangiLyu/nanodet: NanoDet-Plus⚡Super fast and lightweight anchor-free object detection model. 🔥Only 980 KB(int8) / 1.8MB (fp16) and run 97FPS on cellphone🔥NanoDet-Plus⚡Super fast and lightweight anchor-free object detection model. 🔥Only 980 KB(int8) / 1.8MB (fp16) and run 97FPS on cellphone🔥 - GitHub - RangiLyu/nanodet: NanoDet-Plus⚡Super fast and li...github.com 먼저 필요한게 dataset을 xml이나 coco 형식으로 바꾸는 것이다.​예전에 yolov5s를 학습시키기 위해 작성했던 소스를 찾았다​먼저 OID 파일을 다운받고​https://github.com/EscVM/OIDv4_ToolKit GitHub - EscVM/OIDv4_ToolKit: Download and visualize single or multiple classes from the huge Open Images v4 datasetDownload and visualize single or multiple classes from the huge Open Images v4 dataset - GitHub - EscVM/OIDv4_ToolKit: Download and visualize single or multiple classes from the huge Open Images v4...github.com 파일을 적당한 폴더에 옮기고 coco형식.json 파일로 저장하면 된다​https://github.com/Taeyoung96/Yolo-to-COCO-format-converter GitHub - Taeyoung96/Yolo-to-COCO-format-converter: Yolo to COCO annotation format converterYolo to COCO annotation format converter. Contribute to Taeyoung96/Yolo-to-COCO-format-converter development by creating an account on GitHub.github.com [업데이트1]nanodet을 학습시키고 있는데 yolov5s와 비교했을 때 별로 나은게 없다coco dataset에 학습 정확도(map)가 나은것도 아니고 학습속도가 그리 낫지도 않다nanodet-plus-m_416 모델을 학습시키는데 1에폭에 약 78분 정도 걸린다 아직 테스트 안해봤는데빠르게 추론한다고 해도 yolov5s는 npu 가속이 되는데 nanodet은 그러질 못하니 강점을 보이는 거 같지도않고... 적당히 학습시키고 결과를 확인하려고 한다​[업데이트2]10에폭 마다 validation map 결과를 확인하려는데 메모리가 부족해서 튕겼다. 32기가 램으로도 감당이​안되나 보다.  다른 object detection 모델을 찾았는데 실행해 봐야 겠다.​방금 mmdetection을 거쳐 centernet을 학습시키려는데 데이터를 불러들이고 더 이상 창이 진행되질 않았다​원래 object detection을 학습시키려면 gpu 8대의 빵빵한 gpu 서버가 필요한 거 같다​시간도 많이 걸리고 아예 학습이 불가능 하기도 하고...​검색 더 해보고 다음에 시도하겠다​  
[object_detection] 택배 왔는지 확인하기 ,https://blog.naver.com/yhjo029/222668813533,20220310,"내가 자고 있는 사이 혹은 잠시 외출한 사이에 택배가 제 집앞에 도착해 있을 수 있습니다.제 문 앞에 CCTV를 달았다고 가정하고, object_detection을 통해 택배가 왔는지 인지할 수 있는 모델을 만들어보겠습니다. train_data에는 다양한 택배 상자의 데이터 셋이 있습니다.좌우 상하 반전, rotation 등의 데이터 증강으로 총 220개 데이터 셋을 구축하였습니다.​(object_detection 모델은 yolov5 s로 구현하였습니다.)​object_detection은 택배 상자가 포함된 여러 boundary를 먼저 탐색합니다. 그 후 Non-maximum Suppression(NMS)을 통해 confidence가 가장 높은 bounding box를 택배를 포함한 위치로 판별합니다.​훈련과정 ​예측결과 ​​ ​관련 자세한 내용은 아래 링크에 적혀있으며https://blog.naver.com/yhjo029/222641505825 [object_detection] 잘 쓴 마스크와 잘못 쓴 마스크 판별하기지난 포스팅에 잘 쓴 마스크와 잘못 쓴 마스크 판별하는 모델을 만들어 분류를 해보았는데요. https://blog...blog.naver.com ​다음 시간에는 yolo v5 모델의 paper를 리뷰하겠습니다. "
[논문 리뷰] Object Detection - Swin Transformer V2: Scaling Up Capacity and Resolution - 작성중 ,https://blog.naver.com/byunsohyun/222632673895,20220127,"안녕하세요Briana입니다.​오늘은 Object Detection 모델에 대해서 리뷰를 해보겠습니다.​https://arxiv.org/pdf/2111.09883v1.pdf​논문은 위 링크를 들어가면 얻을 수 있습니다.​현재(2022년1월 기준) Code with paper에서 COCO 테스트 데이터 셋 기준가장 높은 검출 정확도를 보이고 있는 모델입니다. 그럼 바로 리뷰 들어가보겠습니다.​1. Introduction- 현재 Scaling up language model은 매우 성공적인 결과를 보여주는데 해당 메소드가 computer vision에서는 classification에서만 주로 사용되었다.- 그래서 규모가 크고 일반적인 vision 모델 만들 때 성공적으로 훈련시키기 위해 다음과 같은 이슈를 정리해본다.​-첫 번째, 활성화 단계에서 수렴되지 못하고 진폭이 커질 경우 모델이 커질 수록 그 규모도 더 커질 수 있다. 그래서, LN을 이동하는 post-norm이라는 새로운 정규화 구성을 제안한다. - pre-norm: layer norm 먼저 수행 후 attention 수행- post-norm: 말 그대로 layer norm을 나중에 수행1) pre-norm에서 post-norm 형식으로 바꾼 것2) dot product attention을 scaled cosine attention으로 바꾼 것3) 기존의 parameterized 접근법에서 log-spaced continuous relative porion bias 접근법으로 변경​이름이 우선 너무 화려하기 때문에 천천히 하나씩 파헤칠 예정이다.다들 작명센스가 탑클래스네 !! ​-두 번째, vision 과제에서 object detection이나 object segmentation의 경우 resolution이 줄어드는 downstream(max-pooling 등) 과정을 거치기 때문에 high-resolution(높은 해상도)가 필요. + 윈도우 크기 분포가 크기 때문에​2. Method딱 두 가지만 보면 될 것 같다.​2-1) Post Normalization- we propose to use a post normalization approach - the output of each residual block is normalized before merged back to the main branch, and the amplitudes of the main branch will not be accumulated when layers go deeper. ​ 위 말을 해석해보면 Figure 2를 이용해서 분석해볼 수 있다.""when we scale up the original Swin Transformer model from small to large size, the activation values at deeper layers grow dramatically"" 본문에 다음 문장으로 기술되어 있다.​즉 layer가 깊어지면서 activation 활성 값들이 더 커지기 때문에 normalziation으로 main branch에서 합병(merge)되기 전에 정규화 과정을 거치겠다는 뜻이다.​2-2) Scaled cosine attention -> 간단하게 설명하면 attention 연산에서 합치는 filter를 pixel 별로 유사도를 구해서 값을 넣겠다는 뜻이다."" The cosine function is naturally normalized, and thus can have milder attention values."" 그 효과를 이렇게 기술하고 있다.더 milder해진다고 한다.​ "
[ML] 인공지능 Object Detection에서의 mAP란? ,https://blog.naver.com/muscle79/223047157463,20230317,"수많은 ML 분야 중에서도 Object Detection 분야의 논문에서 자주 등장하는 모델의 성능평가지표인 mAP.아직은 계속 용어가 헷갈리고 의미가 불확실해 정리해 보았다. mAP (mean Average Precision)AP는 precision과 recall을 그래프로 나타냈을 때의 면적이다. precision : 모델이 정답이라고 답한것들(아래 그림의 파란 박스) 중, 실제로 맞은(TP) 비율recall : 실제 정답들(아래 그림의 빨간 박스) 중에서 내가 정답이라고 말한 비율 각 class마다 한 AP를 갖게 되는데 모든 class의 AP에 대해 평균값을 낸 것이 바로 mAP(mean AP)이다.즉, 모든 class에 대하여 Precision/Recall의 값을 avg취한 것이라고 볼 수 있다.  [참고자료]https://better-today.tistory.com/3출처 : https://artiiicy.tistory.com/25 [ML] 인공지능 Object Detection에서의 mAP란?수많은 ML 분야 중에서도 Object Detection 분야의 논문에서 자주 등장하는 모델의 성능평가지표인 mAP. 아직은 계속 용어가 헷갈리고 의미가 불확실해 정리해보았다 mAP (mean Average Precision) AP는 precision과 recall을 그래프로 나타냈을 때의 면적이다. precision : 모델이 정답이라고 답한것들(아래 그림의 파란 박스) 중, 실제로 맞은(TP) 비율 recall : 실제 정답들(아래 그림의 빨간 박스) 중에서 내가 정답이라고 말한 비율 각 class마다 한 AP를 갖게 되는데 모...artiiicy.tistory.com ​ "
object detection model zoo (3) ,https://blog.naver.com/artistically22/222937236746,20221124,"이제 jupyter notebook으로 다시 돌아가서 두번째 ipynb파일인 2. Training and Detection을 열어준다.0. setup paths들의 코드들을 차례로 실행해 준다. 각 파일들의 위치를 미리정의해 두는 것이다. model zoo에서 ssd mobileNet V2 FPNLite 320x320을 이용할 예정이다.좀더 성능이 좋은 640x640을 이용해도 상관은 없지만 본인의 컴퓨터 사양에 따라 사용하면 된다.https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md models/tf2_detection_zoo.md at master · tensorflow/modelsModels and examples built with TensorFlow. Contribute to tensorflow/models development by creating an account on GitHub.github.com 2번째 코드에서 다른 모델을 사용하고 싶다면 CUSTOM_MODEL_NAME = '본인의 ai파일들이 저장될 폴더 이름' PRETRAINED_MODEL_NAME = 'model zoo에서 사용하려는 모델이름'PRETRAINED_MODEL_URL = '그 모델의 다운로드 링크'TF_RECORD_SCRIPT_NAME = 'generate_tfrecord.py'LABEL_MAP_NAME = 'label_map.pbtxt' 1,2,3줄을 수정하고 4,5줄은 건드리지 않는다.3번째 코드에서는 'IMAGE_PATH': os.path.join('Tensorflow', 'workspace','images','이미지들이 저장된 폴더이름'), 딴건 건드리지 말고 이미지가 저장되어 있는 폴더 이름 정도면 필요하다면 수정해 준다.5번째 코드인 for path in paths.values():    if not os.path.exists(path):        if os.name == 'posix':            !mkdir -p {path}        if os.name == 'nt':            !mkdir {path} 이걸 돌리면 tensorflow\workspace\models\안에 본인이 지정한 이름으로 모델이 저장될 폴더하나가 생성될 것이다. 이 안에 ai가 공부한 기록들을 저장하고, evaluation 을 진행한 결과들도 저장할 것이다.model zoo가 좀 옛날에 작성된 코드라 라이브러리 버전을 맞추는것이 까다롭다. 1. download tf models ~~을 진행해보자. 차례로 코드를 돌리는데 최신버전으로 깔면 분명히 오류가 날 것이다. ﻿!pip uninstall 라이브러리이름 -y!pip install 라이브러리이름==버전 을 입력해 오류코드에서 no module named '라이브러리 이름'이 떴을 때, 최신버전의 라이브러리에서 오류가 날 때 알맞은 버전으로 라이브러리들을 깔아주자. 갖가지 오류들이 참 많이 날 것이다.내가 겪었던 오류들로는 1. 버전 맞춘다고 tensorflow가 다른 버전으로 두 개 이상이 깔릴 경우 -필요한 버전 빼곤 지운다.2. keras와 tensorflow의 버전이 다를 경우3. tensorflow-official같이 tensorflow의 부속 라이브러리들이 tensorflow와 호환이 안되는 경우4. protobuf가 파일에 포함된 코드로 안깔리는 경우- 구글에 검색해서 zip파일을 직접 다운받아 깔아준다.5. numpy같은 라이브러리들의 버전이 호환이 안되는 경우6. 기타 등등의 라이브러리들이 안깔려있는 경우(no module named '라이브러리 이름')등 열심히 전을 굽다보면 코드 중 다음 코드를 돌리고 중간에 무슨 말이 뜨든 마지막에 ok가 보였다면 object detection을 시작해 볼 수 있다. VERIFICATION_SCRIPT = os.path.join(paths['APIMODEL_PATH'], 'research', 'object_detection', 'builders', 'model_builder_tf2_test.py')# Verify Installation!python {VERIFICATION_SCRIPT} ​ "
Object detection(10) _One-stage 2D object detectors: EfficientDet[CVPR'20] ,https://blog.naver.com/summa911/223003726308,20230203,"다른 resolution 을 가진 layer를 똑같은 가중치로 합쳐서 BB를 예측하면 안된다는 문제의식이 있었다.그래서 feature pooling 시, 각 계층에 다른 weight를 둬서 합치기로 함.또, EfficientNet에서 사용되었던 compound scaling도 사용됨. ​Scaling 을 위해서 새로운 BiFPN이라는 방식을 제안했다.input edge가 1개인 노드를 제외해서 불필요한 route를 줄였다.그리고 original input node로 부터 shortcut으로 데이터를 받아 feature를 더 보완하고,같은 방법을 N번 반복하여 좀 더 복잡하지만 풍성해질 수 있는 방식으로 만들고자 했다.​ detection 에 EfficientNet에 적용되었던, compound scaling을 적용했다. BiFPN network에서는 채널 갯수를 exponentially하게 증가시키도록 하였고, BiFPN layer의 갯수는 scaling할 때 점점 linear하게 더해나가도록 모델을 제안했다.​box랑 class prediction 시에도 채널 갯수를 BiFPN과 마찬가지로 exponentially하게 증가시키고,layer갯수도 linearly하게 증가하는 방식을 사용하면 적절하게 scaling이 될 것이라고 예상했다.​또, input image resolution도 같이 linear하게 증가시켰다.​​[출처] https://youtu.be/AfseiFiz9MI ​ "
Voxel Field Fusion 기술을 활용한 3D Object Detection 강화 - Voxel Field Fusion for 3D Object Detection ,https://blog.naver.com/cobslab/223058132003,20230328,"안녕하세요 콥스랩(COBS LAB)입니다.오늘 소개해 드릴 논문은 ‘Voxel Field Fusion for 3D Object Detection'입니다.해당 내용은 유튜브 ‘딥러닝 논문 읽기 모임 'Voxel Field Fusion for 3D Object Detection' 영상 스크립트를 편집한 내용으로, 영상으로도 확인하실 수 있습니다. (영상링크:https://youtu.be/NWAciZYUiXA)   오늘 발표할 논문은 2022년도 cvpr에서 소개된 3차원 객체 인식에 관한 논문입니다. 논문의 제목은 Voxel Field Fusion for 3D Object Detection으로 홍콩 대학 컴퓨터과학과 연구진들이 발표한 논문입니다.    Voxel이란 간단하게 부피를 가진 Pixel이란 뜻으로 Volume과 Pixel의 합성어입니다. 왼쪽 그림을 보시면 2차원에서의 하나의 요소를 Pixel이라고 부르는데 이를 3차원 개념으로 갖고 왔을 때 2차원 Pixel에 해당하는 요소가 Voxel입니다. 논문의 목표인 3차원 객체인식에는 Point cloud data가 대표적으로 많이 쓰이고 있습니다. 이 개념에 대해서 알아보겠습니다.  이 논문에서 Point cloud data와 다른 형태의 Data를 융합하는 방식을 사용했고 Point cloud data란 3차원 공간정보를 시각적으로 표시해 주는 Data로 왼쪽 아래 그림에서 보실 수 있듯이 그림에 모든 Pixel에 대해 값이 있는 것이 아니라 공간정보가 있는 점에 대해서만 그 값을 갖고 있습니다. 한마디로 정형화되어 있는 Data가 아니라 순서가 없이 나열되어 있는 Sparse 한 특징을 갖고 있고 각 점간의 상호작용이나 기하학적 특성을 파악하기 어렵다는 단점을 갖고 있습니다. Point cloud data를 이용한 객체인식 알고리즘은 주로 sparse 한 성질의 한계를 개선하는 방향으로 진전되어 왔습니다.서로 연관관계를 파악하기 어려운 Data로부터 유의미한 정보를 얼마나 효율적으로 추출하는지에 대해서 연구가 진행되어 왔습니다.  대표적으로 PointNet과 VoxelNet이 있는데 간략히 말씀드리면 PointNet은 Point cloud data를 그대로 입력해서 학습하는 모델로 max pooling이나 spatial Trasnfer 네트워크와 같이 Data의 기하학적 성질을 유지할 수 있도록 딥러닝 네트워크를 제시했습니다. 또한 VoxelNet은 Point cloud data부터 Voxel feature를 추출한 다음 이를 해석해서 물체를 검출하는 모델을 제시를 했고 Voxel 형태 그대로 전처리 하는 것이 아니라 Voxel 단위의 feature map을 딥러닝 네트워크를 이용해 만들어 놓은 것이 큰 특징입니다. 관련된 연구가 계속 있지만 이에 대한 한계가 분명히 있어서 최근에는 Cross modality fusion방식을 활용하는 연구가 진행되고 있습니다.Cross modality fusion란 서로 다른 양식을 갖는 Data를 융합하는 것을 말하며 예를 들어서 RGB 이미지와 워드 등 다양한 분야에서 사용되고 있습니다. Cross modality fusion에서 가장 중요한 것은 서로 다른 modality에서 오는 정보나 feature space 자체가 언밸런스하다는 것입니다. 그래서 서로 다른 feature space representation을 갖기 때문에 density variance, context deficiency, Data augmentation 등 발생하는 misalignment 문제를 해결하는 것이 Cross modality fusion의 큰 관건입니다.  이 논문에서 저자가 제시한 framework에 대해 간략히 말씀을 드리겠습니다.먼저 말씀드리자면 논문의 목적은 3D 객체 인식을 어떻게 하느냐에 대한 방법이 아니라 어떻게 하면은 Cross modality 한 값들을 3D 객체 인식을 사용할 때 효과적으로 사용할 수 있는지에 대한 framework를 제시하고 있습니다. LiDAR를 이용하다 얻은 Point cloud data와 카메라에서 얻은 rgb Data를 어떻게 블렌딩 하면 효과적인지 그 방법을 제시하고 있습니다. 결과적으로 Voxel field에 augmented 된 이미지 feature를 ray 형식으로 나타내서 서로 다른 양식의 Data를 사용해도 일관성을 유지하도록 하는 것이 이 논문에서 제시하는 framework의 목적입니다. 저자는 두 가지 modality로 이미지와 point Data를 선택을 했고 효과적으로 융합하는 방식으로 Voxel field fusion(VFF)라는 새로운 framework를 제시했습니다. 기존의 point to point 방식 같은 경우는 싱글 point에 대해서 fusion을 수행했고 point cloud에 sparsity대한 제한이 있었습니다. 이미지가 갖고 있는 풍부한 context cues를 잘 활용할 일이 없어서 한계점이 존재하였습니다. 저자는 row 이미지를 변화시키지 않고 point cloud에 pairwise correspondence 하도록 reverse 트랜스포메이션을 제시를 했습니다. 저자가 제시한 VFF는 3가지의 큰 point가 있습니다.첫 번째로는 Mixed augmentor 두 번째는 Learnable sampler 세 번째로 Ray wise fusion입니다. Mixed augmentor 같은 경우는 LiDAR Data와 이미지 Data의 modality가 달라도 Data augmentation이 가능하도록 align을 시키는 방법을 제시하고 있습니다. Learnable sampler 같은 경우는 이 두 가지 modality에서 나온 feature들의 interaction이 효과적으로 key feature을 선택할 수 있도록 하는 것이 Learnable sampler 목적입니다. 마지막으로 Ray wise fusion 같은 경우는 한 ray를 따라 있는 가운데 있는 Voxel field fusion이라는 박스를 보시면 알 수 있듯이 하나의 ray를 따라서 있는 feature를 fuse 하고 combine 하는 방식을 제시하고 있습니다. 첫 번째로 Mixed augmentor는 이미지와 Point cloud data가 주어지면 이들을 correspondence 하게 만드는 단계입니다. 저자는 sample added 방법과 sample static방법을 사용했는데 sample added 각 scene 안에 sample을 증가시키는 방법으로 GroundTruth를 기반으로 한 sampling 방법입니다. 3차원 객체의 2D 이미지를 잘라서 실제 오더에 맡게 붙이는 방법으로 왼쪽 아래 그림을 보시면 자전거와 차가 있는데 두 개가 합쳐서 어디가 됐을 때 자전거가 차 뒤에 있도록 수정해서 sample을 추가하는 방법입니다. sample static 같은 경우는 새로운 sample을 추가하지 않고 Flip이나 Rescale 등 transformation 2D conv을 사용하고서 이들이 2D Convolution에 영향을 주도록 image level operation을 사용했습니다. 다음으로는 Voxel field의 Construction에 대한 내용입니다. Voxel bin과 이미지 Pixel을 각각 vi와 pi로 나타냈고 Voxel과 Pixel의 correspondence는 오른쪽 그림에서 보셨듯이 Voxel field에서 나타나게 됩니다. Voxel bin v는 Voxel field라 불리는 함수 F으로 나타낼 수 있고 이미지 point Pi로부터 Voxel space로 향하는 ray Ri로 보실 수 있습니다. 이 또한 아래에 수식으로 표현할 수 있습니다. 이 수식이 의미하는 것은 pi로부터 투영된 모든 Voxel들 vj는i번째 ray의 셋인 Ri로 표현할 수 있다는 것을 나타내고 있습니다. 이때 ray R은 W x h개인데 이 모두 ray에 대해서 계산을 수행한다면 sample point 수에 따라 엄청나게 큰 증가폭을 가진 computational cost를 초래할 수 있기 때문에 저자는 learnable sample방식을 소개를 했습니다. learnable sampler는 앞서 말씀드렸듯이 computational burden을 줄이기 위해 제시했으며 기존의 방법과 달리 중요도에 따라 sampling 하는 방식을 제시했습니다. 왼쪽 그림에서 빨간색 박스로 표시된 ray가 투영됐을 때를 나타냅니다.오른쪽 그림의 P는 sample들 Pixel의 세트를 의미하면서 저자는 기존 세 가지 방법과 저자가 제시한 방법을 비교하면서 설명하였습니다. 기존 sampling 방법에는 heuristic 방법을 사용했습니다. uniformity, density, sparsity 다른 방법이 있는데 이런 방법은 그림에서 물체의 위에 부분에 있는 점에서 알 수 있듯이 불필요한 점을 포함하기 때문에 computational cost를 최대한으로 줄일 수 없습니다. 그래서 저자는 learnable sample F를 제시했습니다.이 때는 augment 이미지가 Voxel field에 fusion 하는 빨간색 평면에 투영 됐을 때 이 물체가 해당하는 높은 response를 갖는 중요한 sub region만을 갖는 방법을 제시했습니다. sampling 방법에 대한 수식은 아래 나와 있으며 이때 stacked convolutions sigmoid activation, uniform sampler을 뜻합니다. 이때 indicator 1은 Pixel Pi의 activation response가 0.5를 넘으면 1이 되도록 설정했습니다. sampling Pixel p^은 전체 sampling Pixel p 중에서 S를 거쳐 나온 결과만을 의미합니다. 저자는 오른쪽 그림의 (a)처럼 sample들과  uniformity에 대한 모든 점을 수식에 입력을 해서 반응이 1 이상이 되는 점만 sampling 방식을 제시했습니다. learnable sampler를 통해서 여러 개의 ray 중에 n개의 ray만을 선택했다면 다음으로는 선택된 한 개의 ray를 위해서 어느 점을 선택할지에 대해서 서술했습니다. 왼쪽 아래 그림을 보시면 이번에 프로젝트된 ray Ri에서 선택하는 방법에 대해 얘기를 하고 있습니다. 오른쪽 그림에 빨간 점은 LiDAR point들을 나타내고 있습니다. 초록점들은 ray Ri에 위치한 Voxel bin들 중 반지름 r이 원 안에 있는 점 vj 들로 나타냅니다. 오른쪽 그림에 있는 간단한 훈련방법인 single fusion에 대해 나타나고 있고 이는 주방 3D context에 대한 값이 고려하지 않고 LiDAR 센서 값만을 사용하는 방법입니다. 이후로 Co fusion방법이 나타났고 이는 인지장력을 반지름 r을 space V로 확장시킨 방법입니다. 이때 반지름 r의 영역은 Gaussian ball을 사용하여서 영역을 설정하였습니다. local fusion은 그림의 (b)와 (c)와 같이 aggregation 방법과 propagation 방법을 나타나는데 간단하게 Voxel b는 모이느냐 아니면 퍼트리냐에 방식의 차이가 있습니다. 이때는 가중치 오메가를 사용하여 값을 획득하였습니다. 저자가 제시한 Ray wise fusion 방법은 local fusion 방식에서 착안한 방법이지만 트레이닝 단계에서 label assign 하기 직전에만 수행된다는 차이점이 있습니다. Voxel bin vj 가 해당될 확률 오메가 j는 아래 수식으로 계산될 수 있으며 델타는 sigmoid activation, F Voxel feature를 나타냅니다.  오메가 j는 Voxel bin vj가 위치하는 이미지 feature F에 대한 response를 볼 수 있고  하나의 Ri에서 깊이를 추정하는 회귀 문제에 대한 답으로도 볼 수 있습니다. 마지막으로 생성된 Voxel vj에 대한 feature인 F^은 아래와 같은 수식으로 얻을 수 있으며 이는 향후 소스할 rv RCNNVoxel RCNN, centetpoint 등 다양한 Voxel 기반 3D detection backbone이 instance 할 수 있는 장점을 가집니다. 앞서 말씀드렸듯이 3D detection에 대한 방법이 아니라 그 방법을 3D 하기 위해 어떻게 하면 feature들을 잘 넣을 수 있는지 framework를 제시한 논문입니다. 저자가 제시한 framework 효율을 확인하기 위해서 실험을 수행했고 KITTI Dataset과 nuScenes Dataset을 사용했습니다. 두 개 Dataset 모두 자율주행 분야에서 널리 쓰이는 Dataset입니다. KITTI Dataset 같은 경우는 synced LiDAR Data와camera images를 갖고 있고, nuScenes Data는 large scale bencmark에 주로 사용되는 Dataset으로 10 object categories, 32 beam LiDAR 360도 카메라 여섯 개를 활용해서 큰 영향을 Data를 갖고 있습니다. 또한 저자가 제시한 framework를 사용할 backbone으로는 PV-RCNN, Voxel R-CNNm, Centerpoint를 사용했습니다.PV R-CNN과 Voxel R-CNN은 KITTI Dataset을 Centerpoint backbone에는 nuScene Dataset을 활용했습니다. 각 backbone의 특징점을 간략히 말씀을 드리자면 PV R-CNN 같은 경우는 grid 방식과 point 방식을 결합해서 예전에 나왔던 backbone 대비 큰 성능 향상을 이룬 backbone입니다. 2 스테이지 구조로 되어 있으며 box proposal을 한 후 로컬라이징 하는 것이 특징입니다. 두 번째로 Voxel RCNN 같은 경우는 Voxel의 feature를 최대한 활용하도록 설계된 backbone으로서 coarse 한 point cloud Data를 그대로 사용하도록 제시된 backbone입니다. 3D backbone 네트워크 2D BEV, RPN, Detection head로 구성되어 있습니다. 마지막으로 Centetpoint는 feature extraction extraction을 point cloud Data만 이용해서 수행하고이 물체 Centetpoint를 2D CNN을 이용해서 탐지하는 구조를 갖고 있습니다. 먼저 KITTI Dataset을 이용한 결과입니다. 먼저 왼쪽에 validation set에 대한 결과를 보시면 저자가 제시한 Voxel field fusion 방식을 추가한 결과가 다른 디텍션 결과에 비해 비해 성능이 향상된 것을 볼 수 있습니다. 오른쪽 테스트 셋에 대한결과에서도 Voxel R-CNN에 VFF을 추가한 경우 81.62에서 82.09%로 향상됨을 확인할 수 있었습니다. Large scale 벤치마크에 주로 사용되는 nuScene Dataset에 테스트 셋에 대한 결과입니다.앞서 말씀드렸듯이 nuScene Dataset은 Centerpoint backbone에 대해서 성능 비교를 수행했고, 기존 LiDAR base 센터 point 결과 대비 mAP값이 약 8.1 정도 향상되는 것을 확인할 수 있었습니다. 이로 인해서 저자가 제시한 Dataset에 대해서는 저자가 제시한 framework를 적용했을 때 성능이 더 좋게 나타났다는 결과를 얻을 수 있습니다. 다음으로는 저자가 제시한 framework 단계별 가정에 대한 스터디입니다. 먼저 Mixed augmentor의 효과를 확인했습니다. 이를 확인하는 이유는 이 논문에서 제시하는 framework의 목적이 Cross modality consistency를 유지하는 것이기 때문입니다. 먼저 Aligned Augmentation은 image level transformation이 feature variant augmentation에 효과적임을 알 수 있었습니다.그다음으로 저자가 적용한 sample added 방법과 sample static 방법에 대한 성능을 비교하였는데 결과적으로 두 가지 방법 다 성능향상이 있었으나 저자가 말하기는 sample static 방법에서 성능향상 폭은 sample added 방법에 대해 향상 폭이 유의미하지 않다고 말했습니다. 결론적으로 저자는 sample added 방식만을 사용해도 충분히 성능 향상을 기대할 수 있다고 서술하였습니다. 다음은 Learnable sampler에 대한 효과에 대해 비교했습니다. 저자가 제시하는 importance에 따른 sampling 방법이기존에는 Heuristic 한 방법인 Uniformity, Sparsity, Density 방법 대비 효과가 있음을 보였습니다. 마지막으로는 ray wise interaction에 대한 평가 검증입니다. 싱글 방법과 local 방법 대비 약 82.53이라는 성능 향상을 보였습니다. 이것 또한 저자는 이때 로컬 방식에서 착안한 것처럼 gaussian ball을 사용했는데 gaussian ball의 반지름이 얼마일 때 가장 효과적인지에 대해서도 스터디를 진행했습니다. 그 결과 반지름이 1일 경우가 가장 성능이 좋음을 확인하였습니다. "
[논문리뷰] Center-based 3D Object Detection and Tracking (CVPR 2021) ,https://blog.naver.com/sounghyn105/222781794108,20220621,"PointCloud를 활용한 object detection, 이번에는 2021년 논문을 가져와 보았다.https://openaccess.thecvf.com/content/CVPR2021/papers/Yin_Center-Based_3D_Object_Detection_and_Tracking_CVPR_2021_paper.pdf이번에는 CenterPoint 모델을 리뷰해 보겠다.CenterPoint 모델은 object detection 뿐만 아니라 물체 하나하나의 움직임까지 구별해주는 tracking 알고리즘 까지 제시한다.​기존 모델과의 차별점이라면, 오직 Centerpoint, 중심점과 속도를 가지고 학습을 진행한다는 것! 기존의 모델들은 Anchor-based object detection이다.​Anchor란?https://stackoverflow.com/questions/70227234/anchor-boxes-for-object-detection Anchor Boxes for Object detectionCan someone help me understand how are anchor boxes represented in YoloV5. In the official code it is mentioned as: [10,13, 16,30, 33,23] # P3/8 [30,61, 62,45, 59,119] # P4/16 [116,90, 156,198, ...stackoverflow.com 모델마다 다른데, object detection에서 물체를 분류하기위해 쓰이는 일종의 바운딩 박스 템플릿이라고 보면 된다. 이 모델들은 anchor box를 만드는데 드는 computational cost가 들기 마련인데, centerpoint는 그러지 않아도 된다는 장점이 있다.​각설하고 전체적인 모델구조부터 살펴보고 차근차근 알아보자. 전체적인 구조는 LiDAR에서 불러온 PontCloud를 3D backbone을 통과시키고 (b)와 같이 버드뷰 2d features로 만들어준다. 이때 3D backbone은 Voxelnet이랑 PointPillars를 사용하였다.(b)는 W * H * F (너비, 높이 , feature의 개수)로 구성된 feature map인 것이다. ​이제 이 feature map M을 이용하여 heatmap과 object의 크기, 속도, 조향(yaw)등을 알아낼 차례다. 본 논문에서 제시한 모델은 위와 같이 2stage로 되어있는데, 사실상 이후의 experiment를 보면 알 수 있겠지만 First stage가 main이다.​먼저 b의 3D features를 2D로 projectoin 시킨다. 이때 dense한 맵을 만들기위해 Gaussian radius를 조절했고, training시 focal loss를 이용했다고 한다.​Focal Loss는 object detection에서 backgound와 일반 물체에 주는 loss를 다르게 하여 일반 물체에 대한 학습을 더 가중시키기 위해서 제시된 식이고,https://gaussian37.github.io/dl-concept-focal_loss/ Focal Loss (Focal Loss for Dense Object Detection) 알아보기gaussian37's bloggaussian37.github.io 이제 gaussian radius에 대해서 알아보겠다.기본적으로 커널에 대한 개념을 알면 좋은데, 나중에 커널에 대해서 gaussian process와 함께 리뷰해 보도록 하겠다. 일단 기본적인 개념은 여기https://sanghyu.tistory.com/14 Kernel/Kernel trick(커널과 커널트릭)Input의 선형조합(어떤 weight와의 곱을 다 더함)으로 output을 결정하는 많은 모델들을 살펴봤다. linear regression, logistic regression, SVM, perceptron... 더 나아가 NN에서도 weighted sum을 활용한다. SV..sanghyu.tistory.com 다시 gaussian radius로 돌아와서, gaussian radius는 Cornernet 논문에 정의되었다. https://arxiv.org/pdf/1808.01244.pdfgaussian radius가 필요한 이유는 바로 train할떄, 우리는 center point를 ground-truth로 해서 학습을 진행해야하는데, Heatmap을 만들때 center point 그 점만 정답, 1로 설정해두고 나머지는 다 0으로 동등하게 처리할 것이냐 이말이다. Center point에 가까울수록 0.8,0.7,... 이렇게 penalty를 다르게 주어야 올바를 것이다. 그래서 이를 위해 gaussian radius를 설정하고,  이때 σ는 gaussian radius의 1/3(Cornernet 기준), x,y는 바로 centerpoint와 x,y만큼 떨어진 거리를 의미한다.따라서 centerpoint 점 자체는 p=1이 되고 멀어질수로 0 (background)가 되는 형태이다.​참고로 이렇게 구한 heatmap을 토대로 cornernet에서는 다음과 같은 focal loss를 제시한다. 본 논문이 cornernet에 적잖은 영감을 받을 것을 알 수 있다.​​Heatmap을 구함과 더불어 이제 sub-voxel loction refinement(o), 지면과의 높이, 3D 사이즈, 그리고 yaw 를 regression할 때가 되었다. 이때 sub-voxel location refinement란, backbone network를 거칠때 생긴 quantization error를 보정해주는 역할을 한다.또한, tracking을 위해 velocity도 regression해준다.(L1 loss 이용)​이렇게 만든 heatmap과 regression을 같이 optimize시켜 c와 같이 바운딩 박스를 따낸다.​​Second stage는 mlp를 이용해서 말그대로 refinement시켜주는 노드다.Confidence score를 도출하는 형식으로, Score target(I)및 loss function은 다음과 같다. 왜 score를 이렇게 설정하였는지 궁금하다..이렇게 해서 first stage에서 얻은 score(heatmap에서 얻은것)와 위의 I의 기하평균을 구하여 최종 score를 도출한다.이 최종 score의 최댓값인 것으로 refine을 시켜준다.​- Experiments​Dataset은 Waymo Open Dataset과 nuScenes Dataset을 이용했다.  ​Waymo는 난이도에 따라 Level1,2로 나누어져 있다. ​ Level1,2에서 Ours가 가장 높은 성능을 버여주고 있다.​ 또한 이 논문에서 제시한 Center-based가 기존의 Anchor based보다 baseline이 무엇인지에 관계없이 뛰어남을 보여주었다. ​마지막으로 tracking의 성능에 대한 experiment와 알고리즘에 대해서 얘기해 볼까한다. 이 논문에서 제시한 CenterPoint-Voxel  Point tracker가 M-KF인 칼만필터 계열보다 더 좋은 성능(압도적인 시간)을 보여주었다.​ 알고리즘은 다음과 같은데, 기본적으로 현재 위치 p와 속도 v를 통해서 미래의 예측 위치 p'을 구하고, 후에 미래에 알게된 ground trouth value랑 이 p'값들을 비교하여 F 일정 threshould 이내에 있으면(예측 범위 내에 위치해 있으면) 매칭 시켜주고 아니면 따로 initialize시켜준다. 만약 unmatched된 track이 A이상 짝을 못찾는경우 그냥 수명을 다하는 구조이다.​  "
[Review] R-CNN; Rich feature hierarchies for accurate object detection and semantic segmentation 리뷰  ,https://blog.naver.com/kiaelf/222789864153,20220626," 안녕하세요, HELLO 현재 회사에서는 시계열 데이터를 중심으로 살펴보기에, 다른 형식의 데이터에 대해서는 살펴볼 기회가 많이 없었습니다. 다양한 데이터를 살펴보고 싶은 마음에, 데이터사이언스 대학원 내에서 객체 인식(Object detection) 스터디를 진행하며, 논문을 리뷰하고 있습니다.​우선은 한글로 정리된 내용을 공유하고, 이후에 영어로 정리해서 업데이트할 예정입니다.​이번에 살펴본 논문은 hoya님이 작성하신 2014 ~ 2019년까지의 'object detection 논문 목록'에 R-CNN 입니다.​ 첨부파일1. RCNN paper review_hyunhp.pdf파일 다운로드 첨부파일1. Rich feature hierarchies for accurate object detection and semantic segmentation.pdf파일 다운로드  ◆ 마무리​'Rich feature hierarchies for accurate object detection and semantic segmentation'에 대해서 알아봤습니다.​좋아요와 댓글 부탁드리며,오늘 하루도 즐거운 날 되시길 기도하겠습니다 :)​감사합니다. "
"Object Detection Metric | IoU , Average Precision(AP), Mean Average Precision(mAP) (1) ",https://blog.naver.com/bosongmoon/222840507405,20220806,"​ Object Detection Metric 필요한 이유Obejct Detection 문제 영역의 Ground Truth 데이터는 사람이 지정한 Bounding Box와 class label이다.즉, supervisied learning 이기 때문에, 정답 Ground Truth 데이터와 예측값을 비교하기 위한 Metric이 필요크게 정성적 지표와 정량적 지표가 존재정성적 지표는 Object Detection 내에서만 활용하는 것으로, 해당 모델이 잘 돌아가는지 등 흐름을 파악할 수 있다.정량적 지표는 특정 모델들간의 비교를 위해 만들어진 지표인데, YOLO 모델과 다른 모델 중 성능이 어떤 것이 좋은지 확인하기 위해 필요하다.크게 3가지 지표가 있다. IoU, AP, mAP​ Metric 01 - Intersection over Union(IoU)IoU는  예측 Bounding Box와 실제 Bounding Box가 얼마나 일치하는지를 0.0 ~ 1.0 사이의 값으로 표현한다.2개의 Bounding Box(예측, 실제)가 일치할 수록 1.0에 가까운 값이 되고, 일치하지 않을수록 0.0에 가까운 값이 나온다.  ​​​​​ Precision, Recall, F1AP를 알기 위해선, Precision, Recall, F1에 대해 먼저 알아야 한다. machine learning에 기본적으로 사용하는 confusion matrix와 그 와 관련된 용어들  Object Detection 관점TP (실제 양성 예측 양성) : IOU >= threshold, 올바른 탐지FP (실제 음성 예측 양성): IOU <= threshold, 오탐FN (실제 양성 예측 음성) : 탐지 못함TN (실제 음성 예측 음성) : Object Detection 에서는 사용 X , TN은 올바르게 감지되지 않은 bouding box를 의미, 이미지 내에서는 bounding box가 많기 때문에 사용 X ​Precision(정밀도) : 검색된 것 중 관련된 것들만 식별하는 모델 성능2명의 사람과 1개의 tripod이 있는 image에서 (3개의 boudning box 존재) 총 2개를 검출했고 2개 모두 올바르게 검출했다면, precision 은 2/2로 1의 값에 해당한다.precision = TP / ( FP + TP )TP / all_detectionsRecall(재현율) : 모든 관련된 것​들을 식별하는 모델 성능2명의 사람과 1개의 tripod이 있는 image에서 (3개의 boudning box 존재) 총 2개를 검출했고 2개 모두 올바르게 검출했다면, recall 은 2/3로 0.66의 값에 해당한다.Recall = TP / (TP + FN)true positive /all_ground_truths F1 : Precision과 Recall의 조화 평균으로, Precision과 Recall 을 한번에 비교할 수 있다는 장점이 있다.​​​ "
"Object Detection in 20 Years - 04,  Technical Evolution in Object Detection ",https://blog.naver.com/tory0405/222845416501,20220811,"[논문원본] 첨부파일1905.05055.pdf파일 다운로드 ​탐지 시스템의 몇 가지 주요한 빌딩블록과 지난 20년 동안의 기술적 발전을 알아보자. ​Early Time’s Dark Knowledge 초창기 암울했던 시기 2000년 전 초기의 object detection은 sliding window 감지를 따르지 않고 low-level이나 mid-level의 vision으로  아래 몇 가지처럼 디자인되었다. ​Components, shapes and edges“Recognition-by-components” 은 오랫동안 image recognition 과 object detection에 중요한 핵심 아이디어였는데 초기 연구자들은 Distance Transforms, Shape Contexts, Edgelet 등을 포함하여 객체 구성, 모양 및 윤각 사이의 유사성 측정으로 객체 감지를 구성하여 일정 부분 성과를 보였으나 복잡한 감지에는 효과가 없었다. 결국 복잡한 감지를 해결하기 위해 machine learning 기반 탐지 방법이 번성하기 시작했다. ​ machine learning 기반 탐지는 모향의 통계 모델(statistical models of appearance before 1998), 웨이블릿 특징 표현(wavelet feature representations, 1998-2005), 기울기 기반 표현 gradient-based representations , 2005-2012)을 포함하여 오랜 시간 발전되어 왔다.  그림 5의 Eigenfaces와 같은 객체에서의 Building statistical models은 학습 기반 접근 방식의 첫 번째 시도였다. 1991년  M.Turk은  Eigenface 분해를 이용하여 랩  수준에서의  실시간 얼굴 감지를 달성하였다. 당시의 rule-based 또는 template based approaches 방식과 비교해 보면 statistical model이 학습을 통해 어떤 데이터로부터 특별한 정보를 훨씬 더 전체적인 설명을 잘 하고 있다. ​wavelet feature representation은 2000년부터 시각적 인식 및 객체 감지에 우위를 가지기 시작하였는데  이 방법의 핵심은 이미지를 픽셀에서 웨이블릿 계수 세트로 변환하여 학습하는 것이다. 이중 Haar wavelet은 계산 효율이 높기 때문에 general object detection, face detection, pedestrian detection 와 같은 많은 객체 감지 작업에 주로 사용되었다.  그림 5(d)는 사람의 얼굴에 대해 VJ detector에 의해 학습된 Haar wavelet 기반 세트를 보여준다.  ​Early time’s CNN for object detection컴퓨팅 리소스의 제한된 초창기의 CNN 모델은 오늘날보다 훨씬 작고 얇았지만 계산 효율성이 낮았기 때문에 CNN 기반 모델은 여전히 어려운 부분이 있었다. Y. LeCun은 전체 입력 이미지를 포함하도록 컨볼류션 네트워크의 layer의 확장을 줄이기 위해 “shared-weight replicated neural network”과 “space displacement network”을 제한하여 네트워크의 forward propagation를 한 번만 수행하여 전체 이미지의 임의 위치에 대한 특징을 추출할 수 있도록 하였는데 이 방법이 오늘날  fully convolutional networks (FCN)의 프로토타입으로 간주된다. ​Technical Evolution of Multi-Scale Detection다른 크기, 다른 종횡비를 가진 물체의 Multi-scale detection는 물체 감지의 중요한 기술 중 하나이다. 지난 20년 동안 Multi-scale detection는 그림 6에 나와 있듯이 “feature pyramids and sliding windows (before 2014)”, , “detection with object proposals (2010-2015)”, “deep regression (2013-2016)”,  "" “multi-reference detection (after 2015)”, “multi-resolution detection (after 2016)”와 같이 기술적 변화가 이루어졌다.  ​VJ 검출기 이후 컴퓨팅 파워가 증가함에 따라 “feature pyramid + sliding windows”를 구축하여 좀 더 직관적인 검출 방법에 많은 관심을 가지기 시작했다. 2004년부터 2014년까지 HOG, DPM, Overfeat detector를 포함한 수많은 마일스톤 감지기가 구축되었다. 초창기 VJ, HOG 와 같은 감지 모델은 feature pyramid를 만들고 고정 크기의 detection window를 sliding 하여 “fixed aspect ratio”를 가진 물체를 감지하도록 설계되었지만  “various aspect ratios”의 감지는 고려되지 않았았고. PASCAL VOC에서 좀 더 복잡한 모양의 물체를 감지하기 위해  feature pyramid 외부에서 더 좋은 솔루션을 찾고자 하였다. ​ 종횡 비가 다른 물체를 감지하기 위해 “mixture model”은  여러 모델을 훈련하는 방식을 제시하였고 exemplar-based detection은 훈련 세트의 모든 객체 인스턴스에 대해 개별 모델을 훈련하는 방식을 제시하였다.​오늘날 dataset의 객체가 더 다양해짐에 따라 혼합 모델 또한 더 다양한 객체를 감지할 수 있어야 하는데 “object proposals” 이  다양한 종횡비의 물체를 감지하기 위한 통합된 다중 스케일 접근 방식이라고 할 수 있을 것 같다. ​Detection with object proposals (2010-2015) object proposal은 개체를 포함할 가능성이 있는 후보 box를 나타내는 것으로 2010년 객체 감지에 처음 적용되었다. 또한 object proposal은 이미지 전체에서 과도한 sliding window 검색을 방지하는 데 도움을 준다.object proposal은  정밀도를 높이고 처리 시간을 줄이기 위해  1) high recall rate, 2) high localization accuracy, 3) on basis of the first two requirements 을 충족시켜야 하며 지금의 object proposal은    1) segmentation grouping approaches, 2) window scoring approaches, 3) neural network based approache의 세 가지 범주로 나눌 수 있다. ​초기의 objet proposal은 bottom-up detection 방식으로 이루어졌는데 visual saliency detection에 영향을 많이 받았다고 할 수 있다. 2014년 이후 시각 인식 분야에 Deep CNN이 인기를 얻으면서 top-down 학습 기반 접근 방식이 여러 문제를 해결하는 데  더 효과적임을 보여줬고 bottom-up 방식은 추후 “overfitting to a specific set of object classes”으로 발전하게 되면서  detectors 과  proposal generators 간의 구분이 모호해지기 시작했고  one-stage detectors 과 “deep regression”이 등장하면서 사라졌다. ​Deep regression (2013-2016)최근 몇 년 동안 GPU의 컴퓨팅 성능이 향상됨에 따라  multi-scale detection를 처리하는 방식이 점점 더 간단해지고 범용적으로 사용되게 되었다. multi-scale detection 문제를 해결하기 위해 deep learning features 기반의 bounding box 좌표를 직접 예측하는 Deep regression의 방법은 매우 심플하다. ​Multi-reference/-resolution detection (after 2015)Multi-reference detection는 multi-scale object detection에서 가장 인기가 높은 것으로 이미지의 다른 위치에서 크기와 종횡비가 다른  reference box의 세트를 미리 정의한 다음 이러한 참조를 기반으로 detection box를 예측하는 방식이다. 그리고 최근 2년 동안 인기가 있었던 또 다른 기술은 multi resolution detection으로 CNN이 forward propagation 하는 동안 자연적으로 feature pyramid를 형성하고 깊은 layer에서는 더 큰 객체를 감지하고 더 얇은 layer에서는 더 작은 객체를 감지하는 것이 쉬어지면서 Multi-reference 와  multi-resolution은 객체 감지 시스템에서 기본이 되는 빌딩 블록이 되었다. ​Technical Evolution of Bounding Box Regression Bounding Box (BB) regression는 객체 감지에서 초기 제안이나 ahchor box를 기반으로 예측된 bounding box의 위치를 조정하는 중요한 기술이다.  그림 7은 지난 20년 동안 BB regression의 발전 과정을 보여 준다. ​ ​Technical Evolution of Context Priming시각적 객체는 일반적으로 주변 환경과 함께 일반적인 context에 포함된다. 우리의 뇌는 시각적 지각과 인지를 촉진하기 위해 사물과 환경 간의 연관성을 활용한다.  Context priming은 탐지를 개선하는데 오랫동안 사용되어 왔는데 1) detection with local context, 2) detection with global context, 3) context interactives과 같이 3가지 접근 방식이 존재한다.  ​Technical Evolution of Non-Maximum SuppressionNon-maximum suppression (NMS)는 물체 감지 기술의 중요한 그룹이다. 이웃에 있는 window는 일반적으로 유사한 감지 점수를 가지므로  non-maximum suppression은 복제된 bounding box를 제거하고 최종 감지 결과를 얻기 위한 사후 처리로 사용된다. 객체 감지 초창기에는 물체 감지 시스템이 원하는 output이 명확하지 않았기 때문에  NMS가 통합되지 않았다.  지난 20년 동안 NMS는 그림 9에서 보듯이 1) greedy selection, 2) bounding box aggregation, and 3) learning to NMS 3그룹으로 발전해 나갔다.  ​Technical Evolution of Hard Negative Mining객체 탐지의 학습은 본질적으로 부정확한 데이터 학습 문제를 가지고 있다. sliding window 기반 검출기의 경우 배경과 물체 간 불균형이 모든 객체의 10^4 ~ 10^5 개의 background window로 극단화될 수 있다. 최신 탐지 데이터 세트는 객체 가로 세로 비율 예측을 필요로 하며 불균형 비율을 10^6~10^7로 더욱 증가되었다.  이경우 모든 배경 데이터를 사용하게 되면 방대한 양의 부정확한 학습이 이루어질 수 있고 이 때문에 잘못된 traing이 될 수 있는데 Hard negative mining (HNM)은 이렇게 훈련 중 데이터 불균형이 발생할 때 해결할 수 있다. 그림 10은 HNM의 발전 과정이다.  Bootstrap객체 감지에서의 Bootstrap은  훈련이 배경 샘플의 작은 부분으로 시작한 다음 훈련과정에서 반복적으로 새로운 분류 배경을 추가하는 훈련 기술이다. 초기에는 수백만 개의 배경 샘플에 대한 훈련 계산을 줄이는 목적으로 도입되었으나 점차 데이터 불균형 문제를 해결하기 위해 DPM, HOG 등에서 사용되었다. ​HNM in deep learning based detectorsDeep Learning 시대 후반 컴퓨팅 파워의 향상으로 Bootstrap은 더 이상 사용하지 않게 되었고 대신 Faster RCNN과 YOLO가 사용되었다. 하지만 이 또한 불균형 데이터 문제를 완전히 해결할 수 없음을 알게 되었고 2016년 이후 다시 Bootstrap를  재도입하게 되었다.  "
object detection model zoo (1) ,https://blog.naver.com/artistically22/222937190254,20221124,"사진을 입력하면 그 안에 사람이 어딨는지, 강아지는 어딨는지를 보여주는 프로그램을 본 적이 있을 것이다.이걸 object detection이라 하는데, 구현하는 방법으로 여러가지가 있는것 같지만, tensorflow와 python으로 구현해보자.​사전 준비로는 파이썬(2.x), jupyter notebook, 그리고 object detection파일들을 모아 놓을 가상공간이 필요하다.cmd창에 원하는 위치로 cd한 뒤 python -m venv 폴더이름 을 입력해 주면 가상 공간이 만들어진다. -- 원래 컴퓨터에 깔려있는 파이썬이나 다른 라이브러리를 이용하지 않기때문에 가상공간이고, 그렇기에 필요한 라이브러리들을 새로 이 폴더 안에 깔아주어야 한다.먼저, 이 안에 https://github.com/nicknochnack/TFODCourse GitHub - nicknochnack/TFODCourseContribute to nicknochnack/TFODCourse development by creating an account on GitHub.github.com 위 링크의 파일들을 복사해서 넣어준다. 혹은 깃허브의 클론기능을 이용해도 된다.​cmd 창에서 cd론 파일 위치로 이동하는데 그치기 때문에 가상공간으로 접속하기 위한 명령어 .\폴더 이름\Scripts\activate 를 입력해주면 (윈도우 기준)(폴더이름) 위치가 cmd창에 떠 있을 것이다.이 안에 pip와 jupyter notebook 사용을 위한 ipykernel를 깔아주어야 한다. python -m pip install --upgrade pippip install ipykernelpython -m ipykernel install --user --name=커널이름 위 세줄을 각각 입력해 주면 pip설치, ipykernel설치, jupyter notebook 연동을 위한 커널이 생성된다.이후 cmd창에 jupyter notebook을 입력하면 jupyter notebook이 열린다.출처: https://github.com/nicknochnack/TFODCourse GitHub - nicknochnack/TFODCourseContribute to nicknochnack/TFODCourse development by creating an account on GitHub.github.com ​ "
객체 탐지(Object detection) 기법들 ,https://blog.naver.com/pp22ppgo/222995116283,20230126,"객체 탐지 (Object detection)은 컴퓨터 비전과 이미지 처리와 관련된 기술이다. 디지털 이미지나 영상에서 각 객체의 특정 클래스(사람, 건물, 차 등등)을 탐지한다. 객체 탐지를 하는 기법들로는 다음과 같은 것들이 있다.​  Scale-Invariant Feature Transform (SIFT)이 기법은 이미지에서 객체를 탐지하기 위해 특징을 매칭시킨다. 엣지나 코너 같은 특정 특징이 구분이 되고, 객체가 한 이미지에서 매칭되는 것이 다른 이미지에서도 매칭된다는 아이디어를 기반으로 하고있다.​​​Speeded Up Robust Features (SURF)이 기법은 SIFT와 유사하지만 더 빠르고 계산 비용이 덜 든다. 방법은 특징을 매칭 시켜서 이미지에서 객체를 탐지하는 것을 사용한다.​​​Fast R-CNN이 기법은 합성곱 신경망(convoutional neural network(CNN))을 제안된 객체에서 클래스를 구분하는데 사용된다. 우선 객체들의 제안 세트를 생성하고 여기에 CNN을 적용해 각각의 제안을 객체인지 배경인지 구분한다.​​​Faster R-CNNFast R-CNN을 개선한 기법이다. region proposal network(RPN)으로 객체 제안을 생성한다. 이는 더 빠르고 효율적으로 작업을 수행한다.​​​You Only Look Once(YOLO)single-shot 객체 탐지 알고리즘으로 CNN을 사용해 이미지에서 객체의 바운딩 박스와 클래스 확률을 예측한다. YOLO의 핵심 아이디어는 단일 과정으로 객체 탐지를 수행한다는 것이다. Faster R-CNN 같은 경우 여러 단계를 거쳐 객체탐지를 수행한다.​​​Single Shot MultiBox Detector(SSD)single-shot 객체 탐지 알고리즘으로 CNN을 사용해 이미지에서 객체의 바운딩 박스와 클래스 확률을 예측한다. SSD의 핵심 아이디어는 여러 스케일 피처 맵을 다른 스케일에서 객체를 탐지하는데 사용한다는 것이다.​​​RetinaNet이 기법은 single-shot 객체 탐지 기법들과 비슷하지만, focal 손실 함수를 사용하여 클래스 불균형 문제를 해결한다는 부분이 다르다.​​​CenterNet이 기법은 객체를 바운딩 박스로 나타내는 대신 단일 점(객체의 중심 점)으로 나타내는 것을 제안한다.  위 기법들이 많이 쓰이는 객체 탐지 기법이다. 각 기법마다 장단점이 있어서, 어떤 기법을 사용할지는 적용할 환경에 맞춰서 선택해야한다.​​​#객체탐지 #SHIFT #SURF #RCNN #YOLO #SSD #RetinaNet #CenterNet "
Object Detection ,https://blog.naver.com/janyqueen/222907185414,20221022,"<Object Detection이란>CNN 모델이 하는 일은 이미지를 분류하는 것Object Detection 에서는 classification 뿐 아니라 localization이라는 개념도 포함(Object Detection = classification + localization)→ localization이란 객체라고 판단되는 곳에 직사각형을 그려주는 것​※ Localization / Object Detection / Segmentation [출처 : https://techblog-history-younghunjo1.tistory.com/178#recentEntries]<Object Detection 지표>bounding box를 얼마나 잘 예측하였는지 IoU라는 지표를 통해 측정 [출처 : https://techblog-history-younghunjo1.tistory.com/178#recentEntries]<Sliding Window> [출처 : https://www.researchgate.net/figure/Object-detection-by-sliding-window-approach_fig1_266215670]큰 이미지에서 여러가지 Object를 찾기 위해, 전체 이미지를 적당한 크기의 영역으로 나눈 후에 각각의 영역에 대해 이전 스텝에서 만든 Localization network를 반복 적용원본 이미지에서 잘라내는 크기를 window 크기로 하여 동일한 window 사이즈의 영역을 이동시키며 수행하는 방식​<윈도우를 활용해 객체를 찾는 효율적인 방법>먼저 영역 제안을 하는 것 (이단계 방식)-객체를 포함할 가능성이 높은 영역을 선택적 탐색과 같은 컴퓨터 비전 기술을 활용하거나 딥러닝 기반의 영역 제안 네트워크(Region Proposal Network)를 통해 선택-높은 정확도를 제공하지만 처리 속도가 느린 편-Faster R_CNN , R_FCN , FPN-FRCN​정해진 위치와 정해진 크기의 객체만 찾기 (단일 단계 방식)-각 영역에 대해 형태와 크기가 미리 결정된 객체의 고정 개수를 예측-정확도가 떨어지지만 빠른 처리 가능-YOLO , SSD , RetinaNet​<YOLO>단일 단계 방식의 객체 탐지 알고리즘이미지 내에 존재하는 객체와 해당 객체의 위치는 이미지를 한 번만 보고 예측 가능원본 이미지를 동일한 크기의 그리드로 나눔각 그리드에 대해 그리드 중앙을 중심으로 미리 정의된 형태로 지정된 경계박스의 개수를 예측하고 이를 기반으로 신뢰도 계산미리 정의된 형태를 가진 경계 박스 수 : 앵커 박스앵커 박스 -> K-평균 알고리즘에 의한 데이터로부터 생성탐지 레이어는 많은 회귀 및 분류최적화 도구를 포함하고 있으며 레이어의 개수는 앵커의 개수에 따라 결정​<Faster R-CNN>이단계 방식의 객체 탐지 알고리즘각 관심영역에 대한 피쳐 추출의 계산을 공유하고 딥러닝 기반의 RPN을 도입해 구현할 수 있다RPN이 윈도우를 생성하는 방법 -> 앵커 박스 사용여러 객체 카테고리에 대한 분류 대신 윈도우의 객체 포함 유무에 대한 이진 분류만 수행 "
객체 탐지(Object Detection)란? ,https://blog.naver.com/mo223772/222198315861,20210106,"* 해당 게시글은 CV-Tricks의 게시글을 토대로 작성되었습니다. 이미지 분류란?이미지 분류(Image Classification)란 이미지 안의 객체(Object)를 예측하는 것입니다.예를 들어 고양이와 강아지를 분류해 줄 수 있는 분류기를 설계한다고 한다면, 고양이 혹은 강아지의 이미지를 입력하고 그 이미지가 무엇인지 예측하게 됩니다.​ 출처 : https://github.com/ReiCHU31/Cat-Dog-Classification-Flask-App​하지만 아래의 사진과 같이 한 이미지 안에 고양이와 강아지 모두가 들어있다면 어떻게 할까요? 출처 : https://www.kaggle.com/c/dogs-vs-cats이러한 문제를 해결하기 위해, 고양이와 강아지 두 가지 종류를 모두 예측해 줄 수 있는 multi-label 분류기를 설계하게 됩니다. ​ 객체 탐지(Object Detection)​아직까지 해결되지 않은 문제점이 있는데, 바로 저희는 해당 이미지 안에 개나 고양이가 있다는 것은 알아도 어디에 있는지 위치정보(Location)을 알지 못합니다. 이미지 속 객체의 위치를 알아내는 문제를 Localization이라 부릅니다.  출처 : https://www.smartimagingblog.com/tag/deep-learning/위 이미지는 단순한 Classification과 Localization 그리고 Segmentation을 잘 표현해 주는 그림인데요, Segmentation의 개념은 잠시 미뤄두고 Classification과 Localization에 집중하겠습니다.​이미지 속에 객체가 무엇인지 분류하고 그 객체의 위치가 어디인지를 예측하는 것이 Object Detection입니다.예측 과정에서 이미지 속의 객체를 분류해내고 객체를 포함하고 있는 사각형 박스(Bounding Box)를 함께 예측합니다.​고유한 사각형 박스를 알아내기 위해 4개의 변수(위치 관련 정보)가 필요하게 됩니다.최종적으로 이미지 하나의 각각의 객체를 위해서 저희가 예측해야 할 변수는 다음과 같습니다.​ - 클래스 명(ex. 고양이 or 강아지 등) (class_name) - 사각형 박스의 왼쪽 상단의 x좌표 (bounding_box_top_left_x_coordinate) - 사각형 박스의 왼쪽 상단의 y좌표 (bounding_box_top_left_y_coordinate) - 사각형 박스의 너비 (bounding_box_width) - 사각형 박스의 높이 (bounding_box_height)​다중 클래스 분류기를 만들었던 것과 유사한 방법으로 저희는 이미지 속 다양한 객체를 인식하는 Oject detection 문제를 풀 수 있게 됩니다. 한 장안의 다수의 객체를 예측하고 그 위치까지 파악하는 방식으로 진행됩니다.결과는 다음과 같습니다. 출처 : https://cv-tricks.com/object-detection/faster-r-cnn-yolo-ssd/​그렇다면 이렇게 작동하는 Object Detector를 만들기 위해서는 어떻게 모델을 설계해야 할까요??다양한 방법론들이 존재하지만, 저는 딥러닝 신경망 알고리즘을 이용해서 설계된 모델에 대한 정보를 소개하겠습니다.​Object Detection은 분류 문제로 설계되게 됩니다. 정해진 사이즈의 필터(커널)를 이미지에서 이동시켜가며 이미지의 모든 영역을 탐색하여 분류기에 그 정보를 넘겨줍니다. 출처 : https://cv-tricks.com/object-detection/faster-r-cnn-yolo-ssd/각각의 커널의 정보를 전달받은 분류기에서 해당 커널 안에 객체의 레이블을 예측하게 됩니다. 아무것도 없다면 background(배경)으로 인식하게 됩니다. 이러한 과정을 통해서, 이미지 안에 있는 객체의 클래스와 위치 정보를 알 수 있게 됩니다. ​이렇게만 들으면 쉽다고 생각할 수 있겠으나 조금의 문제가 더 남아있습니다.위에서 일정 크기의 필터를 이미지에서 조금씩 이동시켜가며 해당 필터 안에 객체가 있는지 없는지를 판단하는 방향으로 모델이 예측한다고 말씀드렸습니다. 그렇다면 이때 그 필터의 크기는 어떻게 결정하는 것이 좋을까요??​ 출처 : https://cv-tricks.com/object-detection/faster-r-cnn-yolo-ssd/ 똑같이 강아지를 탐지하는 Object Detection을 수행한다고 가정하겠습니다. 위의 두 가지 사진에서 볼 수 있다시피, 각각 강아지를 잘 찾아낸 모습입니다. 하지만 같은 크기의 두 가지 이미지 안에서 두 강아지의 크기는 다른 것을 알 수 있습니다. 이러한 문제점 때문에 항상 동일한 크기의 필터를 활용한다면 이미지 내의 크기가 다른 객체를 탐지하기에 어려움이 있습니다.​이를 해결하고자 이미지의 크기를 늘이고 줄이는 과정(Scaling)을 통해 이미지 피라미드를 생성해내게 됩니다. 출처 : http://coding-guru.com/image-pyramid-expanding-image/아이디어는 다음과 같습니다.먼저 이미지를 다양한 사이즈로 스케일링을 하고, 앞서 사이즈를 결정한 필터를 스케일링된 이미지에 대입시켜 탐색하는 과정을 거친다면, 필터가 객체를 완전히 포함할 수 있는 크기로 스케일링된 이미지 층이 있을 것이라는 생각으로 진행합니다. 쉽게 말해서 필터의 크기가 작다면? 이미지의 크기도 줄여서 필터 안에 들어갈 수 있게 해준다는 개념입니다.​보통 이미지 피라미드에 64개 이상의 이미지 층을 형성시킵니다. 피라미드의 이미지 각 층을 필터가 돌면서 탐색하고자 하는 객체를 찾는 방식을 통해 객체의 사이즈 문제와 위치 문제를 해소하게 됩니다.​다음 문제는 이미지의 모양 형태가 다를 수 있다는 것입니다. 똑같은 사람의 사진 일지라 하더라도, 이미지 속에서 앉아있을 수도 있고, 일어서 있을 수도 있고, 자고 있을 수도 있습니다.위에 사진에서도 볼 수 있듯 오른쪽의 강아지를 인식한 바운딩 박스는 정사각형에 가깝지만 왼쪽 강아지를 인식한 바운딩 박스는 가로가 더 긴 직사각형의 형태를 하고 있습니다. 이러한 문제는 aspect ratio(가로세로비)를 조절하여 문제를 해결하게 됩니다. 이 부분에 대해서는 다양한 알고리즘에서 활용하고 있는 방식이 다르기 때문에 이후 포스팅에서 알고리즘을 소개할 때 다시 다뤄보도록 하겠습니다.​​ "
 What is Object Detection? compare with other algorithms ,https://blog.naver.com/ongbbb/222803349891,20220707,"What is different between Image classification and Object Detection?Image classification: don't tell where the object(the tag) appears in image→ Its better option for tag that don't really have pysical boundaries such as ‘blurry‘ or ‘sunny‘​Object detection: creates BBox around classified object→ outperform classification network in spotting objects that do have a material presence, such as car ​ object detection vs image localizationimage localization is to identify the location of a single object in the given image​​ Object detection vs Image segmentationImage segmentation: gives a pixel-level classification in an image(create a pixel-wise mask for each object in the image/ without boundaries of each object) ​​ Object detection vs semantic image segmentationmark all pixels belonging to that tag, but won’t define the boundaries of each objectWhat's in this image, and where in the image is it located? In other words, semantic segmentation treats multiple objects within a single category as one entity. Instance segmentation, on the other hand, identifies individual objects within these categories.​  "
SdBAN: Salient Object Detection Using Bilateral Attention Network with Dice Coefficient Loss ,https://blog.naver.com/qhruddl51/222751189892,20220529,"# AbstractVisual attention은 의미있는 부분을 강조함으로써 saliency detection(입체 돌출 감지)에 있어서 매우 중요한 역할을 수행한다. 이 논문에서는 두 가지 방향의 attention을 적용한 새로운  saliency detection 방법론을 제시한다. 제안된 Network는 두가지로 줄기로 구성돼 있다. 하나의 줄기는 공간적인 정보를 학습하고, 다른 하나의 줄기는 attention을 통해 맥락적인 정보를 학습한다. 이렇게 두 줄기로부터 나온 공간적인 정보를 담은 피쳐와 맥락적인 정보를 담은 피쳐를 concatenate으로 통합한다. 그리고 학습데이터에서 Class imbalance 문제를 해결하기 위해 전통적인 cross-entropy loss와 함께 dice coefficient loss를 최소화함으로써 모델 파라미터(가중치)를 최적화한다. 제안된 Network는 후처리 없이 end-to-end 방식으로 돌출된 부분을 예측할 수 있으며, 224x224 크기 이미지 한장을 처리하는데 0.03초 밖에 걸리지 않는다.  ​​​# Backgroundsaliency detection은 이미지에서 시각적으로 구분되는 영역을 추출하는 작업이다. 다른 segmentation 문제와는 달리 saliency detection은 오직 시각적으로 관심있는 객체만을 배경으로부터 구분해낸다. saliency detection은 **image segmentation(영상 분할), object recognition(객체 인식), action recongnition(행동 인식), 약한 지도학습의 **sementatic segmentation(의미 분할), visual tracking(시각적 추적), video compression(비디오 압축) 및 videon summarization(비디오 요약)과 같은 다양한 컴퓨터 비전 분야에 적용될 수 있다. 이전의 hand-crafted 피쳐 기반의 saliency detection 방법론은 대게 이미지의 contrast(대조)를 측정했다. 하지만 이런 방식은 배경과 object의 경계가 모호하거나 배경이 복잡한 형태를 띨 경우는 정확도가 떨어지는 문제가 있었다. 그러나 최근 몇 년 동안 CNN(Convolutional Neural Network) 적용으로 두드러진 물체 감지 및 분할 문제에서 월등히 성능이 개선됐다. 특히 Fully convolution network (FCN)은 공간적 정보를 보존하는 능력을 크게 향상시켰다. U-shape 구조는 객체의 세부적인 정보의 손실을 줄이기 위해 제안됐는데, backbone network의 계층적 피쳐를 잘 합침으로써 점점 해상도를 높이고 세부적인 정보를 잘 catch할 수 있게 됐다. 그러한 이유로 최근의 saliency detection 연구는 U-shape 구조에 기반하는 경우가 많다. Super-pixel-wise convolution neural network는 계층적 대조 피쳐를 사용하는데, 각 스케일의 super-pixel에 대해 2개의 대조 시퀀스가 CNN에 입력된다. Zhao가 제안한 엣지 가이드 network는 여러 스케일의 피쳐에다 돌출 엣지 정보를 함께 보완적으로 활용해 객체의 경계를 예측한다. 이렇게 CNN 기반의 방법은 사전 조건(정보)이 주어지지 않더라도 다양한 이미지에서 좋은 성능을 나타낸다. 하지만, 전체 이미지를 학습하는 방식이기 때문에 배경 영역이 객체 영역보다 넓을 경우에는 배경을 객체로 잘못 인식하는 문제가 있다. 이것을 Class imbalance 문제라고 한다. ​** Semantic segmentation, or image segmentation, is the task of clustering parts of an image together which belong to the same object class.​​​# ArchitectureClass imbalance 문제를 해결하기 위해 이 논문에서는 attention mechanism을 통해 유용한 정보가 있는 영역에 더 높은 가중치를 주는 새로운 saliency detection 방법을 제안한다. attention이란 직관적으로 설명하면 노이즈는 덜어내고 중요한 정보는 살리는 것이다. context attention block을 자세히 살펴보면, 채널 당 하나의 값을 뽑고, 그 결과의 각 스칼라 값에 각 w값을 곱해주고, 시그모이드를 통과시켜 나온 벡터값을 context attention block의 input 피쳐 맵에 그대로 각 채널에 곱해준다. CNN은 채널에 정보가 담기게 되는 구조이다. Convolution block을 많이 통과할 수록 패쳐맵이 채널은 많아지고 resolution은 줄어든다. 이러한 점을 생각했을 때, context attention block은 채널 당의 중요도가 반영되는 것이라고도 생각해 볼 수 있다. EfficientNet의 squeeze excitation과도 유사한데, 용어에 너무 집중하기 보다는 block의 구조와 그 의미를 생각해 두는 것이 더 좋다.  CABGlobal Pooling은 결국 채널 하나를 하나의 값으로 압축하는 작업이다. 제안된 network는 spatial path와 context path 두 가지로 구성되어 있다.  spatial path는 공간적인(위치적인) 정보를 학습하기 위해 skip-connection을 포함한 encoder-decoder 구조로 되어 있다. 반면, context path는 attention을 사용해 맥락적인 정보를 학습한다. 그리고, 두 가지 경로를 통해 도출된 피쳐를 concatenate로 통합하는 블록을 넣어 모든 정보를 손실없이 활용하고자 했다.  ​​​# Loss Function전통적인 cross-entropy loss와 dice coefficient loss를 적절히 조합한 손실 함수를 최소화해 제안된 network를 학습시켰다. cross-entropy loss는 함수의 특징상 자체적으로 Class imbalance 문제를 해결하지 못한다. 객체 영역이 배경 영역보다 훨씬 적은 상황에서 당연히 모델은 배경으로 예측하는 것이 객체로 예측하는 것보다 loss 값을 줄이는 것이기 때문이다. dice coefficient는  두 이미지 간 유사도 측정하는 지표로서 고안되었다. dice coefficient를 최소화함으로써 배경은 무시되고 오직 객체 영역으로 초점이 맞춰진다. 따라서 dice coefficient loss를 최소화하는 것이 Class imbalance 문제를 해결할 수 있게 된다. dice coefficient loss만 사용하면 학습이 불안정하기 때문에 전통적인 cross-entropy loss를 함께 적용해 Class imbalance 문제 없이 안정적으로 학습을 할 수 있도록 했다. 여러 실험 결과들이 제안된 방법이 다른 방법론들 못지 않게 이미지의 질적인 측면이나 다른 기준에서도 좋은 성능을 낸다는 것을 보여준다.   ​​​  Reference● Donggoo Kang, Sangwook Park, J. Paik. SdBAN: Salient Object Detection Using Bilateral Attention Network With Dice Coefficient Loss. 3 June 2020● https://www.researchgate.net/figure/Scheme-of-the-global-max-pooling-mechanism-Global-max-pooling-perform-an-operation-that_fig2_330102108 Figure 4. Scheme of the global max pooling mechanism. Global max...Download scientific diagram | Scheme of the global max pooling mechanism. Global max pooling perform an operation that take the maximum per features map and produce an 1D vector with known size (number of features map) from publication: A CNN adapted to time series for the classification of Supernov...www.researchgate.net ​ "
Yolov5를 이용한 Object detection ,https://blog.naver.com/wooseok6877/222822706922,20220721,"#yolo #yolov5 #computervision #pothole #객체탐지 #pythorch #torch #AI​오늘은 yolov5 모델을 이용하여 image detection을 해보겠습니다.​저는 python==3.9,2torch==1.2.0+cu102버젼을 사용했습니다.​제가 사용한 데이터는 roboflow에서 제공하는 Pothole dataset입니다.​ 위 사진과 같이 도로에 움푹파인 구멍들이 labeling된 데이터셋입니다.​pothole dataset은 아래의 링크에서 다운받을 수 있습니다.​https://public.roboflow.com/object-detection/pothole Pothole Object Detection DatasetDownload 665 free images labeled with bounding boxes for object detection.public.roboflow.com ​다음은 yolov5 github입니다.​아래의 주소를 gitclone 하셔도 좋고 들어가셔서 직접 다운받으셔도 무관합니다​https://github.com/ultralytics/yolov5 GitHub - ultralytics/yolov5: YOLOv5 🚀 in PyTorch > ONNX > CoreML > TFLiteYOLOv5 🚀 in PyTorch > ONNX > CoreML > TFLite. Contribute to ultralytics/yolov5 development by creating an account on GitHub.github.com %cd /put/your/direction/to/download/models!git clone https://github.com/ultralytics/yolov5 다운받은 Yolov5모델은 이렇게 구성되어 있네요. ​ Yolo의 원리 및 세부사항을 알고싶으신 분들은 아래의 링크 참조하시면 될것 같습니다.​저도 많은도움이 되었네요.​https://colab.research.google.com/drive/1KKp9-Fhr-PmN_Cndebb0wSy-JgcZuPST?usp=sharing _객체 탐지 (Object Detection) - YOLO의 모든것.ipynbColaboratory notebookcolab.research.google.com 자 그럼 본격적으로 들어가 보도록 하겠습니다.​ #%%get iamgedatafrom glob import globtrain_img_list=glob('/home/nvidia/dogcat/yolov5/pothole_dataset/train/images/*.jpg')test_img_list=glob('/home/nvidia/dogcat/yolov5/pothole_dataset/test/images/*.jpg')valid_img_list=glob('/home/nvidia/dogcat/yolov5/pothole_dataset/valid/images/*.jpg')print(len(train_img_list),len(test_img_list),len(valid_img_list)) glob을 사용하여 다운받은 pothole dataset에서 이미지 데이터를 가져옵니다.​경로는 개인이 다운받은 경로로 수정하시면 됩니다.​길이를 보니 465 67 133으로 잘 가져와졌군요.​ #%%set imagedataimport yamlwith open('/home/nvidia/dogcat/yolov5/pothole_dataset/train.txt','w') as f:  f.write('\n'.join(train_img_list)+'\n')with open('/home/nvidia/dogcat/yolov5/pothole_dataset/test.txt','w') as f:  f.write('\n'.join(test_img_list)+'\n')with open('/home/nvidia/dogcat/yolov5/pothole_dataset/valid.txt','w') as f:  f.write('\n'.join(valid_img_list)+'\n') 다음은 가져온 파일을 yaml파일로 만들어줍니다.​ #%%writetemplate functionfrom IPython.core.magic import register_line_cell_magic@register_line_cell_magicdef writetemplate(line,cell):  with open(line,'w') as f:    f.write(cell.format(**globals())) 다음은 여러가지 template들은 text처럼 사용할수 있도록 하는 함수를 만들어 줍니다. %cat /home/nvidia/dogcat/yolov5/pothole_dataset/data.yaml cat을 이용해서 data,yaml을 확인해봅니다.​train: ../train/images val: ../valid/images nc: 1 names: ['pothole']​이렇게 나오네요. 이것을 우리가 가진 데이터와 적합하게 바꿔줍니다.​ #%%set data.yaml%%writetemplate /home/nvidia/dogcat/yolov5/pothole_dataset/data.yamltrain: /home/nvidia/dogcat/yolov5/pothole_dataset/train/imagestest: /home/nvidia/dogcat/yolov5/pothole_dataset/test/imagesval: /home/nvidia/dogcat/yolov5/pothole_dataset/valid/imagesnc: 1names: ['pothole'] 이렇게 말이죠.​개인적으로 저는 경로를 사용할 때 절대경로를 사용하는 것을 선호합니다.​ 하지만 상대경로를 사용하여도 무관합니다.​ %cat /home/nvidia/dogcat/yolov5/pothole_dataset/data.yaml 다시 한번 확인해보면 잘 바뀌어져 있네요.​ #%%get numclasses in datawith open(""/home/nvidia/dogcat/yolov5/pothole_dataset/data.yaml"",'r') as stream:  num_classes=str(yaml.safe_load(stream)['nc']) 다음은 data의 확인을 위해 data.yaml을 가져와서 클래스의 갯수를 변수에 저장시켜줍니다.​알고 있지만 습관적으로 이런 작업을 해주면 실수가 줄어들겠죠?​ %cat /home/nvidia/dogcat/yolov5/models/yolov5s.yaml 저는 yolov5s모델을 이용해서 학습할 것이기 때문에 모델확인을 위해 cat을 사용해줍니다 위와 같이 모델의 구조를 확인할 수 있습니다. %%writetemplate /home/nvidia/dogcat/yolov5/models/custom_yolov5s.yaml# Parametersnc: (num_classes)  # number of classesdepth_multiple: 0.33  # model depth multiplewidth_multiple: 0.50  # layer channel multipleanchors:  - [10,13, 16,30, 33,23]  # P3/8  - [30,61, 62,45, 59,119]  # P4/16  - [116,90, 156,198, 373,326]  # P5/32# YOLOv5 v6.0 backbonebackbone:  # [from, number, module, args]  [[-1, 1, Conv, [64, 6, 2, 2]],  # 0-P1/2   [-1, 1, Conv, [128, 3, 2]],  # 1-P2/4   [-1, 3, C3, [128]],   [-1, 1, Conv, [256, 3, 2]],  # 3-P3/8   [-1, 6, C3, [256]],   [-1, 1, Conv, [512, 3, 2]],  # 5-P4/16   [-1, 9, C3, [512]],   [-1, 1, Conv, [1024, 3, 2]],  # 7-P5/32   [-1, 3, C3, [1024]],   [-1, 1, SPPF, [1024, 5]],  # 9  ]# YOLOv5 v6.0 headhead:  [[-1, 1, Conv, [512, 1, 1]],   [-1, 1, nn.Upsample, [None, 2, 'nearest']],   [[-1, 6], 1, Concat, [1]],  # cat backbone P4   [-1, 3, C3, [512, False]],  # 13   [-1, 1, Conv, [256, 1, 1]],   [-1, 1, nn.Upsample, [None, 2, 'nearest']],   [[-1, 4], 1, Concat, [1]],  # cat backbone P3   [-1, 3, C3, [256, False]],  # 17 (P3/8-small)   [-1, 1, Conv, [256, 3, 2]],   [[-1, 14], 1, Concat, [1]],  # cat head P4   [-1, 3, C3, [512, False]],  # 20 (P4/16-medium)   [-1, 1, Conv, [512, 3, 2]],   [[-1, 10], 1, Concat, [1]],  # cat head P5   [-1, 3, C3, [1024, False]],  # 23 (P5/32-large)   [[17, 20, 23], 1, Detect, [nc, anchors]],  # Detect(P3, P4, P5)  ] 복사후 미리 정의해둔 writetemplate를 이용하여 저희가 사용할 모델을 nc를 수정후 저장하여 줍니다.​원본을 수정하기보단 새로 정의하여 주는것이 안전합니다. %cat /home/nvidia/dogcat/yolov5/models/custom_yolov5s.yaml 수정한 custom model을 확인하여보면 역시나 잘 정의되었네요. #%%train!python /home/nvidia/dogcat/yolov5/train.py --img 640 --batch 32 --epochs 100 --data /home/nvidia/dogcat/yolov5/pothole_dataset/data.yaml --cfg /home/nvidia/dogcat/yolov5/models/custom_yolov5s.yaml --weights '' --name pothole_results --cache yolov5의 train.py를 사용하여 학습시켜줍니다.​파라미터들은 위의 값이 절대적인 수치가 아님을 알려드립니다.​각자의 필요에 따라 파라미터를 수정 후 사용하시길바랍니다. #%%load tensorboard%load_ext tensorboard%tensorboard --logdir runs 혹시 google colaboraty에서 파일을 실행하시는분은 위의 코드로 학습결과를 더 간편히 볼 수 있습니다. #%%check result folder list!ls /home/nvidia/dogcat/yolov5/runs/train/pothole_results4/ 학습 후 결과들이 저장된 파일에는 이런형식으로 저장되어있네요. #%%resultfrom IPython.display import Image,clear_output,displaydisplay(Image(filename='/home/nvidia/dogcat/yolov5/runs/train/pothole_results4/results.png',width=1000)) 그 중 result.png파일을 보면 학습과정을 한눈에 볼수 있습니다. validation의 loss를 보니 epoch을 더 높이면 더 좋은 결과를 얻을 수 있을것 같습니다 #%%first trainbatchdisplay(Image(filename='/home/nvidia/dogcat/yolov5/runs/train/pothole_results4/train_batch0.jpg',width=1000)) 첫번째 train batch의 데이터들을 살펴봐줍니다. #%%labeled first validation batchdisplay(Image(filename='/home/nvidia/dogcat/yolov5/runs/train/pothole_results4/val_batch0_labels.jpg',width=1000)) 위의 코드로 라벨링된 validation데이터도 체크해줍니다. #%%check wights folder list%ls /home/nvidia/dogcat/yolov5/runs/train/pothole_results4/weights 가중치 파일을 보면 가장 수치가 좋았던 모델과 마지막으로 학습니 끝난 모델 두개가 저장되어 있네요.​일반적인 상황에서는 best.pt를 사용하면 좋겠죠?​best.pt   last.pt​ #%%run detect.py!python /home/nvidia/dogcat/yolov5/detect.py --weights /home/nvidia/dogcat/yolov5/runs/train/pothole_results4/weights/best.pt --img 640 --conf 0.4 --source /home/nvidia/dogcat/yolov5/pothole_dataset/test/images  detect.py 에 best.pt를 적용해서 test image들을 object detection해봅니다. 제 기준으로는 꽤 빠른 속도로 진행되네요.​size가 작은 모델을 사용한것이 효과 있는것 같습니다. import globimport randomfrom IPython.display import Image,displayimage_name=random.choice(glob.glob('/home/nvidia/dogcat/yolov5/runs/detect/exp2/*.jpg'))display(Image(filename=image_name)) random하게 detection이 끝난 image들을 가져와서 잘 되었는지 확인해줍니다. 잘 detection 되었네요.​이상입니다. 긴 글 봐주셔서 감사합니다~. 공감은 작성자에게 힘이됩니다. "
[object detection 시리즈] YOLOv2 모델 리뷰 ,https://blog.naver.com/koreadeep/222680585642,20220323,"이번 포스팅에서는 지난번 리뷰한 최초의 YOLO 모델(YOLOv1)과 대비하여,그 한계를 극복하기 위한 다양한 시도들과 개선 버전인 YOLOv2 모델에 대해 알아보도록 하겠습니다. Improvement IDEA (개선 아이디어)첫 번째 적용한 아이디어는 Batch normalization(배치 정규화)입니다.Batch normalization 이란 ICML2015에서 ""Batch Normalization: Accelerating Deep Network Training b y Reducing Internal Covariate Shift""이라는논문 제목으로 소개된 개념으로,Gradient Vanishing / Exploding problem 을 개선하기 위해학습에서 불안정 화가 일어나는 이유를 'Internal Covariance Shift'라고 정의하고,배치 정규화는 평균과 분산을 조정하는 과정을 Neural Network Model (신경망 모델) 안에 포함시켜,학습 시 평균과 분산을 조정하게끔 하는 테크닉을 의미합니다.Covariate Shift란 이전 레이어의 파라미터 변화됨에 따라 다음 레이어의 입력 분포가 바뀌는 현상을 의미하는데,각 레이어를 통과할 때마다 Covariate Shift 가 일어나면서 입력의 분포가 점점 변하게 되는 현상을Internal Covariate Shift라고 합니다.​이러한 문제를 개선하기 위해 모든 Convolution layer에 Batch normalization을 적용함으로써mAP 지표로 대표되는 accuracy를 약 2.2% 향상시켰다고 설명하고 있습니다. BN is used on all convolutional layers in YOLO v2 2.2% improvement in mAPYOLO v2 논문두 번째는 High Resolution Classifier입니다.​ImageNet 데이터로 224x224 크기의 이미지로 학습을 완료한 후, 이미지를 두 배로 증가시켜 448x448 크기 이미지에 대해 fine tunning 을 10회 진행했다고 합니다.이렇게 함으로써 고해상도(high resolution) 이미지에 대해 좀 더 최적화된 학습이 될 수 있어약 4% 정도의 mAP 향상 효과를 가져왔다고 설명하고 있습니다. After trained by 224×224 images, YOLOv2 also uses 448×448 images for fine tuning the classification network for 10 epochs on ImageNet. This gives the network time to adjust its filters to work better on higher resolution input.We then fine tune the resulting network on detection. 4% increase in mAPYOLO v2 논문세 번째는 No fully connected layer입니다.YOLOv2 모델의 구조적인 변화는 Fully connected layer를 제거함으로써 weight가 크게 늘어나는 것을 방지하였다고 설명하고 있습니다. YOLOv2 removes all fully connected layersYOLO v2 논문그 밖에도, pooling layer 하나를 제거해서 output feature map의 크기를 좀 더 키움으로써small object를 찾지 못하는 문제를 개선하였다고 합니다.즉, yolov2에서는 입력 이미지로부터 32배 down sampling된 13x13 크기의 feature map을 결과로 얻게 됩니다.​그리고 Faster RCNN에서도 소개된 적이 있는 ""Anchor box""의 개념을 적용하였습니다.anchor box가 적용되지 않았을 때는 mAP가 69.5% recall 성능이 81%였으나,anchor box를 적용하니 mAP는 69.2% recall 성능은 88%가 나왔다고 설명하고 있습니다.(mAP는 약간 감소하긴 했지만, recall 성능이 크게 향상된 것으로 보아 small object를 찾지 못하는 문제를 많이 개선한 것을 유추해 볼 수 있습니다.) One pooling layer is removed to increase the resolution of output.13×13 feature map output is obtained, i.e. 32× down-sampled Without anchor boxes, the intermediate model got 69.5% mAP and recall of 81%With anchor boxes, 69.2% mAP and recall of 88% are obtained.Though mAP is dropped a little, recall is increased by large margin YOLO v2 논문Anchor box를 정하는 방법은 K-means clustering이라는 기법을 이용했습니다.Ground Truth Box를  IOU를 기준으로 5개 그룹으로 그룹핑 한 후, Anchor box 크기와 ratio를 결정하였습니다.5개로 그룹핑 한 이유는 모델 복잡도와 recall 성능 간 trade off 관계를 고려하여 설정하였다고 합니다. Use K-mean clustering in training set for find good prior anchor box.Using standard Euclidean distance based k-means clustering is not good enough because larger boxes generate more error than smaller boxesYOLOv2 uses k-means clustering which leads to good IOU scores.k = 5 is the best value with good tradeoff between model complexity and high recallYOLO v2 논문Figure 1: Clustering box dimensions on VOC and COCO​결론적으로, IOU 기반 5개의 anchor box를 사용한 YOLO v2 모델의 성능이 61%로,Faster RCNN의 성능인 60.9%와 유사한 성능을 보인다고 설명하고 있습니다.(YOLO v2에서 anchor box를 9개로 하면 67.2%까지 올라간다고 합니다.) IOU based clustering with 5 anchor boxes (61.0%) has similar results with the one in Faster R-CNN with 9 anchor boxes (60.9%).IOU based clustering with 9 anchor boxes got 67.2%​다음으로는 Feature map의 자료구조의 변화입니다.13x13 크기의 feature map 은 large object를 detection 하는 데에는 충분하나small object를 detection 하기 위해서 앞 layer 중 26x26x512 크기 feature map을13x13x2048 feature map에 매핑하고, concatenate를 함으로써 1%의 mAP 향상을 가져왔다고 하고 있습니다.  The 13×13 feature map output is sufficient for detecting large object.To detect small objects well, the 26×26×512 feature maps from earlier layer is mapped into 13×13×2048 feature map, then concatenated with the original 13×13 feature maps for detection.1% increase in mAP is achieved 학습 방법 관점으로는 Multi-Scale Training 기법을 사용했습니다.10개의 배치마다 다양한 (320,352~680) 종류의 resolution 을 갖는 이미지들이 랜덤하게 섞이도록 하여variety를 높일 수 있도록 하였다고 합니다. For every 10 batches, new image dimensions are randomly chosen. The image dimensions are {320, 352, …, 608}. The network is resized and continue training Darknet19Inception 모델을 기반으로 하는 YOLO v1에 비해YOLO v2에서는 Darknet이라는 이름의 자체 backbone network를 적용하였습니다.backbone network는 feature extraction 역할을 담당하는 네트워크로써,일반적으로 ImageNet 데이터와 같은 규모가 큰 데이터를 기반으로 학습시키고일반화된 feature를 추출할 수 있도록 합니다.  Besides the above modification, Network architecture is also a factor of affecting the mAP. Darknet-19 classification network is used in YOLOv2 for feature extractionDarknet 모델의 구조는 아래와 같습니다.19개의 convolution layer와 5개의 pooling layer로 구성되어 있고,마지막에는 Global average pooling 과정을 거친 후 softmax를 통해 classification 을 진행하게 됩니다. darknet은 1x1 convolution을 많이 포함하고 있는 것이 특징입니다.이유는 불필요한 파라미터 수를 줄이기 위한 것이며,(이미지당 연산량 : 약 5.58 billion)결과적으로는 성능과 모델 복잡도 간 관계가 꽤 균형을 맞추고 있다고 설명하고 있습니다. Darknet-19 has many 1×1 convolutions to reduce the number of parameters.  Has 19 convolutional layers and 5 max pooling layers. Darknet-19 can obtain good balance between accuracy and model complexity Darknet-19 only requires 5.58 billion operations to process an image yet achieves 74% top-1 accuracy and 91.8% top-5 accuracy on ImageNet. With Top-1 and Top-5 Errors closed to ResNet-50, Darknet-19 has lower model complexity (FLOP) and therefore much faster detection speed (FPS)아래 그림은 VGG16 , Inception 기반 YOLO v1의 base 모델 resnet 50모델과 성능을 비교한 것입니다.Top1, Top5 accuracy 관점에서는 비슷한 수준을 보여주면서도,FLOPS (연산량)과 GPU speed 관점에서는 가장 좋은 성능을 보여주고 있습니다.​ Training for detection이번에는 detection을 위한 학습 방법에 대해 알아보도록 하겠습니다. We modify this network for detection by removing the last convolutional layer and instead adding on three 3 x 3 convolutional layers with 1024 filters each followed by a final 1 x 1 convolutional layer with the number of outputs we need for detectionVOC 데이터를 기준으로,  5개의 bound box를 5개의 속성(x, y, width, height, score)으로prediction 하고 있습니다. 추가적으로 기존 class probability vector는 grid cell이 아닌bound box 수만큼 prediction 합니다.결과적으로 5(bond box 개수)에 20(class probablity; VOC 데이터의 클래스 개수)와 5(bond box 속성)를 더한 것을 곱하는 것이므로, 벡터의 크기는 125가 되게 됩니다. For VOC we predict 5 boxes with 5 coordinates(x, y, width, height) each and 20 classes per box so 125 filters (5*(20+5))) -> 13x13x125Performance아래 그림은 YOLOv2의 성능입니다.FPS는 여전히 타 모델에 비해 압도적인 성능을 보여주고 있습니다(x축 관점)그리고 y 축(mAP)을 기준으로 60~70 사이에 존재하는 YOLO 모델에 비해전반적으로 높은 성능을 보여주는 것을 확인할 수 있습니다.​  5개로 표현되어 있는 YOLOv2모델은 각각 288x288, 352x352,416x416,480x480,544x544 사이즈의 크기로 학습한 결과를 의미합니다.​그럼 이상으로 포스팅을 마치도록 하겠습니다감사합니다:)​  한국딥러닝(주)는 인공지능, 머신러닝, 딥러닝 솔루션을 개발하여 세상이 해결할 수 없는 문제를 해결하고 제안하고 있습니다. 끊임없이 발생되는 새로운 빅데이터와 응용 기술의 홍수 속에서 최신 인공지능 기술 동향을 예측하고 빠르게 반영하는 한국딥러닝을 만나보세요.​ ​ "
[Object Detection 시리즈] overview 및 관련 용어들 정리!  ,https://blog.naver.com/koreadeep/222641903360,20220208,"이번 포스팅에서는 object detection 알고리즘을 이해하기 위한 배경지식에 대해 알아보도록 하겠습니다.먼저 object detection 이 무엇인지 한번 살펴보겠습니다. Object detection 이란Object detection에는 여러 가지 문제 유형이 있습니다.아래의 그림과 같이 네 문제가 대표적이라고 할 수 있습니다. 그림 1. Object detection의 네 가지 문제​Classification 은 이미지 내 object의 class를 판별하는 것으로흔히 우리가 알고 있는, AlexNet, VGGNet, GoogleNet 등이 classification 문제를 위한 네트워크입니다.​classification 을 위한 데이터는 이미지 내에 object가 하나 존재하고그 object에 attention이 되어 있다는 특징이 있습니다.​네트워크 입장에서는 데이터 내에서 주목해야 할 대상이 명확한 편이기 때문에학습이 다소 쉬운 편이라고 할 수 있죠.​​그런데 이 object가 이미지 안에서 다양한 곳에 위치하는 경우도 존재할 것입니다.사방으로 끝에 붙어서 존재한다거나 저 멀리 떨어져 있어서 조그마하게 관찰되기도 할 것입니다.그렇다면 이 object를 '찾아'내는 과정이 선결되어야 할 것입니다.​이 '찾아'내는 과정을 Localization이라고 합니다.즉, Localization은 이미지 내 Object의 위치를 찾아내는 것(Bounding box의 형태로)을 의미합니다.​흔히 우리가 말하는 Object detection은 두 개 이상의 object 들에 대해 Classification + Localization을 동시에 진행하는 것입니다.​마지막으로 Instance Segmentation은 object detection을 통해 찾아낸 object의 형상에 따라 각각의 object의 영역을 표시하는 것입니다.즉, 각 pixel에 대해 classification 을 하는 것이죠.​​Challenges첫째, Classification + Regression을 동시에!앞의 object detection 문제 정의에서 살펴보았듯이이미지에서 여러 물체를 Classification 함과 동시에 위치를 찾아야 한다는 어려움이 있습니다.classification 과는 다르게 object detection은 다양한 크기와 유형의 object가 섞여 있는 상태에서object를 찾아내고, 무엇인지 판별(recognize) 하는 것을 포함하고 있으므로크기가 서로 다르고, 생김새가 다양한 object들이 섞여 있는 이미지에서 detect 해야 하기 때문입니다.​둘째, 중요한 Detect 시간object detection 기능은 다양한 분야에서 응용될 수 있는 만큼, 실제 산업에서의 수요가 굉장히 높은 기능입니다.하지만 대부분 실시간 영상 기반에서 detect를 수행해야 하는 요구 사항이 많기 때문에연산량이 많고 무거워서 inference time이 오래 걸리는 것은 실생활에 적합하지 않습니다.​셋째, 명확하지 않은 이미지object detection은 다양한 위치에 존재하는 여러 종류의 object를 커버하고 있기 때문에 이미지에 attention 되어 있지 않거나 일부만 보이는 등 명확하지 않은 경우도 많습니다.특히 전체 이미지에서 detect할 오브젝트가 차지하는 비중이 높지 않고, 배경이 대부분을 차지하는 경우가 많다는 특징이 있습니다.​넷째, 데이터 셋의 부족학습 데이터를 만들 때, 각 object의 이미지 상 위치를 bound box로 표시하고, 이미지에 대한 상대 좌표로 표기하여 Annotation 정보를 함께 만들어 줘야 하므로 공수가 굉장히 많이 들며,그렇기 때문에 데이터를 확보하는 데 한계가 있습니다.​​​​Region Proposal은 object 가 있을만한 '후보'를 region(영역)의 형태로 추출하는 것을 의미합니다.이 후보들을 인식(recognition) 모델에 넘겨서 무엇인지 예측(prediction) 하는 것이죠.즉, region proposal 이란 Object가 있을 만한 후보 영역을 'bounding box' 형태로 추출하는 알고리즘이라고 정의할 수 있겠습니다.​Sliding Window 기법 그림 2. Sliding Window대표적인 region proposal 기법으로 'Sliding Window' 기법이 있습니다.단어에서 추론할 수 있듯이, Window를 이동(sliding) 시키는 것을 의미합니다.​​배경이 대부분인 이미지에서 특정 위치에 존재하는 어떤 object를 검출해 내기 위해서는모든 영역을 지나가면서 검출해 봐야 할 것이라고 생각해 볼 수 있습니다.그래서 우리가 책을 읽는 우리의 시선의 이동과 같이 좌측에서 우측으로 그리고 행을 한 칸 내려서 또 좌측에서 우측으로 영역을 검토하게 됩니다. ​그리고 다양한 크기의 object를 커버하기 위해서, 그림 2의 빨간 박스와 같이이 window 크기를 다양화해서 여러 번 sliding 시키는 방식으로 수행됩니다.상식적인 접근이지만 굉장히 소모적인 방법이라고도 생각할 수 있습니다.앞서 언급했듯, 대부분이 배경화면에 불과하기 때문입니다.​그래서 최근에는 이러한 방식을 버리고 효율을 택한 알고리즘으로 대체되고 있습니다.​​   새로운 알고리즘으로는 Selective search, Edge boxes 등이 있는데요본 포스팅에서는 Selective Search에 대해 한번 살펴보도록 하겠습니다.​Selective Search 그림 3. Selective SearchSelective Search는 컬러, 무늬, 크기, 형태와 같은 패턴에 따라 유사한 region을 grouping 하는 방식으로 영역을 점차 확장해나가는 방식입니다.최초에는 pixel의 intensity 값을 기반으로 graph-based segment 기법에 따라 over segmentation을 수행합니다.즉, 각각의 object들이 1개의 개별 영역에 담길 수 있도록 최대한 많은 초기 영역을 생성하는 것입니다.​Selective Search 알고리즘은 다음과 같은 동작을 반복하면서 수행됩니다.  1. 개별 segment된 모든 부분들을 bounding box로 만들어서 region proposal 리스트 추가      2. 컬러 무늬, 크기, 형태에 따라 유사도가 비슷한 segment들을 grouping      3. 다시 1번 step region proposal 리스트 추가      4. 유사도가 비슷한 segment들 grouping을 계속 반복하면서 region proposal 수행​이러한 방법을 통해  상대적으로 빠른 detection과 높은 recall 예측 성능을 동시에 충족하는 알고리즘입니다.  ​​다음으로는 Object detection의 성능/평가 지표에 대해 알아보도록 하겠습니다. IOU (Intersection over Union)Object detection은 Localization도 수행하기 때문에 Object의 위치가 이미지 상에서 어디에 존재하는지, 그리고 크기는 어느 정도인지 파악하여 Bound box를 그리는 방식으로예측을 합니다.그렇다면, 얼마나 detection을 잘 했는지는 어떻게 평가할까요?​직관적으로 생각해 보면 예측된 Bound box 와 실제 Bound box 간에 겹치는 영역을 계산해 보면 될 것입니다.IOU는 이 방법을 구체적으로 정의한 평가하는 방법입니다.​아래 그림과 같이 전체 넓이(Area of Union)에 대한 겹치는 부분(Area of Overlap)의 비율을 계산함으로써 구할 수 있습니다. 그림 4. IOU위 그림의 예제를 보면,실제 box는 작은데 예측한 box는 크게 그려져서 IOU 값이 0.4034로 계산되었습니다.(poor)한편 두 번째 그림은 크기도 비슷하고 겹치는 영역도 꽤 많아서 0.7530으로 계산되는 것을 확인할 수 있습니다.(good)마지막 그림은 거의 일치하는 수준으로 0.9264로 계산된 것을 볼 수 있습니다(excellent)​IOU는 Object detection에서 개별 object에 대한 detection 이 성공했는지를 판단하는 지표로 사용됩니다. 일반적으로 0.5 정도 이상이면 예측 성공으로 판단하고 있습니다.​​gIOU(Generalized IOU)​IOU 계산에 따르면 두 box 간 교집합이 없으면, 값이 0이 됩니다.예측하는 단계에서는 두 box가 겹치지 않으면 0인 게 맞긴 하지만,학습하는 단계에서는 겹치지 않더라도 얼마나 오차가 있는지도 중요한 정보입니다.이러한 점을 보완한 지표가 gIOU입니다.gIOU는 아래와 같은 식으로 정의됩니다.box A(GT), box B(predicted)를 모두 포함하는 enclosed box C를 정의한 후기존 두 box 간의 IOU 값과 A, B의 합집합에 대한 C의 비율을 계산한 값에 대해 box C의 영역으로 나눠준 값과의 차이를 계산합니다. 그림 5. gIOU※ C box: GT와 prediction을 포괄하는 가장 작은 박스​이렇게 함으로써 두 예시 모두 IOU 값은 0이지만, box A, B가 거의 붙어있는 첫 번째 그림의 gIOU 값은 0두 박스가 멀리 떨어져 있는 두 번째 그림의 gIOU 값은 -0.7로두 box의 오차가 반영되어 계산된 것을 확인할 수 있습니다.​​object detection을 수행하면 수많은 box들이 예측됩니다.그리고 그 box들의 많은 수들은 동일한 object를 'attention' 하고 있습니다.그러면 그중에서 가장 object를 잘 담고 있는 box만 남겨놓고 나머지는 제거해 줄 수 있다면좀 더 깔끔한 예측 결과를 확인할 수 있겠죠?이러한 작업을 수행하는 알고리즘을 Non Maximum Suppression 일명 NMS라고 합니다. NMS (Non Maximum Supression)아래 그림에서 볼 수 있듯이 Object Detection 알고리즘은 object로 추정되는 위치에서 많은 bounding box를 그리는 경향이 강하다는 특성이 있습니다.따라서, NMS는 detect된 object의 bounding box 들 중 같은 object를 attention 하고 있는 것으로 추정되는 box를 제거하고 가장 적합한 box를 선택하는 기법입니다.​알고리즘의 동작은 다음과 같습니다.​1.Detect 된 bounding box 들 중 미리 정의된 confidence score (threshold) 이하의 값을 갖는 bounding box를 제거     (예, confidence score가  0.5이하인 bounding box 제거)2. confidence score를 기준으로 bounding box를 내림차순 정렬 후,   아래의 단계들을 모든 bounding box에 순차적으로 적용   → 클래스 별로 가장 높은 confidence score를 가진 box와 겹치는(IOU로 판단) 다른 box들을 모두 조사하여 IOU가 특정 threshold 이상인 box들은 모두 제거 (IOU threshold >0.4 )  3. 위 단계를 거친 후 남아 있는 box들만 drawing​즉, Confidence가 높을수록,  IOU threshold가 낮을수록 많은 box가 제거하는 원리라고 할 수 있습니다.  ​​이번 포스팅에서는 앞으로 연재할 Object detection 시리즈에 앞서관련 backgroud 지식들을 아주 가볍게 다뤄보았습니다.'Region proposal', 'IOU','NMS' 등의 키워드들은 앞으로 작성할 포스팅에서도 계속 등장하게 될 용어들이기 때문에잘 이해하고 숙지해놓으신다면,object detection 알고리즘들을 이해하는데 큰 도움이 될 것이라고 믿습니다.그럼 이상으로 포스팅을 마치겠습니다감사합니다:) ​​ ​  한국딥러닝(주)는 인공지능, 머신러닝, 딥러닝 솔루션을 개발하여 세상이 해결할 수 없는 문제를 해결하고 제안하고 있습니다. 끊임없이 발생되는 새로운 빅데이터와 응용 기술의 홍수 속에서 최신 인공지능 기술 동향을 예측하고 빠르게 반영하는 한국딥러닝을 만나보세요.​ ​​​ "
[ML / 영상처리 / Object Detection & 이상행동] ,https://blog.naver.com/myj0406/222613558580,20220105,"씨이랩 인턴십 진행하며 공부한 내용 정리.​논문을 보고 공부하는게 처음이라 어려움을 겪음.README : 사용 방법논문 : 설계과정, 사용된 이론 등 알고자 하는 것에 대해 심도있게 이해할 수 있도록 도움. 따라서 목적, 새로운 이론, 어떤 상황에서 무엇을 비교하는지, 이를 통해 나온 결과가 무엇인지 파악.​*ML (Machine Learning) -Supervised Learning : 입력과 결과 값으로 학습. Classifition(분류)&Regression(회귀)    -> 학습 모델 : SVM, Decision Tree, 선형/로지스틱 회귀 -Unsupervised Learning : 입력만으로 학습. Clastering(군집화)&Compression(압축)    -> 학습 모델 :K-means 클러스터링 -Reinforcement Learning : 결과 값 대신 리워드가 주어짐. Actionseletion, Policy Learning    -> 학습 모델 : MDP(Markov Decision Process) +준지도학습 : ex) WebCam을 사용한 얼굴 인식​[참고 링크]Machine learning - WikipediaWhat is Machine Learning? | IBMWhat Is Machine Learning? | Definition, Types, and Examples | SAP Insightshttps://brunch.co.kr/@gdhan/2  *Object Detection : Multi-Labeled Classfiction + Bounding Box Regression (Localizaion)                                          ->  여러가지 물체에 대한 분류 + 물체의 위치 정보 -1-stage Detector : Localization과 Classification 동시 진행->빠르고 정확도 낮음, R-CNN -2-stage Detector : Localization과 Calssification 순차 진행->느리고 정확도 높음, YOLO,SSD​Object Detection(객체 검출) + Object Recognition(객체 인식) + Object Tracking(객체 추적)                             ->Objext Recognition을 하기 위해 Object Detection이 선행되어야 함. Object Detection Algorithms -> Pre-processing(전처리), Feature X=Extraction(특징 추출), Calssifier(분류)                         +영상에서 전처리 등을 통해 노이즈를 제거하거나, 이미지를 선명하게 만든 후 해당 이미지에서 특징들을 추출하고, 이 특징들을                           이용해 Object Detection에 대해 Classifier하는 Pipe line을 따른다.                         +알고리즘을 구현할 때 사용하는 모듈 : DataLoader,Augmentation Toolkit, Visdom&wandb, Torch summary​        -PASCAL VOC->xml에 좌표값으로 저장        -YOLO->classes.txt 이름+txt파일에 수치 값 저장        -CreateML->좌표값을 json으로 저장​​[참고 링크]https://nuggy875.tistory.com/20https://eehoeskrap.tistory.com/300Obeject Detection 논문 모음(2014~) : https://github.com/hoya012/deep_learning_object_detectionhttps://deepbaksuvision.github.io/Modu_ObjectDetection/posts/01_00_What_is_Object_Detection.htmlhttps://pytorch.org/tutorials/beginner/data_loading_tutorial.htmlYOLO:https://github.com/ssaru/convert2Yolo  * 행동 인식 / 탐지논문 : alpha pose, open pose, dance pose, slowfast -> 뼈기반​1. AlphaPose - 부정확한 bbx에서도 정확하게 Person Pose를 추정할 수 있는 모델을 만드는 것을 목표로 한다.Alphapose는 backbone으로 YOLOv3를 사용해 빠르고, 성능 측면에서도 COCO 데이터셋에서 밀리지 않는다.기존의 two-step multi human pose estimation's Failure cases : 특이한 자세, 겹침, 인식 실패, 오인식를 해결하는 모델이다.Alphapose는 two-step model(bounding box->pose estimation)로 bounding box의 정확도가 품질을 좌우하기 때문에 부정확한 bbx를 줄이는 방법을 찾는다. 이는 SSTN, NMS, PGPG의 3가지 방법을 사용한다. 이 세가지 모듈은 논문에서 제안하는 네트워크 RMPE(Regional Multi-Person Pose Estimation)가 가지고 있다.​RMPE framework Symmetirc STN은 SPPE 전후에 부착된 STN과 SDTN으로 구성되어 있다. STN은 suman proposals를 받고 SDTN은 pose proposals를 생성한다. Parallel SPPE는 training phase를 진행하는 동안 추가 정규화 역할을 한다. 중복 포즈 추정을 제거하기 위해 parametric Pose NMS(p-Pose NMS)를 수행한다.->기존 traing과 달리 PGPG에성 생성된 이미지로 SSTN+SPPE 모듈을 training한다.+STN : detector로부터 들어온 human proposal을 s좌표계로 보낸다.+SDTN : 수식을 거쳐 estimator로 추정된 pose를 다시 원래 좌표계로 보낸다.​-SSTN : Symmetric Spatial Transformer Network -> single person esrimator branch를 하나 옆에 붇였다.  SPPE(Single Person Pose Estimator) 양쪽에 부착해 부정확한 바운딩박스에서도 정확한 single person region을 찾는다. 학습 단계에서   Parallel SPPE를 두고 모든 weight 고정하며 이때 직접 truth pose와 비교해 weight를 수정한다.  Symmetric STN은 SPPE와 함께fine-turnede되는데, 학습을 더 잘되게 하기 위해 SPPE branch를 하나 파서 같이 학습한다. 이 branch는  SPPE와 STN은 같지만 SDTN이 없기 때문에 branch에서의 label은 affine trasformation된 상태에서 비교된다. 이 branch의 weight는 업데     이트 하지는 않고, banch의 STN과 Symmetric STN이 같이 center-located pose error를 보내 STN을 좀 더 정교하게 학습한다.​-NMS : Parametric Pose Non-Maximum Suppression -> 중복된 detection을 줄인다.  가장 정확한 pose를 레퍼런스로 선택하고 너무 가까운 포즈는 elimination criterion을 사용해 지운다. 이때 pose distance를 metric으로 사용  한다. 이 과정을 unique bbx만 남을 때까지 반복한다.​-PGPG : Pose-Guided Proposals Generator -> training sample을 늘린다.  ground truth(정답) bbx 와 detected  bbx의 offset은 포즈마다 다르다. 이 분포를 모델링 할 수 있으면 generated by the human detector  와 비슷한 샘플을 많이 얻을 수 있을 것이다.​[참고 링크] https://github.com/MVIG-SJTU/AlphaPosehttps://eehoeskrap.tistory.com/299https://velog.io/@haejoo/RMPE-Regional-Multi-Person-Pose-Estimation++https://arxiv.org/pdf/1612.00137.pdf​2. SlowFast : Video Recognition을 위한 Network 구조. SlowFast Network = Slow pathway + Fast pathway-Slow pathway : low frame에서 동작, capture sparial semantics -> 객체 파악-Fast pathway : channel capacity를 줄음으로써 lightweigh를 가진다. video recognition에서 유용한 temporal information을 학습할 수  있다. -> 움직임 포착 ++이 부분이 기존의 영상인식에서 Opticalflow로 수행되던 부분이다.Fast pathway에서 전체 연산량의 20%만 수행, 많은 채널의 정보를 사용하지 않는다.  ++박수 인식 = 손 인식+ (손이 무엇을 할 수 있는지 파악) + 행위 인식 -> 행위 인식(손의 정보를 미리 알고 있는 상태)=>계산이 가벼워져 처리속       도가 빠르다.Lateral Connections : 각각의 Pathway에서 나온 출력값의 형태가 다르므로 두 개의 값의 형태를 맞춰주는 방법이다. 1. Time-to-channel : 알파 값을 하나의 frame chennel로 변환시킨다. feature의 형태 {αT, S^2, βC}를 {T, S^2, αβC} 형태로 바꿔준다. 2. Time-strided sampling : {αT, S^2, βC}알파 frame 중 하나만 sampling하여 {T, S^2, βC}의 형태로 바꿔준다. 3. Time-strided convolution : 2βC의 출력 채널과 stride = α 를 가진 5×1^2 커널의 3D convolution을 사용한다. 전체적으로 컨볼루션과 샘     플링 프레임을 채널단위로 바꿔주며 Slow pathway와 Fast pathway를 맞춘다.​​[참고 링크]https://github.com/facebookresearch/SlowFasthttps://arxiv.org/pdf/1812.03982.pdfhttps://junsk1016.github.io/deeplearning/SlowFast-Network/​​​​​+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++내가 일을 진행하며 사용했던 Auto Crawler git 주소https://github.com/YoongiKim/AutoCrawler​아래는 CV에 대해 참고할 만한 링크이다.https://openaccess.thecvf.com/menuhttps://paperswithcode.com/area/computer-vision​(pasing과 compile차이 -> 파싱은 문법적 해부, 컴파일은 기계어 번역) "
Meta-DETR: Image-Level Few-Shot Object Detection with Inter-Class Correlation Exploitation 논문 리뷰 ,https://blog.naver.com/kingjykim/222937402980,20221124,"Paper : https://arxiv.org/abs/2103.11731Git : https://github.com/ZhangGongjie/Meta-DETRAuthor : Gongjie Zhang, Zhipeng Luo, Kaiwen Cui, Shijian Lu​​1. Introduction​  컴퓨터 비전은 최근 몇 년간 상당한 발전을 경험하고 있다. 하지만 여전히 매우 적은 예시로부터 새로운 개념을 학습하는 시스템에서 현재 비전 기술과 인간의 시각 인지 능력 사이에 큰 격차가 존재한다. 대부분의 기존 비전 방법은 주석이 달린 대량의 샘플을 필요로 하는 반면, 인간은 매우 적은 예시로도 쉽게 새로운 개념을 인식할 수 있다. 특히 충분한 학습 샘플을 이용할 수 없거나 샘플들의 주석을 획득하기 힘들 때 인간의 능력과 같이 제한된 예시로 일반화하는 것은 인공지능 비전 시스템에게 매우 바람직하다.  본 연구에서, 적은 학습 샘플만으로 새로운 물체를 탐지해야 하는 few-shot object detection이라는 어려운 과제를 연구한다. 주석이 달린 샘플로부터 극도로 제한된 supervision의 핵심은 base class의 지식을 활용하고 이를 novel class로 일반화하는 것이다. 이를 위해 많은 연구가 메타 학습을 일반적인 객체 감지 프레임워크, 주로 Faster R-CNN로 통합하고 매우 유망한 결과를 달성했다. Figure 2: Existing few-shot detection frameworks tend to suffer from inaccurate region proposals and under-exploitation of inter-class correlation.  그들의 성공에도 불구하고, Fig. 2의 설명처럼 base class 지식을 더 많이 알아내는 것을 방해하는 두 가지 근본적인 한계가 여전히 존재한다. 첫째, 지역 기반 감지 프레임워크는 최종 예측에서 지역 제안에 의존하므로 낮은 퀄리티의 지역 제안에 민감하다. 안타깝게도, 이전 연구에서 조사한 바와 같이, few-shot detection 설정 하에서 제한된 정답을 가진 novel class에 대해 고품질의 지역 제안을 생산하는 것은 쉽지 않다. 이러한 지역 제안의 품질 격차는 base class에서 novel class로의 일반화하는 것을 방해한다. 둘째, 대부분의 기존 메타 학습 기반 접근법은 query 및 support 기능을 집계하기 위해 'feature reweighting' 또는 그 변형을 채택하며, 이는 한 번에 하나의 support class(즉, 감지할 대상 클래스)만 처리할 수 있으며 본질적으로 각 support class를 독립적으로 처리할 수 있다. 하지만 이러한 접근법은 single feed-forward 내에서 여러 클래스를 보지 않고, 서로 다른 support class 간의 중요한 클래스 간 상관관계를 간과한다. 이것은 유사한 클래스(예: 소와 양의 구별)를 구별하고 관련 클래스로부터 일반화하는 능력(예: 양의 감지로부터 일반화함으로써 소를 감지하는 법을 배우는 능력)을 제한한다. Figure 1: Comparison of few-shot object detection pipelines: Prior works (upper part) perform region-level detection, which are often constrained by inaccurate region proposals for novel classes.  위의 한계를 완화하기 위해 이미지 수준에서 메타 학습을 달성하는 동시에 서로 다른 support classes 간의 클래스 간 상관관계를 명시적으로 활용하는 혁신적인 few-shot object detector인 Meta-DETR을 설계한다. 이것은  proposal generation을 건너뛰고 이미지 수준에서 직접 감지를 수행할 수 있는 최근 제안된 DETR 탐지 프레임워크에 메타 학습을 통합하는 것을 탐구하는 첫 번째 작업이다. image-level meta-learning을 통해 제안된 Meta-DETR은 일반적인 few-shot 감지 프레임워크와 같이 부정확한 지역 제안의 제약을 완화한다. 또한, Fig. 1과 같이 Meta-DETR은 대부분의 기존 방법처럼 반복적인 실행으로 클래스별 메타 학습 대신 한 번에 여러 support classes를 참고할 수 있다. Meta-DETR은 여러 클래스를 포함하는 탐지 작업을 메타 학습에 통합함으로써 (i) 관련 클래스 간 일반화를 용이하게 하는 클래스 간 공통성과 (ii) 유사한 클래스 간의 분류 실패를 줄이기 위한 클래스 간 고유성을 포함하여 클래스 간 상관관계를 명시적으로 활용할 수 있다.  요약하면, 이 작품의 기여는 세 가지다. 먼저, 저자는 DETR 탐지 프레임워크에 금속 수익을 통합하는 혁신적인 few-shot 객체 탐지 프레임워크인 Meta-DETR을 제안한다. 최초의 pure image-level meta-detector인 Meta-DETR은 novel class 객체에 대한 부정확한 영역 제안의 격차를 피하여 novel class에 대한 더 나은 일반화를 가능하게 한다. 둘째, few-shot object detection를 위한 새로운 상관관계 종합 모듈을 설계하여 여러 support classes로 쿼리 기능을 동시에 종합할 수 있다. 클래스 간 상관관계를 효과적으로 활용할 수 있어 잘못된 분류를 크게 줄이고 모델 일반화를 향상시킨다. 셋째, 광범위한 실험에 따르면 매력적인 부가 기능들이 없을 경우 제안된 Meta-DETR은 최첨단 방법을 큰 폭으로 능가한다.​​2. Related Work​Object Detection 보통의 object detection 객체 위치 파악 및 분류에 대한 공동 작업이다. 기존 object detection는 대부분 지역 기반이며 크게 2단계 및 1단계 감지기의 두 가지 범주로 분류할 수 있다. 2단계 검출기에는 Faster R-CNN과 그 변형이 있다. 이들은 먼저 지역 제안을 생성하기 위해 Region Proposal Network(RPN)를 채택한 다음 제안을 기반으로 최종 예측을 생성한다. 이와 달리, 1단계 검출기는 밀집 배치된 앵커를 지역 제안으로 사용하고 직접 예측한다. 최근 또 다른 범주의 연구 방법인 DETR와 그 변형을 특징으로 하는 방법은 pure image-level framework, fully end-to-end pipeline 및 동등하거나 훨씬 더 나은 성능의 장점 덕분에 많은 관심을 받고 있다. 그러나 앞에서 언급한 기존 탐지기는 여전히 주석이 달린 훈련 샘플에 크게 의존하기 때문에 few-shot object detection에 직접 적용할 때 급격한 성능 저하를 겪을 것이다.​Few-Shot Object Detection 기존 few-shot object detection은 transfer learning과 meta-learning으로 분류할 수 있다. LSTD, TFA, MPS 그리고 FSCE를 포함한 Transfer-learning 기반 방법은 fine-tuning을 통해 새로운 개념이 학습된다. 이와 다르게 meta-learning 기반 방법은 '학습하는 방법을 학습', 즉 다양한 보조 작업에 대한 class-agnostic 예측 변수를 학습을 통해 다양한 작업을 일반화하여 지식을 추출한다.  제안한 Meta-DETR은 meta-learning 범위에 속하지만, image-level meta-learning 달성과 다양한 support classes 사이 상관관계를 효과적으로 활용하여 기존 방식과는 차이점이 있다. Meta-DETR는 제안된 DETR 프레임워크에 meta-learning을 통합한 첫 번째 작업이다. support classes 간의 상관관계를 meta-learning 기반 few-shot object detection 프레임워크에 명시적으로 통합하는 것도 선구적인 작업이다.​​3. Preliminaries​Problem Definition 두 개의 클래스인 Cbase와 Cnovel이 주어지며 Cbase ∩ Cnovel = ∅,   few-shot object detector의 객체 감지 범위 목표는 Cbase의 풍부한 주석이 달린 객체가 있는 기본 데이터 세트 Dbase와 Cnovel의 주석이 달린 객체가 매우 적은 새로운 데이터 세트 Dnovel에서 학습한 Cbase ∪ Cnovel의 객체를 감지하는 것이다. K-shot 객체 감지 작업에서 Dnovel의 각 novel class에 대해 정확히 K 개의 주석이 달린 객체가 있다.​Rethink Region-Based Detection Frameworks 기존의 대부분 few-shot object detection는 지역 기반 object detector에서 가장 성능이 좋은 Faster R-CNN에서 발전되었으며, 강력한 성능과 간편한 최적화 덕분에 많이 사용되었다. 하지만 감지 결과를 지역 제안에 의존하는 이 접근은 예상대로 few-shot 감지 세팅에 따른 매우 적은 예시 정답 때문에 novel classes의 부정확한 제안에 의해 제한된다. Fig. 2의 (a)에 설명대로, base class와 novel class 간 지역 제안의 퀄리티에는 분명한 차이가 있어서 지역 기반 감지 프레임워크가 novel classes를 일반화하여 완전히 base class의 지식을 습득하는 것을 방해한다. 여러 연구들은 보다 정확한 지역 제안을 획득하려고 시도하지만, 이 문제는 few-shot 학습 설정에서 지역 기반 감지 프레임워크에 뿌리를 두고 있기 때문에 여전히 남아 있다.​Rethink Meta-Learning via Feature Reweighting 다양한 클래스를 일반화할 수 있는 class-agnostic detector를 메타 학습하기 위해, 대부분의 기존 방법들은 ‘feature reweighting’이나 support class에 해당하는 객체를 감지하여 클래스 별 메타 특징을 획득한 뒤 support class 정보와 함께 query 특징을 종합하여 변형하는 것을 채택했다. 하지만, 이러한 종합 접근법은 각 feed-forward process에서 오직 한 개의 support class만 처리할 수 있다. 즉, 각 쿼리 이미지 내에서 C 클래스를 탐지하려면 C 반복 실행이 필요하다. 더 중요한 것은, 각 support class를 독립적으로 처리하여 ‘feature reweighting’은 서로 다른 support class 사이에 내부 클래스 간 상관관계를 간과한다는 것이다. Fig. 2의 (b)에서 보는 바와 같이, 유사한 외관을 가진 많은 객체 클래스들은 높은 상관관계를 가지고 있다. 직관적으로, 그들의 상관관계는 유사한 클래스 간의 구별과 일반화를 효과적으로 촉진할 수 있다. 그러나 Fig. 2의 (c)에서 보는 바와 같이, 기존 방법에서는 상관성이 높은 클래스로 잘못 분류된 개체가 내부 클래스 간 상관관계 부주의로 인해 주요 오류 원인이 된다는 것을 관찰했다.​ Figure 3: The framework of the proposed Meta-DETR.4. Meta-DETR​4.1 Model Overview  Fig. 3은 제안된 Meta-DETR의 아키텍처를 제시한다. 이전에 이야기했던 것을 바탕으로 Meta-DETR은 지역별 예측의 제약을 우회하기 위해 최근 제안된 Deformable DETR, fully end-to-end Transformer-based detector를 기본 감지 프레임워크로 사용한다. 게다가 메타 학습 동안 Meta-DETR은 다수의 support classes의 query 특징을 동시에 종합하기 때문에 다른 클래스 간 잘못된 분류를 줄이고 일반화를 강화하여 내부 클래스 상관관계를 습득할 수 있다.  구체적으로, 인스턴스 주석이 있는 query 이미지와 support 이미지 집합이 주어지면, 가중치 공유된 특징 추출기는 먼저 이들을 동일한 특징 공간으로 인코딩한다. 이후에 상세히 소개될 correlational aggregation module (CAM)은 query 특징과 support classes 집합 간의 매칭을 수행한다. CAM은 더 나아가 support classes 집합을 class-agnostic 방식으로 이러한 support classes를 차별화하는 pre-defined된 작업 인코딩 집합에 매핑한다. 마지막으로, 감지 결과는 객체의 위치와 해당 작업 인코딩을 예측하는 transformer 아키텍처를 통해 얻는다. 감지 대상은 support classes와 작업 인코딩을 통한 매핑에 의해 동적으로 결정되므로, 제안된 Meta-DETR은 메타 학습자로 훈련되어 특정 클래스에 특정되지 않은 일반화 가능한 지식을 추출한다.​4.2 Correlational Aggregation Module   CAM은 Meta-DETR에 학습 구성 요소로, 후속으로 class-agnostic 예측을 위해 support classes와 함께 query 특징을 종합한다. CAM은 동시에 다수의 support classes을 통합할 수 있어 내부 클래스 상관관계를 포착하여 잘못된 분류를 줄이고 모델 일반화를 향상시킬 수 있다는 점에서 기존 통합 방법들과 다르다. 구체적으로 Fig. 4와 같이, support와 query 특징이 주어지면 가중치 공유된 multi-head attention module은 먼저 같은 특징 공간에 인코딩하고 support 특징에 대한 평균 풀링을 적용하여 각 support class의 프로토타입을 얻는다. 그런 다음 CAM은 특징 매칭과 인코딩 매칭을 수행하며, 이는 하위 섹션의 나머지 부분에서 자세히 하여 쿼리 기능을 각각 지원 기능 및 작업 인코딩과 일치시킨다. 그들의 결과는 함께 합산되고 최종 출력을 생성하기 위해 feed-forward network(FFN)에 의해 처리된다.​Feature Matching 특징 매칭은 single-head attention mechanism으로 수행된다. 구체적으로, query 특징 맵 Q ∈ RHW x d와 support class 프로토타입 S ∈ RC x d가 주어지면, 매칭 계수는 다음을 통해 구해진다. 여기서 HW는 공간 크기, C는 support classes의 수, d는 feature 차원, W는 Q와 S가 공유하는 선형 투영법이다. 이후 다음을 통해 특징 매칭 모듈의 출력을 얻을 수 있다. 여기서 σ(·)는 시그모이드 함수를 나타내고, ◉는 아다마르 곱을 나타낸다. σ(S)는 query 특징에서 클래스 관련 기능만 추출하는 기능과 함께 각 개별 support class의 기능 필터 역할을 한다. 일치 계수 A를 σ(S)에 적용하여 어떤 support classes와도 일치하지 않는 기능을 필터링하여 주어진 support classes에 속하는 개체를 강조 표시하는 특징 맵 QF를 생성한다.​Encoding Matching class-agnostic 예측에 필요한 메타 러닝을 달성하기 위해 pre-defined된 일련의 작업 인코딩을 도입하고 주어진 support classes를 이러한 작업 인코딩에 매핑하여,  특정 클래스 대신 작업 인코딩에 대한 최종 예측을 할 수 있다. Transformer의 위치 인코딩을 따라 sinusoidal 함수를 사용하여 작업 인코딩 T∈ RCxd를 구현한다. 인코딩 매칭은 특징 매칭과 동일한 매칭 계수를 사용하며, 매칭된 인코딩 QE는 다음을 통해 얻어진다. Modeling Background for Open-Set Prediction Object detection는 target classes에 속하지 않는 배경이 종종 query 이미지의 공간 대부분을 차지하는 open-set 설정을 특징으로 한다. 따라서, 그림 4와 같이, 우리는 배경 클래스를 명시적으로 모델링 하기 위해 학습 가능한 프로토타입과 각각 BG-Prototype 및 BG-Encoding으로 표시되는 해당 작업 인코딩(0으로 고정)을 추가로 도입한다. 이렇게 하면 query가 지정된 support classes와 일치하지 않을 때 발생하는 모호성이 제거된다.​ Figure 5: Ablation study over the number of support classes for correlational aggregation under different few-shot setups.4.3 Training ObjectiveTarget Generation Meta-DETR의 감지 대상은 support classes와 작업 인코딩에 대한 매핑에 의해 동적으로 결정된다. 구체적으로, query 이미지가 주어지면, 서로 다른 support classes를 나타내는 C support 이미지가 무작위로 샘플링된다. 샘플링된 support classes에 속하는 정답 객체만 탐지 대상으로 유지된다. 또한, 각 객체에 대한 분류 대상은 정답 클래스 자체가 아닌 정답 클래스의 작업 인코딩이다. Fig. 5의 이 하이퍼 파라미터에 대한 절제 연구에 따라 경험적으로 C를 5로 설정했다.​Loss Function 제안된 Meta-DETR에 대한 손실 함수는 Deformable DETR을 따르며, 이는 이분 매칭을 통해 각 객체에 대해 예측하는 세트 기반인 Hungarian loss을 채택한다. Meta R-CNN에 이어, 저자는 설계된 CAM에 의해 얻은 클래스 프로토타입을 분류하기 위해 코사인 유사성 교차 엔트로피 손실을 추가로 도입한다. 그것은 서로 다른 클래스의 프로토타입이 서로 구별되도록 장려한다.​4.4 Training and Inference ProcedureTwo-Stage Training Procedure 학습 절차는 두 단계로 구성된다. 첫 번째 단계는 base training stage이다. 이 단계에서 모델은 각 base class에 대한 풍부한 훈련 샘플과 함께 base dataset인 Dbase에서 학습된다. 두 번째 단계는 few-shot fine-tuning stage이다. 이 단계에서는 제한된 훈련 샘플을 사용하여 base classes와 novel classes 모두에서 모델을 학습한다. K-shot object detection의 각 새로운 범주에 대해 K 객체 인스턴스만 사용할 수 있다. 이전 연구에 이어 base classes의 성능 저하를 방지하기 위해 base classes의 개체도 포함한다. 두 단계 모두에서, 네트워크는 이전에서 설명한 동일한 학습 목표를 가지며 end-to-end 방식으로 최적화된다.​Efficient Inference 학습 단계와 달리 support 이미지를 반복적으로 샘플링하고 기능을 추출할 필요가 없다. 먼저 각 support class의 프로토타입을 한 번에 모두 계산한 다음 예측할 모든 query 이미지에 직접 사용할 수 있다. 이것은 제안된 Meta-DETR의 효율적인 추론을 약속한다.​​5. Experiments​5.1 Datasets  few-shot object detection를 위해 잘 확립된 데이터 설정을 따른다. 구체적으로, 실험에는 널리 사용되는 두 개의 few-shot object detection 벤치마크가 채택된다.​Pascal VOC 20개 클래스의 객체 주석이 있는 이미지로 구성된다. 학습을 위해 train-val 07+12를 사용하고 test 07에 대한 평가를 수행한다. 저자는 세 가지 novel / base 클래스 분할, 즉 (""새"", ""버스"", ""소"", ""모터바이크"", ""소파"" / 기타), (""비행기"", ""병"", ""소"", ""말"", ""소파"" / ""기타""), (""보트"", ""고양이"", ""모터바이크"", ""양"", ""소파"" / ""기타"")를 사용한다. 예시 사진의 개수는 1, 2, 3, 5, 10개로 설정되어 있다. IoU 임계값 0.5에서의 평균 정밀도(mAP)가 평가 지표로 사용된다. 결과는 무작위로 샘플링된 10개 이상의 support 데이터 세트에 걸쳐 평균화된다.​MS COCO 이 데이터 세트는 Pascal VOC의 20개 클래스를 포함하여 80개 클래스를 가지고 있는 더 까다로운 객체 감지 데이터 세트이다. 20개의 공유 클래스를 novel classes로 채택하고, 나머지 60개의 클래스를 base classes로 채택한다. 예시 사진의 개수는 1, 3, 5, 10, 30개로 설정되어 있다. 학습을 위해 train2017을 사용하고 val2017에 대한 평가를 수행한다. MS COCO에 대한 표준 평가 지표를 채택한다. 결과는 무작위로 샘플링된 5개의 support 데이터 세트에 걸쳐 평균화된다.​5.2 Implementation Details  저자는 일반적으로 사용되는 ResNet-101을 특징 추출기로 채택한다. 네트워크 아키텍처와 하이퍼 파라미터는 Deformable DETR과 동일하게 유지된다. 저자는 다른 작업과 공정하게 비교하기 위해 모델을 single-scale 버전으로 구현한다. 또한 FsDetView를 따라 기능 재조정에 비해 약간 더 복잡한 체계로 집계를 구현한다. Deformable DETR에 이어 초기 학습 속도가 2×10-4이고 무게 감소가 1×10-4인 AdamW optimizer를 사용하여 Nvidia V100 GPU 8개로 모델을 훈련시킨다. Batch size는 32로 설정된다. 기본 학습 단계에서 Pascal VOC 및 MS COCO 모두에 대해 50개 시대에 대한 모델을 훈련한다. 학습률은 45th epoch에서 0.1만큼 감소한다. few-shot fine-tuning 단계에서는 동일한 설정을 적용하여 수렴할 때까지 모델을 fine-tune 한다.​5.3 Comparison with State-of-the-Art MethodsPascal VOC Table 1은 Pascal VOC의 novel classes에 대한 few-shot detection 성능을 보여준다. Meta-DETR은 다양한 설정에서 기존 방법을 지속적으로 능가한다는 것을 알 수 있다. 랜덤성을 줄이기 위해 무작위로 샘플링된 support 데이터 세트에 대한 여러 번의 실행을 통해, 저자의 방법은 두 번째로 우수한 방법에 비해 +4.6%의 큰 마진으로 모든 설정에서 최고의 평균 성능을 달성한다. 강력한 성능은 제안된 방법의 우수성과 견고성을 보여준다.​MS COCO Table 2는 MS COCO에 대한 결과를 보여준다. MS COCO는 occlusion 및 대규모 변형과 같은 복잡성 때문에 Pascal VOC보다 훨씬 어렵지만,  Meta-DETR은 여전히 모든 설정에서 기존의 모든 방법들보다 훨씬 더 좋은 결과를 얻는다는 것을 알 수 있다. 이것은 잠재적으로 MS COCO에서 더 많은 클래스 간의 상관관계를 효과적으로 활용한 것으로 볼 수 있다. 또한, Meta-DETR은 더 엄격한 metric AP0.75에서 지역 기반의 다른 방법과 비교하여 예외적으로 우수한 성능을 발휘하며,  이는 저자의 방법이 부정확한 지역 제안의 제약을 효과적으로 제거하여 더 정확한 탐지 결과를 생성할 수 있음을 의미한다.​5.4 Ablation Studies  저자는 아키텍처 설계 선택의 효과를 검증하기 위해 종합적인 절제 연구를 수행한다. 모든 결과는 Pascal VOC의 첫 번째 클래스 분할에서 서로 다른 무작위로 샘플링된 support 데이터 세트를 사용하여 10회에 걸쳐 평균화된다.​Region-Level vs. Image-Level Table 1과 Table 2에서, Deformable DETR (Deformable-DETR-ft-full)의 fine-tuning이 일반적으로 Faster R-CNN (FRCN-ft-full)의 fine-tuning을 능가한다는 것을 알 수 있는데, 특히 MS COCO 데이터 세트에서 복잡성이 높아 novel classes에 대한 정확한 지역 제안을 얻기가 훨씬 어렵다. 이러한 관찰은 지역 기반 프레임워크가 novel classes의 부정확한 지역적 제안으로부터 어려움을 겪는 경향이 있다는 것을 표의 정렬로 보여준다. 이미지 수준의 few-shot object detection의 우수성을 추가로 검증하기 위해 FsDetView, Faster R-CNN 위에 구축된 최첨단 메타 학습 기반 few-shot detector를 저자의 방법과 비교할 수 있는 견고한 기준으로 채택한다. 공정한 비교를 위해 FsDetView에 deformable transformer를 추가하여 transformer 아키텍처가 가져오는 성능 차이를 배제한다. 또한 Meta-DETR에서 제안된 CAM을 FsDetView(Meta-DETR/o CAM으로 표시됨)의 특징 집계 모듈로 대체한다. Table 3에서 볼 수 있듯이 정렬된 네트워크 아키텍처 및 집계 체계에서도 Meta-DETR w/o CAM은 대부분의 설정에서 FsDetView + Deform Transformer를 여전히 능가한다. 결과는 이미지 수준에서 few-shot object detection를 해결하는 것의 우수성을 검증한다.​ Figure 6: t-SNE visualization of objects learned in the feature space with and without our designed CAM.Impact of Correlational Aggregation Module (CAM) Table 4에서 알 수 있듯이 CAM을 모델에 통합할 때 support classes 수를 1로 유지하더라도 CAM이 다른 support classes 간 상관관계를 명시적으로 활용할 수 없음을 의미하는 경우일지라도 CAM은 모든 설정에서 few-shot detection 성능을 향상시킬 수 있다. 이는 query 및 support 정보를 집계하는 CAM의 강력한 능력을 보여준다. 여러 support classes를 사용할 수 있는 경우, CAM은 특히 1-shot(+4.8% mAP) 및 2-shot(+5.0% mAP)에서 lower-shot(≤5) 설정에서 few-shot 탐지 성능을 향상시키기 위해 클래스 간 상관관계를 활용할 수 있다. 10-shot에 대해 명확한 성능 향상이 관찰되지 않으며, 이는 더 많은 학습 샘플을 사용할 수 있을 때 detector가 클래스 간 상관관계를 명시적으로 모델링 하지 않고도 novel classes를 이미 인식하고 유사한 클래스와 차별화할 수 있음을 의미한다. 저자는 또한 설계된 CAM을 일반적으로 사용되는 지역 기반 메타 탐지기 FsDetView에 적용하고 그 결과를 Table 5에 보고한다. 꾸준한 성능 향상은 CAM의 강력한 적응력을 보여준다.  Fig. 6 내부 클래스 간 상관관계의 명시적 활용을 포함하거나 포함하지 않고 학습된 특징 공간에서 서로 다른 클래스의 객체를 시각화하는 것을 추가로 보여준다. 표시된 것처럼 내부 클래스 간 상관관계를 포착하기 위해 CAM이 도입됨에 따라 개체 클래스가 서로 더 잘 분리되어 유사한 클래스 간의 잘못된 분류를 줄이기 위해 클래스 간 상관관계를 활용하려는 동기를 확인할 수 있다. ​Number of Classes for Correlational Aggregation Meta-DETR은 고정된 수의 support classes를 수신하고 query 기능과 동시에 이를 집계하여 서로 다른 support classes 간의 상관관계를 포착한다. Fig. 5는 한 번에 집계할 support classes의 수가 미치는 영향을 조사한다. 지원 클래스의 수가 1에서 10으로 증가하면 lower-shot(≤5) 검출 성능이 먼저 향상된 후 감소하는 반면, 10-shot 성능은 먼저 포화된 후 감소한다. 이것은 또한 lower-shot 설정에서 클래스 간 상관관계를 활용하는 것의 효과를 검증한다. 저자는 상관관계 집계를 위한 많은 support classes에서 성능 저하가 모델의 제한된 용량으로 인해 너무 많은 support classes를 한 번에 차별화할 수 없기 때문이라고 추측한다. 결과에 기초하여, 달리 명시되지 않는 한 다른 모든 실험에서 저자가 제시한 방법의 support classes 수를 5로 설정했다.​Impact of Explicitly Modeling Background Table 4는 또한 백그라운드에 대한 프로토타입 및 작업 인코딩을 명시적으로 모델링 하는 효과를 검증하며, 이를 통해 query 특징이 support classes와 일치하지 않는 'no match' 시나리오를 더 잘 처리할 수 있다.​​6. Conclusion​  본 논문은 새로운 few-shot object detection 프레임워크, 즉 Meta-DETR을 제시한다. 제안된 프레임워크는 다음과 같이 2가지를 달성한다.​(i) novel classes의 부정확한 지역 제안으로 인한 제약을 완화하는 pure image-level meta-learning.(ii) 유사하거나 관련된 클래스 간의 잘못된 분류를 줄이고 일반화를 향상시키는 내부 클래스 간 상관관계의 효과적인 활용.​단순함에도 불구하고, 저자의 방법은 여러 개의 few-shot object detection 설정에 비해 최첨단 성능을 달성하여 이전 작업을 큰 폭으로 능가한다. 본 연구가 좋은 통찰력을 제공하고 few-shot object detection에 대한 추가 연구에 영감을 줄 수 있기를 바란다.​​​2022.11.24 "
End-to-End Object Detection with Transformers ,https://blog.naver.com/sjg918/222774061205,20220615,"2. Related work2.1 set prediction집합을 직접 예측하는 표준 딥 러닝 모델은 없습니다.집합 예측 작업은 다중 레이블 분류로, 기본 접근 방식인 one-vs-rest는 요소 사이에 기본 구조가 있는 detection과 같은 문제에  적용되지 않는다.​one-vs-rest ??https://machinelearningmastery.com/one-vs-rest-and-one-vs-one-for-multi-class-classification/ One-vs-Rest and One-vs-One for Multi-Class ClassificationNot all classification predictive models support multi-class classification. Algorithms such as the Perceptron, Logistic Regression, and Support Vector Machines were designed for binary classification and do not natively support classification tasks with more than two classes. One approach for using...machinelearningmastery.com Not all classification predictive models support multi-class classification.Algorithms such as the Perceptron, Logistic Regression, and Support Vector Machines were designed for binary classification and do not natively support classification tasks with more than two classes.​One approach for using binary classification algorithms for multi-classification problems is to split the multi-class classification dataset into multiple binary classification datasets and fit a binary classification model on each.zzz​it involves splitting the multi-class dataset into multiple binary classification problems. A binary classifier is then trained on each binary classification problem and predictions are made using the model that is the most confident.​For example, given a multi-class classification problem with examples for each class ‘red,’ ‘blue,’ and ‘green‘. This could be divided into three binary classification datasets as follows: ㅋㅋㅋㅋㅋ 푸하하하핫 ㅋㅋㅋA possible downside of this approach is that it requires one model to be created for each class. For example, three classes requires three models. This could be an issue for large datasets (e.g. millions of rows), slow models (e.g. neural networks), or very large numbers of classes (e.g. hundreds of classes).ㅋㅋㅋㅋㅋㅋㅋ​​이러한 작업의 첫 번째 어려움은 거의 중복을 피하는 것입니다. 대부분의 현재 감지기는 이 문제를 해결하기 위해 최대가 아닌 억제와 같은 후처리를 사용하지만 직접 집합 예측에는 후처리가 필요하지 않습니다. 중복을 피하기 위해 예측된 모든 요소 간의 상호 작용을 모델링하는 전역 추론 체계가 필요합니다. 일정한 크기의 집합 예측의 경우 조밀한 완전 연결 네트워크[9]로 충분하지만 비용이 많이 듭니다. 일반적인 접근 방식은 순환 신경망과 같은 자동 회귀 시퀀스 모델을 사용하는 것입니다[48]. 모든 경우에 손실 함수는 예측의 순열에 의해 불변해야 합니다. 일반적인 솔루션은 헝가리 알고리즘[20]을 기반으로 손실을 설계하여 실제와 예측 간의 이분법 일치를 찾는 것입니다. 이것은 순열 불변성을 적용하고 각 대상 요소가 고유한 일치를 갖도록 보장합니다. 우리는 bipartite matching loss 접근법을 따릅니다. 그러나 대부분의 이전 작업과 달리 자동 회귀 모델에서 벗어나 transformers with parallel decoding를 사용합니다. 이에 대해 아래에서 설명합니다.​2.2 transformer and parallel decodingtransformer는 Vaswani et al에 의해 소개되었습니다. 기계 번역을 위한 새로운 어텐션 기반 빌딩 블록으로 [47] 어텐션 메커니즘[2]은 전체 입력 시퀀스에서 정보를 집계하는 신경망 계층입니다. Transformers는 Non-Local Neural Networks[49]와 유사하게 시퀀스의 각 요소를 스캔하고 전체 시퀀스의 정보를 집계하여 업데이트하는 self-attention layer를 도입했습니다. 주의 기반 모델의 주요 장점 중 하나는 전역 계산과 완벽한 메모리로, 긴 시퀀스에서 RNN보다 더 적합합니다. 트랜스포머는 이제 자연어 처리, 음성 처리 및 컴퓨터 비전의 많은 문제에서 RNN을 대체하고 있습니다[8,27,45,34,31]. 트랜스포머는 초기 시퀀스-투-시퀀스 모델[44]에 따라 자동 회귀 모델에서 처음 사용되어 출력 토큰을 하나씩 생성했습니다. 그러나 엄청난 추론 비용(출력 길이에 비례하고 일괄 처리가 어려움)은 오디오[29], 기계 번역[12,10], 단어 표현 학습[8], 음성인식 [6] 영역에서 병렬 시퀀스 생성의 개발로 이어집니다. 우리는 또한 계산 비용과 집합 예측에 필요한 전역 계산을 수행하는 능력 간의 적절한 절충을 위해 변환기와 병렬 디코딩을 결합합니다.​2.3 object detectionMost modern object detection methods make predictions relative to some initial guesses. Two-stage detectors [37,5] predict boxes w.r.t. proposals, whereas single-stage methods make predictions w.r.t. anchors [23] or a grid of possible object centers [53,46].  Recent work [52] demonstrate that the final performance of these systems heavily depends on the exact way these initial guesses are set.[52]논문 ->Bridging the Gap Between Anchor-based and Anchor-free Detection via Adaptive Training Sample Selection의 결론:이 작업에서 우리는 1단계 앵커 기반 및 센터 기반 앵커가 없는 감지기의 본질적인 차이점이 실제로 양성 및 음성 훈련 샘플의 정의임을 지적합니다. 이는 물체 감지 훈련 중에 양성 및 음성 샘플을 선택하는 방법이 중요함을 나타냅니다. 이에 영감을 받아 이 기본 문제를 탐구하고 객체의 통계적 특성에 따라 양성 및 음성 훈련 샘플을 자동으로 분할하여 앵커 기반 검출기와 앵커 프리 검출기 간의 격차를 해소하는 적응형 훈련 샘플 선택을 제안합니다. 또한 위치당 여러 앵커를 타일링해야 하는 필요성에 대해 논의하고 현재 상황에서는 그렇게 유용한 작업이 아닐 수 있음을 보여줍니다. 도전적인 벤치마크 MS COCO에 대한 광범위한 실험은 제안된 방법이 추가 오버헤드를 도입하지 않고 최첨단 성능을 달성할 수 있음을 보여줍니다.라고함. 그래서 ?In our model we are able to remove this hand-crafted process and streamline the detection process by directly predicting the set of detections with absolute box prediction w.r.t. the input image rather than an anchor.streamline 이 간소화하다 뭐 이런 뜻인듯하다.​Set-based loss.여러 물체 감지기[9,25,35]는 bipartite matching loss을 사용했습니다. 그러나 이러한 초기 딥 러닝 모델에서 서로 다른 예측 간의 관계는 합성곱 또는 완전 연결 레이어로만 모델링되었으며 손으로 디자인한 NMS 후처리는 성능을 향상시킬 수 있습니다.이게뭔 ㅋㅋ bipartite matching loss를 썻다는게 뭔 개소리인지[9]:scalable object detection using deep neural network[25]:Ssd: Single shot multibox detection[35]:you only look once: Unified,real-time object detectionSSD와 YOLO loss를 생각해보면...모든 앵커박스에 대해서 gt와의 iou를 계산해서, 가장높은 앵커박스를 gt와 매칭되는것으로 간주하고, 거기에 loss를 발생시킨다. 그런데 저자는 그걸 ? 이분 매칭이라고 이야기한다. 뭐 틀린말은 아니다. 매칭이되거나 되지 않거나 둘 중 하나이니.​보다 최근의 검출기[37,23,53]는 NMS와 함께 정답과 예측 사이에 고유하지 않은 할당 규칙을 사용합니다.More recent detectors [37,23,53] use non-unique assignment rules between ground truth and predictions together with an NMS.[37]:faster R-CNN: Towards real-time objectdetection with region proposal network[23]:focal loss for dense object detection[53]:objects as points먼 ~ 개소리인지 ~ [53]에 규칙같은게 어딧음? ㅋㅋㅋ ""non-unique"" ??? ㅋㅋㅋ학습 가능한 NMS 방법[16,4] 및 관계 네트워크[17]는 주의를 기울여 서로 다른 예측 간의 관계를 명시적으로 모델링합니다. 직접 세트 손실을 사용하면 후처리 단계가 필요하지 않습니다. 그러나 이러한 방법은 제안 상자 좌표와 같은 추가 손으로 만든 컨텍스트 기능을 사용하여 탐지 간의 관계를 효율적으로 모델링하는 반면 모델에 인코딩된 사전 지식을 줄이는 솔루션을 찾습니다.​Recurrent detectors. <- 이새낀 아예들어본적도없음 ㅋㅋ우리의 접근 방식에 가장 가까운 것은 객체 감지[43] 및 인스턴스 분할[41,30,36,42]에 대한 종단 간 집합 예측입니다. 우리와 유사하게 그들은 경계 상자 세트를 직접 생성하기 위해 CNN 활성화를 기반으로 하는 인코더-디코더 아키텍처와 함께 이분법 일치 손실을 사용합니다. 그러나 이러한 접근 방식은 최신 기준이 아닌 작은 데이터 세트에서만 평가되었습니다. 특히, 자동회귀 모델(더 정확하게는 RNN)을 기반으로 하므로 병렬 디코딩이 있는 최신 transformer를 활용하지 않습니다.​3. The DETR model탐지에서 직접 집합 예측에는 두 가지 요소가 필수적입니다. (1) 예측된 상자와 정답 상자 간의 고유한 일치를 강제하는 집합 예측 손실(2) (단일 패스에서) 일련의 객체를 예측하고 이들의 관계를 모델링하는 아키텍처.그림 2에서 아키텍처를 자세히 설명합니다.​ 요게 fig2인데. FFN라길래 햇갈렷네 시 -팔넘들 ㅋ MLP라는 좋은 말을 두고 굳이 FFN이라 표현 했어야했나.class MLP(nn.Module):    """""" Very simple multi-layer perceptron (also called FFN)"""""" <-- ㅋㅋ그런ㄷ ㅔ 저 object queries란놈들은 대체 정체가 뭐임? ㅋㅋ​​생각해보니까 . . . ​원래 트랜스포머는 이렇게 생겼다. . . . 그런데 이 친구만 보고있다보니까 오른쪽의 존재를 완전히 잊어버렸음 ㅋㅋ~ 그래서 object queries가 뭐임??​Image features from the CNN backbone are passed through the transformer encoder, together with spatial positional encoding that are added to queries and keys at every multihead self-attention layer. Then, the decoder receives queries (initially set to zero), output positional encoding (object queries), and encoder memory, and produces the final set of predicted class labels and bounding boxes through multiple multihead self-attention and decoder-encoder attention. ​the first self-attention layer in the first decoder layer can be skipped.먼 ~ 씨 ~~~발개소리인지 ㅋㅋㅋ그래서 논문에 공개한 코드를 봤더니 진짜 query_pos다. . . (머쓱) . . . 생략해도된다는건 진짜인가 ?이게맞는거임? 객체 쿼리는 너무너무너무너무너무너무너무 애매한 설정이 아닌가 싶다.  ViT처럼 했어야하는게 아닌가. 심지어 the first self-attention layer in the first decoder layer can be skipped.란다. ㅋㅋ.​https://github.com/facebookresearch/detr/blob/8a144f83a287f4d3fece4acdf073f387c5af387d/models/transformer.py#L276 detr/transformer.py at 8a144f83a287f4d3fece4acdf073f387c5af387d · facebookresearch/detrEnd-to-End Object Detection with Transformers. Contribute to facebookresearch/detr development by creating an account on GitHub.github.com ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ ㅋㅋ ㅋ ㅋ ㅋㅋtgt = torch.zeros_like(query_embed) ㅋㅋㅋ ㅋㅋㅋ ㅋㅋㅋㅋㅋㅋㅋㅋ 본인이 페이스북주식을 진작에 다 팔아제꼈다면 개추 ㅋㅋㅋ라고할뻔. 이논문의 진가는 (1) 예측된 상자와 정답 상자 간의 고유한 일치를 강제하는 집합 예측 손실(2) (단일 패스에서) 일련의 객체를 예측하고 이들의 관계를 모델링하는 아키텍처. 중에서 (2)가아니라 (1)이다.​그런데. ㅋㅋㅋ 시 ~발 이걸 수정해서 그 ""Swin""을 이긴 알고리즘이 있다 zzz역시 사람들 생각하는게 다 똑같지tgt = torch.zeros_like(query_embed) 이런거 절대 못참거든요~그런데 이게 의미가 있을까 ?A 100으로 10fps가 나오면 . . . 현실성이없는게 아닐까요...​​​​(1) 예측된 상자와 정답 상자 간의 고유한 일치를 강제하는 집합 예측 손실.​1) we compute hungarian assignment between ground truth boxes and the outputs of the model​# Compute the classification cost. Contrary to the loss, we don't use the NLL,# but approximate it in 1 - proba[target class].# The 1 is a constant that doesn't change the matching, it can be ommitted.cost_class = -out_prob[:, tgt_ids]​# Compute the L1 cost between boxescost_bbox = torch.cdist(out_bbox, tgt_bbox, p=1)​# Compute the giou cost betwen boxescost_giou = -generalized_box_iou(box_cxcywh_to_xyxy(out_bbox), box_cxcywh_to_xyxy(tgt_bbox))​C=self.cost_bbox*cost_bbox+self.cost_class*cost_class+self.cost_giou*cost_giou​크기는 전부 MxN인데 . . .costbbox -> iou가 높을수록 cost가 낮아짐costclass -> 올바른 예측을 할 수록 cost가 낮아짐costgiou -> iou가 높을수록 cost가 낮아짐그러면 cost가 낮은 걸 매칭을 잘 시켜줌? 헝가리안 방법이 ?​from scipy.optimize import linear_sum_assignmentimport numpy as npcost = np.array([[-3, -8], [-4, -12], [-8, -4]])row_ind, col_ind = linear_sum_assignment(cost)​[1 2][1 0]아주잘매칭시켜준다.  cost가 낮을수록 gt와 매칭이 된다.그런데 주의할점은, batch 단위로 해야한다는거.​sizes = [len(v[""boxes""]) for v in targets]indices = [linear_sum_assignment(c[i]) for i, c in enumerate(C.split(sizes, -1))]​그 다음, row_ind와 col_ind를 리턴한다. WA SANS !!!return [(torch.as_tensor(i, dtype=torch.int64), torch.as_tensor(j, dtype=torch.int64)) fori, jinindices]​여기서 M은 ? transformer에 들어가는 총 픽셀의 수 x 배치.N은? 배치 x gt 박스 수.​그래서 ? 이 매칭 결과를 가지고 어떻게 loss 를 계산함 ?? (매칭이 안된녀석들의 정보들은 다 버렸음지금 ㅋ)num_classes: number of object categories, omitting the special no-object categoryFor efficiency reasons, the targets don't include the no_object. Because of this, in general,라는데요.이게 말이 되는거임? 추론때 객체가 아닌녀석을 어떻게 찾아냄 ? ? ? ?​prob=F.softmax(out_logits, -1)scores, labels=prob[..., :-1].max(-1)이게끝이다. 허허.대규모데이터셋이 아니면 절대로 활용이 불가능한 알고리즘 되겠다 ~~~~거기다 배치가 그래픽카드당 1개가 올라간다. 이건  뭐 ~ ~ ㅋㅋㅋ​ 구라를 오지게 쳐놨네. Ci=타겟클래스라벨인데 뭐 ? ㅋㅋㅋㅋ "
[Object Detection 논문] RCNN(Regions with CNN features) ,https://blog.naver.com/ongbbb/222816888626,20220718,"Abstractsimple & scalable detection algorithm을 제안  1) bottom-up방식의 regoion proposal을 통한 localize와 segement object 2) labeled training data 부족할 때, pretrained 모델 활용해 fine-tuning했더니 성능 굿sliding window를 사용한 OverFeat의 성능을 뛰어넘었다 → mAP​*scalable : R-CNN은 hashing과 같은 기법 없이도 수천개의 object class로 확장 가능함*mAP : 모든 class에 대하여 Precision/Recall의 값을 avg취한 것 문제 제기sliding window는 large receptive field를 가짐 sliding window를 적용하기 위해서는 input image 사이즈가 커야하기에 속도가 떨어질 수 있다는 의미large CNN을 학습시키기에 labeled training data가 부족한 경우가 있음​ 이 논문의 제안selective search를 사용large dataset학습한 supervised pre-training 모델 활용해 적은 dataset으로 fine-tuning → 데이터가 부족할 때 high-capacity CNN 학습시키기 굿(mAP가 8%증가함)    1) large auxiliary dataset에서 CNN에서 pretained (이미지 분류 수준에서)    2) Domain-specific fine tuning: warped region proposals로 CNN 파라미터들을 fine tuning(SGD)​같은 region proposal을 사용했을 때 spatial pyramid and bag-of-visual-words 보다 성능 굿OverFeat보다도 성능 굿​​​ ArchitectureYOLO, SSD는 1-stage detector였는데 R-CNN은 2-stage detector 이다 1) region proposals(selective search를 통해)​2) 각 region에 대해 fixed-length(4096dim) feature vector 추출  ( large convolutional layers 통해)   2000 x 4097input proposal  size를 227 x 227로 wrapt한후 적용 *데이터를 고정된 크기로 바꾸는 작업을 warping​3) linear SVMs로 classify  2000 x Nfeatures이 추출되면 각 클래스별로 하나의 SVM을 최적화시킴training data가 너무 커 메모리 문제 발생하므로 standard hard negative mining method적용(*실제 negative인데 postive로 잘못에측하기 쉬운데이터로, 이 데이터를 모아 학습시키면 False positive에러에 강해짐)​​​+) 박스 위치 조정(BBox regression=localization)​​​Test-time detection2000개 region proposal > 각 proposal를 warp > 각 class에 대해 SVM을 사용해 score 계산> 각 클래스에 대한 점수를 가진 proposal들에 대해 NMS 방법론 적용​Run-time analaysis1) CNN은 파라미터를 공유함2) 다른 접근법(bag-of visual word encoding)과 달리 CNN에 의해 계산된 feature vectors 는 low dim이다​​​​ Loss function, Bbox regrssion G에 최대한 가깝게 이동시키기 위한 수식으로 P는 하나의 proposal​​ d는 위와같이 pool5에서 나온 features 이용한 linear functionw가 update하게 될 파라미터이며 ★은 w,h,x,y를 의미​ Loss function을 위와같이 정의​​ ​ 실험 & 결과(left) Mean average precision on the ILSVRC2013 detection test set(right) Box plots for the 200 average precision values​​​​ 1-stage vs 2-stage2-stage는 Regional Proposal과 Classification이 순차적으로 이루어짐1-stage는 Regional Proposal과 Classification이 동시에 이루어짐 → classification과 localization 문제를 동시에 해결 https://modernflow.tistory.com/109​  sliding-window고정된 크기 윈도우를 이미지 왼쪽 상단에서 오른쪽 하단으로 점진적으로 이동하면서 객체가 있을 만한 Region들을 Proposal(제안)해주는 것큰 이미지에서 여러가지 object를 찾기 위해, 전체 이미지를 적당한 크기의 영역으로 나눈 후에, 각각의 영역에 대해 이전 스텝에서 만든 Localization network를 반복 적용해 보는 방식Convolution의 kernel이 sliding하는 것처럼​​​문제점매우 많은 갯수의 window 영역에 대해 이미지의 localization을 진행해야하므로 처리해야할 window 갯수만큼 시간이 더 걸림window는 고정된 사이즈인데 단일크기 window로 이를 커버하기는 어려워 속도 문제 더 심각객체가 없는 지역도 슬라이딩을 무조건 하기 때문에 슬라이딩 윈도우를 하는 데 시간이 많이 걸릴 뿐더러 객체를 잘 탐지할 확률도 낮아지게 된다​​​​ Selective Search개별 픽셀들 초기화 →가장 유사성 높은 픽셀 두개 선택→ 두 영역을 하나로 합침→ 하나의 region이 될때까지 반복​유사도(이미지 픽셀의 컬러, 무늬, 크기, 형태) 기반으로 계층적 그룹핑해나가는 방식반복적으로 Selective Search 알고리즘을 이용해 최적의 바운딩 박스를 선정해 Region ProposalSelective Search의 유사성은 [0,1]사이로 정규화된 4가지 요소(Color, Texture, Size, Fill)들의 가중합으로 계산됨 https://velog.io/@ganta/Object-detection​​​ SVMdecision boundary(초평면)를 찾아 분류 or 회귀를 수행하는 방법최대 마진을 가지는 선형 판별로 feature들간 의존성은 고려하지 않음Decision boundary까지 최소 거리를 갖는 모든 점을 support vectos라 한다(최소 거리=margine)​데이터를 고차원으로 가져오면 linearly separable하게 할 수 있음but 저차원> 고차원 시 연산량이 너무 많아짐, 해결 >""kernal trick"" SVM 하이퍼파라미터cost: margine을 얼마로 할지, cost가 적으면(margin이 크면) training error가 높아질 가능성(경계와 가까운 데이터 구별 못하는 경우,but test에서 좋은 성능 나타날 수도 있음, underfitting 가능성)Gamma: training 데이터 하나당 영향을 끼치는 범위최적 파라미터 설정은 Grid search로​​ Bag of visual wordsNLP에서 단어 빈도수에 기반해 만든 벡터를 의미하는 bag of words와 유사이미지의 region별로 여려개의 feature들을 추출, 각 feature별 나타나는 빈도수를 count해 히스토그램 생성어떤 feature들이 많이 나타나면 특정 object일 것이다 라는 추론단점: 이미지의 위치 특성은 고려하지 못함 → 각 region별로 histogram 생성해 해결 가능​​​​ 궁금했던 것들왜 1-stage가 2-stage보다 빠른가, 진짜 더 빠른가?Loss function 을 보면 1-stage는 localization과 classification 텀이 함께 들어가있고, 파라미터들을 동시에 업데이트함 region proposal 자체가 selective search(병렬처리x)나 sliding window를 적용하는데 이 과정 자체도 시간이 걸리며, 이후에 classification을 위한 resize과정이 필요해 시간이 더 걸림 즉 1-stage는 proposal 과정과 pixel이나 box resize과정이 없으므로 더 빠름region proposal이 빨라지면 둘이 비슷해질까?1번보다는 2번의 이유가 큼​sliding window는 large receptive field를 가짐, 이게 안좋은 건가?  - 이전 SSD에서는 큰 receptive field를 얻기 위해  atrous conv를 이용하기도 했음​왜 classify에서 메모리 문제 해결 방법으로  standard hard negative mining method 쓰는지?메모리 문제 해결이란 건 데이터를 줄여서 모델을 학습시키되 성능은 충분히 나오도록 하고싶다는 것. 데이터를 1)실제 negative인데 맞추기 쉬운거/ 2) 맞추기 어려운 거가 있다고 해보자. 데이터를 줄인다고 하면 1)의 데이터를 줄인다는 것 대신 테스트에서 1과 같은 데이터에 대해 잘 맞출 수 있어야함​1)의 데이터 없이 모델을 학습시켜도 테스트 단계에서 잘 맞출 수 있을까?  맞추기 어려운 데이터를 위주로 학습한 모델은 각 클래스의 이미지 특징들에 대해 학습했을 것. 즉, 모델은 각 클래스가 가진 각각의 특징들을 학습한 것이므로 맞추기 쉬운 데이터에 대해서도 잘 할 것이다.   SVM이니까 더 잘 맞출 수 있을 거 같다! boundary 근처 데이터가 맞추기 어려운 데이터인데 맞추기 쉬운 데이터들은 boundary와는 멀리 있으므로 모델이 1의 데이터를 학습하지 않았더라도 잘 맞출 것​이 standard hard negative mining method 은 어떻게 구현하는지   테스트 단계에서 FN데이터들에 대해 테스트   데이터를 학습 시킨 후에 FN 데이터들을 따로 모아서 다시 학습하거나 이 데이터들에 대해서는 penalty 주기​​​​​​​​​​​​​​​​​참고사이트https://ysbstudy.tistory.com/27 R-CNN 논문 리뷰R-CNN(Regions with CNN features) 논문 리뷰 먼저 컴퓨터비전의 문제를 크게 4가지로 분류하면 1. Classificaion 2. Object Detection 3. Image Segmentation 4. Visual relationship 으로 나눌 수 있다. Classi..ysbstudy.tistory.com https://aigong.tistory.com/34 R-CNN 논문 Full SummaryR-CNN 논문 Full Summary arxiv : https://arxiv.org/abs/1311.2524 Rich feature hierarchies for accurate object detection and semantic segmentation - Ross Girshick, Jeff Donahue, Trevor Darrell, ..aigong.tistory.com 원문: https://arxiv.org/pdf/1311.2524.pdf​SVM 이미지: https://sanghyu.tistory.com/14#:~:text=Kernel%20trick(%EC%BB%A4%EB%84%90%ED%8A%B8%EB%A6%AD)%EA%B3%BC%20Kernel(%EC%BB%A4%EB%84%90)&text=%EC%9C%84%EC%9D%98%20%EB%85%B8%EB%9E%80%ED%98%95%EA%B4%91%ED%8E%9C%EC%9D%84,%ED%95%9C%EB%8B%A4%EA%B3%A0%20kernel%20trick%EC%9D%B4%EB%9D%BC%EA%B3%A0%20%ED%95%9C%EB%8B%A4.​​selective search vs sliding window: https://techblog-history-younghunjo1.tistory.com/178#:~:text=Selective%20Search%EB%9E%80%2C%20%EC%9D%BC%EC%A2%85%EC%9D%98%20%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98,%EC%9C%BC%EB%A1%9C%20%EA%B3%84%EC%82%B0%ED%95%98%EB%8A%94%20%EB%B0%A9%EC%8B%9D%EC%9D%B4%EB%8B%A4.​selective search https://donghwa-kim.github.io/SelectiveSearch.html Selective Search (선택적 탐색)Selective Search Bounding boxe들을 찾아주는 super pixel기반( 엣지를 잘 표현하는 patch )의 selective search는 hierarchical grouping algorithm 방식을 사용 학습 방법 Definition $R$: 선택된 region 후보들 {$r_{1}, r_{2},…$} $S$: region들의 유사도 집합 {$s(r_{i}, r_{j})$,…} $r_{1}, r_{2},…r_{n}$들을 초기화 가장 유사성이 높은 $s($ $r_{i}$ , $r_{j}$ $)$을 선택 선...donghwa-kim.github.io ​​ [ML] Object Detection 기초 개념과 성능 측정 방법🔊 해당 포스팅에서 사용된 컨텐츠는 인프런의 딥러닝 컴퓨터 비전 완벽 가이드 강의 내용을 기반으로 했음을 알립니다. 설명에서 사용된 자료는 최대한 제가 직접 재구성한 자료임을 알립니다. 최근에 인프런에서..techblog-history-younghunjo1.tistory.com ​bag of visusal words https://techblog-history-younghunjo1.tistory.com/180 [ML] SPP(Spatial Pyramid Pooling) Object Detection 모델🔊 해당 포스팅에서 사용된 컨텐츠는 인프런의 딥러닝 컴퓨터 비전 완벽 가이드 강의 내용을 기반으로 했음을 알립니다. 설명에서 사용된 자료는 최대한 제가 직접 재구성한 자료임을 알립니다. 이번 포스팅에서..techblog-history-younghunjo1.tistory.com ​ "
[Object Detection] Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks ,https://blog.naver.com/kona419/223061355743,20230331,"논문 : https://arxiv.org/abs/1506.01497 Faster R-CNN: Towards Real-Time Object Detection with Region Proposal NetworksState-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet and Fast R-CNN have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Pr...arxiv.org ​Fast R-CNN + 영역추정네트워크 RPN​AbstractRPN(Region Proposal Network) : 이미지 전체의 convolution feature들을 detection 네트워크와 공유비용이 거의 들지 않는 region proposal을 가능하게 함.높은 퀄리티의 region proposal을 위해 end-to-end에서 학습됨.​Introduction추가적인 convolutional layer를 통해 RPN을 구축함.RPN은 큰 범위의 스케일과 측면 비율로  region proposal을 효과적으로 예측함.deep convolutional 신경망으로 region proposal을 하도록 함.본 논문에서는 anchor box를 소개하는데, 이 박스는 여러 스케일과 측변 비율에서 레퍼런스 역할을 함. a는 이미지를 피라미드 형식으로, b는 필터를 피라미드 형식으로, c는 레퍼런스 박스를 피라미드 형식으로.레퍼런스 박스의 피라미드를 regression funcion에서 사용함.pyramid of anchors이 피라미드는 여러 스케일이나 측면 비율 이미지, 필터를 나열하는 것을 피함. (? 뭔소리야?)​Faster R-CNN​두가지 모듈로 구성됨.영역을 추정하는 deep fully convolutional network추정된 영역을 사용하는 Fast R-CNN detector ① conv layer를 이용해 전체 이미지에 conv 연산을 수행함.② feature map 구함. feature map은 ③ RPN과 ④ classifier에 전달됨. ==> 즉, RPN과 분류기가 feature map을 공유해서 사용함.RPN은 영역 추정으로 통해 feature map 기반으로 객체가 있을만한 곳을 찾아줌.⑤ 영역 추정 결과를 RoI pooling을 진행함.⑥ 최종적으로 feature map과 영역추정 bounding box를 활용해 객체 탐지를 수행함.​Fast R-CNN은 영역추정을 독립된 모듈에서 수행하지만 Faster R-CNN은 영역추정과 이미지 분류를 모두 하나의 네트워크에서 수행함.​ 이 그림이 Region Proposal Network를 나타낸다.저기 슬라이딩 윈도우 안에 있는 점을 anchor라고 한다. (직역하면 닻인데 말 그대로 딱 고정되어있는 점이다.)각 슬라이딩 윈도우 마다 여러 개의 bounding box(= RPN에서는 anchor box)를  만듦.많은 anchor 박스를 바탕으로 후보 영역 추정을 훈련함.​Faster R-CNN의 중요한 특징은 Translation invariance (위치 불변성)이다.translation invariance : 객체의 위치가 변하더라도 같은 객체로 인식하는 것.이게 가능한 이유 : 슬라이딩 윈도우 방식으로 이미지 전체의 영역을 훑기 때문!​RPN을 훈련하기 위해 각 anchor box 마다 이진 분류를 진행함.anchor box안에 객체가 있다 or 없다 여부를 확인.positive label : ground-truth box와 IoU가 가장 큰 anchor & ground-truth box와 IoU가 0.7이 넘는 anchor.​Faster R-CNN = Fast R-CNN + RPNFast R-CNN과 RPN이 conv feature를 공유하지만 서로 독립적으로 훈련함.논문에서는 Alternating Training을 사용함. --> 번갈아 가면서 훈련. RPN을 먼저 훈련하고 Fast R-CNN 훈련.4step Alternating TrainingRPN훈련Fast R-CNN 훈련. (이때 conv layer 공유X)RPN 훈련을 위해 detector 네트워크(Fast R-CNN)를 사용. but, 공유된 conv layer는 고정, RPN만 fine tuning. (이제 conv layer 공유O)공유된 conv layer는 고정, Fast R-CNN만 fine tuning.​​참고 : https://bkshin.tistory.com/entry/%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0-Faster-R-CNN-%ED%86%BA%EC%95%84%EB%B3%B4%EA%B8%B0?category=1258226 논문 리뷰 - Faster R-CNN 톺아보기Faster R-CNN은 기존 Fast R-CNN에 영역 추정 네트워크(RPN)를 더해 속도와 성능을 끌어올린 모델입니다. Faster R-CNN에 와서야 비로소 모든 객체 탐지 구조를 딥러닝으로 훈련할 수 있었습니다. 본 글에서 주요 내용 위주로 Faster R-CNN 논문을 번역/정리했습니다. 글 중간에 로 부연 설명을 달아놓기도 했습니다. 틀린 내용이 있으면 피드백 부탁드립니다. 논문 제목: Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networ...bkshin.tistory.com ​ "
[Object Detection] 정량적 평가 ,https://blog.naver.com/yhjo029/222640562985,20220206,"먼저 Object Detection이란 컴퓨터 비전 영역에서 물체가 있는 영역의 Bounding box를 예측한 후 라벨을 분류하는 것을 뜻한다. 과정은 Locating 그리고 Classifying 두 가지이다.​하지만 Object Detection는 Bounding box의 정답 기준, 예측 못한 객체들 등 에매모한 기준으로 인하여 정량적으로 평가하기는 매우 애매하다. 따라서 오늘은 Ojbect Detection을 정량적 평가할 때 알아야 할Iou, precision, recall, mAP에 대해 알아보자​IoU(Intersection over union)이란예측한 1개의 Bounding box와 정답의 1개의 Bounding box가 얼마나 일치하는 지를 0 ~ 1 사이의 값으로 표현한 수치이다. 이는 오직 Bounding box만을 비교하는 값이다.  Bounding box를 맞췄다는 기준은 데이터셋에 따라 다르며.예로 Pascal VoC 데이터 셋의 경우, IoU가 0.5를 넘기면 맞춘 것으로 간주한다. ​​​Precision(정밀도)는 검색된 결과들 중 관련 있는 것으로 분류된 결과물의 비율이다.100개 중 2개의 Bounding box를 예측하고 classifying할 때 2개를 맞출경우 100%50개 중 2개의 Bounding box를 예측하고 classifying할 때 1개를 맞출경우 50%2개 중2개의 Bounding box를 예측하고 classifying할 때 0개를 맞출경우 0%로 나타낼 수 있다.정밀도는 모델이 예측한 bounding box에서 class를 맞춘 정확도이다.​​Recall(재현율)은 관련 있는 것으로 분류된 항목들 중 실제 검색된 항목들의 비율이다.100개 중 2개의 Bounding box를 예측하고 classifying할 때 2개를 맞출경우 2%100개 중 50개의 Bounding box를 예측하고 classifying할 때 25개를 맞출경우 25%100개 중 100개의 Bounding box를 예측하고 classifying할 때 100개를 맞출경우 100%로 나타낼 수 있다.재현율은 실제 정답 객체에서 모델이 bounding box를 예측하고 class를 맞춘 정확도이다.​​Precision과 Recall은 confidence의 기준에 따라 달라진다. (confidence란 모델이 예측한 bounding box의 정확도이다.)만약 내가 confidence가 높은 예측 결과만 사용한다고 했을 때 정밀도는 커지지만그만큼 여러 낮은 confidence를 무시하기 때문에 재현율이 작아진다.반면 내가 낮은 confidence로 기준을 잡았을 때 정밀도는 작아지지만더욱 많은 객체를 인식하기 때문에 재현율이 커진다.​​이에 Precision과 Recall을 고려한 종합적 평가 지표로 AP가 사용된다.AP(average Precision)은 recall별 precision의 평균이다.​​mAP(average Precision)은 전체 class들의 AP 평균이다.이것이 Object Detection의 평가 지표가 되며 이로 정량적 모델을 평가할 수 있다.​​​​​ "
yolo-object-detection (v7 블로그 요약) ,https://blog.naver.com/jhlee1326/222988551596,20230119,"https://www.v7labs.com/blog/yolo-object-detectiondeep learning 으로 객체인식하려는 시도 R-CNN (말그대로 CNN으로 detect and localize objects in images함)기준: how many times the same input image is passed through a network.​사이트 들어가서 읽어보면 알겠지만 yolo는 obj detection에서 되게 혁신적인st​ yolo구조, 출처는 위와 같음옾소 git에서 다운받아보면 다 구현되어 있을...걸 비교는 안해봤지만 코드는 잘 돌아감YOLO v7의 주요 개선 사항- ""focal loss""라는 새로운 손실 함수를 사용하는 것- 초당 155프레임의 속도로 이미지를 처리 (이전은 45)ㄴ COCO 데이터셋으로 실험- Faster R-CNN 및 Mask R-CNN과 같은 2단계 검출기보다 덜 정확하다는 점 (대신 yolo는 조금 덜 정확해도 빠르다! 이미지를 실시간으로 처리 / 반대는 반대) "
SF-1-2021/Robust Data Association for Multi-Object Detection in Maritime Environments Using ... ,https://blog.naver.com/shinyeongha/223106100925,20230519,"​논문 제목 : Robust Data Association for Multi-Oject Detection in Maritime Environments Using Camera and Radar Measurements저널 : IEEE RA-L키워드(내가 정리) 센서 : camera & radar fusion실험 : 실제 해양환경 수집 데이터비교 알고리즘 :  Nearest Neighbor 방법론 : 2D camera image -> 3D world & Radar image fusion with k-cardinality assignment method. (by finding best matching roll, pitch value)​ 1. 초록 ​ 2. 방법론 ​ 3. 실험 ​ 4. 결론  ​ 5. 적용 가능성 ​ "
ros&gazebo(4) realsense 카메라와 pytorch로 object detection하기 ,https://blog.naver.com/yoon_03_28/222807495538,20220711,"pytorch의 torchvision에서 제공하는 사전 훈련된 faterrcnn 모델로 object detection 하는 노드 작성하기~이글은 ros&gazebo(3)에서 이어짐​0. ms coco object detection label 준비하기여기서는 torchvision에서 제공하는 ms coco 2017 데이터셋으로 훈련된 faster rcnn을 사용합니다.그래서 ms coco dataset의 label에 대한 정보가 필요합니다.​이제 label에 대한 파일을 만들어 볼 것입니다.정석은 데이터를 만든다음 파이썬 데이터 구조를 그대로 저장할 수 있는 hdf5나 json 파일로 저장해서 로드해서 사용하는 것인데.. 그리고 사실상 연구하는 중에 실제 데이터셋을 다루면 레이블만 수 천개일 수 있어서 이 방법은 정말 무모하지만 혼자하는 프로젝트인데다가 귀찮아서 다음 방법처럼 레이블 데이터 파일을 생성해보았습니다.. 혹시라도 누군가 따라하지 않았으면 좋겠어요....^^....​먼저 다음의 텍스트 파일을 준비합니다: https://github.com/amikelive/coco-labels/blob/master/coco-labels-paper.txt ​레이블을 딕셔너리로 만들어 출력할 파일을 생성합니다. 메인함수는 생략했습니다....^^,,, 파일을 실행합니다. dictionary 데이터를 사용하기 위해 import 할 수 있도록 python 파일에 실행내용을 변수명을 달아서 복붙합니다. 저는 이 변수 정보만 담긴 파일을 'cococatefull.py'로 저장했습니다. ​faster rcnn을 사용하도록 node 수정하기​필요 라이브러리,프레임워크, 파일... 임폴트하고 필요한 전역변수 선언 파일 내용을 다음 사진과 같이 바꾸어줍니다. 다른 내용은 ros&gazebo(3)과 동일 ​2. 실행 결과 비디오  ​ "
"You Only Look Once: Unified, Real-Time Object Detection (Yolo v1) ",https://blog.naver.com/grow_bigger/222964375618,20221224,"Abstractyolo 모델은 기존의 객체 탐지 방식이 분류기를 탐지의 목적으로 변형시킨 방식이었는데, 이를 탈피하여 공간적으로 분리된 경계선 박스(bounding box)와 객체가 어떤 것인지에 대한 확률을 예측하는 회귀문제로 접근합니다. yolo모델의 가장 큰 특징이자, 정체성은 바로 속도입니다. Faster Rcnn은 정확성 측면에서 훌륭한 모델이었으나 5fps정도의 속도를 냈지만 yolo v1모델은 45fps의 속도를 냅니다. 신경망을 조금 더 경량화하면 155fps까지 속도를 낸다고 합니다. 굉장히 빠른 속도를 가지고 있는데, 여기에 다른 실시간 탐지 모델들에 비해 mAP가 2배정도는 된다고 합니다. 더 속도를 가속화하기 위해 신경망을 경량화했는데도 trade off관계인 mAP와 speed가 둘다 다른 모델을 넘어서는, 기본 구조에서의 혁신이 일어난 모델입니다. 게다가 yolo 자체가 일반화 능력이 뛰어나서 미술작품에 대해서도 객체탐지 수행이 가능하다고 합니다. 실제로 논문의 후반부에 첨부된 사진들에는 뭉크의 절규(The Scream)같은 미술작품에 대해 수행된 객체 탐지 결과가 첨부되어 있습니다. ​Introduction YOLO모델은 위의 사진과 같이 하나의 cnn층이 여러개의 경계선 박스와 이 경계선 박스에 대한 클래스 확률(이 경계선 박스가 포함하는 내용이 주어진 class에 속하는 확률)을 동시에 예측합니다.YOLO는 훈련 과정에서 전체 이미지를 사용하고 직접적으로 탐지 성능을 최적화합니다. 이러한 통합된 모델은 YOLO가 빠른 속도를 낼 수 있게 했습니다. 뿐만아니라 이미지를 전체적으로 보기 때문에 sliding window같이 일정 부분을 잘라내어 분석하는 방식에 비해 background error가 적습니다. Fast R-CNN에 비해서는 절반 미만이라고 합니다. 아래 이미지를 보면 정확도 자체는 Fast R-CNN에 비해 낮지만, Background error를 나타내는 YOLO의 빨간색 영역이 훨씬 적습니다.   그리고 또 중요한 이점은 일반화능력이 뛰어난 것입니다. 실제 사진으로 학습하고 테스트는 그림으로 하는 경우처럼 도매인이 달라지는 경우 일반적으로 학습데이터에 적합되는 경향이 있기 때문이 성능이 많이 감소합니다. 그런데 YOLO의 경우 DPM이나 R-CNN에 비해 훨씬 일반화능력이 좋아 실제 사진으로 학습한 모델도 미술작품에 대해 비교적 좋은 성능을 냅니다. 다만, SOTA 모델에 비해 정확도는 조금 떨어지며 작은 물체에 대해서는 탐지 성능이 상대적으로 떨어집니다.  YOLO v3모델로 객체 탐지를 수행한 결과(위사진은 논문에 게재된 사진이 아닙니다)​Unified Detection YOLO모델은 입력 이미지를 S x S 의 격자로 나눕니다. 이 격자에 객체의 중심이 들어온다면 해당 격자가 객체를 탐지하는데에 할당됩니다. 각각의 격자는 B개의 경계 상자와 각각의 경계선 상자에 대한 confidence score를 예측합니다. 이 confidence는 모델의 객체 포함 정도에 대한 확신도와 실제 객체 포함 정도를 반영합니다. 실제 포함 정도는 IOU로, 모델의 확신도는 Pr(Object)로 나타낼 수 있을 것입니다. 따라서 confidence는 Pr(Object) * IOU로 표현할 수 있습니다. 이렇게 confidence를 추론하는 방식이 있어, YOLO에서는 각각의 경계 상자가 5개의 추론을 수행하게 됩니다. R-CNN은 경계 상자의 회귀 추론시 4개의 인자를 이용했습니다. 논문에서는 x,y,w,h와 confidence로 경계 상자가 이루어져있다고 표현합니다. x,y는 Faster R-CNN과 같이 상자의 중심이고, w,h는 너비와 높이입니다. ​각각의 격자는 C개의 클래스 확률을 추론합니다. Pr(Classi | Object)로 나타낼 수 있습니다. 이 확률은 격자에 객체가 포함된 경우의 클래스 확률이므로 조건부 확률이며,각 그리드 셀은 하나의 클래스 확률 집합만을 예측합니다.즉, 하나의 격자는 격자내부에 중심이 있는 하나의 객체에 대해 이 객체가 20개면 20개의 클래스 집합에서 각각의 클래스에 대한 확률 20개가 추론되는 것입니다. ​경계 상자 예측과 클래스 확률 예측은 S x S x (B*5+C)의 tensor로 encoding 됩니다. 각각의 격자에 대해 추론이 되므로 격자 개수 SxS만큼 각 격자 셀마다 이루어지는 추론의 양이 생깁니다. 각 셀마다는 (x,y,w,h,conficence)개수 x 경계 상자 개수 + 하나의 경계 상자를 고른 후 구한 클래스 확률 분포 세트만큼의 추론이 수행됩니다. 수식을 보더라도 경계 상자는 여러개를 고르지만 클래스 확률 구할때는 그 중 하나를 고른다는 것을 확인할 수 있습니다. ​Network Design  YOLO의 architecture은 24개의 CNN층에 2개의 FCN(완전연결층)이 결합된 구조입니다. GoogLeNet의 구조로부터 영향을 받은 구조이며, GoogLeNet의 inception모듈을 1x1 reduction layer와 3x3 CNN layer로 대체한 구조입니다. 초반에 155fps속도를 낸다고 했던 fast yolo모델은 24개의 cnn층을 9개로 줄인 모델입니다. ​TrainingYOLO모델은 위 architecture의 앞의 20개의 CNN층을 ImageNet 1000-class competition dataset에서 사전학습을 하였습니다. 사전학습시에는 이 20개 CNN층뒤에 평균 풀링과 완전연결층을 붙여 학습시켰다고 합니다. 여기에 위 architececture의 뒤쪽 4개의 합성곱 층과 완전연결층을 더하였고, 입력해상도를 224x224에서 448x448로 높였습니다.​YOLO모델은 모델의 출력을 sum-squared error로 최적화하는데 이는 localization error와 classification error를 동일하게 취급합니다. 논문에서는 동일하게 취급하는게 적절치 않을 수 있는데, 아마 classificaion자체가 localization에 영향을 받아서 일수도 있고 또는 object detection자체가 localization error의 비중이 큰 것 때문일 수도 있겠습니다. 또 대부분의 경우 격자가 객체를 포함하지 않는 경우가 많은데 이런 경우 confidence가 0이 되버리는 경우가 계속 생기고, 결과적으로 모델의 불안정성이나 조기 수렴에 영향을 줄 수 있다고 합니다.따라서 YOLO모델은 경계 상자 좌표 예측의 loss를 증가시키고, 객체를 포함하지 않는 상자의 confidence의 loss를 줄입니다. ​추가적으로, 큰 상자와 작은 상제에서의 오차를 sum-squared error는 동일하게 취급하는 문제가 있는데, 이는 아래 limitation에서 다시 다룹니다. ​YOLO학습시 bounding박스는 하나만 선택합니다. ground truth와의 IOU가 가장 높은 것을 선택하는 방식을 통해 bounding box predictors의 예측성능을 높입니다. ​전체 loss function은 아래와 같습니다. 첫 줄과 둘째줄은 bounding box에 대한 error을 구하고 세번째와 네번째줄은 confidence 예측 오차를 구합니다. 다섯번째줄은 클래스 예측 오차입니다. 네번째 줄은 객체가 격자에 없는 경우에 대한 confidence오차인데, 이를 억제할 필요가 있기때문에 람다를 0.5로 설정합니다.  Inference추론시에도 훈련과정과 동일하게 하나의 신경망 evalutaion만 필요합니다. 그리고 격자를 이용한 설계는 경계 상자 예측에서 공간적 다양성을 강제한다고 합니다. 아마 경계 상자들이 적절히 퍼지지 않고 특정한 곳에 뭉쳐져 있는 효과가 조금 적다는 의미인 것 같습니다. 다만 물체가 크거나 격자 경계에 놓인 경우 다수의 격자들에 의해 localized가 발생할 수 있기 때문에 NMS(non-maximal suppression)이 어느정도의 효과를 보여줍니다. 다른 R-CNN이나 DPM에 비해서는 필수적이지는 않지만 mAP를 23%정도 더해준다고 합니다.​ Limitations of YOLOyolo는 각각의 격자가 두개의 상자만 예측하고(논문에서는 B=2로 설정하였음) 하나의 클래스만을 갖을 수 있어 경계선 상자의 공간적인 제약이 큽니다. 이런 경우 근접한 물체에 대해서는 적은 수의 물체만 탐지가 가능합니다. 위에 YOLO v3로 추론한 사진을 보면 배경 뒤쪽의 사람들이 모여있는 곳에 대해서 중간에 있는 한명만 탐지를 해냈습니다. 논문에서도 그룹지어 나타나는 새떼와 같은 작은 물체에 대해서는 탐지가 어렵다고 밝힙니다. ​경계 상자를 예측하는 낭력을 데이터로부터 학습하기 때문에 새롭거나 특이한 종횡비를 가지거나 구성을 갖는 경우는 추론이 힘듭니다. 다수의 downsampling때문에 데이터 소실이 많아 coarse한 feature를 사용하다보니 그런것 같습니다. ​ 세번째 한계는 loss function에 대한 부분인데, 작은 상자에서 발생하는 error(오차)와 큰 상자에서 발생하는 error가 분명히 IOU에 다른 영향을 주는데 loss function은 이를 구별하지 않는다는 점입니다. 위의 이미지를 다시 보면 localization 오차가 비중이 큽니다. 그런데 작은 상자에서는 작은 error도 IOU에 상대적으로 큰 영향을 끼침에도 불구하고 loss function자체는 이를 구별하지 않는 것은 YOLO모델의 성능에 큰 제약이 될 수 있습니다.  "
[Object detection 시리즈] One stage detector - YOLOv1 ,https://blog.naver.com/koreadeep/222669635751,20220311,"이번 포스팅에서는 앞에서 다룬 RCNN 시리즈로 대표되는two stage detector 와는 다른 접근 방법인 One stage detector ,그중에서도 YOLO(You only look once)  라는 object detection 모델에 대해 살펴보도록 하겠습니다.  이름에서 알 수 있다시피, YOLO는 'you only look once'의 약자입니다.의미를 이해해 보면 '한 번만 본다'라는 정도로 해석될 수 있겠죠?즉, 입력 이미지를 end - to - end로 한 번에 연산을 수행해서 onject detection 문제를 풀어내는 것을 말합니다.구조적인 차이로는 region proposal 과정이 별도로 존재하지 않는다는 것입니다. ​ Overview아래 그림을 보겠습니다.제일 좌측을 보면 (H, W, 3) 크기의 입력 이미지가 주어졌습니다.그리고 그다음 스텝 이서는 이 그림을 7x7 (S= 7 )의 grid (격자)로 구분하여 표현했습니다.그리고 각 grid cell은 각각 세 개( B=3 )의 bound box를 갖게 됩니다,따라서 최종적으로 산출되는 output은 7x7x(5xB+C) 자료 구조 형태를 갖는 feature map 이 되게 됩니다.입력 이미지가 최종적으로 특정 크기의 feature map으로 representation 되는 것이라고 생각할 수 있고,yolo라는 모델은 입력이미지(로우데이터)를 이러한 자료구조로 변환 시켜주는 일종의 함수라고 생각할 수 있겠죠.​ ​아래 표를 보겠습니다.YOLO 모델은 region proposal 과정이 생략된 만큼inference time이 굉장히 짧다는 것을 볼 수 있습니다.RCNN 시리즈 중 가장 빠른 모델인 Faster RCNN의 FPS 가 7인데 반해,YOLO의 FPS 수치는 45로 7개 가까지 압도한다는 것을 알 수 있습니다.한편 accuracy 성능 척도인 mAP 관점에서는 Fast RCNN에도 밀리는 모습을 보여주고 있습니다. 자 그럼 이제 YOLO 모델에 대해 본격적으로 알아보도록 하겠습니다.​ Model's Structure먼저 YOLO 모델의 구조부터 한번 살펴보도록 하겠습니다.YOLO 모델은 base model은 googleNet(Inception)입니다.아래 그림에서 보는 것처럼 Inception 모델에서 Inception 모듈에 해당하는 부분을1x1 convolution으로 대체하였으며, 24개 층의 convolution layer 및 2개의 fully connected layer가 이어지는 형태로디자인되었습니다. 최종적인 output shape을 보면 7x7x30 인 것을 볼 수 있습니다.7x7 은 앞서 보았듯이, 인풋 이미지를 7x7개의 grd로 나눈 것을 의미하고30이라는 숫자는 bound box(B=2)가 2, 클래스 개수가 20(C=20)으로 설정을 해주었기 때문에B*5+c 계산식에 따라 30이 되는 것입니다. ​좀 더 자세하게 알아보면24개의 convolution layer가 있다고 설명을 했는데,이 중 20개 층은 ImageNet 데이터에 대해 classification 학습을 진행해서feature extraction으로서의 기능을 하기 위한 파트로 학습이 됩니다.그리고 이 pre-trained 된 weigt를 기반으로 뒤에 붙는 4개의 convolution layer와2개의 fully connected layer가 object detection을 위한 fine tunning 학습을 하게 되는 것이죠.즉 아래 그림에서 파란색으로 표시한 부분이 object detection 을 학습하는 파트라고 할 수 있겠습니다. ​ Feature map's data structure(자료 구조)이제 각 grid cell의 자료구조 (벡터)에 대해 알아보겠습니다.B=2, C= 20이므로 계산식(B*5+C)에 의해 벡터의 크기는 30이 됩니다.아래 그림에서 빨간색으로 표시된 grid cell의 벡터를 우측에 녹색으로 표시했습니다.처음에서 5번째에 해당하는 숫자는 해당 grid 셀에서 prediction 한 첫 번째 bound box의 속성을 의미합니다.즉, x, y, w, h, c (x좌표; y좌표;, width 길이, height 길이, confidense score)이죠.그리고 이어서 오는 다섯 개 숫자 또한 해당 grid 셀에서 prediction 한두 번째 bound box의 속성입니다.역시, x, y, w, h, c 값을 의미하죠.그리고 이어서 오는 20개의 숫자가 바로 class probability입니다.자 여기서 B*5+C의 계산식의 의미를 이해할 수 있습니다.상수 5는 bound box의 속성을 의미하는 것으로 불변하는 값입니다.​그리고 bound box의 수는 예시에서는 2로 주어졌지만, 상황에 따라 각 grid cell에서 예측해야 할 bound box가 달라진다면 그 크기만큼 벡터의 크기도 늘어나게 되는 것입니다또한, C 값 또한 예시로 주어진 데이터인 VOC 데이터의 클래스가 20개이기 때문에크기 20인 벡터가 더해지는 것입니다.즉, 클래스의 숫자가 80인 coco data라면 뒤에 80개의 숫자가 이어져 붙게 되겠죠.예를 들어, 각 grid 셀 이 예측해야 할 bound box의 수를 3, 학습 데이터는 coco 데이터를 이용한다고 한다면벡터의 크기는 B(=3) * 5 + 80 = 95가 되는 것입니다.   ​ Loss function (손실/비용 함수)다음으로 yolo 모델의 loss function에 대해 알아보겠습니다.앞서 얘기했듯이, yolo model은 별도의 regiion proposal 과정 없이,bound box regression과 object classification을 regression 문제로 간주하고한 번에 수행되게끔 설계되어 있습니다.따라서 loss fucntion 또한 bound box 좌표와 class probability가 함께 엮여서 표현됩니다.그럼 한번 살펴보도록 하겠습니다.​첫 번째 term은 bound box의 중심 좌표인 x, y 값의 오차를 반영하기 위한 term입니다.오차 계산식은 Mean square error 식을 사용했습니다.그리고 두 번째 term은 width와 height의 값에 대한 오차를 반여 하기 위한 term입니다.마찬가지로 Mean square error를 사용했는데, x, y와 차이점은width와 height에 제곱 근이 씌워져 있다는 것입니다.그 이유는 bound box의 scale이 커지면 오차가 조금 나더라도 IOU 관점에서는 크게 차이가 없지만이것을 동일한 비율의 오차로 연산해서 반영되는 것을 부적절하기 때문에scale이 커질수록 오차의 반영 비율을 낮추기 위함이라고 설명하고 있습니다.두 term의 앞에 lamda (coord) 상수가 곱해진 것은 bound box의 localiztion error의 반영 비율을 어느 정도로 할 것인지 정해주는 일종의 hyper parameter인데요.논문에서는 5로 정해주어, 뒤에서 나오는 lamda(noobj)에 비해 10배에 해당하는 비율로 반영되게 했습니다.(꽤 크게 반영했다고 할 수 있겠습니다.)​세 번째 term은 confidense score에 대한 prediction 값과 실제 값(=1)의 오차를 mean square error 식으로 표현한 것이고,bound box에 object 가 없는 경우에는 네 번째 term이 계산되게 됩니다.한편, 실제로 object deteciton 문제를 풀 때, 대부분은 background에 해당하기 때문에noobj에 해당하는 오차만 많이 계산되어 반영되게 되는 문제가 있습니다.그렇기 때문에 labmda(noobj) 상수를 0.5로 해줌으로써background에서 나오는 오차가 전체 loss에 작게 반영되게 디자인하였습니다.​마지막 term은 벡터로 얻어지는 각 class의 확률 값(probabiliity)과 one hot vector로 표현된실제 확률 값의 오차를 MSE 식으로 표현하여 더해주었습니다.  ​ Peformance (성능)yolo 모델의 성능은 다음과 같습니다.RCNN 모델들에 비해 FPS (frame per second)는 비약적으로 개선된 것을 확인할 수 있습니다.하지만, mAP는 RCNN 시리즈는 70대인데 반해, YOLO는 60대에 머물고 있는 것을 확인할 수 있습니다. Limitation (한계)이러한 yolo 모델은 inference time에서 비약적인 개선을 이루었지만해당 모델을 실제로 상용화하기에는 몇 가지 한계가 있었습니다.첫 번째로는 이미지를 7x7의 grid로 함축해서 표현하다 보니,grid 안에 존재하는 작은 object의 경우에는 bound box가 잘 찾아내지 못하는 문제가 있었습니다.두 번째는 아직 부족한 accuracy였습니다.localication error와 class probability가 기대 수준까지는 못 미치는 문제가 있었습니다. ​다음 포스팅에서는 이 모델의 가장 큰 장점인 inference time(속도)를 유지하면서도정확도를 좀 더 향상시키기 위한 방법들이 적용된 모델들에 대해 살펴보도록 하겠습니다.​감사합니다 ^^​  한국딥러닝(주)는 인공지능, 머신러닝, 딥러닝 솔루션을 개발하여 세상이 해결할 수 없는 문제를 해결하고 제안하고 있습니다. 끊임없이 발생되는 새로운 빅데이터와 응용 기술의 홍수 속에서 최신 인공지능 기술 동향을 예측하고 빠르게 반영하는 한국딥러닝을 만나보세요.​ ​ "
"[SPI] Foreign Object Detection (이물 검사, FOD) ",https://blog.naver.com/marketingmirtec/222875440277,20220915,"이물 검사 혹은 Foreign Object Detection (FOD)라고 불리는 기능은 Solder Paste Inspection(SPI) 장비에서 PCB를 검사함에 있어서 이전과 확연히 다른 검사 결과를 가져다줄 것입니다. 미르기술 MIRTEC미르기술 이물 검사 결과​▮ 미르기술 MIRTEC - Foreign Object Detection (이물검사, FOD)​미르기술 MIRTEC은 고객들이 처한 다변화된 작업 환경에서 발생하는 문제를 해결하기 위해서는 신속한 대응이 필수라고 생각합니다.​따라서 자사는 신속한 대응의 하나로 미르기술 MIRTEC의 이물 검사(FOD)를 소개합니다. 미르기술 MIRTEC의 이물 검사(FOD) 기능은 다양한 종류의 이물을 감지해 검사 오류를 최소한으로 줄임으로써 보다 정확하고 정교한 검사 결과를 보장하고 있습니다. ​상기 사진에서 보이는 바와 같이 이물 검사(FOD) 기능은 검사 환경에서 빈번히 발생하는 플라스틱 조각, 금속 조각, 슬러지(Sludge), 머리카락, 먼지, 오염(얼룩) 등을 포함하는 다양한 크기 및 형태의 이물을 찾아낼 수 있습니다.​특히 SPI 장비의 렌즈별 최소 검출 가능 사이즈가 다르기 때문에 이물 검사(FOD) 기능에 관심이 있으신 고객분들께서는 자세한 정보를 위해 미르기술 MIRTEC으로 연락해 주시기 바랍니다. ​※ 단, 이물 검사(FOD)를 실시할 경우 이물 개수(프레임당) 별 추가 검사 시간이 발생하는 점을 주의해 주시기 바랍니다.  미르기술은 미르기술을 이용하시는 모든 고객들에게 더욱 정확하고 정교한 검사 결과를 제공하기 위해 항상 최선의 노력을 다하고 있습니다. 미르기술의 다양한 최신 기술과 제품에 대해 궁금하신 점이 있으시면 미르기술로 연락해 주시기 바랍니다.​​미르기술 정보 및 장비 문의📲 1544-1062 💻 marketing@mirtec.com  💻 미르기술 홈페이지​ 미르기술Technology 실력이 뒷받침되는 정성의 철학으로 끊임없이 기술을 개발하고 최상의 서비스를 제공합니다. Smart Factory Solutions 스마트팩토리 솔루션 AOI 자동 광학 검사기 SPI 납 도포 검사기 SEMI / LED 반도체 / LED 검사기 Smart Factory Solutions 미르기술의 소프트웨어 솔루션 Intellisys®는 검사 데이터를 장기간 누적 취합하여 빅데이터를 구성하고, 통계적 방법론을 통해 해석하여 불량의 근본 원인을 추적합니다. 또한 원격지에서 장비를 관리ㆍ제어하여 문제를 해결함으로써 공...mirtec.com MirtecUSA - Distributors of Automated Optical Inspection sytems including Desktop AOI, Inline AOI Equipment.Welcome to MIRTEC With over 17,000 systems installed throughout the world and having received a total of 43 Industry Awards thus far for its products and services, MIRTEC has earned a solid reputation as one of the most progressive and dynamic suppliers of Automated Optical Inspection equipment to t...mirtecusa.com MIRTECTechnology Wir entwickeln ständig Technologien und bieten den besten Service auf der Basis unserer Philosophie des Fleißes, der durch unsere Fähigkeit unterstützt wird. Smart Factory Solutions Intelligente Fabriklösungen AOI Automatische optische Inspektionsgeräte SPI Lötzinnauftrags-Prüfungsgeräte ...mirtec.com 日本ミルテック株式会社｜プリント基板実装と半導体の外観検査装置専業メーカー｜東京都中央区--> 2022.3.28 2022年6月15日～17日に、東京ビックサイトで開催される 電子機器トータルソリューション展2022 に出展します。 詳細は追って掲載させていただきます。 2020.10.29 2021年1月20日～22日に、東京ビックサイトで開催される エレクトロテストジャパン に出展します。 詳細は追って掲載させていただきます。 2019.12.3 2019年12月より新大阪駅から徒歩8分の第8新大阪ビルに大阪営業所を開所しました。 2019.12.2 2020年1月15日～17日に、東京ビックサイトで開催される エレクトロテストジャパン に弊社装置を出展します。 出展ブース...mirtec-j.com ​ "
[복습장] CNN_06 Object_Detection_1 ,https://blog.naver.com/dlehdgnsa/223046863204,20230316,"#AI #CNN #YOLO​Obect Detect , Image Detection 이란 컴퓨터 비전 기술 중 하나로, 디지털 이미지에서 특정 객체 또는 패턴을 인식하고 탐지하는 기술입니다. 이번에는 사전 학습된 모델 YOLO v3를 이용하여, 특정 이미지들에서 객체를 탐지해 보겠습니다.  https://github.com/ultralytics/yolov3#quick-start-examples GitHub - ultralytics/yolov3: YOLOv3 in PyTorch > ONNX > CoreML > TFLiteYOLOv3 in PyTorch > ONNX > CoreML > TFLite. Contribute to ultralytics/yolov3 development by creating an account on GitHub.github.com YOLO v3 모델은 위의 주소에서 가져왔습니다. 모델 설치는 위의 순서대로 진행하면 실행됩니다. !git clone https://github.com/ultralytics/yolov3.git !cd yolov3: pip install -r requirements.txt 이 YOLO v3 모델은 총 80가지의 클래스로 객체를 분류해 줍니다. 객체 중 일부는 다음과 같습니다. from google.colab import drivedrive.mount('/content/drive') 이미지는 구글에서 관광지 거리뷰 이미지를 가져왔고, 구글 드라이브 폴더에 저장해 주었습니다.https://github.com/ultralytics/yolov3/releases Releases · ultralytics/yolov3YOLOv3 in PyTorch > ONNX > CoreML > TFLite. Contribute to ultralytics/yolov3 development by creating an account on GitHub.github.com !wget -O /content/yolov3/pretrained/yolov3-tiny.pt https://github.com/ultralytics/yolov3/releases/download/v9.6.0/yolov3-tiny.pt 위 링크에서 사전 학습된 weights를 불러옵니다. !cd yolov3; python detect.py \    --weights '/content/yolov3/pretrained/yolov3-tiny.pt' \    --source '/content/drive/MyDrive/my_data/object/' \    --project '/content/yolov3/detected' \    --name 'images' \    --img 640 \    --conf-thres 0.5 \    --iou-thres 0.4 \    --line-thickness 2 \    --exist-ok \    --device CPU 이제, 모델에 이미지를 넣고 실행시켜줍니다. 모델을 실행시키면, 위와 같이 각 이미지에서 감지한 객체의 수와 종류를 요약해 보여줍니다.  위가 원본 이미지이고, 아래가 모델을 돌린 이미지 입니다.​ "
[수입판매] BALLUFF Capacitive sensors for object detection 발루프 물체 감지용 정전용량 센서 ,https://blog.naver.com/met8257n/222966256000,20221226,"[수입판매] BALLUFF Capacitive sensors for object detection 발루프 물체 감지용 정전용량 센서 정전식 센서는 먼 거리에 있는 물체를 안정적으로 감지합니다. 유리, 목재, 금속, 플라스틱, 복합 재료, 액체 및 벌크 재료로 만들어진 고체의 비접촉식 감지에 적합합니다. 이것은 플라스틱 벽 및 기타 비금속 포장을 통해 달성할 수 있습니다.정전식 센서는 색상, 먼지, 반사성, 광택 또는 투명 표면의 영향을 받지 않습니다.발루프의 정전용량형 센서는 다양한 플러그 및 케이블 옵션과 함께 다양한 구성으로 제공됩니다. 스위칭 또는 아날로그 출력 또는 IO-Link 인터페이스 중에서 선택하십시오. 특히 경제적인 버전도 제공됩니다.​특징​장거리에서 유전율이 낮은 매체를 감지하기 위한 플라스틱 및 스테인리스 스틸 버전의 정전 용량 센서6.5mm의 콤팩트한 구성과 최대 25mm의 스위칭 거리를 가진 디스크 구성과 같은 평면 구성전위차계 또는 케이블을 통해 매립형 장착 및 조정 가능IO-Link 사용 가능 제품 리스트 BCS003LBCS00NCBCS00MTBCS D50OO03-YPC25C-S49GBCS M30BBI2-POC15D-S04KBCS M30B4I2-POC15D-S04KBCS00PYBCS00NMBCS00NEBCS M12BBI1-NSC40D-EP02BCS M30BBI1-PSC15D-EP02BCS M30BBI2-NSC15D-S04KBCS00NPBCS00NNBCS00PZBCS M30BBI1-NSC15D-EP02BCS M30BBI1-POC15D-EP02BCS M12BBI1-NOC40D-EP02BCS001LBCS00NZBCS00UABCS G06T4E1-PSM15C-EP02BCS M18BBN1-PSC80D-EP02BCS D22T402-PSM60C-EP05BCS001MBCS00P0BCS00ZHBCS G06T4E1-POM15C-EP02BCS M12B4G2-PSC40D-S04KBCS Q40BBAA-GPC20C-EP03BCS001NBCS00P1BCS012FBCS G06T4E1-NSM15C-EP02BCS M12B4G2-POC40D-S04KBCS R08RRE-NOM80C-EP02BCS001RBCS00P2BCS017KBCS G06T4D2-PSM15C-S49GBCS M12B4G2-NSC40D-S04KBCS M12K4D2-GSM50C-S04GBCS001TBCS00P9BCS017HBCS G06T4D2-POM15C-S49GBCS M12B4I1-NSC40D-EP02BCS M12K4G1-GSM50C-EP02BCS0026BCS00PJBCS017JBCS M08T4E1-PSM15C-EP02BCS M12BBG2-PSC40D-S04KBCS M12K4G1-GOM50C-EP02BCS002ABCS00PKBCS018NBCS M08T4E2-PSM15C-S49GBCS M12BBG2-POC40D-S04KBCS D50OO02-YPC25C-EP03-GS49BCS002CBCS00PUBCS0028BCS M08T4E2-POM15C-S49GBCS M12BBI1-PSC40D-EP02BCS M08T4E1-NSM15C-EP02BCS002TBCS00R4BCS003FBCS G10T4H-PSM40C-EP02BCS M12B4I1-PSC40D-EP02BCS D30T401-NOC15C-EP02BCS003ABCS00TLBCS003HBCS D30T401-PSC15C-EP02BCS M12BBI1-PSC40D-EP00,35-S04GBCS D22T403-PSM60C-EP02BCS003KBCS00TRBCS00M1BCS D50OO02-YPC25C-EV02BCS Q40BBAA-GPC20C-EP02BCS M18BBN1-POC80D-EP02BCS004KBCS00U6BCS00M3BCS M30BBM3-PPC20C-EP02BCS Q40BBAA-PSC20C-EP00,3-GS49BCS M18BBN1-NOC80D-EP02BCS004MBCS00UJBCS00MHBCS M30BBM2-PPM20C-S04GBCS G34OOI2-PSC15D-S04KBCS M18BBI3-POC80D-S04KBCS004TBCS00WWBCS00MJBCS M30B4M2-PPM20C-S04GBCS M12B4I1-PSC40D-EP00,3-GS04BCS M18BBI3-NSC80D-S04KBCS00AUBCS012ABCS00MRBCS M12TTG1-PSM40C-ET02BCS R08RRE-PSM80C-EP02BCS M30B4I2-PSC15D-S04KBCS00HKBCS012CBCS00N2BCS D22T402-PSM60C-EP02BCS R08RRE-POM80C-EP02BCS M30B4I1-PSC15D-EP02BCS00LKBCS012NBCS00NABCS M18B4N1-PSC80D-EP02BCS R08RRE-PIM80C-EP00,3-GS04BCS M30BBI2-PSC15D-S04KBCS00LRBCS012TBCS0027BCS M18B4N1-POC80D-EP02BCS R08RRE-PSM80C-EP00,3-GS75BCS M08T4E1-POM15C-EP02BCS00M2BCS012UBCS00MFBCS M18BBN1-NSC80D-EP02BCS R08RRE-POM80C-EP00,3-GS75BCS M18B4I3-PSC80D-S04KBCS00M4BCS017LBCS M18B4I3-POC80D-S04KBCS M12K4D2-GOM50C-S04GBCS00M8BCS017MBCS M18BBI3-PSC80D-S04KBCS M12K4D2-PIM50C-S04G ​ "
End-to-End Object Detection with Transformers (DETR) Review ,https://blog.naver.com/taeeon_study/222795849713,20220701,"ViT처럼 완전 CNN free 모델은 아니지만 이 네트워크부터 detection 분야에 트랜스포머 기반 접근이 확실히 쓰이게 됐다.  트랜스포머 기반 접근은 아직 능숙하게 이해하지 못했기에 하나하나씩 논문 리뷰를 확실히 해보면서 넘어가야겠다. 이번에 리뷰할 논문은 Facebook의 AI 팀이 낸 논문이다. ​DETR Abstract우선 object detection task를 direct set prediection 문제라고 접근하고 있다. 그렇기에 NMS(Non Maximum Suppression) 같은 (YOLO를 활용한 적이 있다면 한 번쯤은 접해봤을 것이다.) hand - designed 요소를 효과적으로 제거할 수 있다고 소개한다. 이 네트워크의 main 요소는 set-based global loss와 transformer-encoder-decoder 아키텍처이다. transformer는 연속적인 데이터의 문맥 추출에 강점을 가지고 있다.아래는 github 링크인데 여기서 training code와 pretrained model을 확인할 수 있다.https://github.com/facebookresearch/detr GitHub - facebookresearch/detr: End-to-End Object Detection with TransformersEnd-to-End Object Detection with Transformers. Contribute to facebookresearch/detr development by creating an account on GitHub.github.com 2. Introductionobject detection의 목표는 객체의 category label과 bounding box들을 예측하는 것이다. Modern detector들은 set prediction task를 간접적인 방식으로 주게 된다. (anchor 혹은 sliding window 같은 방식으로.) 이렇게 되면 휴리스틱 하고 전처리에 영향을 많이 받게 된다. 그래서 이 네트워크는 직접적인 방식으로 접근하게 된다. 아래 그림이 이에 대한 보충 그림이다. 위 그림을 보면 CNN backbone으로부터 feature를 뽑아내고 transformer encoder-decoder를 거친 후에 set of box prediction을 직접 설정해준다. 논문의 Fig 1 설명에서도 나와있듯이 directly predict 한다고 나와있다. 만약 많은 object들이 있다면 box set을 많이 설정해줄 필요는 있을 것이다.transfomer 기반 encoder-decoder 아키텍처는 pairwise interaction을 잘 뽑아낼 수 있고 중복된 예측과 같은 set prediction의 구체적인 제약에 적합하다고 설명하고 있다.또한, DETR은 모든 객체를 한번 예측하고 예측된 것과 ground-truth  사이의 양자 매칭을 수행하는 set loss 함수를 가진 end-to-end로 훈련된다고 설명하고 있다. parallel decoding이 가능하기 때문에 병렬적으로 prediction 추출이 가능하다.다만 단점이 훈련 시간이 오래 걸리고 작은 객체에 대해 정확도가 좀 떨어진다. ​3. Related Work- Set predictiondetection 분야에는 directly prediction 하는 네트워크가 없었다. 그래서 near-duplicate에 어려움이 있었고 위에서 언급한 NMS 같은 것을 사용했다. 그러나 direct prediction은 postprocessing-free이므로 강점이 있다. 고정 사이즈 set prediction은 fully connected network가 충분하지만 너무 연산이 많다. 그래서 일반적인 접근은 auto-regressive sequece 모델이다. 모든 경우에 loss function이 불변이여야 하며 헝가리안 알고리즘에 기반하여 loss가 설계됐다. (최적 매칭 알고리즘이라고 보면 편하다.)​- Transformers and Parallel Decoding자세한 Transformer 네트워크는 'attention is all you need'라는 논문을 읽어보면 된다. attention 메커니즘은 전체 input sequence로부터 정보를 통합하는 layer로 작동한다. sequence의 각 요소를 스캔하고 전체 sequence로부터 정보를 통합하므로써 업데이트한다. attention based 모델은 global computation과 perfect memory에 이점이 있다고 한다. (transformer는 global context 추출에 상당한 강점이 있다고 많이 설명된다.)또한 parallel decoding이 가능하기 때문에 computational cost와 set prediction에 요구되는 global 연산량의 능력 사이 trade-off에 적합하다고 설명한다.​- Object detection1- Stage, 2 - Stage approach 소개는 앞선 CNN 기반 object detector 소개에서 다루었기에 생략한다. 여기서 마찬가지로 본래 유명한 네트워크는  preprocessing에 많은 영향을 받았는데 이 네트워크는 그런 게 필요없다는 장점을 언급하고 있다. NMS post-processing이나 Learnable NMS method 언급하면서 굳이 이 네트워크는 필요없다라는 말이 핵심이다. (앞에서 설명했던 것 반복이다.) ​(사실 내가 논문의  Related Work를 볼 때는 모르는 분야가 나왔을 때 이것을 보고 따라가면서 공부하기 위함으로 쓰고 있다. 뭐 여긴 대부분 아는 내용이다만..)​4. The DETR model드디어 모델 설명이다. loss부터 설명한다. detection에서 direct set prediction을 위해서는 2가지 요소가 필수적이라고 설명한다. 그 두 가지는 아래와 같다.1) a set prediction loss that forces unique matching between predicted and ground truth boxes2) an architecture that predicts a set of objects and model their relation대충 unique matching을 위한 loss와 object set와 model 관계를 파악하는 아키텍처가 필요하다는 의미이다.​- Object detection set prediction lossDETR은 N개의 fixed-size set을 inference하게 된다. N은 이미지의 객체보다 더 커야한다. 주된 어려움이 예측된 객체들과 ground-truth를 scoring 하는 것이었다고 설명한다. 아래 loss은 최적 bipartite matching을 만들어낸다고 설명한다.  L_match는 ground truth y_i와 prediction y_hat 사이의 pair-wise matching cost이다. 이것은 헝가리안 알고리즘으로 계산될 수 있다. matching cost는 class prediction과 similarity를 계산하게 된다.  여기서 c_i는 target class label이고 b_i는 ground truth box center 좌표와 높이 너비 정의 벡터이다. (일반적인 바운딩박스 생각하면 된다.) 시그마 (i)는 인덱스라고 생각하면 된다. 이에 대한 class 예측의 확률은 p^으로 정의되고 박스 예측은 b^이라고 정의된다. (위 그림에서는 내가 ~라고 표현했는데 수식에서 ^가 안쳐진다. ㅜㅜ)또한 one-to-one 매칭을 하게 된다.​두 번째 step은 loss function을 계산하는 것인데 이전 step에서 매치된 모든 pair들에게 헝가리안 손실을 계산하는 것이다. loss similarity를 negative log-likelihood와 box loss의 linear combination으로 계산된다. 또한 class imbalance를 위해 down-weight한다. 객체와 None 사이 매칭 cost는 prediction에 의존하지 않고 그 경우 cost는 constant이다. ​- Bounding box loss박스를 직접적으로 예측하기 위해 l1 loss와 IOU Loss의 linear combination을 사용한다. 즉 아래와 같다. L_iou는 scale-invariant 하다. 또한 람다들은 하이퍼파라미터이다. - DETR architecture아키텍처는 매우 간단하고 아래 그림과 같다. architectureCNN backbone으로 pre-training 하고 transformer encoder-decoder를 거쳐 prediction을 direct로 하는 것을 볼 수 있다.​Backbone - 3xHxW에서 CxHxW의 feature로 바꾸게 되는데 C는 2048, H,W는 H/32, W/32로 설정하였다.Transformer encoder - 1x1 conv layer로 dxHW의 feature map으로 바꿔준다. (tranformer input을 위해)Transformer decoder - size d의 N 임베딩을 받고 parallel decoding을 수행한다. N개의 input 임베딩은 다른 결과를 생성해야 하기에 달라야하고 이 임베딩들은 positional encoding을 배우게 된다. Prediction feed-forward networks (FFNS) - FFN은 정규화된 센터 좌표를 예측하고 box의 height, width를 예측한다. 앞에서 설명한 것을 간략 설명하고 있다. Auxiliary decoding losses - auxiliary loss들이 훈련 중에 디코더에서 도움이 된다고 설명하고 있다. 그래서 prediction FFN과 헝가리안 loss를 각 디코더 layer에 추가한다. ​본래 논문: https://arxiv.org/abs/2005.12872 "
Object detection(8) _One-stage 2D object detectors: FPN[CVPR'17] ,https://blog.naver.com/summa911/223003713389,20230203,"​다양한 사이즈의 object를 정확하게 찾기 위해서는, high-level feature와 high-resolution이 필요하다. CNN 과정을 거치면서 해상도는 낮아지지만, feature level은 올라간다. 그래서, 언제 BB를 propose하느냐에 따라서 좋은 부분이 있으면 포기하는 부분도 있다.  YoLo는 마지막에 BB를 예측하기 때문에, 저해상도 상태에서 추측을 하게된다. 때문에 작은물체나 무리지어있는 물체는 정확하게 예측하기 어렵다. ​ ​반면, SSD는 각 layer마다 Box를 추론하기 때문에 여러가지 부작용을 완화 할 수 있다.하지만 resolution과 feature level이 서로 상반된 수준을 가진 상태로 BB를 추출해야하기 때문에 좋은 결과를 얻기 어렵다. ​그래서 high-level feature와 high-resolution을 둘 다 가진 상태로 box proposal 할 수 있는 모델을 제안했다.​FPN은 Feature Pyramid Network의 약자이며, high level feature 와 high resolution의 두가지 목표를 다 달성하고자 했다.  high level feature를 얻은 상태에서, upsampling을 해서 resolution을 억지로 높인 4개의 layer를 만든다.4개의 upsampling layer는 resolution이 실제로 좋은 상태는 아닌데, 실제 layer를 1*1 conv를 통과시켜서 합쳐주면, high resolution 까지 갖춘 layer들을 얻을 수 있다.여기서 각 layer마다 box를 predict 해주면 높은 해상도와 높은 feature level을 가진 BB를 얻을 수 있다.​​[출처] https://youtu.be/AfseiFiz9MI ​ "
(객체 탐지) Object Detection / RCNN 개념 ,https://blog.naver.com/wooy0ng/222869649456,20220907,"개요객체 탐지의 기본 개념을 잘 모른다면 아래의 포스트를 참고하고 오길 바란다. (객체 탐지) Object Detection 개요개요 객체 탐지 문제를 공부하기 이전에 우선 전통적인 이미지 분류 문제를 한번 생각해봐야한다. 위 그림...blog.naver.com ​​​​ RCNN (Regions with Convolutional Neuran Network)RCNN(Regions with Convolutional Neuran Network)은 객체 탐지 분야에 딥 러닝을 처음으로 적용한 Two-Stage detector이다.​RCNN의 기본 흐름은 아래와 같다. ​먼저 CNN 네트워크를 통과하여 생성된 Feature Map을 Object가 있을 만한 영역(Region)을 추천하는 네트워크인 Region Proposal에 투입하여객체가 있을 만한 영역에 대한 정보를 얻어낸다.​​​​​​ Region Proposal우리가 VGGNet을 사용하여 특정 그림에 대한 Feature map을 얻었다고 해보자.Region Proposal은 이 Feature map을 입력으로 받아 객체가 있을 만한 영역에 대한 정보를 반환하게 된다.​​조금만 더 자세히 살펴보도록 하자. 먼저 3 X 3 Convolution(Padding:1, Stride:1)을 한 번 거쳐서 ​입력으로 들어가는 Feature map과 크기는 같지만,Region Proposal의 목적에 맞는 새로운 Feature Map을 만들어낸다.​​​​​​이제 새롭게 만들어진 7 X 7 X 512 크기의 Feature map을2개의 1 X 1 Convolution에 각각 입력으로 넣어주게 된다. ※ anchor box객체를 표현하는 상자를 의미한다.개수는 수동으로 설정할 수 도 있고, K-means 클러스터링 등을 사용하여 자동으로 맞춰 주기도 한다.​​여기서 중요하게 봐야할 것은 1 X 1 Convolution 연산이 입력에 어떤 영향을 미치는지이다.​1 X 1 Convolution 연산은 GoogLeNet에서처럼 연산에 필요한 파라미터 수를 줄여주는 것이 아닌가 생각할 수도 있는데 구조상 그런 용도로 사용된 것은 아닌 것 같으니 깊게 이해해볼 필요가 분명히 있다.​​​​​그림으로 살펴보도록 하자. Input으로 들어가는 Feature Map의 사이즈가 7 X 7 X 512였다.그리고 1 X 1 Convolution을 거치고 나면 위 그림과 같이 압축된 Output을 얻을 수 있게 된다.​​​그렇다면 우리는 무엇을(어떤 데이터를) 압축한 것일까?바로 이미지의 특정 Region에 대한 정보가 될 것이다.​ 위 그림을 자세히 봐보자.VGGNet의 Input Image의 크기는 224 X 224 X 3이고, VGGNet의 Output Feature Map의 크기는 7 X 7 X 512이다.​이말은 즉, Output Feature Map의 한 블록은이미지의 32 X 32 X 3 크기의 특정 부분의 정보를 압축하고 있다는 의미가 된다.​​​​​​다시 본론으로 돌아와서 각 블록에 대해서 1 X 1 Convolution을 통과하고 나면아래와 같이 Region Proposal의 용도에 맞게 압축된 새로운 Feature Map을 얻을 수 있게 된다. ​​​​​​​다시 Region Proposal 네트워크로 돌아가보자. 만약 Anchor Box의 개수를 1개라고 가정한다면 위와 같은 결과를 얻을 수 있게 된다.(이때 Anchor Box의 개수가 1이니 탐지할 수 있는 객체는 1개가 될 것이다)​1) 의 결과는 객체 존재 여부를 판단하기 위해 다시 사용될 것이며2) 의 결과는 Bounding Box을 위해 다시 사용될 것이다.​​​​1) Objectness각각의 map은 객체의 존재 유무를 판단하는 역할을 수행하게 된다. 각각의 map에서 동일 위치의 블록을 가지고와서 ​Non-Object map에서 가지고 온 블록의 값이 1에 가까울 수록 Object가 존재하지 않을 것이라 판단하고Object map에서 가지고 온 블록의 값이 1에 가까울 수록 Object가 존재할 가능성이라고 판단하는Objectness Vector를 만들어낸다.​​​​​​2) Bounding Box각각의 map은 Object의 테두리(Boundary) 위치 정보를 담고 있다. 각각의 map에서 동일 위치의 블록을 가지고온 후,위 그림과 같이 위치 정보를 가지고 있는 Bounding Box Vector를 만들어낸다.​​​​​​​Concatnate위 두 과정을 거쳐 얻게 된 Objectness Vector와 Bounding Box Vector를 서로 Concatnate하게 되면최종적으로 이미지의 특정 부분에 대한 객체 존재 유무와 Bounding Box의 위치 정보를 나타내는 벡터를 얻을 수 있게 된다. ​​​​​​​​​​ RoI PoolingRCNN의 RoI Pooling 층은 온전히 FC Layer 때문에 존재한다. 위에서도 얘기했듯이 위치정보와 마찬가지로 Anchor Box의 크기도 정답이 정해져 있는 것이 아니라서왼쪽의 그림과 같이 모델이 Anchor Box를 크게 그리는 경우도 있고,오른쪽과 같이 모델이 Anchor Box를 작게 그릴 경우도 분명히 발생할 것이다.​​하지만 FC Layer의 경우에는 Input으로 들어갈 Feature의 수를 고정시켜줘야 하기 때문에정해둔 FC Layer의 Input Feature의 크기에 맞추어 Anchor Box의 크기를 조정해줘야 한다. 이 역할을 해주는 것이 바로 RoI Pooling이며 사용자가 미리 정해둔 격자를 기준으로 Max Pooling하여 차원을 축소시키는 역할을 하게된다.​​  ​이것이 RCNN의 기본 Architecture이다.이 포스트에서는 Region Proposal, RoI Pooling의 기본 개념에 대해서 다뤄보았다.​시간이 된다면 구현을 하는 포스트도 올려보도록 하겠다.​​ "
3D Object Detection Survey - 3. Fundamentals ,https://blog.naver.com/mikangel/222476321368,20210819,"이 섹션에서는 3D 객체 감지 문제를 공식화하고 일반적으로 사용되는 표기법을 나열하고 기본 좌표 변환을 소개합니다.​A. 문제 공식화 별도의 언급이 없는 한, 본 설문조사에 사용된 표기법은 표 II에 명시되어 있습니다.  다음과 관련된 모든 방법이 비교를 위해 KITTI 데이터 세트를 기반으로 한다는 점을 감안할 때 본문의 나머지 부분에서는 Karlsruhe Institute of Technology와 Toyota Technological Institute(KITTI) [19, 51] 프로젝트에서 채택한 카메라 좌표계를 따릅니다. x축이 오른쪽을 가리키고 y축이 2D 이미지 평면에 대해 아래쪽을 가리킬 때 z축은 깊이를 나타내며 2D 이미지 평면에 수직인 오른손 좌표계입니다.표시는 {{Bi, Ci} = Φ(Di; Θ): i = 1, ..., M}​M은 다음 섹션 IV에 지정된 모델 Φ(D; Θ)를 통해 3D 경계 상자를 감지했습니다. B = [wi , hi , li , xi , yi , zi , θi , φi , ψi ] ∈ R 9 : i = 1, ..., M , 집합 C는 관심 카테고리이고 D ⊆ R n ×3은 연속 3D 공간의 하위 집합입니다. 또한 wi , hi , li 는 각각 너비, 높이, 길이를 나타내며 xi , yi , zi 는 B 하단 중심의 3차원 좌표입니다. 회전 각도는 모든 물체가 지면에 있다고 가정하여 θi만 고려합니다. 그림에서 볼 수 있듯이 3D 경계 상자는 일반적으로 크기 차원(w, h, l)을 제외하고 3D 중심 좌표(x, y, z)를 포함하는 7개의 매개변수 집합으로 인코딩됩니다.  그림 3: 3D 경계 상자 매개변수화 비교[52]에서 제안된 8개의 모서리, [53]에서 제안된 높이가 있는 4개의 모서리, [54]에서 제안된 축 정렬 상자 인코딩 및 지향성 [22, 32, 41, 43, 45]에서 채택된 7개 매개변수로 구성된 3D 경계 상자.이러한 매개변수화는 대부분의 이전 논문[22, 32, 41, 43, 45]에서 널리 받아들여지고 있습니다. 또한, 그림 4는 포인트 클라우드에서 3D 객체 감지의 예를 보여줍니다. B. 좌표 변환 ​멀티모달 융합 기반이든 포인트 클라우드 기반이든 좌표 변환은 처음부터 끝까지 진행됩니다. 우리는 논문의 범위를 벗어나는 구체적인 수학적 유도를 포함할 생각은 없지만, 실제로 사전 처리(예: 데이터 보강)의 필수 전제 조건이라는 점을 감안할 때 기본 개념을 제공할 필요가 있습니다. 기존 연구의 대부분은 KITTI 데이터셋을 기반으로 하고 있기 때문에 KITTI 데이터셋을 예로 들어 해당 연구에 적용되는 주요 원칙을 소개합니다. 일반적으로 LiDAR 및 카메라 좌표계는 다음과 같이 정의됩니다. 1) LiDAR 좌표: x=forward, y=left, z=up. 2) 카메라 좌표: x=오른쪽, y=아래, z=앞.​LiDAR 좌표의 3D 점 p = (x, y, z, 1)T가 주어지면 i번째 카메라 이미지에서 해당 점 y = (u, v, 1)T는 다음과 같이 주어집니다.  여기서 i ∈ {0, 1, 2, 3}은 KITTI 센서 설정의 카메라 인덱스이며, 여기서 카메라 0은 기준 좌표입니다. 는 LiDAR 좌표에서 카메라 좌표로의 rigid body 변환 행렬이며,  은 rectifying rotation 행렬,  는 rectification 이후 projection 행렬 입니다.​4개의 카메라 중심은 모두 동일한 x/y - 평면에 정렬되어 있다고 가정하면,  이고 여기서 bx(i) 는 기준 카메라 0에 대한 기준선이고, fu(i) 및 fv(i)는 각각 카메라 좌표에서 u축 및 v축으로부터의 카메라 초점 거리를 나타냅니다. 사실, Krect(i)는 카메라 내장 함수를 나타냅니다. 자세한 내용은 [19, 51]을 참조하십시오.​#3d #LiDAR #ObjectDetection​ "
[논문 리뷰] End-to-End Semi-Supervised Object Detection with Soft Teacher ,https://blog.naver.com/tnsgh9603/222692329025,20220405,"본 논문은 semi-supervised 라는 학습 방식에 대해서 다룹니다.​ 우선 semi-supervised learning에 대해서 간단하게 설명드리겠습니다. semi-supervised learning이란 label이 되어 있는 이미지와 label이 되어있지 않은 이미지를 섞어 object detection 학습하는 방법론이라고 이해하면 될 것 같습니다.​ object detection에서는 dataset을 만지는 일 즉, data labeling을 하는 것이 가장 피로한 작업이라고 합니다. 그래서 이런 문제들을 해결하기 위해 최근에 semi-supervised 혹은 weakly supervised learning을 이용하는 방법론이 많이 나오고 있습니다.​우선 용어에 대해서 설명을 드리겠습니다. 위 그림을 보시면, supervision 같은 경우에는 전부 다 라벨을 한 거고, weakly-supervision 같은 경우에는 이미지 내에 어떤 물체들이 있는지만 표시한 것입니다. semi-supervision은 일부의 이미지들에 대해서만 라벨을 한 거고, weakly semi-supervision 같은 경우는 weakly-supervision과 semi-supervision을 섞을 것이라고 이해하면 됩니다.오늘 다룰 내용은 semi-supervision 쪽 입니다.​ semi supervision object detector를 학습하는 큰 두 가지 방법론들이 있는데, consistency methods와 수도(pseudo)-label methods 가 나옵니다. consistency methods는 오늘 발표와 크게 관련이 없어서 간단하게만 설명드리자면 같은 이미지에 weak augmentation을 걸어서 prediction을 하면 결과에 유사성을 띄어야 한다는 점을 이용해서 학습을 하는 방법론이라고 합니다. 그리고, pseudo-label methods 같은 경우는 여기 이렇게 teacher 모델을 생성해서, teacher 모델이 실제 사용하는 모델의 label을 제공하는 방식으로 이루어집니다.​ 그래서 soft teacher가 달성한 contribution은 크게 4가지가 있습니다.우선, 수도 레이블링 프레임워크를 end to end로 가능하게 해서 학습 중에 teacher도 계속 발전하면서 성능이 좋아지도록 training 구조를 만들었습니다. 그리고, soft teacher와 box jittering을 제안하는데, 이 두가지는 수도 레이블의 reliability를 평가하기 위한 방법론들이라고 생각하시면 될 것 같습니다.soft teacher 같은 경우에는 background box에 대해서 진행을 하고 box jittering 같은 경우에는 box의 regression 성능에 대해서 reliability 평가를 진행합니다.Papers with code에 들어가면 soft teacher가 mAP 기준으로 61.3 이라는 굉장히 큰 수치로 SOTA를 갱신했다고 합니다.​ 우선 soft teacher의 전체적인 프레임워크를 먼저 설명드리겠습니다.여기 보시면 이 가운데 두 개의 soft teacher와 student가 오고, train을 시작할 때 각각 랜덤하게 initialize가 됩니다. 그리고 학습이 진행되면서 soft teacher는 student에 EMA 업데이트를 이용해서 soft teacher를 업데이트하게 됩니다.​그래서 학습 루프를 크게 설명드리면, student가 labeled data에서 먼저 loss 계산을 하게 됩니다. 그리고 soft teacher가, 약하게 augmentation이 걸린 image들에 대해서 prediction을 하고, 그 prediction에 NMS를 걸어서 유의미한 box만 남깁니다. 남겨진 box에서도 confidence가 높은 box만 남겨서 양질의 수도 레이블들을 제공합니다. 여기에서 box regression variance 필터라는 것을 적용하는데 이에 대한 자세한 내용은 뒤에 설명을 드리겠습니다. 이러한 구조를 적용해서 regression 자체에 성능이 좋은 특정 box만 몇 개 남기게 됩니다.또한 student 같은 경우에는, student가 soft teacher보다 prediction을 잘하면 안 되기 때문에, unlabeled 데이터에서 augmentation이 강하게 걸린 이미지들을 받아서 prediction을 수행하게 됩니다. 그래서 이 prediction의 결과와 수도 레이블의 비교를 통해 loss를 계산해서, 총 3가지 loss로 detector를 학습하게 됩니다.​ 앞에서 soft teacher를 만드는데 사용되었던 EMA, 즉 Exponential Moving Average를 간단하게 설명드리면, 어떤 student model이 있고 이 모델이 학습이 되면서 체크포인트를 계속 생성하게 될텐데, 생성된 체크포인트들의 최근 가중치에 높은 가중치를 줍니다. 즉, 오래된 데이터가 미치는 영향을 지수적으로 감소하게 만들어 주는 방법입니다.​ 다음은 End-to-End 프레임워크로 학습을 수행했을 때 어느 정도 성능이 좋아졌는지를 보여주는 표입니다. 먼저 supervised 같은 경우에는 coco dataset 기준으로, 전체 dataset에서 10%만 label된 setting으로 faster R-CNN을 ResNet-101에다가 학습시킨 것으로 보면 됩니다.​supervision의 경우 mAP가 27.1밖에 안나오는데 E2E(End-to-End) 학습 방법으로는 30, 그리고 EMA까지 같이 학습했을 때는 31.2의 높은 성능이 나왔다고 하는 실험결과가 있습니다.​ soft teacher가 제안된 배경에 대해 잠시 설명드리겠습니다.왼쪽 그래프는, Foreground score에 따른 precision과 recall의 그래프 입니다. 저자들이 실험을 할 때 foreground score를 threshold(뜨레쉬홀드)를 높여서 precision이 잘 나오는 쪽으로 학습을 하면, 아무래도 object detector의 학습 방법이 mAP이다 보니 좋은 결과가 나온다고 합니다. 그런데 문제는 이렇게 하게 되면, threshold가 조금만 낮아도 많은 box들이 background로 판정을 받게 되면서 학습의 불안정성이 유도가 된다는 단점이 있습니다.​  그래서 이 방법을 극복하기 위해서 soft teacher를 제안하고, teacher 모델이 이렇게 수도 레이블을 생성하게 되는데(마우스 클릭), 여기 옆에다가 student 모델이 prediction을 만들게 됩니다. 왼쪽 같은 경우에는 유사도가 높아서 foreground로 분류될 확률이 높지만, 오른쪽 같은 경우에는 IoU가 0.9이하로 나오는데도 불구하고 이 초랙색 box를 background로 보기에는 문제가 있습니다.​ 그래서 이런 부분들을 완화하고자 student가 계산했던 prediction 즉, 초록색 box에 있는 background score을 다시 teacher 모델에 넣어서, teacher 모델이 box에 background 점수를 계산하게 됩니다.  계산된 background 점수를 어떤 가중치를 계산할 때 사용됩니다.식을 보시면 일반적인 object detector를 학습하게 될 때, foreground loss와 background loss가 들어가게 되는데, background loss에 가중치가 들어가게 되고, 이 가중치는 어떤 reliability라고 부르는 것들의 전체합 분의 이 box가 가지는 reliability로 계산이 되게 됩니다.​ reliability를 계산하는 방법을 비교하는 그림입니다.그냥 단순하게, student 모델이 계산한 prediction confidence 그리고 prediction difference라는 값을 또 새로 제안하게 되고, intersection over union을 사용했을 때의 값들을 비교하게 됩니다. 결론은 student 모델에 prediction을 teacher가 다시 평가하는 방식을 사용했을 때가 mAP 향상에 가장 높았다라고 밝히고 있습니다.​ box jittering에 대해 설명 드리겠습니다.box jittering은 box의 위치가 얼마나 정확하게 생성이 되었는가를 평가하는 지표로서 제시가 됩니다. ​저자들이 pseudo label이 얼마나 잘 생성되었는지 평가하기 위한 방법론들을 찾습니다. 저자들이 처음에는 그냥 confidence가 높으면 localization도 잘 되지 않겠느냐 라는 가정을 했었던 것 같은데 왼쪽 그래프를 보시면 prediction confidence가 높다고 해서 항상 IoU가 높게 나오지는 않습니다. 자세히 보면 0에 가까운 수치가 나올 정도로 IoU와 prediction confidence는 무관합니다. 근데 이제 저자들이 제안하는 box regression variance를 적용 했을 때는, box regression variance가 적을 때 IoU가 높다는 사실을 보이게 됩니다.​그래서 여기서 이 box regression variance 라는 것을 어떻게 계산하는지 보여드리겠습니다. 이 그림을 보시면 먼저, teacher가 prediction을 return 했다고 가정하면, 이 결과 같은 경우에는 prediction을 잘한 경우라고 할 수 있습니다. prediction을 한 과정은 다음과 같습니다. 여러 개로 랜덤한 오프셋을 주어서 prediction 값을 흐트려뜨렸기 때문에 jittered라는 표현을 사용했고, 이 jittered된 box들을 다시 teacher에게 집어 넣는 것 입니다.그래서 원래 prediction이 잘 됐던 애를 조금 흐트러트려도 어차피 얘네들이 오브젝트 탐지기 대상 물체 근처에 있을 것이기 때문에 다시 원래 자리로 돌아올 것이다라는 가정이 들어가 있습니다. 이렇게 되면 최종적으로 jitter된 box들을 다시 soft teacher가 regression을 했을 때 원래 자리로 돌아오기 때문에, 각각의 box들에 대한 분산도가 적을 것이다 라는 설정입니다.​ 그리고 teacher 모델이 수도 레이블을 생성할 때, regression이 잘 안된 bad regression의 경우인데, 여기서 jittered 한 다음에 다시 teacher에게 들어가면은 jittered가 되면서 얘가 완전히 배경 쪽으로 밀려나기 때문에 regression 결과가 여러 군데로 흩어질 것이라는 가정입니다.그 가정을 통해 실험을 하였을 때 앞에서 본 그래프처럼 box regression variance가 IoU와 상관관계를 보이는 것을 확인하였고, box regression variance를 기준으로 수도 레이블들을 필터링해서 학습에 사용합니다.  Experiments​ 그래서 box jittering에 대한 ablation study를 진행했습니다.위 표는 box jittering을 통해 box regression variance가 0.02가 나올 때의 성능이 가장 높았다고 나와있습니다.​아래는 jittered box를 몇 개까지 했을 때 성능이 가장 높냐에 대한 실험인데, 10개 이상으로는 유의미한 차이가 없었다라고 나와있습니다.variznce 값이 하이퍼 파라미터로 들어갑니다.  semi-supervised object detection 같은 경우에 coco dataset 기준으로 전체 데이터셋에서 1%, 5%, 10%가 레이블 되어있을때 진행한 실험의 결과입니다. 전부 soft teacher가 sota를 달성했다고 나와있습니다.​ train data는 전부 supervision loss를 계산하는데 사용하고, unlabel data는 semi supervision loss를 계산하는데 사용하는 방식으로 htc detector와 backbone으로 swin transformer 조합을 soft teacher 방법론으로 학습을 시켰을 때 mAP 59.9의 성능을 달성했다고 합니다.​ soft teacher과 box jittering을 함께 사용한 경우가 mAP 가 높았다고 보여주고 있습니다.  Conclusions​Soft teacher, EMA, box jittering 등을 사용해서 end-to-end semi-supervised object detection에 있어 성능 향상에 도움이 되었다.아쉬운 점Pseudo-labeling 실험 setup이 조금 아쉽다.-> 1%, 5%, 10%가 아니라 30% 50% 80%이면 어땠을까?  Reference​https://www.youtube.com/watch?v=7gza1VFjb0k&t=348s ​Paperhttps://arxiv.org/abs/2106.09018 End-to-End Semi-Supervised Object Detection with Soft TeacherThis paper presents an end-to-end semi-supervised object detection approach, in contrast to previous more complex multi-stage methods. The end-to-end training gradually improves pseudo label qualities during the curriculum, and the more and more accurate pseudo labels in turn benefit object detectio...arxiv.org ​ "
"YOLO(You only look once): Unified, real-time object detection ",https://blog.naver.com/zmffhqj35/222563081306,20211109,"YOLOYOLO You Only Look Once: Unified, Real-Time Object Detection Joseph Redmon, Santosh Divvala, Ross Girshick, Ali Farhadidocs.google.com 01). You Only Look Once 논문리뷰 · GitBookYou Only Look Once - Paper Review 01. Introduction 해당 논문이 나오기 전(2015년 전)에는 딥러닝 기반의 Object Detection System들은 Classification 모델을 Object Detection 시스템에 맞게 변형한 모델들이 주를 이루었습니다. Classification 모델을 변형한 시스템은 다양한 위치 및 크기에 대해서 학습하고 테스트를 진행했습니다. 대부분의 변형된 모델들은 Deformable Parts Models(DPM) 이 sliding window 방법론을...deepbaksuvision.github.io YOLO(You only look once): Unified, real-time object detection 논문 리뷰YOLO(You only look once): Unified, real-time object detection 논문을 정리한 글입니다!zzsza.github.io INTRO & ABSTRACTR-CNN의 속도 문제 개선Undefied DetectionSingle Neural Network & End to Endregion proposal / classification / box regression  -> Regression TaskWhat is Anchor Box​real-time object detectionBase : 45 FPSFast : 155 FPS이미지를 전체적으로 찍듯이 한 번만 봄 (sliding window -> covnolutional neural network)general representations of object를 학습 가능 (일반화 성능 좋음)그림 감지 가능했던가? yes. artwork 이미지를 입력했을 때 다른 모델 대비 detection 성능 좋음.​그러나 작은 객체를 잘 감지하지 못함.state-of-the-art에는 못미치는 성능.​​Unified Detection이미지를 S X S 그리드 영역으로 나눔물체가 있을만한 영역에 해당하는 B개의 Bbox 예측 (x_center, y_center, width, height)Bbox의 confidence 계산 (해당 그리드에 물체가 있을 확률 * IOU(예측한 box & Ground-truth)Pr(Class(i)|Object) 계산. 해당 클래스일 확률. - 기존 모델은 class num + 1(background) "
"You Only Look Once : Unified, Real-Time Object Detection[YOLO] ",https://blog.naver.com/jjunsss/222614443394,20220106,"YOLOwith. YOLOv5이전에 yolov5 를 사용해서 커스텀 데이터 셋을 구현하고, 따로 훈련하는 방법에 대해 작성했었다.이번에는 Yolo에 대해 제대로 알아보기 위해 논문을 살펴보게 되었다.원본은 notion에 정리해두었고 블로그에 옮겨서 작성합니다.혹시나 이 글을 읽는 분 중에 노션에서 깔끔하게 보고 싶으시다면 아래 링크를 !  Notion 정리 링크https://billowy-background-021.notion.site/You-Only-Look-Once-Unified-Real-Time-Object-Detection-2c30021eee90434085f7c99c5ec48d6f You Only Look Once : Unified, Real-Time Object DetectionSummarybillowy-background-021.notion.site  Summary> YOLO : You Only Look Once. 이미지를 전체적으로 한 번에 훑어보고 detection 한다고 해서 저자들이 붙이게 되었습니다.> 2015년 시점으로 가장 뛰어난 성능과 속도를 보여준 Image Detection 방법입니다. ( R-CNN, DPM )> 빠르고 Robust 한 모델이어서 다양한 어플에 적용이 가능합니다.> 다른 모델에 비해 훈련에서 없었던 이미지들도 잘 파악하고 검출합니다.> 아까지도 연구되어지는 검출 방법입니다. ( YOLO v5 )​​Abatract연구진들은 Image Detection을 기존에 사용되던 Classifier 기반의 방법에서 regression 방법으로 재정의하였습니다. 이를 통해 bounding box & class probabilities 를 동시에 계산할 수 있게 되었습니다. 또한 이는 하나의 연산망에 의해 계산됩니다. ( 다른 모델이 여러 개의 연산망을 이어 붙인 것과 차이를 보입니다 )> YOLO는 매우 빠릅니다. ( 2015 년 기준. 2021년 기준으로도 계속해서 업데이트 되어 좋은 성능을 보임 )> 객체 검출 파이프라인이 하나의 신경망으로 구성되어 있습니다. ⇒ end-to-end 형식입니다. https://wooono.tistory.com/228< E-T-E 단 연결에 대해 잘 설명해주셨습니다. > > YOLO는 물체의 일반적인 특성을 학습합니다.​Introduction사람의 시각체계가 매우 빠르고 정교하기 때문에 복합적인 일을 할 수 있다고 합니다. 이를 기반으로 이미지 검출이 가능하다면 자율주행과 같은 역할을 수행할 수 있다고 합니다. 기존에 사용되던 이미지 검출 모델들은 Classifier 를 재정의 하여 시험 이미지의 위치와 크기를 Detection 합니다.이러한 방법의 대표로는 R-CNN, DPM 등이 있습니다. ( 해당 논문에서는 이 두 방법을 계속해서 YOLO와 비교합니다 ) DPM 방법은 이미지 전체에서 슬라이딩 윈도우를 사용하여 객체를 검출합니다. R-CNN 방법은 이미지 안에서 Potential bounding box를 만들고 여기에 classfier 를 돌려 객체를 검출합니다. 이러한 방법들은 파이프 라인을 복잡하게 만들고 최적화가 어렵고 느립게 되도록 방해합니다.따라서 연구진들은 이러한 방법을 개선하여 객체 검출을 Regression(연속변수..?)을 통해 해결하였습니다.우선 YOLO의 이름을 살펴보자면 아래와 같습니다. 번역하면 이미지를 한 번 보면 검출할 수 있다고 합니다. Using our system, you only look once (YOLO) at an image to predict what objects are present and where they areYOLO paper밑으로 특징을 정리해보자면> YOLO는 굉장히 빠릅니다. 기존의 복잡한 파이프 라인을 ETE가 가능한 하나의 회귀 문제로 바꾸었기 때문입니다. ( TITAN X GPU : 45 fps , Fast YOLO : 150 fps )> YOLO는 예측할 때, 이미지 전체를 파악합니다. 앞 서 소개했던 슬라이딩 윈도우, region proposal 방법과는 다르게 욜로는 이미지 전체를 보고, 은연중에 그들이 나타내는 주변정보도 파악합니다. 이로서 욜로는 background error가 다른 이미지 검출 방법에 비해 매우 적게 나오기도 합니다.> YOLO는 객체의 일반적인 부분을 학습합니다. 때문에 다른 방법에 비해 낯선 환경에 두어도 잘 학습합니다.> 다만 YOLO는 최신식의 객체 검출 방법에 비해 정확도가 낮습니다. 빠르게 검출하지만 정확도가 많이 떨어지고 특히나 작은 물체를 검출하는데 애를 먹습니다.​​Unified Detection욜로는 단일 신경망을 통합한 모델입니다. 이미지 전체에 있어 모든 클래스의 바운딩 박스를 동시에 예측합니다.> S x S grid 로 이미지를 나눕니다.> 어떠한 객체가 그리드 안에 들어간다면 해당하는 부분이 검출합니다.> 각 그리드는 bounding box와 신뢰도를 예측합니다. 신뢰도는 바운딩 박스 내부에 객체가 존재하는 것을 어느 정도 믿을만한가?에 대해 판단합니다.논문에서는 아래와 같이 신뢰도를 계산합니다. IOU : intersection over union. 그러니까 실제와 예측의 BB(bounding box)가 어느 정도 겹치는 지를 이야기하는 수식입니다. >  각각의 BB는 x,y,w,h,c(confidence)를 측정합니다. 이는 Normalization 됩니다.>  이후에 각각의 그리드 셀은 conditional class probability를 측정합니다. > BB의 개수에 상관없이 하나의 그리드 셀은 하나의 class set을 가지게 됩니다.> class specific confidence score : BB에 class가 나타날 확률 x 예측된 BB가 그 class가 얼마나 잘 맞아 떨어지는지에 대해 계산하는 값.​ > pascal VOC dataset을 사용하였습니다.​​​Network Design CNN을 사용하였으며network의 앞 부분은 Convolution으로 이미지의 feature를 추출하고, 뒷 부분의 FC는 이미지의 클래스 확률과 BB의 좌표를 예측합니다. 또한 GoogleNet을 기반으로 만들어졌습니다. 여기에는 네트워크의 연산량을 줄이고 이미지 특징을 압축하기 위한 1x1 커널이 존재합니다. 최종 출력은 7 x 7 x 30의 텐서를 가집니다.​Training> 저자가 직접 만든 DNN 전용 프레임워크인 Darknet에서 사용되어졌습니다.> 이미지를 잘 해석하기 위해 224 x 224 ⇒ 448 x 448 로 변경하였습니다.> 마지막 레이어는 class probabilities & BB corrdinate를 측정합니다.> Leack Activation Fucntion을 사용하였습니다.​> SSE loss 계산을 진행합니다. ( Sum-squared error ) > > 하지만 SSE를 최적화하는 방식은 YOLO의최종 목적은 아닙니다.> > 또한 큰 BB를 가진 객체보다 작은 BB를 가진 객체가 위치 변화에 민감합니다. 조금만 틀리게 조정이 되어도 작은 객체를 벗어나게 됩니다. 이를 해결하기 위하여 BB의 너비 및 높이에 Square root를 취합니다.​> 모델의 대부분이 배경인 것은 모델의 학습에 불균형을 초래합니다. ( confidence = 0 ) 따라서 특정 가중치를 두어 배경과 객체의 학습 정도를 다르게 진행합니다.> 하나의 그리드 셀 당 여러개의 바운딩 박스를 측정합니다. 저자들은 여기의 각각 predictor 들이 객체에 대한 책임이 있어야 한다고 합니다.​모델의 Loss를 계산하는 것은 저자들이 식으로 나타내었습니다. 다양한 상황과 가중치를 고려하여 나타내었는데 하나씩 살펴보면 그다지 어렵지 않습니다.​ (1) Object가 존재하는 그리드 셀 i의 bounding box predictor j에 대해, x와 y의 loss를 계산.(2) Object가 존재하는 그리드 셀 i의 bounding box predictor j에 대해, w와 h의 loss를 계산. 큰 box에 대해서는 작은 분산(small deviation)을 반영하기 위해 제곱근을 취한 후, sum-squared error를 구합니다. (같은 error라도 큰 box의 경우 상대적으로 IOU에 영향을 적게 줍니다.)(3) Object가 존재하는 그리드 셀 i의 bounding box predictor j에 대해, confidence score의 loss를 계산. (Ci = 1)(4) Object가 존재하지 않는 그리드 셀 i의 bounding box predictor j에 대해, confidence score의 loss를 계산. (Ci = 0)(5) Object가 존재하는 그리드 셀 i에 대해, conditional class probability의 loss를 계산. (p_i(c)=1 if class c is correct, otherwise: p_i(c)=0)λ_coord: coordinates(x, y, w, h)에 대한 loss와 다른 loss들과의 균형을 위한 balancing parameter.λ_noobj: 객체가 있는 box와 없는 box 간에 균형을 위한 balancing parameter. (일반적으로 image내에는 객체가 있는 그리드 셀보다는 없는 셀이 훨씬 많으므로)1_{ij} : i 번째 셀 내부에 있는 j 번째 BB predictor를 의미합니다.​<출처 : https://bkshin.tistory.com/entry/논문-리뷰-YOLOYou-Only-Look-Once > ​또한 저자들은 LR Scheduler를 사용하였으며 overfitting을 방지하기 위해 data augumentation도 사용하였습니다.​​Inference추론 단계에서도 훈련과 동일한 모델로 사용하면 된다고 합니다. 다만 하나 단점이 있다면 다중 BB로 인하여 여러가지 BB가 겹쳐서 출력이 된다는 점인데 이를 Non-Maximun Suppression 이라는 방법을 통해 하나만 출력이 되도록 합니다. ( 해당 기법은 후에 SSD 에서도 사용되어집니다. 최대로 객체를 포함하는 BB 이외의 값은 제거하도록 하는 방법입니다. ) 욜로는 이를 통하여 2 ~ 3 % mAP 가량 향상시켰습니다.​Limitation of YOLO> 욜로는 각 그리드 셀에서 하나의 객체만 검출되어지도록 공간적 제약을 부여합니다. 따라서 작은 세 때처럼 작은 물체가 모여있는 경우에 허덕이게 됩니다.> 이전에 학습하지 않은 종횡비를 만나게 되면 계산하는데 어려움을 겪습니다.> 모델이 큰 BB와 작은 BB 모두를 같은 가중치를 가지고 훈련을 한다는 단점이 있습니다. 앞에서도 언급했다시피 작은 BB는 조그만한 이동에도 많은 영향을 받게 됩니다. 이를 주요 에러 중에서도 부정확한 지역화( incorrect localization) 문제라고 합니다.​Comparison to Other Detection System다양한 모델이 있지만 2015년에 가장 대표적이 였던 R-CNN과 DPM과 비교합니다.> DPM > > 윈도우 슬라이딩 방법> > 분리된 파이프라인​> R-CNN > > region proposal 방법> > Selective Search를 통해 여러 potential BB를 생성.> > SVM 사용 및 Non-Maximum Suppresion 사용> > 한 이미지당 BB가 대략 2000개. ( YOLO는 98 개 )​​Experiments데이터셋은 Pascal VOC 2007을 사용하였습니다. 이 시점으로 가장 빠른 검출 방법은 Fast R-CNN 입니다.​​Comaprision to Other Real-Time Systems.Real-Time 으로 이미지 검출을 시행하려면 fps가 잘 나와야 합니다. 2015, 주목할 점은 위 Detectors해당 표를 통해 real-time에 적합한 모델들은 yolo 계열임을 확인할 수 있었습니다. (2015년 당시. 현재에는 다양한 모델이 많이 등장하였음. ) ​참고로 100 Hz, 30 hz DPM ( Deformable parts model ) 방법은 서로 같은 방법에 주파수만 다른 것 입니다.​ 해당 이미지는 YOLOv5 에 대한 사진입니다. github : https://github.com/ultralytics/yolov5 에서 제대로 된 이미지를 확인할 수 있으며 또한 다양한 임베디드 보드 또는 사용처에서 어느 정도의 성능을 보이는지 확인할 수 있습니다. GitHub - ultralytics/yolov5: YOLOv5 🚀 in PyTorch > ONNX > CoreML > TFLiteYOLOv5 🚀 in PyTorch > ONNX > CoreML > TFLite. Contribute to ultralytics/yolov5 development by creating an account on GitHub.github.com ​( 참고로 여담이지만, YOLOv3 or v4 (부정확) 부터는 darknet을 사용하여 작동하는 것이 아닙니다. 또한 이 때부터 제작자가 바뀌어서 기존의 YOLO와는 완전 다른 성격이지만 뿌리는 YOLO에서 등장하였습니다.  )​​VOC 2007 Error Analyze어느 부분에서 error 가 많이 나오는지 분석한 part입니다. 여기서도 Fast R-CNN과 비교하게 됩니다. 이러한 Detector들은 대회에서도 마찬가지지만 Error 가 어느정도로 나오는지가 매우 중요합니다. 따라서 ISLVRC 대회에서도 TOP 1 / TOP 5 error rate 를 기준으로 경쟁합니다.​ 해당 저자들은 Detector의 어느 분야에서 Error가 많이 나오는지 계산하였습니다. 또한 각각의 Error는 측정방법을 자세하게 기록해두었습니다.​> Correct : class가 정확하며 IOU > 0.5 인 경우> Localization : class가 정확하고,  0.1 < IOU < 0.5 인 경우> Similar : class가 유사하고 IOU > 0.1 인 경우> Other : class는 틀렸으나, IOU > 0.1 인 경우> Background : 어떤 Object라도 IOU < 0.1 인 경우​그래프를 살펴보면, YOLO는 localization error가 두드러집니다. 반면 Fast R-CNN은 Background error 가 두드러집니다.​​Combining Fast R-CNN and YOLO Yolo + Fast R-CNN 이 75 %의 mAP를 보이며 좋은 성능을 보이는 것을 확인할 수 있습니다. 하지만 이는 각 두개의 모델을 따로 돌리고 앙상블 모델을 구현하여 합치는 것과 같기 때문에 느릴 수 밖에 없습니다. ( YOLO에 비해 ) 하지만, YOLO가 매우 빠르기 때문에 Fast R-CNN을 단독으로 사용하는 것에 비해 훨씬 좋은 속도를 보입니다. 더불어 좋은 성능을 보이기까지 합니다.​​Generalizability : Person Detection in Artwork Fast R-CNN이 mAP 성능은 좋지만 새로운 환경 및 이미지들을 주었을 때 매우 좋지 않은 성능을 보였다고 합니다. 하지만 Y OLO는 훈련과정에 있지 않았던 새로운 이미지에 대해서도 잘 검출하는 모습을 보였다고 합니다. 이러한 특징이 YOLO를 어디서든 사용가능하도록 만들어주었습니다. ​​Real-Time Detection 실제로 저자들이 여러 웹에서 다양한 사진을 가져와 이미지 검출을 시행한 사진입니다. 중간에 사람이 비행기로 출력된 것 이외에는 모두 높은 정확도로 출력된 것을 확인할 수 있습니다.​Conclusion> 욜로는 이전의 모델과 달리 어느 상황에서도 강건하게 이미지 검출을 진행합니다. 이는 여러 어플리케이션에 활용될 가치가 충분합니다.> 여타 이미지 검출 방법에 비해 훨씬 빠르기 떄문에 Real-Time 으로 사용이 가능합니다. "
딥러닝 기반 object detection 과정 정리 (3) - 전반부 ,https://blog.naver.com/taeeon_study/222786780457,20220624,"본격적으로 컴퓨터 비전 분야 중 한 분야인 object detection 분야에서 SOTA(State of the Art)의 흐름을 살펴보고 중요한 네트워크들을 정리해보도록 한다. 대부분 프로젝트로 detection 해본 정도는 YOLO를 그냥 써봤다 정도였는데 실제로 알고 쓰면 더 효율이 좋지 않을까라는 차원에서 이번에 리마인드겸 정리를 해본다.object detection은 앞서 언급했듯이 분류의 문제와 위치 탐지를 동시에 고려해야 한다. 그렇기에 동시에 해결하기 위한 신경망 학습이 필요하고 손실 함수 (loss function)도 이에 맞게 설정될 필요가 있다. 대부분의 backbone은 뭐 그냥 앞에서 설명한 유명한 네트워크들이다. 크게 발전 방향은 2- Stage Detector와 1-Stage Detector로 나눌 수 있다. YOLO는 1-Stage detector이다. 아래 그림은 비록 시간이 꽤 많이 지난 것이지만 과거의 SOTA를 잘 확인할 수 있는 그림이다.  Flow그림 출처: https://mlai.iptek.web.id/2019/01/20/object-detection-state-of-the-art-progress/ Object Detection State of The Art ProgressList of object detection progress: R-CNNOverfeatMultiboxSPP-NetMR-CNNDeepBoxAttentionNetFast R-CNNDeep[ProposalFaster R-CNNOHEMYOLO v1G-CNNAZNetInside-OutsideNet (ION)HypernetCRAFTMultiPathNet (MPN…mlai.iptek.web.id 위 그림에서 빨간색으로 된 것은 상당히 유명한 알고리즘이다. 이것들 위주로 한 번 살펴보도록 하고 이를 넘어 최신 트렌드도 알아보도록 한다. 최신 트렌드는 papers with code에서 잘 확인할 수 있다.  papers with code이미지 출처: https://paperswithcode.com/task/object-detection Papers with Code - Object DetectionObject detection is the task of detecting instances of objects of a certain class within an image. The state-of-the-art methods can be categorized into two main types: one-stage methods and two stage-methods. One-stage methods prioritize inference speed, and example models include YOLO, SSD and Reti...paperswithcode.com 이 사이트로 들어가면 쉽게 체크할 수 있는데, 현재 Swin이 맨 위에 보인다. Swin transformer는 추후에 transformer 기반 네트워크에서 ViT 다음에 설명해보도록 한다. (이미 논문 리뷰까진 해놓았지만 블로그에 쓰는 시간이 꽤 많이 걸리기에... 언제가 될 진 미지수..?)​많은 사람들이 유명한 네트워크는 많이 아는데 도대체 어떤 과정으로 detection이 수행되는지 정확하게 이해하지못한 경우가 많았다. 아래 그림은 전체 순서도를 보여준다. object detector위 그림은 반드시 이해하고 넘어가야 한다. 이것이 전체 틀 구조이다. 지금 리뷰하려는 것들은 Head 부분의 유명한 네트워크 들이다. Backbone: pre-trained를 위한 것이다. (앞서 object detection 정리 (2)에서 다루었던 것들 많이 쓴다.)Head: object에 대한 class와 bounding box들을 예측하는 데 사용된다.Neck: 최근에 개발된 detector들이 대부분 backbone과 head 사이에 약간의 layer들을 삽입하게 된다. 이러한 layer들은 보통 서로 다른 stage들로부터 온 feature map을 모으는데 사용된다. ​이제 본론으로 돌아와서 위 progress 사진에서 빨간색으로 나와있던 네트워크 위주로 한 번 리뷰를 시작한다.​- 2-Stage Detector2-Stage Detector는 말 그대로 두 단계로 작동한다. 첫 번째 단계는 관심영역(ROI)를 추출하는 것이고 두 번째 단계는 분류 단계이다. ROI 각각에 대해 합성곱 네트워크의 입력에 맞추어 사각형으로 크기를 조정하고 CNN을 사용하여 ROI를 분류하게 된다. 이것의 가장 초기 그리고 유명한 알고리즘은 R-CNN부터 출발하고 현재 잘 알려진 Faster R-CNN까지 오게 된다.​R-CNN[1]객체인식을 위한 기본적이고 가장 대표적인 학습 모델이다. 관심 영역을 선정하고 영역 크기를 통힐한 후 각 영역의 CNN 전방 전파 연산을 하여 특징 추출을 하게 된다. 마지막으로 영역을 SVM으로 분류하고 위치를 회귀한다. SVM은 서포트 벡터 머신인데 이 역시 한 개의 블로그 포스트 감이므로 설명은 생략한다. R-CNNFast-R-CNN[2]R-CNN이 선택적 탐색에 의해 선정된 후보군을 CNN에 통과시켰다면 Fast R-CNN은 과정의 순서를 바꿔 R-CNN의 문제를 해결하였다. CNN에 의해 추출된 특징에 대해 연산을 수행하였기에 효율적인 객체 인식이 가능해졌다. Fast R-CNN그러나 ROI 정렬에서 입력 공간과 특징 공간 영역 위치가 정확히 일치하지 않는 문제가 발생한다. 그래서 특징들에서 영역을 제안하는 Faster R-CNN이 제안되게 된다.​Faster R-CNN[3]앞서 언급했듯이 영역 제안망인 RPN을 삽입한다. 격자의 중심점에서 고정적 크기의 박스에 객체가 있는지 예측하게 된다. Faster R-CNNRPN은 특징 맵의 각 점마다 고정 크기의 하나의 앵커박스를 활용한다. 각 점마다 앵커박스에 객체가 있는지를 예측하게 되고 (픽셀 단위로 로지스틱 회귀 적용) 객체가 있다고 판단되는 앵커박스에 대응되는 점에 대해서는 객체의 정확한 크기와 앵커박스 간의 변환을 예측하게 된다. 즉, ROI를 추출하며 객체를 찾는 과정이 RPN이다.정리하자면, 1단계에서는 CNN과 영역 제안망을 통해 사진 단위로 수행하고 2단계에서는 영역 단위로 ROI 요약 및 정렬, 객체 분류 및 위치 예측을 하게 되는 것이다.또, Faster R-CNN의 손실함수를 간단히 언급해보도록 한다. 4가지의 손실함수를 고려하게 되는데 첫 번째는 영역 제안망의 객체 유무 손실, 두 번째는 영역 제안망의 위치 박스 회귀 손실, 세 번째는 각 객체들의 최종 분류 손실, 네 번째는 최종 객체 위치 박스 회귀 손실 함수가 존재한다. loss function- 1-Stage Detector1-Stage Detector는 2-Stage와 달리 1단계만에 ROI 추출과 객체인식 모두 수행하게 된다. 먼저 YOLO original 버전부터 차근히 살펴보도록 한다.​YOLO[4]사전 학습된 CNN 모델을 변형하여 사용하였다. (Feature extractor 역할이라 생각하면 된다.) 다층의 CNN 층과 Fully Connected layer를 거쳐서 1x1x1470의 벡터를 출력한다. 그리고 이를 reshape하여 7x7x30 크기의 최종 tensor를 출력하게 된다. 여기서, tensor는 입력 이미지를 분할하는 7x7의 Grid Cell을 가지게 되고 각 cell마다 물체의 bounding box의 위치와 class 정보를 나타내는 1x1x30 벡터를 할당하게 된다. YOLO v1여기서 cell마다 2개의 bounding box가 지정되어 각각 1x1x5 벡터가 할당되며 confidence score가 있다. 이는 bounding box에 물체가 존재할 확률이며 나머지는 분류 클래스에 대한 확률이 담겨있다. 아래 그림 출처 블로그에 잘 설명 돼있다. prediction출처: https://blog.naver.com/sogangori/220993971883 YOLO, Object Detection NetworkYou Only Look Once : Unified, Real-Time Object Detection Joseph Redmon - University of ...blog.naver.com YOLO는 Bounding Box Regression 다음 Bounding Box filtering 그 후 마지막으로 classification의 과정을 거친다. regression 과정은 초기 anchor box 설정 후에 학습을 통해서 bounding box의 크기와 위치 조절이 되는 것이다. filtering은 confidence score filtering과 non-maximum suppression이 있는데 이 threshold는 사용자가 직접 코드에서 수정 가능하다. Bounding box filtering다음은 YOLO의 손실함수에 대해 알아보자. 아래 그림에서 첫 번째와 두 번째 행은 Bounding Box 크기 및 위치 추론 오차에 대한 term이다. 세 번째 행과 네 번째 행은 confidence loss에 대한 term인데 세 번째 행은 miss-detection에 대한 term이고, 네 번째 행은 false alarm에 대한 term이다. miss-detection에 대한 경우 자율주행 같은 application에서 심각한 문제가 될 수 있다. 마지막으로 다섯 번째 행은 객체 분류 오차이다. 실제와 다르게 예측하면 손실이 증가하게 된다. loss function of YOLO예전에 워드에 정리한 것들을 좀만 리뷰 정리하면서 올리고 있는데 아직 반도 안한 게 무슨 일이지.. 반은 커녕 1/4도 못했다 ㅋㅋㅋㅋ오늘 전반부는 여기까지 하고 중반부부터는 또 시간날 때 써야지...아래는 이번 포스트에서 설명했던 본 논문들이다.[1] GIRSHICK, Ross, et al. Rich feature hierarchies for accurate object detection and semantic segmentation. In: Proceedings of the IEEE conference on computer vision and pattern recognition. 2014. p. 580-587.[2] GIRSHICK, Ross. Fast r-cnn. In: Proceedings of the IEEE international conference on computer vision. 2015. p. 1440-1448.[3] REN, Shaoqing, et al. Faster r-cnn: Towards real-time object detection with region proposal networks. Advances in neural information processing systems, 2015, 28.[4] REDMON, Joseph, et al. You only look once: Unified, real-time object detection. In: Proceedings of the IEEE conference on computer vision and pattern recognition. 2016. p. 779-788. "
Proper Reuse of Image Classification Features Improves Object Detection 논문 리뷰!! ,https://blog.naver.com/cobslab/223037671576,20230307,"안녕하세요 콥스랩(COBS LAB)입니다.오늘 소개해 드릴 논문은 ‘Proper Reuse of Image Classification Features Improves Object Detection’입니다.해당 내용은 유튜브 ‘딥러닝 논문 읽기 모임'Proper Reuse of Image Classification Features Improves Object Detection’ 영상 스크립트를 편집한 내용으로, 영상으로도 확인하실 수 있습니다. (영상링크:https://youtu.be/Ikg5Mx3ITh4)  이 논문은 실험논문이고 논문에 대한 어떤 내용보단 실험 위주의 논문입니다.  가장 기본적으로 알고 있는 방법이 Fine-tuning 방법입니다. Pre-train 된 weight가 있고 여기서 ResNet을 쓰고 있습니다. 거기에다가 어떤 목적을 위한 여기선 object Detector를 붙이고 나서 Fine-tuning을 하게 되면 Pre-train weight도 변경되고 추가된 Detector도 변경되는 게 일반적으로 가장 많이 쓰이는 방법이고 성능이 가장 좋다고 알려져 있습니다. 다음은 다른 방법입니다. 여기서는 Detector가 있는데 추가적으로 backbone을 붙일 때, ResNet을 붙일 때 Pre-train을 붙일 수도 있고 아니면 랜덤 하게 초기화할 수 있고 이렇게 해서 모델을 만든 다음에 training을 오래 하고 나서 이때 나오는 Fine-tuning 된 weight나 Pre-train weight를 가지고 이렇게 하는 방법도 있습니다. 이게 뭐냐면 도메인에 대한 학습데이터가 많은 경우에는 데이터가 많으면 Pre-train을 쓰지 않아도 Longer training을 하면 충분히 좋은 결과를 얻을 수 있다고 합니다. 이때 여기서 training을 오래 했기 때문에 Pre-train 된 representation를 잊어버리게 된다라고 얘기하고 있습니다. 다음은 이 논문에서 제안하고 있는 것들입니다. 보시면 Pre-train 된 weight를 쓰고 그다음에 Detector를 추가한 다음에 이 아래쪽에 있는 Pre-training은 freezing을 하게 됩니다. 그리고 Fine-tuning 하게 되면 여기는 freezing 됐기 때문에 학습되지 않았을 거고 새로 추가된 Detector만 학습되는 결과를 볼 수 있습니다. 이렇게 되면 여기서 Pre-train 된 representation는 계속 유지가 됩니다. 이 논문이 제안하는 학습방법인데 이러면 아래쪽이 학습이 안 되기 때문에  resource로 학습을 할 수 있으면서도 그다음에 좋은 성능을 달성할 수 있다고 주장하고 있습니다. 여기서는 Faster-RCNN 기준으로 실험을 많이 했습니다. 아래쪽 부분이 ResNet이고 freezing이 되고  추가되는데 Detector 네트워크가 학습된다고 보시면 될 거 같습니다. 간단한 실험입니다.ResNet의 FPN에 미치는 가장 기본적인 RCNN을 보시면 이렇게 freezing을 하게 되면 당연히 성능이 떨어지게 됩니다. 이렇게 FPN 대신에 NAS-FPN이나 Cascade NAS-FPN을 붙이게 되면 freezing을 했음에도 불구하고 성능이 좋아지는 결과를 볼 수 있다고 합니다.그래서 추가된 Detector에 크기와 구조가 성능 향상에 중요한 역할을 한다, 그리고 또 한 가지는 Long-tail 즉, 우리가 자주 볼 수 없는, 레어 한 클래스에서도 좋은 성능을 달성할 수 있다고 얘기하고 있습니다. 실험에 들어가기 전에 몇 가지 좀 필요한 것들이 있어서 정리하고 가겠습니다. FPN이라고 하면 보통 이렇게 backbone 네트워크에서 나온 것들을 Upsampling 해 가면서 어떤 특징을 추출해서 그거 가지고 예측하는 게 FPN입니다. NAS-FPN는 인공지능을 이용해서 Neural architecture search를 이용해서 인공지능이 Neural 네트워크를 구성하도록 했다고 합니다.그래서 이게 좀 더 성능이 좋다고 합니다. Faster R-CNN의 마지막 네트워크입니다. Cascade R-CNN은 이렇게 Detector 네트워크를 여러 개 두어서 multi-stage로 추론해 낼 수 있고, bounding box나 클래스를 추론해 낼 수 있도록 하는 것이 Cascade R-CNN입니다. 그다음에 Pre-train에는 두 가지를 썼습니다. ImageNet은 많이 알려져 있습니다. 1.2M 이미지가 있고 약 천 개 클래스가 있다고 합니다. 그리고 JFT-300M 데이터셋이 있는데 300M 이미지가 있고 그다음에 클래스 개수가 18,000 개그리고 이 JFT-300M는  사람이 직접 한 게 아니고 알고리즘을 통해서 라벨링을 했다고 합니다. 그래서 약 20% 정도 label error가 있습니다. 데이터는 JFT-300M이 많지만 데이터 품질은 ImageNet이 좋다 이렇게 생각하시면 될 거 같습니다.   다음에 Fine-tuning에서 실험할 때 쓴 데이터입니다. MSCOCO는 118K의 이미지가 있고 클래스 개수는 91개 비교적 좀 적은 개수가 있습니다. LVIS 데이터는 100k 개의 이미지가 있고 클래스 개수가 1200개 정도 있고 1200개 중에는 Long tail. 즉, 개수가 아주 적은 레어 한 클래스도 있다고 얘기를 하고 있습니다.    그다음에 augmentation입니다. 어떤 이미지에다가 Copy-Paste 하듯이 이미지를 붙여놓기도 하고 이렇게 neural network이 예측하도록 하는 이런 Copy-Paste augmentation을 썼습니다.  그다음에 Residual Adapter입니다.보통 이게 ResNet의 Residual block인데 W가 있고 Activation function이 있어서 이렇게 residual을 구성하게 됩니다. residual adapter는 여러 가지 task를 지원하기 위해서 이렇게 ResNet에다가 추가적으로 이렇게 adapter block들을 추가했습니다. 그러고 나서 w1, w2는 freeze를 하고 추가되는 네트워크들을 Fine-tuning 하도록 하는 게 residual adapter입니다.          실험 부분 정리해 보겠습니다.72 epoch을 학습했을 경우입니다. FPN은 freeze 했을 때 성능이 떨어지는 걸 볼 수 있습니다.그다음에 NAS-FPN을 썼을 때는 feeze를 하면 성능이 올라가고 있는 걸 볼 수 있습니다.72 epoch 보다는 600 epoch을 학습했을 때는 FPN은 성능이 떨어지고 있는데 NAS-FPN는 성능이 올라오고 있습니다.  올라오고 있는데 72 epoch 보다는 600 epoch이 성능이 더 많이 올라가는 걸 볼 수 있습니다. 그리고 freeze 했기 때문에학습이 일부만 되기 때문에 이게 600 epoch을 하더라도 연산양이 많지 않다 이렇게 생각하시면 좋을 것 같습니다.   그다음에 각각에 대해서 비교했습니다.보시면 FPN, NAS-FPN 그다음에 Cascade, ResNet 키워가면서 비교해 봤는데 freeze 하지 않았을 경우에는 ImageNet이나 JFT-300M이 비슷한 성능을 내고 있는 것을 볼 수 있습니다. 그다음에 freeze 했을 경우에는 JFT-300M을 하는 것들이 훨씬 더 좋은 성능을 내고 있는 걸 볼 수 있습니다. 무슨 말이냐면 JFT-300M을 했을 때가 이미지가 조금 더 분포가 좋진 않지만 많은 데이터를 학습하고 weight를 유지함으로써 좋은 성능을 낼 수 있다 이렇게 볼 수 있습니다.   그다음에 성능에 대한 얘기를 하고 있습니다. 가장 위에 것이 JFT로 Pre-train 하고 600 epoch을 한 경우고,두 번째 경우가 JFT로 Pre-train 하고 72 epoch을 한 경우, 세 번째가 ImageNet을 학습하고 600 epoch을 한 경우, 마지막이 ImageNet으로 Pre-train 하고 72 epoch을 했습니다. Pre-training 학습양이 학습을 얼마나 하느냐보다 더 중요하다고 할 수 있습니다.   좀 더 자세하게 비교해 봤습니다.ImageNet과 JFT을 사용하고 그다음에 짧게 72 epoch을 하고 backbone을 freeze 했을 때 보시면 약간의 성능 차이가 있는 것을 볼 수 있습니다. 그다음에 ImageNet과 JFT로 Pre-train 하고 학습을 오래 하고 freeze를 하지 않으면 두 개가 비슷한 것을 볼 수 있습니다. 그리고 여기서 제안하는 JFT로 Pre-train 하고 학습을 오래 하고 backbone을 freeze 하면 가장 좋은 성능을 내는 것을 볼 수 있습니다.   그다음에 NAS-FPN과 Cascade에 대한 성능을 비교하고 있습니다.freeze 한 것들이 성능이 지속적으로 좋아지는 것 들을 볼 수 있습니다. 그렇지만 NAS-FPN 보다는 Cascade를 추가한 게 가장 좋은 성능향상을 보이고 있는 것을 볼 수 있습니다. 이걸 어떻게 볼 수 있냐면 backbone을 freeze 했기 때문에 추가되는 decoder 네트워크가 좀 더 크고 어떤 정보를 담기에 충분하게 어느 정도 파야지 잘 된다라고 decoder 좀 커져야 잘 된다고 얘기를 하고 있습니다.   그다음에 freeze 하고, 하지 않았을 때 Fine-tuning 했을 때랑 freeze 했을 때 패턴을 볼 수 있습니다.freeze 했을 때는 패턴을 찾기가 어렵습니다. 그런데 freeze 하지 않았을 때는 보시면 NAS-FPN이나 FPN, Cascade NAS-FPN 모두 약간 다양성이 보입니다. 그래서 decoder의 크기 Pre-train의 데이터의 양 epoch 등에서 일정한 패턴을 보이고 있는 것들을 확인할 수가 있습니다.   FPN, NAS-FPN, Cascade NAS-FPN 보시면 기본적으로 FPN보다는 NAS-FPN이 파라미터가 많고 그다음에 NAS-FPN보다는 Cascade NAS-FPN 파라미터가 많은 데 training 되는 것들을 실제로 보시면 26%, 39%, 53% 이런 식의 추가된 네트워크들이 많은 것들이 좀 더 많이 training 되는 걸 볼 수 있습니다.이때 데이터를 보시면 decoder 네트워크가 클 때 성능향상이 되는 것을 알 수 있습니다.   FPN 대신에 Convolution을 한 개 또는 두 개 아주 극단적인 테스트를 해봤습니다. Scratch로 training 했을 때는 오히려 Convolution layer가 커지면서 성능이 감소하는 것들을 볼 수 있습니다. 다음에 Fine-tuning 했을 때 보시면 ImageNet을 학습했을 때나 JFT를 학습했을 때나 Convolution layer가 커지면 성능향상이 있는 걸 볼 수 있습니다. 마찬가지로 freeze 했을 때도 Convolution이 커지면 좀 더 성능향상이 많이 되는 것을 알 수 있습니다.   지금까지 RCNN계열이었고 EfficientNet으로 했을 때 보시면 MSCOCO 하고 LVIS에 대해서 봤습니다. 이때  NAS-FPN을 추가했을 때가 좋은 성능을 보이고 있습니다. freeze 했을 때 성능 향상이 되고 있는데 Copy-Paste augmentation을 쓰고backbone freeze를 하면 더 좋은 성능이 나는 것을 볼 수 있습니다.LVIS도 freeze를 하고 Copy Paste를 썼을 때 가장 좋은 성능을 내는 것을 볼 수 있습니다.   라벨의 특징에 대해서 얘기를 하고 있습니다. 2 stage를 했는데 처음에 그냥 일반적인 training을 하고 다음에 class balance Loss를 줬을 때입니다. Fine-tuning 경우에는 freeze를 하지 않았을 경우에 성능 향상들이 있습니다. Rare, Common, Frequent 한 경우를 볼 수 있는데 Frequent 한 경우는 성능 감소가 있는 것을 볼 수 있습니다.그런데 freeze 했을 경우에는 Rare, Common, Frequent 모두 성능 향상이 있었고 특히 Rare 한 곳에서 성능향상을 주목해 볼 수 있을 겁니다. Rare class가 class 임베디언스에서도 조금 더 잘하고 있다고 얘기하고 있습니다.  그다음에 이미지 사이즈입니다.이미지 사이즈가 small, medium, Large 이렇게 했을 때,  freeze backbone을 했을 때 보시면 Large 쪽에서 성능 항상 있는 것을 볼 수 있습니다. 이미지 특성 중에서는 Large를 좀 더 잘 잡아낸다 이렇게 얘기하고 있습니다.   학습 데이터양에 대해서 얘기를 하고 있습니다.데이터양이 늘어날 때 어떻게 되는지 비교해보고 있는데 ImageNet으로 학습했을 경우, JFT를 학습했을 경우 보시면 FPN의 경우는 backbone을 freeze 한 게 성능이 낮은데 Cascade 보시면 아래쪽에서 조금 더 성능 항상이 있는 것을 볼 수 있습니다.데이터가 적은 경우에 상대적으로 성능 향상이 효과가 뛰어나다고 얘기하고 있습니다. 이건 아무래도 파라미터가 작기 때문에 그런 것으로 추정됩니다. 데이터양에서 비교를 하고 있습니다. Copy Paste를 안 하는 경우와 Copy Paste를 한 경우를 비교해보고 있습니다.Copy Paste를 안 한 경우는 데이터 양이 아무리 작기 때문에 기본적으로 성능 향상이 있고 Copy Paste를 augment를 했을 경우에는 성능향상이 적지만 그래도 성능향상이 있다고 볼 수 있을 것 같습니다.     backbone freeze가 최선의 방법이냐라고 얘기하고 있는데 그렇진 않고 이렇게 freeze를 한 다음에 residual adapter를 추가적으로 더 넣었을 경우에 성능향상이 있다고 얘기하고 있습니다.가장 좋은 경우는 Fine-tuning을 하고 residual adapter를 추가하고 다음에 NAS-FPN + Cascade 이 경우가 가장 좋다고 얘기하고 있습니다. 이거는 논문에서는 결국 엄청나게 많은 파라미터를 학습하기 때문에 좋아진 것이다 이런 식으로 얘기하고 있습니다. "
[논문 리뷰] Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks ,https://blog.naver.com/tnsgh9603/222647265025,20220214,"이 논문은 RPN을 제안합니다.RPN은 어떠한 위치에 물체가 있을지를 예측해주는 네트워크입니다.faster R-CNN은 GPU 기반에서 돌아갈 수 있는 RPN을 제안해서, 당시 2015년도에 object detection 분야에서 큰 성능 향상을 가능하도록 했습니다. 저자는 많은 실험을 진행했고 그 내용과 technic들이 모두 논문 내용에 들어가 있습니다.  <Abstract>​흔히 object detection이라고 하는 것은 각각의 물체가 어느 위치에 존재하고, 그러한 물체가 어떠한 클래스로 분류되는지 함께 알려 주는 작업을 의미합니다.당시 object detection 분야는 많은 사람들이 관심을 가지는 분야였습니다. 당시 SOTA에는 SPPnet, Fast R-CNN가 있었습니다.R-CNN 이후에 Fast R-CNN, SPP가 나오면서 당시 object detection의 속도 향상에 큰 영향을 미쳤습니다.​하지만 이런 Fast R-CNN도 여전히 region proposal 단계에서 많은 시간이 소요된다는 단점이 있습니다.사실 이전의 Fast R-CNN에서는 RP 자체를 별도의 selective search와 같은 알고리즘을 사용해 CPU 기반으로 수행했기 때문에 느렸습니다.그래서 본 논문에서는 기존의 Fast R-CNN의 classifier는 그대로 유지한 채로, RPN(Resion Proposal Network)을 따로 제안해서 이러한 네트워크가 cpu가 아닌 gpu에서 돌아갈 수 있도록 했기 때문에, resion proposal에 드는 시간을 획기적으로 단축했습니다.그래서 기존 Fast R-CNN보다 더 빨라졌기 때문에 이름 앞에 Faster이 붙게 된 것입니다.이때, detection network와 이러한 RPN이 사실상 같은 convolution feature을 공유하도록 만들어서 Resion proposal 자체에 필요한 연산을 사실상 거의 없는 형태로 시간을 획기적으로 단축했습니다.​이런 RPN은 fully-convolutional network로 구성되어 있습니다.RPN은 기본적으로 어떠한 위치에 물체가 존재하는지 여부만 판단합니다.여기다가 기존의 Fast R-CNN에서 사용되었던 detection network의 구조를 활용해서 실제 클래스를 예측할 수 있도록 합니다.실제로 전체 네트워크를 end-to-end로 학습하기 위해 이러한 RPN과 CNN을 하나의 single network로 합칠 수 있도록 합니다.​물론 자율주행 자동차와 같이 더욱 더 높은 fps를 요구하는 애플리케이션에는 이러한 속도가 만족스럽지 않을지 몰라도, 2015년 기준으로는 속도 개선 뿐만 아니라 정확도 향상까지 이루어낸 당시의 화젯거리였습니다. 지금도 수만 회의 인용 횟수를 가지고 있을 정도로 매우 좋은 baseline이 되어주는 논문으로 평가 받고 있습니다.  <Introduction>​최근 object detection 문제에서는 후보 영역 추출과 R-CNN이 큰 성공을 거두었다. R-CNN은 소모가 크지만 후보 영역 간 convolution을 공유하면서 이를 많이 낮추었다. 가장 최근의 Fast R-CNN은 매우 깊은 네트워크를 사용하여 거의 real-time rates를 갖게 되었다. ​그러나 여전히 후보 영역을 추출하는 과정에서 소모를 더이상 줄이지 못하고 있었다. 이 논문에서는 깊은 CNN을 사용하여 후보 영역 생성을 거의 cost-free로 만드는 것을 보여준다. 이를 위해 합성곱 레이어를 공유하는 RPNs를 test 단계에도 도입하여 후보 영역을 계산할 때의 marginal cost를 줄였다.​Fast R-CNN에서 봤던 합성곱 특성 맵도 후보 영역 생성에 사용된다. convolutional feature의 맨 위에 레이어를 한 개 더하여 RPN을 만든다. 그래서 RPN은 fully convolutional network와 같다.​ ​RPN은 (a), (b)같은 pyramids 구조를 사용하지 않고 (c)처럼 여러 개의 크기와 가로세로 비율을 반영하는 anchor 박스를 사용한다. 이는 이미지나 필터를 크기, 비율로 줄세우지 않는 pyramid of regression reference이다. 이 모델은 single-scale 이미지에서도 잘 작동하며 그래서 속도가 빠르다. RPN은 Fast R-CNN에 합쳐져서 후보 영역 작업에 대한 fine-tuning을 object detection에 대한 fine-tuning으로 대체한다.​2. Related Work​Object Proposals​널리 사용되는 object proposal 방식은 grouping super-pixels와 sliding windows에 기반하며, detector와 독립된 외부 모듈로 작동한다.​Deep Networks for Object DetectionR-CNN은 후보 영역이 어떤 클래스(또는 배경)인지 구분하는 classifier로 작동하지 객체의 경계 자체를 예측하지는 않는다. 그래서 R-CNN의 정확도는 후보 영역을 얼마나 잘 생성해내느냐에 달렸다. 이를 위해 몇몇 논문은 깊은 네트워크를 사용할 것을 제안했다. OverFeat의 경우, fully-connected(fc) 레이어를 사용하고, 이를 fully-convolutional 레이어로 바꾸어서 여러 개의 클래스를 구분할 수 있도록 했다. MultiBox의 경우, 마지막 fc 레이어가 여러 개의 박스를 만들어 R-CNN에서 후보 영역으로 사용할 수 있도록 한다.합성곱 계산을 공유하는 것은 모델의 효율성을 높인다. OverFeat의 경우, image pyramid에서 합성곱 특성을 계산한다. SPP의 경우, adaptively-sized pooling에서 합성곱 특성맵을 공유하여 효율을 높였다. Fast R-CNN의 경우, detector 간 training에서 합성곱 특성을 공유하였으며, 이는 더 나은 정확도와 속도를 보여주었다.  3. Faster R-CNN​ ​Faster R-CNN은 1) 영역 후보를 제안하는 deep fully convolutional network와 2) 후보 영역을 사용하는 Fast R-CNN detector 두 부분으로 이루어진 object detection network이다. 어텐션 매커니즘은 RPN 모듈이 Fast R-CNN에게 어디를 봐야할 지 알려주는 역할을 한다.3-1. Region Proposal Networks(RPNs)RPN은 이미지를 후보 영역의 input과 output로 사용한다. 이 프로세스는 fully convolutional network로 구현되는데 이후에 Fast R-CNN과 연산을 공유해야 하기 때문이다. 두 네트워크는 같은 합성곱 레이어를 공유해야 한다. 이를 위해 5개의 공유 가능한 합성곱 레이어를 사용하는 ZF와 13개의 공유 가능한 레이어를 사용하는 VGG16을 참고하였다. 후보 영역을 만들 때, 합성곱 특성맵을 n×n window로 sliding 시킨다. (n=3으로 사용했다.) 각 sliding window는 낮은 차원의 특성과 매핑된다. 이 특성은 box-regression 레이어(reg)와 box-classification 레이어(cls)에 적용된다. sliding window에서 네트워크가 작동하기 때문에 fully-connected 레이어(reg, cls)들은 모든 위치에서 공유된다. 이는 n×n 합성곱 레이어에 두개의 1×1 합성곱 레이어(reg, cls)가 따르는 구조로 구현된다. ​ranslation-Invariant AnchorsFaster R-CNN은 이미지에서 객체를 찾아낼 때, 객체가 어느 위치에 있어도 영역과 함수는 항상 같은 결과를 보장한다. 이 특성은 모델의 사이즈를 줄여주며(9개의 anchor 박스가 있을 경우, (4+2)*9-dimension의 fully-connected 레이어가 필요하다.), 과적합 될 위험성도 더 적다.​Multi-Scale Anchors as Regression ReferencesFigure 1에서 볼 수 있듯, multi-scale prediction에는 두가지 방법이 있다. 첫번째는 image/feature pyramid에 기반한 방법으로 각 scale마다 연산되기 때문에 시간이 오래 걸린다. 두번째는 feature map에서 여러 개의 scale과 가로세로 비율을 가진 sliding window를 사용하는 것이다. 예를 들어, Figure 1의 (b)처럼 다른 사이즈를 가진 필터(pyramid of filters)를 적용한다. 두 방법은 주로 같이 사용된다.Faster R-CNN은 pyramid of anchors를 사용하며, 이는 더 효율적이다. 이 방법은 여러 종류의 anchor 박스를 참조하여 바운딩 박스를 classify하고 regress한다. 이렇게 anchor 기반의 multi-scale 구조 덕분에 유일한 사이즈의 이미지에서 연산된 특성을 사용할 수 있으며, 사이즈를 더 계산할 필요 없이 특성을 공유할 수 있다.​ ​3-2. Sharing Features for RPN and Fast R-CNN합성곱 레이어를 공유하면서 RPN과 Fast R-CNN을 학습시키는 것에는 3가지 방법이 있다.​1. Alternating training : 처음에는 RPN을 학습시키고, 여기서 나온 후보 영역들로 Fast R-CNN을 학습시킨다. 네트워크는 Fast R-CNN에 의해 tune되고, 이는 RPN의 초기값을 생성할 때 사용된다. 이 과정은 반복된다. 이 논문에서 나오는 모든 실험은 이 방법으로 수행되었다.​2. Approximate joint training : RPN와 Fast R-CNN을 한 네트워크로 합성한다. 각 SGD의 순전파 과정에서는 후보 영역이 만들어지고 이들이 Fast R-CNN 학습에 사용된다. 역전파 과정에서는 RPN의 loss와 Fast R-CNN의 loss가 합쳐져서 반영된다. 이 방법은 구현하기에는 쉬우나 후보 영역의 좌표를 역전파 과정에서 무시한다는 단점이 있다. (그래서 approximate이라는데 그 의미는 잘 모르겠음.)​3. Non-approximate joint training : Fast R-CNN 내의 RoI pooling 레이어는 convolutional feature뿐만 아니라 RPN에서 예측된 바운딩 박스도 input으로 받는다. 그래서 역전파 과정에서 박스의 좌표에 대한 gradient도 반영된다. 이를 위해서는 RoI warping 레이어가 필요한데 이는 논문의 내용에서 벗어난 내용이므로 다루지 않았다.​4-Step Alternating Training​alternating optimization을 적용하기 위해 4개의 스텝으로 학습이 진행되었다.1. 3-1-3에서 묘사된 것처럼 RPN이 학습된다. initialization은 미리 학습된 ImageNet 모델을 사용하고 fine-tuning된다.2. step 1에서 RPN에 의해 생성된 후보 영역들로 Fast R-CNN이 학습된다. 마찬가지로 initialization은 미리 학습된 ImageNet 모델을 사용한다. 여기까지는 아직 두 네트워크가 합성곱 레이어를 공유하지 않는다.3. RPN을 초기화할 때, detector 네트워크(Fast R-CNN)를 사용한다. 그러나 이때 공유된 합성곱 레이어는 그대로 두고, RPN에게 unique한 레이어만 fine-tuning한다.4. 마찬가지로 공유된 합성곱 레이어는 그대로 두고, Fast R-CNN에게 unique한 레이어만 fine-tuning한다.​3-3. Implementation Details이미지의 가로와 세로 중 더 작은 쪽을 600 pixels로 맞추어 re-scale한다. 이 re-scale된 이미지를 적용한 네트워크는 마지막 합성곱 레이어에서 16 pixels의 stride를 갖는다. 작은 stride를 사용하면 훨씬 더 좋은 결과가 나올 수 있으나 큰 stride도 좋은 결과를 보여주었다. training 단계에서 cross-boundary 영역은 무시되고 loss에 반영되지 않는다. 그러나 testing 단계에서는 모든 이미지에 fully convolutional RPN이 적용되기 때문에 cross-boundary 영역도 생성될 수 있다.RPN이 생성하는 후보 영역은 서로 너무 많이 겹치는 경우가 있어서 이를 줄이기 위해 non-maximum suppression(NMS)를 적용한다. 우선 cls score로 정렬한 후, IoU overlap이 0.7을 넘어가는 후보 영역은 제거하였다. NMS는 detection 정확도에는 영향을 끼치지 않으면서도 후보 영역의 개수를 크게 줄였다.  4. Experiments​4-1. Experiments on PASCAL VOC 2-4행: 후보 영역을 추출하는 방법에 따라 성능을 비교하였다. Fast R-CNN 기반에서 사용된 Selective Search(SS), EdgeBoxes(EB)보다 RPN+Fast R-CNN이 합성곱 레이어를 공유한 덕분에 더 좋은 성능(mAP 59.9%)을 보였다.(mAP : Min Average Precision)​Ablation Experiments on RPN6행: 4-step에서 2번째까지만 실행하고 멈추어서 합성곱 레이어를 공유하지 않았다. 그 결과 조금 낮은 성능(mAP 58.7%)을 보였다. 즉, detector(Fast R-CNN)에 의해 tune된 feature를 RPN의 fine-tuning에 사용하는 3번텝 스텝이 성능을 높인다고 할 수 있다.7-9행: test단계에서 SS 대신 RPN을 사용하여 test단계에서 RPN의 영향력을 확인하였다. training단계와 test단계에서 후보 영역 간 차이가 발생하여 mAP가 조금 낮아지긴 하였으나 비교적 좋은 성능을 보였다.10행: NMS를 사용하지 않았더니 성능이 내려갔다. NMS가 detection에 해를 끼치지도 않을 뿐더러 오히려 에러를 줄여준다고 할 수 있다.11-13행: cls 레이어는 가장 좋은 축에 속하는 후보 영역들의 정확도에 큰 영향을 끼친다.14-15행: reg 레이어는 후보 영역이 잘 추출되는 것에 큰 영향을 끼친다.16행: RPN을 VGG와 같이 사용한 것이 ZF를 같이 사용했을 때보다 더 좋은 성능을 보였다. 그래서 RPN+VGG가 SS보다 더 좋다는 가설을 세울 수 있다.​Performance of VGG-16 Table 3는 VGG-16을 proposal과 detection에 모두 사용했을 때의 결과이다. 모두 SS를 사용했을 때보다 더 좋은 성능을 보여주고 있다. 아래 Figure 5는 PASCAL VOC 2007에 Faster R-CNN을 적용한 결과이다. Faster R-CNN을 사용했을 때 object detecion이 수행되는 결과Table 5는 모델이 돌아가는 데에 걸리는 시간을 보여준다.​Sensitivities to Hyper-parameters Table 8은 anchor 박스의 개수에 따른 모델의 성능을 보여준다. 1개의 anchor를 사용하는 것보다는 여러 개의 anchor를 사용하는 것이 더 좋으며, 사이즈와 가로세로 비율이 크게 관련있는 특성은 아니지만 모델을 더 유연하게 만들기 위해 3개의 사이즈와 3개의 비율을 섞어서 사용한다. Table 9은 lambda의 값에 따른 모델의 성능을 보여준다. lambda의 값이 달라져도 mAP가 1% 내외로 변화하는 것을 보아 모델의 성능은 lambda와 큰 관련이 있지 않음을 알 수 있다.​Analysis of Recall-to-IoU Figure 4는 IoU 임계값에 따른 SS, EB, RPN의 recall of proposal을 보여준다. RPN은 적은 proposal에서 SS와 EB에 비해 높은 recall을 보이며, Faster R-CNN이 proposal이 적어도 좋은 detection 성능을 보인다는 것을 알 수 있다. 앞서 언급했듯이 이는 cls term의 덕분이다.​One-Stage Detection vs. Two-Stage Proposal+Detection region proposal과 object detection이 서로 종속되어 있는 Two-Stage 모델이 One-Stage 모델보다 더 좋은 성능을 보이고 있다. 또한 One-Stage 모델은 처리해야 할 후보 영역이 더 많아서 시간이 오래 걸린다.​Experiments on MS COCOMicrosoft COCO(MS COCO)는 다른 object detection dataset이다. 이 dataset에서도 Faster R-CNN의 성능을 검증해보았다. Table 11에 MS COCO로 검증한 모델의 성능이 나와있다. Fast R-CNN보다 Faster R-CNN이 더 좋은 성능을 보이고 있으며, 이를 통해 RPN이 localization 정확도를 높이는 일에 큰 효과를 발휘하고 있다는 것을 알 수 있다. 아래 Figure 6는 MS COCO의 test-dev 데이터에 Faster R-CNN을 적용한 결과이다. Faster R-CNN in ILSVRC & COCO 2015 competitionsVGG-16을 ResNet-101로 바꾼 모델은 이전보다 더 좋은 성능을 보였다. 이 모델은 ILSVRC 2915 localization과 COCO 2015 segmentation에서 우승했다.​4-3. From MS COCO to PASCAL VOC COCO dataset으로 훈련된 모델을 PASCAL VOC 2007에 적용했을 때(fine-tuning 없이), 76.1%의 mAP를 보였다. 이는 PASCAL VOC dataset은 전혀 사용하지 않았음에도 불구하고 VOC로만 훈련된 모델보다 오히려 더 좋은 성능을 보였다는 것을 알 수 있다. VOC dataset으로 fine-tuning한 것은 더 높아진 성능(mAP 78.8%)을 보였다.  5. Conclusion​이 논문에서는 합성곱 레이어를 공유하기 위해 RPN이라는 효율적인 후보 영역 생성 방법을 제시하였다. 이 덕분에 완전히 하나의 딥러닝 기반의 object detection 모델을 만들 수 있었고, RPN은 region proposal, 나아가 전체적인 object detection의 정확도를 높였다.  의문점1. metric의 정확한 정의가 무엇일까? 단순한 성능지표?2. 합성곱 계산을 공유한다는게 무슨 의미일까?3. cross boundary란? decision boundary와 차이는 뭘까?  Reference​https://blog.naver.com/eunaii9/222468137541 Faster R-CNN 논문 리뷰 :: Towards Real-Time Object Detection with Region Proposal Networkshttps://erratic-tv-151.notion.site/Faster-R-CNN-4d8c2deaae384519ad9c2d3666b91af0blog.naver.com https://www.youtube.com/watch?v=46SjJbUcO-c&t=909s ​ "
"(YOLO v1 논문 리뷰) You Only Look Once Unified, Real-Time Object Detection(2016 CVPR) 리뷰 ",https://blog.naver.com/syon991203/222893521590,20221006,"You Only Look Once Unified(YOLO v1)Unified, Real-Time Object Detection(2016 CVPR) 리뷰​​너무 오랜만에 공부 포스팅오늘 발표했던거,,,​​ You Only Look Once: Unified, Real-Time Object DetectionWe present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bound...arxiv.org 논문 링크​  ​ ppt 출처 : https://m.blog.naver.com/seiru523/221379747026 먼저 Object detection에는 크게 두 가지 방법이 있는데, localization과 classification 을 순차적으로 진행하는지, 병렬적으로 진행하는지에 따라 2 stage와1 stage로 나뉩니다.​ 2 stage의 경우에는 첫 번째 사진과 같이 후보 오브젝트의 위치를 제안한 다음 클래스를 예측하는 것이 순차적으로 진행되는데, 본 논문에서는 r-cnn이나dpm 등 sliding window, selective search 기반 모델을 소개합니다. 정확도는 비교적 높지만 속도 면에서 느린 편! 1 stage는 위와는 달리 region proposal과 classification을 동시에 단일 네트워크만 거쳐 수행, 본 논문에서 다루는 욜로의 경우입니다. 위와 반대로 비교적 빠르지만 정확도는 2 stage detector에 비해 낮습니다.​​​ 이 부분은 설명이 정확하지는 않지만, 어느 정도 이해를 위해 남겨두었다,,댓글 환영해요​​dpm과 R-CNN에서 사용된 방식을 살펴보면, dpm의 경우에는 sliding window를 사용해서 전체 이미지를 일정 크기 영역으로 나눠서 각각의 영역에 대해 Localization network를 적용합니다. 이 방법을 window를 이동시키며 전체 이미지에 대해 탐색합니다. 이 방법을 다양한 크기와 비율의 window를 사용해 반복하기 때문에 효율성이 떨어집니다. ​​R-CNN에서 사용한 selective search의 경우에는 위 sliding window의 비효율성을 어느정도 개선한 방법인데, 비슷한 질감이나 색 등 특성을 갖는 인접 픽셀로 물체가 있을 법한 곳에 segmentation을 하고, 왼쪽 하단 그림처럼 작은 area를 여러개 생성합니다. 그리고 이를 통합하면서 region proposal을 하게됩니다. 이렇게 생성된 2000개의 영역을 cnn 모델에 각각 넣기 때문에 속도가 느립니다.​두 방법 모두 파이프라인이 복잡해서 학습, 예측이 느려집니다.​​​ yolo의 특징을 살펴보자면 이름처럼 이미지 전체를 한 번만 보는 1 stage detector 방식입니다.따라서 single convolutional network가 bounding box를 예측하면서 class probability를 계산합니다. 그리고 이미지 전체의 특성을 학습하여 검출 성능 최적화한다는 장점이 있습니다. ​​2 stage detector에서 한계점으로 언급되었던 속도같은 경우엔 Object detection을 regression problem으로 관점을 전환해서 end-to-end 구조즉 한번에 처리하는 구조로 복잡한 파이프라인 없이 YOLO neural network에 넣어 detection을 해서 개선합니다. 앞선 2 stage detector의 R-CNN: 6 fps, DPM(30Hz): 30 fps 였던 것에 비해서 yolo의 test time은 45 fps, 레이어를 줄인 Fast YOLO는 155 fps로 속도가 훨씬 빠릅니다.​ 그리고 학습 과정에서 이미지 전체를 보고 예측하므로 클래스 주변 정보 즉 맥락도 학습하여 background error가 감소한다는 점을 보여주기도 합니다.​ 그리고 물체의 일반적인 특징을 학습해서 다른 도메인에서도 object detection 성능 우수하다는 점을 실험을 통해 보여줍니다.(피피티 자료 후반부)​ ​yolo의 구조에 대해 더 자세히 살펴보자면, 이미지 전체에서 얻은 feature을 통한 prediction을 생성하고, 이를 바탕으로 bbox를 예측합니다. 그리고 bbox의 모든 클래스에 대한 classification을 하나의 convolutional network를 통해 병렬적으로 실행하는 unified detection을 구현하고 있습니다. ​​SxS 개의 grid cell로 분할하여 물체의 중심점이 있는 cell이 detection에 책임이 있고각 그리드 셀은 b개의 바운딩 박스, 이에 대한 confidence score각 그리드 셀은 c개의 class probability를 가지게 됩니다. ​이런 prediction은 아래와 같은 텐서 형태를 띄게 됩니다.​​ 이러한 detection 과정을 살펴보자면, 실제 yolo는 7x7이지만 예시 이미지로 간략화된 3x3인 이미지입니다.. 먼저 yolo는 이미지를 448*448로 resizing하고 위에서 설정한 값대로 grid 분할을 하게 됩니다. 그리고 각 셀마다 bounding box를 2개씩 예측을 하게 됩니다. ​​그리고 각 bounding box는 중심좌표, 박스의 크기, confidence 점수를 포함하고 있습니다. 이때 중심좌표 위치랑 박스의 높이 너비는 input image와의 크기로 비례해서 0과 1 사이의 값을 가지도록 normalize됩니다. confidence score 또한 물체가 있나, 없나로 구분하는 앞부분의 probability는 0과 1이 되고, 물체가 없으면 0, 물체가 있다면 IoU와 곱해져서 IoU가 됩니다. ​​Probability 같은 경우엔 물체가 bounding box에 있을 때, grid cell의 object가 특정 클래스에 속할 확률을 각 클래스마다 계산합니다. 이 과정에서 주의해야 할 점은 bounding box의 개수와는 상관없이 각 셀이 하나의 오브젝트만 탐지할 수 있게 되어 있다는 점입니다.​​그리고 윗 값을 output tensor 형태로 보면 각 셀의 두 개의 바운딩 박스에 대해 있기 때문에 오른쪽 상단처럼 bounding box의 갯수(여기선 2개) * 5가 되고, 클래스의 갯수가 20개여서 그에 따라 + 20, 이를 셀의 갯수(여기서는 3x3) 만큼 반복하기 때문에 총 3x3x30 dimension의 아웃풋 tensor 형태를 띄게 됩니다. ​​이를 NMS를 통해 최종 bounding box를 선정하게 됩니다.​ ​ ​YOLO의 경우에는 pascal voc를 사용해서 20개의 클래스인 데이터셋이고, 각 이미지를 7x7 그리드로 나누기 때문에 7x7x30 형태의 최종 텐서 형태를 띄게 됩니다. ​​ ​여기서 하나 더 눈여겨볼 점은, prediction의 경우 위처럼 20개의 클래스, 바운딩 박스(2개)당의 정보 5개 총 10개의 정보를 가지고 있어 SxSx30이지만, 학습 단계에서 사용하는 타겟의 경우에는 둘 중 더 높은 확률의 바운딩 박스만 사용하므로 SxSx25 형태가 됩니다.​​ ​이제 네트워크 구조를 자세히 보면 yolo는 GoogLeNet의 구조에서 영감을 받았고, Darknet 프레임워크를 사용해서 앞의 20개의 conv 레이어로 1000-class Imagenet 데이터셋으로 트레이닝을 하고, 나머지 4개의 conv 레이어와 2개의 fc레이어에서 detection을 수행하도록 변경합니다. ​그리고 1x1 reduction layer을 사용해서 연산량을 감소시킵니다.​여기서 인풋 이미지의 크기를 448로 변경하는 이유는 앞선 이미지넷은 분류를 위한 데이터셋이기 때문에 높은 해상도를 요구하지 않지만, detection에서는 더 높은 해상도가 필요한 경우가 있기 때문에 증가시켰다고 설명하고 있습니다. ​최종 아웃풋의 경우에는 클래스 확률 값과 bbox의 위치정보를 담고 있게 됩니다.​​ ​활성함수같은 경우에는 마지막 계층을 제외하고는 좌측의 leaky ReLU를 사용합니다. 마지막 계층은 선형함수를 사용합니다.​​​ ​이제 loss function과 트레이닝 과정에 대해 살펴보겠습니다.​먼저 오른쪽 맨 처음 스칼라값은 cell i의 오브젝트에 responsible한 j번째 박스를 표시하여 loss function에 반영합니다. 이 때 앞서서 말씀드린 것처럼 두개의 bbox중에 하나의 ground truth와의 가장 높은 IoU 값을 가진 박스가 j번째 박스가 됩니다. 나머지 한 박스는 학습 시 전파가 되지 않게 됩니다. ​두 번째 스칼라값의 경우에는 cell i 에 object의 존재 여부를 기준으로 0과 1을 나누게 됩니다. 여기서 loss function의 경우에는 sum squared error을 사용하는데, 차가 클수록 에러값이 커져서 optimize하기 쉽기 때문입니다.아까 저희가 예측한 x y w h 와 클래스 확률, confidence score의 값을 예측값을 뺀 다음 제곱하여 계산합니다.  번호 표시한 순으로 첫 번째와 두번째는 중심점의 좌표 위치와 높이 너비를 ground truth와 비교하여 오차를 구하고, 3,4 번째 는 모든 그리드에서 예측한 confidence score, 마지막은 class probability 를 계산하게 됩니다.  ​​앞서 sum squared error를 사용한다고 했는데, 이 때 이 방법이 욜로의 목표와 완전히 일치하지 않는다고 논문에서 언급합니다. 동일한 가중치로 모든 바운딩 박스의 요소를 계산하기 때문입니다. 예를 들어 로컬라이제이션 에러와 클래시피케이션 에러를 동일한 비중으로 본다거나, 그리드 셀에서 이미지가 없는 경우입니다. ​따라서 이미지의 대부분에는 오브젝트가 없는데, 이 때 컨피던스 스코어가 대부분 0으로 학습되는 문제를 만듭니다. 람다 coord를 통해 좌표값에 대한 가중치를 증가시키고, 객체가 없는 confidence loss에 대해서는 가중치를 작게 만들어서 이러한 문제를 해결하고자 했습니다. 1,2번째 줄에서 람다 coord는 5, 4번째 줄의 람다 noobj는 0.5로 설정하였습니다.  그리고 2번째 라인은 square root를 취해주고 그 다음 제곱을 하는 것을 확인할 수 있는데, 이 때는 큰 바운딩 박스와 작은 바운딩 박스를 동일한 가중치로 계산하는 것을 보완하려는 부분입니다. 작은 바운딩 박스일수록 위치 변화에 따라 객체를 벗어날 가능성이 큰데, 이를 개선하기 위해 너비와 높이에 square root를 취해주어서 증가율을 감소시키는 형태로 보완합니다.  여기에 이제 dropout이나 augmentation을 적용해서 오버피팅을 방지합니다.​​ ​추론 단계에서는 YOLO가 한 이미지당 총 98개의 bbox를 예측하고, 각 박스마다 클래스 확률을 구합니다. 여기서 nms를 적용해서 하나의 오브젝트에 대해 여러 셀이 검출하는 문제를 방지하고, 이를 통해 map를 2~3%정도 향상시켰다고 말합니다. 다른 모델의 앙상블의 경우에는 미미한 차이였어서 이 두 모델간의 앙상블이 유의미하다고 합니다.​​ ​이 부분은 mAP 코드리뷰때 부족한 부분을 추가한거라 mAP에 대한 설명은 계산 방법에 대해서만 아주 간략하게 들어있습니다.​mAP는 object detector의 정확도 평가 지표로 모델이 예측한 것 중 정답과 실제 정답인(?) 것의 비율인 precision과, 전체 정답 중 모델이 실제로 맞은 것의 비율인 recall을 가지고 평가를 하게 되는데, confidence score을 기준으로 내림차순으로 정렬후 이에 따른 precision과 recall을 계산합니다. ​​precision 같은 경우에는 누적 tp/모델의 디텍션 이 되어 분모가 바뀌고, recall의 경우에는 모델이 예측한 것 중 실제로 맞은 것/전체 정답 이기 때문에 분모가 fn과 tp의 합인 실제 정답 전체이므로 변화가 없습니다. ​이를 그래프 안쪽 넓이를 구해 AP 를 계산합니다.​ ​이러한 과정을 전체 클래스에 걸쳐서 한 후 평균을 낸 값이 map가 됩니다.​그리고 보통은 성능 평가 시 하나의 threshold로만 하지 않고 iou threshold 값을 조금씩 바꾸어가며 여과되는 tp와 fp를 재설정해서 성능 결과를 제시합니다.​​ ​다시 yolo로 돌아와서 실험 결과입니다. ​​첫 번째 실험부터 보면 다른 real time detector에 비해 정확도가 높고, 속도도 더 빠르다는 점을 확인할 수 있고, R-CNN 계열이 성능은 좋지만 속도 면에서는 less than real time이기 때문에 real time 중 욜로가 가장 우수하다는 점을 확인할 수 있습니다. 여기서 참고로 R-CNN에 비해 작은 오브젝트나 특정 카테고리에 대해 낮은 성능을 보입니다.  그리고 배경에 아무것도 없는데 물체가 있다고 판단하는 백그라운드 에러가 감소되었다는 점에서 False positive가 감소했다는 점을 알 수 있습니다. ​​그리고 욜로가 R-CNN에 비해 백그라운드 에러가 적다는 점에서 착안하여 Fast R-CNN과 욜로의 결합을 했을 때 71.8%에서 75%로 map가 3.2% 향상되었는데, 다른 모델과의 앙상블 시엔 이만큼 눈에 띄는 향상이 없었다는 점에서 두 모델의 앙상블이 유의미한 결과를 가져왔음을 확인할 수 있습니다. 속도 또한 Fast R-CNN 단독사용과 앙상블 모델 사용 시 큰차이가 없다는 점에서 함께 사용하는 것이  유리합니다.​​ ​그리고 욜로를 실제 사진인 PASCAL voc 데이터셋으로 트레이닝하고 예술작품 데이터셋인 피카소로 테스트해본 결과 욜로가 다른 모델에 비해 가장 높은 정확도를 보였고, 다른 모델은 정확도가 현저히 떨어졌다는 점에서 트레이닝 중에 접하지 못한 이미지 또한 어느정도 잘 검출하고 있음을 확인할 수 있습니다.​​ ​그렇지만 욜로의 한계점도 있는데, 먼저 작은 물체에 대해 검출이 부정확하다는 점과, 하나의 셀이 하나의 물체만 검출하기 때문에 여러 개의 물체가 겹쳐져있는 경우 동시에 검출이 어렵다는 점, 그리고 새로운 비율의 물체에 대한 검출이 어렵다는 점이 있습니다. ​또한 bbox의 크기와 상관없이 loss에 동일한 가중치를 두기 때문에(square root를 씌워 이 점을 보완하기는 하지만) 이 점 또한 한계점으로 지적하고 있습니다.​​  ​fast R-CNN은 따로 공부가 필요하다. 2 stage Detector의 경우 localization 후 이를 기준으로 얻은 박스를 기준으로 한 번 더 localization과 다른 작업을 수행한다고 한다. 이 점이 들어가 있지 않기 때문에 따로 공부해야겠다.​​다음엔 nms,map 코드리뷰로 돌아오겠습니다 ...​​​참고 자료​ https://visionhong.tistory.com/12https://bkshin.tistory.com/entry/논문-리뷰-YOLOYou-Only-Look-Once​ "
[yolov5] 'YOLOv5' 모델로 'Object Detection Inference' 해보기! ,https://blog.naver.com/bsh1004664/222420862788,20210705,"google Colab 환경에서'YOLOv5' 모델로'Object Detection Inference(객체 인식 추론)'  해보기! 위의 영상은 Colab 환경에서 YOLOv5 모델 예제 코드를 활용하여 Object Detection Inference를 해 본 결과입니다.요즘 매우 핫한 실시간 물체 인식 기술!YOLOv5의 사용법과 Object Detection Inference 예제 코드를 다뤄보도록 하겠습니다. ˙ᵕ˙​ YOLOv5가 뭐야?YOLO는 'You Only Look Once'의 약자로,딥러닝 기법을 활용, 매우 빠른 속도로 이미지상에 존재하는 물체의 종류와 2D 좌표를 획득하여 '학습한 물체의 종류와 위치를 실시간으로 파악할 수 있는 Real-Time-Object-Detection-Model'입니다. YOLOv5 모델의 Object Detection Inferece 예제 코드google Colab 환경에서 진행해보도록 하겠습니다. # Mount Google Drivefrom google.colab import drivedrive.mount('/content/drive') # CUDA 버전 및 GPU 상태에 모니터링!nvidia-smi 먼저 google Colab에서 새 파일을 만들어주시고! 드라이브와 연동시켜줍니다. # Clone YOLOv5 GitHub repository%cd /content!git clone https://github.com/ultralytics/yolov5.git 그다음 YOLOv5 모델을 GitHub에서 Clone 해서 받아옵니다.(참고: GitHub - ultralytics/yolov5: YOLOv5 in PyTorch > ONNX > CoreML > TFLite) # YOLOv5를 위한 패키지 설치%cd /content/yolov5/! pip install -r requirements.txt YOLOv5 모델을 사용하기 위한 패키지를 설치하고, # Upload the testing video file 예제from google.colab import files uploaded = files.upload() 위의 코드를 실행하여 test 하고자 하는 영상을 구글 드라이브에 업로드해줍니다.저는 현대자동차그룹 브랜드 필름 영상을 활용했습니다. ˙ᵕ˙ # YOLOv5 모델을 이용하여 Object Detection Inferece 해보기!python detect.py --source /content/yolov5/YOLOv5_test.mp4 # 각자 구글 드라이브에 있는 파일 경로 입력 마지막으로 '!python detect.py --각자의 파일 경로 입력' 코드를 실행하여 YOLOv5 모델의 Object Detection Inferece를 해보겠습니다.각자의 test 파일 경로는 오른쪽 마우스 클릭 후 경로 복사를 통해 확인하실 수 있습니다.​물체 인식 결과 파일은 드라이브에서 확인 가능하며,Colab 왼쪽의 파일 마크에서 해당 파일을 더블 클릭하여 직접 다운로드하거나,  # Object Detection Inference 결과 파일 다운받기from google.colab import filesfiles.download('/content/yolov5/runs/detect/exp2') # 각자의 test 결과 파일 경로 입력 위의 코드를 실행하여 저장할 수 있습니다. YOLOv5 모델의 Object Detection Inferece 결과YOLOv5 모델의 Object Detection Inferece 결과입니다.자동차, 사람 등 다양한 물체를 매우 빠른 속도로 인식하는 것을 볼 수 있습니다.  다음 포스팅에서는 이미 training 되어 있는 Object 뿐만이 아닌,Object Detection Custom Dataset을 만드는 방법에 대해 다뤄보겠습니다! ˙ᵕ˙ "
[LiDAR] 객체 인식(Object Detection) 관련 링크 ,https://blog.naver.com/1rladbdus/222974804407,20230104,"객체 인식 코드목적: 객체인식 + 트래킹 / 알고리즘: classifier: pointNet, Resnet, Vgg (코드에 3개가 다 있는데,  코드 분석을 안해봐서 정확히 어떤 모델 썼는지 모르겠다)clustering, segmentation, detecting, tracking: 이름 적혀있지 않음, 코드 분석을 안해봐서 정확히 어떤 모델 썼는지 모르겠다언어: C++, Pythonhttps://github.com/jingjing12110/object_detection_tracking GitHub - jingjing12110/object_detection_tracking: object detection and tracking (velodyne LiDAR)object detection and tracking (velodyne LiDAR). Contribute to jingjing12110/object_detection_tracking development by creating an account on GitHub.github.com ​알고리즘: Yolo v4 / 언어: matlabhttps://github.com/matlab-deep-learning/Lidar-object-detection-using-complex-yolov4 GitHub - matlab-deep-learning/Lidar-object-detection-using-complex-yolov4: Object detection and transfer learning on point clouds using pretrained Complex-YOLOv4 models in MATLABObject detection and transfer learning on point clouds using pretrained Complex-YOLOv4 models in MATLAB - GitHub - matlab-deep-learning/Lidar-object-detection-using-complex-yolov4: Object detection...github.com ​ 관련 코드목적: LiDAR Segmentation / 알고리즘: SalsaNet / 언어: matlabhttps://github.com/matlab-deep-learning/pretrained-salsanext GitHub - matlab-deep-learning/pretrained-salsanext: Semantic segmentation and transfer learning using pretrained SalsaNext model in MATLABSemantic segmentation and transfer learning using pretrained SalsaNext model in MATLAB - GitHub - matlab-deep-learning/pretrained-salsanext: Semantic segmentation and transfer learning using pretra...github.com 목적: RVix 바운딩 박스 출력 / 언어: c++ / docker 가능https://github.com/Earthwings/annotate GitHub - Earthwings/annotate: Create 3D labelled bounding boxes in RVizCreate 3D labelled bounding boxes in RViz. Contribute to Earthwings/annotate development by creating an account on GitHub.github.com ​​ "
[study] object detection 관련 site 정리 (...ing) ,https://blog.naver.com/sisomimoctrl/222791996780,20220628,"PR12 다시보기​Season 1 www.youtube.com/watch?v=auKdde7Anr8&list=PLWKf9beHi3Tg50UoyTe6rIm20sVQOH1brSeason 2 www.youtube.com/watch?v=FfBp6xJqZVA&list=PLWKf9beHi3TgstcIn8K6dI_85_ppAxzB8Season 3 www.youtube.com/watch?v=D-baIgejA4M&list=PL_skMddDjnzq1wDI3t2cH9hlK6wBBapeASeason 4 www.youtube.com/watch?v=az-OV47oKvA&list=PL0o99tZwBlrMV3QsZ4O79KjMHDhAJpAdW  R-CNN, Fast R-CNN, Faster R-CNN Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks​동빈나, www.youtube.com/watch?v=jqNCdjOB15s&list=PLRx0vPvlEmdADpce8aoBhNnDaaHQN1Typ&index=25동빈나, www.youtube.com/watch?v=46SjJbUcO-c&list=PLRx0vPvlEmdADpce8aoBhNnDaaHQN1Typ&index=26  YOLO You only look once: Unified, real-time object detection​김영득영상 : www.youtube.com/watch?v=84JM_q8zqh8&t=382s​Taegyun Jeon영상 : www.youtube.com/watch?v=eTDcoeqj1_wslide : www.slideshare.net/TaegyunJeon1/pr12-you-only-look-once-yolo-unified-realtime-object-detection  SSD SSD: Single Shot MultiBox Detector​JinWon Lee영상 : www.youtube.com/watch?v=ej1ISEoAK5g&list=PLqAFpvtCnrySi60YxMXf45YAyY9X24hLO&index=8slide : www.slideshare.net/JinwonLee9/pr132-ssd-single-shot-multibox-detector ​ "
Image Classification & Object Detection & Object Tracking ,https://blog.naver.com/qhruddl51/222430219604,20210713,"Image Classification영상 처리에서 이미지를 보고 그게 뭔지 맞추는 문제이다. 즉, output 이 한 가지이다. 예를 들어 아래와 같은 사진이 있다면 ""다람쥐""가 output이 되는 것이다.  ​​​​Localization & Classification한 이미지에 하나의 물체가 있을 때, 그 물체가 무엇인지와 어디에 있는지를 맞추는 문제이다. 위치는 4개 좌표를 가지는  Bounding box로 표현한다.   츨처 : https://medium.com/analytics-vidhya/object-localization-using-keras-d78d6810d0be​​Object Detection한 이미지에 여러 물체가 있다면 어떻게 될까? 그렇다면 output 또한 여러 개일 것이다. 물론 대상의 위치도 맞춰야 할 것이다. 이것이 바로 Object Detection이다. Object Detection이 Classification&Localization과 다른 점은 물체가 몇 개 있을 지 모른다는 것이다. 즉, 몇 개의 output 값을 내야 할지 알 수 없다. Object Detection 에는 One-shot-detection과 Two-shot-detection 방식이 있다. Two-shot-detection은 정확도가 높은 대신 속도가 느리고,  One-shot-detection은 속도가 빠른 대신 정확도가 상대적으로 낮다. - Two-shot-detection(EX : R-CNN) : 두 단계로 나뉘어진다. 첫번째로는 객체가 있을 법한 예상 범위 추출 작업(Region Proposal)을 한다. 이렇게 제안된 영역을 CNN에 넣어 classification을 한다. - One-shot-detection이(EX : YOLO) : 하나의 신경망에 input으로 이미지를 넣어주고 output으로 Bounding box와 class label을 동시에 예측한다.  출처 : https://morioh.com/p/73fce91e9846​​​Instance Segmentation 각 개체를 픽셀 단위로 구분하는 것이다. 각 픽셀이 어느 개체(강아지라면 어느 가)에 속하는 픽셀인지 분류한다.  출처 : https://www.pyimagesearch.com/2018/11/26/instance-segmentation-with-opencv/​​​​Object TrackingObject Detection의 결과로 얻은 각 Bounding box를 이전 장면의 Bounding box와 비교해 ID를 매칭하는 작업을 계속해서 진행하는 것이다. (EX : SORT) 출처 : https://raweb.inria.fr/rapportsactivite/RA2011/pulsar/uid99.html​​​​  Reference​● https://www.youtube.com/watch?v=nDPWywWRIRo&ab_channel=StanfordUniversitySchoolofEngineering​● https://mickael-k.tistory.com/24 Object Detection이란?start() { Object Detection, OD 란? Object Detection(앞으로 'OD' 라고 하겠습니다.)은 물체 검출 입니다. 즉, 카메라나 다른 센서를 이용하여 자동차, 사람, 동물, 물건 등을 검출 하죠. 추가로 이게 뭔지를 나..mickael-k.tistory.com ● https://mickael-k.tistory.com/26?category=798521 Object Tracking 이란?strat() { Object Tracking을 공부하기 앞서 Object Detection에 관해서 그리고 Classification에 관해서 제 다른 포스트에서 공부를 하고 오시면 이해가 더 쉬울겁니다. 1. Classification 2. Object Detection O..mickael-k.tistory.com "
"object detection(pix2seq, detr) ",https://blog.naver.com/kimsjpk/222914622625,20221030,"전 글에서 object detection에 대해서 시도하겠다고 언급을 하였고어제 계속 github를 돌아다녔다. detr이라고 transformer를 object detection에 적용한 기존의 알고리즘에서​인코더를 swin transformer로 바꿔볼려고 자료를 찾다가 다음 사이트를 발견하였다.​SwinDetr/swindetr.py at main · cankocagil/SwinDetr · GitHub​Swin transformer encoder + transformer인데 학습하는 코드를 올려두질 않아서 이거를 기존의detr에 얹어볼려고 시도를 하였는데 문제가 detr의 경우 encoder, decoder를 구분해 두었는데 위의 코드는nn.transformer로 합쳐놓았다. decoder에서 다른 특징을 뽑아내어서 음... 어떻게 하지 하고 있다가 검색을 하였고 다음 사이트를 발견하였다.​GitHub - moein-shariatnia/Pix2Seq: Simple Implementation of Pix2Seq model for object detection in PyTorch​timm 모델을 써서 Deit라는 이미지 특징 추출을 하고 있는데 pytorch-image-models/timm/models at main · rwightman/pytorch-image-models · GitHub​여기서 swin_transformer_v2.py에서 swin transformer 특징추출을 아주 쉽게 할 수 있도록 구성해 두었다.​simple pix2seq가 좋은게 voc 2012 데이터에서 학습이 가능하도록 코드를 구성해 두었고 epoch 역시 25로​두어 빠르게 결과 확인이 가능하다. 한 에폭당 15분 정도 밖에 안걸렸다.​참고로 detr에서 coco 2017 데이터로 학습시 3시간 반 정도 걸렸다. 학습 에폭도 300이었고​당연히 데이터셋의 갯수가 다르니깐 그런건데... 어쨌든 다행이라는 생각이 든다.​이제 transformer decoder를 바꿔야 하는데 아쉽게도 nn.transformer에서는 기본 transformer 디코더만 구현​이 되어 있다. encoder, decoder로 구성된 transformer 변형 github를 찾고 있는데 아무리 뒤져봐도 원하는 코​드가 안나온다. 일단 점심 먹고 집에서 좀 더 찾아봐야 겠다.... OTL​[업데이트1]swin transformer encoder + transformer decoder로 학습한 모델을 추론해 보았다.한 장 당 11초 걸린다. gpu 켰는데도 불구하고...​그리고 transformer 유투브 강의를 들었다.나동빈 씨의 강의였는데, 설명은 잘 하는거 같은데 내가 잘 못 알아 듣겠다. github 에서 소스를 제공하고 코드에 주석을 잘 달아두어서 다른 유투브 강의를 듣고 코드를 살펴볼 생각이다.https://github.com/ndb796/Deep-Learning-Paper-Review-and-Practice/blob/master/code_practices/Attention_is_All_You_Need_Tutorial_(German_English).ipynb​아래는 강의 자료https://github.com/ndb796/Deep-Learning-Paper-Review-and-Practice/blob/master/lecture_notes/Transformer.pdf​방금 다른 유투브 강의를 들었는데 뭔가 좀 더 알거 같다 고려대 교수님의 강의이다.https://www.youtube.com/watch?v=Yk1tV_cXMMU​개념은 이걸 듣고 나동빈씨 강의에 나온 소스분석을 보면 좋을거 같다.​key, query, value를 통한 self attention을 보면 key와 query를 곱하고 다시 value를 곱하여 합하는데 이는 어째보면 mlp와도 유사하다고 생각한다. 가중치를 곱함으로써 학습에 필요한 특징을 추출하는 것이다.​positional embedding 등에 대해서도 자세히 설명하니 강의를 한 번 듣는 것을 추천한다.교수님의 강의라 그런지 상당히 들을 만 하다.​[업데이트 2]텐서플로우로 된 detr 깃허브를 발견하였다. finetuning을 사용하여 10에폭만 학습하게 되어 있다.GitHub - Visual-Behavior/detr-tensorflow: Tensorflow implementation of DETR : Object Detection with Transformers​여기서 finetune_voc.py를 실행하면 된다.inference를 실행하였을 때 한 이미지 당 0.2초 정도 걸렸다. pretrained detr 모델을 불러오지 않고 voc 2012 데이터를 10에폭 학습하였는데 detection 결과가 전혀 나오지 않는 것을 확인하였다. detr 모델을 finetuning 해서 결과가 나오면 inference 코드를 업로드 하도록 하겠다.​그리고 새로운 코드를 발견하였는데GitHub - wangjian123799/L-DETR: A light-weight for end-to-end objectlight weight라고 하니 기대가 된다. coco 2017  dataset으로 학습을 하도록 소스가 짜여져 있고학습 epoch 은 50으로 나와 있다. 학습해 보고 결과가 나오면 결과를 업데이트 하도록 하겠다.한 에폭당 1시간 55분 정도 걸린다. batch_size를 4로 하면 vram을 약 7.6기가 정도 잡아먹는다.​[업데이트2]인터넷을 돌아다니다가 OpenImage 데이터셋으로 학습할 수 있게 한 github를 발견하였다.https://www.ogis-ri.co.jp/otc/hiroba/technical/detr/part2.html Transformerを使った初めての物体検出「DETR」 - 第2回 DETRとTransformerの詳細解説とFine-Tuning | オブジェクトの広場DETRとTransformerの詳細解説と、Fine-Tuning方法について紹介します。www.ogis-ri.co.jp 여기에서는 환경 설정 방법을 올려두었고 https://github.com/woctezuma/detr.git 여기를 링크로 걸어두었다​ 첨부파일221107_OID2JSON.py파일 다운로드 OID 파일을 읽어 json으로 내용을 저장하고 첨부파일221107_shutil.py파일 다운로드 파일을 옮기고 첨부파일221107_pth.py파일 다운로드 pretrained model을 다운받아 저장한다 학습할 때 불러와서 fine tuning을  하면 된다​ 첨부파일221107_predict.py파일 다운로드 학습을 어느정도 하고 결과를 출력해 준다. 8에폭 학습하고 threshold를 0.2 로 낮추어서 balloon이 0.47 확률로​출력되는 것을 확인했다​문제가 inference 하는데 2.5~3초 정도 걸린다​그래서 전에 링크를 걸어둔 L-detr에서 모델을 가져와서 detr 모델에 복사해서 수정후 학습을 진행하려고 한다​L-detr의 구조가 detr-resnet50과 다르기 때문에 fine tuning  은 할 수 없다​처음부터 학습을 하면 이미지 수도 많아야 하고 epoch도 많아야 하기 때문에 학습시간이 많이 걸린다​5개 클래스 전체 데이터로 학습을 하고 결과를 업데이트 하도록 하겠다 ​[업데이트3]학습시킨 걸 아침에 확인했더니 에폭은 122정도 진행되었는데 map가 0으로 나와서... 참 그때 느낌이 참담했다​어떻게 해야하나 하고 잠시 생각하다가 구글 검색창에 detr real time 하고 검색하면서 여러 깃허브를 돌아다녔다​그러다가 다음 깃허브를 발견하였다​GitHub - Atten4Vis/ConditionalDETR: This repository is an official implementation of the ICCV 2021 paper ""Conditional DETR for Fast Training Convergence"". (https://arxiv.org/abs/2108.06152)​Fast Training Convergence가 마음에 들었다. 깃허브 그래프를 보면 detr보다 10배 빠른 수렴을 보여준다​그래서 L-detr의 pplcnet backbone과 conditional detr의 transformer 구조를 합쳤다​L-detr의 가벼운 구조와 conditional detr의 빠른 수렴을 합쳤더니 batch_size를 2에서 4로 늘려서 한 에폭당​학습시간이 9분 정도 걸리고 1에폭 부터 map가 0.0002로 찍히는 것을 확인하였다 5에폭에선 0.0046으로 나온​다. 200에폭 학습을 걸어두었는데 한 이틀정도 걸릴거 같다. 이제 inference 속도만 빨리 나오면 되는데​결과 나오는대로 업데이트 하겠다.​[업데이트4]오늘 아침에 선임에게 물어보니 회사에서 직접 사용할 모델이 필요하다고 한다.detr을 하면서 느끼는 거지만 경량화는 어떻게 해도 실시간 사용은 무리인 듯 하다.DETR : End-to-end방식의 Transformers를 통한 object detection (tistory.com)여기에서 거의 끝 부분에 나와있는 글이다.​연산량 측면에서 보면 DETR이, FPS에서는 Faster R-CNN이 좋은 성능을 보인다.하지만 이 두 모델 모두 Real-time이나 경량화가 목표가 아니라 비교대상은 아니다.​그러하다. 어차피 쓸 수는 없는 모델이니 내가 작업한 코드를 공개하려고 한다. 어차피 원래 있던 코드에​서 모듈만 바꾼거라서 별 의미는 없다.​OID dataset detr -> L-detr -> Fast training convergence detr 순으로 고쳤다.​ 첨부파일detr_L.zip파일 다운로드 https://github.com/woctezuma/detr.git ​여기에서 git clone 받고 위의 zip 파일에서 폴더에 맞게 압축푼 파일을 덮어 씌우면 된다.​real time으로 궤도를 바꿔서 결과가 나온다 싶으면 새로 글을 파서 기록하려고 한다.​참고로 pytorch 1.8 버전에서 실행하였다. torchvision 버전이 높으면 에러메시지 나온다​60 에폭 쯤에 map가 0.12 정도 나왔다. 300 에폭 결과를 확인하려 하니 컴퓨터가 재부팅 되어 있어서​최종 결과를 확인하지는 못했다 "
Feature Pyramid Networks for Object Detection  (CVPR 2017) ,https://blog.naver.com/petraeye/222868401639,20220906,"#FPS #Feature #Pyramid #Networks #object #detection​Lin, Tsung-Yi, et al. ""Feature pyramid networks for object detection."" Proceedings of the IEEE conference on computer vision and pattern recognition. 2017.​ our model that has a similar structure but leverages it as a feature pyramid, with predictions made independently at all levels.각 level의 feature에서 object들을 predict 한다. ​ 방식은 2x로 upsamling을 진행하고, 1x1 conv layer는 channel dimension을 줄여서 더할 때 사용한다. 2개의 feature map을 element-wise addition을 진행하여 output이 나온다. ​정확도가 높아짐. Conv만 사용했을때보다 FPN의 성능이 더 좋음.  ​ "
"classification, object detection, segmentation ",https://blog.naver.com/jinho381/222435290064,20210718,"오늘은 classification, object detection, segmentation에 대한 설명을 시작하겠다. 처음 비전을 공부하면 이 3가지 task가 헷갈린다.​ 해당 사진처럼 3가지의 object를 구분한다고 가정한다.​classification : 말 그대로 '분류'를 말한다. 어떤 이미지가 주어졌을 때, 이미지 자체의 이름을 정한다고 생각하면 된다. 첫 번째 그림을 보면 이미지에 고양이도 있고, 풀도 있고, 땅도 보이지만 이건 우리의 시점이다. 우리는 CAT,DOG,DUCK 3가지를 구분하겠다고 가정했으므로 모든 이미지는 3가지로만 분류한다.  첫 번째 이미지에는 고양이가 그려져 있으므로. 우리는 CAT으로 분류한다. ​Localization : 이미지에서 object가 어디에 있는지 검출하는 과정을 뜻함. 일반적으로 bounding box를 이용하여 검출한다.​Object detection : 3번째 사진을 보면 고양이 오리 강아지가 모두 있다. Classification은 이미지를 특정 이름으로 분류하는 것. Object detection은 이미지에 검출되는 여러 객체들을  Localization함을 말한다.​segmentation: Localization을 진행하는 bounding box에서 object를 정확하게 pixel 단위로 구분하는 작업을 말한다.​segmentation은 sementic segmentation, instance segmentation 두개로 나뉜다. ​sementic segmentation : 특정 클래스들로 segmentation을 진행하는 것. 여러개의 자동차가 검출되어도 하나의 클래스에 속하므로 같은 색으로 segmentation 된다.​instance segmentation :  같은 클래스에서도 여러 가지로 구분하는 것을 말한다. ​​Referencehttps://www.cs.princeton.edu/courses/archive/spring18/cos598B/public/outline/Instance%20Segmentation.pdf​http://cs231n.stanford.edu/slides/2016/winter1516_lecture8.pdf "
"[Object Detection] SSD, YOLO _AI  ",https://blog.naver.com/dkfka1295/222651659167,20220219,"Object Detection(객체 감지)란,이미지나 비디오에서 물체를 식별하고 찾을 수 있게 해주는 컴퓨터 비전 기술이다. ​여러 물체에 대해 어떤 물체인지 분류하는 Classification 문제와그 물체가 어디 있는지 박스를 통해 (Bounding box) 위치 정보를 나타내는 Localization 문제가 합쳐진 기술을 뜻한다. ​Object Detection =  Classification​ +  Localization​객체를 감지하는 것(Object Detection)을 쉽게 설명하면 다음과 같다.이미지에서 객체들은 다양한 영역에 분포되어 있다. 그렇기 때문에 이미지에서 객체를 추출하기 전에 객체가 어느 범위 내에 있는 지를 경계 상자를 먼저 확인해야 한다. 그리고 경계 상자 안에 있는 객체가 무엇인지 레이블을 지정하여 분류한다.​Object Detection이 적용된 아래 예시 사진을 보자. 사진 속의 고양이, 개, 오리가 어디에 있는지 박스를 통해 위치정보(Localization)를 나타내주고, 각 객체에 대하여 어떤 물체인지 분류(Classification)하여 라벨링이 이루어진걸 확인 할 수 있다. ​ ​​Object Detection Algorithms은 영상에서 전처리 등을 통해서 노이즈를 제거하거나, 이미지를 선명하게 만든 후에 해당 이미지에서 특징들을 추출하고, 이 특징들을 이용하여 Object Detection에 대해 분류(Classifier)하는 파이프라인(pipe line)을 따르게 된다.​​Object Detection Algorithms은 크게 다음과 같이 작동한다.● 전처리 (Pre-processing)● 특징 추출 (Feature Extraction)● 분류 (Classifier)​​Deep Learning을 이용한 Object Detection은 Object Detection에서 수행되는 Classification 문제와 ​Localization 문제를 찾는 방법의 차이에 따라 크게 1-stage Detector와 2-stage Detector로 나눌 수 있다.​1-stage Detector는 이 두 문제를 동시에 행하는 방법이고2-stage Detector는 이 두 문제를 순차적으로 행하는 방법이다. 1-stage Detector가 비교적으로 빠르지만 정확도가 낮고, 2-stage Detector가 비교적으로 느리지만 정확도가 높다.하지만 최근에는 1-Stage 방법들이 2-Stage의 정확도를 따라잡으면서 이렇게 구분짓기가 모호해졌다.​각 스테이지의 대표적인 기술들은 다음과 같이 나뉘어 진다.1-stage Detector은 YOLO계열과 SSD 계열 등의 기술이 포함되고,2-stage Detector은 R-CNN계열이 포함된다.​​​다음으로 Object Detection의 대표적인 기술에 대해 살펴보자.​​​​​1. YOLO (You Only Look Once)​YOLO는 이미지 전체에 대해서 하나의 신경망(a single neural network)이 한 번의 계산만으로 Object의 종류와 위치를 추측하는 딥러닝 (Deep Learning) 기반의 물체인식 (Object Detection) 알고리즘을 뜻한다.​​ YOLO 알고리즘YOLO에서는 input 이미지를 SxS grid로 나누고 각 grid 영역에 해당하는 bounding box와 신뢰도(confidence), class probability map을 구한다. 신뢰도는 grid 내 객체 인식 시 정확성을 반영한다.​처음에는 객체 인식과는 다른 bounding box가 설정되지만, 신뢰도(confidence)를 계산하여 bounding box의 위치를 조정함으로써, 가장 높은 객체 인식 정확성을 가지는 bounding box를 얻게 된다.​* bounding box: 객체의 위치를 알려주기 위해 객체의 둘레를 감싼 직사각형 박스* class probability: bounding box로 둘러싸인 객체가 어떤 클래스에 해당하는지에 관한 확률​​이 과정은 그리드에 객체 포함 여부를 계산하기 위해, 객체 클래스 점수를 계산한다. 이 결과로 총 S x S x N 객체가 예측되는데, 이 그리드의 대부분은 낮은 신뢰도를 가진다. 신뢰도를 높이기 위해 주변의 그리드를 합칠 수 있다. 이후, 임계값을 설정해 불필요한 부분은 제거하며 Final detection을 찾게 된다. 디테일한 Achitecture는 아래 그림[2-1]에서 살펴보자.​​​ Figure 1을 보면 하나의 컨볼루션 네트워크(convolutional network)가 여러 bounding box와 그 bounding box의 클래스 확률을 동시에 계산해 준다. YOLO는 이미지 전체를 학습하여 곧바로 검출 성능(detection performance)을 최적화한다.​​YOLO는 이미지 전체를 한 번에 바라보는 방식으로 클래스를 분별하기때문에, 단순한 처리로 속도가 매우 빠르다. 기존 다른 real-time 비전 기술과 비교할 때, 성능 또한 2배 정도 높다.  YOLO를 이용한 실시간 객체 검출​​YOLO의 특징 - YOLO는 단일 신경망 구조이기 때문에 구성이 단순하며, 빠르다. - YOLO는 주변 정보까지 학습하며 이미지 전체를 처리하기 때문에 background error가 작다. - YOLO는 훈련 단계에서 보지 못한 새로운 이미지에 대해서도 검출 정확도가 높다. - 단, YOLO는 SOTA 객체 검출 모델에 비해 정확도(mAP)가 다소 떨어진다.​​​​​2. SSD (Single Shot Detector)​SSD(Single Shot Detector)는 말 그대로 사진의 변형 없이 그 한 장으로 훈련, 검출을 하는 detector를 의미한다.​YOLO에서는 이미지를 grid로 나누어서 각 영역에 대해 bounding box를 예측했다면, SSD 는 CNN pyramidal feature hierarchy를 이용해 예측하는 것이 특징이다.​ [2-1] SSD와 YOLO Architecture 비교​​​SSD는 이미지에서 보여지는 object들의 크기는 매우 다양하고 convolution layer 들을 통해 추출한 한개의 feature map만 갖고 detect 하기에는 부족하다라는 생각에서 나온 기술이다. SSD에서는 CNN을 이용하여 image feature를 다양한 위치의 layer들에서 추출하여 detector와 classifier를 적용한다. 앞쪽 layer에서는 receptive field size가 작으니 더 작은 영역을 detect 할 수 있고, 뒤쪽 layer로 갈수록 receptive field size가 커지므로 더 큰 영역을 detect 할 수 있다는 특징을 이용했다.​이렇게 SSD 알고리즘은 네트워크 중간 계층에서 다수의 feature map을 활용하여, 각 feature map은 크기가 각기 다른 종횡비의 Bounding Box를 가지며, 모델을 통해 계산된 좌표와 객체 클래스 값에 대하여 default box를 활용해 최종 Bounding box를 생성한다. 아래 예제를 보자.​​ ​위 사진 안에서 차지하는 강아지의 크기와 고양이 크기 차이가 크다. 이를 한가지 feature map에서 구하려면 bounding box의 크기차이가 크기 때문에, box의 크기 추정부터 위치 추정까지 많은 과정이 필요할 것이다. 하지만 SSD는 feature map을 여러 개의 크기로 만들어서, 큰 map에서는 작은 물체의 검출을, 작은 map에서는 큰 물체의 검출을 하게 만들었다. 이러한 방식은 위치 추정 및 입력 이미지의 resampling을 없애면서도 정확도 높은 결과를 도출하게 된다.​​SSD는 물체의 Bounding Box를 도출할때 Bounding Box의 정보를 출력하는 것이 아니다. 일반적인 사각형인 default box를 준비해두고 어떻게 변형시키면 Bounding Box가 되는지에 대한 정보를 출력하는데, 이때 디폴트박스를 변형시키는 정보는 오프셋 정보라고 한다.​​​[SSD모델 종류]SSD모델에는 SSD300, SSD512 두가지 모델이 있다. 이때 숫자 300와 512는 fixed-size의 input을 의미한다.​입력된 이미지의 크기를 300 × 300픽셀로 바꾸어 입력하는 SSD300입력된 이미지의 크기를 512 × 512픽셀로 바꾸어 입력하는 SSD512​[ SSD300를 활용한 물체 감지 흐름 6단계 ]Step1. 300 × 300으로 화상 리사이즈Step2. 디폴트 박스 8,732개 준비Step3. SSD 네트워크에 화상 입력Step4. 신뢰도 높은 디폴트 박스 추출Step5. 오프셋 정보로 수정 및 중복 제거Step6. 일정 신뢰도 이상을 최종 출력으로 선정 ​​SSD의 특징- SSD는 객체 검출 속도 및 정확도 사이의 균형이 있는 알고리즘이다.- 다양한 스케일의 물체를 검출 할 수 있다.​​​​​​​객체를 검출할 때, 속도와 정확도는 다음 그림과 같이 trade-off 관계가 있다. 활용 목적에 따라 적절한 알고리즘을 응용하는 것이 좋겠다.​ ​​​​​​​​​​​​​​​​​#AI #ML #DL #ObjectDetection #YOLO #SSD #Detection #Machinelearning #deeplearning #인공지능 #머신러닝 #딥러닝 #객체탐지 #욜로 #객체감지 #CNN #SingleShotDetector #Classifier #컴퓨터비전 #COMPUTERVISION #디텍션 "
HOTR: End-to-End Human-Object Interaction Detection with Transformers 논문 리뷰(CVPR 2021) ,https://blog.naver.com/kingjykim/222879339845,20220920,"Paper : https://openaccess.thecvf.com/content/CVPR2021/html/Kim_HOTR_End-to-End_Human-Object_Interaction_Detection_With_Transformers_CVPR_2021_paper.htmlGit : https://github.com/kakaobrain/HOTRAuthor : Bumsoo Kim, Junhyun Lee, Jaewoo Kang, Eun-Sol Kim, Hyunwoo J. Kim​​1. Introduction​  Human-Object Interaction(HOI) 감지는 이미지 내의 <human, object, interaction> 삼중항 세트를 예측하는 작업으로 ""Visual semantic role labeling""에서 공식적으로 정의되었다. 이전의 방법은 먼저 객체 감지를 수행하고 이후 인간, 객체 쌍을 별도의 후 처리 단계와 연결함으로써 간접적으로 이 작업을 해결했다. 특히, 초기 시도에는 후속 신경망과 이러한 연관성을 수행했기 때문에 시간이 많이 걸리고 계산 비용이 많이 든다.  순차적 HOI 검출기의 중복되는 추론 구조를 해결하기 위해 최근 연구에서는 병렬 HOI 검출기를 제안했다. 이러한 작업은 interaction boxes(객체 쌍의 중심점을 모두 덮는 가장 tightest 한 boxes) 또는 union boxes(객체 쌍의 두 boxes 영역을 모두 덮는 가장 tightest 한 boxes)와의 interaction을 명시적으로 localize 한다. 국소화된 interaction은 객체 감지 결과와 연결되어 <human, object, interaction> 삼중항을 완성한다. 시간이 많이 걸리는 신경망 추론은 거리 또는 IoU와 같은  heuristics에 기초한 간단한 매칭으로 대체된다.  그러나 HOI 검출에 대한 이전 연구는 여전히 두 가지 측면에서 제한적이다. i) 그들은 거의 중복되는 예측 및 heuristic thresholding을 숨기는 등의 추가적인 후 처리 단계를 필요로 한다. ii) 객체 간 관계를 모델링 하는 것이 객체 탐지에 도움이 된다는 것이 입증되었지만,  HOI 탐지의 interaction에 대한 높은 수준의 의존성을 고려하는 것의 효과는 아직 완전히 탐구되지 않았다.  본 논문에서는 예측 접근 방식으로 장면에서 직접 일련의 인간과 객체의 interaction 세트를 한 번에 예측하는 HOTR(Human-Object interaction TRANS-former)이라는 빠르고 정확한 HOI 알고리즘을 제안한다. 저자는 HOI 삼중항 세트를 예측하기 위해 Transformer를 기반으로 한 encoder-decoder 아키텍처 모델을 설계하여 기존의 두 한계를 모두 극복할 수 있도록 한다. 첫째, direct 한 세트 레벨 예측을 통해 수작업의 후 처리 단계를 제거할 수 있다. 모델은 예측된 실제 <human, object, interaction> 삼중항과의 interaction과 일치하는 세트 손실 함수를 사용하여 end-to-end 방식으로 훈련된다. 둘째, Transformer의 self-attention 메커니즘은 모델이 인간과 물체 및 이들의 interaction 사이의 상황적 관계를 활용하도록 하여 높은 수준의 장면 이해에 더 적합한 예측 프레임워크를 만든다.  저자는 HOI 검출에 대해 두 개의 벤치마크: V-COCO과 HICO-DET 데이터를 사용하여 모델을 평가한다. 제안된 아키텍처는 순차적 및 병렬 HOI 검출기와 비교하여 두 데이터 세트에서 최첨단 성능을 달성한다. 또한, 우리의 방법은 직접 설정 수준 예측을 통해 시간이 많이 걸리는 후처리를 제거하여 다른 알고리즘보다 훨씬 빠르다. 이 연구의 기여는 다음과 같이 요약할 수 있다.​• HOI 검출에서 최초의 Transformer 기반 세트 예측 접근 방식인 HOTR을 제안한다. HOTR은 이전 HOI 검출기의 수작업의 후 처리 단계를 제거하는 동시에 interaction 간의 상관관계를 모델링 할 수 있다.•두 개의 병렬 decoder의 출력을 연결하기 위한 HOTR: HO Poiners에 대한 다양한 훈련 및 추론 기술, 최종 HOI 삼중항 세트를 예측하기 위한 재구성 단계, end-to-end training을 가능하게 하는 새로운 손실 함수를 제안한다.•HOTR은 HOI 검출에서 두 벤치마크 데이터 세트를 사용해 이전의 병렬 HOI 검출기(5~9ms)보다 훨씬 빠른 1ms 미만의 추론 시간으로 최첨단 성능을 달성했다.​​2. Related Work ​2.1. Human-Object Interaction Detection  Human-Object Interaction 감지는 ""Visual semantic role labeling""에서 처음 제안되었으며, 순차적 방법과 병렬 방법이라는 두 가지 주된 방법으로 개발되었다. 순차적 방법에서 객체 감지가 먼저 수행되고 감지된 객체의 모든 쌍은 interaction을 예측하기 위해 별도의 신경망으로 추론된다. 병렬적 HOI 검출기는 객체 감지 및 interaction 예측을 병렬로 수행하고 거리 또는 IoU와 같은 간단한 heuristics과 연관시킨다.​Sequential HOI Detectors: InteractNet은 행동별 밀도 맵을 도입하여 인간 중심을 기반으로 대상 객체를 localize 하고 개별 boxes의 특징을 결합하여 interaction을 예측함으로써 기존 객체 검출기를 확장했다. 개별 boxes의 시각적 단서에 기반한 interaction 감지는 종종 상황별 정보의 부족으로 어려움을 겪는다. 이를 위해, iCAN은 localize된 객체/인간으로부터 특징에 보완되는 맥락적 특징을 추출하는 인스턴스 중심의 attention 모듈을 제안했다. No-Frills HOI 검출은 단순한 다층 퍼셉트론만을 사용하는 훈련 및 추론 HOI 검출 파이프라인을 제안한다.  그래프 기반 접근 방식은 HOI 구조를 그래프로 명시적으로 나타낼 수 있는 프레임워크를 제안했다. Deep Contextual Attention는 HOI의 상황별 attention 프레임워크에 의해 상황별 정보를 활용한다. ""Contextual heterogeneous graph network for human-object interaction detection""에서는 인간과 개체를 서로 다른 종류의 노드로 모델링 하는 heterogeneous graph network를 제안한다. Linguistic priors 또는 인간 포즈 정보와 같은 다양한 외부 소스도 성능을 더욱 향상시키기 위해 활용되었다. 순차적 HOI 검출기는 상당히 직관적인 파이프라인과 견고한 성능을 특징으로 하지만 객체 검출 단계 이후 추가적인 신경망 추론 때문에 시간이 많이 걸리고 계산 비용이 많이 든다.​Parallel HOI Detectors: Sequential HOI 검출기보다 더 빠른 HOI 검출을 위한 시도는 최근 작업에서 Parallel HOI 검출기로부터 시작되었다. 이러한 작업은 interaction 예측을 위한 별도의 신경망을 interaction points 또는 union boxes와의 직접 localized interaction을 통해 거리 또는 IoUs와의 단순한  heuristic 기반 매칭으로 대체한다. 기존 물체 감지기와 병렬화될 수 있기 때문에 빠른 추론 시간을 특징으로 한다. 그러나 이러한 작업은  localized된 interaction을 객체 감지 결과와 연결하기 위해 수작업의 후 처리 단계가 필요하다는 점에서 제한적이다. 후 처리 단계는 i) 임계값에 대한 수동 검색이 필요하다. 그리고 ii) 지역화된 interaction(5~9ms)과 각 객체 쌍을 일치시키기 위한 추가시간 복잡성을 생성한다.​2.2. Object Detection with Transformers   CVPR 2020에서 제안된 DETR은 우수한 성능을 보여주면서 물체 감지에서 hand-designed한 구성 요소의 필요성을 없앴다. DETR는 고정 크기의 N 예측 세트를 decoder를 통과하는 단일 경로로 추론하는데, 여기서 N은 이미지의 일반적인 개체 수보다 훨씬 크게 설정된다. DETR의 주요 손실은 예측된 개체와 실제 개체 사이의 최적의 이분 매칭을 생성한다. 그런 다음 개체별 손실(클래스 및 경계 상자)이 최적화된다.​​3. Method Figure 2. Overall pipeline of our proposed model.  이 논문의 목표는 삼중항 세트 사이의 고유한 의미 관계를 end-to-end 방식으로 고려하면서 <human, object, interaction> 삼중항 세트를 예측하는 것이다. 이 목표를 달성하기 위해 HOI 탐지 세트 예측을 공식화한다. 이 섹션에서는 먼저 객체 탐지를 위한 설정된 예측 아키텍처를 HOI 탐지로 직접 확장하는 문제에 대해 논의한다. 그런 다음, 일련의 객체 감지 세트를 병렬적으로 예측하고 인간과 객체의 interaction을 연결하는 동시에 Transformer 안의 self-attention을 통한 interaction 사이의 관계를 모델링 하는 아키텍처 HOTR을 제안한다. 마지막으로 HOI 감지를 위한 Hungarian Matching과 손실 함수를 포함한 모델에 대한 훈련 세부 사항을 제시한다.​3.1. Detection as Set Prediction  먼저 transformer를 사용한 객체 감지 세트 예측에서 시작한 다음 이 아키텍처가 transformer를 사용하여 HOI 검출을 포착하도록 확장하는 방법을 보여준다.​Object Detection as Set Prediction. 객체 감지는 DETR을 통해 세트 예측 문제를 탐구하었다. 객체 감지는 각 객체에 대한 단일 분류와 단일 localization를 포함하기 때문에 DETR의 transformer encoder-decoder 구조는 N 위치 임베딩을 객체 클래스 및 경계 상자에 대한 N 예측 집합으로 변환한다.​HOI Detection as Set Prediction. 객체 감지와 유사하게, HOI 감지는 각 예측이 인간 영역(즉, interaction의 대상), 객체 영역(즉, interaction의 대상) 및 interaction 유형의 다중 레이블 분류를 포함하는 세트 예측 문제로 정의될 수 있다. 한 가지 간단한 확장은 DETR의 다층 퍼셉트론(MLP) 헤드를 수정하여 각 위치 임베딩을 변환하여 인간 상자, 객체 상자 및 동작 분류를 예측하는 것이다. 그러나 이 아키텍처는 여러 위치 임베딩을 사용하여 동일한 객체에 대한 지역화를 중복적으로 예측해야 하는 문제를 제기한다. (예를 들어, 같은 사람이 의자에 앉아서 컴퓨터에서 작업하는 경우, 두 개의 서로 다른 쿼리는 동일한 사람에 대한 중복 회귀를 추론해야 한다.)​ Figure 3. Conceptual illustration of how HO Pointers associates the interaction representations with instance representations.3.2. HOTR architecture  HOTR의 전체 파이프라인은 Figure 2에 나와있다. 저자의 아키텍처는 공유 encoder와 두 개의 병렬 decoder(즉, instance decoder and interaction decoder)를 가진 transformer encoder-decoder 구조를 특징으로 한다. 두 decoder의 결과는 최종 HOI 삼중항 생성을 위해 제안된 HO 포인터를 사용하는 것과 관련이 있다.​Transformer Encoder-Decoder architecture. DETR와 유사하게, global context는 backbone인 CNN과 공유하는 encoder에 의해 입력 이미지에서 추출된다. 그 후, 두 세트의 위치 임베딩(즉, the instance queries and the interaction queries)이 두 개의 병렬 decoder(즉,  the instance decoder and interaction decoder)에 공급된다. instance decoder는 instance 쿼리를 객체 탐지를 위한 instance 표현으로 변환하는 반면, interaction decoder는 interaction 쿼리를 interaction 탐지를 위한 interaction 표현으로 변환한다. interaction 표현에  feed-forward 네트워크(FFN)를 적용하고 인간 포인터, 객체 포인터 및 interaction 유형을 얻는다(Fig. 3 참조). 즉, interaction 표현은 경계 상자를 직접 회귀하는 대신 휴먼 포인터 및 객체 포인터(HO 포인터)를 사용하여 관련 instance 표현을 포인팅함으로써 인간과 객체 영역을 localization 한다. 저자의 아키텍처는 직접 회귀 접근 방식에 비해 몇 가지 장점이 있다. 저자는 객체가 여러 interaction에 참여할 때 경계 상자를 직접 회귀시키는 데 문제가 있다는 것을 발견했다. 직접 회귀 분석 접근법에서는 동일한 객체의 localization이 interaction에 따라 다르다. 아키텍처는 별도의 instance 및 interaction 표현을 가지고 HO 포인터를 사용하여 그것들을 연결함으로써 이 문제를 해결한다. 또한, 아키텍처는 모든 interaction에 대해 중복적으로 localization를 학습할 필요 없이 보다 효율적으로 localization를 학습할 수 있도록 한다. 우리의 실험은 공유 encoder가 두 개의 별도 encoder보다 HO 포인터를 학습하는 데 더 효과적이라는 것을 보여준다.​HO Pointers. HO 포인터가 instance decoder와 interaction decoder의 병렬 예측을 연결하는 방법에 대한 개념적 개요가 Fig. 3에 설명되어 있다. HO 포인터(즉, 인간 포인터와 객체 포인터)는 interaction에서 인간과 객체의 해당 인스턴스 표현의 인덱스를 포함한다. interaction 디코더가 K interaction 쿼리를 K interaction 표현으로 변환한 후, interaction 표현 zi는 벡터 vhi와 voi, 즉 vhi = FFNh(zi)와 voi = FFNo(zi)를 얻기 위해 두 개의 feed-forward 네트워크 FFNh: Rd -> Rd, FFNo: Rd -> Rd에 공급된다. 그리고 마지막으로 가장 높은 유사성 점수를 가진 instance 표현의 인덱스인 인간/객체 포인터 c^hi와 c^oi는 다음과 같이 구해진다. 여기서 µj는 j 번째 instance 표현이고 sim(u, v) = uT v / ||u|| ||v||이다.​Recomposition for HOI Set Prediction. 이전 단계부터 i) N instance 표현, ii) K interaction 표현 z와 그들의 HO 포인터 Cˆh 및 Cˆo를 가지고 있다. γ interaction 클래스가 주어지면, 저자의 재구성은 경계 상자 회귀 및 동작 분류를 위한 feed-forward 네트워크를 각각 FFNbox: Rd → R4, FFNact: Rd → Rγ로 적용하는 것이다. 그런 다음, i번째 interaction 표현 zi에 대한 최종 HOI 예측은 다음과 같이 구해진다. HOTR에 의한 최종 HOI 예측은 다음과 같은 K 삼중항 집합이다, {<bˆhi, bˆoi, aˆi>}Ki=1. ​Complexity & Inference time. 이전의 병렬 방법은 비용이 많이 드는 pair-wise 신경망 추론을 통해 삼중항(거리 또는 IoU에 기반한 해당 인간 영역 및 객체 영역과 interaction 영역을 연결)의 빠른 매칭으로 대체했다. 하지만 HOTR은 K interaction을 N 개의 instance와 연관시켜 객체 감지 후 추론 시간을 추가로 줄여 결과적으로 시간 복잡도 O(KN)로 더 작아진다. interaction 영역 및 삼중항 매칭을 위한 NMS를 포함한 이전의 1단계 HOI 검출기에서 후처리 단계를 제거함으로써 HOTR은 성능 향상을 보여주면서 추론 시간을 4~8ms 단축한다.​3.3. Training HOTR  이 섹션에서는 HOTR training에 대해 자세히 설명한다. 먼저 재구성에 의해 얻은 ground-truth HOI 삼중항과 HOI 세트 예측 사이의 고유한 매칭을 위한 Hungarian Matching의 cost matrix를 소개한다. 그런 다음 매칭 결과를 통해 HO 포인터에 대한 손실과 최종 training 손실을 정의한다.​Hungarian Matching for HOI Detection. HOTR은 행동 유형 a에 대한 human box, object box 및 이진 분류로 구성된 K인 HOI 삼중항 예측한다. 각 예측은 하나 이상의 interaction을 가진 고유한 <human, object> 쌍을 포착한다. K는 이미지에서 일반적인 interaction pairs 수보다 크게 설정된다. 예측된 HOI 삼중항과 실제 HOI 삼중항 사이의 최적의 이분 매칭을 정의하는 기본 비용 함수로 시작한 다음 interaction 표현에 대해 이 매칭 비용을 수정하는 방법을 보여준다.  Y는 실측 HOI 삼중항 set를 나타내고 Yˆ = {yˆi}Ki=1는 K 예측 set로 정의한다. K가 이미지에서 고유한 interacting pairs의 수보다 큰 수가 나오기 때문에 Y도 ∅(interaction 없음)로 패딩된 K 크기의 set로 간주합니다. 이 두 set 사이의 이분 매칭을 찾기 위해 가장 낮은 비용으로 K 요소 σ ∈ SK의 순열을 검색한다. 여기서 Cmatch는 ground-truth yi와 index σ(i)를 가진 예측 사이의 pair-wise 매칭 비용이다. 그러나 yi는 <hbox, obox, action> 형태이고 yˆσ(i)는 <hidx, oidx, action> 형태이므로 비용 함수를 수정하여 매칭 비용을 계산해야 한다.  Φ : idx → box가 객체 감지를 위한 최적의 할당에 의해 ground-truth <hidx, oidx>에서 ground-truth <hbox, obox>로 매핑 함수가 된다. 역 매핑 Φ-1: box → idx를 사용하여, ground-truth box에서 ground-truth idx를 얻는다.  M ∈ Rd×N을 정규화된 인스턴스 표현 µ′ = µ / ||µ|| ∈ Rd, 즉 M = [µ′1 ... µ′N ]의 집합이라고 정의한다. 이것을 바탕으로 다음과 같이 주어진 (1)의 H 포인터에 대한 소프트맥스 예측 집합인 Pˆh ∈ RK×N을 다음과 같이 계산한다. 여기서 ||Ki=1는 행 벡터와 v¯hi = vhi / ||vhi||의 수직 스택을 나타낸다. P​ˆo는 유사하게 정의된다.  ground-truth yi = (bhi, boi, ai), Pˆh, and , Pˆo가 주어질 때, 저자는 ground-truth box를 chi =Φ−1(bhi)와 coi = Φ−1(boi) 에 의해 지수로 변환하고 다음과 같이 쓰인 매칭 비용 함수를 다음과 같이 계산한다. 여기서 Pˆ[i, j]는 i번째 열과 j번째 열에 있는 원소를 나타내며, aˆσ(i)는 예측된 행동이다. 행동 매칭 비용은 Lact(ai, aˆσ(i)) = BCELoss(ai, aˆσ(i))로 계산된다. α와 β는 지수 예측을 위한 비용 함수와 행동 분류의 서로 다른 규모의 균형을 맞추기 위해 고정 숫자로 설정된다.​Final Set Prediction Loss for HOTR. 그런 다음 위에서 일치한 모든 쌍에 대한 Hungarian loss을 계산한다. 여기서 HOI 삼중항 손실은 localization loss과 행동 분류 loss를 다음과 같이 갖는다. localization loss Lloc(chi, coi, zσ(i))는 다음과 같이 정의된다. 여기서 τ는 손실 함수의 smoothness을 조절하는 temperature이다. 저자는 τ = 0.1이 실험 때 가장 좋은 값이라는 것을 발견했다.​Defining No-Interaction with HOTR. DRET에서 softmax 출력에 대한 no-object 클래스의 확률을 최대화하면 자연스럽게 다른 클래스의 확률이 억제된다. 그러나 HOI 감지에서 동작 분류는 각 동작이 개별 이진 분류로 처리되는 다중 레이블 분류이다. 중복 예측을 억제할 수 있는 명시적 클래스의 부재로 인해 HOTR은 동일한 <human, object>  쌍에 대한 다중 예측으로 끝난다. 따라서 HOTR은 interactiveness을 학습하는 명시적 클래스를 설정하고(pair 간에 interaction이 있는 경우 1, 그렇지 않은 경우 0), interactiveness 점수가 낮은 중복 쌍에 대한 예측을 억제한다(No-Interaction 클래스로 정의됨). Table 3에서, 저자는 interaction에 대한 명시적 클래스를 설정하는 것이 최종 성능에 기여한다는 것을 보여준다.​Implementation Details. 저자는 Adam W와 함께 HOTR을 training 한다. 저자는 transformer의 초기 학습률을 10-4로, 가중치 감소를 10-4로 설정했다. 모든 transformer 가중치는 Xavier로 초기화된다. 기준선을 사용하여 공정한 평가를 위해 Backbone, Encoder, 그리고 Instance Decoder는 MS-COCO에서 사전 교육을 받고 training 중에 동결된다. DETR에서처럼 스케일 확장을 사용하여 입력 이미지의 크기를 조정하여 최단 side 최소 480에서 최대 800픽셀이고 최단 side 최대 1333픽셀이 되도록 한다.​​4. Experiments​  이 섹션에서는 HOI 감지에서 모델의 효과를 입증한다. 먼저 벤치마크로 사용하는 두 가지 공개 데이터 세트인 V-COCO와 HICO-DET에 대해 설명한다. 다음으로 HOTR이 mAP와 추론 시간 모두에서 최첨단 성능을 달성하여 HOI 삼중항 캡처에 성공했음을 보여준다. 그런 다음 HOTR 아키텍처에 대한 자세한 연구를 제공한다.(이하 생략)​​5. Conclusion​  본 논문에서는 human-object interaction 문제에서 최초의 transformer 기반 세트 예측 접근 방식인 HOTR을 제시한다. HOTR의 세트 예측 접근 방식은 interaction 간의 상관관계를 모델링 할 수 있는 동시에 이전 HOI 검출기에서 수작업의 후 처리 단계를 제거한다. 저자는 trainging을 위한 병렬 디코더를 사용한 HOI 분해, 추론을 위한 유사성에 기반한 재구성 레이어 및 interaction 억제를 위한 HOTR의 다양한 training 및 추론 기술을 제안한다.  interaction 표현을 instance 표현에 연결하는 HOI 감지를 위한 새로운 세트 기반 일치를 개발한다. 모델은 이전 병렬 HOI 검출기에 비해 상당한 차이를 보이며 HOI 검출의 두 가지 벤치마크 데이터 세트인 V-COCO 및 HICO-DET에서 최첨단 성능을 달성한다. HOTR은 1ms 미만의 추론 시간으로 HOI 검출에서 두 벤치마크 데이터 세트에서 최첨단 성능을 달성하여 이전의 병렬 HOI 검출기(5~9ms)보다 훨씬 빠르다.​​​2022.09.20​ "
[object detection] NMS(Non-Maximum Suppression)  ,https://blog.naver.com/929ok/222914393534,20221030,"Non-Maximum Suppression(NMS, 비 최대 억제) 이미지가 Object detection 알고리즘의 입력으로 들어가게 되면 Object에 Bounding box가 그려지며 어떤 물체(class)일 확률 값(Score)을 가집니다. 이때 아래의 그림처럼 한 가지 객체에 대해 많은 Bounding box가 생기는데 동일안 객체에 대해 여러개의 Bounding box가 있다면, 가장 높은 score의 박스만 남기고 나머지를 제거하는 것을 NMS라 합니다. ​ "
라즈베리파이에서 Tensorflow Object Detection 만들어 보기 ,https://blog.naver.com/seodaewoo/222039614429,20200723,"준비물라즈베리파이 4B, 라즈베리파이 카메라 모듈 picamera(5MB 사용)(키보드, 마우스, 모니터는 기본적으로 연결되어 있다고 가정)​참조 동영상https://www.youtube.com/watch?v=weT_aJ7SJH8&list=LL-tCwI-Pv2ccND4vQP_sW_w&index=5&t=0s 또는 https://github.com/PhysicsX/Tensorflow-Object-Detection-on-Raspberry-pi-4-model-B PhysicsX/Tensorflow-Object-Detection-on-Raspberry-pi-4-model-BContribute to PhysicsX/Tensorflow-Object-Detection-on-Raspberry-pi-4-model-B development by creating an account on GitHub.github.com 동작 화면은 생략...​실제로 동작시켜보니 초당 2프레임 내외로 동작 ㅠ.ㅠgoogle coral을 이용해서 속도를 높여야겠다.​위와 같이 만든 후 나중에 실행할 때는 아래 명령어 입력​cd Desktop/tensorflowRaspvirtualenv envsource env/bin/activatecd models/research/object_detectionpython3 ObjectDetectionPiCamera.py​※ 레이블을 한글로 바꾸면 프로그램이 죽어버린다. 비디오에 한글 출력을 해결해야 됨.​​ "
Object detection ,https://blog.naver.com/qhqoxogh/222503415779,20210912,"tvN 드라마 스타트업tvN 드라마 스타트업에서 공대생 3명이 기술기반 창업을 한다.이들의 창업 아이템으로 나오는 기술이 객체인식(Object detection).기술력은 좋은데 돈되는 사업을 못해서 고전하다가CEO역할을 해줄 수지(극중 서달미)를 만나서 성장하는 이야기.중반부부터 좀 뻔하게 흘러가다가 결론은 대기업 만들고 수지랑 결혼..(한줄 스포 ㅈㅅ)각설하고 요즘 수많은 기업들이 사활을 걸고 달려드는 그 기술을 알아보자.​우선 이 기술은 두 부류로 나뉜다.One stage vs Two stage나뉘는 기준은객체를 네모로 표시해주는 bounding box를 찾는 연산과 classification 연산 각각을독립적인 흐름으로 수행하거나 이미지 학습의 흐름에 포함하거나로 볼 수 있다.​One stage 진영엔 YOLO, SSD, RetinaNetTwo stage 쪽엔 R-CNN 형제들이 대표적이다. 간략하게 살펴보자.​ R-CNNR-CNN은 간단하다. 기존의 기술 두개를 섞었는데 우선1. 사진을 준비하고2. Selective Search 기술 등을 활용하여 bounding box를 찾아낸 후3~4. CNN으로 각 영역을 분류​여기서 Selective search는 Selective search1. segmentation을 통해 영역을 잘게 나누고2. 비슷한 영역을 합친 후3. 객체 후보 영역을 찾아내는 방법이다.​이 기술은 2대 R-CNN인 Fast R-CNN까지 사용되어Region proposal을 만들다가 Faster R-CNN에서 RPN(Region Proposal Network)으로 넘어가게 된다. R-CNN family진화의 키워드는feature map 재사용RoI(Region of inerest) poolingRPN(Region Proposal Network)​R-CNN의 가장 큰 문제는 가령 AlexNet을 CNN으로 사용했다고 치면 이걸 Region proposal의 수만큼 통과시켜야 했다.이를 개선하기 위해 이미지 전체의 convolutional feature map을 한번 계산하고 이로부터 원하는 위치의 tensor를 뜯어와서 sub-tensor만 효율적으로 이용하는 부분이 엄청난 효율 개선을 가져왔다. 이는 SPPNet에서도 비슷하게 사용되는데 어떤게 먼저인지는 잘 모르겠다. 해당 아이디어는 개인적으로 어떻게 구현되는지 한번 찾아볼 필요가 있을 것 같다. 지금 나로서는 이미지를 계속 CNN을 돌리는 방법밖에 모르니..​RoI pooling은 다른 pooling과 비슷한데 Selective Search로 찾은 bounding box의 정보를 압축하는 개념이다. RoI poolingRPN은 Fast R-CNN에 추가로 bouding box를 찾는 것 마저 학습을 시키자는 아이디어다.아래 그림에서 처럼 9개(hyper param)의 anchor box를 미리 정의해두고 RPNFully Convolutional Network를 활용해 이를 학습시키게 되면 최종적으로anchor box * (4k coordinates + 2k coordinates)4k coordinates = (x, y, w, h)2k coordinates = object vs non-object값을 통해 해당 영역에서 bounding box를 쓸지말지 결정짓게 된다.​더 자세한 R-CNN 관련 내용은 이 블로그에 잘 정리되어 있다.​One stage로 해결하는 YOLO(You only look once)와 그 이후의 모델들은bounding box계산과 classification이 동시에 해결된다.이미지 한장만 model을 통과시켜서 box와 classification이 동시에 나와 심플하고 속도도 굉장히 빠르다. YOLO 컨셉그림에선 양갈래지만 이걸 동시에 한다.bounding box들을 랜덤하게 배정하고 이걸 수학적인 알고리즘을 사용하여 confidence값을 주어 최종적으로 box를 찾아내게 되는데YOLOv2로 가서는 Faster R-CNN처럼 미리 anchor box를 정의해두고 가는 방향으로 더 효율적으로 계산한다고 한다. YOLO architecture실제 아키텍쳐도 하나의 모델을 이미지로 통과하면서 최종적으로Bounding box 관련 텐서와 classification 관련 텐서가 결합된 값이 도출 되는 걸 볼 수 있다.(참고로 B는 bounding box의 갯수)YOLO는 다음 블로그에 잘 정리되어 있다.YOLOv3를 체험해 볼 수 있는 github도 있는데 신기하다..ㅋㅋㅋ ​현재는 v5까지 나와있고 ViT를 하기위해 Transformer를 정의해놓고 아직 사용은 안하는거 같던데종종 들여다보면 재밌을것 같다. 조만간 YOLO의 소스코드도 구경해보자.YOLO의 다음 세대 SSD(Single Shot MultiBox Detector)도 비슷한듯 다른데 성능은 월등하다. SSD 아키텍처YOLO의 단점은 가장 마지막 레이어 에서만 prediction을 하기 때문에 localization 성능은 떨어지는 경향이 있다.고로 지난번에 살펴본 UNet과 같이 앞쪽의 특징들을 뒤로 보내주는 작업을 해주게 된다. SSD의 성능​(컨텐츠 출처: 네이버 부스트캠프 AI tech) "
"Object Detection in 20 Years - 06, RECENT ADVANCES IN OBJECT DETECTION ",https://blog.naver.com/tory0405/222852484625,20220819,"[논문 원본] 첨부파일1905.05055.pdf파일 다운로드 최근 3년 동안의 최신 물체 감지 방법을 알아보자. ​Detection with Better Engines 최근 몇 년 동안 Deep CNN은 많은 컴퓨터 비전 작업에서 중심 역할을 했다. 검출기의 정확도는 특징 추출 네트워크에 크게 의존하기 때문에 ResNet이나 VGG 같은 backbone networks을 검출기의 엔진이라고 부른다. 그림 17은 Faster RCNN, R-FCN ,  SSD과 같이 잘 알려진 검출기의 정확도를 보여준다.  ​ AlexNet 8계층 Deep Network인 AlexNet은 컴퓨터 비전에서 딥러닝 혁명을 시작한 최초의 CNN 모델로. 2012 ImageNet LSVRC-2012 대회에서 큰 차이로 우승한 것으로도 유명하다. ​ VGGVGG는 2014년 Oxford의 Visual Geometry Group (VGG)에서 모델의 깊이를 16~19개의 layer로 늘리고 Alexnet에서 사용했던 5*5, 7*7 필터 대신 3*3 컨볼루션 필터를 사용했다. ​GoogLeNetinception이라고도 하는데 2014년 Google에서 제안한 CNN 모델이다. GoogLeNet는 CNN의 너비와 깊이를 모두 증가시켰고 인수분해 및 배치 정규화에 많은 기여를 하였다. ​ResNetThe Deep Residual Networks (ResNet)은 2015년 K. He이 제안한 모델로 이전에 사용된 것보다 훨씬 더 깊은(최대 155개 layer) 새로운 유형의 컨볼류션 네트워크 아키텍처이다. ResNet은 reformulating layser를 참조하여 residual functions을 학습하는 방식으로 layser를 재구성하여 네트워크 교육을 용이하게 하는 것을 목적으로 두고 있다. ResNet은 2015년  ImageNet detection, ImageNet localization, COCO detection, COCO segmentation을 포함하여 여러 컴퓨터 비전 대회에 우승을 차지했다. ​DenseNet2017년 Huang 과  Z. Liu이 제안한 모델로 ResNet이 CNN의  short cut connection이 더 깊고 정확한 모델을 훈련할 수 있게 해준다는 사실을 바탕으로 Huang 과  Z. Liu는  이 관찰을 수용하고 각 레이어를 다른 모든 레이어에  feedforward fashion으로 연결하는 조밀하게 연결된 DenseNet을 소개했다. ​SENetSqueeze and Excitation Networks (SENet)은 2018년  J. Hu 와 L. Shen이 제안한 모델로 feature map의 채널별 중요성을 학습하기 위해    global pooling과 shuffling을 통합하는 방식이다. SENet은 ILSVRC 2017에서 1위를 차지했다. ​최근에는 STDN, DSOD, TinyDSOD, Pelee에서 감지 엔진으로 DenseNet을 선택했고  Mask RCNN에서는 인스턴스 분할을 위해 차세대 ResNet인 ResNeXt를 선택하였다. 그뿐만 아니라 탐지 속도를 개선하기 위해 Incepion의 개선 버전인 Xception의 도입으로 깊이별 분리 가능한 컨볼루션 연산이  MobileNet과 LightHead RCNN에 사용되기도 했다. ​Detection with Better Featuresfeature 표현의 품질은 객체 감지에 매우 중요하다. 최근 최신 엔진을 기반으로 이미지 feature 품질을 더욱 향상시키기 위해 가장 중요한 부분이 1) feature fusion와 2) learning high-resolution features이라고 할 수 있다. ​Invariance 과 equivariance은 이미지 feature 표현의 중요한 2가지 속성이다. 높은 수준의 의미 정보를 학습하는 것을 목표로 하기 때문에 변종 feature 표현에서 분류가 필요하다. 또한 객체의 localization은 위치 및 스케일 변경을 식별하는 것을 목표로 하기 때문에 equivariance 요구되고 객체 감지의 경우 객체 인식과 위치 파악이라는 두 가지 작업으로 구성됨으로 감지가가 Invariance 과 equivariance을 동시에 학습하는 것이 feature fusion이 중요한 이유이기도 하다. ​CNN 모델은 일련의 컨볼류션 및 fulling layer로 구성됨으로 깊은 layer에서는 불변성은 강하지만 등 분산에는 약하다. 이로 인하여 범주 인식에는 도움이 될 수 있지만 객체 감지에 위치 정확도는 어려울 수밖에 없는 이유다.  반대로 얕은 layer는 의미를 학습하는 데 도움은 되지 않지만 가장자리와 윤곽에 대한 더 많은 정보를 포함하고 있어 객체 위치 파악에 도움이 된다. 따라서 CNN 모델에서 깊은 layer의 기능과 얕은 layer의 기능을 통합하면 불변성과 등변성을 모두 개선하는 데 도움이 된다. ​객체 감지에서 feature fusion을 수행하는 방법은 여러 가지가 있는데 최근에는 크게 1) processing flow 과  2) element-wise operation으로 나눌 수 있다. ​processing flow 최근 객체 검출에서 feature fusion은 그림 18(a)-(b)과 같이 1) bottom-up fusion, 2) topdown fusion이 있다. bottom-up fusion은 건너뛰기 연결을 통해 얕은 feature를 더 깊은 layer로 전달한다. 이에 비해 topdown fusion은 더 깊은 layer의 feature를 더 얕은 층으로 피드백 한다.  다른 layer에서 이 feature map은 공간 및 채널 크기가 다를 수 있으므로 채널 수를 조정하거나 고 해상도 map을 적절한 크기의 저 해상도 map으로  upsampling 하거나 down sampling 할 수 있어야 하는데 가장 쉬운 방법이 nearestor bilinear-interpolation이다. 또한 fractional strided convolution (a.k.a. transpose convolution)은 자체적으로 up sampling을 수행하는 적절한 방법을 학습할 수 있기 때문에 기능 맵의 크기를 조정하고 채널 수를 조정하는 또 다른 인기 있는 방법이라 할 수 있다. ​element-wise operation로컬 관점에서 feature fusion은 서로 다른 feature map 간의 요소별 작업으로 간주할 수 있는데. 그림 18(c)-(e)와 같이 : 1) element-wise sum, 2) element-wise product,  3) concatenation으로 나눌 수 있다. ​element-wise sum은 feature fusion을 수행하는 가장 쉬운 방법이다. 최근 많은 물체 검출기에서 자주 사용되었다. element-wise product은 element-wise sum와 매우 유사하지만 합산 대신 곱을 사용하는 차이가 있고 특정 영역 내의 feature 억제하거나 강조 표하는데 장점이 있다. 이를 통해 작은 물체 감지에 도움이 된다.  concatenation은 feature fusion의 또 다른 방법으로 서로 다른 영역의  context information를 통합하는 데 사용할 수 있는 장점과 메모리 사용이 증가하는 단점이 있다. ​receptive field 와 feature resolution는 CNN 기반 검출기의 두 가지 중요한 특성이다. receptive field는 출력의 단일 픽셀 계산에 기여하는 입력 픽셀의 공간 범위를 나타내고  후자는 입력과 feature map 사이의 하향 조정 속도를 의미한다.  더 큰 receptive field를 가진 네트워크는 더 규모의  context information를 캡처할 수 있는 반면 더 작은 receptive field를 가진 네트워크는 로컬 세부 정보에 더 집중할 수 있다. ​feature resolution이 낮을수록 작은 물체를 감지하기가 더 어렵다. feature resolution를 높이는 가장 직접적인 방법은 pooling layer를 제거하거나 컨볼루션 down sampling rate를 줄이는 것이지만  output stride의 감소로 인해 feature resolution가 너무 작아지게 만들어 일부 큰 물체의 감지를 놓칠 가능성이 존재하게 되는 단점이 존재한다. ​receptive field 와 feature resolution를 동시에 향상시킬 수 있는 방법은 컨볼루션 필터를 확장하고  sparse parameters를 사용하는 dilated convolution (a.k.a. atrous convolution, or convolution with holes)을 사용하는 것이다. 예를 들어 팽창률이 2인 3*3 필터는 5*5 필터와 동일한 receptive field를 갖지만 dilated convolution은 매개변수가 9개만 사용하기 때문에 추가 매개변수나 계산 비용 없이 정확도를 개선하는데 효과적인 것으로 입증되었다. ​ Beyond Sliding Window객체 감지가 손으로 만든 기능을 사용하는 것에서 deep  neural network으로 발전했지만 감지는 여전히 “sliding window on feature maps”을 사용하고 있었지만 최근에는 sliding window를 넘어 새로운 감지 방법이 나오기 시작했다. ​Detection as sub-region search는 탐지를 수행하는 새로운 방법을 제공한다. 탐지를 초기 그리드에서 시작하여 최종적으로 원하는  ground truth boxes로 수렴하는 path planning process를 예로 들 수 있다. ​Detection as key points localization 은 얼굴 표정, 포즈 식별과 같이 광범위하게 응용된다. 이미지의 모든 물체는 ground truth boxes의 왼쪽 상단 모서리와 오른쪽 하단 모서리로 고유하게 결정될 수 있으므로, 탐지 작업은 pair-wise key points localizationproblem로 동등하게 처리될 수 있다​Improvements of Localization  localization 정확도를 개선하기 위해 1) bounding box refinement, and 2) designing new loss functions 을 사용한다. ​bounding box refinement은 검출 결과의 후처리 과정으로 지역화 정확도를 개선하기 위한 가장 직관적인 방법 중 하나이다.  bounding box regression이 대부분 최신 객체 감지에 사용되었지만 사전 정의된 앵커에 의해 감지되지 않는  예상치 못한 크기를 가진 물체들이 여전히 존재하기 때문에 부정확한 예측으로 이어지게 된다. 이를 해결하기 위해 “iterative bounding box refinement” 사용하여 예측이 가능한 위치와 크기로 수렴될 때까지 탐지 결과를 BB regression에 반복적으로 사용하는 방법을 제안하였으나 반복적 BB regression이 국소화를 저하 시킬 수 있다는 주장도 존재하기도 한다. ​대부분 최신 검출기에서 object localization은  coordinate regression problem으로 간주되고 있지만 첫째 regression loss function은  localization에 대한 최종 평가에 해당하지 않는다. 예를 들어, 객체의 가로 세로 비율이 매우 클 때 낮은 회귀 오차가 항상 더 높은 IoU 예측을 생성한다고 보장할 수 없다는 것이고 ​둘째, 전통적인 bounding box regression은 복수의 BB가 서로 겹치는 경우 국소화에 대한 확신을 하지 못하기 때문에   non-maximum suppression에서  실패로 이어질 수 있다는 점에서  coordinate regression에 단점이 존재한다. 첫 번째 경우와 두 번째 경우를 완화하기 위해서는 IoU를  localization loss function로 직접 사용하는 새로운 loss funtion이 필요하다.  일부 다른 연구자들은 locatlization을 향상시키기 위해  IoU-guided NMS를 제안한 경우도 있고 probabilistic inference framework에서 locatlization을 향상시키기 위해 노력했다. ​Learning with Segmentation최근에는 semantic segmentation을 통해 객체 감지를 개선할 수 있는 많은 방법이 제안되었는데  semantic segmentation은 객체 감지를 향상 시크는 3가지 이유가 있다. ​첫 번째,  segmentation은 카테고리 인식을 돕는다. 즉 가장자리와 경계는 인간 시각적인 인지를 구성하는 기본 요소이다. 컴퓨터 비전에서 자동차, 사람과 하늘, 무, 풀의 차이는 전자는 일반적으로 폐쇄되고 잘 정의된 경계를 갖는 반면 후자는 그렇지 않다는 것이다. ​두 번째,  segmentation은 정확한 localization를 돕는다. 객체의 실제  ground-truth bounding box는 잘 정의된 경계에서 결정된다. 특이한 모양을 가진 일부 객체의 경우 높은 IoU 위치 예측이 힘들 수 있다. (예를 들어 꼬리가 매우 긴 고양이 ) 이로 인해 분할을 통한 학습은 정확한 객체 위치 파악에 도움이 된다. ​세 번째, Segmentation은 context로 포함될 수 있다. 일반적으로 객체는 하늘, 물, 잔디와 같은 다양한 배경으로 둘러싸여 있으며 이러한 모든 요소가 context로 구성된다.  이렇게  semantic segmentation의 맥락을 통합하면 물체 감지에 도움이 될 수 있다. 예를 들어 항공기는 물보다 하늘에 나타난 다와같이.. ​Segmentation에 의한 객체 감지를 개선하기 위한 2가지 접근 방식은 1) learning with enriched features 과  2) learning with multi-task loss functions이다. ​learning with enriched features의 가장 간단한 방법은 분할 네트워크를 고정된 feature 추출기로 생각하고 이를 추가 feature 로로 감지 프레임워크에 통합하는 방식이다. learning with multi-task loss functions은 기존 감지 프레임워크 위에 추가 분할 분기를 도입하고 이 모델을 multi-task loss function으로 훈련시키는 것으로 감지 속도에 영향을 받지 않는 장점이 있다. 하지만 훈련과정에 pixel-level image annotations이 필요하다는 단점이 존재하는데 일부 연구자들이  “weakly supervised learning”이라는 아이디어를 통해 해결했다. 즉 pixel-level image annotations 기반으로 훈련하는 대신  bounding-box level  기반으로  segmentation brunch를 훈련한 것이다. ​ Robust Detection of Rotation and Scale ChangesCNN이 학습한 특징은 회전과 스케일의 큰 변화에 불변하지 않는다. 그리고 객체 회전은 얼굴 감지, 텍스트 감지 등과 같은 감지 작업에서 매우 일반적이다. 이 방법은 모든 방향의 객체가  데이터로 처리될 수 있도록  데이터를 증가하는 방식이 있고 또 다른 방식은 모든 방향에 대해 독집적으로 훈련하는 방법이 있다. ​Rotation invariant loss functions은 1990년대로 거슬러 올라갈 수 있는데 회전된 객체의 feature를 변경하지 않도록 original  detection loss function에 대한 제약을 도입한 것이다. ​Rotation calibration은 객체 후보의 기하학적 변환을 만드는 것으로 초기 단계의 상관계수가 다음 단계에 도움이 되는 다단계 탐지기에 유용하다. 대표적으로  Spatial Transformer Networks (STN)이 있는데 회전된 텍스트나 회전된 얼굴을 감지하는 데 사용된다. ​Rotation RoI Pooling은 그리드 세트로 균등하게 나눈 다음 그리드 feature을 연결하여 위치와 크기에 관계없이 객체 제안에 대한 고정 길이 feature 표현을 추출하는 것을 목표로 한다. 그리드 메시는 데카르트 좌표에서 수행되기 때문에 feature는 회전 변환을 통해 변화게 된다. 최근에는 극좌표에서 격자를 매시 하여 회전에 강건할 수 있게 처리하였다. ​대부분 최신 감지기는 그림 19(a)와 같이 입력 이미지를 고정 크기로 조정하고 모든 크기에서 객체의 손실을 역전파 한다. 하지만 이 원리는  “scale imbalance”문제가 발생하는 단점이 있다.  탐지하는 동안 이미지 피라미드를 구축하면 이 문제를 완화할 수 있지만 근본적인 해결책은 되지 못한다. 최근에 Scale Normalization for Image Pyramids (SNIP)은 19(b)와 같이 훈련 및 감지 단계 모두에게 이미지 피라미드를 만들고 일부 선택된 scale의 loss만 역전파 하는 방식으로 해결했고 나아가 SNIP with Efficient Resampling (SNIPER)을 통해 이미지를 자르고 하위 영역 집합으로 재조정하는 방식으로 더 효율적인 방법을 제안하였다. ​​대부분 감지기는 다양한 크기의 물체를 감지하기 위해 fixed configurations을 사용한다. 예를 들어 CNN 기반 검출기에서는   anchor의 크기를 정의하고 있지만 예기치 않는 scale의 변경에 adaptive 할 수 없는 단점이 있다.  또 작은 물체 감지의 향상을 위해 “adaptive zoom-in”과 같은 기술을 사용하였고 또 다른 방법으로는 이미지에서 객체의 스케일 분포를 예측하고, 그 분포에 따라 이미지를 적 adaptive 하게 쟤 스케일링하는 것을 학습하게 하는 것이 있다. ​Training from Scratch대부분의 deep learning 기반의 감지기는 imageNet과 같은 대규모 데이터 세트에서 먼저 사전 훈련한 다음 특정 탐지 작업에 대해 미세 조정한다. 하지만 실제 상황에서 객체 감지에 사전 훈련된 네트워크를 적용할 경우 몇 가지 제한 사항이 존재한다. 첫 번째는 손실 함수 및 스케일/카테고리 분포를 포함하여 ImageNet 분류와 객체 감지 간의 차이이고 두 번째는 도메인 불일치이다. 이는 ImageNet의 이미지는 RGB 이미지이고 실제 감지는 깊은 이미지( RGB-D) 또는 3D 이미지도 존재하기 때문이다. 최근 몇 년 동안, 일부 연구원들은 물체 감지기를 Scratch로부터 훈련시키려고 노력해왔다. 훈련 속도를 높이고 안정성을 향상시키기 위해 dense connection과 batch normalization을 통해 얕은 layer로 역전파를 가속화하도록 하였다.  최근에  K. He은 랜덤 한 초기화에서 훈련된 표준 모델을 사용할 경우 훈련 데이터의 10%만 사용하더라도 놀라운 결과를 가져온다는 사실을 알게 되었다. 하지 마 이 방법은 훈련 속도는 높일 수 있지만 반드시 정규화를 제공하거나 최종 탐지 정확도를 향상시킨다는 의미는 아니다. ​ Adversarial Training  A. Goodfellow에 의해 알려진  Generative Adversarial Networks (GAN)은 최근 몇 년 동안 엄청난 주목을 받았다. 일반적인 GAN은  minimax optimization framework에서 서로 경쟁하는 generator networks 과 discriminator networks의 두 개의 network으로 구성되어 있다. generator network은 잠재 공간에서 관심 있는 feature 데이터 분포로 매핑하는 방법을 학습하는 반면 discriminator networks는 실제 데이터 분포의 인스턴스와 generator networks에 의해 생성된 인스턴스를 구별하는 것을 목표로 하고 있다.  GAN은  image generation, image style transfer,  image super-resolution에 널리 사용되고 있고 특히 최근 2년 동안 GAN은 매우 작거나 가려진 물체 감지 개선에 많이 적용되었다. 그리고 GAN은 작은 물체와 큰 물체 사이의 표현을 좁혀 작은 물체에 대한 탐지를 향상시켰고 가려진 물체의 감지 향상을 위해 adversarial training을 이용하여 occlusion masks을 생성하는 방식을 이용하였다. ​Weakly Supervised Object Detection최근 물체 감지기의 훈련에는 일반적으로 수동으로 생성된 lable이 지정된 많은 양의 데이터가 필요하지만 lable을 지정하기 위해서는 시간과 비용이 상당히 많이 들고 비효율적이다. Weakly Supervised Object Detection (WSOD)는 bounding boxes 대신  image level annotations만을 이용하여 감지기를 훈련하는 것을 목표로 한다. 최근에는 multi-instance learning은 WSOD에 사용되는  supervised learning의 한 분야이다.  multi-instance learning은 개별적으로 레이블이 지정된 인스턴스 세트로 학습하는 대신 각각 많은 인스턴스를 포함하는 레이블이 지정된 bag 세트로 학습한다. Class activation mapping은 WSOD에 대한 또 다른 방법 중 하나이다. CNN의 경우 컨볼루션 layer는 객체 위치에 대한 supervision이 없음에도 불구하고 객체 감지기로 동작하는데. Class activation mapping이 이미지 레벨 레이블에 대한 교육을 받았음에도 불구하고 CNN이 localization 기능을 가질 수 있도록 하는 방법이라 할 수 있다.  "
Object Detection + Power Apps _ Demo Video ,https://blog.naver.com/sesme100/222181431182,20201222,#MicrosoftPowerPlatfrom#MicrsoftPowerApps#MicrosoftPowerAutomate#MicrosoftRPA#CitizenDeveloper#MSRPABeginners​#ObjectDetection#Lists#SubmitToLists​안녕하세요 Microsoft Citizen Developer 진미나 입니다. 이전에 AI Builder로 내가 좋아하는 과자들을 개체감지(Object Detection) 해본 내용과 더불어 상점들의 주소나 위도 경도 정보를 가져와서 Bing Maps API로 지도를 구현하고 Lists로 데이터를 보내는 내용을 비디오 데모로 만들어 보았어요!! 보시고 함께 구현해보자고요!!   ​​이전에 제가 Object Detection을 Hands-on으로 구현한 내용을 포스팅 하였습니다. 꼭 들러서 확인하여주세요!https://blog.naver.com/sesme100/222167433410 AI Builder로 최애 소주 인식하기! (Object Detection)#MicrosoftPowerPlatfrom#MicrsoftPowerApps#MicrosoftPowerAutomate#MicrosoftRPA#Citizen...blog.naver.com ​Bing Maps API를 이용하여 지도를 구현한 내용입니다. 신기방기해요!!​https://blog.naver.com/sesme100/222171010732 Microsoft Bing Maps API를 이용하여 지도 연결 하기#MicrosoftPowerPlatfrom#MicrsoftPowerApps#MicrosoftPowerAutomate#MicrosoftRPA#Citizen...blog.naver.com ​ 데모에 구현된 내용은 아직 다 못보여 드려서 너무 아쉬워요 ㅜㅜ 다음에는 더 많은 내용으로 돌아올게요!!​#EdgeofCitizenDeveloper#나도하면너도할수있어#YoucandoitasIdo 
R-CNN 모델(Object detection) ,https://blog.naver.com/grow_bigger/222761375505,20220606,"AbsctractR-CNN은 정체되어있던 Object detection에서 획기적인 변화를 가져온 모델입니다.""Rich feature hierarchies for accurate object detection and semantic segmentation""에서 소개되었습니다. Since we combine region proposals with CNNs, we call our method R-CNN: Regions with CNN features.Rich feature hierarchies for accurate object detection and semantic segmentation이름은 region proposal 방법을 사용해서 R-CNN입니다. sliding-window방식에서 효율면에서 더 개선하려한 방식으로 물체가 있을만한 위치를 찾아내는 방법입니다.   An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each positionFaster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks이 모델 이후에 나온 Faster R-CNN논문에서는 위와 같이 region proposal network를 물체의 경계와 물체가 있을법한 정도를 동시에 예측하는 완전 합성곱 신경망이라고 소개하고 있습니다. R-CNN의 경우 bottom-up region proposal(-> selective search)를 사용했고 라벨(정답)이 적은 경우 supervised pre-training을 적용할 수 있습니다.​*pre-training: 다른 문제나 데이터로 학습되어 설정된 가중치들을 이용하여 모델의 가중치를 초기화하는 방법입니다. 라벨이 없는 데이터가 많은 경우(라벨이 있는 데이터가 적은 경우) 라벨이 없는 데이터로부터 표현을 학습하여 라벨이 있는 적은 수의 데이터에 과적합되는 것을 막을 수도 있고, 비지도 사전 학습을 이용하여 라벨이 있는 데이터의 적음으로 인한 문제를 완화할 수 있습니다.  Introduction기존에는 SIFT나 HOG에 기반하여 시각 인식이 발전했는데 더뎠습니다. SIFT는 간단히 말해서 스케일과 회전에 강건함을 가지는(회전과 크기 변화에도 잘 작동하는) 알고리즘입니다. 그러다가 Fukushima’s “neocognitron”를 시작으로 계층구조를 활용하며 패턴 인식을 하기 시작했습니다. ​이러한 경향 속에서 본 논문은 물체를 찾고 적은 라벨링된 데이터를 가지고 객체 탐지를 수행하는 것을 목표로 하였습니다. 서론에서도 밝혔듯 R-CNN이라는 이름은 'Regions with CNN features'에서 명명한 것입니다. 즉, 객체탐지의 두 가지 과제인 물체 위치 찾기외 물체 종류 구분하기 중 물체 위치 찾기를 수행할 때 CNN을 활용한다는 의미가 모델의 이름에 담겨있습니다. 즉, 한가지 목표인 물체를 찾는 작업에 기존의 'sliding-window'방식 대신 다른 방식을 고안하게 됩니다. 결론적으로는 Selective search를 이용하여 ROI(A region of interest are samples within a data set identified for a particular purpose. from Region of interest - Wikipedia)를 산출한 후 CNN층을 이용하여 고정 길이 특징 벡터를 추출합니다. 또 다른 목표는 적은 데이터로 인한 문제를 해결하는 것인데, 논문의 저자들은 주로 데이터가 부족할때 사용하는 비지도 사전학습 대신 보조 데이터셋에서 지도 사전학습을 하고  목표 데이터셋에 맞춰 튜닝을(파라미터 조정) 해줌으로써 해결하였습니다. 출처: Rich feature hierarchies for accurate object detection and semantic segmentatio위 사진을 보시면 어떤 작업을 하시는지 알 수 있습니다. region proposal을 하여 있을 법한 부분을 localizing해주고 CNN층을 통과시켜 고정 길이 벡터를 산출하면 이미지 부분에 대한 특징들이 압축되어 표현됩니다. 그러면 이 특징 벡터를 가지고 SVM을 통해 분류를 하게 되며, 이로써 Object Detection이 완료됩니다. 참고로 4번 단계에서 SVM은 multi-class classification이 아닌 이진분류기 때문에 각 분류에 대해 맞다/아니다를 판단해 맞는 분류를 찾아가야 합니다. 논문의 2.4부분에 나오는데 R-CNN은 linear한 SVM을 사용했고 비슷한 알고리즘을 쓰는 다른 모델(UVA system)의 경우 non-linear kernel SVM을 이용했는데 타 모델에 비해 더 좋은 성능을 냈다고 합니다.​참고로 상당히 효율적이고 약간만 수정하면 semantic segmentation을 수행할 수 있다고 합니다.모든 합성곱 층 파라미터가 모든 카테고리에 걸쳐 공유되고, 특징 벡터가 다른 모델에 비해 저차원이기 때문에 되기 때문에 논문에서 비교한 UVA system에 비해 컴퓨팅 시간이 크게 줄고, 메모리도 훨씬 적게 쓰게 됩니다. 그래서 논문에서는 2000개의 카테고리에 대해 다루었지만 규모를 더 키운다고 해도 134GB 메모리가 필요한 UVA system에 비해 1.5GB정도인 굉장히 적은 메모리를 차지합니다.​​ Object detection with R-CNN We extract a 4096-dimensional feature vector from each region proposal using the Caffe [21] implementation of the CNN described by Krizhevsky et al. [22].Rich feature hierarchies for accurate object detection and semantic segmentationR-CNN에서 CNN의 구조는 AlexNet의 구조를 썼습니다.  Alex Khrizevsky가 첫번째 저자라서 AlexNet입니다.CNN의 입력 크기가 정해져있기 때문에 input을 (227,277)크기로 warp해줍니다. 다만 warp하기전 주변 context(배경)을 16pixel만큼 원본주변에 있도록 박스를 넓혀줍니다. ​데이터를 CNN에 input으로 넣기 위해 크기를 맞춰주는 작업을 했습니다. 그러면 훈련을 해야겠죠.훈련은 다음과 같습니다.Supervised pre-training. : 보조 데이터셋으로 CNN을 사전 지도 학습합니다.Domain-specific fine-tuning. : SGD로 학습률을 0.001으로 하여 1에서 사전학습한 CNN을 지금 새롭게 해결하고자 하는 과제에 맞게 튜닝(파라미터 조정)을 합니다.Object category classifiers. : 사각형 영역이 목표 객체의 일부만을 담고 있는 경우를 위해 Grid Search를 통해 임계치를 정하고, 클래스당 하나의 SVM을 최적화합니다.(이때 standard hard negative mining method를 적용합니다.)​Visualization, ablation, and modes of errorAblation studies이 section에서 중요한 부분은 ablation studies입니다. ablation studies는 전체에서 일부를 제거하고 결과를 보면서 해당 부분이 어떤 역할이나 중요성을 가지고 있는지 보는건데 이 논문에서는 뒤쪽에 붙은 완전연결층에 주목합니다.  More surprising is that removing both fc7 and fc6 produces quite good results even though pool5 features are computed using only 6% of the CNN’s parameters. Rich feature hierarchies for accurate object detection and semantic segmentatioCNN구조의 완전 연결층(fully connected layer) 2개를 빼도 꽤 좋은 결과를 낸다고 합니다. 아마 완전연결층의 중요성이 낮아진다면 제거하고 임의의 input을 받는 모델을 만들수도 있을 것같습니다.완전연결층은 unit의 개수가 정해져있어 고정길이를 입력으로 받는데 RNN도 그렇고 길이가 고정되면 입력에 제한이 생겨 여러모로 제약이 많아집니다.​Detection error analysis​ 위 사진은 R-CNN이 주로 만드는 오류입니다. false positive는 잘못 positive라고 예측한 유류인데, 주로 LOC(poor localizing)가 주를 이루는 것을 볼 수 있습니다. 이러한 느슨한 localization은 bottom-up region proposals과 전체이미지 분류를 위한 CNN 사전학습으로 부터 학습한 positional invariance때문이라고 합니다.그러니까 이미지 분류를 위해 위치가 움직여도 같은 물체라고 인식하게 하려다보니 box를 넓게(느슨하게) 잡게하고 그러다보니 poor localizing이 발생한다는 것 같습니다.​Semantic segmentation 논문의 앞에서도 조금씩 지속적으로 언급되었는데 semantic segmentation에의 적용도 논문의 마지막에 다루고 성능의 우수함도 보여주었습니다. 세가지의 전략을 사용했다고 하는데 아래와 같습니다.The first strategy (full) ignores the region’s shape and computes CNN features directly on the warped window the second strategy (fg) computes CNN features only on a region’s foreground mask.The third strategy (full+fg) simply concatenates the full and fg features3번은 1+2의 전략이므로 사실상 2가지의 접근법을 사용한 것입니다.  결과는 둘 다 적용한 3번이 가장 우수합니다.  Conclusion이 논문은 객체 탐지에 대한 발전이 정체되있던 흐름 속에서 저차원 이미지 특징을 고차원 맥락과 섞는 복잡한 앙상블 체계를 뛰어넘었습니다. 초록에서 밝힌 것처럼 CNN을 이용했고 적은 데이터를 가지고도 학습하는 방법을 제안함으로써 성능을 크게 높였습니다. 논문의 저자들은 컴퓨터 비전과 심층학습이 서로 다르거나 상관없는 것이 아닌 피할 수 없는 파트너라고 말합니다. (CNN을 컴퓨터 비전에 적용한 논문이라는 걸 고려하면 가장 핵심적인 문장인 것 같습니다.)​감사합니다.  [세부사항 / 용어 설명]논문을 읽으면서 필요한 배경 지식을 따로 정리하였습니다. SVM, SVR, NMS등 논문에서 적용한 다양한 방법들입니다.R-CNN 모델(Object detection.. : 네이버블로그 (naver.com)  [참고자료]1~2R-CNN : Region-based Convolutional Networks forAccurate Object Detection and Segmentation 리뷰 (tistory.com)ImageNet Classification with Deep Convolutional Neural Networks (neurips.cc)Ablation study란 무엇인가? :: 헤헤 (tistory.com)34. DPM (Deformable Part Model) :: Time Traveler (tistory.com)랜덤포레스트(Random Forest) :: BioinformaticsAndMe (tistory.com)41편: Semantic Segmentation 첫걸음!. Semantic Segmentation이란? 기본적인 접근 방법은? | by 심현주 | Hyunjulie | Medium "
미르기술 Advanced SPI Technology : Foreign Object Detection (FOD) ,https://blog.naver.com/marketingmirtec/222767565670,20220610,"사진 1. 미르기술 SPI 이물검사미르기술 MIRTEC은 변화하는 산업 환경에 맞춰 고객들에게 정교하고 정확한 검사 결과를 보장하기 위하여, 미르기술 SPI 장비의 기능을 업그레이드하였습니다.  ▮ 이물 검사 Foreign Object Detection (FOD)​이물 검사 혹은 FOD라고 불리는 기능은 SPI 장비에서 PCB를 검사할 때 고객들에게 이전과 다른 검사 결과를 가져다줄 것입니다.​미르기술 MIRTEC의 이물 검사(FOD) 기능은 현재 고객들의 다변화된 작업 환경 속에서 발생할 수 있는 다양한 종류의 이물을 감지해 검사 오류를 최소한으로 줄이고 가장 정확하고 정교한 검사 결과를 보장합니다.​위의 사진 1과 같이 이물 검사(FOD) 기능은 검사 환경에서 빈번히 나타나는 플라스틱 조각, 금속 조각, 슬러지(Sludge), 머리카락, 먼지, 오염(얼룩) 등을 포함한 다양한 크기와 형태의 이물을 검출 가능합니다. 또한, SPI 장비의 렌즈별 최소 검출 가능 사이즈가 다르기 때문에 이물 검사(FOD) 기능에 관심이 있으신 고객분들께서는 보다 자세한 정보를 위해 미르기술 MIRTEC으로 연락해 주시기 바랍니다. ​(※ 단, 이물 검사(FOD)를 실시할 경우 이물 개수(프레임당) 별 추가 검사 시간이 발생하는 점을 주의해 주시기 바랍니다.)  미르기술은 미르기술을 이용하시는 모든 고객들에게 더욱 정확하고 정교한 검사 결과를 제공하기 위해 항상 최선의 노력을 다하고 있습니다. 위에서 설명해 드린 이물 검사(FOD)를 비롯한 미르기술의 다양한 최신 기술과 제품에 대해 궁금하신 점이 있으시면 미르기술로 연락해 주시기 바랍니다.​​미르기술 정보 및 장비 문의📲 1544-1062  💻 marketing@mirtec.com​  💻 미르기술 홈페이지​ 미르기술Technology 실력이 뒷받침되는 정성의 철학으로 끊임없이 기술을 개발하고 최상의 서비스를 제공합니다. Smart Factory Solutions 스마트팩토리 솔루션 AOI 자동 광학 검사기 SPI 납 도포 검사기 SEMI / LED 반도체 / LED 검사기 Smart Factory Solutions 미르기술의 소프트웨어 솔루션 Intellisys®는 검사 데이터를 장기간 누적 취합하여 빅데이터를 구성하고, 통계적 방법론을 통해 해석하여 불량의 근본 원인을 추적합니다. 또한 원격지에서 장비를 관리ㆍ제어하여 문제를 해결함으로써 공...mirtec.com MirtecUSA - Distributors of Automated Optical Inspection sytems including Desktop AOI, Inline AOI Equipment.Welcome to MIRTEC With over 17,000 systems installed throughout the world and having received a total of 43 Industry Awards thus far for its products and services, MIRTEC has earned a solid reputation as one of the most progressive and dynamic suppliers of Automated Optical Inspection equipment to t...mirtecusa.com Mirtec Europe - Distributors of Automated Optical Inspection SystemsWelcome to MIRTEC Over the past several years, MIRTEC has earned a solid reputation with leading OEM and EMS companies throughout the world for their TECHNOLOGICALLY ADVANCED  Inspection Systems. MIRTEC products have been extremely successful in high-volume markets including cell phone and MP3 playe...mirteceurope.com 日本ミルテック株式会社｜プリント基板実装と半導体の外観検査装置専業メーカー｜東京都中央区--> 2022.3.28 2022年6月15日～17日に、東京ビックサイトで開催される 電子機器トータルソリューション展2022 に出展します。 詳細は追って掲載させていただきます。 2020.10.29 2021年1月20日～22日に、東京ビックサイトで開催される エレクトロテストジャパン に出展します。 詳細は追って掲載させていただきます。 2019.12.3 2019年12月より新大阪駅から徒歩8分の第8新大阪ビルに大阪営業所を開所しました。 2019.12.2 2020年1月15日～17日に、東京ビックサイトで開催される エレクトロテストジャパン に弊社装置を出展します。 出展ブース...mirtec-j.com ​ "
[논문리뷰] Unsupervised Salient Object Detection with Spectral Cluster Voting (CVPRW 2022) ,https://blog.naver.com/sounghyn105/222786447414,20220624,"최근 salient object detection에 대해서 관심이 생겨 관련 논문을 읽고 있다. 2편 정도를 리뷰해 볼까 한다.​Salient Object Detection이란?한국어로 번역하기 정말 애매한데 그냥 그림에서 가장 주목되는? 부분을 골라 내는 것이다. 이렇게 ㅎㅎㅎㅎ.뭐라 설명하기 되게 애매한데 그림으로는 한 눈에 알아낼 수 있따.​이번에 리뷰할 논문은 unsupervised sod, 즉 비지도 학습 기반 모델이다.이 논문의 contribution은 3가지이다.1) Spectral clustering 을 통하여 주목을 받을만한 salient object 후보군을 뽑는다.2) winner-takes-all 시스템, 즉 1) 로 부터 만들어낸 objects 후보 mask들 중 가장 높은 vote를 받은 클러스터 하나를 선택한다. 자세한건 아래 mehtods에서...3) pseudo ground truth를 따로 만들어 이를 self mask와 비교하여 학습을 지도한다.​Related work​- Self-supervised representation learning : 자기지도 학습으로 labeling이 필요 없이 이미지를 분석하는 방식.self-supervised transformer에 영감을 받았다고 하는데 구체적으로 어떤 부분에서 영감을 받았는지는 모르겠다.https://www.youtube.com/watch?v=xJkotCvT71g self-supervised vison transformer에 대한 논문 리뷰한게 있는데 관심 있으면 참고 바람.​- Unsupervised 방식여러가지 중에 이 논문에서 차용한 noisy supervison with pseudo-labels를 알아보자. 기존에는 DNN으로 psudo-label, 즉 가짜 라벨링을 만들어낸 work가 있었다. 또 모델이 반복적으로 pseudo-label을 refine하는 iterative-refinement technique가 있다.​Method 전체 그림을 foreground와 background로 나누는 것이 목적이다. 먼저, spectral clustering 을 통해 k개의 cluster를 얻는다. clustering을 하기 전에는 이미지를 encoder(cnn 계열이나 transformer계열을 사용한다고 함)를 거쳐 dense feature로 만들어준다. 논문에서 clustering과정을 자세히 나와있는데, 선형대수가 중요함을 다시금 깨닫게 한다.https://en.wikipedia.org/wiki/Spectral_clustering Spectral clustering - WikipediaSpectral clustering From Wikipedia, the free encyclopedia In multivariate statistics , spectral clustering techniques make use of the spectrum ( eigenvalues ) of the similarity matrix of the data to perform dimensionality reduction before clustering in fewer dimensions. The similarity matrix is prov...en.wikipedia.org # 라플라시안 행렬​이제 winner-takes-all voting을 통해서 이 클러스터중 어떤 cluster를 object로 표현할 것인지 골라내야한다.clustering을 통해 n개의 feature들을 k개의 cluster로 나눈다. (총 n*k개의 클러스터가 있는셈) 그리고 너비나 높이가 원래 image랑 비슷한 cluster를 제외한다. 왜냐하면 그렇게 큼지막 한거는 background일 확률이 높기 때문그런다음 나머지 클러스터를 전부 1:1로 비교하여 similarity를 계산한 후 가장 겹침의 정도(?)가 많은 후보 1개를 설정한다. 이게 winner-takes-all 전략의 핵심.​마지막으로 이렇게 얻어낸 Pseudo mask와 비교할 mask를 얻어내야한다.자세한 구조는 SelfMask라고 위의 그림에 나와있다. 먼저 아까와 마찬가지로 image encoder를 통해서 image feature f(I)를 얻어내고 그것을 pixel decoder로 넘겨 g(f(I))를 만들어낸다. f(I)는 또 위의 transformer decoder에 들어가서 query q와 만나게 되고, 이것이 g(f(I))와 곱해져 attention을 주어 총 nq(쿼리의 개수)만큼큼의 후보 mask가 탄생한다.  이렇게 식으로 나타낼 수 있다. i는 쿼리의 개수, 쿼리는 '예측'이라고 생각하면 쉽다. ​또 아까 얻은 쿼리를 mlp와 sigmoid를 거쳐서 objectness score, 해당 쿼리가 실제 마스크인가에 대한 점수?를 구한다.​이제 loss  function만 남았다.loss function은 Lmask와 Lrank로 나누어져있다. ​Lmask는 M1~Mnq 마스크 후보군을 아까 offline network에서 구한 Mpsuedo와 비교해 가장 닮은거부터 안닮은것까지 index를 오름차순으로 re-ordering한다. 왜 re-ordering을 하는가? 그것은 바로 다음에 나올 score를 분석하기 위해서다.Lrank는 동일한 수의 objectness score를 위와 같은 순서로 re-ordering한 후 objectness score를 비교해 hinge loss를 구한다.  이렇게 oi (pseudo mask와 비슷한 후보)가 다른 것들보다 점수가 높아지도록 유도한다.​Experiments​데이터셋: DUTS-TR, DUT-OMRON 등등당연한 얘기지만 pseudo-mask를 만들고 training할때 labeling을 하지 않았다고 한다.​evaluation metrics: IOU, pixel accuracy(Acc), Fβ score​- Clusering 기법 : kmeans vs spectral clustering  Self-supervised model의 경우 모든 모델에서 spectral이 압승을 하였다. ​하지만 fully-supervised 모델의 경우 k-means가 더 좋은 성능을 보였다.​  또한, winner-takes-all voting system이 얼마나 효과적인지에 대한 실험도 진행하였는데, 왼쪽 테이블에서는 세가지 기존 self-supervised features를 모두 사용하였을때 psuedo mask의 iou가 늘어난다는것을 확인하였고, 이로써 적어도 하나의 cluster가 foreground를 포함하고 있음을 알 수 있다고 논문에서 설명한다.또한 오른쪽은 랜덤픽, center픽보다 본인들것이 더 좋다는 것을 보여주었다.​ 마지막은 쿼리의 개수를 얼마나 놔야 하는가?이다. 20개 전후로 maximum을 찍고 있는 것을 볼 수 있다. "
[Object detection] 2. R-CNN ,https://blog.naver.com/ollehw/221824251686,20200225,"R-CNN의 원문은 아래와 같습니다.Girshick, R., Donahue, J., Darrell, T., & Malik, J. (2014). Rich feature hierarchies for accurate object detection and semantic segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 580-587).​2020년 2월 25일 현재, 11,891회의 인용을 받고 있습니다. (엄청난 논문이란 소리죠...)​   R-CNN 모델을 간략하게 표현한다면 위 사진이면 충분합니다.​우선 맛보기로 알고리즘에 대해 단순히 설명드리자면1. Input Image로 부터 RoI (Object가 있을 것으로 예상되는 영역)들을 뽑아냅니다.2. CNN은 같은 크기의 Input Image를 받아들여야 하므로, RoI 이미지들을 같은 크기로 Warped.3. CNN으로 각 RoI 이미지들로부터 Feature를 추출하고, 이 추출된 Feature들을 Bound box regressor와 Classification model (SVM)의 Input으로 사용.​이제 각 순서를 자세하게 들여다보겠습니다.​1. Input Image로 부터 RoI (Object가 있을 것으로 예상되는 영역)들을 뽑아냅니다.   위 논문에서는 Selective Search 알고리즘을 사용해서, 2000개의 RoI (Object가 있을 것으로 예상되는 영역)을 뽑아냅니다.Selective Search 알고리즘은 그래프 이론을 기반으로 하고 있는데, 위 알고리즘만 해도 논문 한 편이기 때문에, 정확하게 아실 필요는 없습니다.이미지를 pixel단위로 분할한 다음에, 몇가지 조건 하에서 순차적으로 merge시킴으로써 영역들을 뽑아내는 정도로만 알고 계시면 됩니다.​2. CNN은 같은 크기의 Input Image를 받아들여야 하므로, RoI 이미지들을 같은 크기로 Warped.​   위 논문에서는 CNN architecture로 AlexNet을 사용합니다.따라서 Input 이미지는 224X224 사이즈를 가지도록 만들어야 합니다.하지만 2000개의 RoI는 각각 고유한 이미지 사이즈를 가지고 있기 때문에, Cropping 해서 224X224 사이즈로 바꾸어 줍니다.​이 후, Pretrained 된 AlexNet의 마지막 layer를 사용하고자 하는 데이터 셋의 Class 수로 바꾸고, Finetuning 시킵니다.​3. CNN으로 각 RoI 이미지들로부터 Feature를 추출하고, 이 추출된 Feature들을 Bounding box regressor와 Classification model (SVM)의 Input으로 사용.​SVM 모델을 사용하여, 각 Class 별로 해당 RoI 에서 추출된 Feature가 해당 Class의 Object와 일치하는 지를 Classification 합니다.이 때, 굳이 왜 SVM을 사용하는 가에 대해서는, 논문에서 SVM이 가장 성능이 좋았다고 합니다.​하지만, SVM 모델을 사용함으로써, Backproporgation이 불가능하기 때문에, 앞의 CNN의 Parameter들이 학습이 되지 않는다는 단점이 생기게됩니다.​Bouding box regressor는 아래의 선형 회귀모델을 사용합니다.   위 식의 의미는 RoI의 위치를 최대한 실제 Bounding Box의 위치에 regress 하고 싶다는 의도입니다.따라서 Loss function 또한 위 목적을 달성하기 위해 정의됩니다.​ "
PseCo: Pseudo Labeling and Consistency Training for Semi-Supervised Object Detection ,https://blog.naver.com/cobslab/222952879709,20221212,"안녕하세요 콥스랩(COBS LAB)입니다.오늘 소개해 드릴 논문은 ‘PseCo: Pseudo Labeling and Consistency Training for Semi-Supervised Object Detection’입니다.해당 내용은 유튜브 ‘딥러닝 논문 읽기 모임' 중 ‘PseCo’ 영상 스크립트를 편집한 내용으로, 영상으로도 확인하실 수 있습니다. (영상링크:https://youtu.be/C-NVH6StFQw) 현재 Semi-Supervised Object Detection on COCO 5%, 10%, 100% label 데이터에서 SOTA를 달성하고 있는 논문입니다. 먼저 Introduction입니다. 기본적인 내용부터 간단하게 짚고 넘어가도록 하겠습니다. label 종류에 따라서 학습 방법으로 Supervised learning, UnSupervised, Semi-Supervised, Weakly-Supervised learning이 있습니다. Supervised learning 같은 경우에는 이미지와 annotation이 모두 매칭 되어 있고, UnSupervised learning 같은 경우에는 반대로 이미지만 존재하고 있습니다. Semi-Supervised learning 같은 경우에는 소량의 label데이터와 대량의 unlabel 데이터로 학습을 진행을 하게 됩니다. Weakly Supervised learning 같은 경우에는 타깃으로 하는 task의 annotation이 아닌 다른 label을 가지고 학습을 진행하는 방법이라고 생각하시면 되겠습니다. 오늘 발표드릴 논문은 Semi-Supervised learning을 활용한 Object detection 논문입니다. 저자는 기존의 Semi-Supervised learning의 대표적인 방법인 Pseudo labeling과 Consistency 트레이닝 방법에 대해서 얘기를 합니다. Pseudo labeling 경우에는 label 데이터를 가지고 teacher 모델을 학습을 하게 됩니다. 그리고 unlabel 데이터를 바탕으로 학습된 teacher 모델에 Inference를 진행하게 되고 Inference의 결과를 Pseudo label로 활용을 합니다. 이렇게 생성된 Pseudo label 데이터와 label데이터를 활용해서 student 모델을 최종적으로 학습하여 진행하는 방법입니다. 이러한 Pseudo labeling은 classification이 집중된 방법이라고 얘기를 합니다. Pseudo labeling 기법을 통해 만든 Pseudo label을 분석해서 classification 성능 지표인 precision과 Iou Threshold 관계를 보면 iou가 0.9일 경우에 precision이 31 퍼센트밖에 되지 않습니다. 0.3일 때는 81퍼센트 수준입니다. Iou Threshold가 높아질수록 급격하게 precision은 떨어지게 됩니다. 이러한 지표를 분석했을 때 Pseudo label의 50퍼센트가 0.3에서 0.9 구간에 분포되어 있고 이러한 것은 학습 과정에 좋지 않은 영향을 끼친다고 저자들은 얘기를 하고 있습니다. 이렇게 bounding box 퀄리티가 좋지 않은 Pseudo label의 경우에는 label assignment에서도 문제를 야기시킵니다. 왼쪽 그림을 보면 proposal 박스와 GT를 비교했을 때 iou가 0.39로 낮은 수치이기 때문에 매칭이 되지 않아야 정상이고 그에 따라 백그라운드로 할당이 되어야 하지만 낮은 퀄리티의 Pseudo label과 매칭이 되는 경우에는 IoU가 0.55가 되고 Foreground로 할당이 될 수 있습니다.이러한 잘못된 할당은 Foreground-Background decision 바운더리 생성을 방해하게 되고 최종적으로 성능에도 영향을 끼치게 됩니다. 다음은 Pseudo labeling 방법과 더불어 대표적인 Semi-Supervised learning 방식인 Consistency 트레이닝입니다.label 데이터는 Supervised모델을 학습을 하게 되고 unlabel 데이터는 augmentation을 진행한 뒤 두 개의 이미지 모두 모델의 입력으로 활용을 하게 됩니다. 이렇게 입력된 두 이미지의 classification 결과는 둘 다 person으로 동일해야 되고 이런 동일성을 가지고 kull-back Leibleer Divergence Loss와 같은 Consistency 로스를 계산해서 이를 통해 모델을 학습을 하게 됩니다. 저자는 Consistency 트레이닝 문제를 앞서 설명드린 Pseudo labeling과 같은 맥락으로 detection task의 속성을 고려하지 않았다고 얘기를 합니다. Object detection 같은 경우에는 scale에 invariant 해야 되고 당연히 small Object의 성능을 높이기 위해서 feature 피라미드를 최근에는 당연시하게 사용하고 있습니다. 이처럼 detection task에서 사용되는 FPN처럼 스케일을 고려한 학습 방법이 기존의 Consistency learning에는 없다는 것이 문제입니다. 오른쪽 이미지를 보시면 네트워크에 prediction 결과만을 이용해서 Consistency loss를 계산을 하게 됩니다. 이를 label level Consistency라고 얘기를 하고 본 논문에서는 이런 문제들을 해결하는 방향으로 논문이 진행이 되게 됩니다. 다음은 Related work 부분입니다. 본 논문의 method을 설명하기에 앞서서 관련 임무들을 좀 설명을 드리자면 먼저 mean teacher를 얘기할 수 있습니다. mean teacher의 경우에는 teacher 모델을 student 모델의 웨이트를 활용하는 방법으로 사용하게 됩니다. 기존에는 teacher 모델과 student 모델이 다르게 학습되거나 다른 웨이트를 가졌지만은 mean teacher에서는 exponential moving average 방법을 사용해서 student 모델의 웨이트를 teacher 모델의 웨이트로 사용을 하게 됩니다. 하이퍼 파라미터 알파를 곱한 값을 이전 student 모델 가중치를 알파를 곱해서 이전 student 모델의 가중치를 teacher 모델로 활용을 하는 방법입니다. mean teacher와 동일하게 student 모델과 EMA 방법을 사용을 해서 teacher 모델을 생성을 하게 되고 unlabel 데이터는 horizon flip이나 resizing 같은 weak augmentation을 적용한 이미지와 Cut out이나 로테이션 같은 STRONG augmentation을 적용한 이미지를 둘 다 인풋으로 사용을 해서 학습을 진행하게 되고 teacher 모델의 NMS 결과를 Pseudo box로 활용을 해서 student 모델을 학습하게 됩니다.본 논문에서는 이 Soft teacher의 모델 구조를 그대로 채용을 했습니다. Method에 대한 설명입니다.방금 말씀드렸다시피 기본 구조는 Soft teacher와 동일합니다. 다만 unbiased teacher라는 논문에서 Pseudo 박스를 이용한 box regression loss를 삭제를 하게 됩니다. 그 부분을 본 논문에서도 활용을 하게 됩니다. 이런 box regression loss를 삭제하는 이유는 Pseudo label에 의해 생성된 bounding box는 퀄리티 문제로 학습과정에서 unstable을 야기하기 때문에 사용하지 않는다고 저자는 설명을 하고 있습니다. 본격적으로 본 논문에서 제시하는 방법입니다. 총 세 가지 방법을 제시를 합니다.첫 번째는 prediction guided label Assignment두 번째는 positive proposal consistency voting,세 번째는 Multi view scale invariant learning입니다.PCV 같은 경우는 전체 구조에서 2 부분을 의미하게 되고MSL 같은 경우에는 3 부분을 의미하게 됩니다.이 방법들을 좀 자세하게 설명을 드리겠습니다. 먼저 prediction guided label Assignment에 대해 설명을 드리겠습니다.기본적으로 본 논문의 네트워크는 Faster R-CNN을 사용하게 되고 기존의 방법들은 아까 말씀드렸다시피 teacher 모델의 NMS 결과를 활용을 하게 됩니다. 위에서 언급했듯이 Pseudo label을 이용한 box regression은 학습 과정을 불안하게 해서 해당 loss 텀을 삭제를 했기 때문에 teacher 모델의 bounding box 성능과 관련된 information이 전달이 되지 않습니다. 그래서 이런 문제를 해결하고자 teacher 모델의 RPN 결과를 스튜던트 모델의 RPN 결과와 공유해서 Guidance를 전달할 수 있게 했습니다. 이러한 과정을 통해 생성된 box proposal들을 기존의 GT와 IoU를 이용해서 샘플링하던 방법과 다르게 퀄리티를 계산해서 n개 추출하는 방법을 사용하게 됩니다. 퀄리티 같은 경우는 이 수식과 같으면 S는 confidence score u는 Gt와 IoU 즉, Pseudo label과의 IoU라고 생각하시면 되겠습니다. 이러한 방법으로 샘플링을 하게 되는데 이때 샘플링할 때 N는 하이퍼 파라미터로 결정된 값이 아니라 OTA라는 네트워크에 제안한 dynamic k estintation 통해 선정합니다. 간단하게 말씀드리면 최적의 n을 계산하기 위해서 최적의 n은 regression loss를 잘 계산하는 숫자이다라는 개념을 가지고 n을 다이나믹하게 선정하는 알고리즘이라고 생각하시면 되겠습니다. 두 번째는 positive proposal consistency voting입니다.기존의 박스 필터링을 진행할 때 probality와 confidence 곱을 통해서 스코어링 하던 것을 벗어나서 Localization 성능 또한 고려하기 위해서 제안한 방법입니다. 저자는 regression consistency를 제안을 했고 수식을 보시다시피 IoU 기반으로 웨이트를 계산하는 방식입니다. 이러한 regression consistency를 regression loss에 반영을 해서 localization 성능을 네트워크에 반영하는 방법으로 이용하게 되고 이렇게 제시하는 regression consistency의 실용성을 보기 위해서 이렇게 scatter 그래프를 제시했습니다.그래프를 보시면 실제 IoU와 consistency가 비례해서 이렇게 올라가는 것을 확인할 수 있습니다. 왼쪽 아래 보시면 consistency는 높지만은 IoU가 낮은 경우도 확인할 수 있습니다. 이거에 대해서 저자는 annotation 에러라고 설명을 합니다. 실제로 데이터셋이 완벽할 수 없기 때문에 MS COCO에서 annotation이 되지 않은 부분을 지적을 하고 있습니다.여기서 빨간색과 노란색 박스 들은 전부 다 저자가 제시한 방법으로 학습된 네트워크가 추정한 prediction 결과입니다. 이 빨간색 박스들은 실제로 이렇게 쳐줘야 되지만 데이터셋 자체에서 없는 annotation이라고 설명을 하고 있습니다. 세 번째로는 Multi view scale invariant learning입니다.설명드렸다시피 label level consistency를 반영하기 위해서 본 논문에서는 랜덤 resize ratio를 결합해 label alignment를 진행합니다. Random resizing을 진행한 이미지와 원본 이미지가 동일한 이미지이므로 동일한 label 결과가 나타나야 되며 이를 통해서 consistency 로스를 계산을 하게 됩니다. 추가적으로 feature level Consistency를 반영하고자 했습니다. Object detection의 scale invariant 해야 하는 속성을 반영하기 위해서 FPN 구조를 활용해 feature level Consistency를 제안을 했습니다.view 1 이미지와 view 2 이미지를 각각 네트워크에 통과시키고 해당 FPN 구조에서 동일한 resolution을 갖는 FPN의 스테이지의 feature 맵을 feature alignment 시키는 방법입니다. 예를 들어서 640x640 이미지가 들어가게 되면 P3 단계에서는 160 x 160 이 될 텐데 320 x 320 인풋을 가진 FPN에서 보면은 P2 네트워크가 160x160 resolution을 가질 겁니다. 이러한 두 개의 feature가 같은 resolution을 갖는 feature이기 때문에 동일한 이미지가 있는 인풋을 받았을 때 feature 맵이 동일 해져야 된다라고 얘기를 하고 있습니다. 다음은 experiment 부분입니다.논문에서 제시한 3가지 방법을 바탕으로 실험을 진행했습니다. 먼저 partial labeled 데이터 같은 경우에는 각 n퍼센트만큼 랜덤 샘플링해서 label 데이터를 구성을 하게 되고 랜덤 샘플링이기 때문에 5 Fold 방식을 사용해서 여러 번 학습 및 평가를 진행하게 됩니다. 보시다시피 평균 map와 표준편차를 구해서 이렇게 지표로 활용을 하게 되고 결과는 각각 task에 대해서 제일 좋은 SOTA 성능을 보이고 있습니다. 추가적으로 label 데이터를 활용해 Fully supervised 된 네트워크에서 unlabel 데이터를 활용해 추가적으로 학습한 경우도 제시를 했습니다. map 41이 label 데이터만을 가지고 학습한, 그러니까 Fully supervised로 학습한 방법이고 추가적으로 unlabel데이터로 본 논문에서 제시한 방법으로 학습했을 때 map 5.1 퍼센트 향상을 보인다고 얘기하고 있습니다. 그래서 label 데이터로 학습한 네트워크를 unlabel 데이터로 추가 학습했더니 map가 5.1 퍼센트나 증가한다고 얘기를 하고 있습니다. 위에는 Supervised learning을 통해 학습한 네트워크 결과입니다. 보시다시피 본 논문을 제시한 방법이 좀 더 잘 detection 한 것을 확인할 수 있고 추가적으로 제시하는 방법을 사용했을 때는 기존의 다른 Semi Supervised learning 방식인 Soft teacher보다 빠른 Convergence를 확인할 수 있다고 저자가 설명하고 있습니다. ablation을 진행한 내용입니다. 보시다시피 각 모듈을 사용해서 성능이 지속적으로 형성되는 것을 확인할 수 있습니다. 최종적으로 모든 모듈을 사용했을 때 제일 높은 성능을 보이더라 하는 부분입니다. Multi view scale의 경우에는 single scale 트레이닝 같은 경우는 이전에 랜덤 resize ratio를 가지고 학습을 했다 했는데 고정된 ratio를 가지고 학습했을 때 결과입니다. 확실히 랜덤한 ratio를 가지고 학습했을 때 결과가 더 좋은 걸 확인할 수 있습니다. 또한 각기 모듈에 있는 하이퍼 파라미터를 바꿔가면서 ablation을 진행해서 최적의 파라미터들을 찾아 성능을 냈습니다. 결론입니다. Semi Supervised learning의 문제점을 해결해서 본 논문을 제시한 세 가지 방법을 적용해 SOTA를 달성했다는 부분입니다. 아래는 본 논문을 리뷰하면서 몇 가지 생긴 의문점을 좀 정리를 했습니다. 항상 Semi- Supervised 논문을 보다 보면 정해진 퍼센트만큼의 label 데이터 비율을 평가를 하고 실제 모델의 성능과 labeling cost의 타협점은 몇 퍼센트가 제일 적절한 label 데이터냐라는 것은 고려를 하지 않는 것 같았습니다. 그래서 사실 모델마다 다를 텐데 같이 결과를 내줬으면 실제 사용하는 사람 입장에서 이 방법에 대한 신뢰성이 올라가지 않았을까 생각을 해봤습니다. 두 번째는 성능 부분에 대해서 좀 의문이 좀 들었습니다. 현재 SOTA을 달성하고 있는 detection 리더보드를 보면 다 트랜스포머 기반이고 그리고 본 논문에서 제시한 최대 map 같은 경우는 46.1이고 실제로 보면 최근에 map는 다 60 이상입니다. 최근에 60퍼센트가 넘는 성능을 보유한 것들이 되게 많이 나오고 있는데 성능 적으로 보면 46.1 퍼센트라는 그 수치가 높은 수치가 아닌 것 같다는 느낌이 들었습니다. 게다가 본 논문에서 설명하는 것이 label 데이터를 학습하고 unlabel 데이터를 추가로 학습을 했더니 성능이 올라갔다는 부분도 있었는데 Faster R-CNN으로만 검증을 하고 다른 트랜스포머 기반의 모델에서는 하나도 검증 안 했다는 것은 조금 의문이 들었습니다. 만약에 Faster R-CNN 같은 트랜스포머 이전에 논문에서만 활용이 가능한 방법이라면 실효성이 있을까 혹시 아니면 본 논문에서 제시한 방법 중에 트랜스포머 계열의 논문에 활용할 수 있는 방법도 있을까라고 의문은 좀 들었습니다. 이는 순수하게 궁금증에서 나온 의문점이라는 거를 참고 부탁드리도록 하겠습니다. "
[개별연구(두피분석)] Effect of Color Pre-Processing on Color-Based Object Detection 읽기 - 1 ,https://blog.naver.com/1rladbdus/222873013884,20220912,"* 전처리로 각기 다른 여러가지 색상을 유사하게 바꾸기 위한 배경지식을 쌓고자 해당 컨퍼런스 논문을 읽게 되었다.논문 출처: https://www.researchgate.net/publication/248390823_Effect_of_Color_Pre-Processing_on_Color-Based_Object_Detection​​1. 서론색상: 물체의 표면 속성색상의 정도는 카메라 센서의 스펙트럼 특성, 조명에 큰 영향 받는다.카메라 스펙트럼- 일반적(넓은 스펙트럼 영역, 높은 수준의 색 반응), 일부(고품질 컬러 응답, 좁은 스펙트럼 영역, 필터)CMOS 기술 자주 사용됨논문에서는 스펙트럼 문제만을 고려우리는 센서 스펙트럼 선명화의 잠재력을 활용하여 스펙트럼 중복 필터에서 얻은 색상을 개선하고자함색 항상성 알고리듬에서 사용되는 대각 행렬을 사용하여 곱셈으로 조명 변화를 모델링대각 행렬이 모든 표면의 RGB를 합리적인 근사값으로 변환할 수 있다면, 조명 변화의 대각선 모델이 유효하다는 결론sensor sharpening: 선형 변환을 통해 데이터를 대각선 모델이 더 충실하게 유지하는 새로운 공간으로 매핑변환 색 공간: 새로운 센서를 가진 것 처럼 작용The sensitivity of sharpened sensors tend to look sharper with narrower peaks than ones that do not. sharpened sensors의 의미를 모르겠다!??본 논문에서는 대각행렬보다 스펙트럼 선명화 사용RGB 색 공간은 하드웨어 관점 편리 but 색 기반 객체 감지에 좋지 않다. 따라서 perceptually uniform(PU, 지각적으로 균일한) 사용<- 색 공간 인식 품질 향상-> 객체 탐지에 유리Hue, Saturation and Intensity, HSI color space(색상, 포화 및 강도, HSI 색상 공간) 사용, 왜냐하면 색상으로 물체 감지에 좋은 성능 보인다고 발견되어서​논문에서 재시한 새로운 스펙트럼 선명화 방법: 선명한 색 응답을 HSI로 변환-> 객체는 H-S 존 경계 내에 있는지에 따라 감지됨​2. 색 측정 모델 image formation은 위의 공식에 따라 모델링된다I(x, y, c): 영상에서 (x, y) 위치에 해당하는 영상 관찰(image observation) 강도(intensity)E(x, y, z): 표면 색 x 입사 조명 색c 즉, (r, g, b): (Red, Green, Blue)에 해당하는 색 속성의 일반적인 벡터 표현E(X, Y, 람다): 장면 입사 방사선​이미지의 객체 표면 색은 실제 색에서 입사 조명과 카메라 스펙트럼 필터에 의해 다르게 나타난다.​비디오 신호 샘플링 수식: 카메라 방사선량 보정(camera radiometric calibration): 각 픽셀의 오프셋 b를 추정하기 위해 수행된 후 원시 영상에서 무시됨 : 전체 이미지에 대해 전역 광전 변환 지수(global photo-electric conversion exponent), 전체 이미지에 대해 계산됨​​스펙트럼 선명화: RGB 색 공간 -(선형 변환)-> 이미지 필터가 스펙트럼으로 분리된 것처럼 생성된 것과 유사한 새로운 공간스펙트럼 선명화 행렬: (RGB 값을 아래 행렬에 의해 새로운 벡터로 변환) C_1~C_9 계산: 스펙트럼으로 분리된 필터로 색깔을 매핑한다 -> 그냥 행렬 계산 한다는듯?​HIS의 PU(Perceptually Uniform) 색 공간이 RGB 색공간보다 색 감지/인식에 적합하다.Hue(색조)를 뜻하는 H, Saturation(채도)를 뜻하는 S 계산 공식:  대상 물체 H_S 다이어그램에서  hue angles & saturation values 색상 영역 경계 제한: min: 최소, max: 최대대상의 픽셀 색상이 H_S 영역의 경계 내에 있는지 여부에 따라 이미지에서 객체 감지​* 이후는 추후 추가 예정​9/12 소감: 이미지 처리를 위해서 저런 종류의 수식이 쓰이는구만 정도만 이해했다. 자세한 의미를 이해하지 못했다. 나의 무식함을 다시 한번 깨닫는다..​​[이미지 출처] Effect of Color Pre-Processing on Color-Based Object Detection, https://www.researchgate.net/publication/248390823_Effect_of_Color_Pre-Processing_on_Color-Based_Object_Detection​​ "
R-CNN 모델(Object detection) 세부 설명 ,https://blog.naver.com/grow_bigger/222761567481,20220606,"[SIFT]​SIFT알고리즘은 한 이미지를 사이즈를 달리 조정한 여러 그룹을 생성한 후, 같은 크기인 같은 그룹내 이미지들을 흐릿한정도를 달리 하여 여러장 만듭니다. 그 후 크기가 다른 3장의 이미지를 뺄셈 연산을 통하여 특징점을 찾음으로써 크기에 강건한 특성을 부여합니다. 또 특징점 주변픽셀의 방향 벡터를 구한 후 특징점으로 부터 상대적인 방향을 갖도록 방향벡터에서 특징점 방향벡터를 빼주면 방향에 대해서도 강건함을 갖게됩니다. 두 장의 사진의 매칭시킬때각 사진들에 위의 작업을 해주면 방향과 크기에 영향을 적게 받는 특성들이 추출되어 이 특성을 비교하면 두 이미지를 매칭시킬 수 있게 됩니다.SIFT (Scale Invariant Feature Transform)의 원리::bskyvision​[SVM]SVM은  이진 선형 분류 모델입니다. 학습데이터를 필요로 하지 않고 알고리즘에 따라 선형적으로 분류합니다.SVM은 마진이 가장 커지는 쪽으로 선을 그어 분류하는 방식입니다.(마진은 한 그룹에 가장 가깝게 그은 직선과 다른 그룹에 가장 가깝게 그은 직선간의 거리입니다.)머신러닝 - 2. 서포트 벡터 머신 (SVM) 개념 (tistory.com)​[SVR]SVR은 SVM에서 마진안에 관측값들이 들어가도록 하는 회귀의 한 방법입니다.머신러닝 기초 10 - SVM (서포트벡터 머신 - Support Vector Machine) : 네이버 블로그 (naver.com)​[NMS, non-maximum suppression]IOU가 특정치 이상이면 제거함으로써 비슷한 항목에 여러번 localizing이 되는 것을 방지합니다. 사진 출처: 1704.04503.pdf (arxiv.org)NMS (non-maximum-suppression) (tistory.com)​[AlexNet] AlexNet architecture, 출처: A. Krizhevsky, I. Sutskever, and G. Hinton. ImageNet classification with deep convolutional neural networks. In NIPS, 2012. [Ground-truth]Ground-truth는 학습하고자 하는 실제 값입니다. 데이터의 label이라고 볼 수 있습니다. 논문에서는  We treat all region proposals with ≥ 0.5 IoU overlap with a ground-truth box as positives for that box’s class and the rest as negatives.ich feature hierarchies for accurate object detection and semantic segmentation위와 같이 0.5IoU인 경우를 positive하게 취급하고 그 외의 부분을 negative취급을 한다고 합니다.IoU는 기계가 그린 사각영역을 A, 정답 영역을 B라고 할 경우 A와 B의 교집합 / A와 B의 합집합의로 정의됩니다.[용어정리]Ground-truth (tistory.com)​[hard negative mining]잘못 그려진 bounding box를 모아 확신도를 기준으로 내림차순하여 데이터셋을 만들고, 정답 bounding box를 또 하나의 데이터 셋으로 하여 이 두 데이터셋에서 샘플링을 하여 CNN을 학습시키는 방법이다.모든 negative example을 다 사용하지 않아도 되므로 메모리를 절약할 수 있습니다.[Computer Vision] Hard Negative Mining :: To make me better (tistory.com)​[Semantic segmentation]semantic segmentation은 픽셀 하나하나 마다 class를 분류해주는 작업입니다. 배경을 지운다거나 물체를 정확히 인식하려고 할때 object detection처럼 사각형을 그리는게 아닌 물체 모양대로 인식을 하여 물체와 배경을 물체 모양대로 분리하고 어떤 라벨을 갖는지 예측합니다.1편: Semantic Segmentation 첫걸음!. Semantic Segmentation이란? 기본적인 접근 방법은? | by 심현주 | Hyunjulie | Medium​ "
Tiny Object Detection! ,https://blog.naver.com/cobslab/222820943392,20220720,"안녕하세요 딥러닝 논문 읽기 모임입니다.오늘 소개해 드릴 논문은 'A Normalized Gaussian Wasserstein Distance for Tiny Object Detection'입니다.해당 내용은 유튜브 ‘딥러닝 논문 읽기 모임' 중 ‘A Normalized Gaussian Wasserstein Distance for Tiny Object Detection’ 영상 스크립트를 편집한 내용으로, 영상으로도 확인하실 수 있습니다. (영상링크:https://youtu.be/eGKlg4sZ0Zw)​ ​ COCO Dataset에서 정의한 Object 크기는 Large, Medium, Small로 구분이 되어 있습니다. Large는 96 X 96픽셀 이상, Medium은 32 X 32부터 96 X 96픽셀 사이, Small은 32 X 32픽셀 이하를 의미합니다.작은 것들을 내포하는 데이터셋은 대표적으로 AI TOD(Tiny Object Detection) 데이터셋이 있습니다. Tiny Object Detection에서는 Tiny랑 very Tiny까지 포함되어 있습니다. Tiny는 8 X 8부터 16 X 16이고, Very Tiny는 그 이하를 의미합니다.​ 논문에서 실험을 진행한 데이터셋은 위성과 드론에서 촬영한 영상입니다. Object들이 매우 작고 다양하게 포함이 되어 있는 것을 확인할 수 있습니다.​ APS는 APM이나 APL보다 낮은 성능을 보이는 것을 확인할 수 있습니다. 논문에선 작은 객체들에 대해서 탐지 능력이 떨어진 원인을 IoU가 문제라고 설명하고 있습니다.​ 왜 IoU가 Small Object를 탐지하는데 문제가 되는지 설명드리겠습니다. 왼쪽이 작은 크기의 박스고 오른쪽이 큰 크기의 박스입니다. 박스 B와 C는 각각 A 박스를 기준으로 1 Pixel, C는 4 Pixel이 translation 되어 있습니다. 왼쪽 A와 B 의 IoU를 보시면은 0.53이고 오른쪽 A와B는 0.9 입니다. 같은 크기만큼 translation이 되었음에도 불구하고 IoU값이 차이가 나는 것을 확인할 수 있습니다. 4 Pixel이 움직인 A 와 C는 IoU가 0.65이고 크기가 작은 박스에 경우 0.6인 것을 확인할 수 있습니다. translation시 박스 크기에 따라서 metric이 변한다라는 게 IoU의 문제라고 설명하고 있습니다.​ 위 그림은 동일한 크기의 A와 B의 박스를 비교한 것이고 아래는 다른 크기의 박스를 비교한 것입니다. 색상은 박스 크기를 나타내고요. IoU의 metric은 박스 크기가 같은 경우에 박스 크기에 따라서 IoU가 급격하게 증가하거나 감소하고 박스가 커지면 커질수록 그래프의 기울기와 완만해지는 것을 확인할 수 있습니다.​오른쪽은 논문에서 제안하는 방법인 NWD입니다. NWD는 박스의 크기와 상관없이 동일한 기울기를 가진 metric을 확인할 수 있다고 설명을 하고 있습니다.​크기가 다른 경우에도 IoU를 사용할 경우에는 크기가 다르기 때문에 일정 IoU이상은 존재하지 않습니다. 최대 IoU는 B크기만큼만 되기 때문에 B크기 이상으로는 IoU가 존재할 수가 없지만 NWD를 쓰면 다르게 IoU를 나타낼 수 있습니다. 학습 과정입니다. detection 모델에서 나온 결과 박스들을 가지고 Loss를 계산하고, backpropagation을 진행합니다. 이때 Loss를 계산하기 위해서는 ground truth Box랑 차이를 계산합니다.​ Label Assignment은 수많은 박스들 중 ground truth를 선정하는 방법입니다. 보통은 ground truth와 가장 IoU가 높은 예측 박스 중에 선정을 해서 ground truth를 계산합니다. 이때 IoU가 작은 물체는 Negative Sample, 즉 background라고 생각합니다. 혹은 ground truth가 가장 높은 것은 Object라고 생각하고 Loss를 계산합니다. 일반적으로는 IoU Threshold를 미리 정해 놔서 일정 IoU이하는 전부 Negative Sample로 결정하는 박스 Assignment를 합니다.​ IoU가 어떤 영향을 끼치는지 살펴보겠습니다. 그림에서 A 가 GT(Ground Truth)고 B와 C가 예측된 박스라고 생각했을 때 IoU는 0.9입니다. 일정 Threshold이하면 모두 Negative Sample으로 할당돼서 악영향을 끼칠 수도 있습니다. Tiny Object에서 IoU를 사용하면 빈번하게 발생할 수 있는 문제라고 설명하고 있습니다.​ 다음은 NMS(Non-Maximum Suppression)입니다. IoU가 사용되는 부분은 NMS에서도 사용됩니다. NMS는 inference에만 사용되긴 하지만 NMS에서도 IoU가 사용되기 때문에 문제가 될 수 있다고 설명하고 있습니다. 또한 IoU Loss를 사용하는 Regression Layer에서도 학습과정에서 악영향을 끼칠 수 있다라고 설명을 하고 있습니다.​ 작은 물체를 잘 못 찾는 문제를 해결하기 위해 매우 보편적으로 쓰고 있는 방법인 FPN(Feature Pyramid Network)입니다. Feature 크기를 다양하게 변화시켜서 다양한 크기의 물체를 잘 잡게 만들고 있습니다.​ ATSS는 Label Assignment 문제를 해결하기 위해서 만든 방법입니다. handcraft component인 IoU Threshold를 자동으로 결정을 해 주는 방법입니다. IoU 문제를 해결하기 위한 방법입니다. GIoU, DIoU, CIoU 은 IoU를 대체하기 위해서 나왔습니다. 두 박스가 겹치지 않을 경우에는 IoU가 0으로 떨어지기 때문에 얼마나 박스가 떨어져 있는지 확인을 못 하기 때문에 두 박스 사이의 거리나 면적들을 가지고 IoU를 보완하겠다 라는 개념입니다.​ NWD에서 Wasserstein Distance는 두 확률분포 사이에 Distance를 계산하는 방법입니다. infinum은 최솟값을 찾는 과정이고, X-Y는 Distance expectation, 즉 X와 Y 분포의 distance의 기댓값 중에서 가장 작은 값을 취합니다.​ Method입니다. NWD는 두 개 probability Distribution을 가지고 distance를 계산합니다. 이를 위해서 Bounding Box를 2D Gaussian Distribution으로 Modeling 합니다. 2D Gaussian Distribution 수식을 Density Contour로 나타내면 Mahalanobis distance를 상수 K로 나타냅니다. K값에 따라서 Gaussian Distribution의 오차율이 다르게 나옵니다.​ Bounding Box Component를 가지고 타원 방정식을 구해서 Density Contour까지 구해보겠습니다. Bounding Box가 있을 때 내접하는 타원의 Component는 중심점 그리고 width랑 height가 있습니다. 위에서 설명드린 Mahalanobis distance가 1일 때 타원 방정식과 Density Contour 방정식은 서로 매칭이 될 수 있습니다. U는 일반적으로 Gaussian Distribution에서 말하는 평균값이고 ∑는 공분산 행렬입니다.​ Gaussian Wasserstein Distance에 적용해봤습니다. 이 Gaussian Wasserstein Distance는 Gaussian Distribution 사이에 계산된 Wasserstein Distance의 공식입니다. 공분산 행렬이므로 교환 법칙이 성립합니다.​ 교환법칙이 성립한다고 계산하면 m은 U이므로, U값과 공분산 행렬을 수식에 대입해서 전개를 하였습니다. 각 Bounding Box 요소들 간의 Euclidean distance처럼 간단하게 나타낼 수 있습니다.​IoU metric은 0부터 1까지 Normalize 됐지만 Wasserstein Distance 같은 경우에는 Normalize 돼있지 않기 때문에 metric으로 활용하기 위해서 exp함수를 사용을 해 0부터 1까지로 Normalization 하게 되고 여기서 나타낸 C 같은 경우는 하이퍼 파라미터입니다.​ Label Assignment와 NMS 그리고 Regression Loss의 IoU Loss 대신에 사용할 수 있게 적용을 해서 실험했습니다.​ Experiments 부분입니다. 실험에서 사용한 Baseline 네트워크는 Fast R CNN을 사용했습니다. 다른 metric과 비교해 봤을 때 Tiny Object Detection에서 제일 효과가 좋은 것을 확인할 수 있습니다. 다른 metric들을 적용해봐도 효과가 별로 안 좋은데 NWD를 적용했을 때 효과가 좋은걸 확인할 수 있습니다.​ 각 모듈에도 적용해가면서 평가해봤습니다. Label Assigning에서 제일 효과가 좋은 것을 확인할 수 있고 NMS에 적용했을 때는 오히려 성능이 떨어지거나 조금 오르고, Loss에 사용했을때는 성능 상승폭이 적었습니다. 여러 모듈에서도 테스트를 해 봤을 때 역시 NWD를 사용하는 게 효과가 제일 좋은것을 확인할 수 있습니다.​ 실제 실험 결과입니다. 위쪽은 IoU를 사용했을 때 밑에는 NWD를 사용했을 때입니다. 초록색은 잘 찾은 부분이고, 빨간색은 찾아야 되는데 못 찾은 부분, 파란색은 못 찾아야 되는데 찾은 부분입니다. NWD를 사용하는 게 효과적인 것을 확인할 수 있습니다.​ 전체 비교 테이블입니다. 별표가 있는 것이 NWD를 적용한 네트워크입니다. 그래서 DetectoRS를 적용했을 때 제일 효과가 좋은 것을 확인할 수 있습니다. 그리고 또한 Tiny랑 Very Tiny에서는 효과가 좋은 걸 확인할 수 있는데 s나 m에서는 효과도 안 좋은 것도 있습니다.​ 결론을 내자면 IoU를 대체하는, NWD 사용하는 게 되게 간단한 방법이고 코드 구현도 간단하면서 효과적으로 Tiny Object Detection을 수행할 수 있습니다. 그러나 실제 구현해보니 큰 Object에는 오히려 성능이 떨어지는 것을 확인할 수 있었습니다. "
흥미로운 Move Mirror 그리고  PoseNet의 object detection ,https://blog.naver.com/rmfls4359/222700224184,20220414,"​오늘자 수업에서 재밌는거 했다.소개해보자면사람의 동작을 실시간으로 감지하여 전 세계에서 비슷한 포즈를 취하는 수백 개의 이미지와 일치시킨다.말그대로 Move Mirror이를 구축하기 위해 주요 신체 관절의 위치를 식별하여 이미지 및 비디오에서 인물을 감지할 수 있는 모델인 PoseNet을 사용했다고 함.자세한 내용 아래에https://blog.google/technology/ai/move-mirror-you-move-and-80000-images-move-you/ Move Mirror: You move and 80,000 images move with youThis AI experiment lets you explore pictures in a fun new way, just by moving around.blog.google PoseNet이 무엇이냐 하면실시간으로 사람의 자세를 추정할 수 있는 머신러닝모델이라고 한다.TensorFlow.js 버전도 출시되었다고 함.자세한내용 아래에https://medium.com/tensorflow/real-time-human-pose-estimation-in-the-browser-with-tensorflow-js-7dd0bc881cd5 Real-time Human Pose Estimation in the Browser with TensorFlow.jsPosted by: Dan Oved, freelance creative technologist at Google Creative Lab, graduate student at ITP, NYU. Editing and illustrations…medium.com 나도 오늘 PoseNet 이용해서 object detection 한번 사용해볼라는디자꾸 오류뜸코드와 원만한 합의하고 다음포스팅에서 보여드리겠습니더^^Move Mirror에 엄청난 사진 데이터 썼겠지? 머신러닝 학습시켜야 하니까근데 그 사진데이터 구글포토에서 가져온다고 한다.구글포토 지금은 유료화됐는데 옛날에 쓰셨던 분들 주목해봐첨에 구글포토 약관에 여기에 사용된다고 있었다고 함.다들 알고있었음? 당연히 약관은 안보고 동의하는거 국룰이니까 몰랐겠지 ㅋ​그대들의 사진은 방금 내 동작을 인식하는 데 사용되었다.하하하이거 뭐 현대괴담이야?​💜당신의 개인적인 사진은 내 깜찍한 AI를 학습시키는데 사용되고 있다💜​뭐 이런거 ㅎㅎ ​아래링크는 Move Mirror를 직접 사용해볼 수 있는곳​https://experiments.withgoogle.com/collection/ai/move-mirror/view AI Experiments: Move MirrorExplore pictures in a fun new way, just by moving around.experiments.withgoogle.com 그리고 공주가 사용해보았지 근데 별로 안비슷한거 같은디교수님말로는 일어나서 하면 더 잘된대 ㅋㅋ아주 잠깐의 영상인데 진짜 몸치같다 나 이거 다 웹캠의 오해이고 모함이야.마찬가지로 웹캠과도 원만한 합의하고 올게요​요즘 가고싶은전시회 랜덤다이버시티 프래그넌스,,갑자기 왜 전시회얘기 하냐고?안에 new random city라고object detection을 활용한 포토스팟이 있는거 같다.​https://booking.naver.com/booking/12/bizes/567573 네이버 예약 :: 랜덤 다이버시티 프래그넌스['RANDOM DIVERSITY'는 감각과 인식, 그리고 감정을 알고리즘으로 혼합하여, 비가시적인 것을 가시화시키는 과정에서 인간과 비인간의 앙상블을 통해 새로운 기준을 모색한다.] RANDOM DIVERSITY-FRAGRANCE는 'RANDOM DIVERSITY 알고리즘'을 후각 신경에 적용한 프로그램과 프랑스 파리의 'Janne Passera' 조향사가 제안한 창의적인 조향비율을 기반으로 진행됩니다. 나의 기억과 감정을 자극하는 향기를 찾아 나만의 향수을 만들어가는 체험형 전시로, 본 전시에는 시향과 조향의 과정이 포함됩니다.booking.naver.com https://blog.naver.com/jump_arko/222504024344 [추천전시] 언어로는 규정되지 않는 개인의 고유성 찾기-나를 발견하다, <뉴 랜덤 다이버시티><뉴 랜덤 다이버시티>New Random Diversity 2021. 8. 1(토) ~ 9. 30(목) 송원아트센터 글/사진 아...blog.naver.com 이 포스팅 들어가면 볼 수 있다.카메라 앞에서면 사람이면 person이라 뜨고 핸드폰이면 cellphone이라 뜨는것을 볼 수 있다.object detection 참 간지나는 기술이란 말이지,,,테크니컬아트. 참 멋있어탐난다 너......​학교에서 재밌는거 많이 배우네평생 다니고싶다..~막이래 깔깔ㅎㅎ세상아 방금한말 잊어주어~공주는 졸업할거니까​​ "
"LabelImg, Image Labeling Tool for Object Detection ",https://blog.naver.com/baek2sm/221886666289,20200402,"안녕하세요. 동네코더입니다. 이번 포스팅에서는 간단히 툴 하나를  소개해드리려고 합니다. 이 툴을 이용하면 이미지 레이블링(Image Labeling) 작업을 쉽게 진행할 수 있습니다(그러나 여전히 고단한 작업입니다..). 이 툴은 파이썬으로 개발되었으며 오픈 소스로 공개되어 있습니다. ​LabelImg Github: https://github.com/tzutalin/labelImg tzutalin/labelImg🖍️ LabelImg is a graphical image annotation tool and label object bounding boxes in images - tzutalin/labelImggithub.com 직접 빌드해서 사용하셔도 되지만 파이썬 패키지 관리자로 설치하면 더 편리합니다.아래 명령어로 설치를 진행할 수 있습니다. 저는 파이썬 3.6 환경에서 테스트했습니다. pip install labelImg 실행은 그냥 아래와 같이 프로그램명을 입력해주시면 됩니다. labelImg 프로그램이 실행되면 좌측 메뉴 바에서 Open Dir 버튼을 통해 작업을 위해 불러올 이미지 경로를 지정할 수 있습니다. 그리고 단축키 w를 누르면 오브젝트 영역(Rect Box)을 지정할 수 있는 상태가 됩니다.   작업 후에는 다음과 같이 xml 형태의 파일로 레이블 정보가 저장됩니다.   다음 파일로 넘어갈 때는 단축키 d를, 이전 파일로 돌아갈 때는 단축키 a를 사용할 수 있습니다.​텐서플로우를 이용해 Object Detection 프로젝트를 진행할 때는 xml 파일을 Tensorflow Record 파일로 변환해서 프로젝트를 진행하는 것 같습니다. xml 파일을 csv 파일로 먼저 바꾸고, 그 다음 csv 파일을 record 파일로 바꾸는 순서로요. 다음에는 이 과정도 정리해보도록 하겠습니다. "
26. [논문] - R-CNN (Rich feature hierarchies for accurate object detection and semantic segmentation) ,https://blog.naver.com/steve5636/222878911080,20220919,"<개인 Notion 정리 링크 ▼> R-CNN (Rich feature hierarchies for accurate object detection and semantic segmentation)<논문 링크>amvision96.notion.site   <Abstract>​▶ 표준 PASCAL VOC 데이터 세트에서 측정하는 객체 감지 성능은 지난 몇 년 동안 정체되었다.가장 성능이 좋은 방법은 일반적으로 여러 low-level의 이미지 feature을 높은 레벨의 컨텍스트와 결합하는 복잡한 앙상블 시스템.​​▶ 본 논문에서는 VOC 2012의 이전 최고의 결과와 비교하여 mAP(: mean Average Precision)를 30% 이상 향상시켜 53.3%의 mAP를 달성하는 간단하고 확장 가능한 탐지 알고리즘을 제안.​​▶ 접근 방식은 두 가지 핵심 인사이트를 결합. (1) 하나는 객체를 현지화하고 분할하기 위해 상향식 지역 제안에 고용량 컨볼루션 신경망(CNN)을 적용할 수 있고, (2) 레이블이 지정된 훈련 데이터가 부족할 경우를 위해 보조 작업으로 supervised 사전 훈련과 도메인별 미세 조정에 이어 상당한 성능 향상을 달성.​​▶ regional-proposal을 CNN과 결합하기 때문에 우리의 방법을 R-CNN(Regional-CNN)이라 부름.​​▶ 우리는 또한 R-CNN을 유사한 CNN 아키텍처를 기반으로 최근에 제안된 윈도우 슬라이딩 방식의 검출기인 OverFeat와 비교해, R-CNN 방식이 200-클래스 ILSVRC2013 탐지 데이터 세트에서 OverFeat를 큰 폭으로 능가한다는 것을 발견.  R-CNN 논문을 리뷰했습니다.​2012년 AlexNet의 등장으로 CNN은 Image Classification 분야에서는 널리 쓰이게 되었으나, 기존에 Object Detection 분야는 발전이 정체 되어 있었습니다. 객체를 검출하기 위해서는 한 이미지 내에서 다수의 물체에 대해 정확한 위치를 찾아내고, 찾아낸 위치에 존재하는 물체를 정확히 분류해내는 과정이 결합된, 복잡한 과정이기 때문입니다.​R-CNN의 등장으로 처음 객체 검출 분야에서 CNN 모델이 사용되어, PASCAL VOC 데이터 셋에 대해 이전의 SOTA 모델 대비 30%의 성능 개선을 이끌어내며, 화려한 등장을 알림과 동시에, 추후 무궁무진한 발전 가능성을 보여주었습니다.​이를 계기로 2-stage detector (이질적인 물체의 위치를 정확히 검출 & 검출한 위치 내에서 정확한 클래스 분류) 개념의 시초가 되었으며, 추후 XX-CNN 계열로 이어지는 발전의 토대가 되었습니다.​​▶ 다만 다음의 몇 가지 단점도 함께 보여주며 추후 발전의 여지를 남겼습니다.​영역 추론을 위해 Selective-Search 사용 및 이를 모두 CNN 네트워크에 통과 시키는 방식은 매우 속도가 느립니다.대용량의 CNN, 선형 SVM regressor, Bounding-Box regressor를 사용해 전체적인 모델이 복잡도가 높습니다.단계별로 나누어져 있어, SVM, BB 단계에서의 정보를 역전파 하지 못하고, 단계별로 분할해 훈련을 진행해야 합니다. "
【프로그래밍 「 API 편」】 Google Tensorflow Object Detection API가 이미지 인식을 구현하는 가장 쉬운 방법입니까? ,https://blog.naver.com/dnflsmsskek/222847579575,20220813,"이미지 인식을 수행하는 방법에는 여러 가지가 있습니다. Google은 최근 어디에서나 컴퓨터 비전을 향상시키는 새로운 Tensorflow Object Detection API를 출시했습니다. Google의 모든 제안은 가볍게 여겨서는 안 되므로 이 새로운 API를 직접 사용해 보고 you tube의 동영상에 사용하기로 결정했습니다. 아래 결과를 참조하세요. Tensorflow API에서 객체 감지내 Github 리포지토리 에서 전체 코드를 찾을 수 있습니다.이 프로젝트에 두 번째 단계를 추가했습니다. 여기서 Tensorflow Object Detection API를 사용자 지정 데이터세트에서 사용하여 나만의 장난감 비행기 탐지기를 구축했습니다. 내 기사를 확인할 수 있습니다https://medium.com/@priya.dwivedi/building-a-toy-detector-with-tensorflow-object-detection-api-63c0fdf2ac95그럼 그 경험은 어땠나요? 먼저 API를 이해하겠습니다.API 이해API는 COCO 데이터 세트 (컨텍스트의 공통 개체)에 대해 교육되었습니다. 이것은 가장 일반적으로 발견되는 90개의 개체에 대한 300,000개 이미지의 데이터세트입니다. 개체의 예는 다음과 같습니다. COCO datset의 일부 개체 범주API는 실행 속도 간의 균형을 제공하는 5가지 모델을 제공합니다.그리고경계 상자 배치의 정확성. 아래 표를 참조하십시오. 여기서 mAP(평균 평균 정밀도)는 경계 상자를 감지할 때 정밀도와 재현율을 곱한 것입니다. 네트워크가 관심 개체에 얼마나 민감한지와 잘못된 경보를 얼마나 잘 피하는지에 대한 좋은 결합 측정입니다. mAP 점수가 높을수록 네트워크가 더 정확하지만 실행 속도가 저하됩니다.이 링크 에서 이러한 모델에 대한 자세한 정보를 얻을 수 있습니다.API 사용가장 가벼운 모델(ssd_mobilenet)을 사용해보기로 했습니다. 주요 단계는 다음과 같습니다.고정 모델(.pb — protobuf )을 다운로드하고 메모리에 로드합니다.내장된 도우미 코드를 사용하여 레이블, 범주, 시각화 도구 등을 로드합니다.새 세션을 열고 이미지에서 모델 실행전반적으로 상당히 간단한 단계 집합입니다. API 문서는 또한 주요 단계를 안내 하는 편리한 Jupyter 노트북 을 제공합니다.모델은 샘플 이미지에서 꽤 좋은 성능을 보였습니다(아래 참조). 비디오에서 실행다음으로 일부 비디오 에서 이 API를 사용하기로 결정했습니다 . 이를 위해 Python 무비파이 라이브러리를 사용했습니다. 주요 단계는 다음과 같습니다.VideoFileClip 함수를 사용하여 비디오에서 이미지 추출fl_image 함수는 이미지를 가져와 수정된 이미지로 바꿀 수 있는 멋진 함수입니다. 나는 이것을 비디오에서 추출한 모든 이미지에서 객체 감지를 실행하는 데 사용했습니다.마지막으로 수정된 모든 클립 이미지가 새 비디오로 결합되었습니다.이 코드는 3~4초 클립을 실행하는 데 약간의 시간(~ 1분)이 걸립니다. 그러나 메모리에 로드된 고정 모델을 사용하기 때문에 이 모든 작업을 GPU가 없는 컴퓨터에서 수행할 수 있습니다.나는 매우 감동했다! 약간의 코드만 있으면 일반적으로 발견되는 많은 개체에서 적절한 정확도로 경계 상자를 감지하고 그릴 수 있습니다.더 잘할 수 있을 것 같다는 생각이 든 경우도 있었다. 아래 예를 참조하십시오. 이 비디오에서는 새가 전혀 감지되지 않습니다. 다음 단계이 API에 대한 추가 탐색을 위한 몇 가지 추가 아이디어더 정확하지만 높은 오버헤드 모델을 시도하고 그들이 얼마나 큰 차이를 만드는지 확인하십시오.모바일 장치에서 실시간 개체 감지에 사용할 수 있도록 API 속도를 높이는 방법을 찾으십시오.Google은 또한 이러한 모델을 전이 학습에 사용할 수 있는 기능을 제공합니다. 즉, 고정된 모델을 로드하고 다른 이미지 카테고리가 있는 다른 출력 레이어를 추가합니다.이 게시물이 마음에 드셨다면 ❤️을 주세요 :) 코드를 가져와서 직접 시도해 보시기 바랍니다.저는 저만의 딥 러닝 컨설팅을 운영하고 있으며 흥미로운 문제를 해결하는 것을 좋아합니다. 저는 많은 스타트업이 혁신적인 AI 기반 솔루션을 배포하도록 도왔습니다. http://deeplearninganalytics.org/ 에서 확인 하십시오 .https://medium.com/@priya.dwivedi 에서 내 다른 글을 볼 수도 있습니다.협력할 수 있는 프로젝트가 있으면 내 웹사이트나 info@deeplearninganalytics.org를 통해 저에게 연락해 주십시오. "
Attentive Feedback Network for Boundary-Aware Salient Object Detection ,https://blog.naver.com/worb1605/221650028928,20190916,"본 논문을 review 하기 앞서 Salient Object Detection 의 개념을 먼저 알아보도록 하겠다.​Salient Object Detection은 Salient의 뜻대로 ""자명한""의미를 담아 가장 핵심인 Object를 찾는 detection을 의미한다.   위 그림이 Salinet Detection의 예로 볼 수 있다. Attention과 같은 개념이라 보면 될 것 같다.​-----------------------------------------------------------------------------------------------------------------------------​1. IntroductionSOD(Salient Object Detection)의 개념은 이미지 속 물체의 영역을 추출해내는 것과 같다.(중략......)​2. Related WorkMulti-scale fusion methods - 본 논문에서 참고한 [Deeply supervised salient object detection with short connections] 와 [Amulet : Aggregating multi-level convolutional features for salient object detection.] 논문에서는 계층구조를 사용하여 이상적인 솔루션을 찾아내었다고 설명하고 있다. 위 두 논문들은 계층이 점점 깊어질수록 down sampling하며 resolution이 작아지게 되는데, 이를 활용하여 계층별로 saliency map을 만들었다고 한다.​Coarse-to-fine solution - 서로 스케일이 다른 feature map끼리 concatenating하는 것은 정보의 모호성때문에 에러를 만들어 낼 수 있다고 한다. 따라서 최근의 sota논문들에서는 서로다른 scale의 feature map끼리  concatenating 하는 것에 대한 솔루션을 찾고자 RefineNet, PiCANet, RAS를 발표했다. 이 솔루션들은 여러 stage에 걸친 high resolution을 만들어 낼 수 있는 재귀 구조의 Network를 바탕으로 한다.본 논문에서는 각 계층의 feature map을 skip-connection을 하여 계층을 쌓는 구조를 택하였다.​Attention model - Attention model은 현재 다른 Neural Network 분야에서 사람의 시각인지능력과 같이 작동하는 모델이다. G-FRNet에서는 Encoder 와 Decoder를 활용하여 Attention model을 만들어냈다. (중략)​3. Proposed Methods본 논문에서는 Attentive Feedback Network(AFNet)을 고안하여, Salient Object Detection을 수행한다.우선 Backbone network에 대한 부분이다.본 논문에서는 VGG-16 Network를 backbone network로 사용하여 입력 이미지에서 기본적인 feature map을 추출하고 재구성하는 Encoder-Decoder에 사용하였다.​3.1 Network Overview본 논문에선 VGG-16 Network 를 backbone network로 선택한 후, encoder-decoder 모델로 sod를 추출하는 모델을 설명하고 있다.아래 Figure2에서 볼 수 있듯 본 논문의 Network는 5쌍의 Encoder Decoder pair로 이루어져 있다.   Encoder Network. - 이 논문에서 쓴 Encoder Network는 VGG-16 Network의 Fully Connected Layer를 제거한 후, Fully Convolutional network로 대체한 모델을 사용하였다. 또한 [Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs] 에서 보여준 것 처럼 dilated convolution을 사용하여 padding이나 맨 바깥 pixel의 제외 없이 convolution을 수행할 수 있어 공간적인 손실 없이(spatial detail 손실 없이) Convolution을 수행하도록 하였다.​Global Perception Module. - GPM 모듈은 Encoder의 feature 추출에 도움을 주고, Decoder 의 정제 및 재구성 능력에 도움을 준다. 자세한 내용은 3.2 에서 설명 예정이다.​Decoder Network. - Decoder는 5개의 convolution 계층으로 이루어져있으며, 한개의 계층당 2배씩 up scaling되는 구조로 되어있다. (중략... 뒤 내용은 convolution의 크기, 채널 사이즈가 나오는부분인데, 중요하진 않아서 패스)​3.2 Global Perception Module   위 이미지는 GPM의 형태를 그림으로 표현한 것이다.Saliency Prediction을 하기 위해서 일반적으로 Global view CNN을 사용하지만, 이렇게 할 경우, Convolution이 점점 깊어질 경우, 1번의 Convolution 수행은 각각의 pixel의 이웃들의 값이 겹쳐지는 비중이 점점 커지게 된다. 쉽게 설명하자면 망이 깊어질수록 Feature map의 크기가 점점 작아지기 때문에, 1번의 Convolution이 차지하는 영역의 크기가 커서 이웃한 픽셀끼리의 값들이 겹치기 쉽다는 뜻으로 해석한다.또한, 두번째 문제는 망이 깊어질수록 지역적인 패턴은 없어지기 십상이라는 것이다. 이를 해결하기 위해 GPM이 나온것인데, 동작 원리는 다음과 같다.우선 GPM은 NxNxC 크기의 feature map을 nxn 의 크기로 나눈다. (Figure3 에서 n=2 로 설정할 경우 2x2 로 나뉜것을 볼 수 있다.) 그 후 Kg x Kg 크기의 커널로 Convolution을 진행한다. 이를 위해선 nxn 크기로 나눈 feature map을 위 그림처럼 세로로 쌓는 형태로 reshape을 거쳐야 한다. 그래야만 해당 크기만 따로 convolution을 진행할 수 있기 때문이다.nxn 으로 나눈 후, 해당 소구역에서만 convolution을 한 후, 다시 합치니까 언뜻 보기에 dilated convolution으로 보일 수 있다는 것이다. dilated convolution과의 차이점은 dilated convolution은 이웃한 픽셀값들은 연산에 사용되지 않는데, GPM에선 사용된다는 것이다. 이렇다는 것은 nxn 크기로 나눈 구역별로는 Local regression이 적용되고, 나눠진 부분끼리는 local regression이 적용되지 않는다는 것이다.이렇게 하면 Local pattern과 Global diagram이 동시에 보장된다는 장점이 있다고 한다. ​​3.3 Attentive Feedback ModuleAttentive Feedback Module은 AFM으로 이 논문에선 줄이고 있고, 이를 나타낸 그림은 Figure2에 있다.   위 그림은 5개의 AFM이 계층적으로 쌓여있는 모습을 나타낸 것이고   ​위 그림은 각각의 AFM의 내부구조를 나타낸 것이다.본 논문에서는 각 계층간 정보교환을 AFM을 통해 조절하고자 하였다. 위 그림은 AFM의 Recurrent 스타일의 동작방식을 보여주고 있다.​   ​전체적인 AFM 구조를 다시 쉽게 설명하자면Encoder에서 추출한 feture map은 아랫 단계의 decoder에서 올라온 upscaling 된 fetaure map과 concate 되어 decoder로 입력된다. 또한, encoder와 decoder의 값들은 다음 time step의 같은 레벨의 AFM으로 흘러들어가 Recurrent 구조를 완성하게 된다.여기서 사용된 time step의 개념에 대해 다시 알아보도록 하겠다.위에서 설명한 그림에는 실선과 점선이 있는데, 실선은 first time action을 의미하고, 점선은 second time action을 의미한다. 본 논문의 network는 2step 방식으로 동작한다고 설명하고 있는데, 2step 방식을 통해 재귀 구조를 완성한 것이라 생각한다.​3.4 Boundary-Enhanced Loss일반적인 Convolution들은 output 경계면이 blur 해지는 단점이 있다. 아무래도 zero padding과 같이 테두리에서의 문제가 해결되지 않기 때문이다.이를 해결하고자 이 논문에선 BEL(Boundary Enhanced Loss)를 제안했다. 이 Loss 함수는 Cross-entropy loss를 사용하고 있다. 또한, 직접적으로 boundary Loss를 계산하기 보다는 Average pooling 을 통해 boundary 의 loss를 구하고 있는데, 이를 통해 영상 가운데 부분의 boundary만 선명하고 가장자리는 blur한 문제를 어느정도 극복했다고 한다.      ​ "
5. TensorFlow2 Object Detection API설치(1) ,https://blog.naver.com/alsdhr0155/222557409969,20211103,"# 20211119 일부 내용 수정(뒷부분 오류로 인한 내용추가)​안녕하세요? 이번 포스팅에서는 지금까지 정리한 개념을 활용해 Object Detection API를 설치해보겠습니다.저는 현재 특허 관련 공기업에서 인턴 일을 하고있는데요, 지금까지 특허명세서와 서지사항에 라벨링 이름을 정하고 라벨링 하는 작업을 했습니다. 그렇게 학습시킬 데이터셋을 구축했고 드디어 직접 학습시킬 날이 왔습니다.api설치도 복잡하고 어려웠답니다.. 오류도 엄청 떠서 해결하려고 구글링을 참 많이 했는데, 학교 과제하는 기분이었답니다. ​< 본격적으로 들어가기 전에 제가 알았다면 좋았을 사항에 대해 적어보려합니다. >1. 사용자 등의 이름에 한글 들어가면 안된다. (아마 작동 안되는 것의 대부분 원인인 듯)=> 따라서 기본 설정인 USER(여러분 사용자명)이 아닌, C드라이브에 새로 영어파일 하나 만드시고,      그 안에서 작업하세요!=> 이 또한 cd C:\  키워드로 연결해주세요​2. 경로설정을 제대로 해야한다. ex.내가 만든 가상환경에서 작업하고 있는지, 아님 그냥 base에서 하고있는지 등을 확인하기​3. 해당 패키지 어떤 버전 설치 가능한지 확인 : conda search[패키지이름]:      conda install [numpy==1.17.0]   1. Object Detection API를 사용할 환경 설치 (여러분 환경에 아나콘다 설치되어 있어야합니다.) conda create –n tf_3.7 pip python=3.7 # tf_3.7이라는 가상환경 생성 pip install --ignore-installed —upgrade tensorflow #base에(가상환경 진입 전에) 텐서플로 설치하려니까 오류남ㅠERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.spyder 4.2.5 requires pyqtwebengine<5.13, which is not installed.conda-repo-cli 1.0.4 requires pathlib, which is not installed.anaconda-project 0.9.1 requires ruamel-yaml, which is not installed.tensorflow-metadata 1.4.0 requires absl-py<0.13,>=0.9, but you have absl-py 0.15.0 which is incompatible.spyder 4.2.5 requires pyqt5<5.13, but you have pyqt5 5.15.4 which is incompatible.​저와 같이 오류가 나셨다면 아나콘다에 설치 안된거(spyder나 R studio등등) 다시 설치하신 후에 가상환경에 진입하시고 진행하세요! 또한 cd 키워드로 파일 경로 설정 가능합니다. conda activate tf_3.9 # 가상환경에 진입(tf_3.9) C:\>cd tf_1 # cd키워드를 통한 C드라이브 tf_1 파일로 경로 설정(tf_3.9) C:\tf_1  > pip install --ignore-installed --upgrade tensorflowpython -c ""import tensorflow as tf;print(tf.reduce_sum(tf.random.normal([1000, 1000])))"" # 잘 설치된건지 확인하는 코드  2. Tensorflow Object Detection API 다운로드 및 컴파일: protoc zip 다운받고 [C:\Program Files\Google Protobuf]의 경로 만들어서 Google Protobuf에 파일 저장하기 (tf_3.9) C:\tf_1>cd models/research #  Object Detection API를 컴파일 하기 위해서 API 폴더(models)에서# reserch 경로로 이동해서 다음과 같은 명령어로 컴파일을 해줍니다C:\tf_1\models\research >protoc object_detection/protos/*.proto --python_out=.# 해당 경로에서!!! git에서 설치한 protoc을 사용할 수 있게 됩니다! (tt) C:\tf_1\models\research>protoc object_detection/protos/*.proto --python_out=. 'protoc'은(는) 내부 또는 외부 명령, 실행할 수 있는 프로그램, 또는배치 파일이 아닙니다. # 이런 경우 현재 환경변수 설정에서 path가 정해져있는지 봐야한다.[환경변수] - [시스템변수] - [path] - [C:\Program Files\Google Protobuf\bin 추가] # 이래도 안될 경우 ""C:\Program Files\Google Protobuf\bin\protoc.exe"" object_detection/protos/*.proto —python_out=.      을 아나콘다 프롬프트에 입력=> 컴퓨터는 띄어쓰기를 구분해야할 명령어로 보기 때문에 주소내에 띄어쓰기 있으면 “”로 묶어줘야한다.  3. COCO API 설치저는 coco api 설치에서 참 애먹었습니다ㅠㅠ 앞에까진 잘 됐는데 여기서 오류가 많이 뜨더라고요..현재 텐서플로우 모델이 설치되어있는 경로에 coco api를 설치하셔야해요!!!!저의 경우 (tf_3.9) C:\tf_1\models\research> 에서 시작합니다. conda install git -ypip install git+https://github.com/philferriere/cocoapi.git#subdirectory=PythonAPI git 파일을 설치하는 과정에서 오류가 났었는데요, 아주 이게 시간이 오래 걸렸답니다. 스택오버플로우, 깃허브 등을 돌아다녔는데, 저에게 맞는 해결책은 없었네요..(특히 영어라 더 힘들었어요ㅠㅠ) 결국 과장님의 도움을 받아  원인을 2개 정도로 추려보았어요.1) 환경변수 설정의 문제이거나, 2) visual studio build up tool 이 제대로 설치되지 않은 것으로 추정되네요.​1) 환경변수 경로 설정: 첨부한 사진처럼 [시스템 변수]에서 [PATH] 변수를 편집하세요! 아마 오류 원인이 아나콘다와 각종 파일 연결이 잘 안돼서 그런 것일 수도 있어요! 같이 인턴하시는 분이 제가 한 것 처럼 변경하신 후에 설치 되셨다고 해요! 2) buildup tool: Visual studio 설치하실 때 아래 사진처럼 옵션 선택 후 재설치해보세요!   빌드업 도구 문제로 오류 뜬 것일 수도 있습니다!  4.  Object Detection API 설치 (tf_3.9) C:\tf_1\models\research>copy C:\tf_1118\models\research\object_detection/packages/tf2/setup.py .# 본문 상에는 'cp'로 되어있는데, 이는 리눅스에서만 실행 가능합니다. 아나콘다는 copy 명령어를 사용합니다.(tf_3.9) C:\tf_1\models\research>python -m pip install . 를 실행하면 마지막 코드에서 오류가 발생합니다! 오류라기보다는  DEPRECATION. (tf_3.9) C:\tf_1\models\research>python -m pip install . DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555. 이럴때는 뭐가 잘못된건지 잘 읽어보시면 됩니다. 친절하게 링크까지 알려주네요! 자세한 설명을 원하시면 링크 들어가시면 됩니다. 기존의 DEPRECATION이 발생했던 코드에 [--use-feature=in-tree-build] 라는 flag를 붙이시면 됩니다. [ (tf_3.9) C:\tf_1\models\research>python -m pip install . --use-feature=in-tree-build ] 그럼 성공적으로 설치! (tf_3.9) C:\tf_1\models\research>python object_detection/builders/model_builder_tf2_test.py# 잘 설치 됐는지 TEST하는 코드# Ran 24 tests in 26.857s  OK (skipped=1) # 이렇게 뜨면 성공!! +)1119 사뭇 남다르고 무서운 warning 발생WARNING: --use-feature=2020-resolver no longer has any effect, since it is now the default dependency resolver in pip. This will become an error in pip 21.0. 이 새롭게 뜨기 시작. 뒤에 단계에서 이유 알 수 없는 utf-8 오류로 인해 처음부터 다시 시도하면서 생긴 오류. 과장님도 처음 보신 오류같음.​​​  objective detection api 설치나 오류 관련 글은 다 영어거나 뭐라는지 잘 모르겠더라고요ㅠㅠ 제가 겪은 오류와 해결이 여러분들께 조금이나마 도움이 되셨으면 좋겠습니다!!머신러닝 관심 있으신 분들도 한번 해보시면 좋을 것 같아요!​1) Objective Detection API  튜토리얼: 설명이 잘 되어있어 혼자 공부하시기에 좋을 것 같습니다! 다들 가장 많이 사용하시는 사이트 같아요.: 단, 영어와 전문용어의 혼종이라 저는 이해하는데 시간이 좀 걸렸습니다!https://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/install.html Installation — TensorFlow 2 Object Detection API tutorial documentationAnaconda Python 3.8 (Optional) Although having Anaconda is not a requirement in order to install and use TensorFlow, I suggest doing so, due to it’s intuitive way of managing packages and setting up new virtual environments. Anaconda is a pretty useful tool, not only for working with TensorFlow, but...tensorflow-object-detection-api-tutorial.readthedocs.io 2) 튜토리얼 한글판(?): 위의 튜토리얼의 한글판 느낌이에요! 영어로 이상하게 이해하던 부분을 이 포스팅 덕분에 해결했습니다. https://like-edp.tistory.com/10 Tensorflow2 Object Detection API 설치 및 환경 구성반갑습니다! 이번 글에서는 Tensorflow 2.x 버전을 기반으로 하는 Object Detection API를 Anaconda 가상 환경에 설치하고 확인하는 방법에 대해서 진행해보려고 합니다. 지금부터 설치하는 환경은 Windows10에서..like-edp.tistory.com ​ "
[PaperReview] End-to-End Semi-superviesed Object Detection with Soft Teacher [작성중] ,https://blog.naver.com/johnny9696/222572016810,20211118,"논문 원문글 https://arxiv.org/abs/2106.09018 End-to-End Semi-Supervised Object Detection with Soft TeacherThis paper presents an end-to-end semi-supervised object detection approach, in contrast to previous more complex multi-stage methods. The end-to-end training gradually improves pseudo label qualities during the curriculum, and the more and more accurate pseudo labels in turn benefit object detectio...arxiv.org ​1.IntroductionImage Net같은 대량의 데이터가 트리거 역할을 하여 딥러닝을 이욯한 컴퓨터 비젼이 발전 했다. 하지만 학습을 하는데 드는 시간과 annotatino에 드는 높은 비용으로 인해서 대량의 label되지 않는 데이터를 학습 시키는 방안이 필요 했다. 이러한 이유로 등장한 것이 self-superviesed learning과 semi-supervised learning이다. 이외의 내용들은 soft teacher 모델 관련된 설명임으로 뒤쪽에서 자세하게 설명하도록 한다.  2.Related Works1) Semi-supervised learning in image classification         (1) consistency based model         (2) pseudo-label based model2)Semi-supervised learning in object detection         (1) consistency based model         (2) pseudo-label based model3)object detection         (1) single stage object detectors         (2) two-stage object detectors  3.Methodology아래 그림을 참고하여 Soft teacher에 대한 설명을 진행을 한다. 이 논문에서는 2가지  모델을 이용하여 학습을 진행한다고 제시를 한다. Semi-supervied learning을 이용한다. <그림 1> Soft Teacher process2가지 모델은 teacher & student 모델이다. teacher model의 경우 unlabel 된 이미지를 이용하는 unsupervied learning이고 student모델의 경우 label된 이미지와 unlabel된 이미지를 동시에 이용하여 학습을 한다. 위의 이미지와 같이 teacher모델은 student모델의 EMA를 이용한다.​EMA : 설명 링크 https://dohk.tistory.com/204 이동평균법(EMA) 개념                                                                                                                  ..dohk.tistory.com 3.1.End-to-End Pseudo-Labeling FrameworkTeacher모델의 경우 unlabel된 이미지에 pseudo box를 만들어 낼 수 있도록 한다. Student모델의 경우 Label된 이미지와 Unlabel된 이미지에 box가 쳐진 것을 이용한다. Student Model의 경우 supervised 와 unsupervised를 이용하여 두 가지 loss를 합하여 loss값을 구한다. Ls는 Supervied에 대한 loss값이고 Lu는 unsupervied에 대한 loss값이다. 각각에 대한 설명을 아래 수식을 통해 확인하면 된다. <그림 2> 전체 loss 계산식라벨이 되어 있는 이미지에 대해서는 classification을 진행을 하고, 라벨이 되지 않은 이미지에 대해서는 teacher모델을 통해서 나온 많은 bounding box에 대해서 Non-maximum suppression(NMS)를 이용하여 박스의 수를 줄인다. NMS에 대한 설명은 아래 링크를 참고하기를 바랍니다.https://blog.naver.com/johnny9696/222571951907 [AI] Non-maximum Suppression(NMS)Input image가 Object detection 모델을 통과하여 나오면 Object에 Bounding box(B-box) 가 나오게 되...blog.naver.com  이러한 과정을 통해도 검출한 box에도 아무것도 존재하지 않는 box들이 있을 수 있음으로 foreground score를 매기고 threshold를 정해서 높은 값을들 가지는 box만 남긴다. 이 Threshold에 대하여서는 3.2절에서 나온다.논문에서는 높은 quality의 pseudo box를 얻고 student모델에 대한 학습을 위해서 FixMatch라는  최신의 semi supervised image classification을 이용한다고 합니다. Student 모델에 대해서는 많이 왜곡된 사진을 사용하고 teacher 모델에 대해서는 약한 왜곡을 한 이미지를 사용한다고 합니다.​3.2.Soft TeacherDetector의 성능은 pseudo-label의 질이다. 좋은 질의 데이터를 얻기 위해서 foreground score에 threshold를 걸어 실험적으로 값을 구했다. 아래 표를 참고하면 된다. 실험적으로 0.9에서 가장 좋은 성능이 나왔다. 하지만 theshold를 0.9로 잡았을 때 precision의 수치가 98%센트로 높은 반면에 recall의 수치는 33퍼센트로 낮다. 이러한 상황에서 IoU(Intersection of Union)을 사용할 경우에 일부 foreground labels들이 학습과 성능에 악영향을 줄 수 있다고 한다.​IoU 설명 링크 : https://blog.naver.com/johnny9696/222572692858 [AI] IoU (Intersection over Union)이미지 detection에서 사용하는 loss방식중 하나 보라영역과 하늘색 영역이 ground truth(A영역)라고 하고...blog.naver.com 이러한 문제를 해결을 하기 위해서 soft teacher방안을 논문에서 제시합니다.  Teacher 모델로부터 더 풍부한 데이터를 얻을 수 있다. Student 모델에서는 foreground box와 background box 를 세트로 이용하여 unlabel 이미지에 대한 classification loss와 weight값을 구한다. 이에 대한 식은 아래와 같다. ​3.3.Box JitteringBox jittering 이란 개념 자체가 논문을 읽으면서 단어의 해석부터 이해가 잘 안되는 부분이였다. 그래서 내가 이해한 부분에 대해서 최대한 자세히 적으려고 한다.먼저 Jittering이라는 의미가 불안정한, 흔들리는, 이러한 의미를 가지고 있다. 그래서 나는 Box Jittering이라는 것의 해석을 불안정한 박싱, 또는 정답 박스로 부터 주변에 그려지는 박스로 해석을 하려고 한다. 이부분에 있어서 오류가 있을 수 있으니 정확하게 아시는 분이 있으시면 설명 해주시면 감사하겠습니다.​아래 표를 보면 정확도에 대한 localization과 foreground score는 엄청나게 긍정적인 관계를 보여 주지는 않는다. 즉 high score foregroun가 정확한 localization정보를 제공하지 않을 수 도 있다는 점이다. 이러한 점은 teacher 모델이 생성하는 pseudo box가 box regression에 적합하지 않을 수도 있다는 말이다. 따라서 teacher 모델이 생성한 pseudo box에 대해서 jitter box를 형성한다. 관계식은 아래와 같다. 위의 식을 이용해서 jitter box들을 만들고  이 박스들에 대해서 localization reliability 계산한다.   아래 식은  시그마에 대한 normalize 계산 식이다. 작은 box들은 위의 식에 영향을 많이 받게 된다. 또한 학습시 너무 많은 량의 계산이 소모 됨으로 박스의 수를 줄이기 위해서 foreground score가 0.5이상인 것만을 이용하여 학습을 했다고 한다.​ 위의 그래프를 보면 box regression variance와 localization accuracy가 서로 연관이 있음을 보여준다. Foreground score와 비교하였을 때  box regression variance가 localization accuracy에서 좀더 좋은 결과를 보여준다.    4.ExperimentsMS-COCO에서 2가지 데이터 셋을 이용하였다. train2017set (labeled image)와 unlabeled2017을 사용했다.   5.Conclusion - 내 생각이 논문은 end to end frame의 semi supervised learning에 대한 모델 제시를 했다. 논문을 보면서 느낀 점은 이미지 detection분야에 대한 기초지식의 부제가 논문을 이해하는데 어려움을 주었던 점들이 조금 있었다. 연구 방향성을 정해야 되는 시기에 있어 고민이 많은데, 이미지 detection이나 segmentation관련하여 조금은 나에게 도움이 되었던 부분이다. 최근 연구 동향이 semi supervised learning이라고 하는데 관련 논문에 대해서 많이 찾아 보고 시간이 된다면 종종 논문 정리를 해볼 생각이다.많이 부족한 정리지만 혹시나 잘못된 부분이 있거나 질문이 있으시면 아는 선에서 답해 드리고 수정하도록 하겠습니다. "
딥러닝 기반 object detection 과정 정리 (5) - 중반부 (YOLOX까지)  ,https://blog.naver.com/taeeon_study/222788722357,20220625,"앞선 중반부에서 했던 CNN 기반 네트워크 리뷰를 이어하도록 한다. (이제 이거 쓰면 후반부 transformer 기반 네트워크만 남았고만.)​EfficientDet [1]논문의 Introduction에서 2가지 main challenge를 언급하고 있다.​efficient multi-scale feature fusion1- Stage Detector는 FPN을 사용하고 있는데 기존 모델들은 cross-scale-feature fusion network 구조를 개발해왔다. 이 논문에서는 input feature들 각 해상도가 다르기 때문에 output feature에 기여하는 정도를 다르게 가져가야 함을 언급하여 BiFPN(weighted bi-directional FPN)구조를 제안하게 된다.​2. model scailing기존에는 거대한 backbone network나 큰 input image size에 의존하여 accuracy를 높였다. 여기서는 compound scaling을 제안하여 높은 성능을 달성하게 된다. ​​Cross-scale connections Feature network design위 그림에서 FPN을 위한 방법들을 나타내고 있다. 간단하게 PANet은 bottom-up pathway를 FPN에 추가한 것이다. NAS-FPN은 Auto ML의 Neural Architecture Search를 FPN 구조에 적용한 것이다. NAS-FPN도 한 번 심도있게 공부해볼만 한 것 같다.여기서 제시한 BiFPN은 같은 scale에서 edge를 추가하여 더 많은 feature들이 fusion 되도록 구성하였다.​2. Weighted Feature Fusion전통적인 Feature Fusion은 서로 다른 resolution의 input feature들을 합칠 때, 일반적으로는 같은 해상도가 되도록 resize 시킨 뒤 합치는 방식이었다. 이 논문에서 제안하는 것은 input feature에 가중치를 주고 학습을 통해 가중치를 배울 수 있는 방식을 제안하게 된다.  Weighted Feature Fusion위 이미지 출처: https://airsbigdata.tistory.com/213 [논문 리뷰] EfficientDet: Scalable and Efficient Object Detection Review안녕하세요! 오늘은 랩미팅 중 교수님께서 추천해주신 모델인 EfficientDet 을 리뷰해볼려고 합니다. Google Brain팀에서 publish한 논문이며, Code도 사용할 수 있도록 GitHub에 올려있습니다. Introduction Eff..airsbigdata.tistory.com Learnable weight는 스칼라로 사용하였다. 위 그림에서 Conventional이나 Unbounded나 SoftMax-based는 불안정성 유발 문제가 있었다. 여기서 Fast normalized fusion을 제안하는데 weight들이 RELU 함수를 거치기에 0이 아닌 것이 보장되고 분모가 0이 되는 것을 막기 위해서 0.0001의 임의 상수 입실론을 추가한다.아키텍처는 밑 그림과 같다. EfficientDet architecture​YOLO v4 [2]정확도와 실시간 속도 모두를 만족하는 object detector가 필요했다. 그렇기에 본 논문의 주요 목적은 BFLOP이 아닌 생산 시스템에서 빠른 속도로 동작하는 detector를 고안하고 병렬 계산을 최적화하는 것이었다.GPU의 경우에는 conv layer 내 그룹 수가 작은 네트워크를 이용한다.VPU의 경우에는 grouped-convolution을 사용한다. (EfficientNet-lite etc...)​- Selection of architecture두 가지 목적이 있다. Network의 입력 해상도, conv layer의 개수, parameter 개수, layer 출력 개수 가운데서 최적의 balance를 발견하는 것과 receptive field를 늘릴 수 있는 additional block들과 서로 다른 detector level들을 위한 서로 다른 backbone level들로부터 parameter aggregation을 위한 최상의 기법을 선택하는 것이다. (FPN, BiFPN etc...)최종으로 선택된 기법은 아래와 같다. PANet을 선택한 것도 볼 수 있다. 또한 head 부분은 YOLO v3를 가져온 것을 볼 수 있다.  final selection위 이미지 출처: https://neverabandon.tistory.com/22 [논문읽기/2020] YOLOv4: Optimal Speed and Accuracy of Object Detection문서 버전 발표 버전 Link: https://arxiv.org/pdf/2004.10934.pdf Abstract Convolutional Neural Network (CNN)의 정확도 개선을 위한 수많은 features들이 존재 이러한 feature들의 조합에 대해 대규모 데이..neverabandon.tistory.com - Selection of BOF and BOS훈련 개선을 위해 CNN은 보통 아래의 기법들을 사용하게 된다. method to improve- Additional improvements  Data augmentation 기법을 새로 도입했다. Mosaic은 4개의 training image들을 1개로 mix 할 수 있다. 이를 통해 normal한 context 외부의 object들도 detection 가능하게 만들었다. 다음은 SAT이다. 이는 2단계로 동작한다. 첫 번째는 neural network가 network의 weight 대신 원본 이미지를 변경한다. 그리고 두 번째 단계에서 neural network가 변경된 이미지에서 정상적인 방식으로 detection 하도록 훈련된다. ​2. 하이퍼 파라미터 최적화 수행이다. 여기서 genetic 알고리즘을 사용한다. genetic 알고리즘은 휴리스틱 최적화 기법 중 하나로 인간의 유전 법칙을 토대로 발전한 알고리즘이다. ​3. 효율적 훈련과 detection을 위해 현존 기법을 적합하게 바꿨다. Modified SAM이 있다. SAM을 spatial-wise-attention에서 point-wise attention으로 변경한 것이다. Modified PAN은 shortcut connection을 concatenation으로 교체한 것이다. Cross mini-Batch Normalization은 단일 batch 내에서 mini-batch들 사이에 대한 통계를 수집하는 방법이다. Cross mini Batch normalizationModified SAM and modified PANYOLO v4의 최종 디테일이다. detailYOLO v5 기본적인 배경 없이도 유튜브나 구글링 같은 검색을 통해 가장 쉽게 사용할 수 있다. v5는 논문이 아닌 걸로 알고 있다. 아래는 전체 구조이다.  architecture그림 출처: https://www.researchgate.net/figure/The-network-architecture-of-Yolov5-It-consists-of-three-parts-1-Backbone-CSPDarknet_fig1_349299852​Backbone은 잘 알려져있다. CSPNet을 사용한 것이다. 이를 앞에서 언급하지 않고 온 것 같아 간단히 설명한다.​CSPNet [3]3가지 이슈를 다루고 있다. 첫 번째는 Strengthening learning ability of a CNN, 두 번째는 Removing computational bottlenecks 마지막으로 세 번째는 Reducing costs이다.Method를 간단하게 정리하면 아래와 같다.​- Cross Stage Partial NetworkDenseNet(연결을 통해 이전 레이어와 현재 레이어를 합쳐서 Dense block을 이룸)을 사용하게 된다. DenseNet의 forward pass와 backward pass 과정에서 수많은 gradient 정보가 재사용된다고 언급한다. 이를 어떻게 해결했는지 아래 그림을 통해 할 수 있다. 여기서 다양한 backbone 모델을 전혀 건들지 않았고 모든 backbone에 아이디어를 적용할 수 있는 게 큰 장점이라고 생각한다. 이러한 구조는 DenseNet의 출력 값 연결을 통한 재사용을 유지하면서 gradient 정보가 많아지는 것을 방지한다.​- Exact Fusion Model(EFM)EFM은 FoV를 적절하게 캡처해서 1-stage detector를 강화할 수 있다고 한다. 아래 그림을 통해 알아볼 수 있다. EFMYOLOX [4]기존 YOLO v5와 EfficientDet보다 더 좋은 성능을 보이고 있다고 나와있는데 유명하지는 않다. 주요 변화는 크게 2가지이다. 첫 번째는 Anchor-free 방식이고 두 번째는 좀 더 발전 기술을 적용한 것이다. backbone은 Darknet53을 사용하고 있다. FPN과 SPP라는 것을 추가하여 Neck에 넣은 것으로 보인다. 그리고 여기서 YOLO v3를 다시 사용했다고 나와있다. 결국은 YOLO v3는 정말 좋은 Head임이 분명하다.YOLOX는 Classification과 Localization을 2개의 헤드로 분리했다. 그래서 Classification은 각각의 class에 대한 확률이 나오고 Localization은 bounding box에 대한 거리가 나온다. 이것이 성능 향상을 일으켰다고 나와있다. - Anchor free기존의 anchor를 기반으로 하면 문제가 많았다. 그래서 여기서는 FCOS라는 어떤 포인트가 예측되면 그 포인트와 실제 ground truth와의 거리를 학습하여 예측하는 기법을 도입한다. 또한 simOTA를 사용하여 최적화 문제를 풀었고 Multiple positives에서는 center point를 1x1에서 3x3으로 확장해주는 방법을 택했다고 나와있다.나머지는 기존 YOLO에 있던 것이 많다. 사실 YOLOX는 2021년에 나왔는데 ViT 같은 네트워크가 워낙 핫해졌고 이제부터는 연구는 transformer 기반으로 많이 넘어간 것으로 보인다. 그렇기에 후반부는 transformer 기반 접근을 알아보도록 한다.​원본 논문은 아래에 첨부하였다.[1] TAN, Mingxing; PANG, Ruoming; LE, Quoc V. Efficientdet: Scalable and efficient object detection. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2020. p. 10781-10790.[2] BOCHKOVSKIY, Alexey; WANG, Chien-Yao; LIAO, Hong-Yuan Mark. Yolov4: Optimal speed and accuracy of object detection. arXiv preprint arXiv:2004.10934, 2020.[3] WANG, Chien-Yao, et al. CSPNet: A new backbone that can enhance learning capability of CNN. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops. 2020. p. 390-391.[4] GE, Zheng, et al. Yolox: Exceeding yolo series in 2021. arXiv preprint arXiv:2107.08430, 2021. "
"[YOLO] You Only Look Once : Unified, Real - Time Object Detection ",https://blog.naver.com/tomatian/221889494099,20200403,"You Only Look Once : Unified, Real - Time Object Detectionhttps://arxiv.org/abs/1506.02640​​​Abstraction & Introduction​Yolo 소개 및 요점​  ​① single neural network     = single pipline    = Bounding Box + Classification 동시에 한 번에 진행    = full image input pixel 으로 부터 한 번에 bounding box와 classification 진행    = single convolutional network    = Figure 1에서 보다 시피 하나의 convolutional network로 구성 ( = network 를 구성하는 pipeline이 한 개 )         → end to end 로 optimize 가능         → Fast  → 자율 주행과 같은 실시간 영성 detection이 가능하다.​② Background로 인한 false positive를 적게 만든다.​③ object에 대해 general한 representation을 학습한다.   → natural 이미지부터 artwork 까지 detection 가능​④ image 에 대해 globally 학습 진행    = test, train 하는 동안 전체 이미지에 대해 training, test 진행한다. ( = region proposal을 하지 않는다. )        → class에 대한 contextual 한 특징도 학습가능​⑤ 하지만 accuracy 면에서는 떨어진다.     → 작은 사이즈의 object를 인지하는 능력이 떨어진다.​​​​​​Unified Detection = Yolo Architecture​1. logic  ​① Input 이미지· entire image 전체를 S X S grid로 나눈다.​​② Bounding Box + Confidence· S X S의 grid는 각각 B개의 bounding box를 만든다. · 그럼 총 S X S X B 개의 bounding box가 생성· 각 bounding box에는 하나의 confidence score가 생긴다.    → confidence score = ""box가 물체를 포함하고 있는지"" + ""얼마나 정확하게 물체를 box가 감싸고 있는지""  < confidence score 계산 공식 >​· Pr(Object) = ""Object 가 있을 확률 (유무 한단) 유 = 1, 무 = 0 ""· IoU = ""얼마나 정확하게 물체가 box를 감싸고 있는지""· 결국 confidence score를 IoU 값에 가장 가깝게 하는 것이 목표다. ( ∵ Pr = 1 → confidence score  = IoU )· 각 bounding box는 5가지 parameter를 가지고 있다.     = x, y, w, h, confidence    → x, y는 box의 center 좌표를 나타낸다.​​③ Class Probability map· S X S의 grid는 각각 C개의 class 조건부 probability를 가진다.· 여기서 조건부 확률인 이유는 grid에 object가 있을 때만 확률을 계산하기 때문에 조건부 확률이다.   ex ) object가 있을 경우, 그 object가 Class(i) 일경우의 확률  < class의 조건부 확률 >· C는 B 값과 무관한 파라미터다.​​④ Class-specific confidence score  < class + localization 정보 모두 포함한 식 >​· Pr( class | object )는 class가 있을때만 존재하는 확률이라 Pr( object )는 자동으로 0이 된다.​  image gridS x Sbounding boxS x S x Bconfidence scoreS x S x Bthe number of bounding box parameter5 ( x, y, w, h, confidence )the number of predicted classCfinal tensorS x S x ( B*5 + C )  ​​​2. Network Design​ 세어 보면 총 24개의 convolutional layer가 존재 ​ 이 사진이 더 논문의 설명에 부합하는 사진 ​· initial convolutional layer = 이미지로부터 feature 추출· fully connected layer = output 확률과 x,y,d,h coordinate를 예측하는 layer· 24개의 convolutional layer로 구성 + 마지막에 FC layer 두 개· 3 x 3 convolutional layer 사이 사이에 1 x 1 reduction layer를 넣는 것은 channel 수를 줄이기 위함​​​​​​​Experiments​​  Real - Time detector 중에서는 성능이 좋은 편이다.  yolo 자체 만으로는 accuracy가 굉장히 낮기때문에Fast R-CNN과 결합한 모델을 만들어서 성능을 높였다.​​​​​그림 출처 ) https://www.youtube.com/watch?v=L0tzmv--CGY&feature=youtu.be​​  이상 동산이었습니다.정상에서 만나요 :)​​​​​​ "
"Mask_RCNN - Object Detection, Segmentaion ",https://blog.naver.com/handuelly/221840396866,20200306,"# RCNN 이번 포스팅의 주제는 Mask R-CNN이다.이 모델이 어떤 결과를 주는지 먼저 아래 사진과 영상을 통해 확인해보면 이해에 도움이 될 것이다.  출처 : https://github.com/matterport/Mask_RCNN R-CNN 계열 모델은 4가지 종류(R-CNN, Fast R-CNN, Faster R-CNN, Mask R-CNN)가 있다.RCNN -> Fast RCNN -> Faster RCNN 으로 오면서 예측력은 비슷하게 유지하면서 훈련과 테스트 모두에서 속도가 상당히 빨라졌다.앞에 3개는 모두 객체 검출만을 위한 모델이었으나, Mask R-CNN은 Faster R-CNN을 확장하여 Object Detection + Instance Segmentaion을 적용할 수 있는 모델이다.간략하게, Mask RCNN은 Faster R-CNN이 검출한 객체 각각의 박스에 Mask를 씌우는 모델이다.​따라서 MRCNN 모델은 Object Detection과 Semantic Segmentation을 동시에 한다.이를 위해서 기존의 Faster R-CNN은 객체 검출의 역할을 하고, 여기에 RoI(Region of Interest)에 세그멘테이션을 해주는 작은 FCN(Fully Convolutional Network)를 추가한 구조를 갖는다.​    # 실습 파일 및 코드 첨부파일MRCNN.ipynb파일 다운로드 실습 코드 첨부파일mscoco_labels.names파일 다운로드 90의 클래스 정보가 담긴 파일 첨부파일mask_rcnn_inception_v2_coco_2018_01_28.pbtxt파일 다운로드 Mask RCNN 모델 정보 파일 첨부파일colors.txt파일 다운로드 Segmentation 처리를 위한 컬러 값이 저장된 파일​tensorflow로 학습시킨 가중치 파일은 용량이 커서(대략 65MB) 업로드가 안된다.따라서 사이트에서 다운받고 설치하면 된다. tensorflow/modelsModels and examples built with TensorFlow. Contribute to tensorflow/models development by creating an account on GitHub.github.com Deep learning based Object Detection and Instance Segmentation using Mask R-CNN in OpenCV (Python / C++) | Learn OpenCVDeep learning based Object Detection and Instance Segmentation using Mask R-CNN in OpenCV (Python / C++) Sunita Nayak October 1, 2018 34 Comments The output of an object detector is an array of bounding boxes around objects detected in the image or video frame, but we do not get any clue about the s...www.learnopencv.com  # Mask RCNN 이제 코드를 작성하면서 아래 사진에서 객체를 검출하고, 해당 영역을 세그멘테이션 해보겠다.  사용할 원본 이미지 ​   # 2번 라인 : box를 선택/제거하는 기준 값을 지정한다.# 3번 라인 :  마스크 기준 값을 지정한다.​내부적으로 처리하는 함수들을 정의한다.     ​다음으로 클래스 정보를 불러온다.   # 1번 라인 : 90의 클래스로 객체를 구분하는 파일을 가져온다.# 7번 라인 :  tensorflow로 학습시킨 가중치 파일이다.     # 4~7번 라인 : 그래프 그리는 색(RGB값들)을 미리 지정해둔 것인데, Random으로 지정하면 검출 객체마다 구별이 힘들 수 있으니 사전에 정한 것이다.​이제 이미지를 불러와서 검출하고 세그멘테이션을 그려본다.   # 11번 라인 : 객체 검출, 검출한 객체에 대한 segmentation, 총 2개를 반환받아 저장한다.# 15~17번 라인 : 수행 시간에 대한 결과를 이미지에 텍스트로 출력한다.# 19~21번 라인 : 결과 이미지를 저장하고 출력한다.   ​많은 사람이 겹쳐있다는 점에서 얼마나 잘 구분하고 검출할 수 있는지 확인해보려고 일부러 이 사진을 선택했다.결과를 보면 사람으로 검출하고, confidence score도 확인할 수 있다.다음 포스팅에서는 Mask RCNN 모델이 어떻게 사람을 검출했는지 확인해보는 코드를 조금 더 다뤄보겠다.  # 참고 링크  matterport/Mask_RCNNMask R-CNN for object detection and instance segmentation on Keras and TensorFlow - matterport/Mask_RCNNgithub.com [Review] Mask R-CNN2018.01.24에 발표된 Mask R-CNN에 대해 리뷰하도록 하겠습니다. R-CNN 계열 모델은 R-CNN, Fast R-CNN, Faster R-CNN, 그리고 Mask R-CNN까지 총 4가지 종류가 있습니다. 1. R-CNN (2014) : Rich feature hierar..cdm98.tistory.com MASK RCNN 핵심이해 – Go LabFaster RCNN에서 Masking을 하는 레이어가 하나 추가된 형태로, 오브젝트 디텍션 뿐만아니라 Instance Segmentation까지 다루는 Two Shot Detector. 2019년에도, 각 Competition에서 Instance Segment 제일 선호되는 방법 현재 기술 발전상황은 하루가 멀다하고 새로운 논문이 바로 직전 State Of The Art를 갈아치우면서 발전을 하고있는 거듭하고 있는 상황이다. 예전에는 적당히 바운딩 박스와 물체를 구별해주면 되었는데, 지금은 해당 물체에 정확한 마스크까지 씌워주는 ...machinelearningkorea.com ​ "
"Object Detection in 20 Years - 02,  Traditional Detection / Deep Learning based Detection ",https://blog.naver.com/tory0405/222843224099,20220808,"앞장에서 설명한 것처럼 Object Detection은 크게 전통적인 방식(Traditional Detection)과 딥러닝 기반 방식(Deep Learning based Detection) 이 존재한다. [ 논문 원본] 첨부파일1905.05055.pdf파일 다운로드  그중 전통적인 방식의 대표적인 Detector를  먼저 살펴보도록 하겠다.  ​- VJ Detector( Viola Jones Detections) 18년 전 P.Viola와 M.Jones는 제약 없이 처음으로 사람의 얼굴을 실시간 감지했을 뿐만 아니라 당시 비슷한 탐지 정확도에서 다른 어떤 알고리즘보다 수십, 수백 배 더 빨리 감지하였고 이를 기념하기 위해 저자의 이름으로  VJ Detector라 명명하였다. ​VJ Detector는 가장 직접적인 검출 방법으로 sliding window를 이용하여  사람의 얼굴이 있을만한  모든 위치와 크기를 살펴보는 방식이다. 매우 간단한 과정으로 보이지만 그 당시 컴퓨터의 능력 이상의 계산량이 필요하였고 “integral image”, “feature selection”,  “detection cascades” 3가지 중요한 기술의 통합을 통해 감지 속도를 엄청나게 향상시켰다. ​- HOG Detection(Histgram of Oriented Gradient)HOG의 특징 설명자(Feature descriptor)는 2005년 N.Dala 와 B.Triggs에 의해 제안되었고 그 당시  scale-invariant feature transform 과 shape context의 중요한 개선으로 간주되었다. 특징 불변성과 비선형성의 균형을 맞추기 위해 HOG는 균일한 간격의 cell로 구성된 조밀한 grid에서 계산되었고 정확도 개선을 위해  local contrast normaliztion의 overlapping을 사용하도록 설계되었다. HOG는 다양한 객체 감지에 사용할 수 있지만 주로 보행자 감지(pedestrian detection) 문제에 크게 이바지했다. HOG는 VJ와 달리 다양한 크기의 물체를 감지하기 위해 sliding window의 크기를 변경하는 대신  rescale된  image을 반복적으로 input 하는  방식이다. ​- DPM( Deformable Part-based Model)사실 DPM 방식은 이 논문을 통해 처음 알게 되었고 이해하기가 쉽지 않았다.​말 그대도 직역하자면 ""변경 가능한 부분 기반 모델"" ......DPM은 2008년 P. Felzenszwalb의 해 HOG 검출기의 확장으로 제안되었고 R. Girshick에 의해 다양한 개선이 이루어졌다. DPM은 “divide and conquer” 철학을 따른다.  즉 훈련 데이터는 단순히 개체를 분해하는 적절한 방법에 대한 학습만 이루어지고 추론은 다른 객체 부분에 대한 앙상블로 간주하도록 설계됐다. 예를 들면 자동차를 감지하는 문제를  창(window), 차체(body), 바퀴(wheels)를 감지하는 것으로 간주하였고 이를 ""star-model""이라고 하였다. 일반적으로 DPM 검출기는 root-filter와 part-filter로 구성되어 있고. part-filter의 구성을 수동으로 지정하는 대신 weakly supervised learning으로 part-filter의 모든 구성을  latent variables로 자동으로 학습될 수 있도록 하였다.   R.  Girshick은 이 프로세스를 다중 인스턴스 학습의 특수한 경우로 공식화하였고 탐지 정확도 향상과 탐지 속도 향상을 위해 “hard negative mining”, “bounding box regression”,  “context priming”과 같은 중요한 기술과 함께 캐스케이드 아키텍처를 구현하는 훨씬 빠른 모델을 컴파일 할 수 있는 기술도 개발하였다.   다음은 딥러닝 기반 Detection 알고리즘으로 2010년 이후 딥러닝 기반 Detection 알고리즘은 침체기를 가져왔지만 2012년 이미지에 대한 강력하고 높은 수준의 특징 표현을 학습할 수 있는 Deep CNN과 RCNN(Regions with CNN)을 통해 다시 전례 없는 속도로 발전하기 시작하였다. 딥러닝 기반 Detection은  크게 CNN 기반 One state 감지기와 two state 기반의 딥러닝 감지기로 분류할 수 있다. ​CNN 기반 1단계 객체 감지기(CNN based One-stage Detectors)​- YOLO( You Only Look Once)​YOLO는  2015년 R.  Joseph에 의해 최초 1단계 검출기로 제한되었고  매우 빠르다 YOLO는 You Only Look Once의 줄임말로 이름에서 알 수 있듯이  “proposal detection + verification”의 이전 단계의 탐지 패러다임을 완전히 포기하였다. 대신 전체 이미지에 단일 신경망을 적용하는 완전히 다른 방식을 따른다. YOLO는 이미지를 영역으로 나누고 각 영역에 대한 ""bounding boxes""와 probabilities을 동시에 예측하도록 한다. R.  Joseph는  탐지 정확도를 더욱 향상시키면서 매우 높은 탐지 속도를 유지하는 v2, v3 버전을 을 내 놓았다.​- SSD( Single Shot MultiBox Detector)SSD는 2015년 W. Liu에 의해  2번째로  1단계 검출기로 제안되었고  multi-reference 와 multi-resolution 기술 적용으로 작은 물체에 대한 1단계 감지기의 감지 정확도를 크게 향상시켰다. 그리고 SSD는 감지 속도와 정확도 모두 장점이 있다. YOLO와의 차이점은 YOLO는 서로 다른 계층에서 서로 다른 척도를 사용하여 탐지하는 반면 SSD는 상위 계층에서만 탐지를 실행한다는 것이다. ​- RetinaNet빠른 속도와 단순성에도 불구하고 1단계 검출기는 수년 동안 2단계 검출기의 정확도를 따라잡았다. 2017년 T.-Y. Lin이  RetinaNet을 제안하면서 2단계 검출기의 경우 조밀한 검출기의 훈련 중에 발생하는 극단적인 전경-배경 클래스 불균형이 존재하는 문제를 발견하였고 이를 해결하기 위해   standard cross entropy loss을 재구성하는 “focal loss”이라는 새로운 손실 함수를 도입하여 검출기가 훈련 도중 hard 하고 잘못 분류된 예제에 더 집중할 수 있도록 한 것이다. Focal Loss 을 사용하게 되면 1단계 감지기가 2단계 가지와 대등한 수준의 정확도를 달성하면서 동시에 빠른 속도를 유지할 수 있다.​CNN 기반 2단계 객체 감지기( CNN based Two-stage Detectors) 1단계(one-state detection)와 2단계(two-state detection)로 그룹화할 수 있는데 one-step detection의 경우 ""coarse-to-fine"" 과정을 의미하고 two-state-detection은 1단계의 완료를 의미한다.   ​- RCNN( Regions with CNN)RCNN의 기본 개념은 간단하다. 선택 검색을 통해 일련의 객체 후보를 추출하는 것으로 시작한다. 그런 다음 각 proposal은 고정 크기 이미지로 다시 조정되어 ImageNet을 통해 훈련된 CNN 모델에 공급되어 Feature를 추출하는 방식이다. 마지막 단계에서 SVM 분류기가 각 영역 내에 객체의 존재를 예측하고 범주를 인식한다.  하지만 단점으로는 중복된 많은 proposal에 대한 계산에 의해 감지 속도가 매우 느려진다는 것이다. (GPU 기준 이미지 당 14초)이를 극복한 것이 SPPNet이다. ​- SPPNet(Spatial Pyramid Pooling Networks)2014년 K. He에 의해 제안된  모델로 RCNN에처럼 고정 고정 크기 입력을 사용하지 않고 SPP(Spatial  Pyramid  Pooling) layer를 별도로 두어 CNN이 이미지 또는 관심 영역의 크기에 상관없이 고정 길이 표현을 생성할 수 있게 하였다. SPPNet은 객체 감지를 위해 전체 이미지에 대한 Feature map을 한 번만 계산하여  Detector를 훈련하기 위한임의 영역의 고정 길이 표현을 생성할 수 있으므로 반복적으로 convolutional features 계산하는 것을 피할 수 있게 하였다. 무엇보다 탐지 정확도는 유지하면서 RCNN보다 20배 이상 빠르다는 것이 장점이라고 할 수 있다다만 SPPNet은 학습 단계가 multi-stage이고 이전 layer를 모두 무시하고 fully connected layers에 대해서만 조정이 가능하다는 한계도  존재한다. Fast RCNN이 이것을 극복하였다. ​- Fast RCNN2015년  R.  Girshick은 RCNN과 SPPNet을 더욱 개선한 Fast RCNN을 제안했다.Fast RCNN는 동일한 네트워크 구성에서 detector와 bounding box regressor를 동시에 훈련할 수 있게 하여 RCNN보다 200배 이상 빠르게 처리할 수 있었지만 여전히 탐지 속도에 대한 제한이 존재했다. ​- Faster RCNN2015년 S. Ren은 비용이 거의 들지 않는 Region Proposal Network (RPN)을 도입하였고  최초로 end-to-end 과  near-realtime deep learning detector를 제안하여 Fast RCNN의 속도 병목현상을 극복하였다. ​- FPN(Feature Pyramid Networks)2017년  T.-Y. Lin은 Faster RCNN을 기반으로 FPN을 제안하였는데 FPN 이전에는 대부분 딥러닝 기반 감지기가 네트워크 최상위 layer에서만 감지를 실행하고  깊은 layer에 있는 function은 범주 인식에 도움은 되지만 객체를  localizing objects 화하는 데는 도움을 주지 못했다. 이를 해결하기 위해 FPN은 모든 scale에서 높은 수준의 의미 체계를 구축하기 위해  lateral connections이 있는 top-down architecture를 이용하게 되었다. 기본적으로 CNN은 순방향 전파를 통해 자연스럽게 Feature Pyramid를 형성하기 때문에 이를 통해 FPN은 다양한 scale에서의 물체를 감지하는데 큰 기여를 하게 된다.  "
"OpenCV, Object Detection - 검출과 인식 ",https://blog.naver.com/handuelly/221832172834,20200301,"# 검출 vs 인식 일반적으로 ""객체 검출""과 ""객체 인식""을 혼동하기 때문에 명확히 다른점이 무엇인지 생각해볼 필요가 있다.얼굴 인식을 예로 들어 설명하면, 입력 영상으로부터 얼굴이라고 판단되는 영역을 찾아내는 것은 ""얼굴 검출""이다.과 검출은 다른 이슈라고 생각해야 한다. 그리고 검출된 얼굴에서 누구의 얼굴인지 식별/인식(Identification)하는 것이 ""얼굴 인식""의 영역이다. 검출(detection)과 인식(recognition)의 차이점일반적으로 사람들은 ""객체 검출""과 ""객체 인식""을 혼동해서 표현하곤 한다. 심지어는 컴퓨터 비전이라는 ...m.blog.naver.com  출처 : https://www.fritz.ai/object-detection/  # Detection Object Detection을 위해서는 객체를 정확하게 찾아내는 정확성과 많은 연산을 얼마나 빠르게 처리하는지에 대한 속도 성능을 확보하는 것이 매우 중요한 문제이다.모든 방향으로의 rotation, 입력 영상에서 객체의 다양한 scale 등 객체 검출을 위한 알고리즘이 필요할 것이다.계산을 빠르게 하기 위해서 Scanning 하는 횟수를 줄이거나, 분류기 모델을 개선시켜야 한다. 전통적인 방법에서 Scanning 횟수를 줄이는 것은 한계가 있고, 신경망 모델을 적용해서는 실시간 처리가 불가능하다.​영상에서 '패턴'을 찾아 객체를 검출하는 알고리즘에 대해서는 다음 포스팅에서 자세히 다루겠다. "
Deep learning object detection terminology ,https://blog.naver.com/zotm1517/222585951905,20211203,"▶classificationinput으로 주어진 image 안의 object의 class를 구분하는 행위​▶localization주어진 image 안의 obect가 image 안의 어느 위치에 있는지 그 위치 정보를 출력,주로 bounding box를 많이 사용... pixel의 좌표가 아닌 left top, 혹은 right bottom 좌표를 출력​▶object detectionclassification과 localization이 동시 수행...model의 학습 목적에 따라 특정 object만 detection하는 경우나 multi object detection하거나 등등.종종 localization의 의미로만 사용되는 경우도 O -> 이 경우에는 object 위치만 bounding box로 표현, class 종류는 구분 X​▶object recognition대게 object detection과 같은 의미로 사용but detection은 object의 존재 유무만 의미; recognition은 이 object의 class를 앎...object detection은 detect recognition보다 더 작은 의미​▶object segmentationobject detection을 통해 검출된 object의 형상을 따라 object의 영역을 표시​▶image segmentationimage의 영역을 분할. 이 영역들을 적당히 짬뽕해 object segmentation 수행​▶semantic segmentationobject segmentation: 같은 class or object 같은 color or boundary​▶instance segmentationsemantic segmentation: same class여도 각각 다른 색으로 segmentation​출처: https://light-tree.tistory.com/75 딥러닝 객체 검출 용어 정리 Deep learning Object detection terminology [1]공부를 하다 보면 용어의 정의에 대해서 정확히 알아야 할 필요가, 그리고 정리해두어야 할 필요를 느끼게 됩니다. 잘 정리해서 저장하고 저와 같은 필요를 느낄 분들의 불편함도 줄이고자 블로그에 정리합니다...light-tree.tistory.com ​ "
End-to-End Object Detection with Transformers 논문 리뷰(ECCV 2020) ,https://blog.naver.com/kingjykim/222893760168,20221006,"Paper : https://link.springer.com/chapter/10.1007/978-3-030-58452-8_13Git : https://github.com/facebookresearch/detrAuthor : Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko​​1. Introduction​ 객체 감지의 목표는 각 관심 객체에 대한 경계 상자 및 카테고리 레이블 세트를 예측하는 것이다. 현재까지의 감지 방법은 proposals, anchors 또는 window centers에 대한 간접적으로 회귀 및 분류 문제를 정의하는 간접적인 방식으로 이 세트 예측 작업을 해결한다. 그들의 성능은 post-processing 단계 해당하는 중복 예측을 제거하는 방법인 anchor 세트의 설계 및 anchor에 target boxes를 할당하는 heuristics에 의해 크게 영향을 받는다. 이러한 파이프라인을 단순화하기 위해, 저자는 간접 작업을 우회하는 직접적 세트 예측 접근법을 제안한다. 이러한 end-to-end 방법은 기계 번역 또는 음성 인식과 같은 복잡한 구조화된 예측 작업에서 상당한 발전을 이루었지만, 아직 객체 감지에서는 그렇지 않았다. 이전의 시도인 다른 형태의 사전 지식을 추가하거나 도전적인 벤치마크에 대한 방법은 강력한 기준점을 넘지 못했다. 이 논문은 이 격차를 해소하는 것을 목표로 한다.  객체 감지를 직접적 세트 예측 문제로 간주하여 훈련 파이프라인을 간소화한다. 저자는 시퀀스 예측을 위한 뜨고 있는 아키텍처인 transformers를 기반으로 하는 인코더-디코더 아키텍처를 선택했다. 시퀀스에서 요소 간의 모든 pairwise interactions을 명시적으로 모델링 하는 transformers의 self-attention 메커니즘은 이러한 아키텍처를 중복 예측 제거와 같은 특정 세트 예측 제약 조건에 특히 적합하게 만든다. Fig. 1: DETR directly predicts (in parallel) the final set of detections by combining a common CNN with a transformer architecture.  저자의 DEtection TRansformer(DETR, see Figure 1)은 모든 객체를 한 번에 예측하고 예측된 객체와 실제 객체 간에 이분 매칭을 수행하는 세트 손실 함수를 end-to-end로 학습한다. DETR는 spatial anchors 또는 non-maximal suppression와 같은 사전 지식을 인코딩하는 multiple hand-designed 한 구성 요소를 삭제하여 detection 파이프라인을 단순화한다. 대부분의 기존 감지 방법과 달리 DETR은 사용자가 만든 레이어가 필요하지 않으므로 표준 CNN 및 transformer 클래스를 포함하는 모든 프레임워크에서 쉽게 재현할 수 있다.  직접적 세트 예측에 대한 대부분의 이전 연구와 비교하면, DETR의 주요 특징은 (non-autoregressive) 병렬 디코딩과 이분 매칭 손실 및 transformers의 결합이다. 대조적으로, 이전 연구들은 RNN을 사용한 자기 회귀 디코딩에 중점을 두었다. 저자의 매칭 손실 함수는 실제 객체의 예측을 고유하게 할당하며, 예측 객체의 순열에 불변하므로 병렬로 내보낼 수 있다  저자는 가장 인기 있는 객체 감지 데이터 세트 중 하나인 COCO에서 매우 경쟁력 있는 Faster R-CNN 기준선과 비교하여 DETR를 평가한다. Faster R-CNN은 지금까지 많이 고쳐졌으며, 처음 나왔을 때보다 성능이 크게 향상되었다. 실험에서 새로운 모델은 고쳐진 Faster R-CNN의 성능과 비슷한 결과를 보여 준다. 더 정확히 말하면, DETR은 큰 물체에서 훨씬 더 나은 성능을 보여주는데, 이는 transformer의 non-local 계산에 의해 가능했을 수 있는 결과이다. 그러나 작은 물체에 대한 더 낮은 성능을 보였다. 저자는 향후 FPN의 발전이 Faster R-CNN에 했던 것과 같은 방식으로 낮은 성능을 개선할 것으로 기대한다.  DETR에 대한 학습 설정은 여러 가지 면에서 표준 객체 감지 학습과 다르다. 새로운 모델은 학습 일정을 길게 두어야 하고 transformer의 보조 디코딩 손실의 이점을 필요로 한다. 저자는 성능 입증을 위해 어떤 구성 요소가 중요한지 철저히 탐구한다.  DETR의 설계 기질은 더 복잡한 작업으로 쉽게 확장된다. 실험에서, 사전 훈련된 DETR 위에 훈련된 간단한 분할 head가 최근 인기를 얻은 도전적인 pixel-level 인식 작업인 Panoptic Segmentation에서 경쟁 가능한 기준점을 능가한다는 것을 보여준다.​​2. Related work​  저자는 세트 예측을 위한 이분 매칭 손실, transformer 기반 인코더-디코더 아키텍처, 병렬 디코딩 및 객체 감지 방법과 같은 몇 가지 영역에서 기존 연구를 기반으로 한다.​2.1. Set Prediction  세트를 직접 예측하는 표준 딥 러닝 모델은 없다. 기본 세트 예측 작업은 다중 레이블 분류이며, 기준 접근 방식인 one-vs-rest는 요소(즉, 거의 동일한 상자) 사이에 기본 구조가 있는 감지와 같은 문제에는 적용되지 않는다. 이러한 작업의 첫 번째 어려움은 거의 중복되는 것을 피하는 것이다. 대부분의 현재 감지 방법은 이 문제를 해결하기 위해 non-maximal 억제와 같은 후 처리를 사용하지만 직접 세트 예측은 후 처리가 필요하지 않다. 그들은 중복을 피하기 위해 모든 예측 요소 간의 상호 작용을 모델링 하는 전역 추론 체계가 필요하다. 일정한 크기 집합 예측의 경우, 조밀한 Fully Connected Networks는 성능이 좋지만 비용이 많이 든다. 일반적인 접근법은 Recurrent Neural Networks과 같은 autoregressive 시퀀스 모델을 사용하는 것이다. 모든 경우에서 손실 함수는 예측의 순열에 의해 불변해야 한다. 일반적인 해결책은 Hungarian 알고리즘을 기반으로 손실을 설계하여 실제와 예측 사이의 이분 매칭을 찾는 것이다. 이렇게 하면 순열 불변성이 적용되고 각 대상 요소가 고유한 일치를 갖는다. 저자는 이분 매칭 손실 접근법을 따른다. 그러나 대부분의 이전 작업과 달리, 저자는 autoregressive 모델에서 벗어나 아래에서 설명하는 병렬 디코딩이 있는 transformers를 사용한다.​2.2 Transformers and Parallel Decoding  Transformers는 기계 번역을 위한 새로운 attention-based 구성 요소로 Vaswani 등에 의해 도입되었다. Attention 메커니즘은 전체 입력 시퀀스의 정보를 통합하는 신경망 계층이다. Transformers는 Non-Local 신경망과 유사하게 시퀀스의 각 요소를 스캔하고 전체 시퀀스의 정보를 통합하여 업데이트하는 self-attention layers을 도입했다. Attention 기반 모델의 주요 장점 중 하나는 global 계산과 완벽한 메모리이기 때문에 긴 시퀀스에서 RNN보다 적합하다. Transformers는 현재 자연어 처리, 음성 처리 및 컴퓨터 비전의 많은 문제에서 RNN을 대체하고 있다.  Transformers는 초기 sequence-to-sequence 모델에 이어 출력 토큰을 하나씩 생성하는 autoregressive 모델에 처음 사용되었다. 그러나, prohibitive inference cost(입력길이와 출력길이가 비례하고 배치하기 어려운 작업)는 오디오, 기계 번역, 단어 표현 학습 및 최근의 음성 인식 영역에서 병렬 시퀀스 생성의 개발로 이어진다. 저자는 또한 계산 비용과 세트 예측에 필요한 global 계산을 수행하는 능력 사이의 적절한 균형을 위해 transformers와 병렬 디코딩을 결합한다.​2.3. Object detection  대부분의 최신 객체 감지 방법은 일부 초기 추측에 상대적인 예측을 한다. Two-stage 감지기는 상자 w.r.t. 제안을 예측하는 반면, single-stage 방법은 가능한 물체 중심의 격자 또는 w.r.t. anchors를 예측한다. 최근 연구(본 글에서는 앵커와 앵커-free의 격차를 연구한 논문을 말한다)는 이러한 시스템의 최종 성능이 초기 추측에 설정된 정확한 방식에 의해 크게 좌우된다는 것을 보여준다. 저자는 모델에서 anchor가 아닌 입력 이미지에서 절대적인 box 예측으로 감지할 데이터 셋을 직접 예측함으로써 hand-crafted 프로세스를 제거하고 감지 프로세스를 간소화하였다.​Set-based loss. 몇몇 객체 감지기는 이분 매칭 손실을 사용했다. 그러나 이러한 초기 딥 러닝 모델에서 서로 다른 예측 간의 관계는 convolutional 또는 fully-connected layers로만 모델링 되었으며 hand-designed된(k-means같은 군집화 전략으로 앵커사이즈를 직접 생성하기 때문) NMS 후 처리는 성능을 향상시킬 수 있다. 더 최근의 감지기는 NMS와 함께 실제와 예측 사이에 non-unique 한 할당 규칙을 사용한다.  학습 가능한 NMS 방법 및 관계 네트워크는 서로 다른 예측 간의 관계를 attention을 통해 명확하게 모델링 한다. 직접 세트 손실을 사용하면 후 처리 단계가 필요하지 않다. 그러나 이러한 방법은 제안된 box 좌표와 같은 hand-crafted된 context 기능을 사용하여 감지 간의 관계를 효율적으로 모델링 하는 동시에 모델에 인코딩된 사전 지식을 줄이는 솔루션을 찾는다.​Recurrent detectors. 우리의 접근 방식에 가장 가까운 것은 객체 감지 및 instance 분할을 위한 end-to-end 한 세트 예측이다. 저자와 유사하게 그들은 CNN 활성화를 기반으로 한 encoder-decoder 아키텍처와 함께 이분 매칭 손실을 사용하여 직접적으로 바운딩 박스 세트 생성한다. 하지만 이 접근 방식은 소규모 데이터 셋에서만 평가되고 현대 기준점에 대해서는 평가되지 않았다. 특히 그들은 autoregressive 모델(더 정확히는 RNN)을 기반으로 했고, 최신 transformers와 병렬 디코딩을 활용하지 않는다.​​3. The DETR model   감지를 통한 직접 세트 예측에서 두 가지 필수적인 성분이 있다. (1) 세트 예측 손실은 예측과 실제 박스 사이 unique matching을 강조한다. (2) 아키텍처는 객체의 세트를 예측하고(단일 경로로) 그 관계를 모델링 한다.  Fig. 2: DETR uses a conventional CNN backbone to learn a 2D representation of an input image.3.1. Object detection set prediction loss  DETR은 디코더를 통과하는 단일 경로를 통해 고정 크기의 세트 N 개를 예측하여 추론하는데, 여기서 N은 이미지의 일반적인 개체 수보다 훨씬 크게 설정된다. 학습의 주된 어려운 점 중 하나는 실제를 반영한 예측된 객체(class, position, size)를 채점하는 것이다. 저자의 손실은 예측된 객체와 실제 객체 사이에 최상의 이분 매칭을 생산하고 개체별(bounding box) 손실을 최적화한다.  y를 실제 객체 집합으로, N 예측 집합을 yˆ = {yˆi}Ni=1라고 정의한다. N을 이미지 안에 객체의 수보다 많다고 가정할 때, y 또한 ∅ (no object)로 패딩 된 N 개의 집합으로 간주한다. 이 두 집합 사이의 이분 매칭을 찾기 위해 가장 낮은 cost와 함께 N 요소 σ ∈ GN의 순열을 찾는다. Lmatch(yi, yˆσ(i))은 실제 yi와 예측 인덱스 σ(i) 사이에 쌍별 매칭 비용이다. 최적의 할당은 이전 연구에서 증명된 Hungarian algorithm를 통해 효율적으로 계산된다.  매칭 비용은 클래스 예측과 예측된 객체 box와 실제 객체 box의 유사성을 모두 고려한다. 실제 집합의 각 요소 i는 yi = (ci, bi)처럼 보일 수 있으며 여기서 ci는 목표 클래스 레이블(∅일 수 있음)이고 bi ∈ [0, 1]4는 실제 객체의 box 중심 좌표와 이미지 크기의 높이와 너비 관계를 정의한 벡터이다. 인덱스 σ(i)를 사용한 예측을 위해 저자는 ci 클래스의 확률을 pˆσ(i)(ci)로, 예측된 box를 bˆσ(i)로 정의한다. 이 표기법들은 Lmatch(yi, yˆσ(i))를 −1{ci=∅}pˆσ(i)(ci) + 1{ci=∅}Lbox(bi,bˆσ(i))로써 정의한다.  매칭 찾는 절차는 현대 감지기에서 제안 또는 anchors를 실제 객체에 매칭 시키는 데 사용된 heuristic 할당 규칙과 동일한 역할을 한다. 주된 차이점은 저자는 직접 세트 예측에서 중복 없이 one-to-one 매칭을 찾아야 한다는 것이다.  두 번째 단계는 손실 함수, 즉 이전 단계에서 매칭된 모든 쌍에 대한 Hungarian 손실을 계산하는 것이다. 저자는 공동 객체 감지기의 손실과 유사하게 손실을 정의한다. 즉, 클래스 예측을 위한 음의 로그-우도와 나중에 정의된 box 손실의 선형 조합: 여기서 σˆ는 첫 번째 단계 (1)에서 계산된 최적 할당이다. 실제로 저자는 클래스 불균형을 설명하기 위해 ci = ∅일 때 로그-확률 항을 인수 10으로 down-weight 한다. 이것은 어떻게 Faster R-CNN 학습 절차가 subsampling을 통해 긍정/부정 제안의 균형을 유지하는지와 유사하다. 객체와 ∅사이에 매칭 비용은 예측에 의존하지 않고, 이 경우 비용은 상수인 것에 주목한다. 매칭 비용에서 저자는 로그-확률 대신에 pˆσˆ(i)(ci) 확률을 사용한다. 이것은 클래스 예측 항이 Lbox(·, ·) (아래 설명)와 상호 보완될 수 있으며, 더 나은 성능을 발견한다.​Bounding box loss. 매칭 비용과 Hungarian 손실의 두 번째 부분은 bounding boxes의 점수인 Lbox(·)이다. 초기 추측을 ∆ w.r.t.로 box 예측을 하는 많은 감지기와 달리, box 예측을 직접적으로 하도록 만들었다. 동시에 같은 접근 방법을 적용하는 손실이 상대적인 스케일링에 문제가 포즈를 취하고 있는 구현을 간소화해 준다. 흔하게 사용된 l1 손실은 상대적으로 에러가 유사할지라도 작은 box와 큰 box에 대해 서로 다른 척도를 갖는다. 이러한 문제를 완화시키기 위해 규모와 변하지 않는 l1 손실과 일반화된 IoU 손실 Liou(·, ·)의 선형 조합을 사용한다. 전반적으로 box 손실은 λiouLiou(bi, bˆσ(i)) + λL1||bi − bˆσ(i) ||1 로써 Lbox(bi, bˆσ(i))로 정의되는데 이때 λiou, λL1 ∈ R는 하이퍼 파라미터이다. 이 두 손실은 batch 안에서 객체의 수에 의해 정규화 된다.​3.2. DETR architecture  Fig 2로 설명되어 있는 전반적인 DETR 아키텍처는 놀라울 정도로 간단하다. 아키텍처는 다음과 같이 3개의 중요 구성이 있다. 빈틈없는 특징 표현 추출을 위한 CNN 백본, encoder-decoder transformer, 그리고 최종 감지 예측을 위한 간단한 feed forward network (FFN)이다.  최신의 많은 감지기들과 달리, DETR은 단  수 백 줄의 공통 CNN 백본이나 transformer 아키텍처 구현과 같은 모든 딥러닝 프레임워크에서 구현될 수 있다. DETR 추론 코드를 PyTorch에서 50줄 내외로 구현할 수 있다. 저자는 이 단순한 방법이 감지 커뮤니티로 새로운 연구자들을 끌어드릴 수 있기를 희망한다.​Backbone. 초기 이미지는 ximg ∈ R3×H0×W0 (3가지 색상 채널 포함)가 주어지며, 전통적인 CNN 백본은 저해상도 활성화 맵 f ∈ RC×H×W 를 생성한다. 보통 사용하는 값은 C = 2048 and H, W = H0/32 , W0/32이다.​Transformer encoder. 먼저, 1x1 컨볼루션은 높은 수준의 활성화 맵 f의 채널 차원을 C에서 더 작은 d 차원으로 축소한다. 새로운 특징 맵 z0 ∈ Rd×H×W을 만든다. encoder는 시퀀스를 입력으로 예상하고, 이후 공간의 차원 z0를 1차원으로 축소시켜 결과적으로 d×HW 특징 맵을 만든다. 각 encoder 레이어은 표준 아키텍처를 갖고 multi-head self-attention와  feed forward network (FFN)로 구성된다. transformer 아키텍처는 순열-불변이기 때문에 각 attention 레이어의 입력을 추가되는 고정 위치 인코딩으로 보완한다.​Transformer decoder. decoder는 multi-headed self- 와 encoder-decoder attention mechanisms을 사용하여 사이즈 d의 N 임베딩은 변환하는 transformer의 표준 아키텍처를 따른다. 저자의 모델과 기존 transformer와의 차이점은 각 디코더 레이어에서 N 개의 객체를 병렬로 디코딩 하는 반면 Vaswani와 같은 방법은 출력 시퀀스를 한 번에 한 요소씩 예측하는 autoregressive model을 사용한다. 저자는 개념이 익숙하지 않은 독자에게 보충 자료를 언급한다. 디코더 또한 순열-불변이기 때문에, N 개의 입력 임베딩이 달라야 다른 결과가 생성된다. 이 입력 임베딩은 object queries라고 언급하는 학습된 위치 인코딩이며, 인코더와 유사하여 각 attention 레이어에 그것들을 추가한다. N 개의 object queries는 decoder에 의해 결과 임베딩으로 전환된다. 그것들은 feed forward network에 의해 독립적으로 box 좌표와 클래스 레이블로 디코딩 되어 N 개의 최종 예측을 한다. 이러한 임베딩에 대해 self-와 encoder-decoder attention을 사용하여 모델은 모든 객체에 대해 함께 그들의 쌍별 관계를 사용하여 전체적으로 추론하며, 동시에 전반적인 이미지의 context를 사용할 수 있다.​Prediction feed-forward networks (FFNs). 최종 예측은 ReLU 활성화 함수와 숨겨진 차원 d, 그리고 선형 투영 레이어인 3-layer 퍼셉트론에 의해 계산된다. FFN은 입력 이미지에서 정규화된 box w.r.t.의 중심 좌표, 높이 그리고 너비로 예측하고, 선형 레이어는 softmax 함수를 사용하여 클래스 레이블을 예측한다. 일반적으로 이미지 안에 관심 있는 객체의 실제 개수보다 훨씬 고정 크기의 N 개의 바운딩 박스 집합을 예측하기 때문에 추가적인 특별 클래스 레이블 ∅로 슬롯 안에서 객체가 감지되지 않음을 표현된다. 이 클래스는 표준 객체 감지 접근에서 “background” 클래스와 유사한 역할을 한다.​Auxiliary decoding losses. 저자는 decoder에서 학습하는 과정에서 auxiliary 손실 사용이 도움이 된다는 것을 찾아냈으며, 특히 모델이 각 클래스의 객체의 정확한 수를 출력하는 데 도움이 된다는 것을 발견했다. 저자는 각 decoder 레이어 후에 예측 FFNs와 Hungarian 손실을 추가한다. 모든 예측 FFNs는 그들의 파라미터를 공유한다. 다른 decoder 레이어에서 예측 FFNs에 대한 입력을 정규화 하기 위해 추가적으로 공유된 layer-norm을 사용한다. ​​4. Experiments​  COCO 데이터 셋을 활용한 평가를 통해 DETR 결과를 Faster R-CNN과 비교하여 경쟁력 있다는 것을 보여준다. 그리고 나서, 통찰력과 질적 결과와 함께 아키텍처와 손실의 세부적인 ablation 연구를 제공한다. 마지막으로, DETR이 다재다능하고 확장 가능한 모델이라는 것을 보여주기 위해, 저자는 고정된 DETR 모델에서 작은 확장만 학습하는 panoptic 분할에 대한 결과를 제시한다.​Dataset. 저자는 118k training 이미지와 5k validation 이미지를 포함하고 있는 COCO 2017 감지와 panoptic 분할 데이터 셋으로 실험을 수행한다. 각 이미지는 바운딩 박스와 panoptic 분할 주석이 있다. 이미지당 평균 7개의 인스턴스가 있고, training 세트에서 단일 이미지에 최대 63개의 인스턴스가 있으며, 동일한 이미지가 작은 것부터 큰 것까지 다양하게 있다. 지정하지 않으면 AP를 여러 임계값에 대한 통합 metric인 bbox AP로 보고합니다. Faster R-CNN와 비교를 위해 마지막 training epoch에서 검증 AP를 보고하고, ablation 위해 마지막 10 epochs는 검증 결과를 중간값 초과만 보고한다.​Technical details. 저자는 DETR을 AdamW 세팅인 초기 transformer의 학습률을 10−4, 백본 학습률 10-5, 그리고 가중치 감소 10-4로 설정한다. 모든 transformer의 가중치는 Xavier init로 초기화 되며, 백본은 고정된 batchnorm 레이어가 있는 torchvision의 ImageNet을 사전 학습한 ResNet 모델로 초기화 한다. ResNet-50과 ResNet-101인 두 개의 다른 백본을 가지고 결과를 보여준다. 일치 모델은 각각 DETR과 DETR-101로 불린다. ""Fully convolutional instance-aware semantic segmentation""에 따르면, 저자는 또한 백본의 마지막 단계에 확장 기능을 추가하고 이 단계의 첫 번째 컨볼루션에서 stride를 제거함으로써 특징의 해상도를 높인다. 일치 모델은 각각 DETR-DC5와 DETR-DC5-R101 (확장 C5 단계)로 불린다. 이러한 수정은 해상도를 2배 증가시켜 encoder의 self-attentions에서 16배 높은 비용으로 작은 객체의 성능을 향상시켜 계산 비용이 전체적으로 2배 증가한다. 이러한 모델의 FLOPs와  Faster R-CNN의 전체 비교는 Table 1에 나와 있다.  저자는 입력 이미지의 사이즈를 가장 짧은 면이 최소 480픽셀이고 최대 800픽셀이며 가장 긴 1333픽셀이 되도록 재조정하여 규모를 증강시킨 뒤 사용한다. encoder의 self-attention을 통한 global 관계를 알아내는 것에 도움이 되기 위해 학습 중에 무작위로 자르고 증강을 적용하여 성능을 대략 1AP 향상시킨다. 특히, 훈련 이미지는 0.5의 확률로 랜덤한 직사각형 패치로 잘라낸 다음 800-1333으로 다시 크기로 조정한다. transformer는 dropout이 0.1로 학습된다. 추론 시간에 일부 슬롯은 빈 클래스를 예측한다. AP를 최적화하기 위해, 해당 신뢰도를 사용하여 이 슬롯들의 두 번째로 높은 점수의 클래스로 예측된 것들은 무시한다. 이 방법은 AP를 빈 슬롯을 필터링하는 것보다 2포인트 향상시켰다. 다른 학습 하이퍼 파라미터는 A.4.절에서 찾을 수 있다. 최적화 실험을 위해 300 epochs를 사용하며, 200 epochs 이후 학습률이 10배 감소하며, 이때 1 epoch마다 모든 훈련 이미지에 한 번 통과한다. 16개의 V100 GPU에서 300 epochs에 대한 기준 모델을 학습하는 데 GPU 당 4개의 이미지(따라서 총 배치 크기는 64개)로 3일이 걸린다.  Faster R-CNN과 비교하기 위해 500 epochs 학습을 했는데 400 epochs 이후 학습 속도 저하가 나타났다. 이 스케줄은 짧은 스케줄에 비해 1.5 AP를 추가한다. Table 1: Comparison with Faster R-CNN with a ResNet-50 and ResNet-101 backbones on the COCO validation set.(이하 생략)​​5. Conclusion  저자는 새로운 설계로 직접 세트 예측을 위한 transformer와 이분 매칭 손실을 기반으로 객체를 감지하는 시스템인 DETR을 제시한다. 이와 같은 접근은 COCO dataset에서 최적화된 Faster R-CNN 기준점과 결과를 비교할 만한 성과를 보였다. DETR은 간단하게 구현하고 쉽게 panoptic 분할로 확장 가능한 유연한 아키텍처라서 경쟁력이 있는 결과를 얻을 수 있다. 게다가 DETR은 self-attention에 의해 수행되는 global 정보 처리 덕분에 Faster R-CNN보다 큰 객체에서 상당히 좋은 성능을 보였다.  이 감지기의 새로운 설계는 작은 객체에 대한 학습, 최적화 그리고 성능과 관련된 새로운 과제들이 있다. 현재 감지기는 유사한 문제에 대처하기 위해 몇 년의 개선이 필요했으며, 향후 연구가 DETR을 통해 그것들을 성공적으로 해결할 것으로 기대한다.​​​2022.10.06 ​​​ "
3D Object Detection Survey - 1. Introduction ,https://blog.naver.com/mikangel/222476165876,20210819,"Abstract​3D 객체 감지는 특히 경로 계획, 모션 예측, 충돌 방지 등을 위해 이러한 인식 시스템의 핵심 기반 역할을 합니다. 일반적으로 해당 3D 포인트 클라우드가 있는 스테레오/단안 이미지는 이미 3D 객체에 대한 표준 레이아웃입니다. 정확한 깊이 정보가 제공되면서 포인트 클라우드가 점점 더 널리 보급되고 있습니다. 기존의 노력에도 불구하고 포인트 클라우드의 3D 객체 감지는 특성상 포인트 클라우드의 높은 희소성과 자연환경의 불규칙성, LIDAR 조감도를 위한 병함방법, 원거리에서 중첩 및 스케일 변동으로 인해 아직 초기 단계입니다. 최근에 이 비전 작업을 해결하기 위해 많은 문헌이 조사되면서 3D 물체 감지가 크게 발전했습니다. 따라서 우리는 센서, 기초 및 최근의 최첨단 탐지 방법을 포함한 모든 주요 주제를 장단점과 함께 이 분야의 최신 진행 상황에 대한 포괄적인 검토를 제시합니다. 또한, 우리는 인기 있는 공개 데이터 세트에 대한 정량적 비교를 제공합니다.​1. Introduction​3D 물체 감지란 무엇입니까? 3D 객체 감지는 3D 센서 데이터에서 물리적 객체를 감지하는 것입니다. 방향이 지정된 3D 경계 상자가 추정되고 특정 카테고리가 할당됩니다. 3D 물체 감지는 3D 장면 인식 및 이해의 핵심 역할을 합니다. 자율 주행, 하우스키핑 로봇, 증강/가상 현실 등과 같은 수천 개의 후속 애플리케이션이 다양한 유형의 3D 센서를 사용할 수 있게 되면서 발전했습니다. 일반적으로 포인트 클라우드, 메쉬, 볼륨메트릭 그리드를 포함하여 세 가지 유형의 3D 표현이 일반적으로 존재하며 이 중 포인트 클라우드가 많은 경우 선호되는 표현입니다. 포인트 클라우드는 많은 수의 면으로 구성된 메쉬만큼 스토리지를 소비하지 않으며 양자화로 인해 볼륨메트릭 그리드와 같은 원래의 기하학적 정보를 잃지 않습니다. 포인트 클라우드는 아래 원시 LiDAR 센서 데이터에 가깝습니다. ​ ​3D 물체 감지는 눈에 띄는 발전을 이루었지만, 아직까지는 2D에는 뒤쳐져 있습니다. 3D 객체 감지는 3D 위치, 방향 및 점유 볼륨과 같은 정확한 기하학적, 모양 및 규모 정보를 사용하여 특정 클래스의 시각적 객체를 감지하여 기계 주변에 대한 더 나은 이해를 제공함과 동시에 어려운 기술적 과제를 제시합니다. 일반적으로 Convolution Neural Networks의 성공 비결은 조밀한 표현에서 공간적으로 지역적인 상관 관계를 활용하는 능력이라고 믿어집니다. 그러나 포인트 클라우드에 CNN 커널을 직접 적용하는 것은 필연적으로 모양 정보의 손실과 포인트 순서의 분산이 발생합니다. ​반면 딥 러닝 방법은 위조에 취약한 것으로 이미 입증되었습니다. 예를 들면 고유한 보안 위험(예: 고의적 방해, 불리한 조건, 사각 지대 등)이 발생합니다. 궁극적으로, 3D 객체 탐지와 관련된 적대적 공격은 대부분 초기 단계에 있습니다. 기존 논문과 비교하여 우리의 기여를 다음과 같이 요약합니다.​1) 보다 세분화된 새로운 분류법을 사용한 조사: 기존과 비교하여 각 방법의 특성을 쉽게 파악할 수 있도록 세분화된 분류를 제공하기 위해 더 파고듭니다. 직관적이고 구체적으로. 예를 들어, 포인트 클라우드 기반 방법은 포괄적인 것을 의미하지만 포인트 클라우드 기반 방법을 다시 표현 학습을 기반으로 다시 점 기반, 복셀 기반, 포인트 기반 및 포인트 복셀 기반으로 그룹화하면 , 포인트 클라우드 기반 방법의 주요 아이디어를 식별할 수 있게 됩니다.​2) 보다 체계적인 새로운 분류법을 사용한 조사: 아래 그림에서 볼 수 있듯이 2018년 이후에 발생한 방법은 방법 자체의 사회적 관심과 효율성에 관계없이 심각한 전환을 겪었습니다. 3D 인식 시스템은 PointRCNN, PV-RCNN과 같은 고성능 감지기가 2018년 이후에 제안될 때까지 실제로 확립되지 않은 지속적인 자체 개선 프로세스를 목격했습니다. 반면 이전까지의  조사 논문은 2018년 이전의 진행 상황만을 다루고 있습니다. 게다가, 우리가 아는 한, 자율 주행에 초점을 맞춘 아직 미개척 분야는 말할 것도 없이 소수의 문헌만이 3D 포인트 클라우드와 관련되어 있습니다. 3) 보다 포괄적인 새로운 분류법을 사용한 조사:  Guo et al. Part-A2, PV-RCNN, Point-GNN을 ""기타 방법""으로 그룹화하여 문제를 미해결 상태로 두었습니다. 다른 연구에서는 분류학에 대해 주의 깊게 논의했지만 다중 모드 융합에 관해서는 각 방법이 어느 범주에 속하는지 명시적으로 밝히지 않고 초기 융합, 후기 융합 및 심층 융합의 기본 개념만 도입했습니다. 반면, 우리는 지속적인 변화에 적응하기 위해 두 가지 새로운 패러다임을 정의했습니다.​4) 대안이 아닌 보완적인 새로운 분류법을 사용한 조사: 기존의 조사와 달리 3D 포인트 클라우드의 모든 관련 하위 주제보다는 자율주행 맥락에서 3D 객체 감지에 특히 중점을 둡니다. 예: 3D 형상 분류, 3D 포인트 클라우드 분할 및 추적 등) 사용 가능한 공간이 제한되어 있으므로 모든 재료가 관련된 세부 사항을 탐구할 수 없습니다. 대신, 우리는 매우 기본적인 개념으로 시작하여, 우리가 정의한 패러다임에 따라 자율 주행 측면에서 3D 물체 감지의 진화를 엿볼 수 있으며, 공개적으로 사용 가능한 데이터 세트에 대한 포괄적인 비교와 함께 장단점을 현명하게 제시합니다.​이 문서의 나머지 부분은 다음과 같이 구성됩니다. 섹션 II에서는 일반적으로 사용되는 3D 센서에 대해 설명합니다. 섹션 III에서는 기본 개념과 표기법을 소개합니다. 섹션 IV에서는 자율 주행의 맥락에서 기존의 최신 3D 물체 감지 방법에 해당하는 장단점을 검토합니다. 일반적으로 사용되는 메트릭, 최첨단의 포괄적인 비교 및 ​​공개적으로 사용 가능한 데이터 세트는 섹션 V에 요약되어 있습니다.​​ "
[개별연구(두피분석)] Effect of Color Pre-Processing on Color-Based Object Detection 읽기 - 2 ,https://blog.naver.com/1rladbdus/222873798339,20220913,"* 전처리로 각기 다른 여러가지 색상을 유사하게 바꾸기 위한 배경지식을 쌓고자 해당 컨퍼런스 논문을 읽게 되었다.논문 출처: https://www.researchgate.net/publication/248390823_Effect_of_Color_Pre-Processing_on_Color-Based_Object_Detection​3. 전처리 알고리즘​스펙트럼 선명화 방법의 목표: 수식 (3)의 적절한 계수(coefficients, C1~C9 의미) 값 추정계수: 색 반응(color response)에서 다른 색 반응으로 바꿀 때 몇 퍼센트 빼야 하는지 의미하는 물리적 양색 반응: RGB삼원색을 물체에 비췄을 때 반응하는 정도인듯..?통계적 제약(statistical constraints)을 사용하여 계수를 도출하는 방법: 다른 부분 빛 강도(intensity patch)로 변할 때 색 반응을 관찰해서 계수 구할 수 O ∴장면 부분 빛 강도(intensity patch)가 scale로 알려져 있을 때, 한 색 반응에서 다른 색 반응으로 바뀔때 몇퍼센트 빼야하는지 계산 O, 장면이 실제 카메라 스펙트럼 특성으로 관찰되어야 한다장면이 실제 카메라 스펙트럼 특성으로 관찰되어야 하므로, 실제 실험 진행:<실험 방법> Fig. 1 . The reference calibration chart with 11 reference patches. 1. 여러 개의 참조 패치(reference patches)가 있는 보정 차트로 장면을 배치, 논문에서는 그림1과 같이 11개의 참조 패치로 진행2. 새 sharpened sensor set 필터(예: 빨간색 필터)를 추가하고 씬(scene)의 평균 이미지를 기록(노이즈 효과를 줄이기 위해 10개 이상의 프레임을 평균 구해 만듦)3. 11개의 참조 표준 패치(reference standard patches)의 평균 응답을 계산4. R-G, R-B, G-B 색상 사이에 곡선 그리기5. 곡선의 기울기를 계산​각 곡선의 색을 연관시키는 방정식을 만들 수 O(내가 임의로 만든 예: aR=bB2)곡선은 선으로 근사, 선의 기울기는 한 색이 다른 색에 기여하는 비율을 나타냄 -> 이 비율을 빼면 (한 색에 대한 반응만 알아도 다른 색에 대한) 독립적인 반응 얻을 수 O​주의! 색상을 선명하게 하려면 위의 과정을 1번만 수행한 뒤, 계수(coefficients)를 변경하지 않고 사용하기 Fig. 2 . The reference calibration chart with 11 reference patches. Table 1. The computed coefficients of the spectral sharpening matrix as per Eq.(3).  예시: 왼쪽 위의 그림은 R-G 샘플 곡선, 기울기(0.2337)는 수식(3)의 C4를 결정 / 오른쪽 위 그림은 결정된 계수 (C4 자리에 -0.2337이 있다. 기울기 * (-1)을 해서 구하는듯?)​ Fig. 3. The Algorithm for color preprocessing and color-based detection. 색상 선명화 알고리즘 순서도: 이미지 캡쳐 -> 선형화 & 선명 -> HIS색공간으로 변환 -> HS히스토그램에서 미리 정의된 영역과 한계에 따라 색상 식별 -> 영상 분할(segment) (=대상 영역으로 클러스터링)​​4. 실험(구현)구현 언어: C, 사용 라이브러리: OpenCV​카메라 오프셋 제거를 위한 컬러 이미지 포착 & 보정 알고리즘 구현변수: Table 1에서 계산한 선명화 행렬 계수(coefficients of sharpening matrix) 사용특징: 픽셀 단위 연산을 기반 -> 빠름 Fig.4. a. Original Image of CMOS camera, b. The pre-processed color image.  단일 이미지 & 비디오 실시간 처리하는 프로그램 구현방법: Hue-Saturation 영역의 측정을 통해 시스템에 식별->대상의 색상 속성은 나중에 비교할 수 있도록 저장변수: 색상 범위(hue range) 0°~360°(0°=360°임에 유의), 채도 범위(saturation range) 0~255특징: 몬드리안을 H-S 히스토그램의 색상 외관을 색 감지 능력의 척도로 사용 (몬드리안: 색상 항상성 알고리즘 테스트하기 위한 고전 스타일 실험)예: 위의 몬드리안 이미지 -> 색이 선명해짐 확인​​​ Fig. 5.  Fig. 6.의 Red, Green, Blue, Yellow 패치에 대한 H-S diagram 왼쪽은 선명화 전, 오른쪽은 선명화 후(아마 가로축이 밝기, 세로축이 Fig. 6. 참조된 패치, 원본(좌), 선명화(우) 패치 [이미지 출처] https://m.blog.naver.com/PostView.naver?isHttpsRedirect=true&blogId=mankeys&logNo=220780289498H-S 히스토그램: 가로축은 색상(Hue), 세로축은 채도(Saturation)인 히스토그램H-S 히스토그램 해석: 그래프의 어두운 영역은 동일한 H-S 점을 갖는 대상 영역의 높은 픽셀 수에 해당선명화 결과 분석: - H-S 히스토그램의 색상 영역은 모든 색상 패치에 대해 더 높은 포화도로 이동- 각 색상의 H-S 히스토그램은 파란색을 제외한 모든 색 패치의 히스토그램에서 투영 면적이 감소 ∵모든 색에 대해 선명한 색 더 뚜렷하게 + 경계구역 차원↓(boundary zone is of less dimensions) (경계구역 차원 적다는게 뭔말?)   - 파란색이 예외인 이유: 파랑조명 ∴색 항상성 알고리즘과 결합 -> 예외 해결될 것기대 효과: 색 임계값 세밀하게 정의 O, 다른 색과 잘 분리, 색 감지 능력↑​​​​ Table 2 Correlation between color responses compared before and after correction. Bench mark values are stated for high quality digital still camera for 7 diverse color patches.효과에 대한 정량적 설명: 이미지 내부의 컬러 밴드 간 상관계수를 측정으로 설명스펙트럼 선명화(sharpening) 전과 후에 이 상관관계를 측정동일한 장면 색상과 동일한 조명 조건에서 캡처하는 고품질 디지털 스틸 카메라의 상관 관계 값을 벤치마킹(결과: 표 2)선명화 장점: 이미지 컬러 밴드 간의 상관계수↓선명화 특징: 모든 색상 대역에서 감소폭이 동일X(G-B 영역에서는 차이 O&효과↓, R-G 영역에서 효과↑), 벤치마크에 비해 R-G 및 R-B 영역 색상 상관관계↓, G-B 영역 색상 상관관계↑​ Fig.7. The Hue Saturation diagram showing regions of darker intensity as those corresponding to higher voting of the target object pixels ( corresponding to red color in this case). The range of hue is from 0 to 360 degree, and for saturation from 0 till 255.Fig.7. 설명: H-S 히스토그램(상), 해당 컬러 이미지(하), 원본 이미지(좌), 선명화 후 이미지(우)​이미지 내 물체 검출의 신뢰성 및 색상 외관에 대한 선명화 영향: H-S 히스토그램에서 원본 색상(서로 가까움, 영역↓), 선명화 후 색상(채도↑ 클러스터된 색상 영역↑)(원래 뭉쳐있어서 해석이 힘들었던 부분을 선명화로 채도를 높이고 퍼뜨려서 해석에 용이하게 함)결과: 선명한 컬러 이미지로부터 색상을 쉽게 감지할 수 있음​5. 결론​카메라 센서 스펙트럼 감도 함수가 이미지 색상의 품질에 미치는 영향을 연구중복 센서: 색 포화도가 낮은 표면 색에 대한 해석을 제공∴ 중복 센서가 없어도 있는것처럼 소프트웨어 처리(스펙트럼 중복되지 않는 필터 세트에 의해 이미징된 것처럼 더 많은 포화 색상을 생성할 수 있는 실험 접근 방식을 기반으로 한 새로운 스펙트럼 선명화 알고리즘)그 결과, 선명한 색상은 히스토그램에서 면적↓ 색 포화도↑ & 선명화는 이미지 컬러 밴드 간의 상관관계↓∴ 이미지에서 컬러 기반 객체 감지 프로세스를 개선하는 H-S 히스토그램에서 클러스터된 영역↑​​9/13 소감: 생각보다 빨리 다 읽어서 놀랐다. 색공간을 변환함을 통해서 H-S를 퍼뜨려서 객체 탐지에 더 좋게 한다는 내용이 신기했다. 의문: 경계 범위가 줄었는데 클러스터된 영역은 늘었다는 것이 이해가 잘 안간다.. 둘이 비례관계여야하는거 아닌가??​​[Fig, Table 이미지 출처] Effect of Color Pre-Processing on Color-Based Object Detection, https://www.researchgate.net/publication/248390823_Effect_of_Color_Pre-Processing_on_Color-Based_Object_Detection​​ "
논문 요약:DETReg: Unsupervised Pretraining with Region Priors for Object Detection ,https://blog.naver.com/qwopqwop200/222455778728,20210808,이 논문은 Object Detection을 비지도 학습으로 DETR을 pretraining하는 방법을 구현합니다. 위는 미세조정 없이 테스트한 결과 입니다. 놀랍게도 미세조정없이 즉 아무 레벨 없는 데이터에서도 물체를 잘 잡고 있습니다.이것이 가능한 이유는 Selective Search 때문입니다. Selective Search는 쉽게 말해 모델없이 알고리즘 만으로 object를 찾아주는 알고리즘입니다.하지만 selective search는 노이즈가 많고 물체를 딥러닝 보다는 잘 찾지 못합니다. 아무튼 이런 Selective Search를 통하여 top k방식으로 생성된 k개의  bounding box를 생성하고 그 bouning box를 학습하도록 합니다이렇게 b를 학습시킵니다.또한 c는  Selective Search로 찾은 bounding box에는 1 그렇지 않은면 pad로 0이런식으로 학습시킵니다.또한 z의 경우는 swav를 사용하여 이미지의 자기 지도 학습을 통하여 이미지의 특징을 embbing하도록 학습합니다.이렇게 사전 학습된 DETReg은 기존 방법에 비해 적은 데이터을 사용하여 높은 성능을 이끌어 냈습니다. 오느른 여기 까지입니다. 혹시 논문에 관심이 있으시면 한번 읽어 보시기 바랍니다.https://arxiv.org/pdf/2106.04550v1.pdf 
2021-09-28 DL( object detection ) ,https://blog.naver.com/mggod145/222520421566,20210929,사진 사물검출 (object detection)​​​​​ ​​YOLO ​YOLO 진행 순서​1. 사람 또는 자전거와 같이 사진이 어떤 사진인지 학습된 신경망이 준비되있어야 함.2. 이미지가 한장이 들어오면 이미지를 여러개의 격자로 나눠서(수천장) 그 여러개의 격자중에 사물이 있을 확률이 높은 격자만 선별함.3. 사물이 있는 격자를 찾았으면 그 격자를 신경망에 넣고 사람인지 자전거인지 분류한다.4. 원본 이미지에 바운딩해서 사람인지 자전거인지 나타낸다.​​주로 사용하는 YOLO 버전​yolo4는 정확하지만 yolo5에 비해 속도가 느리다.​​yolo 실습하기​​1. 코랩의 GPU 사양을 확인합니다. !nvidia-smi -L 2. 구글 드라이브를 마운트 시킵니다. from google.colab import drivedrive.mount('/content/gdrive')  3. 텐써 플로우 1.x 버전을 쓰겠다고 지정합니다.  %tensorflow_version 1.x 4. 사진속 사물검출을 하기 위한 코드와 이미지들을 다운로드 받습니다. !git clone https://github.com/heartkilla/yolo-v3.git 5. requirements.txt 에 있는 내용데로 패키지를 설치합니다. %cd /content/yolo-v3!pip install -r /content/yolo-v3/requirements.txt 6. 사물 디텍션하기 위해 미리 학습된 가중치를 다운로드 받습니다. %cd /content/yolo-v3!wget -P /content/yolo-v3/weights https://pjreddie.com/media/files/yolov3.weights 7. 텐써플로우 버전이 1.x 버젼인지 확인합니다.  import tensorflow as tf​tf.__version__ 8. 내려받은 가중치를 신경망에 로드합니다. !python /content/yolo-v3/load_weights.py  9. 사진속에 이미지를 찾아서 바운딩 합니다.  %cd /content/yolo-v3!python detect.py images 0.5 0.5 data/images/dog.jpg data/images/office.jpg 출처 : https://github.com/heartkilla/yolo-v3#notification-settings  ​​​​​​  ​​​​​동영상 사물 검출 (object detection)​​​​​​​​​​​​1. 구글에서 할당해준 GPU 를 확인합니다. !nvidia-smi -L 2. 구글 드라이브와 구글 코렙을 연동합니다.  from google.colab import drivedrive.mount('/content/gdrive') 3. 현재 디렉토리를 확인합니다. %pwd 4. /content/gdrive/MyDrive 로 이동합니다.  %cd /content/gdrive/MyDrive 5. 사물검출에 필요한 코드와 동영상을 다운로드 받습니다.  !git clone https://github.com/heartkilla/yolo-v3.git 6. 텐써 플로우 1.x 버전으로 내립니다. %tensorflow_version 1.x import  tensorflow  as  tfprint ( tf.__version__) 7. 구현에 필요한 패키지들을 설치합니다. %cd /content/gdrive/MyDrive/yolo-v3!pip install -r /content/gdrive/MyDrive/yolo-v3/requirements.txt 8. 가중치를 다운로드 받습니다.  %pwd !wget -P /content/gdrive/MyDrive/yolo-v3/weights https://pjreddie.com/media/files/yolov3.weights 9. 가중치를 신경망에 셋팅합니다. !python load_weights.py 10. 필요한 코덱을 다운로드 받습니다.  !apt install ffmpeg libopencv-dev libgtk-3-dev python-numpy python3-numpy libdc1394-22 libdc1394-22-dev libjpeg-dev libtiff5-dev libavcodec-dev libavformat-dev libswscale-dev libxine2-dev libgstreamer1.0-dev libgstreamer-plugins-base1.0-dev libv4l-dev libtbb-dev qtbase5-dev libfaac-dev libmp3lame-dev libopencore-amrnb-dev libopencore-amrwb-dev libtheora-dev libvorbis-dev libxvidcore-dev x264 v4l-utils unzip 11. 아래의 디렉토리로 이동합니다. %cd /content/gdrive/MyDrive/yolo-v3%pwd detect.py 는 미리학습된 가중치가 셋팅된 인공신경망이 영상에서의 사물을 박스로 디텍션하는 코드인데 이 코드안에 동영상을 플레이하는것과 연관된 코드를 3개를 주석처리합니다. 그리고 동영상 형식을 mp4v  로 변경해줍니다.  이렇게 하고 돌리면 결과물이 현재 디렉토리에 detections 밑에  생성되게 됩니다.  ​12. 동영상을 사물검출합니다. !python detect.py video 0.5 0.5 /content/gdrive/MyDrive/yolo-v3/data/video/shinjuku.mp4  13. 동영상을 다운로드 받습니다.​ from google.colab import filesfiles.download('./detections/detections.mp4') ​​ 
[Object detection] 1. 알고리즘 발전의 역사 ,https://blog.naver.com/ollehw/221824217794,20200225,"안녕하세요.아래의 사진에 Object detection 알고리즘의 발전 과정이 한 눈에 보기 쉽게 정리되어있습니다.   획기적인 알고리즘 개선으로 많은 관심을 얻고 있는 논문들은 빨갛게 처리되어 있습니다.​Object detection이 가장 처음 제안된 R-CNN은 현재로써는 성능이 좋지 않은 편이지만, 시초가 되는 논문으로 굉장히 고평가되고 있습니다.​저는 R-CNN -> Fast R-CNN -> Faster R-CNN -> Yolo -> SSD 순으로 정리해보도록 하겠습니다.​앞의 3개인 R-CNN 계열은 Two-stage Detector로 Object detection의 핵심적인 목적인 Classification과 Bounding box Regressor를 서로 다른 모델로 구현하는 모델의 구조를 가집니다.​반면에 뒤의 2개인 Yolo와 SSD는 One-stage Detector로 두 목적을 하나의 단일 모델에서 학습을 진행합니다.​이러한 차이에 따라, One-stage Detector는 Two-stage Detector에 비해 속도가 상당히 빠르지만, 정확도는 살짝 뒤쳐집니다.​정확도고 물론 중요하지만, 최근에는 속도가 더 중요시 되고 있기 때문에, One-stage Detector가 주목을 더 많이 받고 있습니다.​ "
딥러닝 기반 object detection 과정 정리 (1) - 기초설명 ,https://blog.naver.com/taeeon_study/222786427536,20220624,"이번에는 객체인식 발전 과정에 대해서 한 번 리뷰한다. 자세한 알고리즘 설명은 하지 않고 상당히 유명한 알고리즘 순서대로 한 번 써보도록 한다. (새벽에 심심할 때 해야지..)​컴퓨터 비전의 주 목적은 이미지를 이해하는 것이다. 이미지 데이터로부터 특징 및 정보를 추출하게 된다. 이에 따른 어플리케이션 분야는 상당히 많고 객체 분류, 식별, 탐지 및 위치 추정, 객체 및 인스턴스 분할, 자세 추정, 동영상 분석, 장면 복원 등 상당하게 많은 분야가 있다. 이번에는 객체 탐지 (object detection)에 관련한 알고리즘들이 어떻게 발전되어 왔는지 그 시대에 나왔던 큰 알고리즘들에 대한 간략 설명으로 알아보도록 한다.​- 객체 인식이란?객체 인식은 이미지 (비디오) 등의 영상 데이터 안에서 물체가 탐지되면 이를 라벨링하고 인식하는 것을 의미한다. (classification + localization) 개념이라고 생각하면 쉽다. ​- 전통적인 접근법이미지 데이터는 고전적인 영상처리 기법으로 detection에 접근할 수 있다. 입력 이미지를 전처리하고, 관심 영역을 추출한 뒤에 탐지된 개체를 분류할 수도 있다. 또, 정확하지는 않지만 픽셀 값으로 히스토그램을 분석하여 탐지를 수행하는 방법도 있으며 edge detection 후에 객체의 모양 등으로 판단하는 방법 등 상당히 다양하고 HOG detector[1]는 전통적인 객체인식 접근법이다.​- 딥러닝 기반 접근법하드웨어의 발전으로 인해 러닝 계열이 상당히 핫한 분야가 되었고 급격하게 발전하는 CV 분야도 최신 알고리즘이 거의 대부분이 러닝으로 접근될만큼 상당히 많이 사용하고 있다. 딥러닝 기반은 훈련을 통해 시각적 인지를 모델링하기 때문에 대응 능력이 좋다는 것 등의 장점이 있다. 우선 딥러닝을 이해하기 전에 머신러닝을 이해할 필요가 있는데 말 그대로 기계가 스스로 학습한다는 의미이다. 학습한다는 의미는 어떤 성능 척도가 있을 때 이것이 계속해서 나아진다는 의미이다. 여기서 성능 척도는 대부분 cost function, loss function 등의 단어로 많이 쓰이게 된다. 이제 머신러닝 기법 중에 하나가 딥러닝이 되는 것이다. 많은 층의 네트워크를 쌓아서 단순한 선형 분리기가 아닌 비선형 변환을 통해 분류 및 회귀를 가능하게 한다. 이번 포스트에서는 딥러닝 기반 object detection에서 기본적으로 알아야 하는 것을 설명한다. ​- SLP첫 번째는 SLP(Single Layer Perceptron)이다. learning 알고리즘 대부분은 분류 뒤에 목적함수에 따라 다시 학습을 한다고 생각하면 쉽다. 그렇기에 분류 단계와 학습 단계를 나누어서 설명해보도록 한다. ​분류?​분류 단계에서는 단순히 파라미터인 W와 input 노드의 dot product로 수행하게 된다. 퍼셉트론은 단순히 선형 분류기이며 특징 공간을 둘로 나누는 경계인 것이다. 단층 퍼셉트론은 선형 연산밖에 안되기 때문에 층을 많이 쌓아서 대형 분류기를 만든 것이 다층 퍼셉트론이라고 생각하면 된다. 또한 한 은닉노드 이후에 비선형 함수를 한 번 거칠 필요가 있다. 왜냐하면 선형 연산 후에 바로 선형 연산을 해버리면 그 역시 전체적인 하나의 선형 연산이 되어 버리기 때문에 의미가 없어져버리기 때문이다. 여기서 거치는 비선형 함수를 주로 활성화 함수라고 많이 부르게 된다. 활성화 함수에는 많은 종류가 있지만 대표적으로 많이 쓰이는 것이 softmax 함수, sigmoid 함수, relu 함수 등이 있다. tanh 함수도 종종 쓰인다.  perceptron분류 단계는 단순히 설정된 가중치와 노드 값과의 dot 연산만 수행하는 것이라고 하였다. 그렇다면 학습은 어떻게 이루어지는 것인지 알아보도록 한다.​학습?​머신러닝에서의 학습 단계도 최적화 기법 중 일부라고 생각하면 된다. 목적 함수를 정의하고 이것을 각 가중치에 대해 미분하고 이 값을 학습률과 곱해서 계속해서 최적 값으로 나아가도록 하는 것이 바로 학습 단계에서의 역할이다. 흔히 gradient descent 방식이라고 하며 error를 계속해서 앞으로 전파시키기에 오류 역전파 알고리즘이라고도 한다. 왜 최적화 함수인지 이해하고 넘어가도록 한다. 대부분의 최적화 함수는 내가 원하는 cost function 또는 objective function을 정의하고 이 함수 값이 최대 또는 최소가 되도록 하는 것이 목적이다. 즉, 매우 쉽게 생각하면 예전부터 배웠던 함수의 극값을 찾는 문제와 비슷하다. 최적화 알고리즘에는 매우 다양한 방식이 있지만 현재 기계학습의 원리는 단순히 gradient를 활용한 방식으로 구현한다. 언급했던 극값을 찾는 문제와 매우 비슷한 원리이다. 흔히 지도 학습에서 보편적이고 많이 쓰이는 목적함수는 MSE이다. 단순히 각 데이터의 예측 값과 실제 레이블 값과의 차이를 계산하고 이를 모두 더하여 평균내는 것이다. 그렇지 않다면 확률적으로 cross entropy를 사용한다던지 등의 방법도 존재한다. 최적화 문제에서 목적함수 설정은 상당히 까다롭고 많은 application 분야에서 아직까지 연구가 많이 진행되고 있다. Loss function, cost function, objective function 모두 거의 같은 용어라고 보면 되기에 용어 사용에 차질이 없도록 한다. Cost function이 정의됐다면 이 함수를 각 노드 사이에 물려있는 가중치에 대해서 gradient를 구하고 점진적으로 학습시키면 된다. 가중치 학습은 아래 식과 같이 정의된다.  learning여기서 E는 cost function, rho는 학습률이다.​​- MLP​SLP를 배웠다면 MLP(Multi Layer Perceptron)로 확장하면 된다. 이제부터 본격적으로 네트워크를 깊게 쌓기 시작한다. 단순히 SLP는 하나의 층이기 때문에 선형 분류기 밖에 되지 않는다. 다만 이를 여러 개의 층으로 쌓는다면 선형이 쌓이고 쌓이면 결국은 분류가 가능하게 될 것이다. 바로 이 개념이다. 이렇게 깊게 쌓게 되면 강한 분류기 또는 예측기가 가능하게 되는 것이다. 마찬가지로 분류 단계, 학습 단계가 있는 것은 동일하다. 다층 퍼셉트론은 back propagation에서 에러와 노드 값을 곱한 값이 가중치에 대한 미분이 되게 되는데 한 노드의 에러는 그 노드와 연결 돼 있는 모든 노드의 에러의 합이 되게 한다. 그렇기 때문에 오류가 계속해서 역전된다고 하여 오류 역전파라고 하는 것이다. back propagation그림 출처: https://m.blog.naver.com/PostView.nhn?isHttpsRedirect=true&blogId=laonple&logNo=220507299181 [Part Ⅱ. Neural Networks] 4. Backpropagation [1] - 라온피플 머신러닝 아카데미 -Part I. Machine Learning Part V. Best CNN Architecture Part VII. Semantic Segmentat...m.blog.naver.com - CNN? 어차피 러닝이므로 학습의 개념은 비슷하다. 다만 기존 MLP를 영상에서 쓰면 픽셀 하나 하나를 입력으로 받아서 그에 따른 가중치가 하나 하나씩 다 존재하였는데 CNN에서는 Convolution 연산을 이용하여 그 conv filter 안에는 같은 가중치를 공유하여 연산량을 완전히 줄이고 성능도 높였다. 아직까지 상당히 많이 쓰인다. 주요 단어에 대해 알아보도록 한다.​convolution한글로 합성곱이라는 의미이다. convolution 연산은 전통 영상처리에서도 상당히 많이 쓰는 연산이며 신호 처리에서도 이에 대한 개념이 나온다. 하나의 함수와 또 다른 함수를 반전 이동한 값을 곱한 다음, 구간에 대해 적분하여 새로운 함수를 구하는 연산자이다. 이 결과로 특징 맵을 만들 수 있다. convolution이미지 출처: : http://deeplearning.stanford.edu/wiki/index.php/Feature_extraction_using_convolution​2. Filter (Kernel) and strideFilter는 위의 Convolution 설명 이미지에서 나왔 듯이 Input 이미지와 합성곱 연산을 하게 되는 것이다. Fully connected layer에서 각 노드 사이의 가중치와 비슷한 개념이다. 다만, 필터가 지정된 간격으로 이동하면서 전체 입력데이터와 합성곱을 하여 특징 맵을 만드는 것이다. Stride는 간단히 설명하면 필터가 몇 칸씩 뛸 것 인가이다. 예를 들어서 stride가 2로 설정된다면 필터가 2칸씩 이동하면서 특징 맵을 추출하게 된다. 아래 그림은 이에 대한 설명이다.​ Filter and stride이미지 출처: https://www.ibm.com/cloud/learn/convolutional-neural-networks What are Convolutional Neural Networks?IBM Cloud Learn Hub What are Convolutional Neural Networks? Convolutional Neural Networks By: IBM Cloud Education 20 October 2020 Convolutional Neural Networks Learn how convolutional neural networks use three-dimensional data to for image classification and object recognition tasks. What are convo...www.ibm.com 3. paddingFilter와 Stride 작용으로 특징 맵 크기가 입력 데이터보다 작을 수 있다. 그렇기에 주위에 패딩을 채워 넣어 이를 방지한다. 대개 zero padding을 많이 사용하거나 구석에 있는 픽셀을 그대로 채운다.​4. poolingConvolution 연산이 아니라 필터 크기에 따라 그 지역의 평균 값이나 최대 값을 추출하게 된다. (대표 값 추출)아래는 이에 대한 설명 그림이다. pooling이미지 출처: https://en.wikipedia.org/wiki/Convolutional_neural_network Convolutional neural network - WikipediaConvolutional neural network From Wikipedia, the free encyclopedia For other uses, see CNN (disambiguation) . This article needs additional citations for verification . Please help improve this article by adding citations to reliable sources . Unsourced material may be challenged and removed. Find s...en.wikipedia.org 그렇다면 왜 CNN을 썼는지 알아보자. CNN 사용 전 Fully-connected layer가 사용되었을 때, 매개변수가 상당하게 많고 공간 추론의 부족성 등의 문제를 가지고 있었다. CNN은 기존 FCN에 비해 아래와 같은 차별성을 가진다.​1.     입출력 데이터의 기존 형상 유지​2.     공간 정보를 유지하면서 인접 이미지와의 특징을 효과적으로 인식​3.     많은 필터로 이미지 특징 추출 및 학습 가능​4.     Pooling layer​5.     파라미터 수 매우 줄일 수 있는 메리트​다만 요새는 CNN도 단점이 많이 발견되었다. 단점은 고정된 필터 사이즈를 가지기 때문에 관심 영역 밖의 관계를 파악할 수 없는 것, 가중치가 고정되기 때문에 input이 바뀐다면 인식하기 어려워진다는 단점 등이 있다. 또한 앞서 transformer 기반 edge detection 논문 리뷰에서 설명했던 long-range-dependency 등에서도 transformer는 상당한 강점이 있기에 요새 트렌드가 많이 넘어온 추세이다. 우선 전역적인 특징을 파악하는 것도 중요하다는 것이 증명된 것이다. 그렇기에 transformer에 대해서도 간단하게 여기서 설명하도록 한다.​- transformer본래 NLP (자연어처리)에서 많이 쓴 구조[2]이다. 이를 이해하기 위해서는 어텐션이라는 메커니즘을 이해할 필요가 있다. 어텐션은 유사도 측정에 목적이 있다. 이 유사도 측정 방식을 dot product로 수행하는 것이다. 본래는 RNN의 단기 기적의 제한성을 막기 위해 사용된 구조인데 여기까지 올라가면 설명이 너무 길어지므로 어떤 것의 유사도 측정이 dot product 기반이 많다는 것만 우선 이해한다. 트랜스포머는 합성곱 층이나 fcn 층 같은 것을 전혀 사용하지 않고 어텐션 메커니즘만 사용해서 상당히 좋은 성능을 얻은 구조이다. 아래는 트랜스포머 구조의 그림이다. transformer architecture트랜스포머 모델이 자연어 처리에서 제시 되었기에 자연어처리에서 쓴 방식대로 설명한다.왼쪽 부분은 인코더인데 단어의 시퀀스로 표현된 문장 배치를 입력으로 받게 된다. 또한 오른쪽 부분은 디코더이다. 디코더는 훈련 동안 타깃 문장을 입력으로 받게 된다. 또한 그림에서 보이듯이 디코더는 인코더의 출력도 받게 된다. inference (추론) 때에는 디코더에 타깃을 넣지 않는다. 타임 스텝마다 디코더는 가능한 다음 단어에 대한 확률을 출력하게 된다. 자세한 input size, output size 등은 생략하고 넘어간다.가장 중요한 것은 Multi-Head Attention 층이다. 이것은 관련이 많은 단어에 더 많은 주의를 기울이면서 각 단어와 동일한 문장에 있는 다른 단어의 관계를 인코딩하게 된다. 멀티 헤드 어텐션은 scaled dot product attention layer이다. 여기서, (쿼리, 키, 값) 세 개의 단어가 나오게 되는데 이를 쉽게 설명하기 위해 한 문장을 예로 들어보자. 'I am a boy'라는 문장이 있다.  인코더가 {주어: I, 동사: am ...}이라는 딕셔너리를 만들면 디코더가 '동사'라는 키에 해당하는 값을 찾는 것이다. 쿼리는 룩업에 사용하게 된다.(이 단어가 어떤 것과 가장 비슷한지 물어보는 것이라고 생각하면 된다.) 쿼리와 딕셔너리에 있는 각 키 사이의 유사도를 계산하여 가장 비슷하다면 키의 가중치가 1에 가까워질 것이다. 물론 scaled dot product에서는 쿼리와 키의 dot product 후에 각 값을 곱하게 된다. 트랜스포머에서는 이 유사도 측정을 dot product로 수행하였다.결국 scaled dot product 연산은 아래와 같다. scaled dot product또한 논문에서 나온 그림 자료이다. sclaed dot product and multi head attention picture이제 위 그림에서 보듯이 멀티 헤드 어텐션은 scaled dot product의 묶음이다. 각 층은 값, 키, 쿼리의 선형 변환이 먼저 이루어지게 된다. 하나의 scaled dot product만 넣는다면 많은 특징을 한 번에 수행할 수 밖에 없지만 묶음으로 쓴다면 여러 subspace로 투영 가능합니다. 이 subspace는 결국 단어의 일부 특징만 바라보게 되는 것입니다. ​Positional Encoding( 위치 인코딩)은 문장에 있는 단어의 위치를 나타내는 단순한 밀집 벡터이다. 단어 위치를 알려주는 것은 중요하기에 이 과정은 꼭 필요하다.​그렇다면 트랜스포머를 비전 분야에서는 어떻게 사용해야 할까?최근에 많은 분야에서 SOTA를 받았던 ViT 아키텍처를 간단히 살펴보면 이미지를 patch 형태로 나누고 입력 임베딩을 실시한다. 그러고 약간의 처리 후에 encoder로 들어가게 된다. 자세한 설명은 딥러닝 기반 object detection 후반부에 ViT  논문을 간단히 살펴보면서 이해하도록 한다.​트랜스포머는 설명하기 너무 방대해서 대충 한 것 같은데 [2] 논문을 살펴보면 상당히 많은 도움이 될 것이다.​[1] DALAL, Navneet; TRIGGS, Bill. Histograms of oriented gradients for human detection. In: 2005 IEEE computer society conference on computer vision and pattern recognition (CVPR'05). Ieee, 2005. p. 886-893.[2] VASWANI, Ashish, et al. Attention is all you need. Advances in neural information processing systems, 2017, 30.​​ "
YOLOv5로 object detection 해보기 (windows 기준) ,https://blog.naver.com/cyberxirex/222581493131,20211129,"#object_detection #YOLO #YOLOv5 #python #pytorch #학습 #machine_learning #파이썬 #인공지능 #파이토치 #물체_감지 #물체인식 Object detection (물체 인식, 물체 감지)사람은 사진이나 동영상에서 특정한 물체를 인지할 수 있다. 아래와 같은 사진에서 사람 얼굴, 넥타이, 손동작등을 쉽게 인지할 수 있는데, 이를 컴퓨터가 하게 하려면 어찌해야할까?  출처 : YOLOv5 colab page오늘은 YOLO라는 잘 알려진 open source code를 이용해서, 학습을 시켜 물체를 구분하는 방법을 알아보겠다.​GitHub - ultralytics/yolov5: YOLOv5 🚀 in PyTorch > ONNX > CoreML > TFLite 일단 YOLOv5 github page에 가서 이리저리 좀 둘러보자. 기본적으로 python 과 pytorch로 이루어져있고 (tensorflow로 porting된 버젼도 있음), 체험가능한 dataset도 준비되어있다. ​자 이제 우리는 이를 쉽게 ""Google Colab.""을 이용해서 실행해보고, 두번째는 local PC에 설치해서 돌려보도록 하겠다. (Google Colab. 의 경우 설정이 매우 쉽지만, 1) 실행시간 제약이 있고, 2) 연결이 끊기면 업로드해둔 데이터등이 없어지는 단점이 존재한다.)​ YOLOv5 in Google Colab. 위의 Github page에 접속해서 스크롤을 내리다보면, 아래와 붉은 줄 친 것과 같이 open in Colab link가 있다.  클릭해보자!! (다른 가상환경으로도 실행해도 되지만, 대부분 구글 계정은 있으므로 Google Colab. 으로 시작해보자)​ 이런 페이지가 열릴텐데, Python을 Jupyter notebook interface에서 사용해본 경험이 있으면, 누구나 쉽게 사용할 수 있을 것이다. (위의 회색 네모칸을 클릭해서 커서가 나오면, 실행은 shift+enter로 하면 된다)​자 이제 맨 위에 Setup부분의 회색 부분을 클릭하고 shift+enter를 눌러보자 아마도 이런 경고가 뜰텐데 살포시 무시하고 계속하기를 눌러주자. 위의 그림 처럼 뭔가가 뱅글뱅글 돌아가는데, 아래 setup completed라고 나오면 기본 환경 설정이 성공한 것이다. ​​  [코드 설명] !git clone https://github.com/ultralytics/yolov5  # clone !를 붙인 것은 shell에서 명령어를 수행하겠다는 얘기이고, git이란 명령어는 위의 Github에 있는 YOLOv5를 그대로 복사해오겠다는 명령어이다. (이렇게 명령어를 수행하면 현재 path에 YOLOv5란 폴더가 생성되게 된다.) Colab page에서 왼쪽에 폴더 모양으로 생긴 아이콘을 누르면, 현재 사용할 수 있는 파일들이 나타나는데, 보이는 것처럼 yolov5라는 폴더가 생긴것을 확인할 수 있다. ​ %cd yolov5%pip install -qr requirements.txt  # install %도 마찬가지로 shell에서 명령어를 수행한다는 의미인데, 현재 폴더에서 실행을 의미한다. yolov5 폴더로 들어가서 pip 명령어로 requirements.txt에 들어있는 package들을 모두 설치하라는 의미인다.  Requirement.txt는 아래와 같은 명령어로 되어있다. 마지막으로 from yolov5 import utilsdisplay = utils.notebook_init()  # checks 설치된 yolov5로 부터 util에 있는 code를 import하여, Jupyter notebook에서의 출력 환경을 initiate해주는 것이다.  자!!! 벌써 간단헤 모든 설치과정이 마무리 되었다!!! 이제 실제로 무엇인가를 돌려보자.​ Training 시키기물체를 detection하기 위해서는 training이라는 과정을 거쳐야하는데, 이는 컴퓨터가 이해할 수 있도록 공부자료를 주는 것이다. 우리가 준비해야하는 것은 labeling이 된 여러장의 사진이다. (labeling 은 사진의 특정 부분이 무엇이라고 기록하는 것이라고 생각하면 된다.) 예를 들어 위의 그림에서 사람은 파란색 박스로, 자전거는 붉은색 박스로 위치를 지정해주고, 이것이 무엇인지를 컴퓨터가 알아들을 수 있도록 기록해주는 것이다. ​YOLOv5에는 이미 training을 시킬수 있도록 dataset이 준비되어있다. (조금후에 나만의 이미지를 이용하여 training dataset을 만드는 법을 알아보겠다.)​Colab. 무지성으로 아래 쪽으로 이동하여 training을 시켜보자. (아래 그림 붉은 부분) 위 명령어를 수행하면, 알아서 dataset을 다운받고, training을 시키게 된다. (일단은 --epoch는 몇번 교육을 시킬 것인지이고 많이 반복 교육 시키면 정확도가 올라간다 정도만 알아두자) 정상적으로 끝났다면 run/train폴더 아래에 exp로 시작하는 폴더가 생성되었음을 확인할 수 있다. (수행때마다 exp1, exp2, ... 이렇게 숫자가 늘어나게 된다.)​자 대충 어떠한 결과가 나오는지를 확인해보자, 만들어진 파일에서 val_batch2_label.jpg 와 val_batch2_pred.jpg를 살펴보자. (여기서 _label은 정답이라고 생각하면되고, _pred는 학습시킨 컴퓨터가 시험을 본 것이라 생각하면 된다.)  왼쪽이 정답 / 오른쪽이 컴퓨터가 3회 학습후 내놓은 결과이다.오른쪽 그림의 경우 뒤에 숫자들이 보이는데, 이는 몇% 정도의 확신성을 갖는지를 말해준다.예를 들어 pizza 0.6은 60% 정도의 확실성으로 pizza라는 결과를 얻었다는 것이다. ​다음 블로그에서는 custum dataset으로 학습과 윈도우 환경 local PC에서 YOLO를 수행해보는 것을 알아보겠다.​    Written by FlatEarth "
End-to-End Object Detection with Transformers(DETR) ,https://blog.naver.com/wsz87/222569408683,20211116,"다음의 논문은 Object Detection을 하나의 direct set prediction problem으로 보고 detection pipline을 간소화하고 DETR 모델 이전의 모델에서 많이 등장했던 사전지식을 통한 복잡한 작업들을 없앴다는 점에서 의의가 많은 논문이다.(실제로 논문이 나온 지 얼마 되지 않아 매우 많은 관심을 받았다.)​DETR model에서 핵심적으로 제시하는 component는 크게 두가지로 볼 수 있다.​1. 이분 매칭을 통한 전역적 집합 손실함수를 이용함으로써 모델이 서로 다른 물체를 detection할 수 있도록 한다.​2. Transformer encoder-decoder를 이용해 전체 이미지의 문맥을 final output을 출력하는데에 바로 이용할 수 있게 하고 연산이 병렬적으로 이루어질수 있도록 했다. ​​IntroductionObject Detection의 목적은 바운딩 박스와 카데고리 레이블들을 원소로 하는 집합을 예측하는 것으로 정의할 수 있다.현대의 detector들은 이러한 set prediction task를 간접적인 방식으로 해결하려 했다.그리고 그러한 간접적인 방식은 여러개의 바운딩 박스가 겹치는 문제를 해결해야 하고 이 문제를 위해 추가적인 후처리 단계가 필요했다.그래서 DETR논문 이전의 모델들은 후처리 단계의 기법의 성능에 따라 영향을 굉장히 많이 받았다.하지만 이렇게 사전지식이 많이 들어가고 섬세하게 설계해야하는 구조들을 간소화 하고 trasformer encoder-decoder를 이용함으로써 이미지에서 멀리 떨어진 픽셀들의 연관성을 잘 학습할 수 있게 했다.또한 transformer를 이용함으로써 바운딩 박스가 많이 겹치는 문제를 줄일 수 있었는데 이에 대해서도 뒤에서 자세히 살펴보자.​The DETR modeldirect set predictions in detection을 위해 필수적인 두가지 요소를 알아볼 필요가 있다.(1) a set prediction loss that forces unique matching between predicted and ground truth boxes.(2) an architecture that predicts a set of objects and model their relation.( by transformer encoder-decoder )​ [1] DETR모델의 개괄적인 사진.​object detection set prediction loss모델은 미리 정해진 N(object queries의 개수)개의 predictions를 추론하는데 이때 N은 사진에 나타날 수 있는 물체의 평균적인 수를 잘 커버할 수 있는 수로 설정한다.이렇게 추론된 N개의 predictions을 ground truth와 어떻게 연결지어 loss함수를 구성해야하는지가 문제이다.어떻게 연결짓는가에 대한 문제를 해결하기 위한 식을 살펴보자. [1] -1.  where y_i is the ground truth set of objects, y_hat is the set of N predictions.위의 식을 정리하면, 예측 집합과 label 집합간의 연결들 중 matching loss가 가장 작아지도록 하는 최적화 과정이다. 다음 최적화 과정은 Hungarian algorithm을 이용하는데 이 알고리즘은 다음의 사이트에서 자세히 설명하고 있으니 참고하도록 하자.( https://python.plainenglish.io/hungarian-algorithm-introduction-python-implementation-93e7c0890e15, https://youtu.be/cQ5MsiGaDY8)​​그럼 이제 matching cost 에 대해서 알아보자.이 loss는 물체의 class에 대한 예측과 bounding box의 위치 정보를 정답과 비교하여 계산한다.이때, y_i = (c_i, b_i) where c_i is the target class label(which may be nothing) and b_i는 위치정보를 중심 좌표, 박스의 높이, 너비로 이루어진 4-dimensions vector ( also normalized between zero and one)이다.다음의 기호들을 통해 다음과 같이 matching cost를 정의할 수 있다. [1]-2. 이때, 확률값에 log를 취하지 않고 그대로 사용한 이유는 bounding box loss와 비슷한 크기로 scaling 을 진행한 결과로 이해할 수 있고 실험적으로 더욱 더 좋은 성능을 유도함을 논문에서 밝히고 있다.이 loss를 최소화함으로써 확률 값이 1에 가까운 prediction이 class가 있는 ground truth set 과 연결되도록 matching 이 학습된다.이제 model의 prediction 성능 향상을 위한 loss function ( = Hungarian Loss)를 정의하고 최적화를 진행한다. [1]-3.  a linear combination of a negative log-likelihood for class prediction and a box loss defined later:이때 sigma_hat은 [1] - 1사진에서 최적화된 matching path이다.또한 object가 없다고 나오는 prediction의 log-likelihood term에는 down-weighting 을 해줌으로써 class imbalance의 문제를 줄여줄 수 있다고 한다.(내 생각에는 아무래도 물체가 있다고 판단되는 예측보다는 없다고 판단되는 예측이 더 많을 것이다. 특히 N이 매우 큰 경우가 이에 해당한다. 따라서  위의 hungarian loss에서 log-likelihood term들이 같은 weight를 가지고 더해진다면 모델은 물체가 없다는 prediction으로 편향되어 물체가 있음에도 불구하고 없다고 잘못 판단할 수 있다.)​위에서 보면 matching cost, hungarian loss 둘다 bounding box loss가 들어 있는데 이에 대해서 자세히 살펴보자.이는 바운딩 박스에 대한 평가에 해당하는 부분이다. [1][1].bounding box loss는 각각 lambda들은 scalar로 두 loss의 비중을 조절하는 역활로 사용되고, L1 loss와 generalized IoU loss의 선형결합으로  이루어진다.만약에 L1-loss만 사용할 경우 비슷한 에러 상황에 대해서도 박스의 크기에 따라 다른 loss값을 산출할 수 있기때문에 크기에 상관없이 일정하게 구해지는 IoU loss(논문 A.2 참조)를 추가한 것으로 이해할 수 있다.​​DETR architecture​Backbone.3 x H_o x W_o 차원의 이미지가 conventional CNN backbone 을 거쳐 낮은 해상도의 높은 C x H x W로 출력된다.(C = 2048, H, W = H_o / 32, W_o / 32)이 과정을 통해 compact feature representation을 추출한다.​​Transformer encoder.CNN을 거쳐 출력된 feature map의 channels 수가 2048로 매우 큼을 알 수 있다.따라서 channels 수를 줄여주기 위해 1 x 1 conv_layer를 거쳐 channels수를 d로 줄여준다.또한 feature map에서의 channel dimension은 유지한 상태로 flatten을 진행해 공간 정보를 없앤다.(d x HW)각각의 transformer encoder layer는 multi-head self-attention와 feed forward network로 이루어져 있다.​앞에서 우리는 input들을 flatten 해줌으로써 공간 정보를 없앴다.따라서 모델에게 공간 정보를 추가해주기 위해 원 transformer 논문에서 소개된 positional encoding을 input에 더해준다.(혹시 transformer에 대한 상세한 설명을 원할 경우, 내 블로그의 attention is all you need 글을 참고.)​​Transformer decoder.이 또한 encoder에서와 마찬가지로 standard transformer구조를 대부분 따른다.(조금의 변형만 알면 된다.)제일 주목해야하는 변형은 decode를 진행할 때, N개의 predictions를 병렬적으로 진행한다는 것이다.machine translation을 위한 원 transformer논문에서는 decoder 파트에서 각 time step마다 한 단어를 생성하는 auto-regressive 방식을 채택했다. (이 점이 제일 큰 차이점이다.)또한 encoder 파트에서와 마찬가지로 N개의 object queries은 permutation-invariant이므로 각 object query마다 다른 결과를 내보내 learnt positional embeddings의 역활을 하도록 설정한다.그리고 encoder에서와 마찬가지로 이 input embeddings를 각각의 layer의 input으로 넣어준다.최종적으로 N개의 object queries는 transformer's decoder를 거쳐 N개의 서로 다른 output embeddings이 된다.그 후에 각각 독립적으로 FFN(바로 다음 section에서 설명)을 거쳐 서로 다른 물체에 대한 detection을 진행할 수 있도록 한다.""self- and encoder-decoder attention을 거치면서 물체들의 상호 연관성을 추론할 수 있다."" ​​Prediction feed-forward networks ( FFNs).최종 예측은 3-layer의 MLP로 이루어진다.(with relu activation function and hidden dimension d)현 단계에서는 해당 object에 대한 class(using softmax), bounding box의 정보를 예측한다.​​Auxiliary decoding losses.논문에서는 모든 decoder layer에 FFNs를 적용해 예측을 진행하고 Hungarian loss를 구해준다.(이때, FFNs은 모든 decoder layer에서 같은 parameters를 공유한다.) + layer normalization또한 진행해준다.이 보조의 loss를 추가해줌으로써 성능을 향상시킬 수 있다.​​Detailed architecture [1].: Architecture of DETR’s transformer. Please, see Section A.3 for details.​​​Experiments​Ablations.​-Number of encoder layers-논문에서는 global level self-attention의 중요성을 알아보기 위해 encoder layer의 개수를 조절해본다.encoder layer의 개수가 줄어들수록 성능이 더욱 더 안좋아지는 것을 알 수 있다.​ [1]. Encoder self-attention for a set of reference points. The encoder is able to separate individual instances. Predictions are made with baseline DETR model on a validation set image.위의 그림은 마지막 encoder layer에서 self-attention 의 attention map을 시각화한 것이다.각각 참조 point들이 attention weight가 높은 pixel위치가 노랑게 표시되어 있다.위의 attention map을 봤을 때, 각각의 참조 점들이 포함되어 있는 물체를 다른 물체와 잘 구분하며 attention하고 있는 것을 알 수 있다.이를 통해 논문 저자들은 다음과 같은 가설을 주장했다.""We hypothesize that, by using global scene reasoning, the encoder is important for disentangling objects.""​​-Number of decoder layers.위에서 보조적 loss를 이용해주기 위해서 각각의 decoder layer의 output을 FFN의 input으로 넣어 예측을 진행한다.이때, 각각 서로 다른 깊이의 layer에서 진행된 object detection 성능을 확인해봄으로써 decoder의 깊이의 중요성을 확인해볼 수 있다.​앞서 살펴본 바와 같이 이분매칭을 통한 loss를 계산하고 최적화함으로써 NMS와 같은 장치가 필요없다. 즉, 같은 물체에 대해서 중복의 예측을 피하기 위한 장치인 NMS는 잘 학습된 imbedding을 통한 예측을 더 좋게하기 힘들 것이다.(다시 말하면 NMS를 적용하더라도 성능의 향상이 매우 적어야 한다.) [1]. : AP and AP50 performance after each decoder layer. A single long schedule baseline model is evaluated. DETR does not need NMS by design, which is validated by this figure. NMS lowers AP in the final layers, removing TP predictions, but improves AP in the first decoder layers, removing double predictions, as there is no communication in the first layer, and slightly improves AP50.위의 그래프를 해석해보면, low layer of decoder에서는 output-elements들 간의 상호 연관성을 잘 학습하지 못해 중복된 예측을 많이 하게 되고, NMS를 통해 AP점수가 증가하는 것을 알 수 있다.하지만 decoder layer가 깊어질수록 성능의 향상정도가 작아지고, 결국 final decoder layer에서는 오히려 NMS를 적용하는 것이 성능을 하락시킨다는 결과를 얻게 된다.이를 통해 알 수 있는 것은 decoder layer가 거듭되면서 model 스스로 중복 예측을 피하는 방향으로 image를 파악해나간다는 것이다. [1] . Visualizing decoder attention for every predicted object (images from COCO val set). Predictions are made with DETR-DC5 model. Attention scores are coded with different colors for different objects. Decoder typically attends to object extremities, such as legs and heads. Best viewed in color. 위의 사진은 decoder part에서 encoder attention의 weight를 공간정보를 복원해 시각화한 것이다.각각의 predicted object들에 대한 attention map을 같은 색으로 시각화했을 때, 물체의 경계에 weights가 높게 설정되어 있는 것을 그림을 통해 확인할 수 있다.​이를 종합해보면, encoder에서는 global attention을 통해서 각각의 instances를 구분하고decoder에서는 class를 추출하고 bounding box를 예측하기 위해 물체를 구분 짓는 경계를 학습한다고 이해할 수 있다.​​또한 논문에서는 더욱 더 다양한 ablation 실험을 진행했는데,이를 통해 transformer layer안의 FFN, spatial positional encoding, output positional encoding(object qureies), L1-loss, GIoU loss모두 성능을 향상시키는 장치임을 알 수 있다.(자세한 사항은 논문 참조)​​​Analysis​-Decoder output slot analysis [1]. Visualization of all box predictions on all images from COCO 2017 val set for 20 out of total N = 100 prediction slots in DETR decoder. Each box prediction is represented as a point with the coordinates of its center in the 1-by-1 square normalized by each image size. The points are color-coded so that green color corresponds to small boxes, red to large horizontal boxes and blue to large vertical boxes. We observe that each slot learns to specialize on certain areas and box sizes with several operating modes. We note that almost all slots have a mode of predicting large image-wide boxes that are common in COCO dataset. 각각의 서로 다른 query slot은 다른 구역의 다른 박스 크기로 detection에 집중하는 것을 알 수 있다.​​​-Generalization to unseen numbers of instances.학습데이터로 쓰인 COCO set에서 희귀한 class가 한 사진안에 많이 등장하는 경우와 같이 train의 상황에서는 볼 수 없는 data에 대해서도 성능이 좋은 지 확인해보기 위해 다음 그림과 같이 기린 사진을 연달아 이어붙여 성능을 확인해본 결과이다. [1]. Out of distribution generalization for rare classes. Even though no image in the training set has more than 13 giraffes, DETR has no difficulty generalizing to 24 and more instances of the same class.그림 설명에서 말한 것처럼 처음 본 data에 대해서도 24개의 개체를 높은 확신을 가지고 detection할 수 있음을 보이고 있다.​​이렇게 좋아보이는 DETR 모델도 역시 단점이 존재한다.training, optimization, performances on small objects과 관련해서는 아직 개선의 여지가 많아 보인다.이를 개선하려는 논문을 추후에 공부할 예정이다.(가능하다면 블로그에 올리겠습니다...)​코드 참고 깃허브 사이트: https://github.com/facebookresearch/detr GitHub - facebookresearch/detr: End-to-End Object Detection with TransformersEnd-to-End Object Detection with Transformers. Contribute to facebookresearch/detr development by creating an account on GitHub.github.com 부족한 점이 많은 글 읽어주셔서 감사하고 혹시 오개념이 있을 경우 댓글로 언제든 지적 부탁드립니다.좋은 하루 보내세요~ ^^ ​<reference>[1]https://arxiv.org/pdf/2005.12872.pdf "
Object Detection - traditional approach ,https://blog.naver.com/hasukmin12/222152681460,20201124,"자율주행을 할 수 있게하는 핵심 기술​ ​실제 상용화 수준까지 성능이 올라왔다.교수님이 보시기엔 성능이 좋은 RCNN 보단속도가 빠른 YOLO가 더 관심의 대상으로 보인다.​​ ​Classification보단 Location에 더 집중해서 본다.Location이 우선적으로 잘 된다면해당 윈도우의 feature map이 Dog Model의 feature map과 유사한지를 판별하여 Classification을 진행​​ ​위와 같은 요인으로 인해 classification에 문제가 생긴다.​Object detection- detection에서는 object의 종류와 위치가 중요하다.- detection을 위해서는 illumination, object pose clutter, occlusions, intra-class appearance, viewpoint등 고려해야 할 문제들이 있다.- detection을 위해서는 bounding box를 찾아야하는데 bad localization, confused with similar object & dissimilar objects, misc background 등의 문제들이 있다.​​ ​Detection에 있어서 가장 중요한 문제는 Location이기 때문에이 bounding box(window)를 정확하게 찾아주는게 제일 큰 목표다.​특히 위의 사진의 Bad Location 속 강아지 처럼전체를 bounding한게 아닌 일부만 bounding한 이런 애매한 상황이 많이 일어나고오른쪽 confused with similar object같은 고양이는 생김새가 강아지랑 비슷하기에 모호한 부분이 있다. 이러한 부분이 남은 문제점들​​​​Object detection 순서 Traditional한 방식이든 Deep를 통한 방식이든 모두 이 flow로 작동한다.​1. Specify object model : 찾고자하는 object를 modeling한다.object detection을 하기 위해서 세상에 있는 모든 object를 다 찾을 수는 없다. 우리가 찾고자하는 특수한 object를 잘 modeling 해줘야한다.​  1) Statistical template in bounding box : (옛날 방식) image에서 feature를 뽑아서, feature의 분포나 통계치를 이용해서 object modeling을 구현 경계 상자와 좌표가 정의되야 한다. (x,y,w,h)좌측의 image를 통해 오른쪽 feature를 뽑아내면, 앞에서 배운 bag of Wordf 방식으로feature 모양들의 cluster를 만들고 해당 cluster의 분포를 기반으로 classification을 진행한다(예를들어, 좌대각선이 30%, 우대각선이 50%, 가로직선이 15%, 세로직선이 5%로 구성된 feature map을 가진 object는 자전거라고 classify한다.)  ​  2) Articulated parts model : 앞의 자전거 같은 경우는 모양이 바뀌지 않지만, 우리가 찾고자 하는 object가 사람이거나 동물이면(팔 다리가 자유자재로 움직이는 object라면 = non-rigid body) 난이도가 굉장히 높아진다. 이렇게 pose가 바뀌어도 사람이라고 검출을 해 낼 수 있게 만들기 위해 사람 전용 object modeling을 만들었다. body 파트들의 연결성(팔꿈치는 항상 팔과 연결되어있음에 착안)을 사전에 반영하여 modeling ​​  3) Hybrid template/parts model : 위의 두 가지 방법을 함께 사용하는 방법part connection 정보와 edge 정보를 동시에 이용 ​​​2. generate hypotheses : 이미지 안에서 hypotheses를 생성이미지가 엄청나게 많은 pixel과 patch들로 구성되어 있는데, 어떻게 bounding box를 구성해서 object를 찾을것인가?? 그 hypotheses를 만들어서 제공하는 단계가 이곳이다. 이미지에서 여러가지 candidate 템플릿을 만들어내는 과정이라고 이해하면 된다.=> 여기서 생성한 candiate 템플릿(window)에서 뽑아낸 feature를 가지고 있던 modeling된 템플릿의feature map과 비교해서 detection을 구현 ​​- Sliding window : 모든 패치들을 검사해서 modeling된 템플릿의 feature map과 비교하여 object가 있는지 없는지 판단한다. - 이미지의 크기를 바꿔서 다양한 scale에 패치를 뽑으면 좀 더 robust한 모델을 만들 수 있다. (Multi-Scale Approach, 이미지의 크기가 작은 경우 많이 사용)- 패치의 사이즈를 바꿔서 candidate를 만드는 방법도 있다. (이미지의 크기가 클 경우 많이 사용)- 이미지의 크기를 줄이고 패치를 뜯어내면 같은 패치안에 더 많은 정보가 들어온다. 왜 이렇게 하느냐? 아까 맨 위의 confused with similar object같은 고양이처럼 이미지에 대부분의 영역을 object가 차지하는 경우 detection하기 어렵기에 image의 스케일을 줄여서 object가 패치안에 들어오게 만들어준다.- 반대로 object가 너무 작은 경우엔 image의 scale을 키우고 패치를 뜯어내면 object가 패치안에 들어오게 된다.- 물론 패치 사이즈를 변화하는 approach도 있다. 이 방법이 앞의 방법보다 계산량이 적어 더 효율적이다.​  modeling된 템플릿위의 사진에서 검사한 패치를 아래 사진 속 modeling된 템플릿과 비교하여 detection을 진행한다.​​​​3. Score Hypotheses : 뜯어낸 패치(sliding window)와 우리가 가진 템플릿이랑 얼마나 유사한가 similarity를 측정하는 과정. 이때 similarity를 측정하려면 같은 space에서 해야하는데, 일반적으로 feature space에서 훈련된 모델(템플릿)과 비교.만약 비교해서 similarity가 높다면 object가 이 patch안에 있다고 판단similarity가 낮다면 object가 이 patch안에 없다고 판단하여 다음 patch를 뜯어본다.이때 threshold 방식을 사용하면 classification도 구현 가능(simple)좀 더 복잡한 classifier를 고려한다면 NN를 사용하여 같은 object의 feature인지 classification 가능 (deep)​하지만 최종적으로 decision을 내려주기전에 부딪히는 문제가 있는데 모든 패치들을 뜯어내서 candidate를 만들어내면 위와 같이 빨간색 patch와 초록색 patch가 생길텐데(여기서 초록색이 가장 정확한 bounding box, 빨간색이 그거보다 살짝 옆에 있는애라고 가정)문제는 이 둘 다 score가 좋게 나온다(여기서 score가 작을수록 좋은거다)왜냐하면 이 빨간부분도 object의 많은 부분을 포함하고 있기 때문이다.즉, 해당 bounding box가 높은 similarity를 가지고 있다면 그 주위도 높은 similarity를 가질 수 밖에 없다.​우리는 이 많은 유능한 candidate중에서도 Top 1을 골라내야 한다.이는 Non-max suppression 으로 해결 가능하다. 이는 candidate중에서 local maximum한 Top 1 하나만을 골라준다. 그런데 굳이 이렇게 꼭 Top1을 골라내야하는 이유는=> 만약 우리가 이런 다른 candidate들을 제거하지 않게 되면 수많은 object가 존재한다고 판단해버릴 수 있다.위의 그림에서 왼쪽은 사람 얼굴이 6개가 있다고 판단해버린다.그렇기에 traditional한 방식이든 deep을 사용한 방식이든 NMS은 반드시 필요하다!!​​4. Resolve Detections : detection 완료​​​- 오늘의 핵심 -Design challenge1. 어떻게 하면 modeling을 잘 할건가?2. 어떻게 하면 좋은 candidate를 만들어낼 건가?3. candidate가 있으면 그걸 model과 어떻게 비교하고 similarity를 mearsure 할 것인가?4. scale의 변화가 있을때 어떻게 robust하게 할 수 있을것인가?​​[참고] [CV] Object detection|작성자 Jun "
[Python] TensorFlow Object Detection (윈도우) ,https://blog.naver.com/yug311861/222380905884,20210602,"TensorFlow Object Dection API텐소플로우 (윈도우)이번에는 Object Dection API를 황요해보도록 하겠습니다.제가 공부를 하면서 발생했던 오류부분을 수정하여 포스팅을 하고자합니다.  1. TensorFlow Model 다운로드[Python] TensorFlow 이미지라이브러리 slim 교육 (윈도우버전)파이썬 TensorFlow 이미지 학습 예제 연습하기 딥러닝에 대해서 관심이 있어서 Tensoflow의 py예제에 대...blog.naver.com 해당 포스팅에 작성한 그대로를 이어간다는 점 체크해주시면 감사하겠습니다.역시 기본 샘플은 깃헙자료를 사용하였습니다.https://github.com/tensorflow/models 2. 텐서플로우 오브젝트 디텍션 API를 위한  사전 준비conda install protobuf /* 이것은 실제로 파일을 이용함 설치 필요가 없음 */conda install absl-pyconda install lxmlconda install matplotlibconda  install Cythonconda install pillowconda  install pandasconda install jupyterconda  install opencv-python protobuf의 경우에는 깃헙에서 자료를 받아서 zip을 해제하고 환경변수를 셋팅하여 활용해주시는게 좋습니다.제가 다른 여러 정보들을 보면서 해보았는데 이슈가 조금 있는 듯 합니다.​3.4 버전 (활용)https://github.com/protocolbuffers/protobuf/releases/tag/v3.4.0참고로 저는 3.4버전을 설치하여 proto 파일을 컴파일했습니다.​3.17 버전https://github.com/protocolbuffers/protobuf/releases​3.14 버전conda install protobuf 명령으로 설치하면 텐서플로우 버전이 저와 동일한 1.15 또는 1.14 이면 3.14가 설치되는 것 같습니다. 3. TensorFlow ObjectDetection API 사용3-1. 프로토콜 컴파일 진행하기​받으신 protobuf zip 파일을 자신이 원하는 폴더에 알집을 해제해주세요.그 후 확장변수 Path에 해당 protobuf.exe가 존재하는 폴더를 추가해주세요.이제 본격적으로 프로토콜 컴파일을 진행하도록 하겠습니다. object_detection/protos 내부에 .proto 확장자들을 .py파일로 컴파일링 하기 위한 작업이라고 생각하면됩니다.중요한 것은 anaconda prompt를 활용하지 않고 cmd를 사용하시면됩니다. cmdreseach 폴더 위치에서 실행protoc object_detection/protos/*.proto --python_out=. ※반드시 research폴더에서 진행을 하셔야합니다 종속관계로 인해서 오류가 발생할 수 있습니다.​3-2. 안하여도 무관한거로 판단되는 setup.py 실행​이것은 사람들 강의마다 정말 다다릅니다.어떤사람은 해당 행위를 진행하고 어떤사람은 그냥 넘어가고 입니다.저는 해당 행위를 했으나 오류가있어서 그냥 무시했습니다 (즉, 의미가 없나봐요..) 대부분 강의에서는 해당 setup.py를 빌드하고 실행하라는 정보가 많습니다.그런데 실제로 그 파일의 위치가 도대체 어디인지를 알 수가 없습니다.제 생각에는 packages 에 있는  tf1 또는 tf2 폴더에있는 setup.py를 말하는 것으로 보이며 해당 파일을 실제로는 파이썬 위치에서 해제해야할 것으로 보입니다. 어떤 강의에서는 그렇게 한 것 같아요 저는 그냥 오류나서 무시하고 옮겨서 시도는 안했습니다. python setup.py buildpython setup.py install 3-3. 환경변수 설정​환경변수의 이름 PYTHONPATH 변수값 : research 폴더와 slim 폴더를 알려주세요. ​3-4. 이미 교육된 모델 활용하여 API 동작 테스트하기​https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf1_detection_zoo.md이곳에 가면 COCO-trained models 라고 쫙 있습니다.이중에 솔직히 자기가 원하는 모델을 고르시면 될 것으로 보입니다.하지만 대부분 강의에서 rcnn_inception_v2_coco를 활용하여 저 또한 동일한 교육모델을 활용하였습니다.해당 집 파일으 받아서 obejct_detection 하위에 바로 그냥 풀어주세요 https://github.com/EdjeElectronics/TensorFlow-Object-Detection-API-Tutorial-Train-Multiple-Objects-Windows-10동일하게 교육된 정보를 가지고 있는 파일을 다운로드 받아서 동일하게 풀어주세요(덮어쓰기)​3-5. jupyter를 활용한 테스트​이제 마지막으로 실제로 object_detection을 통해서 사물 판단을 하는지 체크해보겠습니다!!아나콘다에서 object_detection_tutorial.ipynd를 실행해주세요! jupyter notebook object_detection_tutorial.ipynb ※잠깐 또  파일이 어디있는지 모르는 상황 아마 깃헙자료가 계속 바뀌면서 기존에는 object_detation안에 있었는데 바뀐거로 보입니다.​경로 : object_detection\colab_tutorials 해당 폴더에 접근해보시면 우리가 찾고자하는 object_detation_tutorial.ipynd 보이죠 이것을 상위폴더로 옮기고 실행해주세요.​웹 기반 컴파일러인거같아요 주피터가 실행이됩니다. 상단의 RUN을 눌러서 단계별로 실행을 해주세요. pycocotools 오류가 발생할 수 있습니다 간단하게 무시해주세요.해당 오류는 그렇게 큰 이슈가 발생하지 않습니다.​★ 중요한 수정 부분 ★ 여러 강의를 보면 주피터를 실행하면 그냥 다 된다고만 있습니다.저 또한 그런 이야기일 수 있지만 중간중간에 이런 부분에서 이슈가 발생합니다.​PATH_TO_LABELS를 수정했습니다 실제 아나콘다가 지칭한 위치기준으로 잡아서 data폴더의 mscoco_label_map.pbtxt 지칭​pathlib.Path( 이부분은 절대경로를 활용했습니다 파알못이라 상대경로로 주려고해도 glob오류가 계속나와서 그냥 절대 경로로 이미지 폴더 test_image를 지칭​ 그리고 우리가 받은 coco 모델을 선택해주세요 (아주 재미진건 ... 실제로 아까 전에 coco 받았잖아요.. 코드상 다시 받네요 ㅎㅎ)​그리고 Run을 통해서 쭉쭉쭉 실행하시면 이렇게 결과를 확인 할 수 있습니다.실제로 obejctDection API를 활용한 것이고 교육을 시키는 것은 자료들을 좀더 찾아봐야할 것 같아요.  #텐서플로우 #텐소플로우 #Tensorflow #Object #Detection #ObjectDetection "
[Paper] Rich feature hierarchies for accurate object detection and semantic segmentation (R-CNN) ,https://blog.naver.com/nm1lee/222503520040,20210912,"​ 'Rich feature hierarchies for accurate object detection and semantic segmentation (R-CNN)'https://arxiv.org/abs/1311.2524 ​ 0. Intro- Obejct Detection : 이미지가 무엇인지 판단하는 Classification과 이미지 내의 물체의 위치 정보를 찾는 Localization을 수행하는 것을 말한다. 영상 내의 객체가 사람인지 동물인지 물건인지 등을 구별하여 각 객체가 어디에 위치하는지 표시하는 것이 가능하다.​- R-CNN은 object detection 모델에서 2-stage detector의 대표적인 R-CNN계열의 시초가 되는 모델이며, R-CNN계열로는 R-CNN, fast R-CNN, faster R-CNN 등이 있다.​​ 1. Abstract- 지난 몇 년 동안 PASCAL VOC 데이터셋에서 Object Detection의 가장 좋은 성능을 내는 것은 high-level context의 복잡한 앙상블 모델이었다.​- 이 논문에서는 VOC 2012 데이터를 기준으로 이전 모델에 비해 mean average precision(mAP)가 30%이상 향상된 더 간단하고 확장 가능한 detection 알고리즘을 소개하였다.​- detection 알고리즘의 2가지 인사이트  1. 객체 localize 및 segment하기 위해 bottom-up(상방향)방식의 region proposal(지역 제안)에 Convolutional Neural Network(CNN)를 적용2. labeled data가 부족할 때, 성능 향상을 위해서 domain-specific fine-tuning과보조작업(auxiliary task)으로 supervised pre-training 적용 ​ 2. Introduction- 이 논문에서 R-CNN의 의미 : Regions with CNN features (CNN + Region Proposals 의 결합) ​​[ R-CNN의 프로세스 ] 1. 이미지를 입력한다. (Input Image)2. 약 2000개(2k)의 독립적인 bottom-up region proposals를 추출한다.3. CNN을 이용하여 각각 region proposal마다 고정된 길이의 feature vector를 추출한다.  - 위 사진 중간의 'warped region'은 각각의 regioin proposal의 크기가 각각 다르므로 이를 CNN의   입력으로 넣어주기 위해서 크기를 맞춰주는 역할을 한다.4. 각 region 마다 category-specific linear SVM을 적용하여 label을 classification(분류)한다.  ​ 3. Object detection with R-CNNR-CNN은 위의 프로세스를 수행하기 위해서 3가지 모듈로 나뉜다. 1. Region Proposals : detector가 이용가능한 영역 후보의 집합2. CNN : 각각의 region에 대한 고정된 크기의 feature vector를 추출3. Linear SVM : classification 진행  1. Region Proposals​- 독립적인 region proposal을 생성하기 위한 방법은 여러가지가 있다. - 해당 논문에서는 이전 detection 작업들과 비교하기 위하여 Selective Search라는 최적의 region proposal 기법을 사용하였다. ​[ selective search 프로세스 ] - bottom-up : 왼쪽 사진을 보면 하단부터 상향으로 유사한 영역끼리 결합하는 것을 확인할 수 있음 -1. 이미지의 초기 세그먼트를 정하여, 수많은 region 영역을 생성한다.2. greedy 알고리즘을 이용하여 각 region을 기준으로 주변의 유사한 영역을 결합한다.3. 결합되어 커진 region을 최종 region proposal로 제안하기.  2. CNN - Feature Extraction 1. Selective Search를 통해 도출 된 각 region proposal로부터 CNN을 사용하여 4096차원의 feature vector를 추출한다.2. feature들은 5개의 convolutional layer와 2개의 fully connected layer로 전파되는데, 이때 CNN의 입력으로 사용되기 위해 각 region은 227x227 RGB의 고정된 사이즈로 변환되게 된다.  3. Linear SVM 1. SVM 학습을 위한 라벨로서 IoU를 활용하였고 IoU 가 0.5이상인 것들을 positive 객체로 보고 나머지는 negative로 분류하여 학습하게 된다. 각 SGD iteration마다 32개의 positive window와 96개의 backgroud window 총 128개의 배치로 학습이 진행된다.​2. 이후는 fine-tuning과 마찬가지로 positive sample 32개 + negative sample 96개 = 128개의 미니배치를 구성한 후 fine-tuning된 AlexNet에 입력하여 4096 dimensional feature vector를 추출한다. ​3. 추출된 벡터를 이용해 linear SVMs를 학습한다. - SVM은 2진 분류를 수행하므로 분류하려는 객체의 종류만큼 SVM이 필요하다. - 학습이 한 차례 끝난 후, hard negative mining 기법을 적용하여 재학습을 수행한다.​4. R-CNN에서는 단순히 N-way softmax layer를 통해 분류를 진행하지 않고, SVMs통해서 하게 된다. - SVM을 사용했을 때 성능이 더 좋기 때문이다. - 성능 차이의 이유는 SVM을 학습시킬 때  positive sample를 더 엄밀하게 정의하며, SVM이 hard negative를 이용해 학습하기 때문이다.​6. linear SVM에서는 output으로 class와 confidence score를 반환한다.​​4. Traning - 학습에 사용되는 CNN 모델의 경우 ILSVRC 2012 데이터 셋으로 미리 학습된 pre-trained CNN(AlexNet) 모델을 사용한다.- Classification에 최적화된 CNN 모델을 VOC 데이터셋에 적용하기 위해, VOC의 region proposals을 통해서 SGD방식으로 CNN 파라미터를 업데이트 한다.- ​CNN을 통해 나온 feature map은 SVM을 통해 classification 및 bounding regreesion이 진행되게 되는데, 여기서 SVM 학습을 위해 NMS(non-maximum suppresion)과 IoU(inter-section-over-union)이라는 개념이 활용된다. NMS(Non-maximum suppresion)1. 예측한 bounding box들의 예측 점수를 내림차순으로 정렬2. 높은 점수의 박스부터 시작하여 나머지 박스들 간의 IoU를 계산3. IoU값이 지정한 threhold보다 높은 박스를 제거4. 최적의 박스만 남을 떄까지 위 과정을 반복 ​ 4. Results on PASCAL VOC 2010-12- 위 테이블은 VOC 2010 테스트 데이터에 대한 각 모델별 결과이다.- 맨 오른쪽에서 mAP를 확인할 수 있는데, 논문에서는 같은 region proposal 알고리즘을 적용한 UVA모델과 R-CNN의 각각 mAP의 결과를 비교한다.- 위 표를 보면 UVA 모델의 mAP는 35.1%이고, R-CNN의 mAP는 53.7%인 것을 확인할 수 있으며 이것은 높은 증가율이라고 저자는 말한다. - 추가로, VOC 2011/12 데이터 셋 또한 53.3% mAP로 높은 성능을 나타냈다.​​ 5. Problems- R-CNN의 가장 큰 문제는 복잡한 프로세스로 인한 과도한 연산량최근에는 고성능 GPU가 많이 보급 되었기 때문에 deep한 neural net이라도 GPU연산을 통해 빠른 처리가 가능하다. 하지만 R-CNN은 selective search 알고리즘를 통한 region proposal 작업 그리고 NMS 알고리즘 작업 등은 CPU 연산에 의해 이루어 지기 때문에 굉장히 많은 연산량 및 시간이 소모된다.​- SVM 예측 시 real-time 분석의 어려움region에 대한 classification 및 bounding box에 대한 regression 작업이 함께 작동하다 보니 모델 예측 부분에서도 연산 및 시간이 많이 소모된다.​- 새로운 모델의 등장R-CNN의 위와 같은 한계점들로 인해, 추후 프로세스 및 연산 측면에서 Fast R-CNN과 Faster R-CNN 모델이 나오게 된다. - Faster R-CNN이 이전 모델보다 비교가 안될 정도로 더 빠르다는 것을 알 수 있으며, 성능 또한 향상되었다고 한다. ​ 6. References[1] R-CNN 논문(Rich feature hierarchies for accurate object detection and semantic segmentation)[2] https://velog.io/@skhim520/R-CNN-논문-리뷰[3] https://velog.io/@jaehyeong/R-CNNRegions-with-CNN-features-논문-리뷰​ "
딥러닝 기반 object detection 과정 정리 (4) - 중반부 (EfficientNet까지) ,https://blog.naver.com/taeeon_study/222787718798,20220625,"트래킹 경로 계획 아이디어가 잘 떠오르지 않아서 블로그나 하나 쓰고 긱사 가야겠다. 앞서 YOLO 기본 버전까지 리뷰했다. 이제 기본적인 것은 거의 언급을 했으니 흐름 위주로 쭉 리마인드 하면서 써야겠다. 여기서는 대부분 abstract, Introduction, Related Work은 생략하고 들어간다.​SSD(Single Shot multi-box Detector) [1]일반적으로 다단계의 합성곱 층을 거치면서 특징 맵의 Receptive Field는 넓어지는 동시에 Resolution은 감소한다. Layer가 깊어지면서 더 전역적인 정보를 갖는 특징 맵이 만들어진다는 것이 핵심적이다. 그렇기에 각 Layer의 특징 맵에서 bounding box를 예측하면 다양한 크기의 객체에 대한 탐지 및 Localization이 가능하다.  architectureTraining objectivex^k_ij= {1,0} 이며 I 번째 default box과 j 번째 ground truth 박스의 category p에 물체 인식 지표입니다. P 물체의 j 번째 ground truth와 I 번째 default box 간의 IOU가 0.5 이상이면 1이고 아니면 0이 됩니다.​N = number of matched default boxesI = predicted boxG = ground truth boxD = default boxCx, cy = box’s x, y coordinateW, h = box’s width, heightAlpha = 1 ​YOLO 9000(v2) [2] 기존 YOLO의 문제는 localization error가 컸고 recall이 낮은 문제가 있었다. 2 번째 버전에서는 성능과 속도 측면에서 모두 향상시켰다. 이 논문에서 Better, Faster, Stronger로 장점이 설명 돼있다. Better의 section부터 간단히 살펴본다.Batch Normalization기존 YOLO에 배치정규화를 추가하여 성능 지표인 map가 2% 이상 상승하였다.​2. High Resolution Classifier​기존 YOLO는 입력이 244x244인 classifier pre-train 모델로 사용하면서 입력 사이즈를 448x448로 증가시켜 모델을 학습시켰다. 여기서는 pre-train model 입력 사이즈를 448x448로 증가시키고 10 epoch동안 학습을 더하여 성능 향상을 이루었다.​3. Convolutional with Anchor Boxes​기존 YOLO는 bounding box들을 직접 예측하였다. 여기서는 fully connected layer를 빼고 Pooling layer를 하나 빼서 Convolution layer의 해상도를 늘렸다고 나와있다. 앞서 설명했듯이 기존 YOLO는 {P(object), x1, y1, w1, h1, … C1, …, Cn}의 output을 가졌다. 여기서는 Anchor box 개수만큼 클래스와 객체가 있는지에 대한 여부를 예측합니다. 정확도 측면이 살짝 떨어졌지만 recall이 증가하였다.​4. Dimension Clusters​사전에 좋은 anchor box를 선택하면 기존 YOLO의 dimension hand-pick 문제가 나아질 것이라고 생각하게 된다. Kmeans을 통해 학습 데이터에서 적합한 anchor box의 후보군을 찾는다. 본래 kmeans 알고리즘은 거리 기반으로 작동하는데 이 논문에서 거리는 IOU 기반으로 설정한다. K=5로 설정하였다고 한다.​5. Direct location prediction​기존에 anchor box를 사용했을 때 모델의 불안정성 문제가 또한 존재하였다. 여기서는 offset 값을 [0,1]의 범위로 주고 logistic activation을 사용한다.​  Direct location prediction6. Fine-Grained Features​Final Feature map의 크기를 13x13으로 변경하고 이전 layer에서 26x26의 feature map을 concatenation 한다. Resnet과 비슷해 보인다.​7. Multi-Scale Training​모델 학습시 Input Size를 변경하면서 학습하는 것을 제안한다. 몇 iteration마다 model의 input 크기를 변경해주는데 이는 10 batch마다 model의 downsample factor가 32인 것을 고려하여 {320,352,...,608} 중 임의로 이미지 차원 크기를 선택하여 학습하게 된다. 멀티 스케일 훈련은 중요한 개념이다.​8. Further Experiments​아래 참고 사진보면 된다. performance다음은 Faster 부분인데 Darknet-19라는 새로운 분류 모델을 제안하게 된다. 3x3 filter kernel을 사용하고 global average pooling을 이용하여 예측하며 1x1 filter를 사용하여 3x3 conv 특징 맵을 압축하는 방식을 사용하였다. 그리고 배치 정규화를 사용한다. 19개의 합성곱 층과 5개의 풀링층으로 이루어져있다. Darknet-19마지막으로 Stronger에 대한 설명이다.​1.     Hierarchical classification​여기서 WordNet 기반으로 계층적인 tree를 구조화했다. 이를 활용해 multi label 학습을 진행한다.  WordTree2. Joint classification and detection​Detection label에 대해서는 기존에 쓰던 방식으로 학습을 진행합니다. 그러나 classification label은 하위 node가 알 수 없는 문제가 있습니다. Object detection loss에서 classification loss만 전파하고 그 외는 전파하지 않습니다.​다음은 FPN에 대해 설명하겠습니다. 후에 언급할 EfficientDet이나 YOLO v4에서 구조를 적용하기도 한다.​FPN [3]Layer가 깊어지면서 더 전역적인 특징을 갖는 특징 맵을 추출하게 된다. 깊은 Layer에서 추출한 특징 맵을 현재 Layer의 특징 맵과 Concatenation 하여 동시에 고려한다. 이는 객체 인식의 성능을 개선한다. ​논문에서 (a) Featurized image pyramid, (b) Single feature map, (c) Pyramidal feature hierarchy, (d) Feature Pyramid Network 로 나누어 Introduction에서 설명하고 있다. 여기서는 (a), (b), (c)는 생략하고 중요한 (d)를 알아본다. various pyramid structured) FPN은 Top-down 방식으로 특징을 추출한다. 위에서 언급했듯이 low-resolution과 high-resolution을 묶게 된다. 각 레벨에서 독립적으로 특징을 추출하여 객체를 탐지하여 멀티 스케일 특징들을 효율적으로 사용할 수 있다.​ 1.     Bottom-up pathway​위로 올라가는 forward 단계에서는 layer 마다 의미 정보를 응축하는 역할을 한다. 깊은 모델의 경우에는 같은 레이어들을 하나의 단계로 취급하여 마지막 레이어를 skip-connection에 사용하게 된다. 즉, 각 단계의 마지막 layer 출력을 특징 맵의 Reference Set으로 선택한다. ​2.     Top-down pathway and lateral connections​하향식 과정에서는 많은 의미 정보들을 가지고 있는 특징 맵을 2배로 업샘플링 하여서 더 높은 해상도의 이미지를 만드는 역할을 한다. 여기서 skip-connection을 사용하여 손실된 local 정보를 보충한다. 업샘플링된 맵은 Element-wise addition에 의해 상향식 맵과 병합되는 과정을 거친다. 여기서, 1x1 conv layer를 거친다. 이것은 마지막 resolution map이 생성될 때까지 반복하게 된다. 마지막으로 합쳐진 map에 3x3 convolution을 추가하여 Aliasing 효과를 줄이게 된다. FPN의 핵심까지 했고 다음은 retinanet으로 넘어간다. ​RetinaNet [4]Focal Loss라는 개념이 들어오게 되는데 사실 이것이 90퍼 이상을 차지 하는 듯하다.​Focal Loss?Focal Loss는 cross entropy의 변형이다. Cross entropy는 모델이 이미 잘 detect 할 수 있는 것에는 더 잘 detect 할 수 있도록 하지만 어려운 부분은 계속 어렵게 한다는 문제가 있다. Easy example이 많고 Hard example이 적은 class Imbalance 이슈는 object detection이 가지고 잇는 고유 문제였다. 앞서 설명한 2 – Stage Detector는 RPN에서 물체가 있을만한 높은 확률 순으로 필터링을 먼저 수행했었다. 그러나 1 – Stage Detector는 같이 수행하므로 이로 인한 성능 저하는 컸다. 즉, Easy example에 대한 Loss가 압도적으로 컸습니다. 그래서 이것을 이미 높은 confidence로 예측하는 것의 Loss는 많이 낮추고 낮은 confidence로 예측하는 것의 loss를 조금 낮춤으로써 해결하게 된다. 보충 설명 자료위 이미지 출처: https://junha1125.github.io/blog/artificial-intelligence/2020-08-23-RetinaNet/ 【Paper】 RetinaNet - Focal Loss, FPNRetinaNet에 대해서 차근 차근 핵심만 파악해보자.junha1125.github.io Focal Loss논문에서 gamma를 2~5 값을 사용하고 alpha는 balanced variable이다. Gamma = 2, alpha = 0.25가 best였다고 한다.​ architectureAnchors의 부분에는 IOU threshold가 0.5 이상이면 anchor, 0.4이하이면 background로 판단하고 있다. SSD 처럼 anchor에 대해 GT box로 가기 위해서 어떻게 늘리고 이동해야 하는지에 대한 localization 값을 구하기 위해 anchor를 사용한다. 마지막으로 Box Regression Subnet이 있는데 각 Anchor에 대해 4개의 값을 예측하게 된다. 깊은 설명은 논문을 읽어보는 것도 좋은 것 같다.​이제 현재에도 많이 쓰이는 YOLO v3로 넘어와본다. ​YOLO v3 [5]​1.     Bounding Box Prediction​추가로 v3에서는 다른 탐지 알고리즘의 Matching Strategy를 가져왔다. 기존 YOLO와 다르게 각각 bounding box마다 objectness score를 예측하고 이 때, prior box와 ground truth의 IOU가 가장 높은 박스를 1로 두어 매칭하였다. (앞서 언급한 SSD를 참고하면 좋을 것 같다.) Loss를 prior box와 같은 index와 위치를 갖는 예측된 box의 offset만 계산해주겠다는 의미이다.​2. Class Prediction​Multi-label이 있을 수 있으므로 class prediction으로 softmax를 쓰지 않고 independent logistic classfier를 썼다고 나와있다. 그렇기에 loss도 binary cross entropy를 사용하였다.​3. Predictions Across Scales​3개의 bounding box, 3개의 feature map을 활용한 feature map에서의 output 형태는 N × N × [3 ∗ (4 + 1 + 80)] 즉, (Grid x Grid x (number of bounding box x (offset + objectiveness  + class))이다. YOLO v2처럼 9개의 anchor box는 kmeans를 활용하여 결정한다. ​ Darknet-53​Mask R-CNN도 언급해야 하는데 이는 아직 공부를 안했기에 패스 ㅜㅜ​대충 이까지가 2018년 정도까지 나온 논문들일 것이다. 이제 이 뒤로 나온 논문은 2019년부터라고 생각하면 된다. 물론 이 분야는 1년 차이도 상당히 크다고 하지만 비교적 최근인 것은 틀림없다. EfficientNet부터 출발한다.​EfficientNet [6]​CNN의 입력 크기를 키워줄수록 성능이 좋아졌다. 그러나 리소스와 trade-off가 있었다. 그렇기에 최적의 depth, width, resolution을 찾는 네트워크를 제안하게 된다. ​Depth: Layer를 더욱 늘려서 깊게 쌓는 것, Width: Filter의 개수를 늘리는 것, Image resolution: 입력 이미지의 해상도를 키우는 것으로 이해하시면 된다.​ Model ScailingCompound Model Scailing​1.     Problem Formulation기존 ConvNets의 Formulation은 아래 그림과 같다.​ N = Convolution Network, F_i = 한 stage에서 반복적으로 수행하는 함수 (skip connection etc..) L_i = 한 stage에서 F_i를 몇 번 반복할 것인지를 나타낸다일반적으로 F_i는 고정합니다. 여기서 length나 resolution, width를 조절해서 최적 모델을 찾는 과정을 반복합니다. 그래서 이 논문에는 Formulation 문제를 개선해서 design space를 줄이기 위해 모델 scale을 결정하는 3가지 요소를 모두 고려하는 방법을 제시하게 된다. 보충 설명이미지 출처: https://hcshin.tistory.com/4 [paper] EfficientNet 리뷰EfficientNet - Improving Accuracy and Efficiency through AutoML and Model Scaling 안녕하세요. 이 포스팅은 EfficientNet review 를 정리한 것으로 PR-168 발표 강의를 기반하고 있습니다. Google에서 ICML20..hcshin.tistory.com ​2. Scailing Dimensions​네트워크가 증가함에 따라 accuracy gain이 감소하는 경향을 보이며 Depth나 Width는 Saturation point에 도달하여 연산량이 증가함에 따라 정확도 향상이 미미한 것을 보이고 있다. 보충 그림3. Compound Scailing 논문에서 2가지 관찰이 나오게 된다. 그것은 아래와 같다.​Observation1: Scaling up any dimension of network width, depth, or resolution improves accuracy, but the accuracy gain diminishes for bigger models​Observation 2 – In order to pursue better accuracy and efficiency, it is critical to balance all dimensions of network width, depth, and resolution during ConvNet scaling​이 관찰로 인해 새로운 compound scailing method를 제안하게 된다. Phi는 사용자가 가지고 있는 parameter이다. S.t.는 네트워크를 scalling up하기 용이하기 위해 한 것이다. Alpha, beta, gamma의 지수승을 다르게 한 이유는 연산량에 비례하여 FLOPS를 계산하기 위함이다. 그러나 채널 수 (beta)나 resolution (gamma)를 2배로 증가시키면 연산량이 2의 지수승으로 증가하게 된다. 여기 논문에는 2^phi로 증가한다고 나와있다.​Architecture는 아래와 같다. Architecture이제 EfficientNet이 나오고 이를 backbone으로 한 EfficientDet이 등장하게 된다. 2020년까지 SOTA를 유지했다고 알고 있다. 다음 포스트에서는 EfficientNet을 Backbone으로 한 EfficientDet부터 출발하여 YOLOX까지 리뷰하고 중반부를 마무리 해야겠다. ​이번 포스트에서 리뷰했던 논문 본문은 아래에 첨부한다. ​[1] LIU, Wei, et al. Ssd: Single shot multibox detector. In: European conference on computer vision. Springer, Cham, 2016. p. 21-37.[2] REDMON, Joseph; FARHADI, Ali. YOLO9000: better, faster, stronger. In: Proceedings of the IEEE conference on computer vision and pattern recognition. 2017. p. 7263-7271.[3] LIN, Tsung-Yi, et al. Feature pyramid networks for object detection. In: Proceedings of the IEEE conference on computer vision and pattern recognition. 2017. p. 2117-2125.[4] LIN, Tsung-Yi, et al. Focal loss for dense object detection. In: Proceedings of the IEEE international conference on computer vision. 2017. p. 2980-2988.[5] REDMON, Joseph; FARHADI, Ali. Yolov3: An incremental improvement. arXiv preprint arXiv:1804.02767, 2018.[6] TAN, Mingxing; LE, Quoc. Efficientnet: Rethinking model scaling for convolutional neural networks. In: International conference on machine learning. PMLR, 2019. p. 6105-6114.​​​​​​​​​​​​​​​​​​​​​​​​​​​​​ ​​​​​ "
[object detection] R-CNN & Region Proposal ,https://blog.naver.com/lsj952005/222603653256,20211224,"​ <R-CNN 진행 과정>(1) Input image(2) selective search 알고리즘으로 object가 있을 법한 곳에 약 2000개의 region 추출(3) resize (227by227) (4) 각 region 별로 CNN 통과시켜 feature를 추출(5) linear SVM을 이용해 각 region의 classification 결과와 bounding box를 어디로 옮겨 좋을지를 맞추는 regression을 계산(높이 너비 그리고 중심정 x,y)​​- 관련 논문에 따르면 RCNN은 GPU를 쓰면 한장에 13초, CPU를 쓰면 한장에 53초가 소요되며, 이는 앞서 말 했듯이 2,000개의 region을 모두 개별적으로 CNN에 들어가야 하기 때문에 시간이 오래걸리는 것을 알 수 있습니다. 이것이 RCNN의 보틀넥이라고 할 수 있습니다. 이 후 보게될 RCNN은 이 보틀넥을 개선시키고자 CNN을 한번만 돌리게 됩니다.​  <Region Proposal>Region proposal에는 여러가지 방법이 있는데 그 중 2가지 방법에 대해 살펴보겠습니다.​ 우선 selective search 입니다.인접한 영역끼리 유사성을 측정해 큰 영역으로 차례대로 통합하는 과정으로 위에서 살펴본 RCNN과 이 후 살펴볼 Fast RCNN이 이 방법을 사용하고 있습니다.​  https://cdn-images-1.medium.com/max/600/1*44PEagy9PNIcBLqNI-zLcg.gif<Selective Search Algorithm 진행 과정>(1)입력 이미지에 대해 segmentation을 실시(2) 가장 유사도가 높은 영역 i와 j 선택(3) 선택된 영역을 t로 병합(4) i와 j가 연관된 다른 유사도 집합들은 제거(5) 병합된 t영역과 나머지 영역들간의 유사도 재정의(6) 새로운 유사도 집합에 합쳐진 영역을 추가 포함(7) 하나의 영역이 될 때 까지 반복유사도 측정방법 : 0과 1 사이로 정규화된 4가지 요소(색상, 재질, 크기, 채움-Fill)들의 가중합으로 계산​- selective search는 Cpu 기반에서 수행되기 때문에 이미지 한 장 소요시간이 비교적 길다는 단점이 있습니다. (2초가량)​  <Sliding Window> https://cdn-images-1.medium.com/max/600/1*44PEagy9PNIcBLqNI-zLcg.gif- 다양한 scale을 가진 window를 이미지 전체를Sliding 하며 score를 얻는 방식 - 너무 많은 영역에 대하여 확인해야 한다는 단점이 있다.​​​​​​참고 논문: Rich feature hierarchies for accirate object detection and semantic segmentation Tech report (v5) "
[Object Detection] Intersection over Union  Metric 이해하기 ,https://blog.naver.com/qkrdnjsrl0628/222815344155,20220717,"* R-CNN 논문을 리뷰하기 전 Object Detection Metric 내용 포스팅 합니다. IOU(Intersection over Union)     - Intersection over Union은 Object Detector의 정확도를 측정하는데 이용하는 평가 지표이다.       알고리즘을 통하여 예측한 바운딩 박스는 IoU를 이용하여, 평가하는 것이 가능합니다.​     - IOU를 적용하기 위한 2가지 조건          - ground-truth bounding boxes(testing set에서 object위치를 labeling한 것)          - predicted bounding boxes (model이 출력한 object 위치 예측 값)​* 하단 이미지의 predicted bounding box는 빨강색이며, ground-truth는 초록색입니다. IOU 계산 공식    - Area of union : predicted bounding box와 ground-truth bounding box                                   를 둘러싼 영역을 의미합니다.    (합집합)    - Area of overlab : predicted bounding box와 ground-truth bounding box                                      를 둘러싼 영역을 의미합니다.  (교집합) Why using IOU(Intersection Over Union)?    - 머신러닝으로 분류 문제를 수행할 때는 predicted class가 맞는지 틀렸는지 쉽게 확인하면 됩니다.       맞다, 틀리다 두가지로 구분하면 되기 때문입니다.​    - 하지만, Object Detection에서는 predicted bounding box 와 ground-truth bounding box가       서로 일치하는 경향이 거의 없습니다. IOU Pytorch[파이토치] 구현          - Bounding box를 표현하는 2가지 방식                (1) midpoint                            - midpoint 형식의 바운딩 박스는 4개의 변수를 갖습니다. (x1, y1, w, h)                              여기서 (x1, y1)은 바운딩 박스의 중앙점, (w, h)는 바운딩 박스의 넓이(w), 높이(h)입니다.                (2) corner                            - corner 형식의 바운딩 박스는 (x1, y1, x2, y2) 이다.                              여기서 (x1, y1)은 좌측 상단이며, (x2, y2)는 우착 하단의 좌표를 의미합니다.​* Python Ellipsis에 관하여,  알아주시기 바랍니다.https://tech.madup.com/python-ellipsis/ 파이썬 세 개의 점, Ellipsis 객체는 무엇인가요?파이썬에서 사용되는 Ellipsis 객체의 여러가지 용법에 관한 내용을 번역하였습니다.tech.madup.com * 이때, 이미지의 경우의 좌표값을 주의해서 보시기 바랍니다! (일반적인 상식과 다릅니다.)https://nomalcy.tistory.com/221 matplotlib 축 지우기 : axis(""off"")<!DOCTYPE html> matplotlib 축 지우기 axis(""off"") matplotlib 축 지우기 : axis(""off"")¶    ⏳   패키지 불러오기 In [1]: from matplotlib import pyplot as plt import cv2     ✔    옵션 없..nomalcy.tistory.com import torchdef intersection_over_union(boxes_preds, boxes_labels, box_format = ""midpoint""):  # boxes_preds shape is (N, 4) Where N is the Number of bboxes  # boxes_labels shape is (N, 4)  if box_format == ""midpoint"":    	box1_x1 = boxes_preds[..., 0:1] - boxes_preds[..., 2:3] / 2    	box1_y1 = boxes_preds[..., 1:2] - boxes_preds[..., 3:4] / 2    	box1_x2 = boxes_preds[..., 0:1] + boxes_preds[..., 2:3] / 2    	box1_y2 = boxes_preds[..., 1:2] + boxes_preds[..., 3:4] / 2    	box2_x1 = boxes_labels[..., 0:1] - boxes_labels[..., 2:3] / 2    	box2_y1 = boxes_labels[..., 1:2] - boxes_labels[..., 3:4] / 2    	box2_x2 = boxes_labels[..., 0:1] + boxes_labels[..., 2:3] / 2    	box2_y2 = boxes_labels[..., 1:2] + boxes_labels[..., 3:4] / 2  if box_format == ""corners"":    box1_x1 = boxes_preds[..., 0:1]    box1_y1 = boxes_preds[..., 1:2]    box1_x2 = boxes_preds[..., 2:3]    box1_y2 = boxes_preds[..., 3:4]     # (N, 1)    box2_x1 = boxes_labels[..., 0:1]    box2_y1 = boxes_labels[..., 1:2]    box2_x2 = boxes_labels[..., 2:3]    box2_y2 = boxes_labels[..., 3:4]    x1 = torch.max(box1_x1, box2_x1)  y1 = torch.max(box1_y1, box2_y1)  x2 = torch.min(box1_x2, box2_x2)  y2 = torch.min(box1_y2, box2_y2)  # clamp(0) is for the case when they do not intersect  intersection = (x2 - x1).clamp(0) * (y2 - y1).clamp(0)  box1_area = abs((box1_x2 - box1_x1) * (box1_y1 - box1_y2))  box2_area = abs((box2_x2 - box2_x1) * (box2_y1 - box2_y2))  return intersection / (box1_area + box2_area - intersection + 1e-6)   Referencehttps://www.youtube.com/watch?v=XXYG5ZWtjj0&ab_channel=AladdinPersson  https://deep-learning-study.tistory.com/402 [Object Detection] IoU(Intersection over Union)를 이해하고 파이토치로 구현하기 안녕하세요 이번 포스팅에서는 IoU에 대해 알아보도록 하겠습니다. IoU(Intersection over Union)은 무엇일까요?  Intersection over Union은 object detector의 정확도를 측정하는데 이용되는 평가 지표입니다..deep-learning-study.tistory.com ​ "
[object detection] Fast R-CNN & Faster R-CNN ,https://blog.naver.com/lsj952005/222603688474,20211227,"[Fast R-CNN] <Fast R-CNN 진행 과정>(1) selective search 방법으로 bounding box 생성(2) 영역을 전혀 고려하지 않은체 CNN을 통해 feature map을 만든다.(3) 이미지에서 얻은 bounding box를 feature map에 projection(4) 고정된 크기의 벡터를 만들기 위해 ROI Pooling 거침(5) fully connected layer에 넣어 softmax를 이용해 Classification과 bounding box regression을 진행​- Fast RCNN은 기존 RCNN의 보틀넥인 2000개의 region을 각각 CNN에 넣어야 한다는 점은 개선시켜 이미지 한번만 CNN 모델에 넣음으로 속도 개선을 시켰지만, RCNN과 마찬가지로 CPU 기반의 selective search 방법으로 region을 추출하기 때문에 아직까지 속도가 느기게 동작한다는 보틀넥이 존재 하고 있습니다.​​(ROI Pooling) https://deepsense.ai/region-of-interest-pooling-explained/ 분류를 위해 fully connected layer를 이용해야 하기 때문에 fully connected layer의 입력으로 고정된 크기의 벡터를 만들기 위해  진행하는 과정입니다.예를 들어 2by2 크기의 feature를 추출한다고 하면 -> feature map에 대해서 RoI를 투영하고 -> 다음과 같이 투영한 feature map의 RoI를 2by2로 최대한 같은 비율로 적절히 나누어 질 수 있도록 합니다. ->  이 후 각각의 영역마다 가장 큰값을 선택하여 고정된 크기의 벡터를 만들게 됩니다.​이것을 이용해 classification과 regression을 진행하게 됩니다.​  [Faster R-CNN](Fast RCNN의 속도의 단점을 개선한 Faster R-CNN) Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks; Shaoqing Ren, Kaiming He…- 기존 RCNN과 fast RCNN은 딥러닝과 무관한 다른 방법을 통해 bounding box를 뽑았기 때문에 이 부분에서 보틀넥이 있었지만, Faster RCNN 부터 딥러닝을 이용한 region proposal이 진행되게 됩니다. Faster RCNN을 간단하게 살펴보면 기존 fate RCNN에 RPN을 더한 구조입니다.​​<Region Proposal Network (RPN)> Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks; Shaoqing Ren, Kaiming He…- RPN은 feature map이 주어졌을 때 물체가 있을 법한 위치를 예측하는데 사용됩니다.미리 정해져 있는 서로 다른 k개의 anchor box가 사용되고 sliding window 방식으로 각 위치에 대해 regression과 classification을 수행합니다. 논문에서는 3종횡비와 3개의 크기를 가지는 총 9개의 anchor box를 사용하고 있습니다. 여기서 regression은 boundin box의 중간점의 좌표 x, y 와 높이와 너비에 대한 예측을 수행하고 Classification은 물체의 존재 유무에 대한 분류를 수행합니다. 자세히 살펴보자면 입력으로 받은 feature map의 각 픽셀을 센터로 하는 k개의 anchor box를 만들고, 만들어진 anchor box 들을 성능지표를 바탕으로 객체가 존재할 것이라고 분류한 위치에 대해 bounding box를 만듭니다. 해당 논문에서는 IoU 값이 크거나, 0,7 이상인 것을 positive로 놓고, iou가 0.3 보다 낮은 것을 negative(background)로 설정해 bounding box를 만들어 갑니다.추가적으로 anchor box의 크기와 어디의 위치하는 것이 좋은지를 regression 학습통해 얻어 내는 과정입니다. 이 후 한 객체당 여러 proposal 값이 나온 것을 NMS 알고리즘을 사용해 box들 중 confidence가 가장 높은 box 하나만 남기고 모두 지워줌으로써 최종적으로 ROI를 찾아 내게 됩니다.이러한 RPN의 도입으로 RCNN과 Fast RCNN의 보틀넥을 개선함으로써 Faster R CNN은 전체 프레임워크를 end-to-end 방식으로 진행되고 이전 모델 보다 더 빠르고 강력한 결고를 얻을 수 있었습니다.​​​​https://github.com/factorLee/Faster-RCNN_Tensorflow GitHub - factorLee/Faster-RCNN_Tensorflow: This is a tensorflow re-implementation of Faster R-CNN: Towards Real-Time ObjectDetection with Region Proposal Networks.This is a tensorflow re-implementation of Faster R-CNN: Towards Real-Time ObjectDetection with Region Proposal Networks. - GitHub - factorLee/Faster-RCNN_Tensorflow: This is a tensorflow re-impleme...github.com ​ "
"Fast R-CNN(Object Detection, 객체 탐지) ",https://blog.naver.com/grow_bigger/222769686711,20220612,"AbstractFast R-CNN은 Deep convolutional networks를 기반으로 한 객체탐지의 한 모델입니다. 이름의 Fast를 봐도 알 수 있듯이 속도 향상이 핵심적인 발전이라고 할 수 있습니다. 물론 정확도도 향상이 되었다고 하니 객체탐지 모델의 발전 흐름에서 중요한 연구로 인식이 되고 있습니다. 속도가 빨라진다는건 나중에 나오는 1-stage방식의 YOLO처럼 '실시간 탐지'로까지의 발전을 생각해 볼 수 있었기 때문에 정확도를 손해보더라도 굉장히 유용한 특징이 될 수 있습니다. 참고로 YOLO는 이 논문 이후에 나오는데 1-stage 방식의 대표적인 모델입니다. 현재기준 버전4까지 나온 걸로 알고 있습니다.​Introduction  First, numerous candidate object locations (often called “proposals”) must be processed. Second, these candidates provide only rough localization that must be refined to achieve precise localization.Fast R-CNN위의 인용에서 얘기한내용은, 심층합성곱 신경망을 활용한 객체 탐지 모델들이 localization(객체 위치 파악) 때문에 복잡해 진다는 내용입니다. 사진에 객체 후보군이 많고, 이러한 위치 탐지 결과는 대락적인 위치 정보이기 때문에 다시 한번 정밀하게 교정되야 합니다.​본 논문의 저자가 짚은 R-CNN의 단점입니다1. 학습이 다단계 파이프라인이다.R-CNN은 합성곱 신경망 학습 / SVM(서포트 백터 머신)  학습 / bounding-box 회귀 학습 이렇게 3번의 학습 단계가 필요하다.2. 학습이 공간과 시간에서 비용이 크다.(디스크를 많이 차치하고 시간이 많이 걸린다.)3. 각각의 객체 제안에서 features를 추출하다보니 객체 탐지과정이 느리다.​정리를 하자면 학습시에도 자원소모가 크고, 실제로 객체 탐지를 수행하는 과정(test time)에서도 각각의 이미지 처리하는데 시간이 많이 소모되는 문제가 있다는 것입니다. VGG16의 경우 GPU로 이미지당 47초가 걸렸다고 합니다. Fast R-CNN의 경우 객체 위치 제안 과정(object proposal)을 제외하면 이미지당 0.3초가 걸리는 것과 비교하면 R-CNN도 객체 위치 제안 과정(object proposal)을 제외하더라도 확실히 느립니다.​ 왜 R-CNN은 느릴까요? 이유는 각각의 객체 제안에 대해 합성곱 연산을 별도로 하기 때문입니다. 만약 객체가 하나의 입력 이미지 안에 많다면 처리속도는 훨씬 더 느려질 것입니다. 그래서 이러한 문제를 계산을 공유하는 방법을 통해 해결하려는 연구가 있었습니다. 바로 Spatial pyramid pooling networks (SPPnets)입니다.이 SPPnets는 입력이미지 전체에 대해 합성곱 특징 맵(feature map)을 만든 후에 이 특징 맵에서 특징 벡터를 추출하여 객체 위치 제안을 추출합니다. 이 추출된 객체 위치 제안을 분류하여 최종 결과를 만드는데, 이 모델에서 핵심은 특징 맵을 공유한다는 것입니다. 그래서 굉장히 속도가 향상된 결과를 만들어 냈습니다.​다만 SPPnets도 여러 결함을 가지고 있었습니다. 하나는 R-CNN이랑 비슷하게 3단계의 학습과정을 가진 학습이 multi-stage pipeline이라는 점입니다. 다른 하나는 spatial pyramid pooling 앞의 convolutional layers을 fine-tuning할 수 없다는 것입니다.  위 그림에서 위쪽은 R-CNN입니다. R-CNN은 이미지 크기를 잘라 동일하게 맞춰주는 등의 작업을 추가로 했었습니다. SPPnets은 그림의 아랫부분인데 fine-tuning이 불가능한 부분이 바로 입력 다음의 convolutional layers입니다.​ 1. Higher detection quality (mAP) than R-CNN, SPPnet 2. Training is single-stage, using a multi-task loss 3. Training can update all network layers 4. No disk storage is required for feature caching Fast R-CNN저자들이 얘기하는 Fast R-CNN의 장점들입니다. 일반적으로 훈련이 단순해졌고 모델의 성능이 우수해졌습니다.또한 SPPnerts와 달리 모든 층에 대해 학습이 가능하므로 합성곱 계층이 깊어져도 정확도를 높일 수 있습니다.디스크 사용도 절약하는 특징까지 추가되었습니다.​Fast R-CNN architecture and training [architecture] Fast R-CNN architectureFast R-CNN은 하나의 전체 이미지와 위의 사진에서는 빨간 사각 영역인 객체 제안(object proposal)을 입력으로 받습니다. 이러한 입력은 가장 먼저 여러 합성곱 계층과 max pooling층으로 처리되어 합성곱 특징 맵( conv feature map)을 만듭니다. (사진에서 전체 이미지 처리한 흰 사각형과 그보다는 작은 회색 사각 영역.)그러고 난 후, ' region of interest 풀링 층'은 각각의 객체 제안에 대해 특징 맵으로 부터 고정길이 벡터를 추출합니다. 객체 제안이 여러개라면 특징 벡터도 여러개일 텐데 이러한 각각의 특징 벡터는 완전 연결 계층(fc layers)을 통과한 후 두 출력 층으로 분기합니다.하나는 K개의 객체 클래스와 모든 배경 클래스에 대해 소프트맥스 확율 추정치를 생성합니다. 관심있는 클래스 K개와 관심없는 그 외 객체를 하나로 묶어 별도의 하나의 클래스로 취급합니다.나머지 하나는 K개의 객체 클래스에 대해 실수 4개씩 출력합니다. 이 실수 4개씩 해서 한세트는 K개의 클래스에 대해 조정된 bounding-box 위치를 인코딩합니다.​[Initializing from pre-trained networks]Fast R-CNN의경우 세가지 변형을 적용하여 사전 학습된 신경망으로 Fast R-CNN신경망을 초기화하였습니다.Fast R-CNN의 경우 완전연결 층과 사용할 수 있도록 크기를 고정해야 합니다. 따라서 VGG16의 마지막  max pooling 층을 RoI층으로 대체하였습니다. 저자들은 7x7의 크기로 출력이 되어 완전 연결 층으로 입력되도록 설정하였습니다.VGG16의 마지막 fc layer과 소프트맥스는 K+1클래스(카테고리)에 대한 fc layer과 소프트맥스, 그리고 category-specific bounding-box regressors로 교체되었습니다. bounding box regressor을 추가한 것입니다. 원래는 fc layer가 하나였고 이 완전 연결층의 최종 output을 소프트맥스가 산출하는 것이었으나 이를 두가지의 층으로 교체함으로써 나중에 training을 분류작업과 객체 위치 제안에 대해 동시에 훈련할 수 있게 됩니다.신경망이 이미지 뿐만 아니라 RoI(region of interest)도 입력으로 받습니다.​[Fine-tuning for detection]위에서 저자들이 Fast-RCNN에대해 언급한 장점들에서 훈련과정이 하나의 단계인 점과 모든 network layer를 업데이트할 수 있다는 점이 있습니다. 바로 위에서 CNN의 구조를 변경하여 훈련과정을 하나의 단계로 할 수 있도록 했었는데, 이번에는 모든 layer를 업데이트 하는 방법을 소개합니다.​Fast RCNN training에서 미니배치를 뽑는데 이 샘플은 이미지를 뽑은 것입니다. 이 선택된 이미지들(이미지 몇개를 뽑은게 미니배치)에서 또 R/N개 만큼의 RoI들을 샘플링합니다. 이때 N을 작게 잡으면 연산이 줄어드는데, 왜냐하면 하나의 이미지에서 많은 RoI를 뽑아 학습을 하게 되는 방식이 되기 떄문입니다. 총 100번의 학습을 하고 싶은데 100개의 이미지에서 1개의 RoI를 뽑기보단 10개의 이미지에서 10개의 RoI를 뽑으면 미니배치 연산을 덜 할 수있습니다.(미니배치를 샘플링하는 등의 작업을 줄일 수 있습니다.)​저자들은 하나의 이미지에서 64개의 RoI를 샘플링했습니다.(R=128, N=2) 하나의 이미지 내에서 RoI 들이 연관되어 학습을 방해하는 경우가 발생하지 않아 좋은 성능을 보여주었습니다. SPPnets가 convolutional layers를 fine-tuning하지 못하는 이유로 저자들은 비효율성을 언급했는데, 이렇게 연산을 줄이는 방법을 통해 해결하였습니다.​추가로, 계층적 표본화(hierarchical sampling)말고도 Fast R-CNN은 훈련과정에서 분류기와 bounding-box regressor를 한꺼번에 최적화하였습니다.​[Multi-task loss] from Fast R-CNN paper위의 식은 multi-task Loss를 구하는 식입니다.  u는 class의 라벨이고, v는 bounding-box의 라벨입니다.(편의상 ground-truth을 라벨이라고 적었습니다.)  각각의 항은 위의 식으로 이루어져있습니다. Lloc에서 t가 예측된 좌표(top-left)와 가로, 세로 정보이고, v는 라벨입니다. 여기서 중요한 점 한가지는, class는 K+1이었다는 점입니다. 관심이 없는 객체 클래스 하나가 추가되었었는데 관례상 u=0이라고 합니다. 그런데 v, 즉 bounding-box는 관심이 없는 물체였으니 정답이라는 것자체가 없을 것입니다. 만약 관심없는 것까지 다 경계선을 그린다고 하면 풀 한포기마다 다 경계선이 잡혀버릴 것입니다. 따라서  background RoIs에 대해서는 bounding-box 라벨이 없어으므로 Lloc를 무시합니다. Fast R-CNN에서는 L1 loss를 사용하는데 L2가 사용된 R-CNN이나 SPPnet에 비해 이상치에 대해 덜 민감해지는 효과를 가집니다. 거기에 기울기 폭발을 방지하기 위해 바로 위의 사진의 식을 이용합니다. smooth L1은 L1과 L2를 혼합된 형태의 함수입니다. Huber loss(Huber Loss란? (velog.io))를 참고하면 좋을것 같습니다. 어쨌든 x가 오차이므로 오차가 일정 이상일때는 기울기 폭발을 방지하되, 오차가 일정한 값 이내로서 작다고 판단되는 경우 기울기가 선형인경우보다 훨씬 빠르게 감소합니다. 그러므로 모델의 가중치가 더 빠르게 움직일 것입니다.​마지막으로 람다는 두  loss의 균형을 조절하는데 논문의 저자는 1로 놓고 사용하였습니다.​[Mini-batch sampling]미니 배치 샘플링은 연산자원을 아끼기 위해 고안했던 미니 배치 경사하강법을 위한 샘플링입니다. 한 이미지 당 64개의 RoI들을 사용하는데, 논문에서는 최소 intersection over union (IoU)가 0.5이상인 객체 위치 제안중에서 25%를 사용하여 객체 탐지 훈련 데이터로서 사용합니다.배경 클래스에 대해서는 ground truth와의 IoU가 0.1~0.5범위인  object proposals중에서는 최대인 것들로부터 샘플링합니다. 일반적인 경우 IoU가 0.2이런 것보단 0.4, 0.3정도의 RoI들이 사용될 것입니다. 즉,  Multi-task loss에서 u=0인 샘플로써 사용한다는 것입니다.​[back-propagation through RoI pooling layers]CNN층이 이미지를 잘 표현해내는 것이 성능에 큰 영향을 미칠 것이라는 점은 쉽게 예측할 수 있습니다. 그래서 이 논문은 SPPnet이 CNN층을 fine-tuning하지 못하는 점을 단점으로 짚었었습니다. fine-tuning하지 못한다는 말은 해당 부분이 훈련을 해도 업데이트 되지 않는 다는 뜻입니다. 즉, 오차 역전파등의 방법으로 가중치를 수정할 수 없는 것인데 Fast R-CNN의 경우는 모든 층에 대해서 수행할 수 있습니다. 이부분은 Fast-RCNN의 특징인 RoI pooling층에 대해서 어떻게 연산이 이루어지는지 N=1이라 가정하여 하나의 이미지만 다르는 상황을 통해 간략히 보여주는 것입니다. 이미지들이 어차피 독립적으로 다루어져서 N=1로 나두어도 확장하는데는 큰 어려움은 없습니다.​자 그럼, 식을 설명하기 앞서 변수들에 대해 알아보겠습니다. 우선 xi ∈ R는 feature map이자 RoI pooling층의 입력의 한 부분에 대한 실수 값입니다. i는 위치정보이므로 xi는 해당 유닛(또는 픽셀)의 값 자체입니다. (CNN을 통과하면 행렬이 나오는데 행렬의 한 원소라는 얘기입니다.) yrj는 레이어의 j번째 출력입니다. 레이어는 RoI pooling층을 말합니다. r번째 RoI를 풀링층에 통과시킨 후 나오는 행렬의 j번째 출력 값입니다. ​정리하자면 x는 입력, y는 출력입니다. 그런데 풀링층은 MaxPooling과 AveragePooling이렇게 두가지가 대표적입니다. 따라서 어떤 풀링층을 사용했는지 수식으로 표현해줄 필요가 있습니다. 따라서 논문에는 다음의 식이 있습니다.  yrj = xi ∗(r,j)인데 위치 정보인 i ∗(r,j)는 위 사진의 식으로 구해집니다. R(r, j)은 RoI의 작은  sub-window입니다. 하나의 sub-window가 pooling층을 통과하면 하나의 y을 출력으로 갖습니다. 자, 그럼 식의 의미가 명확해집니다.   i ∗(r,j)는 sub-window중 xi'가 최대인 행렬의 원소위치입니다. 즉, 최대 풀링 계층이라는 것입니다.​RoI pooling층에서 역전파는 다음 식처럼 loss를 각각의 xi에 대해 구합니다. 식에 대괄호가 보이는데 이 대괄호는  Iverson bracket입니다. 조건을 만족하면 1 아니면 0입니다. 대괄호안의 조건이 만족하면 우변은 ∂L/∂yrj 그 자체이므로 i가 주변 x중 가장 큰 x의 인덱스라면 ∂L/∂yrj 이 누적됩니다. 객체가 겹쳐있거나 RoI가 비슷한 영역에 여러개가 잡할 수 있습니다. 그러면 같은 x가 여러번 선택될 수 있는데 누적한다는 거는 이렇게 여러번 선택되는 경우를 다 더해서 가중치를 수정하라는 얘기입니다. RoI하나당 한번 업데이트하고 다음 RoI에 또 같은게 선택되면 또 하고 이렇게 하지 않고 한꺼번에 진행합니다. 역전파에서 ∂L/∂yrj은  최상위 RoI pooling층의 backwards function에서 계산이 이미 됩니다.​[ Scale invariance]규모 불변성에 대한 내용인데 이를 위한 두 가지 방법이 있습니다. 하나는 사이즈를 고정시켜버리는 것이고 하나는 이미지 피라미드를 이용해 훈련시에는 이미지 샘플링때마다 랜덤한 스케일의 피라미드를 사용하여 훈련하는 방법입니다.​ Fast R-CNN detection이제 전반적인 모델의 매커니즘에 대한 소개는 끝났습니다. 모델은 다음과 같이 작동합니다.1. 모델의 test시에 r번째 RoI에  Pr(class = k | r) ∆= pk인 추정확률(RoI 가 주어졌을때 어떠한 클래스에 속할 확률)을 이용하여 detection confidence를 할당합니다. 2.  non-maximum suppression을 적용합니다.위의 방식으로 훈련이 아닌 test시 모델을 작동시킵니다.그런데 이 논문의 이름에서도 볼 수 있듯 속도가 중요합니다. 아래 사진을 보면 fully connected layer처리를 하는데 전체적인 시간의 상당부분을 할애합니다. 조금 가속화할 필요가 있는 부분입니다. ​이때  truncated SVD를 적용해볼 수 있습니다.완전연결계층은 W로 매개변수화되는데 특잇값분해를 통해 W다음 식처럼 분해될 수 있습니다.W ≈ UΣtV T그런데 truncated SVD는 위 식의 우변의 크기를 줄여버리는 방식입니다. 따라서 연산량을 줄일 수 있습니다.그러면 truncated SVD로 분해된 W, UΣtV T이 나옵니다. 이때 U와  ΣtV T의 내적으로 보고 각각을 하나의 층으로 보면 하나의 완전연결층을 두개의 완전연결층으로 볼 수 있습니다. (참고로, ΣtV T 는 편향이 없는 가중 행렬, U는 W와 연관된 편향을 가진 층입니다.)​이러한 일련의 방법으로 논문의 저자는 모델의 속도를 향상시켰습니다. 이런방법이라는게 W를 줄이는 방법이 있는건 아니기때문에 이를 분해하고 분해하고나면 각 행렬을 조금씩 줄여나갈 수 있으므로 결론적으로 연산량을 줄일 수 있는방식을 사용했다는 얘기입니다.​ Main results[performance] Fast R-CNN모델은 VOC12에서 최고의 mAP를 기록했으면서도 2배 빨랐습니다. VOC10에서는  segmentation annotations을 훈련에 추가한 SegDeepM이 좋은 성능을 냈으나 큰 데이터셋을 사용하여 훈현한 모델은 SegDeepM을 앞질렀습니다. VOC2007에서는 다른 경우처럼 모델에 대한 정보가 당시에 없는 경우와 달리 모델들이 비슷한 방식을 썼기 때문에 모델간의 일부 차이에 대한 비교가 가능했는데, 이를 통해 conv layers가 mAP의 향상에 큰 기여를 한다는 점을 확인할 수 있습니다.​[speed] 표만 보더라도 굉장히 Fast R-CNN이 빠른 것을 볼 수 있습니다. 또 Fast R-CNN은 수백기가 수준의 디스크 소모도 줄여 확실히 발전된 모델임을 여러 지표들을 비교하더라도 확신할 수 있습니다.with SVD라고 되어있는 행이 있는데 truncated SVD를 이용한 것입니다. fc6에서는 25088 × 4096 matrix에서 상위 1024개 값, fc7에서는 4096×4096 matrix에서 256개의 값을 이용했는데 표에서 볼 수 있듯이 엄청난 속도 향상을 이루었습니다.​[tuning]모든 층을 전부 학습시킬 수 있는 것은 Fast R-CNN의 강점이었습니다. SPPnet은 fc층만 fine-tuning하기때문에 convolutional layer는 학습이 따로 가능하지 않았습니다. 논문의 저자는 합성곱층 학습을 동결시키고(가중치 업데이트를 멈추고) 학습한 경우 mAP가  66.9% 에서 61.4%로 감소하는 것을 실험을 통해 밝혀내 RoI pooling layer층의 학습이 중요하다는 사실을 증명했습니다.(일부층이 학습이 안되는 경우 전체적인  모델의 성능이 떨어짐을 발견했기때문에 RoI pooling layer층도 학습이 필요할 것임을 알 수 있습니다.)다만 모든 층을 다 학습시킬 필요는 없습니다. 미세한 성능 향상을 위해 학습이 과도하게 느려지는 경우가 발생하거나 컴퓨팅 자원의 수준을 넘기기도 합니다. 그래서 실제로 Fast-RCNN은 VGG16의 conv3_1이상의 레이어만 투닝한걸 사용했다고 합니다.​ Design evaluation​[Multi-task] 위의 표는  간략하게 말하면 클래스분류와 박스 영역 그리기를 동시에 한 경우 가장 성능이 좋다는 결과를 보여주는 것입니다. 서로 영향을 주는 경우가 있기 때문에 Fast-RCNN의 경우 학습을 클래스만 분류하기보단 박스 회귀를 하는것이 좋고, 이 두가지 과제를 하나씩 수행하기보단(staeg-wise) 동시에 하면 더 성능이 향상되는 결과가 나타납니다.​[Scale invariance] 위에서 크기 불변을 위해 이미지를 조정하거나 피라미트 크기를 랜덤하게 사용하여 훈련하는 방법을 소개했습니다. 그런데 이 결과는 그렇게 하지 않아도 어느정도 이미 크기에 대해 강건한 성질을 가지고 있다는 것을 보여줍니다.표를 보면 알수있는데 크게 차이가 없습니다. ​Conclusion이 논문은 R-CNN이나 SPPnet보다 깔끔하고 좋은 성능을 이끌어냈습니다. 뿐만아니라 과거에는 희소 객체 제안이 탐지기의 성능을 높이지만 이것은 시간적인 비용이 높아 과거에는 조사하기 어려웠으나 Fast R-CNN으로 실용적이 되었습니다.​보통 모델의 성능(이 경우는 mAP)과 시간 및 컴퓨팅 자원의 소모에서 둘 다 향상시키기는 어렵습니다. 자연어 처리에서도 transformer구조가 압도적인 성능을 내지만 모델 학습이나 모델 자체도 자원소모가 심해 lstm을 선택해야할 때도 있습니다. 그러나 Fast R-CNN은 그 이전의 모델과 이 모델중에 뭘 선택할지 고민하지 않아도 될 것같습니다. CNN연산을 공유하는 구조적인 개선뿐만아니라 truncated SVD로 자원소모를 아낄 수 있도록 세부적인 조정도 했습니다. 심지어 전체 모델을 학습할 수 있지만 훈련과정에서 중요한 층만 학습하는 방법을 제시하면서 훈련과정도 조정하여 훨씬 좋은 모델을 만들어 내었습니다. R-CNN때보다 논문에 기본적인 구조뿐 아니라 다양한 방법론이 많이 담겨있어서 지금 작성하는 글도 R-CNN보다 더 길어진 거 같습니다.​감사합니다.  [세부사항 / 용어 설명]논문을 읽으면서 필요한 배경 지식을 따로 정리하였습니다. 특잇값 분해, bounding box regression등 논문에서 적용한 다양한 방법들입니다.Fast R-CNN 모델(Object dete.. : 네이버블로그 (naver.com) "
논문 소개:End-to-End Object Detection with Transformer ,https://blog.naver.com/qwopqwop200/222095303793,20201202,"End-to-End Object Detection with Transformers요약:객체 탐지를 위해 기존의 복잡한 사전지식과 복잡한 기법을 사용하지 않고 매우 간단하고 성능면에서 충분히 경쟁력있는 새로운 객체 탐지 모델을 제안함https://arxiv.org/pdf/2005.12872.pdf  객체 탐지는 원래부터 상당히 어려운 분야 입니다. 일단 문제가 복잡한데 그것을 예측하는 모델 마저도 NMS라는 후처리 기법이 필요했습니다.그렇기에 많은 라이브러리를 필요로 했습니다. 하지만 이 모델은 기존의 모델과는 다르게 그저 간단한 라이브러리 ('pytorch,텐서플로우')만으로도 쉽게 구현할수 있게 되었으며 conv층과 dense 층으로 쉽게 구현 할수 있게 되었습니다.DETR의 아키텍쳐를 보면 쉽게 이해 할수 있습니다. 저기서 문제가 되는것은 backbone이지만 이것은 기존에 아는 그 resnet을 사용했습니다.다음은 손실이지만 이곳에서는 중복문제을 해결하기 위해 이분 매칭 손실을 사용했습니다. 이에 대해서는 논문을 참고 하시기 바람니다.  결과기존의 faster rcnn과 비슷한 성능을 내면서도 충분한 경쟁력을 보여 주었다는 것을 증명했습니다.하지만 이모델은 큰 물체에 대해서는 기존의 목표모델보다 좋은 성능을 보여주었습니다.하지만 작은 물체에 대해서는그리좋지 못한 성능을 보여주었습니다.  나의 생각:이모델은 생각보다 굉장히 흥미롭게 읽은 논문중 하나 입니다. 일단 저는 yolov4를 구현 해보고 싶었지만 구현난이도의 어려움때문에 실제로 구현을 할려다가 실패하는 경우가 빈번했습니다. 하지만 이논문을 읽고 이 논문 정도는 구현할수 있게다 할정도로기존보다 쉽게 해주었습니다. 저는 이논문의 의의는 기존의 모델보다 압도적인 구현의 간편성이라고 합니다.즉 어렵게 구현할 필요가없다는것에 느껴집니다.이 논문의 중간 말을 인용하면 We hope that the simplicity of our method will attract new researchers to the detection community.​저 또한 이 간단한 구조에 이끌린 사람중 한명이 된것 같습니다.  일단 저는 인공지능 분야를 공부한지 얼마 되지 않았습니다. 그렇기에 제가 놓친 부분이 많을 것입니다.그렇기에 흥미가 있으면 논문을 읽으시길 추천합니다. "
Feature Pyramid Networks for Object Detection - 논문리뷰 ,https://blog.naver.com/bshlab671/222941581106,20221129,"진짜 고려대 논문리뷰는 없는게 없네요 Feature Pyramid구조는 이전에도 다양한 scale로 물체 감지를 위해 사용한 기본 요소였습니다. 그리고 Feature Pyramid구조는 scale별로 추론하기 때문에 물체의 크기가 변해도 같은 물체로 인식할 수 있어 scale-invariant 특징을 가지고 있습니다. (b)는 FPN이 없는 방법이며 단일 scale만 쓰는 원래의 방법입니다.(a)는 각 피라미드 featuremap에서 특징을 찾아내서 추론하는 방법으로 시간이 4배가량 더 걸립니다. 추론시간의 증가뿐만 아니라 메모리 측면에서 부족현상이 나타날 가능성이 크기 때문에 테스트 시간에만 사용한다고 합니다. Fast R-CNN, Faster R-CNN은 기본적으로 이 옵션을 사용하지 않는다고 합니다.(c)의 경우는 SSD에서 쓰인 방법입니다.(나중에 SSD논문을 읽으면서 자세히 봐야겠습니다.)(d)는 FPN의 구조입니다. 논문에서 FPN의 목표는 모든 층에 시멘틱한 정보가 모두 전달되게 하는 것입니다.​Related Work는 이전에 사용되었던 Hand-engineering과 multiple layer 에 대해 말하는 부분이라 넘어가겠습니다. FPNFPN은 feature hierarchy를 전달하며 이 구조는 백본에 대해서 독립적이기 때문에 Semantic, Panoptic segmentation에도 범용적으로 쓰일 수 있습니다. FPN실험에서 백본은 ResNet을 사용합니다. 각 stage에서 마지막 Feature map을 가져와 Residual block으로 사용하는데 여기서 Stage란 pooling이 일어나지 않는 Feature map의 크기가 동일한 층들을 말합니다. 가장 첫번째 stage는 메모리를 너무 많이 차지하여 제외했다고 합니다. 이제 백본에서 뽑아낸 Feature map을 합쳐야 하는데 크기와 채널이 다 다릅니다. 깊은 층에서 뽑은 작은 Feature map의 경우는 nearest neighbor upsampling을 사용하고(단순함을 위해 = 시간 단축) 백본에서 가져오는 feature map은 1x1 conv계층을 통해 채널을 줄여줍니다.​마지막으로 각 feature map에 엘리어싱 효과를 줄여주기 위해 3x3 conv filter를 거쳐서 최종적인 feature map을 생성합니다. (논문을 읽을 수록 생각보다 신호처리 관련 내용이 딥러닝에 많이 내제 되어 있는 것 같네요) Aliasing - WikipediaAliasing From Wikipedia, the free encyclopedia This article is about aliasing in signal processing, including computer graphics. For aliasing in computer programming, see Aliasing (computing) . In signal processing and related disciplines, aliasing is an effect that causes different signals to becom...en.wikipedia.org 논문에서는 이 추가 conv계층에 대해 더 깊은 층으로 구성을 할 수록 성능이 더 좋아졌다고 얘기합니다만 초점에서 벗어나므로 간단한 3x3 filter로 실험했다고 합니다.​이제 이 단계를 마치면 각 층을 내려가면서 top-down으로 element-wise하게 더해줍니다. 이는 resnet과 densenet의 아이디어와 유사합니다.(bottom-up에서 나온 feature를 가져와 더하는 것을 lateral connection이라고 부릅니다)​RPN is a sliding-window class-agnostic object detector 이부분은 Faster R CNN을 읽었는데도 해석이 잘 안되네요. 슬라이딩 윈도우이면서 클래스에 무관한? 객체탐지라고 해석되는데 맞는지 잘 모르겠습니다...​아래 그림 출처는 아래 링크에 있는 것 같은데 찾지를 못하겠네요 GitHub - facebookresearch/detectron2: Detectron2 is a platform for object detection, segmentation and other visual recognition tasks.Detectron2 is a platform for object detection, segmentation and other visual recognition tasks. - GitHub - facebookresearch/detectron2: Detectron2 is a platform for object detection, segmentation a...github.com 어쨌든 다음으로 넘어가기 전에 이 그림을 한번 보고 가면 좋을 것 같아서 가져왔습니다. FPN은 P2, P3, P4, P5, P6에서 종횡비만 1:1, 1:2, 2:1로 다르고 각 level의 stage에서 single scale의 achorbox를 사용한다고 합니다.그리고 conv filter가 보는 영역은 322, 642, 1282, 2562, 5122로 조절했다고 합니다. 결국 피라미드 네트워크 전체에서 앵커는 15개입니다.​Note that scales of ground-truth boxes are not explicitly used to assign them to the levels of the pyramid; instead, ground-truth boxes are associated with anchors, which have been assigned to pyramid levels. As such, we introduce no extra rules in addition to those in Faster R-CNN. 역시 영어로도 해석이 안되면 한국어 번역기 돌려도 무슨말인지 모르겠습니다. 이 부분은 나중에 찾아봐야겠습니다.-추가-위에 이해를 못했던 부분에서 얘기하는 말이 GT의 스케일이 어떤 level에 적용될지 결정하는 것이 아니라 각 level의 앵커와 GT의 IOU관계가 학습에 사용할지 결정하는 것이라는 말 같습니다.​저자는 FPN의 각 나눠진 피라미드에서 ROIHead를 거칠 때 파라미터를 공유하는 것과 공유하지 않는 것을 실험했는데 유사한 정확도가 나왔다고 했습니다. 이를 모든 수준에서 의미론적 정보가 잘 전달되었다고 해석을 합니다.(제 생각인데 보통 성능이 조금 좋거나 유사한 경우 real-time을 위해서 파라미터를 덜 쓰는 방향으로 가는 것 같습니다. 그래서 논문에서도 파라미터 공유를 택한 것 같습니다.)​Fast R-CNN은 RPN이 없어서 각 stage의 GT와 그 stage에서 나온 Feature map을 비교해야 하는데 이를 위해 아래와 같은 식을 사용합니다. 여기서 구한 k로 각 층에 맞는 feature map의 GT와 비교해주면 됩니다. 그런데 backbone을 교체하면 매번 새로 계산해줘야 해서 귀찮아 보이네요...​이제 나머지는 실험 내용입니다. 실험은 간단하게 어떤것이 있는지만 적어보겠습니다.How important is top-down enrichment?How important are lateral connections?How imporatn are pyramid representations?about share feature...​간단 요약 결론 : FPN은 깔끔하고 간단한 구조이며 추가 계산 비용이 적게 든다.​더 개선된 모델 성능을 원한다면 여기서 더 발전된 PAFPN, NASFPN, biFPN등을 읽어보면 될 것 같습니다.​논문 링크 : https://arxiv.org/pdf/1612.03144.pdf​참고 링크 : Understanding Feature Pyramid Networks for object detection (FPN)Detecting objects in different scales is challenging in particular for small objects. We can use a pyramid of the same image at different…jonathan-hui.medium.com ​ "
Object Detection - 템플릿 매칭 (1) ,https://blog.naver.com/dongju0531hb/222441630785,20210723,"#시각지능 #객체검출 #AI #비전인식 #인공지능​0. Object Detection이란?객체를 검출하다. 어떤 물체를 찾아내는 것을 뜻한다.​​  ​1. template이란?템플릿은 흔히 파워포인트를 하다 보면 듣는 말이다. 파워포인트에서 템플릿은 하나의 예시 또는 형식이 갖춰진 슬라이드다. template은 형판, 견본, 본보기란 뜻이다.네이버 영어사전그럼 Object Detection에서 template을 매칭한다는 말을 무엇일까?이미지나 영상에서 내가 찾고 싶은 template이 있는 위치를 찾는 것을 말한다. Object Detection에서 template은 찾고자 하는 대상이 되는 작은 크기의 영상을 의미한다.전체 이미지template​​​  2. 어떻게 전체 이미지에서 작은 template이 있는 위치를 알 수 있을까?자 컴퓨터에 입장에서는 지금 이 상황이다.인간의 눈은 저게 저 위치에 있는 걸 바로 알지만, 컴퓨터는 미로 속에 갇혀 있어서 이리저리 움직이다가 template 위치를 찾아야한다. 출처 : unplash그렇기 때문에 template matching에서는 template을 왼쪽에서 오른쪽으로, 위에서 아래로 로봇청소기가 청소하듯 움직이면서 각 부분에서 유사도를 저장하고, 가장 유사도가 큰 위치가 template이 있는 위치가 되겠다. 출처 : gitbut위에서 그림(a)에서 template위치를 옮겨가면서 전체를 다 훑고 보니, (b)는 각 부분의 유사도 값을 어두움에서 밝음으로 표시한 것이다. (b)에서 중앙에 휜부분이 가장 유사도가 크다. (c)에서 결과를 보면 딱 저위치에 template이 존재한다는 것이다.​​​​​  3. 그럼 유사도는 어떻게 계산할까?자 우리가 꽃 이미지를 정말 자세히 보면 오른쪽처럼 정사각형 pixel이 자세히 보일 것이다. 자 pixel의 어떤부분은 밝고, 어떤 부분은 어둡다. 그러면 pixel이 밝을 때는 value(값)이 클것이고, 어두울 때는 작을 것이다 ㅎㅎ 비트맵 이미지 확대하면 pixel이 보임!자 그러면, 두 이미지가 같은지를 비교할려면 이미지를 이루는 pixel 하나하나의 밝기(밝고 어두운 정도)가 두 이미지에서 똑같으면 같다고 할 수 있겠지?자 그럼 a=b 가 될려면 a-b =0 이겠다. 즉 이것은 두 이미지를 이루고 있는 각각의 픽셀 값들을 비교하면서a 이미지의 x,y 좌표에 픽셀값과b 이미지의 x,y좌표에 픽셀 값을 a-b를 했을 때 그 차가 작을수록 둘이 비슷하겠지?이는 그럼 a,b이미지에서 전체 x,y 좌표값에 대해서 픽셀값을 똑같이 비교해 가면서 각각의 좌표에서 차이 값을 다 합쳤을 때제일 작으면 제일 유사도가 큰, 다시말해 비슷한 이미지니깐, template matching이 되는 것이다.​  이런 원리로 수식은 여러가지가 있다. 참고 : https://docs.opencv.org/3.4/de/da9/tutorial_template_matching.html​1. template 이미지 픽셀 x,y에서 gray scale(밝기 값)은 T(x,y), 전체 이미지는 I(x,y) 값의 차이의 제곱을 한 후 각 위치에 대해 다 더한 형태는 TM_SQDIFF​2. TM_SQDIFF와 분자는 같지만, 분모에 각각의 값을 정규화해준 값이다. 정규화 하는 이유는 만약 차의 제곱 합이 같다고 했을 때, 만약 2과 1의 차이와 100과 99의 차이는 다를 것이다. 아마도 100과 99의 차가 더 무의미하다. 1~10의 값으로 이루어진 이미지에서 차이 1의 값은 매우 큰 특징을 가질 수 있지만, 100~110의 이미지에서 차이 1의 값은 매우 무의미한 특징을 가지는 것이다.​1과 2의 경우는 값이 작을수록 차이가 작아서 가장 작은 값의 x,y 위치가 template이 현재 있는 위치이다.​3. TM_CCORR의 경우에는 상관관계인데, 상관관계는 값이 클수록 두 이미지가 관계가 있다는 의미이고 같은 이미지라는 뜻이다.4. TM_CCORR_NORMED의 정규화된 버전이다.​5. TM_CCOEFF는 상관계수를 구하는 공식이다. template과 전체 이미지의 x,y 위치에서 픽셀 값에 각각의 평균 값을 빼주어 baseline을 맞추고 그렇기 때문에 평균보다 작으면 음수값을 띈다. 근데 T'(x,y)와 I'(x,y)가 각각 양수*양수 or 음수 * 음수는 양수로 바뀌고, 만약에 전혀 다른 양수 * 음수 이런 경우에는 확 상관계수가 커지는 것을 확인할 수 있어, 상관계수 공식의 값을 사용하면 상관관계 식 값보다 전혀 다른이미지일때 값이 훨씬 작고, 같은 이미지 일때는 눈에 띄게 크기 때문에 더욱 잘 구별할 수 있다.​따라서 template mathcing에서는 TM_CCOEFF_NORMED  정규화된 상관계수 매칭방법이 잘 사용된다.​​​​  4. PseudoCode (Low level) 템플릿 이미지에서 x',y' 위치의 pixel 값 T(x',y')전체 이미지에서는 x, y위치를 중심으로 template의 상대적 위치 x',y'만큼 이동한 위치의 pixel 값 I(x+x',y+y') x,y 값은 이렇게 증가시켜 줄 것이다.​ TM_CCOEFF_NORMED 열거형의 경우 수식for 반복문 : x, y 값을 이렇게 증가시켜줄 것이다 {  // 위에 그림 참조   for 반복문 : x', y' 값을 증가 시켜준다 {       위의 식대로 합계 계산   }  R(x,y) 계산 완료 x,y 위치의 배열에 해당 값 저장}이렇게 모든 x,y 값에 저장되면 R(x,y) 값이 가장 큰 x,y 좌표 쌍을 알아낸다. 즉 상관계수가 값이 가장 큰 위치​바로 여기가 template이 있는 위치 (x,y)!!!​ ​​​​  5. OpenCV를 사용한 C++로 작성한 Code (High level)1) 라이브러리 설정 #include ""opencv2/opencv.hpp""#include <iostream>using namespace cv;using namespace std; 1. opencv 라이브러리 불러오기2. C++ 사용할거임3,4. cv::, std:: 이거 매번 안 붙힐려고​​2) 이미지 불러오기img는 큰 이미지templ은 작은 template 이미지, 찾고자 하는 대상 img 이미지templ 이미지int main(){    Mat img = imread(""ch13/template/circuit.bmp"", IMREAD_COLOR);	Mat templ = imread(""ch13/template/TEST.png"", IMREAD_COLOR);	namedWindow(""templ"", WINDOW_NORMAL);	imshow(""templ"", templ);	if (img.empty() || templ.empty()) {		cerr << ""Image load failed!"" << endl;		return;	} ​3) 이미지 살짝 다르게 변화주기 (그냥 test용도로 noise 추가하는 것. 똑같은 이미지에서 찾으면 재미없으니깐^^)     img = img + Scalar(50, 50, 50); // image에 50,50,50씩 더하기 밝기 증가	Mat noise(img.size(), CV_32SC3); // img 크기에 noise를 만든다. 4byte short 자료형, Channel은 3채널	randn(noise, 0, 10);			 // noise에 0~10의 임의의 값 대입	add(img, noise, img, Mat(), CV_8UC3); // img에 noise를 더하고, 다시 img로 img + noise = img mask는 빈배열 ​4) matchTemplate( ) 함수 사용해서 R(x,y) 계산해서 res에 저장     Mat res, res_norm; // res, res_norm 마련	matchTemplate(img, templ, res, TM_CCOEFF_NORMED); //  img에서 templ을 찾고 res에 저장 이때 TM_CCOEFF_NORMED 수식 사용	normalize(res, res_norm, 0, 255, NORM_MINMAX, CV_8U); 	// res를 res_norm으로 noramlize 시킨다 0~255 값으로 정규화하고 NORM_MINMAX 현재 res 행렬을 이미지로 볼 수 없다. 왜냐면 R(x,y)는 (-) 값도 있는데 -100은 밝기는 존재하지 않는다. 0~255만 존재!그래서 scale 범위를 0~255범위로 바꿔져서 res_norm으로 보여주면 아래처럼 보여지고, 제일 밝은 점이 바로 template이 있는 위치!! res를 normalize해서 res_norm으로 보게 되면​5) res는 x,y 위치에 따라 R(x,y) 값을 가지고 있는 행렬. 여기서 minMaxLoc 함수로 최대값과 그 위치를 찾아보자!     double maxv; // MAX 값 변수 설정	Point maxloc; // 맥스 값 point 담을 변수	minMaxLoc(res, 0, &maxv, 0, &maxloc); // res에서 가장 큰 값을 알아내고, maxv에 가장 큰 값 저장, maxloc에는 위치값 저장	cout << ""maxv: "" << maxv << endl; ​6) rectangle( ) object 만들어서 maxloc 위치를 중심으로 template의 크기 만큼 직사각형을 그리자.     // maxloc와 maxloc x,y값을 기점으로 templ.cols와 templ.rows 크기만큼, 빨간색으로 테두리 색칠 2두꼐로	rectangle(img, Rect(maxloc.x, maxloc.y, templ.cols, templ.rows), Scalar(0, 0, 255), 2);	//imshow(""templ"", templ);	imshow(""res_norm"", res_norm);	imshow(""img"", img);	waitKey(0);	destroyAllWindows(); template 찾기 성공!​​  자세한 함수 사용법에 대해 질문이 있으면 댓글로 남겨주세요!!OpenCV4로 배우는 컴퓨터 비전과 머신 러닝​ "
"You Only Look Once: Unified, Real-Time Object Detection ",https://blog.naver.com/younho9468/222304194003,20210409,"Introduction 1단계로 물체를 예측하는 YOLO저자는 맨 처음에 사람은 엄청 빠르게 정확하게 인지를 하기 때문에 운전과 같은 매우 복잡한 작업도 가능하다고 설명하였다. 하지만 현재까지 발표된 object detection 모델들은 매우 느린 알고리즘을 가지고 있다고 설명한다. 우선 처음으로 외부 모듈을 통하여 region proposal을 만들어내고 classifier에 넣어 proposal이 어떤 object인지 분류한다. 그 뒤 bounding box를 약간 수정하고 중복되는 bounding box를 없앤 뒤, 다시 rescoring하는 과정을 거친다. 저자는 이러한 복잡한 과정이 object detection을 느리게 한다고 주장하였으며 전체 과정을 하나의 regression 문제로 대체하였다. 위 그림을 보면 하나의 convolution network가 여러 bounding box와 그에 해당하는 class의 확률을 예측하는 것을 볼 수 있다. ​YOLO의 장점을 총 3개가 존재한다 한다.   1. 매우 빠르다. (기본 버전은 1초에 45프레임, 빠른 버전은 1초에 150 프레임까지 처리가 가능하다 한다.)   2. Global context를 고려한다. (기존에는 sliding window 방식으로 처리하여 고려하지 않는다 한다)   3. 물체의 일반화된 representation을 배워 outlier에 대해 robust하다.​Unified Detection YOLO의 unified detection processing 과정YOLO 에서는 input image를 S x S grid로 나누어 각 grid는 class 및 bounding box를 예측하도록 구성하였다. 각 grid는 bounding box와 confidence score을 예측하도록 구성하였고, confidence는 Pr(object) * IoU 가 되도록 구성하였다. Pr(object)는 bounding box에 object가 존재할 확률을 뜻하고 모델의 confidence score output을 뜻한다. 그래서 만약 GT와 predicted bounding box가 겹치지 않으면 0으로 설정되고, 겹치면 겹친 부위만큼 GT가 된다.​그 뒤에 class 마다 확률을 뽑도록 설정하였다. class의 확률은 일단 첫번째로 해당 grid cell에 object가 있는지 없는지 판단하고 있으면 class probability를 가지게 되고, 없으면 가지지 않게 된다. Conditional class prediction지금까지를 종합해보자면 image를 S x S 로 나누고, 각 grid cell 마다 B만큼의 bounding box를 만들어 내며, 각 Bounding box는 x,y,w,h 와  confidence score, 각 class의 확률에 대한 정보를 가지고 있다. 여기서 S = 7, B = 2 이고 VOC 데이터셋은 20개의 클래스를 가지고 있으므로, 모델의 predication은 7 x 7 x ((4 + 1) * 2 + 20) 이다.​Network Design YOLO network 구조.YOLO의 network는 GoogleNet에 영향을 많이 받았다고 한다. GoogleNet에서 사용되었던 inception module은 1 x 1과 3 x 3 convolution으로 대체되었다고 한다(NIN). 24개의 convolution network을 일반적으로 사용하였으며, Fast YOLO를 구현할 때에는 9개의 layer를 사용했다 한다.​Training YOLO loss function(좌) 루트 그래프(우) 위에서 보여준 network에서 뒤에 4개의 convolution을 제외하고 나머지 20개의 convolution과 GAP 및 FC를 적용하여 ImageNet을 훈련시킨 후 학습된 checkpoint를 이용하여 20개의 convolution을 그대로 load 시키고, 그 뒤에 4개의 covolution과 2개의 Fully connected layer를 붙여 random으로 초기화하고 input size를 2배로 늘인 후 학습을 진행하였다. ​마지막 fully-connected-layer에서는 leaky RELU를 사용하였다. loss함수로는 sum-squared error을 사용했다. bounding box의 coordinate, classification, confidence에 대한 error를 각자 내어 weight를 취한 후 더하였다. 저자는 만약 이렇게 weight를 조정하지 않으면 어떤 한 Error가 다른 Error에 묻이기 때문에 이렇게 조정하였다고 주장하고 있다. 보통 object detection 데이터셋은 background가 많이 존재하기 때문에 object가 있는 부분에서는 5의 weight를 주고 없는 부분은 0.5의 weight를 준다. 또한 coordinate의 loss를 추출할 때 x,y값은 그대로 값을 비교하지만 w 하고 h같은 경우에는 그대로 비교하게 되면 큰 box에 loss함수가 영향을 너무 많이 받게 되므로 루트를 씌웠다. 위의 그래프를 보면 작은 값일 때에는 급격하게 증가하고 1에 가까워지면 질수록 원만한 곡선이기 때문에 small box는 좀 더 강조하고 big box는 좀 덜 강조하는 것을 알 수 있다.​1objij를 통해 해당 bounding box에 객체가 있는 경우를 뜻하고, 1noobjij 같은 경우에는 bounding box에 객체가 없는 경우를 뜻한다.이 내용 뒤에는 구현을 어떻게 했는지 어떤 하이퍼파라미터를 사용했는지에 대한 내용이라 생략하겠다.​InferencePascal VOC에서 실험을 하였고, 하나의 이미지마다 7 x 7 x 2 = 98 개의 bounding box를 만들어 내었다. 그 뒤에 confidence score와 class score을 곱하여 가장 높은 확률을 가진 순서대로 bounding box를 정렬하였고 차례 차례 NMS를 적용하여 bounding box를 정제하였다. 해당 내용에 대한 자세한 내용은 다음 링크를 보면 된다.​Limitations of YOLOYOLO는 각 영역별로 하나의 class만 할당이 되므로 각 영역 안에 작은 object가 여럿 있는 경우에는 성능하락이 필연적으로 발생하게 된다. 또한 일반적이지 않은 aspect ratio를 가진 object같은 경우에는 잘 못맞추는 단점을 가지고 있고, downsampling을 함으로써 다른 모델에 비해 비교적 coarse한 feature을 사용한다. YOLO loss function에서 큰 bounding box와 작은 bounding box를 비슷하게 처리하기 때문에 발생하는 문제 또한 존재한다.​Experiments Real Time system benchmark on PASCAL VOC 2007Pascal VOC 2007과 VOC 2012에서 다른 object detection model과 비교를 진행하였다. 다른 real time model과 비교를 하였으며 다른 모델보다 훨씬 더 빠르고 2배가량 더 좋은 성능을 보였다. VGG16 을 backbone으로 사용한 YOLO도 실험하였지만 실시간까지는 아니였다.​ Error Analysis저자는 Fast RCNN, YOLO에서 틀린 image를 분석하였다. Correct는 classification을 잘하고 IOU가 0.5 이상인 경우, localization은 class는 맞췄지만 IOU가 0.1 ~ 0.5 사이 인경우, similar은 class가 비슷하고, IOU가 0.1 이상인 경우 Other은 class가 틀렸고 IOU가 0.1 이상인 경우 Background는 IoU가 0.1 이하인 경우이다.​ Object detection model benchmark in PASCAL VOC 2012VOC 2012 test set에서도 실험을 진행하였으며 상당히 좋은 결과를 낸 것으로 보인다.​ 위의 그래프 및 표에서 볼 수 있듯이 Picasso Dataset 및 People-Art dataset에서도 다른 detection model보다 더 좋은 결과를 내어 맨 처음에 YOLO 장점에서 generalization이 잘 되었다는 말을 한것 같다.​Conclusion일반적으로 two-stage로 진행하는 object detection task에서 처음으로 통합된 object detection model을 내놓아 모델의 속도를 굉장히 빠르게 하여 실시간으로 프로세싱 되도록 구성하였으며 다른 real-time model보다 2배 더 좋은 성능을 내었다. 또한 다른 도메인에서도 좋은 성능을 보임으로써 새로운 도메인에 robust한 model을 만들었다. "
"Image Ai를 통한, Object Detection ",https://blog.naver.com/maro1992/221537049698,20190514," 쥐새끼랑, 고닉은 인식을 못한다. 인식 한다면 그게 미친거겠지만;;;​​​Image Ai를 통한 물체 인식인데, 미리 학습된 데이터 셋 중에 골라서 인식한다.그런데 환경 셋팅이 매우, 졸라 힘들다... ​python, CUDA, CuDnn, tensorflow-gpu 셋팅 이후에https://jsh93.tistory.com/55 ImageAI를 활용한 Object Detection개발 환경 운영체제 : 윈도우 10 Edu 1. 사용 전 종속성 설치 출처 : https://github.com/OlafenwaMoses/ImageAI 사용할 언어는 Python3 이며 설치가 되지 않은 경우 설치해야 합니다. 2018/06/25 - [Programming/..jsh93.tistory.com 여기 참조해서 관련 lib들을 전부 설치 ​또한 해당 링크에 있는,pre-train 데이터를 받아서 파이썬 파일과 함께 둬야 정상 작동한다.​​​ex)​( 내가 찍은 사진이 없어서 구글에 있는 아무사진 활용... 초상권 때문에 모자이크 한거지 다른 이유는 없습니다. )   ​​그냥 돌리는 경우   매우 잘~ 찾는다. ​ from imageai.Detection import ObjectDetectionimport osexecution_path = os.getcwd()detector = ObjectDetection()'''yolo.h5 사용할 경우'''detector.setModelTypeAsYOLOv3()detector.setModelPath( os.path.join(execution_path , ""yolo.h5""))detector.loadModel()detections = detector.detectObjectsFromImage(input_image=os.path.join(execution_path , ""test.jpg""),                                             output_image_path=os.path.join(execution_path , ""test_con.jpg""),                                             minimum_percentage_probability=30)for eachObject in detections:    print(eachObject[""name""] , "" : "", eachObject[""percentage_probability""], "" : "", eachObject[""box_points""] ) 설명은 위의 링크에 있음​​특정 대상 지정   ​​위 링크에 관련 내용이 없어 찾아보니, 해당 Image ai 메서드 중에 관련 내용이 있었다.   ​​ from imageai.Detection import ObjectDetectionimport osexecution_path = os.getcwd()detector = ObjectDetection()custom = detector.CustomObjects(person=True)'''yolo.h5 사용할 경우'''detector.setModelTypeAsYOLOv3()detector.setModelPath( os.path.join(execution_path , ""yolo.h5""))detector.loadModel()detections = detector.detectCustomObjectsFromImage(custom_objects=custom,                                                   input_image=os.path.join(execution_path, ""test.jpg""),                                             output_image_path=os.path.join(execution_path, ""test_new.jpg""), minimum_percentage_probability=30)for eachObject in detections:    print(eachObject[""name""] , "" : "", eachObject[""percentage_probability""], "" : "", eachObject[""box_points""] ) custom의 추가와 detector.detectCustomObjectsFromImage의 사용만으로 인식대상 선택이 가능해졌다.​https://imageai.readthedocs.io/en/latest/detection/ Detection Classes — ImageAI 2.0.2 documentationImageAI latest Contents: Prediction Classes Detection Classes Video and Live-Feed Detection and Analysis Custom Training and Prediction Classes Docs » Detection Classes Edit on GitHub Detection Classes ImageAI provided very powerful yet easy to use classes and functions to perform ** Image Object De...imageai.readthedocs.io image Ai 클래스 관련 사용 설명이 매우 잘 되어있다. 그냥 찾아쓰면 될 정도로;​​​결국 망할 환경 셋팅만 잘 끝낸다면원리를 모르는 나같은 늅늅도 객체 인식이 가능하다 ㄷㄷㄷ;;; "
객체 탐지 Object Detection ,https://blog.naver.com/st0421/222846608540,20220812,"객체 탐지(Object Detection) 기술 - CLIPSOFT작성자 : 조지훈 차장   객체 탐지(Object Detection) 기술   1. 들어가며 신분증, 도장 인식 관련 기술들을 정리 및 공유하려 합니다. 객체 인식(Object Recognition)을 하기 위해서는 객체가 있고, 그것이 무엇인가를 찾는 문제이기 때문에, 객체 감지(Object Detection)가 선행되어야 합니다. 그래서 이번 글은 객체 감지(Object Detection)에 대한 내용을 소개합니다.   2. Object Detection 이란 컴퓨터 비전과 이미지 …clipsoft.co.kr ​Grayscale vs BinarizationGrayscale - 흑백 사진처럼. 색 정보는 없지만 밝기 정보는 가짐.Binarization - 완전한 흙과 백으로만 표현​​특징점(feature point) 검출  - 일반적인 object detection은 찾고자하는 특징을 사전에 추출하고 해당 특징을 detection하는 접근 사용 - 관련 algorithmCanny Edge Detector : 대표적인 detector. 낮은 error, 좋은 결과, 최소 응답Harris corner Detector : 모서리 감지 연산Harr-like feature : 명암으로 패턴 구함.HOG(Histogram of Oriented Gradient) : 일정 크기 셀로 분할, 셀마다 edge픽셀 방향에 대한 히스토그램을 구한 후 bin 값들을 일렬로 연결,  변화가 심하지 않은 형태, 윤곽선으로 식별 가능한 경우에 적합.SIFT(Scale Invariant Feature Transform) : 영상에서 식별이 용이한 특징(코너 점)을 선택하여 그것을 중심으로 한 local patch에 대해 특징 vector 추출​​ "
"Object Detection in 20 Years - 07, Application ",https://blog.naver.com/tory0405/222857584818,20220825,"[논문 원본] 첨부파일1905.05055.pdf파일 다운로드 지난 20년간 보행자 감지, 얼굴 감지, 텍스트 감지, 교통 표지/조명 감지 및 원격 감지를 포함하여 몇 가지 중요한 감지 응용 프로그램을 검토합니다. ​Pedestrian Detection 보행자 감지는 중요한 물체 감지 응용 프로그램으로 자율주행, 영상감지, 범죄수사 등 많은 영역에서 많은 관심을 받았다.   HOG  감지기,  ICF  감지기와  같은  초기  보행자  감지  방법은   feature 표현,  classifier 설계,   detection 가속 측면에서 일반 객체 탐지를 위한 견고한 토대를 마련했다. 최근에는 Faster RCNN 과 같은 일반적인 물체 감지 알고리즘이 보행자 감지에 도입되어 크게 발전시켰다. ​보행자 감지의 어려움에 대해 나열해 보면 Small pedestrian : 그림 20(a)는 카메라에서 멀리 캡처된 작은 보행자의 몇 가지 예를 보여준다. Caltech  Dataset에서 보행자의 15%는 높이가 30픽셀 미만이다. Hard negatives : 스트리트 뷰 이미지의 일부 배경은 그림 20(b)에 표시된 것처럼 시각적으로 보행자와 매우 유사하다. Dense and occluded pedestrian:그림 20(c)는 조밀하고 폐쇄된 보행자의 몇 가지 예를 보여 준다. Caltech  Dataset에서 차단되지 않은 보행자는 전체 보행자 인스턴스의 29%만 차지합니다.Real-time detection : HD 비디오 실시간 보행자 감지는 자율 주행 및 비디오 감시와 같은 일부 Application에서 매우 중요하다. 보행자 감지는 매우 긴 연구 역사를 가지고 있다. 개발은 전통적인 방식과 딥러닝 기반 보행자 감지의 두 가지 기술 기간으로 나눌 수 있다.  ​Haar 웨이 블릿 기능은 컴퓨팅 자원의 한계로 인해 초기 보행자 감지에 널리 사용되었다. 가려진 보행자의 감지 개선을 위해 당시 대중적인 아이디어 중 하나는 “detection by components”였다. 예를 들어 머리, 다리 및 팔, 컴퓨팅 파워가 증가함에 따라 사람들은 더 복잡한 탐지 모델을 설계하기 시작했고 2005년 이후 기울기 기반 표현과 DPM이 주류가 되었다. 2009년 통합 이미지 가속을 사용하여 효과적이고 가벼운 feature 표현으로 사용하는 Integral Channel Features (ICF)가 제안되었다. ICF는 그 당시 보행자 감지의 새로운 기준으로 되었다. feature 표현에 추가하여 외향 불변성 및 모양 대칭 및 스테레오 정보와 같은 일부 도메인 지식도 고려되었다. ​조밀하고 가려진 보행자 감지를 개선하기 위해 CNN에 깊은 Layer를 사용하는 것만으로는 해결을 할 수 없어 사람과 다른 주변 물체를 고려할 수 있는 별도의 loss function 설계가 필요하고  Target occlusion은 밀집된 보행자에게 생길 수 있는 또는 문제인데 이를 해결하기 위해 ensemble of part detectors 나   attention mechanism를 고려할 수 있다. ​Face DetectionFace Detection은 가장 오래된 컴퓨터 비전 응용 프로그램 중 하나이다. VJ 감지기와 같은 초기의 Face Detection은 오늘날 객체 감지에서도 여전히 중요한 역할을 하고 있다. 디지털카메라의 미소 감지,  전자 상거래에 이루어지는  “face swiping”, 모바일 앱의 facial makeup 등과 같이 face detection은 모든 삶의 영역에 적용되었다.  face detection의 어려움과 도전은 다음과 같이 요약할 수 있다. ​Intra-class variation : 얼굴은 그림 21(a)와 같이 다양한 표정, 피부색, 포즈 및 움직임이 존재한다. Occlusion: 그림 21(b)와 같이 다른 물체에 의해 부분적으로 가려질 수 있다. Multi-scale detection: 그림 21(c)와 같이 일부 작은 얼굴에 대해 다양한 스케일의 얼굴이 존재한다. Real-time detection: 모바일 장치에서 일반적으로 실시간 cpu 감지 속도가 필요하다. 얼굴 검출에 대한 연구는 1990년대 초반으로 올라간다. 그런 다음 2001년 이전 초기 얼굴 감지, 2001-2015년까지의 전통적인 얼굴 감지, 2015년 이후 현재까지 딥 러닝 기반 얼굴 감지와 같은 역사를 가지고 있다. ​2001년 이전의 초기 face detection 초기의 얼굴 감지는 1) Rule-based methods, 2) Subspace analysis-based methods 3) Learning based methods와 같이 크게 3그룹으로 나눌 수 있겠다.  ​Rule-based methods은 일반적인 얼굴을 구성하는 요소에 대한 인간의 지식을 인코딩하고 얼굴 요소 간의 관계를 챕처하는 방법이다. Subspace analysis-based methods은 기본 선형 부분 공간에서 면 분포를 분석하는 방법으로   Eigenfaces가 대표적인 방법이라고 할 수 있다. Learning based method은 얼굴 감지를 sliding window + binary classification (target vs background)로 진행한다.  신경망 및 SVM이 이 방식을 사용한다. ​2000~2015년 사이의 전통적인 얼굴 감지이 기간 동안  built based on boosted decision trees 기법과 초기 s convolutional neural networks 방식 2개의 얼굴 감지 기법이 사용되었는데 built based on boosted decision trees의 경우 계산은 쉬웠으나 일반적으로 복잡한 장면에서 낮은 정확도로 어려움이 많았다. ​2015년 이후 딥러닝 기반 얼굴 감지 딥러닝 시대에 대부분 얼굴 감지 알고리즘은 Faster RCNN 및 SSD와 같은 일반적인 객체 감지 방식을 따랐다. ​얼굴 감지 속도를 높이기 위해서  Cascaded detection 방식이 가장 일반적인 방법이었고 또 다른 방식으로는 얼굴의 scale distribution을 예측한 다음 일부 선택된 scale에서 감지를 실행하는 것이다. ​multi-pose와 가려짐에서 얼굴 감지 향상을 위해서 “face calibration”을 사용하였는데 이는 보정 매개 변수를 추정하거나 여러 감지 단계를 통한 점진적 보정을 사용하여 다중 포즈 얼굴을 감지하는 방식이다. 가려진 얼굴 감지 개선을 위해서 최근  “attention mechanism”과 “detection based on parts”이 제안되었다. ​multi-scale에서의 얼굴 감지 향상을 위해서 multi-scale feature fusion과 multi-resolution detection을 포함하여 일반 객체 감지 방법과 유사하게 처리하였다. ​ Text Detection텍스트는 수천 년 동안 인간의 주요 정보 전달자였다. Text Detection의 근본적인 목표는 주어진 이미지에서 텍스트가 있는지 판별하는 것이고 존재하면 이를 인식하는 것이다. 그리고 텍스트 감지는 매우 광범위한 응용 분야를 가지고 있다. 시각 장애인이 도로 표지판과 화폐를 읽는 데 도움을 준다. 지리 정보 시스템에서 집 번호와 도로 표지판을 감지하고 인식하여 디지털 지도를 쉽게 만들기도 한다. ​Text Detection의 어려움은 몇 가지로 요약 Different fonts and languages :  그림 22(a)와 같이 font, color, language가 각각 다를 수 있는 점. Text rotation and perspective distortion : 그림 22(b)와 같이 방향이 방향이 다를 수 있고 원근 왜곡이 있을 수 있는 점. Densely arranged text localization : 그림 22(c)와 같이 가로, 세로 비율이 다르고 레이아웃이 조밀한 텍스트 라인은 정확한 위치를 지정하기 어려운 점  Broken and blurred characters : 깨진 문자 및 흐릿한 문자는 거리 뷰 이미지에서 보편적이다는 점. ​​Text Detection은 : 1) text localization 과  2) text recognition 두 가지 관점에서  독립적인 작업으로 이루어지고 기존에는 “step-wise detection” 과 “integrated detection”이라는 2가지 방법으로 이루어졌다. ​step-wise detection은 문자 분할, 후보 영역 검증, 문자 그룹화, 단어 인식과 같이 일련의 처리 단계로 구성된다. 이 방법은 배경을 분리하여 계산을 크게 줄일 수 있는 장점이 있지만 모든 단계의 매개변수를 신중하게 설정해야 하고 각 단계에서 오류가 발생하고 누적된다는  단점이 존재한다. integrated detection은  문자의  localization, grouping,  recognition이  통합으로 처리되는 probability inference problem으로 텍스트를 감지한다.  이 방법은 누적 오류를 방지하고 언어 모델을 쉽게 통합할 수 있지만 많은 수의 문자 클래스와  candidate windows를 고려할 때 계산량이 많다는 단점이 있다. ​전통적인 텍스트 감지 방법은 unsupervised way으로 텍스트의 대칭성과 획의 구조 같은 일부 영역을 찾기 위해  Maximally Stable Extremal  Regions (MSER) 분할과  morphological filtering이 사용되었다. recognition보다는 localization 문제에 더 많은 관심을 기울이고 있는데 두 가지 방법이 제안되었다. 첫 번째 방법은 텍스트 감지를 일반 객체 감지의 특별한 경우로 처리하는 방식으로  감지 프레임워크가 통합되어   있지만 방향이 존재하거나 종횡비가 큰 텍스트를 감지하는 데는 비 효율적이다. 두 번째 방법은 텍스트 감지를 이미지 분할로 처리하는 방식으로 텍스트의 모양과 방향에 특별한 제한은 없지만 촘촘한 배열된 텍스트 라인을 구별하기 힘든 단점이 있다. ​딥러닝 기반의 텍스트 감지 방법은 전통적인 방식의 문제에 대해 몇 가지 방법을 제안하였다. For text rotation and perspective changes : 이 문제는  회전과 원근 변경과 관련된 anchor boxes 와 RoI pooling layer에 추가로 매개변수를 도입하여 해결하였다. ​To improve densely arranged text detection : 이 문제는 segmentation-based approach을 통해 해결하였는데  “segment and linking”과 additional corner/border detection을 추가하는 방식이다.  여기서 segment는 문자의 heatmap을 나타내고 linking은 동일한 단어 또는 텍스트 줄에 속해 있음을 나타내는 두 개의 인접한 segment 간의 연결을 의미한다.  그리고 corner/border은 개별 텍스트에 대한 closed boundary를 의미한다. ​To improve broken and blurred text detection : 텍스트의 깨짐과 흐릿한 텍스트 처리에는 wordleve recognition과 d sentence level recognition을 사용한다. 그리고 글꼴이 다른 텍스를 처리하는 가장 효과적인 방법은 synthetic samples을 사용하는 것이다. ​Traffic Sign and Traffic Light Detection자율주행 기술의 발달로 최근 몇 년 동안 교통 표지 및 신호등의 자동 감지가 큰 주목을 받았다. 그리고 오랫동안 컴퓨터 비전에서 신호등처럼 고정 패턴을 일반 물체로 감지할 경우 쉽게 인식할 수 있다는 생각은 잘못된 것이라고 할 수 있다. ​ traffic sign/light detection 이 어려운 이유 Illumination changes로 그림 23(a)와 같이 태양빛이 비치는 곳이나 야간이 특히나 어렵다. Motion blur는  on-board 카메라로 촬영한 이미지는 그림 23(b)와 같이 자동차의 움직임으로 인해 흐려진다. Bad weather로써 비가 오거나 눈이 오는 날과 같은 악천후에서는 그림 23(c)와 같이 이미지 품질에 영향을 준다. Real-time detection은 자율주행에 특히나 중요한 요소이다. ​​​​전통적인 traffic sign/light detectionvision 기반 traffic sign/light detection은 20년 전으로 거슬러 올라갈 수 있다. traffic sign/light은 특정 모양과 색상을 가지고 있으므로 색상 임곗값, 시각적 돌출 감지, 형태학적 필터링, 가장자리 / 윤곽 분석을 기반으로 한다. 하지만  이 방법들은 저 수준의 vision 기반으로 설계되었기 때문에 일반적으로 복잡한 환경에서는 실패율이 높아 GPS와 같은 것을 결합하는 vision 기반 접근을 하기 시작하였다.  “feature pyramid + sliding window” 방식이 당시 일반 물체 감지 및 보행자 감지를 위한 표준 프레임워크가 되었지만 매우 적은 수의 작업에만 사용되었다. ​딥러닝 기반  traffic sign/light detection 딥러닝 시대에는 Faster RCNN 및 SSD와 같은 일부 잘 알려진 감지기가  traffic sign/light detection에 적용되었다. 이를 통해  attention mechanism 과 adversarial training과 같은 새로운 기술이 복잡한 트래픽 환경에서 탐지를 개선하는 데 도움을 주었다. ​Remote Sensing Target Detection원격탐사 영상 기술을 통해 지구를 더 잘 이해할 수 있는 문이 열렸다. 최근에는 원격탐사 영상 해상도가 높아짐에 따라 원격탐사 표적탐지(비행기, 선박, 기름받이 등 )가 연구의 화두로 떠오르고 있고 군사 조사, 재난구조, 도시 교통관리와 같은 광범위한 응용 분야를 가지고 있다. ​Remote Sensing Target Detection의 어려운 문제 Detection in “big data” 부분으로 원격 감지 이미지의 방대한 데이터양에 의해 원격 감지 대상을 빠르고 정확하게 감지하는 방법이 여전히 힘들다. 그림 24(a)는 원격 탐사 영상과 자연영상의 데이터량을 비교한 것이다. Occluded targets의 경우 지구 표면의 50% 이상이 매일 구름으로 덮여 있다. 가려진 표적의 몇 가지 예가 그림 24(b)에 나와 있다. Domain adaptation은 다른 센서(다른 변조 및 해상도)로 캡처한 원격 감지 이미지는 높은 수준의 차이를 나타낸다. ​대부분 전통적인 방법은 1) candidate extraction 과 2) target verification과 같이 2단계 탐지 패러다임을 가진다. candidate extraction 단계에서 자주 사용되는 방법에는 gray value filtering , visual saliencybased methods,  wavelet transform based methods,  anomaly detection based methods 등이 존재하는데 공통점은 모두 unsupervised methods으로 복잡한 환경에서 실패하는 경우가 많다는 것이다.  target verification 단계에서 자주 사용되는 방법은 HOG, LBP, SIFT 등이 있다. 이외에도 sliding window detection을 따르는 경우도 있다. ​유조선 및 연안 선박과 같은 특정 구조 및 모양의 표적을 탐지하기 위해 일부 도메인 지식이 사용되기도 한다. 예를 들어 oil-pots 감지는  circle/arc detection으로 간주될 수 있고 근해 선박 감지는 선수와 선미의 탐지로 간주될 수 있다. 그리고 가려진 대상의 감지 개선을 위한 일반적인 방법은 “detection by parts”인데  “mixture model”을 이용하여 방향이 다른 대상에 대해 서로 다른 감지기를 따로 훈련시키는 방식이다. ​딥러닝 기반의 방법으로는 2014년 RCNN의 큰 성공 이후 Deep CNN이 원격 감지 대상 탐지에 적용되기 시작했다. 원격 감지 이미지와 일상적인 이미지에 큰 차이가 존재하기 때문에 원격 감지 이미지에 대한 연구가 이루어졌다. 이때 연구자들은 Deep CNN이 spectral data에 대해 기존 방법보다 나은 점을  발견하지 못해  더 좋은 rotation invariance를 처리하는 ROI Pooling layer을 개선하였다. 또한 일부 연구자들은 탐지 단계에서 테스트 이미지 분포를  adaptive 하게 update 하는   Bayesian view를 공식화했고 attention mechanisms 과 feature fusion strategy도 작은 표적 탐지 개선에 사용하기도 했다.  "
Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks ,https://blog.naver.com/hanchaa/222647784450,20220215,"Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks, Ren et al., 2016.arxiv: https://arxiv.org/abs/1506.01497 Faster R-CNN: Towards Real-Time Object Detection with Region Proposal NetworksState-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet and Fast R-CNN have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Pr...arxiv.org Abstract​SOTA object detection 네트워크의 경우 object의 위치를 얻기 위해 region proposal 알고리즘에 의존한다. SPPnet이나 Rast R-CNN의 경우 detection network의 running time은 줄였지만 region proposal 연산의 경우 병목현상을 일으켰다 (Fast R-CNN의 경우 region proposal로 selective search 알고리즘을 사용하는데, 이는 CPU에서 수행되므로 속도가 느리다). 이 논문에서는 Region Proposal Network(RPN)을 제안하였는데, 이는 전체 이미지의 convolutional feature를 detection entwork와 공유하므로 region proposal이 거의 cost-free하다. 이 RPN은 end-to-end로 학습 되고, Fast R-CNN detection에서 사용되는 높은 품질의 region proposal을 생성한다.​Introduction최근의 object detection의 발전은 region proposal method 들과 region-based convolutional neural network 들로 인해 이루어졌다. Region-based CNN의 경우 처음 등장했을땐 연산비용은 많았지만, 최근들어 그 비용이 매우 줄어들었고, Fast R-CNN의 경우 region proposal에 사용되는 시간을 제외하고는 거의 실시간 처리 속도를 보여주었다.​가장 유명한 region propsal 방식인 selective search는 효율적인 detection network들과 비교했을 때, 이미지당 CPU에서 2초의 시간이 걸릴 정도로 느렸다. EdgeBoxes 모델은 현재 proposal의 품질과 속도 사이의 가장 좋은 tradeoff를 제공하는데, 이미지당 0.2초의 시간이 걸린다. 그럼에도 불구하고 region proposal step은 detection network만큼 많은 시간을 소비한다.​이러한 차이는 CNN의 경우 GPU를 사용하여 이득을 볼 수 있지만 region proposal method들의 경우 CPU에서 실행되기 때문에 이러한 속도 차이가 발생한다. Proposal을 가속화할 수 있는 한가지 방법은 GPU를 사용하도록 재구현하는 것인다, down-stream detection network를 무시한채 재 구현하는 것은 연산을 공유할 수 있는 중요한 기회를 놓칠 수 있다.​이 논문에서는 detection network의 계산이 주어졌을 때 proposal의 연산이 거의 cost-free한 알고리즘을 제안한다. 이를 Region Proposal Networks (RPNs)라고 부르며, SOTA object detection 네트워크들과 convolution 레이어를 공유한다. 이를 통해 test-time에서 proposal을 위한 계산 시간은 매우 작다 (eg. 이미지당 10ms).​이 논문에서는 region-based detector에서 사용된 convolutional feature map 또한 region proposal을 위해 사용될 수 있다는 것을 깨닫고, convolutional feature들 위에 몇개의 convolutional layer들을 추가해서 RPN을 만들었다. RPN은 region bound들을 regress 함과 동시에 regular grid 상의 각 위치에서의 objectness score를 계산한다. RPN은 일종의 fully convolutional network이므로 detection proposal을 생성하는 작업에 대해 end-to-end로 학습시킬 수 있다.​RPN은 효율적으로 넓은 범위의 scale과 aspect ratio를 가지는 region proposal을 예측하기 위해 디자인 되었다. 이를 위해 이미지 또는 필터 피라미드를 사용하는 기존의 방식과는 다르게, ""anchor""를 도입하였다. 이는 regression reference들의 피라미드라고 생각하면 되는데, 이미지나 필터의 다양한 scale 또는 aspect ratio들을 반복하는 것을 피함으로써 속도 향상을 가져다준다.​Faster R-CNNFaster R-CNN은 2개의 모듈로 구성되어있는데, 첫 번째 모듈은 fully convolutional network로 region을 제안하고, 두 번째는 Fast R-CNN detector로 proposed region들을 이용한다. 전체 시스템은 object detection을 위한 하나의 통합된 네트워크이다. ""Attention"" 방식과 같이 RPN 모듈은 Fast R-CNN 모듈이 어디를 봐야하는지 알려준다.​· Region Proposal NetworksRPN은 아무런 크기의 이미지를 입력으로 받고 각각의 objectness score와 함께 사각형의 object proposal 집합을 반환한다. 이 과정을 fully convolutional network를 이용하여 모델링하였다. 이 논문에서의 최종 목표는 Fast R-CNN object detection netowrk와 연산을 공유하는 것이므로, 두 network 모두 공통의 convolutional layer들을 사용했다고 가정한다.​Region proposal들을 만들기 위해, 마지막으로 공유된 convolutional layer의 output인 convolutional feature map에 작은 네트워크를 sliding 시키는데, 이 네트워크는  n * n spatial window를 input으로 받는다. 각각의 sliding window는 lower-dimensional feature로 mapping 된다 (ZF를 이용하는 경우 256-d, VGG를 이용하는 경우 512-d). 그리고 이 feature는 box-regression layer(reg)와 box-classification layer(cls)의 입력으로 들어간다. 이 논문에서는 n = 3을 이용했다고 한다. ZF를 이용한 경우 RPN- Anchors각각의 sliding-window 위치에서, 동시에 여러 개의 region proposal들을 예측하는데, 각 위치에서 가능한 최대 갯수를 k로 표기한다. 따라서 reg layer는 k개의 박스의 좌표를 encoding 하는 4k개의 output을 만들어 내고, cls layer는 object인지 아닌지의 확률을 예측하는 2k개의 점수를 output으로 만들어 낸다. 이 k개의 proposal들을 anchor라고 부르며, 이들은 sliding window의 한 가운데에 정렬된다. 이 논문에서는 3개의 scale과 3개의 aspect ratio들을 이용해 각 sliding-window에서 k = 9개의 anchor들을 만들어낸다.​Translation-Invariant Anchors만약 이미지에서 물체의 위치가 이동한다면 proposal 또한 이동해야하고 똑같은 함수가 어느 위치든 proposal을 예측할 수 있어야 한다. 이러한 translation-inavariant 속성이 RPN에서는 보장된다고 한다.​이런 translation invariant 속성은 모델의 사이즈 또한 줄여주는데, 이러한 속성이 없는 MultiBox 방법은 (4 + 1) * 800 차원의 FC output layer를 가지고 있는 반면, RPN의 경우 (4 + 2) * 9 차원의 convolutional output layer를 가진다 (k = 9인 경우). 그 결과 RPN의 output layer는 512 * (4 + 2) * 9 (2.8 * 104)개의 파라미터를 가진다 (VGG-16을 사용하는 경우).​feature map이 h * w * 512인 경우 1) 1 * 1 * 512짜리 필터 2개를 이용해 각 위치에서 하나의 anchor의 objectness vs non objectness score를 구한다.2) 이를 9개의 anchor box에 대해 구해야 하므로 512 * 2 * 9개의 파라미터가 필요.3) 1 * 1 * 512짜리 필터 4개를 이용해 각 위치에서 하나의 anchor의 regression 결과를 구한다.4) 이를 9개의 anchor box에 대해 구해야 하므로 512 * 4 * 9개의 파라미터가 필요.5) 총 512 * (4 + 2) * 9개의 파라미터가 필요.​Multi-Scale Anchors as Regerssion References 위 사진은 multi-scale prediction을 위해 두가지 유명한 방법을 보여준다. (a)의 경우 이미지가 여러 scale로 resize되고 feature map들이 각각의 scale에 대해 계산되는데, 이는 유용한 반면 시간이 많이 걸린다. (b)의 경우에는 여러 scale과 aspect ratio의 filter들을 sliding window에 사용하는 방법이다.​이와 비교해서 PRN의 anchor-based method(c)는 더 cost-efficient 한데, 이 방법은 여러가지 scale과 aspect ratio의 anchor box들을 참조하여 bounding box들을 classification과 regression을 수행한다. 이는 한가지 사이즈의 이미지와 feature map, filter를 이용한다. 이런 multi-scale anchor들이 추가적인 비용 없이 feature를 공유하는데 중요한 역할을 한다.​- Loss FunctionRPN을 학습하기 위해 각각의 anchor에 object인지 아닌지 binary class label을 부여하였는데 아래의 조건에 따라 positive label을 부여하였다.​1) Ground-truth 박스와 IoU가 가장 높은 anchor.2) 아무 ground-truth 박스와 IoU가 0.7보다 높은 anchor.​보통은 2번 조건만으로 충분하지만, 2번째 조건만으로 positive sample을 찾을 수 없는 드문 경우를 위해 1번 조건도 추가하였다고 한다. 그리고 non-positive anchor의 모든 ground-truth box들과의 IoU가 0.3 보다 작은 경우 negative label을 부여하였고, positive도 negative도 아닌 경우 학습에 영향을 끼치지 않았다.​ RPN은 다음과 같은 Fast R-CNN의 multi-task loss를 따르는 loss function을 최소화 시킨다.​i: mini-batch에서 anchor의 indexpi: i번째 anchor가 object일 확률pi*: i번째 anchor의 ground-truth label로 positive인 경우 1, negative인 경우 0ti: 예측된 bounding box의 4개의 좌표를 나타내는 vectorti*: positive anchor와 연관된 ground-truth box의 4개의 좌표를 나타내는 vector​Lcls: object인지 아닌지 2가지 class에 대한 log lossLreg: R(ti - ti*)로, Fast R-CNN에서 사용된 robust loss (smooth L1)​lambda: Lcls와 Lreg 사이의 balancing weight로 이 논문에선 10을 사용하였다. smooth L1 loss (https://www.researchgate.net/figure/The-curve-of-the-Smooth-L1-loss_fig5_322582664)​이때 Lreg 앞에 pi*가 곱해지는데, 이는 regression loss가 anchor가 true일 때만 영향을 끼치도록 한다는 의미이다. cls 레이어와 reg 레이어의 output은 각각 {pi}와 {ti}를 만들어 낸다.​Bounding box regression을 위해 4개 좌표의 parameterization을 다음과 같이 적용한다. 이때 x, y, w, h는 box의 중심 좌표와 넓이, 높이를 나타낸다. x, xa, x*는 각각 예측된 박스, anchor box, ground-truth box를 나타낸다.​이전의 방법(eg. Fast R-CNN) bounding-box regression이 임의의 사이즈 ROI로 부터 pool 된 feature에 대해서 이루어졌으며, regression weight들이 모든 region size들과 공유 되었다. 하지만 이 논문에서는 regression을 위해 feature map 위의 똑같은 spatial size의 feature map들을 사용한다. 다양한 사이즈를 위해 k개의 bounding-box regressor가 학습되는데, 각 regressor는 하나의 scale과 aspect ratio를 담당하며 weight들을 공유하지 않는다.​· Sharing Features for RPN and Fast R-CNN이제까지는 region-based object detection CNN에 대한 고려 없이, RPN을 어떻게 학습할지에 대해 설명하였다. Detection network를 위해서 Fast R-CNN을 사용하였다고 한다. 독립적으로 학습 된 RPN과 Fast R-CNN 모두 convolution layer들을 다른 방향으로 수정할 것이기 때문에, 두 네트워크 간의 convolutional layer들을 공유할 방법을 찾았다고 한다.​(i) Alternating training이 방법에서는 우선 RPN을 먼저 학습시킨 후, 학습된 RPN에서 구한 proposal들을 이용해 Fast R-CNN을 학습한다. 이 후 Fast R-CNN으로 튜닝된 네트워크를 RPN을 초기화하는데 사용하고, 이러한 과정이 반복된다. 이 논문에서는 이 방법을 이용한 학습 방법을 사용했다.​(ii) Approximate joint training이 방법에서는 RPN과 Fast R-CNN 네트워크가 하나의 네트워크로 합쳐져 학습을 진행한다 (제일 첫번째 사진과 같은 방식). 각 SGD iteration에서 forward pass에서 region proposal들을 생성하는데, 이것들은 Fast R-CNN detector를 학습할 때 처럼 고정되고, 미리 계산된 proposal들 처럼 여겨진다. Backward propagation은 평소처럼 일어나는데 RPN loss와 Fast R-CNN loss 모두가 결합되어 있다. 이러한 방식은 구현하기 쉽지만 proposal box들의 좌표와 관련된 미분값들을 무시한다고 하는데 이 부분은 왜 그런지 잘 이해가 안된다. 이 방식이 실험을 통해 비슷한 결과를 내지만 학습 시간을 25-50% 정도 줄였다고 한다.​(iii) Non-approximate joint trainingRPN에 의해 예측된 bouding box들의 좌표의 미분값들 또한 고려하는 방법인데, 이 논문의 범위를 벗어난다고 한다.​4-Step Alternating Training이 논문에서는 공유된 feature들을 위해 4-step training 알고리즘을 적용했다고 한다. ​첫 번째로, RPN을 3.1.3절에서 설명한 방식대로 학습을 진행한다. 이 네트워크는 ImageNet-pre-trained 모델이며 region proposal task를 위해 end-to-end로 fine-tune 되었다.두 번째로, Fast R-CNN을 이용해 또 다른 네트워크를 학습 시키는데, 이때 step-1의 RPN에서 만들어진 proposal들을 이용한다. 이 네트워크 또한 ImageNet-pre-trained 모델이며, 아직까지는 두 네트워크가 convolutional layer들을 공유하지 않는다.세 번째로, detector network를 RPN 학습을 위해 사용한다. 하지만 공유된 convolutional layer들은 고정한 채 RPN에만 사용되는 레이어들만 fine-tune을 한다. 이제 두 네트워크는 같은 convolutional layer들을 가지게 된 것이다.마지막으로, 공유된 convlutional layer들을 고정한채, Fast R-CNN에만 사용되는 레이어들을 fine-tune 한다.​이를 통해 두 네트워크는 똑같은 convolutional layer들을 공유하고 결합된 네트워크를 형성한다. 더 많은 반복을 진행할 수 있지만 무시할 수 있을만큼의 향상 밖에 없었다고 한다.​· Implementation DetailsSingle scale 이미지를 이용하여 RPN과 object detection network들을 학습하고 테스트했다고 한다. 이미지를 짧은 쪽이 600 픽셀이 되도록 re-scale 하였다. 그 결과 re-scale된 image에서 ZF와 VGG 모두 마지막 convolution layer에서의 stride가 16 픽셀이라고 하는데, 자세히는 모르겠지만 re-scale된 이미지에서의 sliding window의 stride가 16픽셀이라는 뜻 같다.​Anchor는 총 3개의 scale(1282, 2562, 5122)을 사용했으며, 3개의 aspect ratio(1:1, 2:1, 1:2)를 사용했다고 한다. 이미지의 경계를 넘는 anchor 박스들은 학습 과정에서 무시하였다. 테스트를 할 때는 경계를 넘어가는 proposal box들은 경계에 맞게 잘랐다고 한다.​몇몇 RPN 제안들은 서로 서로 매우 많이 겹칠텐데, 이를 줄이기 위해 cls score를 기반으로 non-maximum suppression 방식을 사용했다. NMS를 위한 IoU threshold로는 0.7로 사용하였다.​Expriments PASCAL VOC 2007 test set에 대한 결과이다. Detector로는 ZF를 이용한 Fast R-CNN을 이용하였고, 학습과 테스트를 위해 다양한 proposal method들을 사용하였다. 그 결과 Faster R-CNN이 SOTA를 달성했다.​Ablation study에서는 RPN의 영향에 대해 실험하였다. RPN과 detector가 분리된 network를 사용한 경우 성능이 살짝 줄어들었다. 다음으로는 Fast R-CNN을 학습할 때 RPN의 영향에 대해 실험하였는데, 학습을 할 때는 Selective Search를 사용하고 테스트를 할 때 RPN을 사용하였다. 이때도 RPN과 detector는 feature들을 공유하지 않는다.​마지막 줄에 보면 detector를 SS + ZF로 유지한 채 RPN을 학습하기 위해 VGG-16을 사용한 결과를 볼 수 있는데, 59.2%로 꽤나 견주어 볼만한 결과를 보여주었다. PASCAL VOC 2007 test setPASCAL VOC 2012 test setRPN+VGG를 detector와 같은 feature를 사용하게 학습해서 테스트한 결과 당연히 SS+VGG-16를 사용한 것 보다 높은 점수가 나왔다.​ 전체 object detection system의 running time을 정리한 표이다. region-wise에는 NMS, pooling, fully-connected, softmax layer를 수행하는데 걸린 시간이 포함되어 있다.​SS를 이용한 경우 한 이미지를 처리 하는데 거의 2초나 걸렸지만 RPN을 사용하자 200ms로 줄었고, 좀 더 가벼운 모델인 ZF를 사용한 경우 60ms까지 줄일 수 있었다. 위 표들은 여러가지 하이퍼 파라미터 값에 따른 결과 차이인데 표로도 바로 볼 수 있으니 따로 설명은 하지 않겠다.​ ​ "
Object Detection 구성요소 ,https://blog.naver.com/doctor_song/222699980635,20220413,"1. 영역 추정(딥러닝 이전 부터도 o)Region Proposal : Object 있을만한 위치에 hint 줘야 한다.    Region Proposal 근처에서 BBox 예측​2. DL NetworkFeature Extracter + Classification layer [Backbone]Feature Pyramid Network(FPN) [Neck]Network Prediction [Head] - bbox regression, classification  3. etcIOUNMSmAPAnchor box​※ Object Detection 어려운 이유1. classification, bbox regression 동시에 찾아야함2. object 크기와 유형이 다양함3. detect하는 시간 중요!(실시간 영상)4. img가 명확x(배경 대부분)5. dataset 부족(annotation 만들어야)​출처 : https://pytorch.org/hub/hustvl_yolop/​ "
딥러닝 컴퓨터 비전 완벽 가이드 | 1. Object Detection 개요와 역사 ,https://blog.naver.com/kimsamuel351/222739668805,20220520,"Localization/Detection/Segmentation 1. Image Classfication2. Localization : 단 하나의 Object 위치를 Bounding box로 지정하여 찾음3. Object Detection : 여러개의 Object들에 대한 위치를 Bounding box로 지정하여 찾음4. Segmentation : Detection보다 더 발전된 형태로 Pixel 레벨 Detection 수행​- Localization/Detection : Object의 위치를 Bounding Box로 찾기 (regression) + Bounding Box 내의 오브젝트를 판별 (Classification)  Object Detection History - Two-stage detector : 위치 찾고, detection => 느려서 실시간 적용 어려움- One-stage detector : 성능 떨어지는 문제 -> 발전하면서 성능 증가​ ​ "
Object Detection - HOG 알고리즘 개념편 (1)   ,https://blog.naver.com/dongju0531hb/222443993008,20210724,"#시각지능 #객체검출 #AI #비전인식 #인공지능 #HOG​​0. Intro...지난 시간에는 Haar-like filter를 가지고 이미지가 haar feature를 얼만큼 가지고 있는 지 계산하여, 얼굴을 검출하는 알고리즘비올라-존스 face detection 을 공부해보았다.​이번 시간에는 이미지에서 gradient 개념을 이용하여 feature vector를 정의하고, SVM 알고리즘을 이용하여 보행자 위치를 검출하는 알고리즘 HOG 알고리즘을 알아보자!​​​  1. Image에서 Gradient란?이미지에서 기울기라는 말은 무슨 뜻일까?우리가 보통 기울기라는 말은 x의 변화한 양 대비 y가 얼만큼 변했는 가 할때 사용하는 단어이다. https://m.blog.naver.com/PostView.naver?isHttpsRedirect=true&blogId=junhyuk7272&logNo=221090781691​이미지도 마찬가지이다. 이미지는 수많은 픽셀로 이루어져있어, 이들이 다닥다닥 붙어있는데, 그 붙어있는 pixel끼리 각각의 밝기 값(gray scale 값)이 똑같을 수도 있고 다를 수도 있다.그러면 이때 그 pixel 좌표 값 차이 대비 밝기 값의 얼만큼 변했는 가가 이때 image에서 gradient라 할 수 있을 것이다.​자 그럼 생각해보자.이미지에서 물체의 edge 부분과 물체 중심 부분 중 어느것이 Gradient가 더 클까?​답은 edge이다. 왜냐면 우리가 대부분 object의 edge 를 볼 수 있는 이유가  눈이 object에 반사되고 받아들이는 빛의 밝기가 다르기 때문이다. 밝기가 아닌 색깔이 달라서 edge 구별되는 이유는 RGB 컬러 영상에서 특정 컬러 채널의 밝기 값이 더 큰거나 더 작은 수치상의 차이가 나서이기 때문이다.​이미지의 gradient가 더 자체히 알고 싶다면, 아래로 ↓↓↓↓↓↓↓↓ https://theailearner.com/2019/05/11/understanding-image-gradients/ Understanding Image GradientsIn the previous blogs, we discussed different smoothing filters. Before moving forward, let’s first discuss Image Gradients which will be useful in edge detection, robust feature and texture …theailearner.com ​​​​  2. HOG(Histogram of Oriented Gradient)에서 이미지의 feature를 어떻게 표현할까? ​Histogram of Oriented Gradient 에서는 영상의 feature(특징)를 기본적으로방향에 따른 gradient 값의 histogram 으로 표현한다.​보행자 검출(  Pedestrian Detection ) 목적을 위한 HOG는 기본적으로 64x128 크기의 영상 을 이용한다.  1) 먼저 입력 영상으로부터 각 pixel의 Gradient의 크기와 방향을 계산한다. 영상의 이미지의 픽셀들이 z = f(x,y) 형태의 밝기 값(gray scale) 으로 표현되기 때문에,x,y  평면에서 gradient가 어떤 방향인지, 즉 크기와 방향 성분으로 계산되고, 방향성분은 0도부터 180도 까지 설정된다.아래 그림의 gradient vector로 이해하면 쉬울 것이다. (참고로 edge direction)이랑 다름!!​gradient  출처  : https://yeoeun-ji.tistory.com/20 ​  2) 8x8 pixel로 묶은 단위를 cell이라고 한다. 우선 64x128 영상을 8 x 8 pixel 크기의 component 단위로 분할할 수 있는데, 이 8x8 component를 cell 이라고 부른다.그러면 64x128 영상에서 cell은 가로 방향으로 8(64/8)개, 세로 방향으로 16(128/8)개 생성된다.(참고, 근데 꼭 8x8 로 안 묶어도 된다.)​​  3) 각각의 cell로부터 Gradient의 방향에 대한 Gradient 크기 히스토그램을 구한다.방향 성분을 20도 단위로 구분하면 총 9개의 bin( 히스토그램 x축 ) (참고 bin 갯수는 자기 마음대로)[0~20˚, 20~40˚, 40~60˚, 60~80˚, 80~100˚, 100~120˚, 120~140˚, 140~160˚, 160~180˚  ]으로 구성된 방향 히스토그램이 만들어진다.  gradient direction에 맞는 bin에, gradient magnitude 값을 더하자. 이 경우는 방향 값이 bin 중심에 떨어진만큼 가중치를 해서 4가 2,2 반반으로 나눠진 방식이다.이때 cell안에서 특정(x0,y0) 위치에서 gradient(변화)가 a만큼이고 방향이 b라면,우리가 나눈 histogram의 해당 방향 b가 속하는 위치의 bin에 a 크기를 더한다. 이렇게 cell에서 각각 pixel의 방향에 속하는 gradient 값을 더해나가면, cell를 표현할 수 있는 gradient 히스토그램이 만들어진다. 출처 : https://customers.pyimagesearch.com/lesson-sample-histogram-of-oriented-gradients-and-car-logo-recognition/​ https://heartbeat.fritz.ai/introduction-to-basic-object-detection-algorithms-b77295a95a63​이렇게 방향을 나타내는 Orientation Histogram의 특징은 edge의 양(gradient가 얼마나 큰지)과 방향(gradient가 어느 방향인지)를 구분하는 feature 을 가지고 있다.또한 HOG의 경우 pixel하나하나 구하는 것이 아닌 cell단위로 overlap(gradient 누적 값)을 하여 구하기 때문에 model 자체가 robust(견고)하기 때문에, 잡음(noise)에도 강하다. 마치 Haar-like처럼 영역 안에서 하나하나 pixel 값을 더하고 빼서 이 feature를 얼마나 가지고 있는지 영역의 feature를 보여주어 noise에 둔한 것처럼, HOG의 overlap도 어느정도의 변화를 수용하겠다는 뜻이다. ​​​  4) 인접한 4개의 cell을 합쳐서 block이라고 정의한다.즉, 8x8pixel 은 cell → cell을 4개로 묶으면 하나의 block (16x16pixel) 이 된다. (참고, 9개로도 묶을 수 있다) https://customers.pyimagesearch.com/lesson-sample-histogram-of-oriented-gradients-and-car-logo-recognition/block의 종류는 가로와 세로 방향으로 각각 한 개의 cell만큼 이동하면서 달라질 수 있다.64x128 영상에서 block은 가로 방향으로 cell 단위로 움직여서 7(8-1)개, 세로 방향으로 15(16-1)개 정의할 수 있다.결국 64x128 영상에서 block은 가로 방향으로7개, 세로 방향으로 15개 정의하여 64x128 이미지에서는 block이 105개 가 나올 수 있다.​그럼 현재 block의 각 cell(4개)에 대해 Oriented Gradient Histogram을 연결한 다음,전체 feature vector를 정규화하는 L1, L2를 이용한다.  5) block 안 cell의 histogram을 연결시킨 후 L1,L2 정규화를 하여 최종적인 영상의 feature vector를 완성시킨다. 책, Recent Advances in Applied Thermal Imaging for Industrial Applicationsv는 정규화되지 않은 vector이고, 주어진 block에서 cell들을 concatenate(연결, 즉 9개의 bin이 36개로 x축으로 연달아 이어붙인 셈)된 histogram을 구성한다고 하자. (그럼 36개의 성분이 있는 vector인 셈, 여기 분모를 정규화하자)vk는 k- norm vector라고 하고 e는 vk가 0이 되었을 때 분모가 0이 되는 것을 방지하기 위함이다.그러면 정규화를 하면 L2 norm과, L1 norm 위의 수식으로 표현된다.​이러한 유형의 정규화는 각 cell이 최종 feature vector에 예를 들어 위의 그림 cell#5를 기준으로 block1, block2, block3, block4에 여러번 표현되지만, 각 block에서 다른 값으로 정규화된다. 이러한 한 cell의 다르게 표현되는 것은 중복되고 공간을 낭비하는 것처럼 보이지만, 실제로 feature descriptor ( 영상의 특징을 잘 설명해주는 )의 성능을 향상시킨다.​​ L2 norm이 더 좋다. 그래도 정규화안한 것보다 훨씬 낫다.​이렇게 모든 block이 각각에서 정규화된 histogram을 가져와 최종 feature vector로 처리한다.​105개의 전체 block에서 추출되는 방향 히스토그램(orientation histogram)에서 실수 값(Magnitude of Gradient) 개수는 105 x 36 ( 9bins * (2x2cells / 1block) ) = 3780이 된다.​이 3780개의 실수 값이 64x128 영상을 표현하는 HOG에서 feature vector 역할을 한다.​​​​  2. 어떻게 특정 image가 이 HOG feature임을 알 수 있게 학습할 수 있을까? ​바로 답은 SVM(Supporter Vector Machine)이다.SVM은 기존 Linear Classfication의 경우,분류의 기준의 되는 hyperplane이 학습의 초기 hyperparameter나 data에 영향을 받아 hyperplane이 다르게 형성되기 때문에, best solution이라고는 할 수 없다.좀 더 robust하고 최적의 hyperplane을 만들기 위해서 분류된 vector(data)들 중 군집의 바깥쪽 테두리로 구성된 convex hull의 vector들과 hyperplane 사이의 여러 개의 거리 중 최소값을 가장 크게 만들어주는 hyperplane을 구하는 알고리즘이다.아래 설명이 잘되있다.1. 직관적인 이해https://eehoeskrap.tistory.com/45 [인공지능] SVM (Support Vector Machine, 서포트 벡터 머신)SVM (Support Vector Machine) 서포트 벡터 머신은 인공지능의 기계학습 분야 중 하나로, 패턴인식, 자료분석을 위한 지도학습 모델이다. 즉, 2개의 범주를 분류하는 이진 분류기이다. 주로 분류와 회귀 분석을 위..eehoeskrap.tistory.com  2. 수식적인 이해https://www.youtube.com/watch?v=qFg8cDnqYCI ​​​​​  3. 요약 ​정말 HOG를 잘 설명해주는 flow diagram인 것 같다. 1input image에서 cell을 나누고, cell에서 각 pixel마다 gradient의 크기 값과 방향을 구한다.2구한 gradient 방향을 bin으로 크기 값의 histogram을 만들고3cell을 4개를 묶어서 block을 만들어 bin 갯수의 * 4가 되는데, 이것을 vector라고 했을 때, 이 vector의 절대 값을 나누어준다. (L1, L2 norm)으로 즉 정규화시켜준다.4각 block은 36개 feature를 갖게 되고, image 전체에서의 block 갯수 * 36이 되면 이 image를 나타내는 feature vector가 된다. 5이미지(feature vector)과 label로 이루어진 쌍(example)은 선형 SVM 분류기를 통해 학습된다.6test 이미지를 주면, feature vector를 가지고 detection window에서 내가 찾고자 하는 label이 있는지를 확인하고, 그 window의 위치를 알려주는 것이다.  자세한 설명이 필요한 부분은 댓글로 남겨주세요참고 : https://studyingfox.tistory.com/7https://customers.pyimagesearch.com/lesson-sample-histogram-of-oriented-gradients-and-car-logo-recognition/https://studyingfox.tistory.com/6https://heartbeat.fritz.ai/introduction-to-basic-object-detection-algorithms-b77295a95a63​ "
Two-stage Object Detection Models (R-CNN 계열) ,https://blog.naver.com/siniphia/221622491750,20190820,"​Object Detection이란? 먼저 Detection을 이해하려면 Classification과 Localization의 개념을 알고 있는 것이 좋다. 먼저 아래 설명은 이미지에 물체 하나만 존재한다는 전제를 가정했지만, 이미지 하나에 여러 물체가 존재하는 경우도 있다.​(1) Classification Classification은 이미지 안에 존재하는 물체의 Class를 분류하는 것이다. 예를 들면 사진 속 동물이 개인지 고양이인지 맞추는 작업 등이 있다. 이 때 Object의 위치, 경계, 크기 등의 공간적인 문맥은 추론 대상이 아니며 이미지의 단순 분류에만 관심을 둔다. 따라서 비교적 쉬운 작업이며 이미 ImageNet 등의 여러 Public Dataset에 대해 인간의 성능을 뛰어넘었다.​(2) Localization Localization은 Classification과는 반대로 Object의 Class 분류는 고려하지 않고, 배경으로부터 물체의 Location을 찾아내는 작업이다. 주로 사각형 박스로 물체의 위치를 표시하는 Bounding Box의 좌표를 추론하는 방향으로 학습된다. Bounding Box는 보통 4가지 값인 (x, y, w, h)로 표현하는데 구체적인 설정은 모델마다 다를 수 있으므로 (x, y)가 박스의 중심 좌표일수도, 왼쪽 위 모서리 좌표일수도 있다. (w, h)는 각각 Box의 너비와 높이를 의미한다.​(3) Detection 보통의 경우 Detection은 Classification + Localization으로 이해하면 편하다. 즉 이미지로부터 물체의 Location을 찾아내는 동시에 어떤 Class에 속하는지도 분류해야 하는 작업이다. 따라서 이미지 하나 당 (c, x, y, w, h)의 다섯가지 Value를 추론하게 된다. 두 문제를 동시에 해결해야 하므로 난이도가 높은 편이고 이에 성능을 높이기 위한 다양한 방법론들이 제시되었다. 이들을 크게 두 항목으로 나누어 하나씩 알아보자.​​​Two-stage Object Detection Methods 딥러닝을 활용한 Detection Method는 Two-stage와 Single-stage로 분류할 수 있다. 그 중 이번 글에서 다룰 Two-stage Method에는 R-CNN 계열 모델들이 속한다. Two-stage라고 명명된 이유는 이미지 하나에서 여러 후보 영역을 뽑아내는 Region Proposal 단계와 이 후보들을 사용한 Detector의 학습 단계로 구분되기 때문이다.  이들은 R-CNN, Fast R-CNN, Faster R-CNN 그리고 Mask R-CNN으로 발전되었고, 알고리즘의 비효율적인 병목 현상을 개선해 나가며 점점 빠르고 정확해졌다.​​1. R-CNN (2014)   ​ R-CNN은 위 그림처럼 후보 영역(노란색 상자)을 제안한 뒤 고정 크기로 변환(Warp)시키고 CNN에 연결된 Classifier와 Regressor를 학습시킨다.​ 먼저 후보 영역 제안에 사용된 알고리즘은 Selective Search라는 고전적인 방식인데 딥러닝이 아니기 때문에 CNN 이후 단과 완전히 별개이다. 이 단계를 통해 이미지 하나 당 약 2,000장의 영역이 제안되고 전부 하드디스크에 저장된다.​ 이후 저장된 영역들을 Detector의 학습 재료로 삼아 Pre-trained CNN을 돌려 Feature를 추출한다. 이 때 Detection을 Classification과 Localization으로 나눠서 해결하게 되며, 이를 위해 각각 Classifier와 Regressor를 따로 학습시킨다.​ R-CNN은 사실상 4-stage 방식이다. Selective Search, CNN, Classifier, Regressor를 전부 따로 수행해야 하기 때문이다. 또한 모든 후보 영역을 CNN에 넣기 때문에 만약 이미지가 50,000장이라면 개당 2,000개 가량의 후보 영역이 생성되므로 대략 1억 번의 과중한 CNN 연산이 필요하며 추론 속도도 느려진다.​​​2. Fast R-CNN (2015)    Fast R-CNN은 기존 방식의 비효율적인 부분들을 아래 세 방식으로 개선했다.​· ROI Projection 먼저 후보 영역의 이미지 파일 대신 좌표만을 저장한다. 이미지에 비해 좌표는 (x, y, w, h)의 매우 작은 용량을 차지하므로 디스크가 아닌 메모리에 올려두고 빠르게 활용할 수 있다는 장점이 있다. 이후 후보 영역들을 전부 CNN에 돌리는 대신 원본 이미지만 대표로 Feature를 추출한다. 이 때, CNN을 통과하면서 줄어드는 이미지의 사이즈에 비례해 모든 후보 영역들의 좌표 정보도 조정한다. (예를 들어, Pooling 연산 등에 의해 원본 이미지가 1/4로 줄어들었다면 좌표의 w와 h도 1/4로 줄인다.) 결과적으로 기존 2000번의 CNN 연산이 1번으로 줄어든다.​· ROI Pooling Warp를 사용하는 대신 임의 크기의 Feature를 받을 수 있는 Pooling 기법을 사용했다. 이는 SPPNet에 사용된 Spatial Pyramid Pooling의 특수 케이스로 1층만을 사용한다.​· Multi-task Loss Function Classifier와 Regressor를 따로 학습시키는 대신 두 Loss Function을 하나로 합쳐 동시 학습을 가능하게 했다.​​​3. Faster R-CNN (2015)    기존 방식인 Selective Search는 CPU에서 작동했기 때문에 2000개 가량의 후보를 뽑아내는 시간 자체가 오래 걸렸다. 하지만 Faster R-CNN은 RPN(Region Proposal Network)을 도입해 후보 영역 알고리즘마저도 CNN에 포함시켜 완전한 GPU 연산을 가능하게 했고 그로 인해 더 나은 속도 향상을 이루어냈다. ​ 먼저 기존 모델과는 알고리즘 순서가 반대가 되어 이미지를 CNN에 넣어 Feature를 추출한 뒤에 Region Proposal을 수행한다. RPN은 Sliding Window 방식으로 Feature를 순회하며 각 지점마다 K개의 Anchor를 뽑아낸다. Anchor란 후보 영역과 같은 의미로 Faster R-CNN에서 사용한 용어일 뿐이다. 그 결과 총 2000개 가량의 Anchor들이 추출되고 이는 Selective Search를 사용할 때와 비슷한 갯수이지만 연산 속도는 훨씬 빠르다는 장점이 있다.​​​4. Mask R-CNN (2017)    Mask R-CNN은 Detection 뿐만 아니라 Segmentation까지 가능하도록 확장된 모델이다. 이는 픽셀 단위로 클래스를 예측하는 정밀한 작업이기 때문에 기존 ROI Pooling을 개선한 RoIAlign을 도입했다.​ ROI Pooling의 문제점은 좌표가 줄어들 때 소수점이 반올림으로 무시된다는 것이었다. 대략적인 위치를 표현하는 Bounding Box Regression에는 큰 영향을 미치지는 않지만 Segmentation에는 치명적일 수 있다. RoIAlign은 Bilinear Interpolation으로 소수점을 보정시켜 더 정밀한 학습을 가능하게 했다. ​​​Conclusion R-CNNFast R-CNNFaster R-CNNMask R-CNN후보영역 알고리즘Selective SearchRPNPooling Method PoolingROI PoolingRoIAlignLoss FunctionSeperate LossMulti-task Loss ​ "
"Object Detection in 20 Years - 05, Speed-Up of Detection ",https://blog.naver.com/tory0405/222851516420,20220818,"[논문 원본] 첨부파일1905.05055.pdf파일 다운로드 ​ 물체 감지 속도는 오랫동안 중요했지만 어려운 도전 과제이기도 하다.  지난 20년 동안 “speed up of detection pipeline”, “speed up of detection engine”,  “speed up of numerical computation” 과같이 크게 3그룹으로 발전되어 왔음을 그림 12에서 보여 주고 있다. ​ Feature Map Shared Computation 객체 탐지의 다양한 계산 단계 중에서 feature extraction은 일반적으로 계산량이 많은데 이는 인접 인도우간의 겹침으로 인해 발생하는 것이고 sliding window의 경우 위치와 스케일 모두에서 계산 중복성이 나타나는데 이는 인접 스케일 간의 feature correlation이 발생하기 때문이다. 이렇듯 공간 계산 중복을 줄이기 위해 가장 일반적으로 사용되는 방법이 feature map을 공유하는 것으로 window을 sliding 하기 전 전체 이미지의 feature map을 한 번만 계산하는 방법을 이용하는 것이다. 예를 들어 HOG 보행자 감지의 속도를 향상시키기 위해 그림 13과 같이 전체 입력 이미지의 HOG Map을 사용하였으나 두 셀 사이에 작은 물체가 있으면 무시되는 단점이 존재한다. ​ Speed up of ClassifiersHOG나 DPM과 같이 기존의 sliding window 기반 감지기는 계산 복잡성이 낮기 때문에 비선형 분류보다는 선형 분류에 주로 사용되었고 kernle SVM 같은 경우 비선형 분류에 높은 정확도를 제시하였지만 동시에 높은 계산 오버헤드를 가지게 된다. 표준 비모수적 방법( standard non-parametric method)의 전통적인 kernel 방법은 고정된 계산 복잡성은 가지고 있지 않지만 훈련 세트가 커지면 감지 속도가 느려지는 단점이 존재한다. ​객체 감지에서  “model approximation”은 가장 일반적인 속도 개선 방법 중 하나이다.Reduced  Set  Vector는 커널 SVM에서 적은 수의 합성 벡터를 이용하여 속도를 개선한  “model approximation”라고 할 수 있다. 객체 감지에서 kernel SVM 속도를 개선할 수 있는 또 다른 방법은 decision boundary를 조각별 선형으로 근사하여 일정한 추론 시간을 얻는 방법이 있고 sparse encoding methods 등으로도 가속화할 수 있다. ​Cascaded DetectionCascaded Detection도 객체 감지에서 일반적으로 사용되는 기술이다. 간단한 계산을 이용하여 심플한 배경을 필터링한 다음 복잡한 계산을 통해 복잡한 window를 처리하는 방식으로 VJ 검출기가 대표적인  Cascaded Detection이라고 할 수 있다. 그 후 HOG, DPM도 Cascaded Detection를 사용하여 발전해 나갔다. 최근에는 딥러닝 기반 감지기에도 적용되었는데 특히, face detection,  pedestrian detection,   “small objects in large scenes”등에 적용되었다. ​Network Pruning and Quantification ""Network  pruning""과  ""network  quantification""은 CNN 모델의 속도를 높이기 위해 일반적을 사용되는 기술로 전자는 네트워크 구조 또는 가중치를 잘라 크기를 줄이고 후자는 활성화나 가중치의 코드 길이를 줄이는 것을 나타낸다. ​Network PruningNetwork Pruning에 대한 연구는 이미 1980년대로 거슬러 올라갈 수 있다. 당시  Y. LeCun은 다층 퍼셉트론 네트워크의 매개변수를 압축하기 위해 “optimal brain damage”을 제안했는데 중요하지 않는 가중치를 제거하기 위해 2차 도함수를 취함으로써 근사화 한 방법이다. 전통적인 Network Pruning은 단순히 중요하지 않는 가중치를 제거하기 때문에  컨볼루션 필터에 의해 약한 연결이 발생할 수 있어 CNN 모델을 경량화하는데 직접 적용할 수 없었으나 독립 가중치를 사용하는 대신 전체 layer를 제거함으로써 이 문제를 해결하였다. ​ Network Quantification네트워크 정량화 작업에 대한 최근 작업은 주로 network binarization에 중점을 두고 있다. network binarization은 활성화 함수 또는 가중치를 이진 함수( 0또는 1)로 정량화하고 부동 소수점 연산을 AND , OR, NOT 연산으로 변환하도록 하여  네트워크 속도를 빠르게 하는 것이다.  그리고 네트워크 이진화는 계산속도를 크게 높이고 네트워크 스토리지 사용률을 줄여 모바일 장치에도 쉽게 배포할 수 있는 장점이 있다. 구체적인 구현 방법으로는 least squares method에 의한 이진 변수로 컨볼루션을 근사화하는 방법이 있다. ​Network DistillationNetwork Distillation은  큰 규모의 네트워크( teacher net)에서 작은 네트워크(student net)으로 압축하는 일반적인 프레임워크이다. 최근에는 이 아이디어로 물체 감지 가속화에 사용되었는데 teatcher net이 student net에 교육을 지시하여 감지 속도를 높일 수 있도록 하였다. 또 다른 방법으로는 teatcher net과 student net 사이의 특징 거리를 최소화하도록 후보 영역을 변환하는 방법이 있다. ​Lightweight Network DesignCNN 기반 감지기의 속도를 높이는 마지막 방법은 off-the-shelf detection engines을 사용하는 대신 경량 네트워크를 직접 설계하는 것이다. 연구자들은 제한된 시간 비용에서 정확도를 얻기 위해 네트워크의 구성을 오랫동안 연구해 왔었고 “fewer channels and more layers” 설계 외에도 1) factorizing convolutions,  2) group convolution, 3) depth-wise separable convolution, 4) bottle-neck design, ,  5) neural architecture search와 같은 다른 몇 가지 접근도 하였다. ​Factorizing Convolutions​ ​컨볼류션을 인수분해 하는 것은 경량 CNN을 구축하는 가장 간단하고 직접적인 방법으로 2가지로 나눌 수 있다. ​첫 번째 방법은 그림 14 (b)와 같이 큰 컨 블루션 필터를 공간 차원의 작은 필터로  분해하는 것이다. 예를 들어 7*7 필터를 3개의 3*3필터로 분해할 경우 더 효율적이다.  또 다른 예는 k*k 필터를 k*1 필터와  1*k 필터로 분해하여 사용하면 큰 필터에 비해 효율적이다. 두 번째 방법은 큰 그룹의 컨볼루션을 해당 채널에서 두 개의 작은 그룹으로 분배하는 것이다. 그림 14 (c)와 같이  d""  filters + a nonlinear activation + another d filters ( d"" < d)를 이용하여 d filters 와  c channels의 feature map이 있는 컨볼루션 lay로 근사화하는 방식이다 ​Group Convolution그룹 컨 블루션은  특징 채널이 많은 다른 그룹으로 분할하여 컨볼루션 레이어의 매개 변수 수를 줄이는 것을 목표로 하고  각 그룹에 대해 독립적으로 컨볼루션하는것이다. 그림 14 (d)처럼 다른 구성을 변경하지 않고 피처 채널을 m 그룹으로 균등하게 나누면 이론적으로 컨볼루션의 계산 복잡도는 이전의 1/m로 감소한다. ​Depth-wise Separable ConvolutionDepth-wise separable convolution은 그림 14 (e)와 같이 최근에 널리 사용되는 경량 CNN 구축 방법이다. 만약 d filters와 c channels의 feature map이 있고, 필터의 크기가 k*k라고 가정한다면 Depth-wise Separable Convolution은 모든 k*k*c 필터를 k*k*1 크기의 c 개 슬라이스로 분할한 다음 필터의 각 슬라이스로 각 채널에서 개별적으로 컨볼루션을 수행하고 마지막에 최종 출력이 d 개의 채널을 자지도 록 차원 변환을 수행한다 ​Bottle-neck Designbottleneck layer은 이전 계층에 비해 노드가 거의 없는 것이 특징이다. 이는 deep autoen coders에서 일반적으로 사용되는 감소된 차원으로 입력되는 데이터의 효율적인 인코딩을 학습하는 데 사용할 수 있다. 최근에는 Bottle-neck Design이 경량 네트워크 설계에 널리 사용되었는데 탐지기의 맨 처음부터 계산량을 줄이기 위해 탐지기의 입력 layer를 압축하는 것이 일반적인 접근 방식이 하나 있고 다른 방법으로는 감지 layer의  출력을 압축하여 특징 맵을 더 얇게 만드어 다음 감지 layer에서 좀 더 효율적으로 처리하게 하는 방법이 있다. ​Neural Architecture Search최근에는 전문가의 경험과 지식에 크게 의존하는 대신  neural architecture search (NAS)에 의해 자동으로 네트워크 아키텍처를 설계한다. NAS는 대규모 이미지 분류, 객체 감지, 이미지 분할에 적용되었고 최근에는 검색 과정에서 예측 정확도와 계산 복잡성에 대한 제약이 모두 고려되는 경량 네트워크 설계에서도 유효한 결과를 보여 주고 있다. ​ Numerical Acceleration 객체 감지에 자주 사용되는 4가지 중요한 numerical acceleration인  1) speed up with the integral image, 2) speed up in the frequency domain, 3) vector quantization,  4) reduced rank approximation에 대해 알아보도록 하겠다. ​Speed Up with Integral Image적분 영상은 영상 처리에 방법에서 이미지 하위 영역에 대한 합을 빠르게 계산할 수 있는 중요한 방법 중에 하나이다.  적분 영상의 본질은 signal processing에서 컨볼루션의  integraldifferential separability이 가능하다는 것이다. 만약 dg(x)/dx가 sparse signal이라고 한다면 오른쪽 방정식을 토해 컨볼루션이 가속화될 수 있다. 적분 영상은 color histogram, gradient histogram 등에도 속도를 높이는데 사용할 수 있다. 적분 HOG MAP은 기존 적분 이미지에 픽셀 값을 누적하는 대신 그림 15와 같이 이미지 기울기 방향을 누적하여 작은 계산량으로 임의의 위치와 크기를 확인할 수 있다. 이를 통해 HOG MAP은 보행자 감지에 있어 정확도를 잃지 않고 수십 배의 가속도를 이루게 되었다.  2009년 후반에는 P. Dollar는 Integral Channel Features (ICF)라는 새로운 유형의 이미지 특정을 제안했는데 이는 적분 영상보다 좀 더 일반화되어 보행자 검출에 성공함으로써 거의 실시간에 가까운 탐지 속도와 정확도를 달성하게 되었다. ​Speed Up in Frequency Domain선형 검출기은 특징 맵과 검출기의 가중치 사이의 window-wise inner product으로 볼 수 있기 때문에 컨볼루셔을 대신할 수 있다.  컨볼루션은 여러 가지 방법으로 가속화할 수 있는데 특히 Fourier transform은 큰 필터의 속도를 높이는데 매우 실용적인 방법 중 하나이다. 주파수 영역에서 컨볼루션을 가속화하기 위한 이론적 기반은 두 신호에 대해 Fourier transform을 통해 가능하다는 것이다.  여기서 F는 Fourier transform이고 I와 W는 입력 이미지와 filter을 의미하고 *은 컨볼루션을 의미한다.  또한 해당 식은 Fourier Transform (FFT) 와  the Inverse Fast Fourier Transform (IFFT)를 통해 가 속화할 수도 있다. FFT와 IFFT는 CNN 모델과 일부 오래된 선형 객체 감지기의 속도를 높이는데 자주 사용되어 10배 이상의 속도 향상을 이루었다. 그림 16은 주파수 영역에서 선형 물체를 감지하는(HOG, DPM 등) 속도를 높이는 표준 파이프라인을 보여준다.  ​​​​​​Vector Quantization벡터 양자화(Vector Quantization, VQ)는 작은 프로토타입 벡터 세트로 대규모 데이터 그룹의 분포를 근사화하는 것을 목표로 하는 신호 처리의 고전적인 방식으로 데이터 압축 및 객체 감지에서 내적 연산을 가속화하는데 사용된다. 예를 들어 VQ를 사용하면 HOG 히스토그램을 그룹화하고 프로토타입 히스토그래 벡터 섹터로 정량화할 수 있게 된다. 그런 다음 감지 단계에서  table-look-up operation을 통해 간단하게 특징 벡터와 감지 가중치 사이의 내부 생성을 할 수 있다. 이를 통해 부동소수점 곱셈과 나눗셈을 할 필요가 없으므로 SVM과 같은 검출기 가속화를 이룰 수 있게 된다.  ​Reduced Rank Approximation심층 네트워크에서  fully-connected layerdms 두행렬의 곱이다. 그렇기 때문에  매개변수 행렬이 크게 되면 검출기의 계산이 많아진다. 예를 들면 Fast RCNN 검출기에서 forward pass time의 거의 절반이  fully connected layers를 계산하는 데 사용된다. 감소된 rank approximation는 행렬 곱셈을 가속화하는 방법이고 이때  행렬 W는 하위 분해를 목표로 한다. 여기서 U는 W의 첫 번째 t 개의 특이 벡터로 구성된 u*t 행렬이고 Σ t는 W의 상위 t 개의 특이값을 포함하는 t * t 대각행렬인데  Truncated SVD으로 불린다.  Truncated SVD은 Fast RCNN 검출기를 가속화하는데 사용되어 2배 이상의 속도를 향상시켰다.  "
Object Detection 계보를 정리한 사람! 멋져. ,https://blog.naver.com/tlqordl89/221757604373,20200102,"#object detection#RCNN #fastRCNN #FasterRCNN #maskRCNN​https://github.com/hoya012/deep_learning_object_detection hoya012/deep_learning_object_detectionA paper list of object detection using deep learning. - hoya012/deep_learning_object_detectiongithub.com ​           ​성능까지 기록을 해뒀네요. 엄청난 분입니다. 저 자료가 삭제될 것을 대비한 캡쳐입니다. 모든 것은 저분이 해두신 것이네요.​2014  [R-CNN] Rich feature hierarchies for accurate object detection and semantic segmentation | [CVPR' 14] |[pdf][official code - caffe][OverFeat] OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks | [ICLR' 14] |[pdf] [official code - torch][MultiBox] Scalable Object Detection using Deep Neural Networks | [CVPR' 14] |[pdf][SPP-Net] Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition | [ECCV' 14] |[pdf][official code - caffe] [unofficial code - keras] [unofficial code - tensorflow]2015  Improving Object Detection with Deep Convolutional Networks via Bayesian Optimization and Structured Prediction | [CVPR' 15] |[pdf] [official code - matlab][MR-CNN] Object detection via a multi-region & semantic segmentation-aware CNN model | [ICCV' 15] |[pdf][official code - caffe][DeepBox] DeepBox: Learning Objectness with Convolutional Networks | [ICCV' 15] |[pdf] [official code - caffe][AttentionNet] AttentionNet: Aggregating Weak Directions for Accurate Object Detection | [ICCV' 15] |[pdf][Fast R-CNN] Fast R-CNN | [ICCV' 15] |[pdf] [official code - caffe][DeepProposal] DeepProposal: Hunting Objects by Cascading Deep Convolutional Layers | [ICCV' 15] |[pdf][official code - matconvnet][Faster R-CNN, RPN] Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks | [NIPS' 15] |[pdf] [official code - caffe] [unofficial code - tensorflow] [unofficial code - pytorch]2016  [YOLO v1] You Only Look Once: Unified, Real-Time Object Detection | [CVPR' 16] |[pdf] [official code - c][G-CNN] G-CNN: an Iterative Grid Based Object Detector | [CVPR' 16] |[pdf][AZNet] Adaptive Object Detection Using Adjacency and Zoom Prediction | [CVPR' 16] |[pdf][ION] Inside-Outside Net: Detecting Objects in Context with Skip Pooling and Recurrent Neural Networks | [CVPR' 16] |[pdf][HyperNet] HyperNet: Towards Accurate Region Proposal Generation and Joint Object Detection | [CVPR' 16] |[pdf][OHEM] Training Region-based Object Detectors with Online Hard Example Mining | [CVPR' 16] |[pdf] [official code - caffe][CRAPF] CRAFT Objects from Images | [CVPR' 16] |[pdf] [official code - caffe][MPN] A MultiPath Network for Object Detection | [BMVC' 16] |[pdf] [official code - torch][SSD] SSD: Single Shot MultiBox Detector | [ECCV' 16] |[pdf] [official code - caffe] [unofficial code - tensorflow] [unofficial code - pytorch][GBDNet] Crafting GBD-Net for Object Detection | [ECCV' 16] |[pdf] [official code - caffe][CPF] Contextual Priming and Feedback for Faster R-CNN | [ECCV' 16] |[pdf][MS-CNN] A Unified Multi-scale Deep Convolutional Neural Network for Fast Object Detection | [ECCV' 16] |[pdf][official code - caffe][R-FCN] R-FCN: Object Detection via Region-based Fully Convolutional Networks | [NIPS' 16] |[pdf] [official code - caffe] [unofficial code - caffe][PVANET] PVANET: Deep but Lightweight Neural Networks for Real-time Object Detection | [NIPSW' 16] |[pdf][official code - caffe][DeepID-Net] DeepID-Net: Deformable Deep Convolutional Neural Networks for Object Detection | [PAMI' 16] |[pdf][NoC] Object Detection Networks on Convolutional Feature Maps | [TPAMI' 16] |[pdf]2017  [DSSD] DSSD : Deconvolutional Single Shot Detector | [arXiv' 17] |[pdf] [official code - caffe][TDM] Beyond Skip Connections: Top-Down Modulation for Object Detection | [CVPR' 17] |[pdf][FPN] Feature Pyramid Networks for Object Detection | [CVPR' 17] |[pdf] [unofficial code - caffe][YOLO v2] YOLO9000: Better, Faster, Stronger | [CVPR' 17] |[pdf] [official code - c] [unofficial code - caffe] [unofficial code - tensorflow] [unofficial code - tensorflow] [unofficial code - pytorch][RON] RON: Reverse Connection with Objectness Prior Networks for Object Detection | [CVPR' 17] |[pdf][official code - caffe] [unofficial code - tensorflow][RSA] Recurrent Scale Approximation for Object Detection in CNN | | [ICCV' 17] |[pdf] [official code - caffe][DCN] Deformable Convolutional Networks | [ICCV' 17] |[pdf] [official code - mxnet] [unofficial code - tensorflow] [unofficial code - pytorch][DeNet] DeNet: Scalable Real-time Object Detection with Directed Sparse Sampling | [ICCV' 17] |[pdf] [official code - theano][CoupleNet] CoupleNet: Coupling Global Structure with Local Parts for Object Detection | [ICCV' 17] |[pdf][official code - caffe][RetinaNet] Focal Loss for Dense Object Detection | [ICCV' 17] |[pdf] [official code - keras] [unofficial code - pytorch] [unofficial code - mxnet] [unofficial code - tensorflow][Mask R-CNN] Mask R-CNN | [ICCV' 17] |[pdf] [official code - caffe2] [unofficial code - tensorflow][unofficial code - tensorflow] [unofficial code - pytorch][DSOD] DSOD: Learning Deeply Supervised Object Detectors from Scratch | [ICCV' 17] |[pdf] [official code - caffe] [unofficial code - pytorch][SMN] Spatial Memory for Context Reasoning in Object Detection | [ICCV' 17] |[pdf][Light-Head R-CNN] Light-Head R-CNN: In Defense of Two-Stage Object Detector | [arXiv' 17] |[pdf] [official code - tensorflow][Soft-NMS] Improving Object Detection With One Line of Code | [ICCV' 17] |[pdf] [official code - caffe]2018  [YOLO v3] YOLOv3: An Incremental Improvement | [arXiv' 18] |[pdf] [official code - c] [unofficial code - pytorch] [unofficial code - pytorch] [unofficial code - keras] [unofficial code - tensorflow][ZIP] Zoom Out-and-In Network with Recursive Training for Object Proposal | [IJCV' 18] |[pdf] [official code - caffe][SIN] Structure Inference Net: Object Detection Using Scene-Level Context and Instance-Level Relationships | [CVPR' 18] |[pdf] [official code - tensorflow][STDN] Scale-Transferrable Object Detection | [CVPR' 18] |[pdf][RefineDet] Single-Shot Refinement Neural Network for Object Detection | [CVPR' 18] |[pdf] [official code - caffe] [unofficial code - chainer] [unofficial code - pytorch][MegDet] MegDet: A Large Mini-Batch Object Detector | [CVPR' 18] |[pdf][DA Faster R-CNN] Domain Adaptive Faster R-CNN for Object Detection in the Wild | [CVPR' 18] |[pdf] [official code - caffe][SNIP] An Analysis of Scale Invariance in Object Detection – SNIP | [CVPR' 18] |[pdf][Relation-Network] Relation Networks for Object Detection | [CVPR' 18] |[pdf] [official code - mxnet][Cascade R-CNN] Cascade R-CNN: Delving into High Quality Object Detection | [CVPR' 18] |[pdf] [official code - caffe]Finding Tiny Faces in the Wild with Generative Adversarial Network | [CVPR' 18] |[pdf][MLKP] Multi-scale Location-aware Kernel Representation for Object Detection | [CVPR' 18] |[pdf] [official code - caffe]Cross-Domain Weakly-Supervised Object Detection through Progressive Domain Adaptation | [CVPR' 18] |[pdf][official code - chainer][Fitness NMS] Improving Object Localization with Fitness NMS and Bounded IoU Loss | [CVPR' 18] |[pdf][STDnet] STDnet: A ConvNet for Small Target Detection | [BMVC' 18] |[pdf][RFBNet] Receptive Field Block Net for Accurate and Fast Object Detection | [ECCV' 18] |[pdf] [official code - pytorch]Zero-Annotation Object Detection with Web Knowledge Transfer | [ECCV' 18] |[pdf][CornerNet] CornerNet: Detecting Objects as Paired Keypoints | [ECCV' 18] |[pdf] [official code - pytorch][PFPNet] Parallel Feature Pyramid Network for Object Detection | [ECCV' 18] |[pdf][Softer-NMS] Softer-NMS: Rethinking Bounding Box Regression for Accurate Object Detection | [arXiv' 18] |[pdf][ShapeShifter] ShapeShifter: Robust Physical Adversarial Attack on Faster R-CNN Object Detector | [ECML-PKDD' 18] |[pdf] [official code - tensorflow][Pelee] Pelee: A Real-Time Object Detection System on Mobile Devices | [NIPS' 18] |[pdf] [official code - caffe][HKRM] Hybrid Knowledge Routed Modules for Large-scale Object Detection | [NIPS' 18] |[pdf][MetaAnchor] MetaAnchor: Learning to Detect Objects with Customized Anchors | [NIPS' 18] |[pdf][SNIPER] SNIPER: Efficient Multi-Scale Training | [NIPS' 18] |[pdf]2019  [M2Det] M2Det: A Single-Shot Object Detector based on Multi-Level Feature Pyramid Network | [AAAI' 19] |[pdf][official code - pytorch][R-DAD] Object Detection based on Region Decomposition and Assembly | [AAAI' 19] |[pdf][CAMOU] CAMOU: Learning Physical Vehicle Camouflages to Adversarially Attack Detectors in the Wild | [ICLR' 19]|[pdf]Feature Intertwiner for Object Detection | [ICLR' 19] |[pdf][GIoU] Generalized Intersection over Union: A Metric and A Loss for Bounding Box Regression | [CVPR' 19] |[pdf]Automatic adaptation of object detectors to new domains using self-training | [CVPR' 19] |[pdf][Libra R-CNN] Libra R-CNN: Balanced Learning for Object Detection | [CVPR' 19] |[pdf][FSAF] Feature Selective Anchor-Free Module for Single-Shot Object Detection | [CVPR' 19] |[pdf][ExtremeNet] Bottom-up Object Detection by Grouping Extreme and Center Points | [CVPR' 19] |[pdf] | [official code - pytorch][C-MIL] C-MIL: Continuation Multiple Instance Learning for Weakly Supervised Object Detection | [CVPR' 19] |[pdf] | [official code - torch][ScratchDet] ScratchDet: Training Single-Shot Object Detectors from Scratch | [CVPR' 19] |[pdf]Bounding Box Regression with Uncertainty for Accurate Object Detection | [CVPR' 19] |[pdf] | [official code - caffe2]Activity Driven Weakly Supervised Object Detection | [CVPR' 19] |[pdf]Towards Accurate One-Stage Object Detection with AP-Loss | [CVPR' 19] |[pdf]Strong-Weak Distribution Alignment for Adaptive Object Detection | [CVPR' 19] |[pdf] | [official code - pytorch][NAS-FPN] NAS-FPN: Learning Scalable Feature Pyramid Architecture for Object Detection | [CVPR' 19] |[pdf][Adaptive NMS] Adaptive NMS: Refining Pedestrian Detection in a Crowd | [CVPR' 19] |[pdf]Point in, Box out: Beyond Counting Persons in Crowds | [CVPR' 19] |[pdf]Locating Objects Without Bounding Boxes | [CVPR' 19] |[pdf]Sampling Techniques for Large-Scale Object Detection from Sparsely Annotated Objects | [CVPR' 19] |[pdf]Towards Universal Object Detection by Domain Attention | [CVPR' 19] |[pdf]Exploring the Bounds of the Utility of Context for Object Detection | [CVPR' 19] |[pdf]What Object Should I Use? - Task Driven Object Detection | [CVPR' 19] |[pdf]Dissimilarity Coefficient based Weakly Supervised Object Detection | [CVPR' 19] |[pdf]Adapting Object Detectors via Selective Cross-Domain Alignment | [CVPR' 19] |[pdf]Fully Quantized Network for Object Detection | [CVPR' 19] |[pdf]Distilling Object Detectors with Fine-grained Feature Imitation | [CVPR' 19] |[pdf]Multi-task Self-Supervised Object Detection via Recycling of Bounding Box Annotations | [CVPR' 19] |[pdf][Reasoning-RCNN] Reasoning-RCNN: Unifying Adaptive Global Reasoning into Large-scale Object Detection | [CVPR' 19] |[pdf]Arbitrary Shape Scene Text Detection with Adaptive Text Region Representation | [CVPR' 19] |[pdf]Assisted Excitation of Activations: A Learning Technique to Improve Object Detectors | [CVPR' 19] |[pdf]Spatial-aware Graph Relation Network for Large-scale Object Detection | [CVPR' 19] |[pdf][MaxpoolNMS] MaxpoolNMS: Getting Rid of NMS Bottlenecks in Two-Stage Object Detectors | [CVPR' 19] |[pdf]You reap what you sow: Generating High Precision Object Proposals for Weakly-supervised Object Detection | [CVPR' 19] |[pdf]Object detection with location-aware deformable convolution and backward attention filtering | [CVPR' 19] |[pdf]Diversify and Match: A Domain Adaptive Representation Learning Paradigm for Object Detection | [CVPR' 19] |[pdf][GFR] Improving Object Detection from Scratch via Gated Feature Reuse | [BMVC' 19] |[pdf] | [official code - pytorch][Cascade RetinaNet] Cascade RetinaNet: Maintaining Consistency for Single-Stage Object Detection | [BMVC' 19] |[pdf]Soft Sampling for Robust Object Detection | [BMVC' 19] |[pdf]Multi-adversarial Faster-RCNN for Unrestricted Object Detection | [ICCV' 19] |[pdf]Towards Adversarially Robust Object Detection | [ICCV' 19] |[pdf]A Robust Learning Approach to Domain Adaptive Object Detection | [ICCV' 19] |[pdf]A Delay Metric for Video Object Detection: What Average Precision Fails to Tell | [ICCV' 19] |[pdf]Delving Into Robust Object Detection From Unmanned Aerial Vehicles: A Deep Nuisance Disentanglement Approach | [ICCV' 19] |[pdf]Employing Deep Part-Object Relationships for Salient Object Detection | [ICCV' 19] |[pdf]Learning Rich Features at High-Speed for Single-Shot Object Detection | [ICCV' 19] |[pdf]Structured Modeling of Joint Deep Feature and Prediction Refinement for Salient Object Detection | [ICCV' 19] |[pdf]Selectivity or Invariance: Boundary-Aware Salient Object Detection | [ICCV' 19] |[pdf]Progressive Sparse Local Attention for Video Object Detection | [ICCV' 19] |[pdf]Minimum Delay Object Detection From Video | [ICCV' 19] |[pdf]Towards Interpretable Object Detection by Unfolding Latent Structures | [ICCV' 19] |[pdf]Scaling Object Detection by Transferring Classification Weights | [ICCV' 19] |[pdf][TridentNet] Scale-Aware Trident Networks for Object Detection | [ICCV' 19] |[pdf]Generative Modeling for Small-Data Object Detection | [ICCV' 19] |[pdf]Transductive Learning for Zero-Shot Object Detection | [ICCV' 19] |[pdf]Self-Training and Adversarial Background Regularization for Unsupervised Domain Adaptive One-Stage Object Detection | [ICCV' 19] |[pdf][CenterNet] CenterNet: Keypoint Triplets for Object Detection | [ICCV' 19] |[pdf][DAFS] Dynamic Anchor Feature Selection for Single-Shot Object Detection | [ICCV' 19] |[pdf][Auto-FPN] Auto-FPN: Automatic Network Architecture Adaptation for Object Detection Beyond Classification | [ICCV' 19] |[pdf]Multi-Adversarial Faster-RCNN for Unrestricted Object Detection | [ICCV' 19] |[pdf]Object Guided External Memory Network for Video Object Detection | [ICCV' 19] |[pdf][ThunderNet] ThunderNet: Towards Real-Time Generic Object Detection on Mobile Devices | [ICCV' 19] |[pdf][RDN] Relation Distillation Networks for Video Object Detection | [ICCV' 19] |[pdf][MMNet] Fast Object Detection in Compressed Video | [ICCV' 19] |[pdf]Towards High-Resolution Salient Object Detection | [ICCV' 19] |[pdf][SCAN] Stacked Cross Refinement Network for Edge-Aware Salient Object Detection | [ICCV' 19] |[official code] |[pdf]Motion Guided Attention for Video Salient Object Detection | [ICCV' 19] |[pdf]Semi-Supervised Video Salient Object Detection Using Pseudo-Labels | [ICCV' 19] |[pdf]Learning to Rank Proposals for Object Detection | [ICCV' 19] |[pdf][WSOD2] WSOD2: Learning Bottom-Up and Top-Down Objectness Distillation for Weakly-Supervised Object Detection | [ICCV' 19] |[pdf][ClusDet] Clustered Object Detection in Aerial Images | [ICCV' 19] |[pdf]Towards Precise End-to-End Weakly Supervised Object Detection Network | [ICCV' 19] |[pdf]Few-Shot Object Detection via Feature Reweighting | [ICCV' 19] |[pdf][Objects365] Objects365: A Large-Scale, High-Quality Dataset for Object Detection | [ICCV' 19] |[pdf][EGNet] EGNet: Edge Guidance Network for Salient Object Detection | [ICCV' 19] |[pdf]Optimizing the F-Measure for Threshold-Free Salient Object Detection | [ICCV' 19] |[pdf]Sequence Level Semantics Aggregation for Video Object Detection | [ICCV' 19] |[pdf][NOTE-RCNN] NOTE-RCNN: NOise Tolerant Ensemble RCNN for Semi-Supervised Object Detection | [ICCV' 19] |[pdf]Enriched Feature Guided Refinement Network for Object Detection | [ICCV' 19] |[pdf][POD] POD: Practical Object Detection With Scale-Sensitive Network | [ICCV' 19] |[pdf][FCOS] FCOS: Fully Convolutional One-Stage Object Detection | [ICCV' 19] |[pdf][RepPoints] RepPoints: Point Set Representation for Object Detection | [ICCV' 19] |[pdf]Better to Follow, Follow to Be Better: Towards Precise Supervision of Feature Super-Resolution for Small Object Detection | [ICCV' 19] |[pdf]Weakly Supervised Object Detection With Segmentation Collaboration | [ICCV' 19] |[pdf]Leveraging Long-Range Temporal Relationships Between Proposals for Video Object Detection | [ICCV' 19] |[pdf]Detecting 11K Classes: Large Scale Object Detection Without Fine-Grained Bounding Boxes | [ICCV' 19] |[pdf][C-MIDN] C-MIDN: Coupled Multiple Instance Detection Network With Segmentation Guidance for Weakly Supervised Object Detection | [ICCV' 19] |[pdf]Meta-Learning to Detect Rare Objects | [ICCV' 19] |[pdf][Cap2Det] Cap2Det: Learning to Amplify Weak Caption Supervision for Object Detection | [ICCV' 19] |[pdf][Gaussian YOLOv3] Gaussian YOLOv3: An Accurate and Fast Object Detector using Localization Uncertainty for Autonomous Driving | [ICCV' 19] |[pdf] [official code - c][FreeAnchor] FreeAnchor: Learning to Match Anchors for Visual Object Detection | [NeurIPS' 19] |[pdf]Memory-oriented Decoder for Light Field Salient Object Detection | [NeurIPS' 19] |[pdf]One-Shot Object Detection with Co-Attention and Co-Excitation | [NeurIPS' 19] |[pdf][DetNAS] DetNAS: Backbone Search for Object Detection | [NeurIPS' 19] |[pdf]Consistency-based Semi-supervised Learning for Object detection | [NeurIPS' 19] |[pdf][NATS] Efficient Neural Architecture Transformation Searchin Channel-Level for Object Detection | [NeurIPS' 19] |[pdf][AA] Learning Data Augmentation Strategies for Object Detection | [arXiv' 19] |[pdf][EfficientDet] EfficientDet: Scalable and Efficient Object Detection | [arXiv' 19] |[pdf]2020  [Spiking-YOLO] Spiking-YOLO: Spiking Neural Network for Real-time Object Detection | [AAAI' 20] |[pdf]Tell Me What They're Holding: Weakly-supervised Object Detection with Transferable Knowledge from Human-object Interaction | [AAAI' 20] |[pdf]Distance-IoU Loss: Faster and Better Learning for Bounding Box Regression | [AAAI' 20] |[pdf] "
ViLD: Open-Vocabulary Object Detection Via Vision and Language Knowledge Distillation ,https://blog.naver.com/hanchaa/222701805166,20220415,"Open-Vocabulary Object Detection Via Vision and Language Knowledge Distillation, Gu et al., 2021.arxiv: https://arxiv.org/abs/2104.13921 Open-vocabulary Object Detection via Vision and Language Knowledge DistillationWe aim at advancing open-vocabulary object detection, which detects objects described by arbitrary text inputs. The fundamental challenge is the availability of training data. Existing object detection datasets only contain hundreds of categories, and it is costly to scale further. To overcome this ...arxiv.org 요약pre train된 CLIP의 지식을 이용해서 open vocabulary object detection을 가능하게 한다.이미지를 backbone과 RPN을 통과시켜 ROI embedding을 얻으면, 이를 CLIP의 text encoder를 통과한 label의 text embedding과 유사도가 최대가 되도록 학습시킨다.​Introduction 다음 그림과 같이 학습시에 주어진 toy와 같은 base category에 대해서만 객체 탐지를 하는 것을 뛰어 넘어 toy elephant와 같은 흔하지 않는 카테고리를 탐지하는 모델을 만드는 것이 가능한가에 대해 보여주는 논문이다.현재 존재하는 object detection 알고리즘들은 데이터셋에서 주어지는 카테고리에 대해서만 탐지하도록 학습하는데, 탐지가 가능한 객체의 목록을 늘리기 위해서는 라벨링 된 카테고리의 수를 늘려주어야 한다.최근에 LVIS(Gupta et al., 2019)dhk rkxdl 1,203개의 카테고리를 가지는 객체 탐지 데이터셋을 만들었지만, 모든 카테고리에 대해서 충분한 학습 데이터를 얻는 것은 매우 힘든일이다.​하지만 image-text 쌍의 경우 인터넷에 흔히 있는데, CLIP이라는 논문이 4억개의 image-text 쌍을 이용해 학습시켰고 30개가 넘는 classification task에 대해 좋은 성능을 보여주었다.여기서 사전 학습 된 text encoder가 어떤 임의의 카테고리로 zero-shot transfer를 가능하게 하는 중요한 요소이다.이 논문에서는 CLIP의 사전학습된 지식을 이용하여 open-vocabulary detection을 학습시킨다고 한다.​저자들은 R-CNN 스타일의 접근 방법을 이용하였는데, open-vocabulary detection을 두가지 문제로 분리하였다고 한다.1) generalized object proposal2) open-vocabulary image classification​처음에는 RPN을 base category들에 대해 학습시킨 후, object proposal에 대해 crop한 이미지를 open-vocabulary image classification 모델에 넣었다고 한다.LVIS에 대해 벤치마크 결과 이미 성능이 supervised 모델을 뛰어넘었지만 object proposal들을 classification 모델에 하나하나 넣어야했기 때문에 매우 느린 inference 속도를 보여주었다고 한다.​이를 해결하기 위해 ViLD를 제안하였는데, 이 모델은 ViLD-text와 ViLD-image 두개의 컴포넌트들로 이루어져 있다.ViLD-text: 카테고리의 이름을 CLIP과 같은 사전 학습된 text encoder에 넣은 후, 유추된 text ebmbedding들을 인식된 영역에 대해 분류하는데 사용된다.ViLD-image: object proposal들을 사전 학습된 image encoder에 넣어 image embedding들을 얻어낸 후, object detector에서 탐지된 박스들의 region embedding들을 이 image embedding들과 정렬한다. ViLD-text와는 다르게 novel category들에 대해서도 knowledge distillation을 수행한다.​MethodNotationsCB: Base categoriesCN: Novel categoriesT(·): Text encoder in the pretrained open-vocabulary image classification modelV(·): Image encoder in the pretrained open-vocabulary image classification model​· Object Proposals for Novel Categoriesopen-vocabulary detection의 첫 번째 문제는 novel object들에 대한 localize였는데, Mask R-CNN과 같은 standrad two-stage object detector의 class-specific한 localization module을 class-agnostic한 module로 수정하였다고 한다.Faster R-CNN을 보면 roi pooling 후, roi head를 지나고 regression head에서 클래스 수 만큼의 좌표를 만들어 주는데, 이를 클래스와 관계 없이 동일한 좌표를 출력하게 수정하였다.​· Open-Vocabulary Detection with Cropped Regionsobject 후보들이 localize된 후, 사전학습된 open-vocabulary image classifier를 재사용해 각 영역에 대해 분류하는 법을 제안하였다.​Image embeddings: proposal network는 CB에 대해서 학습한 후, region proposal들을 offline으로 추출하였다.이 때 offline이란건 gradient의 계산에 영향을 주지 않는 외부 모듈을 의미하는 것 같다.그러고 난 후 사전 학습된 이미지 인코더 V에 이미지 임베딩을 계산하기 위해 region proposal에 맞게 crop한 이미지를 넣어주었다. region proposal의 1x, 1.5x로 자른 이미지들의 이미지 임베딩들을 앙상블 또한 해주었다고 하며, 1.5x crop이 더 많은 맥락을 제공하기 때문이라고 한다.앙상블된 임베딩들은 unit norm으로 renormalize 하였다. ​Text embeddings: ""a photo of {category} in the scene""과 같은 prompt template들을 이용해 만든 category text들의 text embedding들을 text encoder T를 이용해 offline으로 생성하였다.여기서도 마찬가지로 여러 prompt template들을 앙상블하였다.​마지막으로 image와 text embedding들 간의 cosine similarity들을 계산하였다.또한 softmax를 이용하였으며, per-class NMS를 이용해 최종 결과를 얻었다.이 방법의 문제는 모든 crop된 영역들이 V로 제공되어야 했기에 추론 속도가 느렸다고 한다.​· ViLD: Vision and Language Knowledge Distillation 앞서 언급한 느린 추론 속도를 해결하기 위해 ViLD를 제안하였다.ViLD는 2-stage detector에서 각각의 proposal r을 나타내기 위해 region embedding들을 학습한다.region embedding을 R(φ(I), r)과 같이 나타내는데, φ(·)는 모델의 backbone을 의미하며 R(·)은 region embedding을 형성하는 lightweight head이다.​Replacing classifier with text embeddingsViLD-text는 region embedding들이 text embedding들을 이용해 분류될 수 있도록 학습시키는 것을 목표로 한다.Fig. 3(a)가 기본적인 detector인데 여기에 나와있는 학습 가능한 classifier를 이전 챕터에서 나온 text embedding들로 대체한다. 이때 T(CB)들만 학습에 사용되었다.특이한게 CB와 매칭되지 않는 proposal들은 모두 백그라운드 카테고리로 할당해주었다고 하는데, novel class에 해당하는 proposal들을 모두 백그라운드로 할당하면 novel 카테고리에 대한 region embedding이 잘 generalize될지 모르겠다.이를 해결하기 위해 background categry의 text embedding인 ebg를 학습 가능하게 하였다곤 한다.이후 region embedding인 R(φ(I), r)과 T(CB), ebg간의 cosine similarity를 계산한 후 temperature tau를 이용한 softmax activatoin을 적용시켜 cross entropy loss를 구하였다고 한다. 2-stage region proposal network의 첫번째 stage region를 학습시키기 위해 regision proposal r을 online으로 추출한 뒤 ViLD-text와 같이 scratch로 부터 학습시켰다고 한다.​Distilling image embeddingsViLD-image는 teacher image encoder V의 지식을 student detector에 전달하는 것이 목적이다.이를 위해 offline으로 추출한 M개의 region embedding을 region proposal에 맞게 crop한 이미지의 embedding과 정렬시켰다.여기서 ViLD-text와 다른 점은 CB 뿐만 아니라 CN에 속한 객체에 대해서도 distillation을 수행해주었다.loss로는 L1 loss를 이용해 region과 image embedding 사이의 거리를 최소화하였다. ViLD의 최종 training loss는 두 loss의 weighted sum 형태이다. fig. 3(d)는 전체적인 모델의 모습을 보여주는데, ViLD-image distillation은 학습시에만 이루어진다.추론 과정에는 ViLD-image, ViLD-text, ViLD 모두 같은 text embedding들을 detection classifier로 사용하고 같은 구조의 open-vocabulary detection을 이용한다.​· Model Ensembling이 챕터에서는 base와 novel category들에 대해 최선의 탐지 성능을 보여주기 위해 model ensembling을 수행하였다고 한다.​우선 ViLD-text detector의 prediction과 open-vocabulary image classification 모델을 결합하였는데, ViLD-image가 teacher model의 예측을 approximate하므로 teacher model을 바로 사용하는 것이 성능을 높힐 것이라고 생각했다고 한다.​학습된 ViLD-text detector를 이용해서 top k개의 candidate region들과 그들의 confidence score들을 구하였는데, pi,ViLD-text를 proposal이 카테고리 i에 속해 있을 confidence score를 나타낸다.그런 다음 proposal에 맞게 자른 이미지를 open-vocabulary classification model에 넣어 teacher의 confidence score인 pi,cls를 획득하였다고 한다.두 모델이 base와 novel 카테고리에 대해 다른 성능을 가지므로 다음과 같은 기하평균을 사용하였다. 이 방식의 경우 Sec. 3.2만큼이나 비슷하게 느린 추론 속도를 보여주었다고 한다.​다음으로 또 다른 앙상블 방법을 제안하였는데, ViLD-text의 cross entropy loss와 ViLD-image의 L1 distillation이 같은 region embedding에 이루어지므로 두 loss 사이의 충돌이 일어날 수도 있다.따라서 저자들은 ViLD-text와 ViLD-image를 위한 region head를 따로 만들었다고 한다.text embedding이 두 region embedding에 사용되어 pi, ViLD-text와 pi, ViLD-image를 계산하였고, 위의 식과 같이 앙상블 해주었다고 한다. "
 Object Detection - HOG 알고리즘 개념편 (1) ,https://blog.naver.com/justbabo/222944118034,20221202,"출처 :  Object Detection - HOG 알고.. : 네이버블로그 (naver.com)0. Intro...지난 시간에는 Haar-like filter를 가지고 이미지가 haar feature를 얼만큼 가지고 있는 지 계산하여, 얼굴을 검출하는 알고리즘비올라-존스 face detection 을 공부해보았다.​이번 시간에는 이미지에서 gradient 개념을 이용하여 feature vector를 정의하고, SVM 알고리즘을 이용하여 보행자 위치를 검출하는 알고리즘 HOG 알고리즘을 알아보자!​​​1. Image에서 Gradient란?이미지에서 기울기라는 말은 무슨 뜻일까?우리가 보통 기울기라는 말은 x의 변화한 양 대비 y가 얼만큼 변했는 가 할때 사용하는 단어이다. https://m.blog.naver.com/PostView.naver?isHttpsRedirect=true&blogId=junhyuk7272&logNo=221090781691​이미지도 마찬가지이다. 이미지는 수많은 픽셀로 이루어져있어, 이들이 다닥다닥 붙어있는데, 그 붙어있는 pixel끼리 각각의 밝기 값(gray scale 값)이 똑같을 수도 있고 다를 수도 있다.그러면 이때 그 pixel 좌표 값 차이 대비 밝기 값의 얼만큼 변했는 가가 이때 image에서 gradient라 할 수 있을 것이다.​자 그럼 생각해보자.이미지에서 물체의 edge 부분과 물체 중심 부분 중 어느것이 Gradient가 더 클까?​답은 edge이다. 왜냐면 우리가 대부분 object의 edge 를 볼 수 있는 이유가 눈이 object에 반사되고 받아들이는 빛의 밝기가 다르기 때문이다. 밝기가 아닌 색깔이 달라서 edge 구별되는 이유는 RGB 컬러 영상에서 특정 컬러 채널의 밝기 값이 더 큰거나 더 작은 수치상의 차이가 나서이기 때문이다.​이미지의 gradient가 더 자체히 알고 싶다면, 아래로 ↓↓↓↓↓↓↓↓https://theailearner.com/2019/05/11/understanding-image-gradients/ Understanding Image GradientsIn the previous blogs, we discussed different smoothing filters. Before moving forward, let’s first discuss Image Gradients which will be useful in edge detection, robust feature and texture …theailearner.com​​​​2. HOG(Histogram of Oriented Gradient)에서 이미지의 feature를 어떻게 표현할까? ​Histogram of Oriented Gradient 에서는 영상의 feature(특징)를 기본적으로방향에 따른 gradient 값의 histogram 으로 표현한다.​보행자 검출( Pedestrian Detection ) 목적을 위한 HOG는 기본적으로 64x128 크기의 영상 을 이용한다.1) 먼저 입력 영상으로부터 각 pixel의 Gradient의 크기와 방향을 계산한다. 영상의 이미지의 픽셀들이 z = f(x,y) 형태의 밝기 값(gray scale) 으로 표현되기 때문에,x,y 평면에서 gradient가 어떤 방향인지, 즉 크기와 방향 성분으로 계산되고, 방향성분은 0도부터 180도 까지 설정된다.아래 그림의 gradient vector로 이해하면 쉬울 것이다. (참고로 edge direction)이랑 다름!!​gradient  출처 : https://yeoeun-ji.tistory.com/20​2) 8x8 pixel로 묶은 단위를 cell이라고 한다. 우선 64x128 영상을 8 x 8 pixel 크기의 component 단위로 분할할 수 있는데, 이 8x8 component를 cell 이라고 부른다.그러면 64x128 영상에서 cell은 가로 방향으로 8(64/8)개, 세로 방향으로 16(128/8)개 생성된다.(참고, 근데 꼭 8x8 로 안 묶어도 된다.)​​3) 각각의 cell로부터 Gradient의 방향에 대한 Gradient 크기 히스토그램을 구한다.방향 성분을 20도 단위로 구분하면 총 9개의 bin( 히스토그램 x축 ) (참고 bin 갯수는 자기 마음대로)[0~20˚, 20~40˚, 40~60˚, 60~80˚, 80~100˚, 100~120˚, 120~140˚, 140~160˚, 160~180˚ ]으로 구성된 방향 히스토그램이 만들어진다.  gradient direction에 맞는 bin에, gradient magnitude 값을 더하자. 이 경우는 방향 값이 bin 중심에 떨어진만큼 가중치를 해서 4가 2,2 반반으로 나눠진 방식이다.이때 cell안에서 특정(x0,y0) 위치에서 gradient(변화)가 a만큼이고 방향이 b라면,우리가 나눈 histogram의 해당 방향 b가 속하는 위치의 bin에 a 크기를 더한다. 이렇게 cell에서 각각 pixel의 방향에 속하는 gradient 값을 더해나가면, cell를 표현할 수 있는 gradient 히스토그램이 만들어진다. 출처 : https://customers.pyimagesearch.com/lesson-sample-histogram-of-oriented-gradients-and-car-logo-recognition/​ https://heartbeat.fritz.ai/introduction-to-basic-object-detection-algorithms-b77295a95a63​이렇게 방향을 나타내는 Orientation Histogram의 특징은 edge의 양(gradient가 얼마나 큰지)과 방향(gradient가 어느 방향인지)를 구분하는 feature 을 가지고 있다.또한 HOG의 경우 pixel하나하나 구하는 것이 아닌 cell단위로 overlap(gradient 누적 값)을 하여 구하기 때문에 model 자체가 robust(견고)하기 때문에, 잡음(noise)에도 강하다. 마치 Haar-like처럼 영역 안에서 하나하나 pixel 값을 더하고 빼서 이 feature를 얼마나 가지고 있는지 영역의 feature를 보여주어 noise에 둔한 것처럼, HOG의 overlap도 어느정도의 변화를 수용하겠다는 뜻이다. ​​​4) 인접한 4개의 cell을 합쳐서 block이라고 정의한다.즉, 8x8pixel 은 cell → cell을 4개로 묶으면 하나의 block (16x16pixel) 이 된다. (참고, 9개로도 묶을 수 있다) https://customers.pyimagesearch.com/lesson-sample-histogram-of-oriented-gradients-and-car-logo-recognition/block의 종류는 가로와 세로 방향으로 각각 한 개의 cell만큼 이동하면서 달라질 수 있다.64x128 영상에서 block은 가로 방향으로 cell 단위로 움직여서 7(8-1)개, 세로 방향으로 15(16-1)개 정의할 수 있다.결국 64x128 영상에서 block은 가로 방향으로7개, 세로 방향으로 15개 정의하여 64x128 이미지에서는 block이 105개 가 나올 수 있다.​그럼 현재 block의 각 cell(4개)에 대해 Oriented Gradient Histogram을 연결한 다음,전체 feature vector를 정규화하는 L1, L2를 이용한다.5) block 안 cell의 histogram을 연결시킨 후 L1,L2 정규화를 하여 최종적인 영상의 feature vector를 완성시킨다. 책, Recent Advances in Applied Thermal Imaging for Industrial Applicationsv는 정규화되지 않은 vector이고, 주어진 block에서 cell들을 concatenate(연결, 즉 9개의 bin이 36개로 x축으로 연달아 이어붙인 셈)된 histogram을 구성한다고 하자. (그럼 36개의 성분이 있는 vector인 셈, 여기 분모를 정규화하자)vk는 k- norm vector라고 하고 e는 vk가 0이 되었을 때 분모가 0이 되는 것을 방지하기 위함이다.그러면 정규화를 하면 L2 norm과, L1 norm 위의 수식으로 표현된다.​이러한 유형의 정규화는 각 cell이 최종 feature vector에 예를 들어 위의 그림 cell#5를 기준으로 block1, block2, block3, block4에 여러번 표현되지만, 각 block에서 다른 값으로 정규화된다. 이러한 한 cell의 다르게 표현되는 것은 중복되고 공간을 낭비하는 것처럼 보이지만, 실제로 feature descriptor ( 영상의 특징을 잘 설명해주는 )의 성능을 향상시킨다.​​ L2 norm이 더 좋다. 그래도 정규화안한 것보다 훨씬 낫다.​이렇게 모든 block이 각각에서 정규화된 histogram을 가져와 최종 feature vector로 처리한다.​105개의 전체 block에서 추출되는 방향 히스토그램(orientation histogram)에서 실수 값(Magnitude of Gradient) 개수는 105 x 36 ( 9bins * (2x2cells / 1block) ) = 3780이 된다.​이 3780개의 실수 값이 64x128 영상을 표현하는 HOG에서 feature vector 역할을 한다.​​​​2. 어떻게 특정 image가 이 HOG feature임을 알 수 있게 학습할 수 있을까? ​바로 답은 SVM(Supporter Vector Machine)이다.SVM은 기존 Linear Classfication의 경우,분류의 기준의 되는 hyperplane이 학습의 초기 hyperparameter나 data에 영향을 받아 hyperplane이 다르게 형성되기 때문에, best solution이라고는 할 수 없다.좀 더 robust하고 최적의 hyperplane을 만들기 위해서 분류된 vector(data)들 중 군집의 바깥쪽 테두리로 구성된 convex hull의 vector들과 hyperplane 사이의 여러 개의 거리 중 최소값을 가장 크게 만들어주는 hyperplane을 구하는 알고리즘이다.아래 설명이 잘되있다.1. 직관적인 이해https://eehoeskrap.tistory.com/45 [인공지능] SVM (Support Vector Machine, 서포트 벡터 머신)SVM (Support Vector Machine) 서포트 벡터 머신은 인공지능의 기계학습 분야 중 하나로, 패턴인식, 자료분석을 위한 지도학습 모델이다. 즉, 2개의 범주를 분류하는 이진 분류기이다. 주로 분류와 회귀 분석을 위..eehoeskrap.tistory.com2. 수식적인 이해https://www.youtube.com/watch?v=qFg8cDnqYCI ​​​​​3. 요약 ​정말 HOG를 잘 설명해주는 flow diagram인 것 같다. 1input image에서 cell을 나누고, cell에서 각 pixel마다 gradient의 크기 값과 방향을 구한다.2구한 gradient 방향을 bin으로 크기 값의 histogram을 만들고3cell을 4개를 묶어서 block을 만들어 bin 갯수의 * 4가 되는데, 이것을 vector라고 했을 때, 이 vector의 절대 값을 나누어준다. (L1, L2 norm)으로 즉 정규화시켜준다.4각 block은 36개 feature를 갖게 되고, image 전체에서의 block 갯수 * 36이 되면 이 image를 나타내는 feature vector가 된다. 5이미지(feature vector)과 label로 이루어진 쌍(example)은 선형 SVM 분류기를 통해 학습된다.6test 이미지를 주면, feature vector를 가지고 detection window에서 내가 찾고자 하는 label이 있는지를 확인하고, 그 window의 위치를 알려주는 것이다. [출처] Object Detection - HOG 알고리즘 개념편 (1)|작성자 d d​ "
2주걸려 customizing해 본 object detection(2) ,https://blog.naver.com/dong961015/221874273045,20200326,"이어서 모델 훈련과 모델 활용하는 과정을 설명드리겠습니다. ​​6. xml파일 >>> csv파일 >>> tfrecord파일로 변환하기 / labelmap.pbtxt수정6-1labelmeimg를 이용해서 만든 xml파일을 먼저 csv파일로 변환합니다.(tensorflow1) C:\tensorflow1\models\research\object_detection> python xml_to_csv.py6-2csv파일을 tfrecord파일로 변환합니다.generate_tfrecord.py에서  def class_text_to_int(row_label):    if row_label == 'nine':        return 1    elif row_label == 'ten':        return 2    elif row_label == 'jack':        return 3    elif row_label == 'queen':        return 4    elif row_label == 'king':        return 5    elif row_label == 'ace':        return 6    else:        None 부분을 변경해줍니다. 저 같은 경우는 눈과 입을 detect하기위해 아래와 같이 바꾸었습니다. def class_text_to_int(row_label):    if row_label == 'eye':        return 1    elif row_label == 'lip':        return 2    else:        None 코드 변경 후object_detection폴더 위치에서python generate_tfrecord.py --csv_input=images\train_labels.csv --image_dir=images\train --output_path=train.record python generate_tfrecord.py --csv_input=images\test_labels.csv --image_dir=images\test --output_path=test.record 을 실행해줍니다.​6-3labelmap.pbtxt수정을 합니다. pbtxt 형태의 파일을 처음 볼 수 있겠지만 놀라실 필요 없습니다. 연결프로그램에서 '워드패드'를 이용하면 손 쉽게 고치실수 있습니다. (사실 저도 pbtxt파일을 처음보고 이게 뭐지....구글링해봐도..답도없네...포기할까...라고 생각하게 해준 단계입니다.)아래와 같이 수정하였습니다.  7. 모델 훈련시키기~7-1faster_rcnn_inception_v2_pets.config file을 수정해 줍니다.1)Line 9 Change num_classes를 카테고리 숫자로 수정 (저 같은 경우는 눈과 입 두개니까 2로)2)Line 106 fine_tune_checkpoint위치 수정""C:/tensorflow1/models/research/object_detection/faster_rcnn_inception_v2_coco_2018_01_28/model.ckpt""3)Lines 123 and 125 input_path : ""C:/tensorflow1/models/research/object_detection/train.record""label_map_path: ""C:/tensorflow1/models/research/object_detection/training/labelmap.pbtxt""4)Line 130. Change num_examples to the number of images you have in the \images\test directory.저 같은 경우 TEST이미지 수 10장이므로 10으로 변경5)Lines 135 and 137input_path : ""C:/tensorflow1/models/research/object_detection/test.record""label_map_path: ""C:/tensorflow1/models/research/object_detection/training/labelmap.pbtxt""​7-2이제 모델을 훈련 시킵시다!!object_detection directory에서python train.py --logtostderr --train_dir=training/ --pipeline_config_path=training/faster_rcnn_inception_v2_pets.config을 실행...하면 될줄 알았는데 여기서부터 지옥이 한번 더 열렸었습니다.​1)tensorflow.compat.v1 관련 ERROR 발생tensorflow.compat.v1' has no attribute 'contrib'해결책 : compat.v1이 보이는대로 삭제예를들어 tensorflow.compat.v1어쩌구 저쩌구 적혀있으면 compat.v1을 삭제하고 tensorflow.어쩌구저쩌구로 사용2)model_builder.py에서 ERROR발생주로 SSD관련 애들이 문제를 일으키길래 SSD관련 부분을 전부 삭제했습니다. 저는 FASTER_RCNN_INCEPTION모델을 쓰고 싶었으니깐요  3) INIT_SCOPE에러AttributeError: module 'tensorflow' has no attribute 'init_scope'가 떴읍니다. 하~~ 인생~~~검색해보니 TF 1.12.0버전을 이용하면 가능하다고 하더군요 그래서 TF1.5를 삭제하고 TF 1.12.0을 설치 하였습니다pip install tensorflow-gpu==1.12.04) cudnn7.0.5 에러cudnn7.0.5와 TF1.12.0이랑 버전이 호환되지 않는지 또 에러가 뜨더군요 구글링을 통해 TF1.12.0이랑 버전이 호환되는 CuDNN 7.3.1을 설치하였습니다.​이 과정을 모두 거치고 나서야 비로소 모델 훈련이 시작 되었습니다. 정말 짜릿한 순간이였습니다.​  훈련은 3시간정도 진행되었습니다. 학습은 3만번정도 진행하였고 마지막 EPOCH를 기준으로 LOSS는 0.04정도나왔습니다.​​8. 훈련 과정 확인하기 (tensorflow1) C:\tensorflow1\models\research\object_detection>tensorboard --logdir=training코드를 이용하면 훈련 과정에서의 LOSS값을 볼 수 있습니다.​9. generate the frozen inference graphpython export_inference_graph.py --input_type image_tensor --pipeline_config_path training/faster_rcnn_inception_v2_pets.config --trained_checkpoint_prefix training/model.ckpt-XXXX --output_directory inference_graph 코드를 이용합니다. XXXX부분은 마지막으로 체크포인트가 저장된 EPOCH의 NUMBER를 적어주시면 됩니다. 저같은 경우는 30615였습니다.​10. 모델 활용하기이제 기다리고 기다리던 모델 활용하는 시간입니다.Object_detection_image.PY를 활용하여 잘 예측하는지 보도록 하겠습니다. 얼추 잘 나온거 같습니다.   후기: OBJECT DETECTION은 활용할 수 있는 범위가 무궁무진하기 때문에 이를 바탕으로 여러가지 모델을 더 만들어 볼 생각입니다. 만드는 2주간 정신과 시간의 방에 있는거마냥힘든 시간이었지만 깔끔한 결과물을 보고나니 너무 뿌듯합니다. 이게 딥러닝의 매력인가 싶습니다. 이후에는 IMAGE SEGMENTATION에 도전해 볼 생각입니다. OBJECT DETECTION만큼 아니면 그 이상으로 힘든 과정일 수도 있지만 끝까지 가보고 싶은 마음입니다. 지금까지의 글을 읽어주셔서 감사합니다. "
[DL with Matlab #10] Object Detection 을 위한 전처리 함수들 ,https://blog.naver.com/sonyi/222406971757,20210623,"일단 Object detection부터 시작하고 전처리 함수들부터 정리해본다.​일단 ground truth 데이타가 필요한데 이를 위해서는 image상에서 bounding box가 각 training data set에서 정의되어야 한다. 이는 Image Labeler App에서 수행될 수 있다. ​Ground truth 데이타는 image, bounding box, label 이렇게 세 개의 element를 가진다. 일단 이 App을 통해서 만든 다음에 다음과 같이 불러올 수가 있다.​하나의 그림에 다수의 label이 생성될 수 있으므로 아래와 같은 table 형태로 imageFilename과 각 label이 열로서 정의되고 각 열에 만약 존재하면 bounding box의 위치가 4개의 position으로 표현되고 (x,y, width, height) 없을 경우 []로 표현되는 형태로 구성되게 된다. ​이러한 데이타에서 그림을 불러오는 것은 아래와 같이 하면 된다. im = imread(petGroundTruth.imageFilename{1})imshow(im) bounding box의 경우는 아래와 같이 불러올 수 있다. 위에서 첫번째 그림의 경우 Madeline만 정의가 되어 있으므로 아래와 같이 한다. bbox = petGroundTruth.Madeline{1}annotationLabel = 'Madeline' 아래와 같이 image에 bounding box등도 넣어줄 수 있다. labeledim = insertObjectAnnotation(im, ""rectangle"", bbox, annotationLabel) 이경우 예제에서 실제는 아래와 같이 표시된다. 뭐 만들어서 쓸 수도 있겠다만 생각보다는 유용한 툴인 거 같다.  ​자, 다시 ground truth 데이타의 전처리 과정을 정리해보면 아래와 같다.​table로부터 image와 bounding box 를 각각 분리해서 Datastore로 저장하고 이를 combine해서 data라는 데에 집어넣는다. 그리고 transform이라는 함수를 통해서 data를 scale을 변환한다.이 작업은 scaleGT라는 함수 안에 정의가 되어 있고 함수 핸들을 인자로 넣어준다.함수의 내용도 아래에 나와 있다.  imds = imageDatastore(petGroundTruth.imageFilename)bxds = boxLabelDatastore(petGroundTruth(:,2:end))data = combine(imds, bxds)scaledData = transform(data, @scaleGT)function data = scaleGT(data)      targetSize = [224 224];    % data{1} is the image    scale = targetSize./size(data{1},[1 2]);    data{1} = imresize(data{1},targetSize);    % data{2} is the bounding box    data{2} = bboxresize(data{2},scale);end scale이 변화된 그림에서도 비슷하게 bounding box와 label이 표시된 그림을 만들어보고자 한다면 아래와 같이 하면 된다. preview 함수는 image와 bounding box 정보, label 정보를 담는 cell array를 return해준다.  newGT = preview(scaledData)im = insertObjectAnnotation(newGT{1},""rectangle"",newGT{2},newGT{3});imshow(im) ​​ "
"Weakly Supervised Region Proposal Network and Object Detection (2018, ECCV) ",https://blog.naver.com/siniphia/221544046833,20190522,"Weakly Supervised Region Proposal Network and Object Detection (2018, CVPR)http://openaccess.thecvf.com/content_ECCV_2018/papers/Peng_Tang_Weakly_Supervised_Region_ECCV_2018_paper.pdf​​Introduction Object Detection을 위한 딥러닝 모델들은 일반적으로 다음과 같은 파이프라인을 갖는다.​ 1. Region Proposal Generation 2. Proposal Feature Extraction 3. Proposal Classification​ Faster R-CNN을 제외하면 대부분의 선행연구들은 1번 단계에 Selective Search나 Edge Boxes 등 기존의 고전적인 방식을 수정 없이 적용했다. 이 논문은 초기에 제안되는 Region이 딥러닝 예측 품질에 많은 영향을 끼칠 것이라는 생각으로 1번 단계를 개선시킨 모델을 제안한다. 또한 이 모델을 Wealky Supervised Object Detection (WSOD) 모델에 이식시켜 Bounding Box 없이 Image Annotation만으로 Object Detection을 가능하게 만들었다. ​​​Contribution 이 논문의 주요 Contribution은 다음과 같으며, 1번이 가장 핵심적인 아이디어이다.​  1. 기존 EdgeBox를 개선시켜 각 CNN Layer의 고유 특성을 이용한 Region Proposal Algorithm 제안  2. 새로운 Weakly Supervised Object Detection 모델로 PASCAL VOC와 ImageNet에 대해 SOA 달성​​​Network Architecture    이들은 초기 Proposal P0로부터 Coarse Proposal P1을 골라내고 다시금 P2로 정제한 뒤, 기존의 WSOD 모델로 최종 Object Detection을 수행한다.​​1. Coarse Proposal P1    이 단계가 논문의 가장 핵심 부분이다. 일단 기존의 EdgeBox 방식과 기본적으로는 같다. 먼저 한 이미지에 대해 단순한 Sliding Window 방식으로 모든 Proposal들을 뽑아낸다. 이를 P0라고 한다. 이제 EdgeBox를 사용해 각 P0들의 Objectness Score를 매기고 Score가 높은 Proposal들만 골라낸다.​ 여기서 기존 방식과 다른 점은 EdgeBox의 Score 계산을 위해 사용되는 이미지가 원본이 아니라 Conv Layer의 Feature Map이라는 점이다. 특히, 위 Figure에서 볼 수 있듯이 2~4번째 Conv Layer들은 Edge에 민감하게 반응한다. 이들을 원본 이미지 크기로 Resizing 한 뒤, 모든 Channel들을 더해 평균을 낸 Map을 EdgeBox의 Input으로 사용하게 된다. ​ 왜 원본 대신 Feature Map을 사용했을까? EdgeBox는 이름에서 드러나듯이 이미지의 Edge를 탐색해 Region의 Objectness Score를 매기는데, 원본 이미지보다 Edge가 강조된 2~4 Layer의 Feature Map을 대신 사용한다면 알고리즘의 품질을 높일 수 있을 것이라고 생각한 것이다. 물론 Feature Map은 원래 이미지가 아니므로 이 방식이 Optimal 하다고 증명할 수 없기에 추가적인 연구가 필요하다고 밝혔다.​ (저자는 VGGNet을 사용했기 때문에 2~4 Layer가 Edge에 반응한 것이고, 다른 모델과 다른 데이터셋을 사용할 경우 Edge에 반응하는 Layer가 달라질 수 있기 때문에 이 아이디어를 사용하고자 한다면 실험적으로 본인의 모델에 맞는 레이어를 선택할 필요가 있다.)​​2. Refined Proposal P2 위 단계에서 골라낸 P1들은 여전히 Noisy하다. 왜냐하면 위에서 사용한 Feature Map은 사실 Edge 뿐만 아니라 Background에도 크게 반응하기 때문이다. 따라서 Background를 제거하는 추가적인 정제 단계가 필요하다. 이 모델의 목표는 WSOD이기 때문에 주어진 정보는 Class Label뿐이고, 이를 위해 저자가 이전 해에 발표했던 SOA급 WSOD 모델(Multiple Instance Detection Network with Online Instance Classifier Refinement; 2017, CVPR)을 사용해 Object일 확률을 계산하고 이를 P1에서 사용한 Score와 곱해 (증명되지 않았으며 실험적으로 덧셈보다 곱셈이 낫다고 언급) 점수가 낮은 Region은 Background로 간주해 제거했다. 이 모델 말고도 다른 WSOD를 사용해도 상관은 없다.​ (또한 그냥 Supervised Object Detection 모델에 적용하고 싶다면 그냥 Bounding Box와의 IoU를 계산해서 거의 겹치지 않는 Region을 제거하면 될 것 같다.)​​3. Weakly Supervised Object Detection 이제 Region Proposal은 끝이 났고 이들을 가지고 Object Detection만 돌리면 되는데, 역시 위에서 언급한 논문에 나온 것과 거의 똑같이 학습시킨다. 이 단계는 원본 논문을 읽어보는게 좋을 것 같다.​​​Result    PASCAL VOC와 ImageNet에 대한 결과이며 SOA급 성능을 달성했다. Metric으로 쓰인 mAP는 https://eehoeskrap.tistory.com/237 이 분이 완벽하게 설명해 주셨고, CorLoc은 예측한 Bounding Box들이 Groundtruth와 몇개나 일치하는지에 대한 비율이며, 겹치는 부분이 0.5 이상이면 일치로, 0.5 미만이면 불일치로 간주해 계산한다.​​​Comment 기존에 EdgeBox를 사용하던 모델이 있다면 위 방법을 적용해 성능향상을 노릴 수 있을 것 같다. Supervised Object Detection 업계에도 적용할 수는 있겠지만 Faster R-CNN이나 YOLO처럼 굉장한 모델들이 존재하기 때문에 또 2-stage이기 때문에 실제로는 굳이 사용하지 않을 것 같다. 다만 접근방식이나 아이디어가 인상적이었고 추가로 논문 구성이 좋아서 여러모로 재미있게 읽었다. "
AI Builder로 최애 소주 인식하기! (Object Detection) ,https://blog.naver.com/sesme100/222167433410,20201209,"#MicrosoftPowerPlatfrom#MicrsoftPowerApps#MicrosoftPowerAutomate#MicrosoftRPA#CitizenDeveloper#MSRPABeginners​#진로이즈백 #참이슬 #처음처럼 #한라산 #듣기만해도침꼴깍​​안녕하세요 Microsoft Citizen Developer 진미나 입니다. Power Apps 내장되어 있는 AI Builder로 내가 좋아하는 소주들을 개체감지(Object Detection) 해보았어요 ㅋㅋ AI를 training 하는데는 사실 Python의 라이브러리를 이용하여 Deep Learning을 구현하고 hyper parameter들을 세세하고 조밀하게 조절하여 내가 원하는 모델을 섬세하게 훈련을 시켜야 하지만!!!  Power Apps에 내장되어있는 AI Builder는 아주쉽게 훈련이 가능하고 훈련된 모델을 Power Apps로 가져와서 장난감마냥 신나게 구현 가능해요!! 코딩 한줄 없이 AI 훈련시키기 한번 해볼까요? ​​AI Buidler 훈련 준비물: 1. 내가 개체 감지 훈련시키고 싶은 Object (물체) 또는 브랜드명 또는 선반 위 물체들이 있는 이미지들을 최소 15개 가져오기2. Power Apps 의 Tenant와  Environments 일치 시키기3. AI Builder Premium 라이선스 ( 사실 프리미엄 라이선스라서 돈 더줘야 하는데 Microsoft 에서 무료로 제공해줌)   # 우선 https://powerapps.microsoft.com/en-us/ 으로 들어가서 로그인 해주세요.  Power Apps는 어떤 로그인 계정이던 1개월 무료로 사용가능하니깐 고고~~ 일단 질러~~!!  Business Apps | Microsoft Power AppsBuild professional-grade apps the easy way Increase agility across your organization by rapidly building low-code apps that modernize processes and solve tough challenges. Get started quickly Empower your team to start building and launching apps right away using prebuilt templates, drag-and-drop si...powerapps.microsoft.com # Power Apps에 로그인 한 후 AI Builder 에 빌드로 들어가세요! 그럼 여러가지 종류의 AI가 보이는데요. 우리는 개체 감지를 쓸거예요!개체감지는 물체를 인식하고 이름을 붙여서 인식하는 자동인식이고요, 범주분류는 텍스트를 인식하여 의미별로 분류하고, 양식처리는 form 문서에서 정보를 읽어와서 처리 하고, 엔티티 추출은 비지니스 데이터의 특정정보를 인식하여 구분합니다! (비지니스에서 사용되는 모든 정보의 관계도 같이 연게해서 분석해줘요. 예를 들자면 Finance - Accounting 과 같이 연계해서 인식한다는 의미이죠!) 우리는 개체감지 (Object Detection)으로 들어갑니다~유후~​# 중요한점은 우리가 생성하는 AI Builder를 통하여 만든 AI 모델과 Power Apps (또는 모든 Microsoft 사용 프로그램의) Tenant 와 Environment가 일치해야해요!! -> 그러니깐 만약 제가 outlook.com이라는 도메인을 통하여 어떤 특정그룹 (회사, 학교)의 Tenant 계정을 사용하면 모든 MS Program의 로그인된 Tenant가 같아야 합니다. 예를 들면,  제가 Outlook.com 으로 로그인해서 아웃룩 대학교 Tenant를 들어 갔으면 Power Apps도 똑같은 Tenant로 로그인을 해야하고 AI Builder도 똑같은 Tenant로 사용해야하며 만약 Power Apps가 엑셀아나 Power Point와 연결되어있다면 이 모든 제품들이 모두 똑같은 Tenant로 로그인 되어야 연결 할 수 있어요.​# Environment (환경)도 최소 단위 그룹인데요. 하나의 팀처럼 생각하시면 되어요. Tenant의 Administrator가 관리하며 주로 Default Environment가 있어요. 여러가지 Environment가 있는데요. 샌드박스라고 해서 일회성 Environment가 있고, 평가판(Trial 30일체험) Environment, Production Environment (실제로 Tenant안에서 Production을 위해 사용, 어드민에 의해 Control 되어짐), 그리고 Default Environment (주로 영원히 사용가능) 가 있어요.  만약 제가 하나의 Environment에 내 앱을 만들면 이 Environment에 있는 모든 사용자가 내 앱을 살펴볼 수 있습니다. 그러나 편집등은 제가 권한을 줘야지만 가능해요. 참고로 Tenant의  라이선스에 따라 3개의 환경을 만들 수 있는걸로 알고 있어요. (Tenant가 돈 더 주면 환경 더 만들기 가능 ㅋㅋ 자본주의 ㅋㅋ )https://docs.microsoft.com/en-us/power-platform/admin/environments-overview Environments overview - Power PlatformLearn about environments in Power Apps and how to use themdocs.microsoft.com #001# 일단 만들어볼게요! 개체감지에서 모델 이름을 넣어주고요! #002# 모델 도메인을 선택해줍니다. 모델 도메인 종류 중에 공용개체는 전반적인 물건 인식을 해주고요( 물컵, 책 등). 소매점 선반에 있는 개체는 우리가 이마트나 홈플러스 가면 선반에 있는 판매대에 있는 물건들을 좀더 구체적으로 인지합니다. 브랜드 로고는 우리가 잘 알고 있는 브랜드 이름을 잘 인식 시켜줘요!우리는 이마트나 홈플러스에 있는 소주들을 몽땅 모아 AI 개체 인식 (+ 브랜드 이름) 훈련을 할거예요 #003​# 개체를 선택할게요! AI 모델이 인식해야 하는 사물이나 사물의 이름을 Labeling하기 위해서는 사용자가 개체 이름을 지명을 해줘야 해요!저는 수 많은 소주들중에 4개만 선택해서 AI 모델이 인식 할 수 있게끔 해줬어요~ 아이러브 쏘주~!!#진로이즈백 #참이슬 #처음처럼 #한라산 #004​# 그럼 각각의 소주 이미지당 최소 15개의 이미지를 훈련 시켜야 하는데요!! 훈련된 이미지가 많으면 많을 수록 AI모델의 정확성 %가 높아져요!! 저는 제 Local PC에 있는 이미지들을 사용해 볼게요~ #005# 사실 Sharepoint 또는 Azure Blob Storage (비정형 데이터 저장소)에 저장된 이미지들을 가져올 수 도 있어요! 그치만 Local PC에서 가져오는게 쉽자나~! #006# 내가 가지고 있는 모든 소주 이미지를 가져와야 해요! 참고로 한개의 개체 (진로이즈백)당 최소 15개의 이미지는 꼭 있어야 합니다!! 개체 4개를 훈련시키는 거니깐 최소 60개 이미지 이상은 되어야 한다는 말씀~ 저는 참고로 1개체당 이미지 25장씩 사용하였습니다. 백장 ㅜㅜ  #007​# 이미지 1개에 있는 모든 개체들 (내가 인식시켜주고 싶은 개체)를 하나 하나 Tagging (딱지 붙이기) 해줘야해요! 이미지에 박스를 설정해주면 개체선택 박스가 자동으로 뜨는데요! 그럼 개체에 맞게 선택을 해주면 됩니다. 여기서는 참이슬은 ""참이슬""로 선택해주고 진로이즈백은 ""진로이즈백""을 선택하면 됩니다! 움하하하 쉽죠? 근데 이게 100장이야.... 훈련을 시키는데 몇시간 걸립니다. 그치만 생각해 보십쇼. 강아지에게 이건 ""참이슬""이야~ 이건 ""진로이즈백""이야~!! 해서 훈련시키는거보다는 훨씬 빠르겠죠?​# 태그 적용을 보시면 ""참이슬""이 25개 태그 되었어요!! ""진로이즈백""은 5개 (최소 15개 인식되어야 함), ""한라산""은 8개, ""처음처럼""은 10개가 태그 되었네요! 참이슬은 25개가 태그되어서 만족한다고 보라색으로 바뀌었어요!  더 많은 이미지를 훈련시키면 더 잘 알아봅니다. 정확도는 그만큼 올라 가겠죠?   #008# 한라산 중에 우리가 알고 있는 한라산 모양만 태그를 해줬어요~ 그래야 개체인식을 더 잘 할테니깐요. ​# 중요한건 다양한 관점의 사진들을 훈련시켜줘야 해요. 개체의 앞, 뒤, 위, 아래로 찍은 사진들로 태그 하고 노이즈를 줘야(빈칸도포함, 물건이 겹치거나, 해상도가 다르게, 채도도 다르게) 그만큼 훈련이 더 잘 된다는 점 입니다. 왜냐고요? AI 모델은 훈련하는 데이터의 general variable 을 찾아서 MSE(에러)가 나오게끔 해줘야 하고 Overfitting이 안되게끔 해줘야 해서!! 그말은 AI에게 앞면만 주구 장창 훈련 시켜주면 옆면이나 위에서 바라본 사진은 못알아보는 융통성없는 AI가 된다는 말씀!!  앞면만 알아보는 AI는 쓸데가 없자나요 ㅋㅋ!! #009#010# 4개의 개체가 없는 사진은 과감하게 삭제~ 막걸리는 다음에 훈련시켜 줄게 #011# ""처음처럼""도 훈련시켜줬어요.. 처음처럼 6병 먹으면 기억력도 처음처럼 ㅋㅋ #012​# 이미지를 개체에 태그 지정 완료! 4개의 개체 모두 15개 이상씩 태그 되었어요!! 참이슬은 총 31개 태그, 진로이즈백은 28개 태그, 한라산은 24개, 처음처럼은 26개 태그 ~  #013​# 모델요약 확인 하고 그럼 기차(Train)를 눌러줍니다. ㅋㅋㅋㅋㅋㅋ (누가 번역했니? 번역기 너니??) 그럼 AI모델을 기차(Train)해봅시다. ㅋㅋㅋ #014# 학습(Training)은 생각보다 오래 걸렸어요... 내가 100장 훈련시켜서 그런가 약 1시간 걸린거 같아요. 이때 Power Apps 페이지를 중간에 끄더라도 놀라지 말아요... 어짜피 Azure가 Computation 하는거라서요!! 내 PC 의 CPU& GPU는 놀고 있다라는 말씀.. ㅋㅋㅋㅋㅋㅋ 내 컴터에 GPU없어도 어짜피 일은 Azure가 합니다. Azure 짱인듯 #015# 훈련 후 정확도는 81%가 나왔어요!! 더 많은 이미지를 가지고 훈련을 시키면 정확도는 올라가겠죠?? Power Apps AI Builder는 업데이트도 가능합니다. 아주 칭찬해~~ (게다가 버전 업도 쉬워!!) ​# 게시(Publish)를 해야지만 Power Apps에서 연결해서 사용가능해요!! 무조건 게시를 하셔야 합니다!!  #016# 테스트를 해보아요 ㅋㅋ 짜잔~! 한라산도 인식 잘해~ 참이슬도 인식잘해!! 아주아주 칭찬해!! 게다가 개체당 정확도를 표현해 주네요!! #017# ""진로이즈백""은 더 잘 인식하네요. 정확도가 더 커요! #018 # Power Apps에서 잘 되는지 봅시다! Power Apps에서 만들기(Create)에서 캔버스 앱을 선택합니다. 그 후 Power Apps의 이름을 넣고 저장!! #019# Power Apps의 삽입 탭 - AI Builder탭 으로 들어가서 개체감식기를 선택합니다. #020# 프리미엄 라이선스래요~(돈 더 내야 한다는거죠!) 그치만 써봅시다ㅋ 공짜로 쓰게 해줘요! ㅋ  #021​# Power Apps 에 AI Builder로 생성한 나의 소중한 모델을 연결해봅시다!! ㅋ 내 소중한 AI 모델이름은 Object Detection 20201202 그럼 연결~ (참고로 같은 Tenant에 있는 동료직원, 그룹원, 팀원, 친구에게 같이 공유 할 수 있어요! 그럼 그분들도 내 모델을 다시 업뎃 가능합니다! 내가 모델 만들어 놓고 친구에게 AI 모델 훈련 더 시키라고 할 수 있어요) #022​# 개체인식기가 생성 되었어요! 당근 제가 만든 AI 모델 Object Detection 20201202 에 연결 되었지요!!  #023​# 개체인식기의 검색을 눌러주세요 (Alt를 누른 상태에서 마우스 클릭 해야 작동 되어요!!) 내가 원하는 사진을 업로드 합니다. 이 때 중요한건 훈련을 시킬때 사용했던 사진은 사용 노노!! 새로운 사진을 올려줍시다! 얼마나 잘 인식하는지 보려면 채점 정답이 없는 새로운 사진을 업로드 해줘야 해요! #024​# 와우!! 보셨나요?? ""참이슬"", ""진로이즈백"", ""처음처럼"" 모두 잘 인식되어 태그 되었어요!! Gallery라는 테이블처럼 생긴 데이터 UI 에 몇개가 인식되었는지 개수를 보여줍니다!! #025 # Gallery UI는 다음시간에 알아보아요!! 여태껏 코딩의 코짜 한줄도 안썼다는 사실!!!!!! Microsoft 사랑해!!​  #EdgeofCitizenDeveloper#나도하면너도할수있어#YoucandoitasIdo "
딥러닝 기반 object detection 과정 정리 (2) - CNN 기반 기본적인 backbone ,https://blog.naver.com/taeeon_study/222786717107,20220624,"이 때까지 들었던 대부분의 강의 자료 혹은 책 그리고 학교 수업은 CNN까지 다루고 대부분 ResNet 까지 다루면 대부분 강의가 마무리 되었다. 그렇기에 ResNet까지는 많은 사람들이 알고 있고 상당히 유명한 네트워크임을 알고 있기 때문에 간략하게 설명하고 넘어가도록 한다.과정 정리 (1)에서 CNN을 간략하게 설명하였고 이제 CNN이 어떤 네트워크에서 쓰였는지 흐름을 알아본다.​AlexNet[1]이는 convolution 층 5개와 fully connected layer 3개로 구성되어 있다. 합성곱 층은 약 2만개의 가중치를 가지고 완전 연결층은 약 6천 5백만 개의 매개변수를 가진다. ImageNet이라는 큰 데이터 베이스와 GPU를 활용하여 병렬처리 하였고 ReLU 활성함수를 사용하였으며 과적합 방지 여러 규제 기법들을 적용한 케이스이다. AlexNet​2. VGG-16 [2]큰 크기의 커널이 여러 개의 작은 크기 커널로 분해될 수 있다는 아이디어가 핵심이다. 이도 기초적인 CNN 공부를 했었다면 많이 들어봤을 네트워크이다. 작은 필터 사용으로 인해 가중치 개수가 줄고 신경망은 깊어졌다. conv 층 13층과 완전연결층 3층으로 이루어져있다.​3. GoogleNet [3]구글넷의 핵심은 인셉션 모듈의 사용일 것이다. 이는 다양한 크기의 필터들을 사용한 컨볼루션 연산을 통해 다양한 특징을 추출하는 것을 의미한다. 인셉션 모듈을 9개 결합하여 층이 상당히 깊어졌고 완전연결층이 1개이다. 아래는 인셉션 모듈의 그림이다. GoogleNet​4. ResNet [4]ResNet은 아직까지 backbone으로 상당히 많이 쓰이고 있고 지금까지 언급된 네트워크 중에서 가장 중요하다. 핵심은 잔류학습이라는 개념을 이용하여 성능 저하를 피하고 층수를 대폭 늘린 것이다. Gradient vanish 문제를 해결하였습니다. 아래 그림의 Residual block이 핵심인데 4가지 주요 기능이 있다.​1) 연속된 3x3 filter2) Bottleneck 구조3) Residual Connection4) Batch Normalization​ residual function간단한 CNN의 기초적인 네트워크에 대한 설명이었고 다음 포스트부터는 본격적으로 object detection 분야에서 SOTA progress를 살펴보면서 핵심 네트워크 리뷰를 시작한다.​아래는 이번 포스트에서 간단히 설명했던 네트워크들에 대한 본 논문들이다. [1] KRIZHEVSKY, Alex; SUTSKEVER, Ilya; HINTON, Geoffrey E. Imagenet classification with deep convolutional neural networks. Advances in neural information processing systems, 2012, 25.[2] SIMONYAN, Karen; ZISSERMAN, Andrew. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014.[3] SZEGEDY, Christian, et al. Going deeper with convolutions. In: Proceedings of the IEEE conference on computer vision and pattern recognition. 2015. p. 1-9.[4] HE, Kaiming, et al. Deep residual learning for image recognition. In: Proceedings of the IEEE conference on computer vision and pattern recognition. 2016. p. 770-778. "
Cascade R-CNN : Delving into High Quality Object Detection ,https://blog.naver.com/tomatian/221853527701,20200314,"Cascade R-CNN : Delving into High Quality Object detectionhttp://openaccess.thecvf.com/content_cvpr_2018/papers/Cai_Cascade_R-CNN_Delving_CVPR_2018_paper.pdf※ Github - pytorch zhaoweicai/Detectron-Cascade-RCNNCascade R-CNN in Detectron. Contribute to zhaoweicai/Detectron-Cascade-RCNN development by creating an account on GitHub.github.com 모델 아키텍쳐만 궁금하신 분은 아래로 내려가 주세요  ​Abstraction​1 . Object Detection의 문제점  IoU  IoU= ground truth와 prediction box의 교집합/합집합​​ IoU threshold를 낮은 혹은 일반적인 0.5로 사용하게 되면 noisy detection 발생그렇다고 IoU threshold를 높이면 performance(AP)가 저하된다.​이러한 두 문제점은 또 다른 부가적인 문제를 발생시킨다.​1. IoU threshold너무 높다 → positive sample 줄어든다 → overfitting2. IoU값이 IoU threshold와 유사할 시에만 optimal한 detector가 된다.​​2. 해결책​Cascade R-CNN은 이러한 문제를 해결한다.∵   IoU threshold를 증가해가면 training 시킨 모델이고, close flase positive (positive가 아닌데 IoU threshold를 넘어서는 prediction들)에 강하다.​(본 논문에서는 close false positive라는 단어가 빈번하게 등장한다. 잘 기억해 두자!)​​​​​​Introduction​1 . Object Detection의 과제   localization(object가 어디에 위치하는 지, bounding box) + classification (object가 무엇인지)이 두 과제는 object detection에서 가장 기본적인 두 과제이다.​위 두 가지 과제는 여러가지 close false positive들로 수행하기 어려워진다.detector의 과제 : close false positive를 억제하고 true positive를 찾아야한다.​​2. Object detection과 IoU threshold​object detection의 prediction 판별 기준은 IoU로 인해 정해진다.다음 그림을 보자.   그림에서 IoU threshold를 0.5(낮음)으로 하면 ground truth와는 관련이 없어 보이는 잡다한 bounding box들이 true positive로 판별된다.하지만 IoU threshold를 0.7로 설정했을 떄는 조금 더 정제된 모습을 보인다. ​그리고 IoU threshold를 0.5보다 낮추게 되면 더 다양한 sample들을 수용할 수 있지만,close false positive를 걸러내지 못한다.​​   위 그림에서 regressor은 localization을 의미한다.(bbox regression에서 나온 말이라 추측)​(c) 그림에서 input IoU = 0.5일 때와 0.95일 때를 비교해서 보면 이해가 쉽다.input IoU는 각 bbox의 IoU값이다. output IoU는 detection performance라고 보면 된다.(나는 output으로 나오는 이미지의 IoU라고 말그대로 이해했다. 이렇게 이해하는 게 더 쉽다.)input IoU가 0.5일 때는 u=0.5일 때 output IoU가 가장 높다. input IoU가 0.95일 때는 u=0.7가 일 때 output IoU가 가장 높다.즉 ) input의 IoU = 0.6 일 때 / IoU threhold =0.7→ fp 취급, IoU threshold = 0.5 → tp취급​(d)는 IoU threshold가 높아질수록 average precision (AP)가 작아진다는 것이다.이는 자명하다. IoU threshold가 높아질수록 tp가 줄어들어 ap도 작아지는 것.추가적으로 IoU threshold를 높이게 되면 overfitting가능성이 있다.​결국  abstract에서 거론한 부차적인 두가지 문제가 발생하는 것이다.​​2. Cascade R-CNN이 해결책이다. (개괄적 소개)​cascade r-cnn은 r-cnn의 multi stage extension이다.stage가 깊어질수록 close false positive를 걸러내는데 유용하게 만들었다.​stage는 순차적으로 학습되는데,train에서 하나의 stage에서의 output이 다름 stage를 train하는데 사용된다.각 stage는 다음 stage의 training을 위해 close false positive group을 찾는 것이다.​결론적으로 IoU가 높은 hypotheses(prediction)들을 더 얻어서 높은 IoU threshold를 사용하면 성능이 좋아진다는 것이다.​​​​​Object detection​Cascade R-CNN은 Faster R-CNN에서 착안하였으나, region proposal을 RPN에만 의존하지 않는다.cascade learning = ensemble learning, 여러 detector(classifier)의 정보를 합하여 output을 내는 것​그리고 이 논문은 이전 모델들에서 사용한 기법을 소개한다.따라서 이 부분에서는 이전의 방식들이 모두 한계가 있음을 이야기한다.​1. Bounding Box Regression → localization​bounding box = (bx, by, bw, bh)prediction = b / ground truth = bregressor =  f(x, b) / training sample (정답) = (gi, bi)​Fast R-CNN에서의 loss function   loc 은 localization을 의미한다.여기서 사용되는 distance vector   하지만 이렇게 계산을 진행하면 localization의 loss가 classification의 loss보다 훨씬 작아지므로 평균과 표준편차로  normalization을 진행한다. 그 결과는 아래와 같다.    ​다음으로 iterative bounding box regression이라는 개념이 나온다.   하지만 이렇게 loss를 구하게 되면 두 가지 문제점이 발생한다.앞서 계속 말했듯이 높은 IoU를 가진 input에 IoU threshold=0.5는 최적의 결과를 내지 못한다.그리고 각 stage마다 bounding box의 distribution이 달라지는 단점이 있다.​​2. Detection Quality (classification)​training set = (xi, yi)yi = class (정답) / h(xi) = 예측 값cross - entropy loss는 다음과 같다.   여기서의 cls는 classification을 의미한다.하지만 box에는 어느 정도의 background가 포함되기 마련이다.그래서 IoU를 기준으로 class를 확정한다.​이 논문은 반복되는 말이 아주 많다.또 하나의 classifier가 여러 IoU에 대해 유사하고 좋은 성능을 내기는 어렵다고 말한다.또한 RPN이나 selective search로서는 한계가 있다고 한다.​그래서 새롭게 나온게 integral loss다. classifier의 ensemble을 생각한 것이다.수식은 다음과 같다.   하지만 이 방식은 각각의 loss를 구할 떄 각기 다른 개수의 positive에 기반하는 점을 다루지 못한다.또한 IoU가 높아질수록 성능은 낮아져 ensemble 방법은 최적의 결과를 내지는 못한다.​​​  ​​Cascade R-CNN드디어 Cascade R-CNN을 소개한다.   ​1. cascaded bounding box regression (localization)cascade R-CNN은 single regressor로 다양한 IoU level에 무관하게 uniform한 결과를 낼 수 있다.   T는 cascade stage의 total 개수를 의미한다.이전 stage보다 높은 IoU를 그 다음 detector에 설정하고 성능을 높여간다. 더 정확한 input이 다음 stage로 들어갈 것이고, 앞에서 계속 이야기 했던 두 가지 문제가 해결된다.firgure 3에서 (b) (d)가 같아 보일 수 있지만 (b)는 stage마다 IoU를 높여가지 않는다.즉 resampling 작업이 없다. (더 정확한 input을 다음 detector로 넘기는 작업)​즉, f가 단계별로 resample되는 점이 cascade R-CNN의 가장 큰 차이점이다.또한 distance vector들도 각 stage에서 각각 다르게 normalization된다.​​2. Cascaded detection (classification)​IoU와 상관없이 IoU threshold를 진행하면 baseline보다는 항상 높은 output IoU가 나온다는 것을 확인하였다.이에 착안하여 cascade R-CNN은 resampling mechanism을 적용한다.​∴ IoU threshold를 적용하면 적용할 수록 output IoU가 높아지고, 각 stage를 진행할 수록 IoU를 더 높여가면 더 좋은 output IoU를 얻을 수 있다.그래서 다음 stage로 넘어갈 수록 높은 input IoU를 가진 image가 남는 것이고, 이로써 IoU threshold를 다음 stage에 높여도 output IoU가 크게 떨어지지 않는 것이다.​이런 방식으로 진행하면 우선 overfitting이 일어나지 않는다. ( ∵  positive sample들이 각 stage를 지나도 줄어들지 않고 계속 많은 양 유지)그래서 deeper stage에서도 높은 IoU threshold를 적용할 수 있게 된다.자연스럽게 각 stage를 진행할 수록 outlier들을 제외된다.​   classification loss 역시 전 stage에 영향을 받는다.​​​3. 최종   Cascade R-CNN은 최종적으로 4 stage가 있다.1 : RPN (region proposal network)2, 3, 4 : detecor, IoU threshold를 0.5, 0.6, 0.7로 변경해 가면서 얻음​​​​​Results​   IoU를 변경해 가지 않으면 train 시킨 것보다 AP가 높다.​   stage를 진행해 갈수록 IoU threshold에 무관하게 비슷한 모양의 그래프를 보인다.​​   sota 모델들과 비교를 해보면 성능 비교가 더 쉽다.주목해야할 것은 AP75라고 생각한다. (IoU threshold = 0.75)다른 모델들에 비해 더 높은 AP를 가지는 걸 볼 수 있다.​​​​​​​​​  이상 동산이었습니다.잘못된 점이 있다면 댓글 남겨주세요 :) "
Object Detection API에서 Step과 epoch 사이의 관계 ,https://blog.naver.com/kyoungseop/222979157857,20230109,"Step은 파라리터가 한번 업데이트 되는 단위인데, 보통 데이터 입력은 배치단위로 처리되므로 처리 데이터가 50000개 배치가 16이면 전체 데이터가 한번 처리되는 1 epoch 당 50000/16 = 3125 step이 소요된다.그러므로 10 epoch을 동안 돌리고 싶으면 50000/16*10 = 31250 step이 된다. "
Object Detection project ,https://blog.naver.com/iadslba/222451816204,20210731,"- 한국국방학회 주관으로  국방 분야 object detection project 프로젝트를 구현하고 그 진행과정을 교재로 만들어 국방분야 사업체, 국방분야 업무에 종사하는 교육생들에 강의하는 과정입니다. -  K1A1, K2, M1A1 등 전차 이미지 전처리와 Data augmentation을 진행한 다음 실시간으로 구분하는 모델을 생성하고 실제 구현과정을 실습해 보는 내용의 강의 내용입니다.  - 다양한 모델을 테스트 하고 최적의 결과를 내는 Object Detection모델을 이용해 결과를 도출하였습니다. - 다음 목표는 자연어 처리에 대하여  교재를 만들어 프로젝트 진행과정을 수업하길 기대합니다.  "
캐글 Open Images 2019 - Object Detection(2)데이터처리! ,https://blog.naver.com/kmh03214/221571025682,20190626,"저번 글! 에 이어서 (아래)https://blog.naver.com/kmh03214/221567732969 캐글 Open Images 2019 - Object Detectionhttps://www.kaggle.com/c/open-images-2019-object-detection/data현재 Google Research에서 진행중(~20...blog.naver.com ​open Images의 Object Detection의 데이터를 어떻게 받고 확인해야 할지 알아보도록 하자! 사실 영어도 잘하고 경험이 있으신분들이라면 금방했겠지만,, 나는 처음이라 삽질을 굉장히 많이 하는 중이다.. ㅠㅠ​일단 캐글 대회의 링크인 아래의 링크에 들어가게 되면 다음과 같은 화면을 볼 수 있다.​https://www.kaggle.com/c/open-images-2019-object-detection/data Open Images 2019 - Object DetectionDetect objects in varied and complex imageswww.kaggle.com   모델 한번쯤 돌려보신 분들이라면 모델학습을 시킬때 Input데이터 / 정답데이터 가 나뉘어 있고 input에 따른 정답데이터를 맞추는 함수(모델)을 학습시켜야 한다는 것을 알고 있을 것입니다.​그래서 보면, 정답데이터를 받는 링크와 Image data를 받을 수 있는 링크를 제공해 주는겁니다. 근데 첫번째 링크인 Open Images Challenge page를 가서도 이미지를 받을 수 있는 링크가 걸려 있습니다 ^^;;​왜 따로 제공을 하느냐? 저도 잘 모르겠습니다...;;; 이미지 데이터가 엄청 크기 때문인걸까..(?)​그래서 일단 두 part로 나누고 차례차례 보겠슴당1. 정답 데이터 받기2. Image 데이터 받기​1. 정답데이터 받기https://storage.googleapis.com/openimages/web/challenge2019_downloads.html Open Images Challenge 2019 DownloadsThe annotated data available for the participants is part of the Open Images V5 train and        validation sets (reduced to the subset of classes covered in the Challenge).        Please read the V5 download page for a description of file formats.        To explore the data, use Open Images V5 v...storage.googleapis.com 이 링크에 들어가시면 or (대회사이트에서 Open Images Challenge page를 클릭하면)   요러한 화면을 볼 수 있습니다. 처음에 무진장 헷갈렸는데요.2번 이미지 받는건 두번째에 보여드리고1번은 지금 같이 볼거고...​3번. 아래로 내리다보면 지금대회 목적과 맞지 않아 쓰레기라고 표현하긴 했지만... (쓰레기는 아닙니다!!)대회에서 사용(활용)할 수 있는 다른 정답데이터(annotations) 들이 있습니다. ​사실 Open Images 대회가 3개로 분류가 되는건지(?)는 잘 모르겠으나, 제 추측이 맞다면..​1. 우리가 하고있는 OD(Object Detection)문제- 이미지 안에 객체를 찾고(바운딩박스) 객체의 종류(class)를 맞추는 대회!​2. Segmentation 문제- 이미지 안에 객체들을 테두리를 따라 분리(구분)하는 문제​3. Relationships Detection 문제뭔진 잘 모르겠지만.. 관계를 파악하는 문제 입니다. 사진속에 ""한명의 여자가 기타를 연주한다"" 이런 거 같은데.. 잘모르겠슴다.. 찾아보는거 추천드려요!​​그래서 OD(object detection)을 하기 위한 Image와 그 정답(bounding box & Image label) 만 필요한거죠!​이 부분이 제일 헷갈렸어요 ㅋㅋㅋ..... 그 다음부터는 수월합니다~​Object Detection track annotations에서 Image-level labels를 클릭하여 파일을 받게 되면 아래 사진의 두번째 파일Boxes 를 클릭하여 받게 되면 첫번째 파일이 생성됩니다!   둘다 csv(comma separated value) 파일이구요 첫번째 bbox 파일을 열면!1) Bounding Box ( XMin, XMax, YMin,Ymax ) 의 좌표정보가 주어지고2) LabelName이라고 Bounding Box 안의 객체가 무엇인지 에대한 정보 3) 이미지고유의 ImageID값이 주어지고 등등.. 정보가 주어지게 됩니다!   ​그래서 또 다음 파일(train-annotations-human-imagelabels-boxable.csv )을 열어보게 되면아래와 같이1) 이미지고유ID2) LabelName이 주어져 있습니다   ​으잉? 그럼 Image안에 객체의 Label은 /m/014j1m 이런식으로 주어졌는데 뭔지 어떻게아냐! 라고 할 것 같아요​   바로! metadata의 클래스를 다운받으면 됩니다~ 아래처럼 각 ImageLabel의 key값마다 value로 주어져 있어요   네! 1번 끝났습니다​​2. Image 데이터 받기   2번동그라미의 Download from CVDF에 들어가면 Github가 나옵니다!가면.. AWS(아마존웹서비스) 로 받는방법 Google Storage Transfer 방법이 설명 돼 있어요!​아래를 보면 아시겠지만.. 이미지 데이터 용량이 장난아니에요 ㅡㅡ.. 압축이 돼 있는데 저정도면몇 TB짜리 하드디스크가 있어도 한번에 다운받기가 꺼려질거 같습니다...    ​그래서 어떻게 훈련을 시킬지 방법을 찾아야 하는데 아래로 쭉내리면.. 이렇게 tsv파일을 받을 수 있슴다!   자 tsv이 뭐냐구요? 일단 받고 열어보면 알게 됩니다 ㅎㅎ 받고 열어보면   이런식으로 tsv파일이 써있는 것을 볼 수 있어요! (tsv = tap separated value)바로 첫번째 열에 있는 것이 이미지의 URL 입니다!!! ㅋㅋㅋㅋㅋ​맞아요.. 저는... tsv파일을 pandas로 열고.. url마다 이미지를 받아서 batch를 만들고.. 학습을 시키려구요..ㅠㅠ 하하...하하ㅏ..하하하ㅏ..하ㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏ​Colab에서 테스트코드를 돌려 보았습니다..   ​다들 저 처럼 삽질하지마시고... 편하게 한글로 보셨으면 하는 바램입니다...​(아니 왜 한글로 정리해 논 문서를 왜 난 못찾겠지? ㅠㅠ) "
[기록] tensorflow object detection api #1 ,https://blog.naver.com/eyunju2/222442364468,20210723,"​  안녕하세요! 앨리입니다오늘은 하 거의 잡담이긴 한데 할말 많음​2d object detection해서 대상 물체까지의 거리를 가져오는 프로그램을 만들어야 해서모델을 알아보던 중에 많이 사용되는 YOLO를 사용하려고 했으나,visual studio 로 빌드를 해야 하는것 같다. > visual stuiod 라이선스 문제로 사용 X​YOLO포기하고 Vgg16으로 넘어갔는데성능 안좋다는 말이 너무 많았다.​그러던 중, gta영상을 가지고 물체 인식하는 블로그의 글을 보고 따라했다.매우 잘 된다!이제 할일은 custom data를 학습 시키는 일만 남았다.​그렇게 많은 블로그를 따라했는데...​뭐 씨 \Anaconda3\ 어쩌구 App line in 40무슨 40번째 줄에 App 뭐시기가 자꾸 없다는 둥 안된다는둥 지랄에 지랄을...하.............진짜 이틀동안 삽질했다..​labelImg로 데이터 셋도 다 만들고 다 잘했는데...테스트 코드도 다 돌아갔는데...(참고로, object detection api 사용할때 tutorial 돌리라는 말이 있는데 지금 폴더가 바뀐것 같아요.)colab_tutorials 이 폴더 안에 tutorial 이 있습니다. (2021.07.23 기준)​진짜 개 빡쳐서새로운 마음을 다잡고...가상환경부터 새로 만들었다. (처음부터 이럴걸)​진짜 텐서플로우 1.13.1 깔았다가 지웠다가 1.15.0 깔았다가 지웠다가 2.2.0 도 깔아보고아주 별걸 다했다..........​tensorflow.compat.v1 인가? 이 코드로 수정하지 마시고 그냥 tf 버전 다운그레이드 하는거 추천드립니다.아니 그냥 가상환경부터 새로 만드는거 대추천... raise ValueError(""No variables to save"") ValueError: No variables to save하...... 이 에러가 대체 뭐길래....​일단 용량이 제일 많이 원인으로 꼽히는 것 같아서, 디스크 사이즈가 여유있는 곳으로 경로도 바꿔봤으나 무소용..config 의 batch size를 줄여야 하는 것 같다..​결론적으로 코드를 돌리는데 성공은 했는데, 이거 대체 언제까지 학습하는건지...https://seoftware.tistory.com/108?category=929874 1. 내 데이터로 객체 인식 학습시키기 Object Detection with Custom Dataset :: tensorflowJetson Nano에서 Yolo를 이용해서 object detection을 수행했더니 너무 느리더라고요,,, FPS가 10도 안 나오는 것 같아요,,, 그래서 찾아보니까 SSD Mobilenet 이 젯슨 나노에서 빠르게 잘 돌아가는 예제를 보고..seoftware.tistory.com 참조한 곳은 여러곳이지만, 최종적으로 성공한 것은 이 사이트와 여기서 참조한 해외 사이트를 반반 섞어서 해결했다.흑흑 ㅜㅜ​이거 돌리면서 나오는 웬만한 에러는 다 본것 같음. ㅎpycoco 뭐시기도 설치하고 참...휴.................학습 얼른 되었으면...... "
Object Detection(객체 탐지) - 오브젝트디텍션이란? ,https://blog.naver.com/85honesty/222734180751,20220516,"객체 탐지(客體探知, object detection)는 컴퓨터 비전과 이미지 처리와 관련된 컴퓨터 기술로서, 디지털 이미지와 비디오로 특정한 계열의 시맨틱 객체 인스턴스(예: 인간, 건물, 자동차)를 감지하는 일을 다룬다.  [Reference]위키백과  dailyman데일리남매일매일 좋은 추억 한두개씩 함께 만들어요😀 선한영향력 긍정적인변화 즐거운하루 같이 가치있게 만들어 가요😄www.youtube.com  구독과 공감 좋아요는 콘텐츠를 만드는 데 큰 힘이 됩니다. "
Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks ,https://blog.naver.com/grow_bigger/222783606662,20220622,"Abstract SPPnet과 Fast R-CNN는 객체 탐지 신경망의 실행 시간을 줄였는데 위치 제안 연산에서 병목 현상이 있었습니다. 이 논문에서 전체 이미지의 합성곱 특성을 공유하는 Region Proposal Network (RPN)을 제안했습니다. 이 RPN은 위치 제안에 거의 비용이 들지 않아 RPN을 객체 탐지 신경망에 적용하면 병목현상으로 인한 자원 소모를 줄일 수 있습니다. RPN이 적용된 신경망의 작동은 다음과 같습니다. 1. RPN이 객체 위치 제안 및 물체 가능성 예측2. Fast R-CNN으로 위치 제안 전달3. 기존과 같이 Fast R-CNN이 물체가 어떤 것인지 판별Fast R-CNN도 위치 제안과정을 제외하면 거의 실시간 탐지가 가능한 속도였는데 이 Faster R-CNN은 위치 제안과정을 빠르게 진행하여 VGG-16 모델에서 GPU로  5fps정도 수준으로 속도를 향상시켰습니다. ​INTRODUCTION 객채 탐지는 주로 region proposal methods과 region-based convolutional neural networks이 좋은 성능을 보여주었습니다. 그런데 R-CNN에서 봤듯이 CNN연산이 효율적이지는 않았습니다. 특히나 객체 탐지의 경우에는 속도가 느리면 활용할 수 있는 경우가 이미지로 제한이 되어버립니다. 그래서 SPPnet이나 Fast R-CNN은 속도를 향상시키는 것을 목표로 특성맵을 공유하는 방법을 통해 속도를 향상시켰고, 특히 Fast R-CNN은 모든 층을 학습시킬 수 있는게 장점임에도 불구하고 결과에 영향을 크게 미치지 않는 층을 골라 학습시키지 않음으로써 속도 향상에 더 매진한 모델입니다. 하지만 Fast R-CNN 역시 Selective Search기반의 위치 제안방식을 변경하지는 않았기 때문에 위치 제안과정에 병목 현상이 있었습니다. 이 병목현상을 완화하기 위해서 다음의 두 가지 방법을 고려해 볼 수 있습니다.1. Selective Search보다 10배 빠른 EdgeBoxes방식을 사용하는 방법2. 위치 제안과정을 GPU에서 진행논문에서는 위의 두 가지 방법을 먼저 검토해봤습니다. 다만 1번의 경우 속도가 빨라지기는 하지만 충분하지는 않고, 2번의 경우 특성 맵 공유를 하기 힘듭니다. 따라서 객체 탐지 신경망과 합성곱 층을 공유하는 RPN이라는 새로운 알고리즘을 제안하게 된 것입니다. 이 RPN의 핵심적인 아이디어는 특성 맵이 객체가 어떤 것인지 판별하는 분류 과제 뿐 아니라 위치 제안에도 사용될 수 있다는 것 입니다.​RPN은 이전의 모델처럼 여러 크기의 이미지를 사용하거나 다양한 크기의 필터를 사용하지 않는 새로운 방식입니다. 이 RPN이 속도가 빠를 수 있는 이유가 몇가지 있습니다. 일단 첫 번째는 논문에서 제시한 것처럼 RPN방식은 다양한 크기와 종횡비를 참조하는 anchor box로, 'pyramid of regression references'이라고 할 수 있는 방식을 사용합니다. 이 세부적인 내용은 나중에 나오겠지만, 결론은 다양한 크기의 이미지나 필터를 계속 사용해서 학습할 필요가 없다는 장점을 가지고 있습니다. 아래 사진만 보더라도 이미지나 필터를 반복적으로 생성하고 적용할 필요가 없다는 점은 볼 수 있습니다. 사진 1속도 향상을 만들어낸 두 번째 이유는 훈련 과정에서 번갈아 가면서  fine-tuning(파라미터 조정)을 하는 것입니다. Fast R-CNN에서 두 가지 과제를 동시에 훈련하는 과정이 있었습니다. 객체탐지는 위치 제안과 이 물체가 어떤 것인지 판단을 하는 작업으로 이루어져 있습니다. 따라서 box boungding regression과 object classification fine-tuning도 해야합니다. Fast R-CNN에는 다음의 손실함수가 정의되어 있었습니다.  식 1그런데 위 식의 우변을 동시에 최적화하면 사실 늦게 수렴합니다. 실제로 GAN모델 같이 판별자와 생성자가 동시에 잘 훈련이 되야하는 경우도 번갈아 가면서 최적화를 해주는 경우가 많습니다.​RELATED WORK[Object Proposals] 대표적인 위치 제안은 grouping super-pixels (질감 색상등을 기준으로 그룹핑하는 방법; Selective Search, CPMC, MCG)과 sliding windows(objectness in windows , EdgeBoxes)가 있습니다.​[Deep Networks for Object Detection]가장 대표적인 연구는 R-CNN입니다. 사실상 객체 탐지의 중요한 모델 중 첫 모델이므로 다른 연구들도 이 모델에 기반한 경우가 많았습니다. object proposal다음에 Deep Networks for Object Detection을 소개하는 이유는 이유도 R-CNN으로부터 본격화된 발전상때문일텐데, R-CNN은 주로 분류기의 역할이었습니다. 객체 위치 제안은 R-CNN이 했다기 보단 region proposal module이 한것입니다. 당시에는 주로 Selective Search방식을 사용했던 것일 뿐입니다. 이후에는 object bounding box를 예측하는 연구들이 몇 개 있는데 그중 대표적인 것은 완전연결층으로 박스의 좌표를 예측하여 localization하는 OverFeat이 있습니다. 다만 OverFeat은 하나의 물체에 대한 에측을 했기 때문이 이를 좀 더 일반화한 MultiBox가 등장했습니다. MultiBox는 신경망이 다수의 위치 제안 영역을 생성합니다. 그러면 이 클래스와 무관한 박스영역은 R-CNN의 입력이 되는 구조입니다. R-CNN의 입력이 되려면 역시 입력의 크기가 일정해야합니다. 따라서 이미지를 잘라냅니다. 마지막으로, DeepMask방식은 segmentation 제안을 하도록 설계된 방식입니다.이러한 제안 과정에 대한 연구뿐 아니라 합성곱 연산에 대한 연구에 관심이 증가했는데 그중에는 Adaptively-sized pooling(SPP)나 Fast R-CNN이 합성곱 연산을 공유하는 대표적입니다.​ FASTER R-CNN 사진 2Faster R-CNN의 전반적인 구조는 위의 사진에서 볼 수 있습니다. 위치를 제안하는 모듈과 Fast R-CNN detector모듈이 하나의 신경망으로 결합된 모습입니다.​[Region Proposal Networks]Region Proposal Network (RPN)은 어떠한 크기의 이미지를 입력으로 받고 사각형의 객체 제안 집합을 출력합니다. 저자들은 Fast R-CNN 신경망과 RPN이 연산을 공유하도록 하는게 목표였는데, 이를 위해 두 신경망이 일반적인 합성곱 신경망 집합을 공유할 거라고 가정을 하고 조사를 해봤는데, Zeiler and Fergus model은 5개, Simonyan and Zisserman model은 13개정도가 있었습니다. 일단 연산을 공유하려면 RPN이 합성곱 층을 이용하여 무언가 출력을 만들어야 합니다. 이 Faster R-CNN에서는 합성곱 층을 통과한 특성 맵 위에서 작은 신경망을 밀면서(slide) 객체 제안을 만들어 냅니다. 위의 사진에서 보듯이 특성 맵은 마지막 공유 합성곱 층의 결과입니다. ​여기서 잠깐 Fast R-CNN의 구조를 참고해볼 수 있습니다.  사진 3위의 사진을 보면, 이 Faster R-CNN과 차이가 두드러지는 부분이 객체 제안부분인데요, Fast R-CNN에서는 RoI projection을 따로 준비해서 가져다 붙이는 방식입니다. 따라서 이부분이 연산을 공유하지 못했습니다. 심지어 변형된 이미지에 맞춰서 RoI도 같이 변해야하니 객체 제안 과정에서 비효율적인 연산이 발생할 수 있는 구조였습니다.다시 Faster R-CNN의 RPN으로 돌아가서, 아까 특성 맵 위에서 slide하는 작은 신경망은 입력을 n × n spatial window으로 받은 후 이 각 sliding window는 저차원 특성에 맵핑됩니다. 이 저차원 특성은 box-regression layer과 box-classification layer 이 두개의 완전연결층으로 들어갑니다.​이 과정은 아래의 사진에서 그 구조를 한눈에 볼 수 있습니다. 사진 4논문에서는 이러한 구조는 n×n convolutional layer로 구현되고 이 뒤에는 1 × 1 convolutional layers가 있다고 하는데 이미지 크기에 무관하도록 하기위함입니다. [1]​즉, 아래의 흐름을 갖습니다.1. 윈도우에서 영역 추출(HxWxC) -> 256채널 conv => INRETMEDIATE LAYER (HxWx256)2. INRETMEDIATE LAYER    ㄴ1x1 conv (2k channel) => HxWX2k    ㄴ1x1 conv (4k channel) => HxWX4k​[Anchors] 위에 있던 사진을 보면 파란색의 'multiple reference'를 볼 수 있습니다. 저자들은 이 각각의 sliding-window에서 동시에 여러 위치 제안을 하는데 이걸 'anchor'라고 명명했습니다. k개까지 위치 제안을 할 수 있도록 설정되었으므로 위치 제안(좌표 및 가로/세로 길이로 총 4개로 표현)은 총 4k, 분류기는 객체인가 아닌가로 2 × k =2k개만큼의 출력을 갖게됩니다. 각 anchor는 sliding window의 중심에 위치하고 스케일과 종횡비에 연관이 되어 있는데 저자들은 스케일에 따라 3개, 종횡비에 따라 3개 총 k=9개로 설정하였습니다. 따라서 하나의 이미지를 처리한 W × H 크기의 합성곱 특성맵에서는 WHk개의 anchor가 나옵니다.​앵커를 중심으로 다양한 종횡비의 박스들이 k개 생기면 이 박스에 물체가 있는지 없는지 판단한 후(위 문단의 객체인지 아닌지를 판단하는 분류기) 박스의 위치를 재조정하는 과정을 거칩니다.​[Translation-Invariant Anchors ]Faster R-CNN은 translation invariant합니다. anchor나, anchor과 연관된 위치 제안을 계산하는데 중요한 물체가 이미지내에서 움직이면 위치 제안도 따라 움직여야 한다는 것입니다. 결국 어느 위치에건 물체의 위치 제안이 잘 이루어지는 속성입니다. 특히 이 속성 때문에  translation invariant하지 않은 MultiBox모델보다 모델의 크기가 훨씬 작아 MultiBox의 완전연결층의 파라미터의 절반 정도의 파라미터만 필요합니다. 따라서 데이터셋이 작은 경우 발생할 확률이 높은 과적합이 발생할 위험이 적을 것으로 기대할 수 있습니다.​[Multi-Scale Anchors as Regression References ]multi-scale prediction에는 image/feature pyramid를 사용하거나 다양한 크기의  sliding window를 특성맵에 사용합니다. image/feature pyramid방식은 이미지의 크기를 다양하게 준비한 후 각각에 대해 특성맵을 구하는 방식입니다. 따라서 여러모로 시간을 많이 소비합니다. sliding window의 크기를 다양하게 사용하는 경우는 필터 피라미드 형식으로 생각해 볼 수 있는데, 다양한 필터 크기를 이용해서 모델 각각을 훈련시키는 것입니다. 보통은 이미지 피라미드 방식과 필터 피라미드 방식을 같이 적용합니다. 논문에서 제시한 방식은 위의 두 방식과는 다른 방식입니다. 사진 4를 보면 sliding window는 하나고, anchor가 다양한 형태로 있습니다. 그러니까 분류와 박스 회귀 작업이 다양한 크기와 종횡비를 갖는 anchor의 참조로 이루어집니다. 그 외의 이미지, 필터는 하나의 크기만을 사용합니다. 이러한 방식을 통해 Fast R-CNN detector에서처럼 단일 크기 이미지에서 계산된 합성곱 특성들을 사용할 수 있습니다.​[Loss Function]RPN을 훈련시키기 위해 각 앵커는 객체인지 아닌지 이 두가지 class 라벨을 사용합니다. 이때 두 클라스 중 객체임을 뜻하는 클래스에 속하는 경우를 정하는건 직접 설정을 해야하는데 논문에서는 IoU가 가장 높은 anchor가 객체인 클래스에 속하도록 설정을 했습니다. 물론 임계치를 두고 예를 들어 논문에서 기술한 바와 같이, IoU가 0.7이상인 경우를 객체임을 의미하는 클래스로 분류할 수 있지만 종종 이런 경우에는 객체를 둘러싼 것으로 분류되는 경우가 아예 없는 경우가 발생합니다. 이렇게 되버리면 정상적인 학습이 불가능합니다. 다만 객체가 아닌 클래스에 속하는 경우는 최저치인 anchor를 택하는게 아닌 0.3이하인 경우 해당 클래스로 분류하였습니다.  ​위와 같이 정의를 하면 아래처럼 손실함수를 정의할 수 있습니다.  i : anchor인덱스  pi: i번째 anchor가 객체일거라고 예측한 확률  / p*i: ground-truth, anchor가 positive면 1아니면 0  ti : 4개의 파라미터로 구성된 모델이 그린 경계선 박스의 좌표  / t*i:positive anchor의 ground-truth box Lcls ;  객체인지 아닌지에 대한 2진분류의 로그 손실값 Lreg: 박수 회귀 손실함수, Lreg(ti , t∗i ) = R(ti −t∗i )로 정의하는데 이때 R은 smooth L1 출처: Fast R-CNN 손실함수의 p∗ i Lreg는 anchor가 객체를 감지한 경우에만 박스 회귀 작업을 수행한다는 의미입니다.Ncls와 Nreg로 정규화를 하기 위해 곱해진 건데, 논문에서는 Ncls는 미니 배치 사이즈, Nreg는 anchor location수로 설정합니다. 참고로 두 loss간의 균형을 맞춰주는 람다는  λ = 10으로 설정하여 사용했다고 하며 정규화는 굳이 필요하지는 않고 생략될 수 있다고 합니다.  ​식에서, bounding box regression를 위해서 아래의 파라미터를 사용합니다. x,y는 박스의 좌표, w,h는 각각 너비와 높이x, xa, x*는 각각 예측된 박스, anchor 박스, ground-truth 박스(y,w,h도 동일)손실함수 식을 보면 t와 t*의 차이를 줄이는 게 목표가 될 것입니다. smooth L1식에서의 x를 구하기 위해 단순히 t와 t*를 빼보면 x와 x*같이 예측과 실제가 같아지도록 하는 것이 목표가 되는 것을 알 수 있습니다.이러한 bounding-box regression는 기존의 모델에서도 매번 수행되었었는데, 기존의 모델에서는 임의의 크기의 RoI에서 풀링된 특성 위에서 수행되었고 회귀 가중치가 모든 영역 크기에서 공유되었었습니다. 그러나 이번 방식의 경우 회귀 작업이 수행되는 특성(feature)은 특성 맵내에서 (3 × 3)으로 균일하게 한정된 공간 크기입니다. 따라서 다양한 크기에 모델이 적용되기 위해 k개의 bounding-box regressors가 학습됩니다. ​ [Training RPNs]RPN은 역전파와 확률적 경사하강법(SGD)에 의해 훈련이 가능합니다. 저자들은 “image-centric” sampling strategy를 이용하여 신경망을 학습시켰습니다. 확률적 경사 하강법은 이제는 주로 미니 배치를 뽑아 가중치를 수정하는 방법을 가리키는 용어가 되었는데, 이 경우도 마찬가지로 미니배치를 샘플링합니다. 하나의 이미지가 있고 여기에 sliding window가 있고 이 sliding window에는 anchor들이 있습니다. 이 anchor들은 객체라고 분류되는 것과 아닌 것이 있을텐데, 미니배치를 구성할때 256개의 anchor를 랜덤하게 추출하여 손실함수값을 구합니다. 모든 anchor의 손실함수를 최적화할 수도 있지만 negative smaple이 많으면 편향이 되서 심플링을 무작위하게 하여 positive와 negative의 비율을 최대 1:1까지되도록 샘플링을 수행하는 것입니다. ​[Sharing Features for RPN and Fast R-CNN]RPN을 학습시키는 방법을 지금까지 다루었습니다. 그런데 전체적인 신경망을 훈련하는 방법은 아직 다루지 않았습니다. 부분부분을 학습시키고 초기화를 하고 전이학습을 하더라도 전체적인 모델을 완결된 학습을 진행하여 하고자 하는 작업에 적합하도록 조정해야합니다. 논문에서는 총 3가지 방법을 통해 전체적인 모델을 학습하는데 각 방법은 다음과 같습니다. Alternating training이 방법은 RPN을 먼저 학습시키고 RPN이 위치 제안을 생성하면 이것을 이용하여 Fast R-CNN부분을 학습시킵니다. Fast R-CNN 신경망이 학습되면 이 결과를 가지고 RPN을 초기화합니다. 이후에는 이러한 절차를 반복하는 것인데, 동시에 두 모듈을 학습하면 수렴이 잘 안되기 때문에 사용하는 방식입니다. GAN처럼 판별자와 구분자가 모두 학습이 필요한 경우도 이렇게 번갈아가며 학습시킵니다.​Approximate joint training이 방법은 RPN과 Fast R-CNN신경망을 학습시에 하나의 신경망으로 합치는 방법입니다. Fast R-CNN detector을 학습할 경우에는 순전파는 region proposal이 고정된 것같은 상태로 생성합니다. 역전파시에는 일반적인 경우와 비슷하게 작동하는데, 공유 신경망층에서 RPN loss와 Fast R-CNN loss가 결합됩니다. 구현은 쉽지만 제안된 박스의 좌표에 대한 미분을 무시하기 때문에 근사적인 방법입니다. 다만 성능이 크게 떨어지지는 않는 반면 학습시간을 alternating training에 비해 25-50%가량 줄여줍니다. ​Non-approximate joint training위에서와 달리 이론적으로 유효한 backpropagation solver는 박스의 좌표에 대한 미분을 포함한다는 점을 이용합니다. 박스 좌표에 대해 미분가능한 RoI pooling층이 필요합니다. 다만 간단하지 않은 문제입니다. RoI wraping 층으로 해결 방안을 구할 수 있다고 하는데 이 이상의 내용은 논문의 범위를 넘어가기 때문에 구체적인 내용이 더 있지는 않습니다.​[4-Step Alternating Training]번갈아가며 훈련하는 과정을 4단계로 구성한 학습 방법입니다. RPN훈련: ImageNet-pre-trained model로 초기화하고 위치 제안 과제를 수행하며 미세 조정합니다. Fast R-CNN부분을 학습: 앞단계의 RPN에서 생성된 위치 제안을 이용하여 Fast R-CNN부분을 학습시킵니다. 역시 ImageNet-pre-trained model로 초기화합니다. 이 단계까지는 두 신경망이 합성곱 층을 공유하지 않습니다.탐지 신경망을 이용하여 RPN을 훈련합니다. 다만 두 신경망끼리 공유하는 합성곱 신경망은 가중치를 동결하고 RPN고유의 층만 미세조정합니다. 이때부터 신경망이 합성곱층을 공유합니다.공유 합성곱 층은 계속 고정하고, Fast R-CNN 고유의 층만 미세조정합니다.​[Implementation Details]​ 위 사진은 anchor을 사용했을때의 강점의 결과라고 볼 수 있습니다. 이미지나 필터 피라미드를 사용하지 않고도 넓은 범위의 크기와 종횡비를 수용할 수 있다는 점을 사진에서 다양한 박스의 형태와 크기로부터 알 수 있습니다. 논문의 저자는 논문의 알고리즘이 예측의 크기가 기본적인 필터(수용장)크기보다 클 수도 있다고 합니다. 그러니까 물체의 중간이 보여야만 물체에 대한 규모를 알 수 있는건 아니라는 뜻인 것 같습니다.​그런데 anchor를 사용할 때 주의를 해야합니다. 저자들은 이미지 경계선을 가로지르는 anchor box는 무시하여 손실함수 계산에서 제외를 했다고 하는데, 1000 × 600 크기의 이미지의 경우 나오는 20000가지의 anchor를 이미지당 6000개로 줄입니다. 제외하지 않으면 오류가 발생하며 수렴도 하지 않는다고 합니다. anchor의 문제 다음에는 객체탐지의 어쩌면 기본적인 문제인 위치 제안 중첨 문제가 남습니다. 저자들은 non-maximum suppression (NMS)을 cls 점수에 기반하여 이러한 이미지당 2000개의 위치 제안만 남겨놓습니다.​ CONCLUSION탐지 신경망과 합성곱 특성을 공유함으로써 위치 제안 과정을 거의 비용이 들지 않도록 할 수 있었습니다. 이를통해 통합된, 심층학습 기반 객체 탐지 모델이 거의 실시간 프레임 속도로 실행되도록 할 수 있었습니다. 뿐만 아니라 학습된 RPN은 region proposal의 질을 향상시켰고 결국 객체 탐지 정확도도 향상되었습니다.  참고자료[1] 갈아먹는 Object Detection [4] Faster R-CNN (tistory.com) "
Object Detection - YOLO(2) ,https://blog.naver.com/handuelly/221836732810,20200304,"# YOLO 지금까지 객체 검출을 위한 여러가지 모델들을 다뤄봤는데, 간단하게 요약하자면 아래와 같다.        · Haar-like : 가장 단순한 아이디어, 밝고 어두운 패턴을 기준으로 검출        · Cascade : 여러 단계를 거치면서 객체라고 판단되는 영역만 검출하는 방식        · Ada boost : 간단하고 단순한 약분류기를 반복 적용하여 강분류기처럼 되는 것        · Hog : Edge를 모두(방향 속성까지 포함) 계산하는 Strong Classifier        · ORB : 로컬 특징점을 기반으로 객체 검출, 회전 또는 크기 변화 등에 대한 변수에도 검출 용이​위 방식들은 특정한 패턴 또는 정확한 특징점을 근거로 찾기 때문에 한편으로는 ‘제한적이다’는 단점이 있다. YOLO는 CNN을 사용한 새로운 방법이고, 지난 포스팅에서 간략하게 다뤘기 때문에 조금 더 자세히 다룬 후 코드를 작성하고 실질적으로 결과물을 확인해보려고 한다. Object Detection - YOLO# YOLOYOLO는 자유롭게 산다는 '욜로'가 아니라, ""You Only Look Once""의...blog.naver.com (지난 포스팅 링크)  # Semantic Segmentation Segmentation을 먼저 설명하면, Detection과 다른점을 이해해야 한다.검출은 객체가 있는 위치를 찾아서 물체에 Boxing을 하는 과정이고, 세그멘테이션은 영상을 픽셀 단위로 구분해서 각 픽셀에 어떤 물체(class)가 있는지 구분하는 문제이다.  출처 :https://missinglink.ai/guides/tensorflow/tensorflow-image-segmentation-two-quick-tutorials/ ​다시 말해, Semantic Segmentation은 '이미지를 의미있게 분할""하는 것을 목적으로 한다.이미지의 각 픽셀이 어느 클래스에 속하는지 예측하고, 물체들을 깔끔하게 분할해내는 것이 성능 지표이다.대표적인 알고리즘으로는 FCN(Fully Convolutional Networks)가 있는데, 포스팅 하단 링크를 참고한다.​추가로 참고할만한 영상을 함께 보면, 어떤 결과물을 위한 알고리즘인지 이해하기 쉬울 것이다.  # YOLO Algorithm YOLO는 격자 그리드로 나누어 한 번에 클래스를 판단하고, 이를 통합해 최종 객체를 구분하는 방식이다.기존 방식들과 달리(스캔을 하지 않음) 객체 검출을 이미지 픽셀 좌표에 대응되는 bounding-box를 찾고, 이에 대한 class 확률을 구하는 Single Regression Problem으로 해결한다.    cf) R-CNN은 region proposal이라는 수백개의 이미지 후보를 생성하고 각각에 대해서 분류    cf) 수행 속도 : R-CNN 20s, Fast R-CNN 2s(0.5FPS), Faster R-CNN 140ms(7~8FPS), YOLO 45~155FPS  YOLO 알고리즘 YOLO 프로세스에 대한 이해는 지난 포스팅에서도 적었지만, 간략하게 다시 정리해보겠다.우선 Input Image를 SxS grid로 분할하고(해당 셀에 물체의 중심 위치로 가정), cell은 B개의 bounding box와 각 객체 존재에 대한 confidence score로 구성한다.그리고 cell은 C개의 클래스 확률로 구성 박스가 있는데, 결과적으로 마지막 prediction layer는 S x S x ( B x 5 + C ) 사이즈가 된다.즉, 각 셀마다 크게 3가지(오브젝트의 좌표 정보 + 물체일 확률 + 클래스 확률) 값이 있을 때 각 셀마다 물체라고 추정되는 2개씩의 박스를 가져오고, 최종적으로 선택한다.  # 문제점 우선 confidence가 낮은(object가 아니라고 추정되는 것)은 제거해야 한다.Sliding window를 이용하면, 한 픽셀을 기준으로 할 때 특정 객체라고 여러 번(중복) 추정 되는 문제가 있다.즉, 한 영역에서 다수의 bounding box가 발생하면, 이것을 통합(grouping)해 대표값 하나만 검출해야 한다.하지만 단순 평균 값을 이용해서 통합하면 부수적인 문제(얼굴을 좁은 영역에서 검출하거나, 두 얼굴이 가까이 있는 경우)가 발생할 수도 있다.​Yolo에서는 2가지 방법으로 이 문제를 해결한다.1) Thresholding by Object Confidence Score        - confidence가 낮은 것을 버리는 방법이다.        - 각 bounding box의 objectness score가 특정 임계값 이하면 모든 속성을 0으로 설정한다.        - 참고 : https://dojinkimm.github.io/computer_vision/2019/08/06/yolo-part4.html2) Non-maximum Suppression        - 주변 상대 값들과의 차이가 상대적으로 최대인 것을 구한다.        - 차이가 최대가 아닌 값들을 삭제한다는 것으로 이해하면 된다.        - IoU값으로 Proposal을 모두 정렬시키고, RoI 값이 높은 Proposal과 다른 Proposal에 대해 오버레핑을 비교한 뒤, 오버레핑이 높은 것 중 임계감 이상이면 삭제한다. 이 과정을 반복하면서 하나만 남긴다.​다음 포스팅에서는 본격적으로 YOLO 코드를 작성하고, 결과물을 확인해보자.  # 참고 링크 CNN을 활용한 주요 Model - (4) : Semantic SegmentationAn Ed editionreniew.github.io semantic segmentation의 목적과 대표 알고리즘 FCN의 원리요즘 semantic segmentation을 활용하는 연구를 하나 진행하고 있다. semantic segmentation, 이름만 봐서는 이것이 무엇인지 감이 안오는 분들이 있을 것이다. 구지 번역하자면 '의미적 분할' 정도로 번역이 가능..bskyvision.com NMS (non-maximum-suppression)오래간만의 포스팅. 요즘 딥러닝을 이용한 여러 Object Detection 알고리즘을 구경하는데, 대부분 NMS (non-maximum suppression)을 사용하여 연산량을 줄이고, mAP도 올리는 효과를 본다고 한다. 물론 필수로 필..dyndy.tistory.com 비최대값 억제 (NMS) 알고리즘비최대값 억제 (NMS) 알고리즘 Sep 27, 2017 • 정한솔 본 포스트에서는 영상 처리 및 객체 검출에서 흔히 사용되는 비최대값 억제(Non maximum supression) 알고리즘에 대해서 설명하겠습니다. 캐니 엣지 비최대값 억제 알고리즘은 국지적인 최대값을 찾아 그 값만 남기고 나머지 값은 모두 삭제하는 알고리즘입니다. 가장 단순한 사용례로는 OpenCV에서도 지원하는 외곽선 검출 알고리즘인 캐니 엣지(Canny Edge) 알고리즘이 있습니다. 캐니 엣지 알고리즘에서는 가우시안 필터와 소벨 마스크를 거쳐서 나온 색상 ...inspace4u.github.io 6-6 Non Maximum SuppressionObject Detection 모델을 이용하여 객체를 찾아내고나면 동일한 타겟에 대한 결과가 다수 겹쳐서 나오게 됩니다. 이를 해결하기 위해서 non-maximum suppression (NMS) 알고리즘을 사용해야 합니다. NMS를 간단히..jetsonaicar.tistory.com "
Object Detection과 Segmentation ,https://blog.naver.com/doctor_song/222699956743,20220413,"1. classification : cat 이라 알아냄, 1개의 객체 2. localization : 객체를 위와 같이 box로 나타냄, 1개의 객체 3. Object Detection : 1,2번을 동시에 수행, 다수의 객체     즉 cat(1번), 빨간box(2번), dog(1번), 파란box(2번), duck(1번), 초록box(2번)4. Instance Segmentation : 1번 시행 but 2번 처럼 box 가 아닌 pixel 단위로 물체 구별, 다수의 객체 ​※ Bounding box를 찾는것은 regression 이다. (cat : classification)​출처 : https://lynnshin.tistory.com/3​​ "
파이썬 ImportError: cannot import name 'preprocessor_pb2' from 'object_detection.protos' 에러 ,https://blog.naver.com/alaldi2006/222149498745,20201120,ImportError: cannot import name 'preprocessor_pb2' from 'object_detection.protos'​https://github.com/protocolbuffers/protobuf/releases Releases · protocolbuffers/protobufProtocol Buffers - Google's data interchange format - protocolbuffers/protobufgithub.com protoc 다운로드압축풀기​환경변수에 경로 지정시스템속성 - 환경변수 - 시스템변수 Path에 새로 만들기로 추가 C:~~~~\protoc\protoc-3.14.0-win64\bin​protoc object_detection 폴더 protoc object_detection/protos/*.proto --python_out=.​https://blog.naver.com/alaldi2006/222149491430 파이썬 ./object_detection/protos/anchor_generator_pb2.py: Permission denied 파이썬파일 권한 거부 에러./object_detection/protos/anchor_generator_pb2.py: Permission denied​https://github.com/protocolbu...blog.naver.com 권한 거부 에러나서 다시 수정​protoc 3.4 버전으로 다운로드 및 protoc object_detection/protos/*.proto --python_out=.​​https://blog.naver.com/alaldi2006/222149495002 파이썬 ImportError: cannot import name 'preprocessor_pb2' from 'object_detection.protos' 에러ImportError: cannot import name 'preprocessor_pb2' from 'object_detection.protos&#x...blog.naver.com ​아나콘다 4.9.2 / Python 3.7 환경짜증나게 자꾸 protoc 했는데도 에러가 뜬다.알고보니 아나콘다 tensorflow object detection 경로에 파일이 없어서 그런거였음​C:\Users\유저이름\Anaconda3\envs\tensorflow\Lib\site-packages\object_detection-0.1-py3.7.egg\object_detection\protos여기에 object_detection/protos 에서 protoc 한 파일들 복사해줌. ​​​ 
[ML 용어] Agnostic  (feat. class-agnostic) | 컴퓨터 비전에서의 agnostic이란🙄? (ft. object detection) ,https://blog.naver.com/cheeryun/222358332253,20210520,"agnostic ↔  aware - agnostic : 초경험적인 것의 존재나 본질은 인식 불가능하다 (있든 없든 우린 느끼지 못한다 🤨). - awareness : 알아차림이란 개체가 개체-환경의 장(domain)에서 일어나는 중요해선 내적/외적 사건들을 지각하고 체험하는 것이다🤔.​사람이 신(神, god)이나 잡귀(鬼, ghost)를 본적도 없으면서 믿는 것이 agnostic에 해당한다 (예수를 만났다든 귀신을 봤다든 하는 간증은 일반적인 게 아니기 때문에 생략). ​​Agnostic & Aware in Computer Vision(ex) Object detection을 하고 있다고 가정하자:- class-agnostic : 감지된 객체의 클래스(사람? 사물? 등등)는 인지가 불가능하다. 단지, 그곳에 객체가 존재한다는 정도만 안다 (i.e., 물체의 종류를 모르는 상태). - class-aware : 물체의 종류가 뭔지도 인지한 상태 ​​(usage) - 영상에서 'foreground' 객체만 찾고싶다. 객체의 클래스는 중요하지 않다 ⇒ class-agnostic detector - 감지만 객체가 어떤 클래스인지도 분류하고 싶다 ⇒ class-aware detector ​  Reference [1] class-agnostic 이란? / class-aware detector  vs. class-agnostic detector​​​ "
9. TensorFlow2 Object Detection API설치_(3) Setting for drawing result with checkpoints ,https://blog.naver.com/alsdhr0155/222583954666,20211201,"안녕하세요! 오늘은 드디어 지금까지 설치한 환경을 바탕으로 결과를 만들어보려고합니다.주피터 노트북에서 작업합니다. 학습데이터를 불러온 후 설치한 detection 모델을 활용해 인식률이 표시되는 이미지파일을 생성합니다.​아나콘다 프롬프트에서 자신의 가상환경에 진입하신 후, jupyter notebook을 실행해주세요!그 후, 자신의 파일경로에 맞게 코드를 수정하신 후 실행하면 됩니다!  1. 이미지 불러오기 import os, fnmatchos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'    # Suppress TensorFlow logging (1)import pathlibimport tensorflow as tf#tf.get_logger().setLevel('ERROR')           # Suppress TensorFlow logging (2)listOffiles = os.listdir('./test_data-1')  ##### 수정Img_filenames = []print(listOffiles)# Enable GPU dynamic memory allocation#gpus = tf.config.experimental.list_physical_devices('GPU')#for gpu in gpus:#    tf.config.experimental.set_memory_growth(gpu, True)def Load_images():    base_path = ""C:/test/workspace/training_1/test_data-1/""   #####수정    pattern = ""*.jpg""  #####수정    for entry in listOffiles: ## Get file names        if fnmatch.fnmatch(entry, pattern):            entry = base_path + entry            Img_filenames.append(str(entry))            #print(entry)    image_paths = []    for filename in Img_filenames:        #image_path = base_path + filename        image_path = filename        #image_path = pathlib.Path(image_path)        image_paths.append(str(image_path))        #print(filename)    return image_pathsIMAGE_PATHS = Load_images()print (IMAGE_PATHS)// ['212015063716813000010001.jpg', '212015064090156000010001.jpg', ...]  2. 모델 로드하기 import timefrom object_detection.utils import label_map_utilfrom object_detection.utils import config_utilfrom object_detection.utils import visualization_utils as viz_utilsfrom object_detection.builders import model_builderPATH_TO_MODEL_DIR = ""C:/test/workspace/training_1/models/my_ssd_resnet50_v1_fpn""  #####수정PATH_TO_CFG = PATH_TO_MODEL_DIR + ""/pipeline.config""PATH_TO_CKPT = PATH_TO_MODEL_DIR + ""/""print('Loading model... ', end='')start_time = time.time()# Load pipeline config and build a detection modelconfigs = config_util.get_configs_from_pipeline_file(PATH_TO_CFG)model_config = configs['model']detection_model = model_builder.build(model_config=model_config, is_training=False)# Restore checkpointckpt = tf.compat.v2.train.Checkpoint(model=detection_model)ckpt.restore(os.path.join(PATH_TO_CKPT, 'ckpt-4')).expect_partial()@tf.functiondef detect_fn(image):    """"""Detect objects in image.""""""    image, shapes = detection_model.preprocess(image)    prediction_dict = detection_model.predict(image, shapes)    detections = detection_model.postprocess(prediction_dict, shapes)    return detectionsend_time = time.time()elapsed_time = end_time - start_timeprint('Done! Took {} seconds'.format(elapsed_time))//Loading model... Done! Took 0.046866655349731445 seconds  3. 학습데이터에 맞게끔 클래스 작성한 label map(pbtxt파일) 불러오기 PATH_TO_LABELS = ""C:/test/workspace/training_1/annotations/label_map.pbtxt"" #####수정category_index = label_map_util.create_category_index_from_labelmap(PATH_TO_LABELS,                                                                    use_display_name=True)  4. 학습데이터를 시험시킨 결과 인식률로 표시하는 파일 생성 import numpy as npfrom PIL import Imageimport matplotlib.pyplot as pltimport warningswarnings.filterwarnings('ignore')   # Suppress Matplotlib warnings%matplotlib inline def load_image_into_numpy_array(path):    """"""Load an image from file into a numpy array.    Puts image into numpy array to feed into tensorflow graph.    Note that by convention we put it into a numpy array with shape    (height, width, channels), where channels=3 for RGB.    Args:      path: the file path to the image    Returns:      uint8 numpy array with shape (img_height, img_width, 3)    """"""    return np.array(Image.open(path))for image_path in IMAGE_PATHS:    print('Running inference for {}... '.format(image_path), end='')    image_np = load_image_into_numpy_array(image_path)    # Convert image to grayscale    # image_np = np.tile(    #     np.mean(image_np, 2, keepdims=True), (1, 1, 3)).astype(np.uint8)    input_tensor = tf.convert_to_tensor(np.expand_dims(image_np, 0), dtype=tf.float32)    detections = detect_fn(input_tensor)    # All outputs are batches tensors.    # Convert to numpy arrays, and take index [0] to remove the batch dimension.    # We're only interested in the first num_detections.    num_detections = int(detections.pop('num_detections'))    detections = {key: value[0, :num_detections].numpy()                  for key, value in detections.items()}    detections['num_detections'] = num_detections    # detection_classes should be ints.    detections['detection_classes'] = detections['detection_classes'].astype(np.int64)    label_id_offset = 1    image_np_with_detections = image_np.copy()    viz_utils.visualize_boxes_and_labels_on_image_array(            image_np_with_detections,            detections['detection_boxes'],            detections['detection_classes']+label_id_offset,            detections['detection_scores'],            category_index,            use_normalized_coordinates=True,            max_boxes_to_draw=200,            min_score_thresh=.30,                        #max_boxes_to_draw=200,            #min_score_thresh=.30,            agnostic_mode=False)    plt.figure(figsize=(32,32))    #plt.figure()    #plt.figure(figsize=(4,4))    plt.imshow(image_np_with_detections)    print('Done')    plt.savefig(image_path, dpi=300)    #plt.savefig(image_path, dpi=300, bbox_inches='tight')    #plt.savefig(image_path)plt.show()# sphinx_gallery_thumbnail_number = 2//Running inference for C:/test/workspace/training_1/test_data-1/212015063716813000010001.jpg... Done//Running inference for C:/test/workspace/training_1/test_data-1/212015064090156000010001.jpg... Done 위의 코드를 실행하시면 주피터 노트북 상에서 바로 인식률을 확인할 수 있습니다.퍼센트 별로 일치하는 정도를 나타내는건데요, 저희가 학습시킨 애들은 인식률이 그리 높진 않네요^^앞으로도 많은 수정이 필요해 보입니다 하하 --... 주피터 노트북에서 확인할 수 있는 인식률뿐만 아니라 원래 이미지 파일이 인식률로 표시된 이미지 파일로 변경됩니다.  참 긴 여정이었습니다. 오류도 많고 배움도 많았어요.. 학교 다닐 때 과제로 나오는것도 이렇게 열심히 하진 않았던 것 같은데, 이게 일이 되니까 마무리는 하네요~!! 참 뿌듯합니다. 얼른 복학해서 공부하고싶어요..(학교 과제에 치이는 미래의 내가 본다면 뭔소린가 싶겠지?)​같은 인턴 분들과 함께 해서 마무리 할 수 있었던 것 같아요! 혼자였다면 이미 초장부터 그만 뒀을 것 같답니다^_^'왜 오류가 나는지', '저는 여기서 오류가 났는데 ~님은 어디서 오류가 나셨고 어떤 오류가 나셨는지', '저는 이런 오류는 처음 보는데 ~님은 왜 이런 오류가 났을까요?', '저 이 오류 이렇게 해결했는데 코드 보내드릴테니 한번 해보시는거 어때요?', '이 과정이 오류가 나는 과정이었던가요..? 저는 처음 보네요ㅠ', '이거 찾아봐도 수정해봐도 해결이 안되는데 과장님한테 여쭤보는건 어떨지(과장님이 도움 주신 부분도 있었어요! 십년감수...)' 등 참 질문도 많이했고, 대답도 많이 한 시간이었어요. 사실 저는 그 오류들이 왜 해결됐는지 모르겠는게 아직도 많아요!! 솔직히 진짜 어이없어요. 근데 또 이게 별미랍니다~​그래두 3학년 1학기 마쳤는데, 꽤 많이 배웠다고 생각했어요. 근데 정말 아니었습니다. 특히 컴퓨터 관련 전공 지식이 정말 부족함을 깨달았어요. 전 프로그래밍 언어랑 알고리즘, DB 정도만 알아도 '뭐든 잘 할 수 있지!'라고 생각했는데, 정말 '컴퓨터'와 관련된 부분을 모르니 프로그램이 돌아가는 매커니즘이 이해가 안되더라고요. 그게 이해가 안되니까 오류를 어디서부터 수정해야하는지 감이 안왔어요. 파일 경로를 어떻게 설정하는지나 기본적인 조작이나 여러 설정들이요. 계속 파고들다보니까 '컴퓨터는 어떻게 켜지는거지?'라는 생각에 이르더라구요. 컴퓨터가 동작하는게 당연하다 생각해서 그게 '왜' 그런건지 알지도 못하고 냅다 코딩만 배운 것 같아 아쉬웠어요. 다음학기에는 컴퓨터 구조나 os, 네트워크 관련 과목을 수강하고싶네요. 하드웨어 관련 과목도 열심히 들어서 조금 더 탄탄한 지식을 가진 사람이 되고싶다는 다짐을 합니다. ​이제 인턴도 곧 끝나가네요. 아마 다음에는 인식률 관련해서 어떻게 해석하는지 포스팅할 것 같습니다.오늘이 12월 1일이에요. 다들 남은 2021년도 잘 보내시길 바랍니다:) "
AI 학습을 통한 Object Detection(python)(1) ,https://blog.naver.com/jinwoo6612/221788691901,20200129,"#opencv #python #COCO #object_detection #custom_detection #사물인식 #image #video #processing #py #tensorflow #keras #Jupyter_notebook​안녕하세요. 진우네에서 진우를 맡고있는 진우입니다.​ 오늘은 Github의 소스를 참고하여 학습된 Parameter or 학습을 통해서 만든 Parameter set을 통해서 사진 & 영상에서 Object Detection 하는것을 소개하겠습니다.​ 순서를 말씀드리자면, Image dataset 모으기 -> Labeling -> 학습 -> 학습된 Parameter -> Test 순입니다.​ 먼저 AI학습을 통해서 영상이나 이미지로 부터 물체를 인식하려면 가장 먼저 인식하고자 하는 물체가 무엇인지에 대해서 알아야하고 컴퓨터에게 이 물체(객체)의 정보를 인식시켜 학습하려면 사진에서 해당 객체들만 모아서 수많은 Dataset을 만든 후 학습을 하여야합니다. 학습을 할 모델을 선택하여야 하는데 활용 용도에 따라서 Compute 속도가 빠른 YOLO, SSD 등의 모델을 사용할 수도 있고 보다 정확한 Detection을 위해 MRCNN 모델을 사용할 수도 있습니다. 저는 MRCNN(Mask RCNN)모델( resnet50) 깊이를 통해서 학습을 할 것이며, 직접 학습 모델을 만들어 보아도 되지만 일단은 MRCNN모델을 사용하기로 하였습니다. (이후 실시간 detection에서는 mobile_faster_rcnn를 활용하여보았습니다.) 모델에 Dataset을 넣고(MRCNN COCO모델의 경우 xml이 아닌 json형태의 Labeling & Class정보가 있는 파일로 학습을 함) 학습을 시작하면 모델을 통해 학습을 하고 이때 tensorflow-gpu 가 호출되어 CPU보다 병렬처리 성능에서 월등히 뛰어난 성능을 가진 GPU(그래픽카드)가 해당 계산작업(LOSS를 구하는 Back-Propagation)을 합니다. 학습이 끝나면 .h5확장자의 log파일(Parameter Weight)이 생성됩니다.*여기서 json 형태의 파일은 Labeling하여 나온 각 객체의 polygon형태 좌표값을 json형태로 저장해둔 파일입니다. 생성된 log파일을 통해서 Git에서 받은 MRCNN 코드 Sample을 변형하여 자신이 직접 Class를 분류한 다물체 인식을 수행할 수 있습니다. 저는 자동차와 관련된 자율주행이나 모바일 앱의 이미지 프로세싱 분야에 관심이 있어서 사람얼굴 인식과 자율주행 시 필요한 차선, 자동차 등을 인식하는 작업을 수행하여 보았습니다.​ 이제부터 처음부터 하나씩 순서대로 저의 작업 순서를 소개하도록 하겠습니다. 저의 작업 환경은 다음과 같습니다.OS : WindowsGPU : GTX960(4G)RAM : 16GPackage : Tensorflow-GPU 1.14.0 ver, keras 2.1.3 verPython : 3.6.8 verIDE : Pycharm Community ver, Annotation Tool 1 ver, 그림판(Eclipse & MongoDB, MariaDB를 활용하여 dataset 관리를 수월하게 할 수도 있습니다.)​ 먼저 MRCNN 모델을 활용하기 위해서는 Git을 통해서 모델을 받아주어야 합니다. Git - DownloadsDownloads Mac OS X Windows Linux/Unix Older releases are available and the Git source repository is on GitHub. Latest source Release 2.25.0 Release Notes (2020-01-13) Download 2.25.0 for Windows GUI Clients Git comes with built-in GUI tools ( git-gui , gitk ), but there are several third-party tools...git-scm.com  Git을 설치하고 아래 링크는 제가 참고하였던 Git 페이지 입니다.​ matterport/Mask_RCNNMask R-CNN for object detection and instance segmentation on Keras and TensorFlow - matterport/Mask_RCNNgithub.com 위는 이미지 Object Detection을 위한 것이고 아래는 opencv를 이용하여 영상에서의 detection 코드가  있습니다.​ 위 소스를 다운받아주신 후 jupyter notebook에서 /Mask_RCNN-master/samples/demo.ipynb 에서 Demo를 진행하여 보실 수 있습니다.*jupyter notebook은 anaconda3 설치 시 자동으로 설치되며, 아래와 같이 생긴 아이콘을 클릭하면, Explore창이 켜지면서 jupyter notebook이 실행됩니다.   ​demo.ipynb 수행 화면(저는 다른 이미지 사진을 사용했습니다.)  * 수행 결과 ​ 다음과 같이 사전에 학습된 파라미터 파일(mask_rcnn_coco.h5)를 통해서 80가지(class)의 객체에 대해서 다물체 인식이 가능합니다.​ Custom Object Detection에 앞서서 MRCNN에 대한 개념을 잠시 보고 넘어가겠습니다.  Mask_RCNN은 2014년 RCNN에서 점차 모델 개선을 통해 2017년 MRCNN으로 발전하였고 MRCNN의 전신인 Faster-RCNN을 확장하여 Instance Segmentation에 적용하고자 하는 모델로 MRCNN이 나왔습니다. Mask-RCNN은 Faster RCNN에서 각 픽셀이 객체인지 아닌지 판별하는 CNN층(Binary Mask)을 하나 더 두었고 MRCNN은 어떤 모델들 보다 우수한 성능을 보여주었습니다.​Faster RCNN에서 MRCNN이 되면서 바뀐점은 (1)'bbox 인식을 위한 브랜치'에 병렬로 '오브젝트 마스크 예측 브랜치'를 추가하고 (2)ROI pooling 대신 ROI Align을 사용합니다. (1) 기존의 Faster RCNN을 Object detection 역할을 하도록 하고 각각 RoI에 mask segmentation을 해주는 FCN(FNN)을 추가하였습니다. (2) Faster RCNN은 Object detection만 수행하였고 RoI Pooling에서 객체의 정확한 위치정보는 Pooling을 거치며 무시(왜곡)되었지만 RoI Align을 통해 정확한 위치정보를 담도록 하였습니다. RoI Align을 통해 위치정보 왜곡 방지 기법  * RoI Align 설명  * RoI Align 설명  쉽게 말해서 각 픽셀에서의 비중을 곱해줌으로써 정확한 위치 정보를 갖고 Pooing하게 된다.   -> Mask Detection에 상당한 성능향상을 보여준다.​MRCNN 은 resnet 네트워크를 사용하는데, 4번째 스테이지의 마지막 Conv Layer에서 Feature를 뽑아낸다.Resnet50, Resnet101 을 주로 사용한다.      Loss Function의 경우 Classification, bbox regression, binary Masking의 총 합을 전체 Loss로 구하는데,- classification : Softmax Cross Entropy- bbox : bbox regression- mask : Binary Cross Entropy​네트워크의 구조 비교(RCNN ~ MRCNN)   ​ 이제 자신만의 Custom dataset을 갖고 학습을 하기 위한 방법을 소개하겠습니다. Custom dataset을 이용하려면 먼저 Dataset을 모아야합니다. 저의 경우는 많은 Dataset을 모을 수 있는 여건이 안되기에 차선, 자동차에 대한 이미지 data를 모으기 위해서 웹을 이용하였습니다.    * 학습을 위해 모은 이미지  이미지를 모을 때 저장하는 위치는 /Mask_RCNN_master/samples/balloon 폴더 전체를 복사하여 'CUSTOM'과 같은 이름으로 변경 후 CUSTOM폴더 내부에 기존dataset폴더 삭제 후 dataset이라는 폴더를 생성 해주시고 dataset 폴더 내부에 train, val 두가지의 폴더를 저장한 뒤 각 각 폴더에 서로다른 이미지를 저장합니다.(train 폴더의 이미지 수가 훨씬 많아야 좋다. val은 validation set이기 때문)validation set을 ML에서 사용하는 이유는 아래 링크에서 친절하게 설명 해주셨습니다. Machine Learning에서 validation set을 사용하는 이유validation set은 machine learning 또는 통계에서 기본적인 개념 중 하나입니다. 하지만 실무를 할때 귀찮은 부분 중 하나이며 간과되기도 합니다. 그냥 training set으로 training을 하고 test만 하면 되지 왜..3months.tistory.com    위와 같은 사진들을 여러장 많이 모아주시고 컴퓨터는 눈이 없기 때문에 인식하여 학습할 객체에 대해서 학습 전 labeling(Annotation)을 해주어 각 객체의 위치정보를 담고있는 json 형태의 file을 추출해주어야 합니다. Labeling Tool은 아래 링크를 통해서 진행하시면 됩니다. VGG Image AnnotatorRegion Shape Loaded Images Region Attributes File Attributes Keyboard Shortcuts ◂ Load images to start annotation or, see Getting Started . www.robots.ox.ac.uk (*사실 위 Labeling Tool이 사용하기에 불편한 점이 많아서 Annotation Tool을 만들고 있는 중입니다.)​ Labeling Tool 사용법을 간단히 적어보겠습니다.  1. Load or Add Images를 클릭하여 이미지를 불러온다.  2. CoCo model에 맞게 polygon형태를 클릭하여 Labeling을 진행한다.  3. 마우스를 클릭하여 테두리를 따라 그린다.(사실 더 많이 그려야한다.)  4. Region Attributes를 클릭하여 name & class명을 적어준다. 반드시 Region Attributes여야 한다.(필자는 처음에 File Attributes로 했다가 다물체 인식에 실패했었다.)  5. Annotation -> Save as JSON으로 저장한다. 반드시 json형태로 한다. CSV 형태(, , , )로 하면 또 파싱해서 json형태로 어차피 고쳐주어야한다.  6. 저장완료  저장을 하면 해당 이미지들이 있는 폴더 내에 JSON 형태 파일이 생성된다.  *json file  Indenataion이 안맞긴 하지만 대충 위와 같이 저장된다.key : File명size : file sizeregions : Locationname : 형태(Type)region_attributes의 name : name : 저장한 class 이름all_points_x : polygon의 각 꼭지점 x좌표all_points_y : polygon의 각 꼭지점 y좌표0, 1, 2, ... : 1개의 image에서 그린 annotation 수(Labeling시 한 이미지가 빠지거나 name이 비거나 약간의 문법오류만 있어도 학습시 에러가 발생하기에 주의해야한다.)​ 다음으로 할 것은 json파일로 학습을 시킬 것입니다.앞서 복사 해둔 CUSTOM 폴더에서 'balloon'이라고 적힌 .py파일을 저는 헷갈리지 않게 road.py로 이름을 변경 후 활용할 것 입니다. road.py를 여시면 주석으로 설명이 적혀있습니다. (road.py 에 대한 설명과 수정 사항들 나열) 저는 pre_trained 된 coco weight에서 학습을 시킬 예정이므로 나중에 anaconda 환경에서 학습을 할 때 사용할 명령어를 미리 아래와 같이 고쳐둡니다.  * Training 시 CMD에 날릴 명령어 road.py에서 아래 코드는 Weight에 대한 경로 관련 코드입니다. 경로 관련 오류가 발생하면 아래 코드를 수정해보시면 됩니다.  * PATH 코드 아래와 같이 절대경로로 수정하여도 무관합니다.   ​다음은 configuartion 관련 코드입니다.mrcnn폴더의 config.py에 Training 설정에 관련된 많은 값들이 있지만 여기서는 주요 config를 설정할 수 있습니다. 저는 NAME = ""line""으로 하였고 사용중인 GPU는1개이므로 1, NUM_CLASSES는 background(기존이미지) + CLASS 갯수 만큼 들어가야 합니다. 저는 Car, Slane, Llane 3개의 class를 labeling시에 작성하였으므로 1+3이 됩니다. STEPS_PER_EPOCH 1Epoch(모든 training Data를 한번씩 이용한 것)당 Training Step수를 결정하고 DETECTION_MIN_CONFIDENCE는 Object Detection을 할 때의 해당 정확도이상에서만 Detection 한다는 것을 의미합니다.   좌 : 기존 코드  /  우 : 수정된 코드 다음은 앞서 작성한 json 파일로 부터 data를 Load 작업을 하는 코드입니다. add_class의 첫 인자는 NAME과 같고 2번째 인자는 class의 번호, 3번째 인자는 Labeling시 작성한 이름입니다.subset 위치에 'train'이나 'val'이 있는지 assert로 확인 후 dataset_dir에 dataset의 경로를 넣습니다.   *좌 : 기존코드  /  우: 수정된 코드  기존의 코드는 1개의 class에 대한 학습을 위한 코드이기 때문에 생성할 클래스 수에 맞게 name과 id(class번호)를 넣어주는 코드를 작성하였다.  *코드 수정 전  * 코드 수정 후  *mrcnn 의 utils.py  위 utils.py의 일부 코드를 보면 class_info에 대한 정보를 얻을 수 있으며 class_info는 dictionary 형태로 source, id, name이 저장된다.​기존 코드는 이미지 갯수만큼 for loop를 돌며 regions의 좌표정보를 polygons에 저장하고 이미지에 좌표 정보를 넣는다. 하지만 여기서는 다물체에 대한 class 분류는 들어가있지 않으므로 코드를 다음과 같이 수정했다.class_name을 가져와서 num_ids에 각 이미지마다 class_names 값을 담는다.   * 좌 : 기존코드  /  우 : 변경된 코드 ​   * num_ids를 인자에 담아서 self.add_image(utils.py)로 넘겨준다.  *utils.py의 add_image(image_id:파일이름, source:NAME, path:경로)  *image_info는 list형태로 정의되어 있습니다.(utils.py) **kwargs는 키워드된 가변 갯수의 인자들을 함수에 보낼 때 사용하며 num_ids를 받을 수 있게 됩니다.​ 기존 load_mask의 ""balloon""을 NAME=""line""이었으므로 ""line""으로 고쳐줍니다.   ​기존의 코드에서는 self.image_info[image_id](*image_id = 해당 이미지 1장)에 class에 대한 정보가 없으며 새로운 코드에서는 class 정보를 포함한 self.image_info를 info변수로 받아서 'num_ids'값을 받아 Numpy type으로 변환 후 np.ones([mask.shape[-1]], dtype=np.int32) 대신 num_ids를 return 합니다.  *기존 코드  * 수정된 코드  'balloon'을 'line'으로 수정    def train(model): 함수에서 train폴더와 val폴더의 Training Dataset Load(가져오기)   ​def train(model):함수에서 model.py의 train함수를 training 정보를 담아 호출합니다.<Epoch수 설정, config.py의 LEARNING_RATE 설정, layers 설정>*Learning rate : 학습을 통해 parameter update시 변화 값의 정도를 Control*Epoch 수 : 학습횟수(위에서 Epoch당 training step은 100으로 설정)*layers는 모델에서 'heads'는 COCO set에서 마지막 layer에 대해서만 parameter update를 진행  -> ex) 3+의 경우 3 Layer 이후의 Parameter에 대해서 update 진행   color_splash는 Detected Object에 대해서 color를 입히는 함수입니다.    Batch Size 설정 : Batch size = GPU_COUNT x IMAGES_PER_GPU 그리고 training config에 대한 정보를 display 시켜줍니다.  *batch size는 1로 설정되었다. ​ 나머지 코드들은 추후에 더 분석하고 이제 anaconda Prompt를 실행시켜서 tensorflow가 설치된 가상환경에 접속하여 training을 진행합니다. -> road.py의 directory로 cd명령어로 이동하여 앞에서 주석처리되어있던 road.py의 train 명령어를 복사해와서 실행시켜줍니다.  *training 명령어  *prompt training 실행화면 training을 진행하며 cmd창으로도 loss값을 볼 수 있지만 tensorboard를 통해서 GUI로 training 수행과정을 확인 할 수 있습니다. 방법!1. 현재 트레이닝하여 log파일(.h5)이 생성되는 logs 폴더 directory로 cd 명령어를 통해서 접근합니다.2. tensorboard --logdir=./log폴더이름/    명령어를 통해서 tensorboard를 실행합니다. 저의 경우 예시 : C:\Users\jinwo\Mask_RCNN-master\Mask_RCNN-master\logs>tensorboard --logdir=./circle20200128T1339/3. Chrome 주소창에 localhost:6006을 입력하면 아래와 같이 LOSS 그래프를 확인할 수 있습니다.  *tensorboard 실행 시 cmd 창  * tensorboard  이렇게 training이 끝나면 log data로 .h5 (Weight) 파일이 생성됩니다.    위 파일을 /Mask-RCNN-master 폴더로 복사합니다.​저의 .h5 파일 용량이 작은 이유는 저는 training 시 config.py에서 'resnet101'이 아닌 'resnet50'을 사용하였기때문입니다.(mrcnn의 config.py)    또한 Image에 대한 Resizning 여부를 설정할 수 있는데 기본이 ""square""이고 이미지에 대한 resizing max, min 값을 정해주게 됩니다.  ""none""으로 설정 시 resizing을 거치지 않고 training을 하며 이때 이미지의 픽셀이 가로, 세로 모두 2^6 으로 나누어 떨어지는 수여야 합니다.(Ex) 640 x 320, 960 x 512) 이미지 pixel size가 2^6으로 나누어 떨어지지 않는다면 ""square"", ""pad64""등을 사용하거나 dataset을 resizing하여 ""none""으로 사용할 수도 있습니다.(resizing을 거치지 않게 되면 Forward Compute시 Detection 속도가 빨라지는것을 확인 할 수 있었습니다.)   아래는 model.py의 일부 코드이며 ""none"" 선택시 config.py 만 수정하면 되도록 하기위해 약간의 수정을 하였습니다.  *model.py  이제 학습된 model의 parameter Weight를 이용하여 Object Detection을 할 것입니다.영상에서의 detection을 진행하기 전에 사진(이미지)에서의 detection을 먼저 수행하도록 하겠습니다.​ inspect_train_image.ipynb 파일을 jupyter notebook으로 연 뒤 소스를 Pycharm IDE로 복사해옵니다. 소스를 받아온 뒤 import balloon 대신 import road를 하여주고  Weight path와 Dataset path를 수정합니다.   mode는 test 모드로 'inference' 이며 gpu를 사용할 것이므로 '/gpu:0' 으로 작성합니다.   Weight를 불러오고 test할 image를 선택합니다. (random.choice, image_id = x, for image_id in range(x) 등 사용)  * 이미지에 대한 detection 까지 수행 아래 코드는 추가된 평가함수입니다. 10개의 random하게 image를 뽑아서 test후 정확도 값(평균)을 반환합니다.(np.mean = 평균)   ​samples 폴더의 shapes.py에서 다물체 객체 인식에 대한 힌트를 얻을 수 있습니다.  *shapes.py Custom Object Detection 수행 결과      Epoch를 10번밖에 돌리지 않아서 정확도가 높지는 않지만 더 많은 Dataset과 긴 Training epoch수를 가진다면 더욱 정확한 Detection이 가능할 것 입니다. 다음 포스팅에서는 영상에서의 Object detection을 소개하고 Mask_RCNN이 아닌 다른 모델을 사용한 CUSTOM Object Detecion도 소개하겠습니다. MRCNN의 model.py, config.py 코드 분석 소개는 다음에 하겠습니다.​refhttps://mylifemystudy.tistory.com/82 (MRCNN 정리)https://3months.tistory.com/118 (train set & validation set)https://chess72.tistory.com/132  (python 경로 관련 설명)https://github.com/jinu0124/mask_rcnn_master_custom (위 코드 참고본)​​ "
Object Detection vs Semantic Segmentation ,https://blog.naver.com/lcs5382/222190607174,20201230,Object Detection 이미지 내에서 Object에 해당하는 부분에 Box를 쳐주는 것​ Semantic Segmentation 모든 픽셀을 class별로 분류하는 것 왼쪽: Image Segmentation오른쪽: Semantic Segmentation​https://medium.com/hyunjulie/1%ED%8E%B8-semantic-segmentation-%EC%B2%AB%EA%B1%B8%EC%9D%8C-4180367ec9cb 1편: Semantic Segmentation 첫걸음!Semantic Segmentation이란? 기본적인 접근 방법은?medium.com ​ 
"[07.25] Dense Net, Object Detection (딥러닝) ",https://blog.naver.com/yl95yl/221595603806,20190725,"​​​* Dense Net​   ​이미지 분류에서 좋은 성능을 나타내는 네트워크입니다. 굉장히 네트워크가 슬림하기 때문에 트레이닝 시간도 짧고, 구현도 쉬워서 유명해진 네트워크입니다. 이런 데이어들의 피쳐 맵을 아웃풋으로 다음 레이어의 입력과 연결하는 방식이며, ResNet에서 사용되기도 하였습니다. 다만 ResNet은 피쳐 맵끼리 더하기를 해주는 방식이었다면, 덴스 넷은 피쳐 맵끼리 concatenation을 해주는 것이 가장 큰 차이입니다.​피쳐맵 1이 있고, 피쳐맵 2가 있다고 해보시면. 더한다는 뜻은, pixel-wise로 더해줍니다. 덧셈을 해서 다른 피쳐맵을 만드는 방식이었습니다. 즉 정보를 더해서 강화시키는 것이 ResNet이었습니다. => (48, 48, 1) + (48, 48, 1) => (48, 48, 1) 이 됩니다.그러나 Dense net은 concatenation을 합니다. 채널이 되어 피쳐맵1의 뒤에 피쳐맵2가 붙는 것입니다. (48, 48,1) + (48, 48, 1) => (48, 48, 2)가 됩니다. 이를 통해 특성을 조금 더 풍부하게 가져올 수 있습니다. 이로 인해​1. Vanishing Gradient가 개선되었습니다. ReLU를 쓰며 아무리 배니싱 그라디언트 문제가 해결되었더라도, 꼭 그라디언트가 사라지곤 했습니다. 그러나 Dense Net을 사용하며, 위의 그림을 보시면 점점 채널이 늘어나고 있는 걸 보실 수 있죠. 각 레이어를 노드로, 레이어끼리 이어져있는 것을 엣지로 보시면 BackPropagation 때 한 레이어씩 타고 뒤로 에러를 전달해주는게 아니라, 바로 맨 앞 레이어로 엣지를 타고 에러를 전달해줄 수 있습니다. 이런 의미에서 그라디언트 문제가 개선되고 있습니다.이를 거꾸로 생각해보면 feature propagation도 더 강화되었습니다. Feed Foward로 생각해보시면 그만큼 간단하고, 특성이 유지되는 채로 레이어가 전달되는 것이니까요.​2. Feature Reuse 강화가 되었습니다. Convolution 할 때, feature map끼리 쌓여있고 이것을 펼쳐서 합성곱을 진행하는 것이므로 기존에 만든 피쳐맵을 리유즈하게 됩니다. ​3. Parameter 수를 절약할 수 있습니다. ​다만 채널이 너무 많아질 수도 있는데, 한정적으로 채널을 붙여주기 위해서 growth rate(k) 라는 것을 사용합니다. 위의 그림을 다시 보시면, k=4라고 명시되어 있습니다. 즉 4의 배수로 늘어납니다. 최초 레이어가 6개의 채널이었으면, 6+4=10개의 채널이 되고, 그 다음은 6+4+4=14 개의 채널이 되고, 이런 식입니다.​   보시면 ResNet 경우에는 elemkemt-wise addition으로 매번 새로운 레이어를 사용하게 되고, DenseNet의 경우에는 기존의 feature map은 유지한 채 새롭게 만들어지는 feature map을 뒤에 추가해서 붙이는 식입니다. 위의 엣지도 눈여겨 볼 부분입니다. 모든 노드들이 다 이어져있죠.   x0이 기존의 레이어죠. h1이 convolution 과정입니다. x1이 x0을 컨볼루션 한 결과로서의 레이어인데, 이를 x0의 뒤에 붙입니다. 다시 이걸 합성곱합니다. 이런 식으로 계속 레이어를 쌓아올리는 것입니다. 아무래도 기존 특성은 유지하면서 새로운 특성을 붙이니까 특성이 보다 풍부하겠죠?​   ​​   ​​​DenseNet도 pre-trained model로 공개된 것이 많으니, 그냥 가져다 쓰셔도 됩니다.​​   https://www.kaggle.com/xhlulu/aptos-2019-densenet-keras-starter APTOS 2019: DenseNet Keras StarterUsing data from multiple data sourceswww.kaggle.com ​실행하고 싶은 분은 왼쪽 상단에 점 세개(?)를 누르시면 Copy ans edit Kernel이 있습니다. 이를 누르시면 실행이 가능한 노트북으로 넘어가게 됩니다.​   ​그냥 쥬피터노트북 사용하듯이 Shift + enter 누르시면 실행됩니다. 트레인도 가능하구요.​keras.callbacks은 특정 함수를 사용하기 위해 불러온 라이브러리입니다. 하나하나 같이 보겠습니다.​ import jsonimport mathimport osimport cv2from PIL import Imageimport numpy as npfrom keras import layersfrom keras.applications import DenseNet121from keras.callbacks import Callback, ModelCheckpointfrom keras.preprocessing.image import ImageDataGeneratorfrom keras.models import Sequentialfrom keras.optimizers import Adamimport matplotlib.pyplot as pltimport pandas as pdfrom sklearn.model_selection import train_test_splitfrom sklearn.metrics import cohen_kappa_score, accuracy_scoreimport scipyfrom tqdm import tqdm%matplotlib inline train_df = pd.read_csv('../input/aptos2019-blindness-detection/train.csv')test_df = pd.read_csv('../input/aptos2019-blindness-detection/test.csv')print(train_df.shape)print(test_df.shape)train_df.head() 안에 데이터가 얼마나 있나 확인하기 위한 코드입니다. pd를 csv를 불러오기 위해 사용한다고 하네요.컴페티션 내용 보시면 병이 한종류가 아니고요. 여러 종류입니다.​ train_df['diagnosis'].hist()train_df['diagnosis'].value_counts()   def display_samples(df, columns=4, rows=3):    fig=plt.figure(figsize=(5*columns, 4*rows))    for i in range(columns*rows):        image_path = df.loc[i,'id_code']        image_id = df.loc[i,'diagnosis']        img = cv2.imread(f'../input/aptos2019-blindness-detection/train_images/{image_path}.png')        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)                fig.add_subplot(rows, columns, i+1)        plt.title(image_id)        plt.imshow(img)        plt.tight_layout()display_samples(train_df)   보시면 혈관이 막혀있는 것들이 질병이 발견된 데이터인 듯 싶습니다. 자세한건 모르겠습니다.​생각보다 트레인 과정이 오래걸려 자세한 내용은 넘기겠습니다. 직접 링크에 가서 코드 한 번 구경해보세요.​​​​* Object Detection​- Ground TruthObject Detection의 정확도의 계산은 Sementation Area와 Ground Truth의 비교를 통해 Accuracy를 얻게 됩니다.​• Image Classification의 경우에는 GT가 이미지의 class인 반면, ObjectDetection은 이미지의 각 object의 해당하는 Bounding Box와 Box안의 class를 의미합니다.• 즉 정확도가 높다는 것은 모델이 GT와 유사한 Bounding Box를예측(Regression)하면서 동시에 Box 안의 object의 class를 잘예측(Classification)하는 것을 의미합니다. 즉 class도 정확하게예측하면서, 동시에 object의 영역까지 잘 예측을 해야 합니다.​오브젝트 디텍션의 경우에는 바운딩 박스가 뭔지 잡으면서 분류도 진행합니다. 즉 바운딩 박스를 예측하면서, 박스 안의 오브젝트 역시 분류해야 합니다.  다만 class 분류가 우선됩니다.  클래스 분류가 제대로 된 후 바운딩 박스의 정확도를 기준으로 정확도를 측정하게 됩ㄴ미다.​- Bounding Box 예측=> IoU 라는 지표를 통해 측정합니다. Intersection Over Union으로, 교집합/합집합 으로 이해하시면 됩니다.      ​- Precision양성인 것 중 진짜 양성인 것을 Precision이라 부릅니다.   검출 결과가 얼마나 정확한지를 나타내기 위해 Precision을 계산하게 됩니다.​- Recall데이터가 주어졌을 때 얼마나 잘 양성을 잘 검출했는지가 Recall입니다.​   얼마나 잘 검출했는지를 보여줍니다.​- AP​   precision과 recall은 반비례 관계를 갖게 됩니다. 저렇게 구한 것을 평균을 구한 게 mAP입니다. ​https://www.youtube.com/watch?v=OOT3UIXZztE https://www.youtube.com/watch?v=7p2XL8wApfo ​​캐글 챌린지 중 intel 로 검색하시면 나오는 코드를 한 번 보겠습니다.커널 들어가셔서 [BEG][TUT] 치시면 상단에 나오는 코드 보시면 됩니다.​https://www.kaggle.com/uzairrj/beg-tut-intel-image-classification-93-76-accur [BEG][TUT]Intel Image Classification[93.76% Accur]Using data from Intel Image Classificationwww.kaggle.com ​위 코드에서 모델 부분만 바꿔보는 실습을 해보시면 되겠습니다.​​ "
Feature Pyramid Networks for Object Detection ,https://blog.naver.com/tomatian/221865008339,20200321,"Feature Pyramid Networks for Object Detectionhttps://zpascal.net/cvpr2017/Lin_Feature_Pyramid_Networks_CVPR_2017_paper.pdf※Github - keras DeanDon/FPN-keras根据这个https://github.com/matterport/Mask_RCNN修改的FPN. Contribute to DeanDon/FPN-keras development by creating an account on GitHub.github.com FPN의 아키텍쳐만 궁금하신 분은 아래로 내려가 주세요  ​​​Abstraction​1. FPN 소개 및 요점​· feature pyramid network는 이전까지 계산량 메모리양을 많이 차지하여 피해왔던 feature pyramid를 계산량을 줄여서 사용한다.· lateral connection, top-down architecture를 적용한다.·  이 두 가지를 사용함으로 써 고수준의 semantic feature map을 얻을 수 있다. · FPN은 object detector(그냥 single model)가 아니라 feature detector다. · multi-scale pyramidal hierarchy 를 사용· semantic feature = class의 정보를 담고 있는 feature, localization feature = 위치 정보를 담고 있는 feature  【 scale은 이미지 사이즈를 의미하는 가?】아니다. 딥러닝 분야에서 scale이란 이미지를 보는 크기를 의미한다.​  ​왼쪽이 scale이 작은 것이다.전통적으로는 image 사이즈를 변경해가면서 scale을 변화시킨다.  결국 이런식으로 보는 box는 같게, 이미지 사이즈를 달리해서 본다는 것이다.  ​​​​​​​Introduction​1. Scale invariant​· scale invariant = 여러 scale에서 객체 인식이 가능하다는 것· model이 positions과 pyramid level 어디 stage, level에서든지 model을 scan 할 수 있다는 것  → large range of scale 인식 = scale invariant· computer vision에서 중요한 특성이다.·Image pyramid는  multi scale  feature 을 계산하기 위한 방법이다.   【 Image Pyramid : figure 2 (a)】  이런식으로 이미지 사이즈를 변경해가면서 이미지를 여러 scale에서 분석하는 기법이다.이 기법은 power, speed, memory 소모가 많고, end - to end learning(딥러닝)에 적합하지 않다.  ​​ figure 2 ​2. ConvNet : Figure2 (b)​· Scale invariant한 특성을 지니고 있는 것은 ConvNet = deep convolutional network(=CNN)이다.· ConvNet은  input을 하나만 넣는다.· input하나만 넣고 pyramidal shape을 통해 multi scale을 만든다.· image pyramid 대신 feature hierarchy 를 layer by layer 사용한다. → 각기 다른 spatial resolution(크기)의 feature map을 만든다.· 하지만 마지막에는 single scale features만​ 사용해서  different depth로 인한 large semantic gap이 발생한다.​​​​3. SSD : Figure3 (c)​· ConvNet의 ​feature hierarchy를 사용하기는 한다. (feature image pyramid = image pyramid 대신 사용) · 하지만 low-level feature을 피하기 위해 pyramid를 세우는 대신 이미 계산된 layer를 다시 사용한다. (=reuse pyramidal feature)· 그리고 계속 layer를 추가한다.· 하지만 이러면 high - resolution map (figure 2 (c)에서 맨 아래 큰 이미지에서 나오는 feature map으로 추정 & high level feature와 다른 의미다.) 을 사용할 수 없게된다. → small object detection 잘 못한다.​​​4. FPN과 비슷해 보이는 구조​  · top down 구조 + skip connection· 하지만 finest level ( high - level feature map )에서만 prediction이 나온다.​​​​​5. FPN은 해결책을 제시한다.​· FPN은 ConvNet에서의 feature hierarchy 특성을 유지하되, 모든 scale에 strong semantic을 유지한다.· 해결책 = low - resolution (sementically strong feature) + high - resolution (semantically strong feature) high resolution pyramid level이 subsample이 덜 돼서 더 정확한 위치 정보를 가지고 있긴 하나, semantic 정보는 안 좋음  · 이렇게 하면 결과는 모든 scale에 대해서 rich semantics를 도출할 수 있다.· 또한 그림에서도 보듯이 하나의 input 이미지로 빠르게 결과를 낼 수 있다.· in - network feature pyramid를 구성하는 것이다.· end - to - end learning이 가능하다. test/ train time consistency· 계산 시간 증가 없이 더 정확한 accuracy를 낼 수 있다.​내 생각에는 resolution은 spatial resolution (크기)를 의미하는 듯 하다. 그리고 high-level이라는 용어를 많이 쓰는데 나는 strong semantic과 같은 뜻으로 보았다. 그래서 각 글자의 위치 그 글을 의미하는 곳이라고 생각한다.​​​​​​​​Feature Pyramid Networks  FPN의 Goal = ConvNet의 pyramidal feature hierarchy + semantic gap X= low, high level 모두의 semantics를 가지고 있다 +  모든 level에 strong semantic를 가지고 다닌다​FPN의 구성 = bottom - up pathway + top-down pathway + lateral(측면) connection​  ​​​1. Bottom - up pathway​· 이 논문에서는 ResNet을 이용해서 결과를 도출한다.· ConvNet의 backbone 네트워크 Feed - forward computation = 다양한 scale의 feature hierarchy를 포함한다.· 그림에서는 bottom - up 단계에서 4개의 stage가 존재한다. (bottom - up 단계에서 4가지 사이즈의 이미지)· 각 stage의 마지막 layer의 output이 feature map이 reference가 된다. · 각 stage의 layer가 깊을수록(up부분) stronger feature을 가진다.· 밑의 그림에서 bottom - up 부분에 있는 하나 하나의 block이 residual block이다. ( ∵ resnet backbone)· stride는 각 stage에서 (4, 8, 16, 32) 이렇게 두배씩 증가하면서 진행한다.· 맨 아래 conv1은 pyramid에 포함시키지 않는다. ( ∵ memory footprint)​  ​​​2. Top - down pathway ​· strong semantic feature를 가지고 있는 layer를 2배 upsampling 한다. ( top 부분 = higher pyramid에서 나온 feature map = resolution은 low(coarse)인 pyramid level에서 나온 feature map) · M5를 만들기 위해서 1 X 1 convolution을 가장 coarse한 feature map 에 해준다. 그러면 256-d channel로 맞춰진다. 그리고 element wise하게 더해준다. · Nearest neighbor upsampling 기법을 사용한다. nearest neighbor upsampling ·  위의 그림에서 보면 prediction을 낼 때는 3 X 3 convolution을 사용하는데, upsampling 된 이미지를 merge할 때 올 수 있는 aliasing effect를 없애주는 역할이다.· 모든 단계의 prediction (feature map) 에서 같은 bbox regressor과 classifier를 사용한다.​​​​​2. Lateral connection (skip - connection)​·  top down 과정을 할 때 나온 feature들은 lateral connection으로 더 강력한 feature를 가지게 된다.· bottom up  각 level 중 같은 spatial resolution을 가진 feature map과 연결된다.​  ​​​​​​​Applications(RPN, Faster R-CNN, Fast R-CNN의 개념을 알고 있어야 이해됨)​1. FPN for RPN (region proposal network)  · single scale feature map을 FPN으로 대체한다. → 1개의 feature map 대신 4개의 feature map 생성· 기존의 RPN에서와 다르게 anchor를 한 level에 하나의 scale의 anchor만 부여한다. ( ∵head 부분이 densly하게 slide 해서 많은 anchor가 필요 없다. )· RPN에서는 3 X 3 convolutional layer를 이어서 두 개의 1X1 convolutional layer로 하나씩 각각 bbox regressor과 classifier가 만들어지는데, 이걸 head라고 하고, 이러한 head는 4 단계의 feature pyramid에 각각 모두 적용된다.· {32^2 , 64^2 , 128^2 , 256^2 , 512^2} 사이즈의 anchor를 각각 {P2, P3, P4, P5, P6}에 적용한다. (단, ratio sms 3가지 {1:2, 1:1, 2;1})· 작은 feature map에 큰 anchor를 사용하고, 큰 feature map에 작은 anchor를 사용하는 것이다.​· training label은 IoU로 정해진다.· IoU >= 0.7 → positive / IoU <= 0.3 → Negative· ground truth는 직접적으로 feature pyramid ( bottom up 부분 ) 에 배정되지 않고, anchor를 통해서 배정되는 것이다.· 위에서 말한 head 부분(bbox regressor, classifier 각각의 1X1 convolution)의 parameter는 모든 feature map에서 같은 것이 사용된다. (sharing parameter)· sharing parameter에서 좋은 성과를 냈다는 것은 feature pyramid의 각 level에서의 semantic 정도가 유사하다는 것이다. 이것은 ConvNet에서 따오고 싶어한 feature hierarchy pyramid를 성공적으로 적용했다는 것이다.​​​​2. FPN for Fast R-CNN  ​· RPN을 적용하기 위해서는 scale이 다른 feature pyramid에 각각 RoI를 배정해 줘야한다.  · k = feature map​ /  k0 = target level· k = 3이면 P3을 feature map으로 사용한다는 것· w X h = 224^2· 원래 Faster R-CNN에서 C4(bottom - up 라인에 존재)를 feature map으로 사용하기 때문에 k0 = 4로 둔다.· RoI scale이 작아질수록 (<224^2) low resolution level에 RoI를 배정해야 한다. ( k =4 대신 , k= 3에 배정하는 등)· 모든 level에서 나온 모든 RoI들을 head에 붙인다. (head = bbox regressor, classifier)· RoI pooling으로 7X7 features를 추출하게 하고, head 전에 2개의 1024-d fully connected layer를 붙인다. (이 fc layer가 ResNet에서 대신 사용되는 Conv5 head 보다 가볍다.)​​​​​​​​Experiments on Object Detection​ datasetCOCO minival setbackbone network dataset pre - trained on the ImageNet classification set fine-tuned on the ImageNetdetection setbacbone networkResNet 50 & ResNet-101 ​​1. Region Proposal with RPN​   AR : Average Recall​​※ Comparisons with baselines· 유의미한 정보를 담고 있는 높은 수준의 feature map이 있더라도 하나이면 결과가 좋지 않다. · →low(coarser) resolution과 stronger semantics간의 trade off가 생긴다.​​※ top - down enrichment의 중요성· top - down 구조가 없다는 것은 feature hierarchy pyramid에서 bottom - up 구조만 있다는 것이다.· 이러한 구조는 feature hierarchy pyramid에서 이전 level의 feature map의 결과를 다시 사용한 다는 것이다.· 하지만 이러게 되면 pyramid의 각기 다른 level간의 semantic gap이 너무 커져 좋은 accuracy가 나오지 못한다.​​※ lateral connection 의 중요성· top - down 구조에서 워낙 이미지가 upsample, downsample 많이 돼서 localization에 대한 정보가 부족하다.· bottom - up 구조의 결과를 가중함으로써 더 정확한 localization을 얻을 수 있다.​​※ pyramid 형태의 중요성· pyramid 구조를 사용하지 않고, p2에서 conv를 하여 결과를 내면, pyramid가 다양한 scale을 담고 있지 못하기 때문에 anchor수를 늘려야 한다. 하지만 이렇게 늘어난 수의 anchor는 accuracy를 높이는데 이바지하지 못한다.​​​​​2. Region Proposal with Fast / Faster R-CNN​  ​※ Apply to Fast R-CNN· RPN에서 나온 region proposal의 성능이 좋기 때문에 FPN에 이를 적용하였다. (small object에 성능이 좋음)· feature 들을 fasr R-CNN과 RPN에 나눠서 사용하지 않았다.· top - down 과 pyramid 구조가 없으면 성능이 안 좋았다.· 기존의 일반 fast R-CNN보다 좋은 성능을 보였다.​​※ Apply to Faster R-CNN· feature sharing을 하기 위해 faster R-CNN과 RPN, fast R-CNN이 동일한 backbone network를 사용하였다.· image를 600pixel 대신 800 pixel로 resize하였다.· 512 RoI 사용 / 5가지 scale의 anchor 사용 / epoch : 1000 · back bone : ResNet 50· RPN과 Faster R-CNN 간의 sharing features · Running time은 줄어들고 basline에 비해 성능이 좋아졌다.​​​​​그림 참고 ㅣ https://medium.com/@jonathan_hui/understanding-feature-pyramid-networks-for-object-detection-fpn-45b227b9106c​​​​  이상 동산이었습니다.열심히 공부하는 사람이 됩시다!그리고 제가 열심히 혼자 공부하고 해석하면서 작성한 글이라 무단배포 + 복붙하시면 안됩니다!출처를 밝혀주세요~​​​ "
Object detection(7) _One-stage 2D object detectors: SSD[ECCV'16] ,https://blog.naver.com/summa911/223003674965,20230203,"1) Anchor Box :YoLo에서 proposal한 box는 모호함.하지만 SSD모델에는 anchor box라는 개념이 있다.box자체에 class probability개념을 가지고 있어서 정확성이 높다.SSD는 anchor box기술을 이용해서 FC layer 없이 8732개의 박스를 빠르고 정확하게 예측할 수 있다. ​​2)SSD- Resolution YoLo는 7*7의 one size gird 였다면, SSD는 grid size가 다양하다.CNN을 통과하다 보면 feature 크기가 점진적으로 줄어드는데 YoLo는 feature크기가 다 줄어든 다음에, 7*7상태에서 box proposal을 한다.반면 SSD는 feature map의 크기가 줄어가고 있는 다양한 단계에서 다양한 크기의 box를 proposal을 한다. ​3) SSD - Architecture Yolo는 마지막에 fully connected를 이용해서 global하게 전체 이미지에 대한 box proposal을 살펴주어야 한다. 하지만, SSD는 CNN을 통과하면서 여러 단계에서 여러 사이즈의 box를 선택했기 때문에, 전체 이미지에 대한 box를 살펴보지 않아도 된다.  ​또, SSD는 early layer에서도 box를 proposal해서, fine detail을 높인다. early layer는 low 한 feature를 가지겠지만, 위치정보를 정확하게 가지고있다.( 픽셀이 모두 high resolution으로 남아있기 때문에)마지막 레이어에서 전체적인 이미지를 살핌과 동시에 초기 레이어에서 디테일을 살필 수 있다.그래서 YoLo 버전 1 보다 낮은 해상도를 가지는 이미지를 인풋으로 줘도, 더 높은 정확도를 보인다. 그리고  SSD는 YoLo보다 더 많은 box를 제안한다.​4)Loss function YoLo는 처음부터 BB를 제안하면서 에러가 너무 많아서 알파가 5정도 여야 하는데, SSD는 처음에 제안된 박스가 랜덤 박스가 아니라, 정제된 predetermined raito가 있기 때문에(anchor box개념을 썼기 때문에) 처음부터 loss가 크지 않고, 1 정도로 작아도 된다. ----???? ​[출처] https://youtu.be/MRPsDN7dRJY [출처]W. Liu et al., ""SSD: Single shot multibox detector""https://arxiv.org/abs/1512.02325 SSD: Single Shot MultiBox DetectorWe present a method for detecting objects in images using a single deep neural network. Our approach, named SSD, discretizes the output space of bounding boxes into a set of default boxes over different aspect ratios and scales per feature map location. At prediction time, the network generates scor...arxiv.org ​ "
캐글 Open Images 2019 - Object Detection ,https://blog.naver.com/kmh03214/221567732969,20190621,"https://www.kaggle.com/c/open-images-2019-object-detection/data Open Images 2019 - Object DetectionDetect objects in varied and complex imageswww.kaggle.com 현재 Google Research에서 진행중(~2019.09)인 Open Images 2019이다.과제는 객체탐지이며 객체탐지는 객체의 Bounding Box의 좌표를 찾는 문제이다.​따라서, 1. Bounding Box를 찾는 것2. Bounding Box내의 객체가 무엇인지?(개,고양이,사람...)에 대한 분류가 기본이며, 이를 수행하는 것이 올해의 과제이다.이해를 위해 다음을 보자   위를 보면 알 수 있듯, 1. 이미지 내의 객체에 대해 일단 Box들을 찾는다.(객체탐지-object detection)2. 그 후 객체가 뭔지 맞춘다. (객체인식-object recognization)두개는 같은의미로 많이 혼동하며 쓰지만, 사실은 각자가 작동되는 방식(모델)은 다소 다르기 때문에 혼동하지 않았으면 한다.​사실 실무에서는 데이터를 수집하는 것 부터가 업무의 시작이며 라벨링하는 것 또한 귀찮은 일 중에 하나일텐데...고맙게도 이미지와 labeling이 달린 엄청난 데이터(2TB였나?) 를 제공해준다.​그렇기 때문에, 걱정없이 모델만을 설계하거나, 기존에 있는 모델을 불러다 fine tuning등을 시키며 학습해 제출해도 될 것 같다. 이미지에 대한 500개의 Class가있고, 이에대한 annotation(label - 정답set)은 전문적인 사람들이 했으니 걱정말라는 얘기와... 단순한 이미지가 93%고  7%의 복잡한 이미지(많은객체가 포함된 이미지)가 포함 돼 있다고한다.​우리의 목표는 단순하다. 인풋으로 image가 모델에 들어가면 이에 따라 annotation을 출력하면 되는것. 그렇게 만드는 Model(함수)를 찾는 것이다.​근데 원래 문제가 단순하고 조건이 적을수록 어려운 문제...^^;;; 사실 상 해야할건 많을 것 같다.​​   ​사실 눈에 바로 들어오는건 25000$의 상금이다..! 2500만원!!!(그치만,, 1등이 7000, 2등이 6000,3등이 5000,4등이 4000,5등이3000,,,,)​뭐 상금이 중요한건 아니다. 이 대회에 참여하면서 정말 많은 것을 배울 수 있을 것(자기가 얼마나 하냐에 따라..?)이게 중요한거지...​사실상 처음으로 대회에 참여하는 Kaggle은 처음이라 나도 자세히는 모르겠다...​ 이 글을 보고 정보같은거 공유해주신다면 감사하겠습니다. ^^​다음시간에는 저 많은 데이터를 전부다 다운받을 수는 없고... 어떻게 처리를 해야 할 지에 대해서 알아보도록 합시다.​아, 참고로 위에 이미지에서1. Data에 들어가면 Data에 대한설명, 2. Kernels는 사람들이 나같은 초보자(..)를 위해 남겨놓은 고마운 정보들.. 3. Discussion에는 말그대로 토론 4. Leaderboard에는 현재 순위등이 나와있다 "
[ML] Recent Advances in Deep Learning for Object Detection ,https://blog.naver.com/horajjan/221623021832,20190821,"In this paper, we give a comprehensive survey of recent advances in visual object detection with deep learning. By reviewing a large body of recent related work in literature, we systematically analyze the existing object detection frameworks and organize the survey into three major parts: (i) detection components, (ii) learning strategies, and (iii) applications & benchmarks.출처 : https://arxiv.org/abs/1908.03673v1    Recent Advances in Deep Learning for Object DetectionObject detection is a fundamental visual recognition problem in computervision and has been widely studied in the past decades. Visual object detectionaims to find objects of certain target classes with precise localization in agiven image and assign each object instance a corresponding class lab...arxiv.org "
AI 학습을 통한 Object Detection(python)(2) ,https://blog.naver.com/jinwoo6612/221825311219,20200226,"#opencv #python #COCO #object_detection #custom_detection #사물인식 #image #video #processing #py #tensorflow #keras #Jupyter_notebook #video #detection​ 안녕하세요. 진우네에서 진우를 맡고있는 진우입니다.​ 오늘은 Github의 소스를 참고하여 학습을 통해서 만든 Parameter set을 통해 영상에서 Object Detection 하는것을 소개하겠습니다.​ 먼저 영상에서 Obejct detection을 하려면 이전 포스팅에서 진행하였던  Custom Labeled 학습 데이터를 가지고 있어야 하며 Detection 할 영상, 각 프레임마다 Detection을 진행하기 위해 각 영상 프레임 당 Detection을 하도록 알고리즘을 넣어주어야 합니다. Python으로 영상을 불러오기 위해서 opencv 패키지를 활용하였고 가장 활용도가 높아서 opencv를 사용하였습니다.(*이후 image Calibration하기도 추가)​가장 먼저 아래와 같이 학습된 파라미터를 가지고 있어야 합니다. (Obejct detection test & val 시 사용)   ​ 그 다음 Github의 https://github.com/markjay4k/Mask-RCNN-series 해당 링크를 통해서 process_video.py 와 visualize_cv2.py를 다운로드 합니다. 다운로드 후 이 또한 MRCNN을 이용하므로  이전 포스팅에서 받았던 Mask RCNN master폴더에 .py 소스를 복사합니다.  *Git Source 화면        복사 후 저의 경우 mrcnn.py의 model과 utils.py를 복사하여 상위 폴더내로 복사 해주었습니다. -> process_video.py, visualize_cv2.py, utils.py, model.py 묶음​ 이제 process_video.py와 visualize_cv2.py의 수정을 통해서 영상에서의 학습된 Custom Object Detection을 하겠습니다.​ 먼저 video_process.py는 opencv를 통해 영상을 받고 각 프레임을 돌때 마다 model로 이미지 프레임을 보내어 Compute Forward하여 Object Detection하여 Masking 된 결과를 받아서 출력하는 역할만 합니다. visualize_cv2.py는 학습된 Parameter파일(.h5)를 받고 기존 config.py에 있던 간단한 설정을 사전에 해줄 수 있고 model.py로 부터 모델을 Create하여서 Parameter를 적용시키고 class_name을 지정하여 Object Detection을 수행 한 뒤, Detection image들을 Masking 하여서 반환 받아 이미지를 보여주는 역할을 합니다.​ video_process.py부터 소개하겠습니다.- 기존 코드 import cv2import numpy as npfrom visualize_cv2 import model, display_instances, class_namescapture = cv2.VideoCapture('videofile.mp4')size = (    int(capture.get(cv2.CAP_PROP_FRAME_WIDTH)),    int(capture.get(cv2.CAP_PROP_FRAME_HEIGHT)))codec = cv2.VideoWriter_fourcc(*'DIVX')output = cv2.VideoWriter('videofile_masked.avi', codec, 60.0, size) opencv, numpy, visualize_cv2.py를 import 해줍니다.cv2.VideoCapture 함수를 통해서 영상을 capture 변수에 받아줍니다.인자에는 해당 .py directory내의 영상 이름.확장자명 을 사용해도 되고 타 directory의 영상을 사용해도 됩니다. 타 directory 영상을 사용시에는 C://Users/jinwoo/video/machine_learning.avi 와 같이 인자를 주면 됩니다. size에는 get(cv2.CAP_PROP_FRAME_*)함수를 통해서 영상의 (가로*세로)프레임 크기를 받아줍니다. 저의 경우 Detection 후 Detection 된 영상을 새로운 파일로 저장을 해주고 싶기 때문에 codec, output을 정의합니다. cv2.VideoWriter('저장 될 파일 명', codec, 저장될 파일 Frame Rate, Frame size) 입니다. while(capture.isOpened()):    ret, frame = capture.read()    if ret:        # add mask to frame        results = model.detect([frame], verbose=0)        r = results[0]        frame = display_instances(            frame, r['rois'], r['masks'], r['class_ids'], class_names, r['scores']        )        output.write(frame)        cv2.imshow('frame', frame)        if cv2.waitKey(1) & 0xFF == ord('q'):            break    else:        break while문의 조건으로는 capture에 cv2.VideoCapture를 통해서 받은 영상이 True값을 가질 때 동안(열려 있을 동안) 반복 됩니다.capture.read()는 영상의 반환 값 유무, frame(각 frame 이미지)를 받아옵니다.영상에 반환 값이 있다면 모델에 이미지를 보내고 Detection 결과를 results.로 반환 받습니다.* verbose: 0 은 stdout에 로그를 주지 않으며, 1 은 진행 바 형태(매 배치)의 로그, 2 는 epoch 당 1 줄의 로그를 줍니다. visualize_cv2.py의 display_instances를 호출하고 이미지, 반환받은 결과 값을 인자로 줍니다.* (visualize_cv2.display_instances에서 Detection이미지를 받아서 Masking 한 후 이미지를 반환해 줍니다.) 반환 받은 frame을 output.write을 통해서 영상을 저장합니다. cv2.imshow('상단 제목 표기', 이미지(frame))을 통해 프레임 별로 이미지를 띄워줍니다. 'q'버튼 이벤트가 발생 하거나 opencv에 받은 영상의 Compute Forward 과정이 끝이 나면 break하여 while문을 탈출합니다. capture.release()output.release()cv2.destroyAllWindows() capture, output을 갱신 한 후 cv2.destroyAllWindows()를 통해서 모든 창을 닫고 종료합니다.(*cv2.destroywindow('window_name'))도 가능- ----------------------------------------------------------------------------------------------------------------------------------process_video.py저는 아래와 같이 소스를 수정하였고 맥락은 같습니다. import cv2import numpy as npfrom visualize_cv2 import model, display_instances, class_namesimport timefrom datetime import datetimeimport mathimport oscapture = cv2.VideoCapture(""road_960_512.mp4"") # cv의 VideoCapture 클래스를 사용size = (    int(capture.get(cv2.CAP_PROP_FRAME_WIDTH)),    int(capture.get(cv2.CAP_PROP_FRAME_HEIGHT)))Video_w = (capture.get(cv2.CAP_PROP_FRAME_WIDTH))Video_h = (capture.get(cv2.CAP_PROP_FRAME_HEIGHT))frame = int(capture.get(cv2.CAP_PROP_FRAME_COUNT))fps = capture.get(cv2.CAP_PROP_FPS)print(""fps :"", fps)length = frame/fpsprint(""Width :"", Video_w, ""Height :"", Video_h)Video_w_20 = round(Video_w * 0.2)#반올림 함수 roundVideo_w_80 = round(Video_w - Video_w_20)Video_h_35 = round(Video_h * 0.35)print(Video_w_20, Video_w_80, Video_h_35)#Video_w_20 : 화면 상 좌 20% 지점 / Video_w_80 : 화면 상 우 80% 지점codec = cv2.VideoWriter_fourcc(*'DIVX')output = cv2.VideoWriter('videofile_masked_road_20%35%_2x_50_inc.avi', codec, 30.0, size)  저는 영상으로부터 받은 데이터를 활용하여 자율주행에 관심이 있어서 이를 위해 필요한 영상에 대한 정보를 조금 더 받아주고 필요한 설정들을 선언해 줍니다. 픽셀 프레임을 받아서 상하 좌우 값들을 받아준 이유는 실제 자율 주행 시 카메라에 비치는 상단(하늘)과 같은 부분은 detection을 할 필요가 없으며 또한 오탐지율을 줄이기 위해서 입니다. flag = 0masking = 0 # 인코딩(Boxing) 처리 성능 향상을 위해서 한프레임씩 건너서 Boxing(Object Detecting) -> 속도 향상print(""Start masking"")now = datetime.now()print(""Start at :"", now)start = round(time.time())  선언부 while(1):#capture.isOpened()    ret, frame = capture.read() # ret 받은 이미지가 있는지 여부 , 각 프레임 받기    if ret and masking == 0:        results = model.detect([frame], verbose=0)# 모델 사용 -> 모델에서 Forward Compute 해서 Detection 결과를 반환        r = results[0]        masking = masking + 1        frame = display_instances(            frame, r['rois'], r['masks'], r['class_ids'], class_names, r['scores'], Video_w, Video_w_20, Video_w_80, Video_h_35        )        # display_instances를 호출(수행)할 때 마다        output.write(frame)        cv2.imshow('frame', frame)#원본 영상에 Masking이 입혀진 영상 보여주기 함수        if cv2.waitKey(1) & 0xFF == ord('q'):            break    elif ret and masking > 0:        masking = masking + 1        if masking == 2: # 몇 프레임 당 Compute할것인지            masking = 0        if cv2.waitKey(1) & 0xFF == ord('q'): # waitkey & 0xFF = 1111 1111 == 'q'            break        output.write(frame) # Model forward Compute를 거치지 않고 바로 출력        cv2.imshow('Drive', frame)    else:        break  영상의 매 frame당 detection을 수행할 필요는 없으므로 if문을 사용했습니다. now = datetime.now()print(""End at :"", now)end = round(time.time())taken_time = end - startminute = math.floor(taken_time/60)sec = taken_time%60print(""taken_time :"", minute, "":"", sec)rate = length/taken_timeprint(""encoding rate :"", rate, "": 1"", ""1보다 커야 실시간O"")capture.release()output.release()cv2.destroyAllWindows()  실시간으로 detection이 가능한지에 대한 계산으로 소스가 마무리 됩니다.​ visualize_cv2.py를 import해서 사용하게 되는데요. 여기서 저는 수정한 부분만 설명하겠습니다. ROOT_DIR = os.getcwd()MODEL_DIR = os.path.join(ROOT_DIR, ""logs"")COCO_MODEL_PATH = os.path.join(ROOT_DIR, ""mask_rcnn_line_0010_multi.h5"")#트레이닝된 모델로 부터 Weight를 가져오기 기존 COCOif not os.path.exists(COCO_MODEL_PATH):    print(""Downloading COCO_MODEL_PATH"")    utils.download_trained_weights(COCO_MODEL_PATH)  학습된 모델을 받고(없다면 CoComModel 다운로드) class InferenceConfig(load.loadConfig):    GPU_COUNT = 1    IMAGES_PER_GPU = 1  Config 설정을 배치size 1로 오버라이딩하며 config = InferenceConfig()config.display()#Create model in inference MODEmodel = modellib.MaskRCNN(    mode=""inference"", model_dir=MODEL_DIR, config=config)  config를 display하고 모델을 만들어줍니다. model.load_weights(COCO_MODEL_PATH, by_name=True)class_names = ['BG', 'car', 'Slane', 'Llane']  Weight 파일을 load합니다.(가중치 모델) + .h5의 가중치 모델에는 class label 정보가 담겨있지 않기 때문에 class_names 라는 리스트로 class들을 정의해 줍니다. def apply_mask(image, mask, color, alpha=0.5):  #여기서 image는 kernel 색상 RGB    """"""apply mask to image""""""    for n, c in enumerate(color):        image[:, :, n] = np.where(            mask == 1,            image[:, :, n] * (1 - alpha) + alpha * c,            image[:, :, n]        )    return image  이미지에 마스크를 씌우는 함수입니다. def display_instances(image, boxes, masks, ids, names, scores, Video_w, Video_w_20, Video_w_80, Video_h_35):    """"""        take the image and results and apply the mask, box, and Label    """"""    #Video_h_15는 영상 화면 상 상단 35%는 안식하지 않을 것이기 때문에 설정하였음(좌상단 부터 좌표 0,0)    n_instances = boxes.shape[0] # 분류 인식할 classes 의 갯수 (COCO data set을 사용할때)    names_num = list() # names_num를 담기위한 list 선언    if not n_instances:        print('NO INSTANCES TO DISPLAY')    else:        assert boxes.shape[0] == masks.shape[-1] == ids.shape[0]    AR_inc = 0    AL_inc = 0    laneR_num = 0    laneL_num = 0    for i in range(n_instances): # 인식된 Boxes의 갯수 -> 동 Frame에서 Detecting 순서(i)는 정확도가 높은순으로 Numbering  : (정확도최대)1, 2, 3, 4, ....        y1, x1, y2, x2 = boxes[i]        x = (x1 + x2) / 2  # 생성된 박스(Detection)의 좌우Center 좌표        if not np.any(boxes[i]) or Video_w_80 < x or x < Video_w_20 or y2 < Video_h_35:            #continue # Python의 continue : 아래 코드를실행하지않고 넘김(Pass)            return image        # box가 없을 시, continue를 통해서 받은 영상 이미지 그대로 반환        # boxes[i] 는 각 박스의 좌표 상좌하우 순으로 리스트형태로 들어있음        # 아래 if문은 영상에 mask를 씌우는 역할        if scores[i] >= 0.87:  #and names[ids[i]] == 'mouse'      <- Coco Model에서 특정 객체만 Detection boxing할때 조건            #boxes[i] 는 i번째 Detection 객체의 BOX 상좌하우 좌표값을 가짐            a = "" L""            b = "" R""            #names_num.append(names[ids[i]] + a) #동시성 객체에 번호를 붙여서 구분            # BOX들 중에서 정확도가 높은 순으로 네이밍 번호가 들어감 1, 2, 3, ... 순            #print(""image Location :"", names_num[i], ""y1(상) :"",y1, ""y2(하) :"",y2, ""x1(좌) :"",x1, ""x2(우) :"", x2)            #좌상단이 좌표 0, 0 입니다.            if (x1+x2)/2 < (Video_w/2): # and video_h_15 < y1 : <- Index 에러 발생 가능(label = names_num[i]에서 네이밍이 안들어 갈 수 있음 )                names_num.append(names[ids[i]] + a)                L_inc = -((y2-y1)/(x2-x1)) # Left lane 박스의 기울기                L_inc = (math.atan(L_inc) * (180/math.pi))                #L_loc = (x2+x1)/2 # 박스의 좌표값에 따라서 Lane 유지하기 추가하기                if L_inc < -22:                    AL_inc = L_inc + AL_inc                    laneL_num = laneL_num + 1            else:                names_num.append(names[ids[i]] + b)     #박스 R, L 에 따라서 이름 붙이기 -> 추후 활용                R_inc = ((y2-y1)/(x2-x1))                R_inc = (math.atan(R_inc) * (180 / math.pi))                if R_inc > 22:                    AR_inc = R_inc + AR_inc                    laneR_num = laneR_num + 1 # 차선 Detection 박스에서 박스의 대각으로의 기울기를 구해서 양쪽 차선 각도에 따라서 L: - 값  , R: + 값            #label = names[ids[i]]            label = names_num[i]#Box에 네이밍할 네임 -> names[ids[i]]에서 names_num[i]로 바꿔서 박스별로 넘버를 매기고 표시            color = class_dict[names[ids[i]]] # 원래 class_dict는 [label]이 인자였지만, label을 바꿨으므로 names[ids[i]]로 변경시킴            #color 에는 위에서 정의한 class_names [ , , ]의 라벨링 명과 같아야하기 때문            score = scores[i]            caption = '{} {:.3f}'.format(label, score) if score else label # 소수점 3자리까지 박스 정확도 표시            mask = masks[:, :, i]            image = apply_mask(image, mask, color)# Mask 씌우기            image = cv2.rectangle(image, (x1, y1), (x2, y2), color, 2)      #cv2함수 : 박스 만들기            image = cv2.putText(                image, caption, (x1, y1), cv2.FONT_HERSHEY_COMPLEX, 0.7, color, 2            )    if AL_inc != 0 and AR_inc != 0: # 양쪽 차선 둘다 인식되었을 때  -> 추후 한쪽 Detection이 없을 때도 Steering 구현하기###        AL_inc = AL_inc/laneL_num        AR_inc = AR_inc/laneR_num        inc = AL_inc + AR_inc        if inc > 0:            turn = ""R""        else:            turn = ""L""        image = cv2.putText(image, '{}{:.3f}'.format(turn, inc), (Video_w_20*2, Video_h_35), cv2.FONT_HERSHEY_COMPLEX, 0.6, color, 2) # 영상에 text 입히기    return image 영상의 프레임 이미지와 영상 이미지에 대한 필요 정보, process_video.py의 model.detect()를 통해서 detection한 정보들을 받아서 영상을 처리하여 내보내기 전 필요 부분에서만 detection 하여 마스크를 씌운 뒤 필요 정보만 return 해줍니다.​https://github.com/jinu0124/object-detetion-video jinu0124/object-detetion-videoContribute to jinu0124/object-detetion-video development by creating an account on GitHub.github.com ​수행결과 : 영상에서 line과 car를 detection하며 이에 따른 조향각을 표시해보는 것을 하였습니다.        다음과 같이 차선과 차를 잘 구분하여 인식합니다. 하지만 실시간 detection을 활용하기에는 MRCNN이 Compute 과정이 무겁기에 다른 모델을 통하여 보다 실시간처리에 유용한 버전을 만든것을 소개하도록 하겠습니다. "
Rich feature hierarchies for accurate object detection and semantic segmentation(R-CNN) ,https://blog.naver.com/yerim1656/222471570399,20210815,"https://arxiv.org/pdf/1311.2524.pdf​0. Abstractsimple and scalable detection algorithm that improves mean average precision(mAP) ​two key insights:(1) apply high-capacity convolutional neural networks to bottom-up region proposals in order to localize and segment objects(2) when labeled training data is scarce, supervised pre-training for an auxilary task, followed by domain-specific fine-tuning, yeilds a significant performance boost.​region proposals + CNN => R-CNN: Regions with CNN features.  1. Introductionvisual recognition tasks는 SIFT와 HOG에 상당 부분 의존. canonical visual recognition task(PASCAL VOC object detection) 발전 느림.​SIFT와 HOG는 blockwise orientation histogram. 그러나 recognition occurs several stages downstream -> hierarchical, multi-stage processes for computing features가 더 유용 for visual recognition.​Fukushima의 ""neocognitron""이 초기 시도. biologically-inspired hierarchical and shift-invariant model for pattern recognition그러나 supervised training algorithm이 부족했음.​-> LeCun et al에서 stochastic gradient descent via back-propagation(역전파)가 CNN(a class of models that extend the neocognitron)을 학습시키는 데 효과적임을 보여줌.​CNN은 1990년대 많이 사용되었으나 support vector machines(SVM)의 부상으로 잘 사용되지 않다가 2012년 Krizhevsky et al.이 CNN의 인기를 다시 불붙임. CNN은 ImageNet Large Scale Visual Recognition Challenge(ILSVRC)에서 상당히 높은 이미지 분류 정확도를 보임. * 1.2 million labeled images로 CNN 학습 + LeCun's CNN 수정=> 의문이 생김ImageNet에서의 CNN 분류 결과가 PASCAL VOC Challenge에서의 객체 탐지에서 어느 정도로 일반화가 가능할까?"" ""To what extent do the CNN classification results on ImageNet generalize to object detection results on the PASCAL VOC Challenge?"" ​=> 이 질문에 답하기 위해 image classification과 object detection의 차이를 연결함. 이 논문에서 최초로 CNN이 PASCAL VOC에서 object dection에서 엄청나게 높은 객체 탐지 성능을 보였음을 알림. (HOG-like features 기반 시스템들보다)​이를 보여주기 위해 두 문제에 집중:(1) localizing objects with a deep network 깊은 네트워크로 물체들을 localize하기(2) training a high-capacity model with only a small quantity of annotated detection data 적은 양의 정답이 있는 detection 데이터로 high-capacity 모델 훈련시키기​(1) 이미지 분류와 달리, detection은 localizing objects within an image가 필요. - localization을 regression problem으로 봄. -> 그러나, 현실에서 잘 안 맞음​- build a sliding-window detector. -> 이 방법으로 CNN이 사용되어 왔음. 특히, 제한된 대상 카테고리에서. 높은 공간 해상도를 유지하기 위해, 이 CNN은 오직 두 convolutional layer와 pooling layer를 가졌음.그리고 여기서 sliding-window approach를 적용하려고 했으나, input image의 매우 큰 receptive fields와 strides로 인해 precise localization within the sliding-window paradigm을 할 수 없었음. 대신 ""recognition using regions"" paradigm 사용.* object detection이랑 sementic segmentation에서 괜찮은 성능을 보이는 paradigm.test time에서, 이 방법은 input image에서 약 2000개의 category-independent region proposal을 생성해내고, CNN을 이용해서 각 proposal에서 fixed-length feature vector를 추출해내고, 이후 category-specific linear SVMS로 각 region을 분류해낸다.-> CNN에 region proposal을 합쳤으니까 R-CNN이라 부를 것. (Regions with CNN features)​1. input image로부터 독립적인 region proposal 생성 추출2. warp를 통해 resize -> 이미지 변형3. CNN을 통해 고정된 길이의 feature vector 추출4. 각 region마다 linear SVM을 통해 classification​(2) labeled data가 부족.-> supervised fine-tuning을 거친, unsupervised pre-training 사용​이 시스템은 efficient함.class-specific 계산은 상당히 작은 matrix-vector product와 greedy non-maximum suppression뿐. ​또한, simple bounding-box regression 방법이 mislocalization을 상당히 줄여준다는 것을 알아냈음.* mislocalization이 지배적인 error mode.​R-CNN은 region에서 작동하기 때문에 semantic segmentation까지 확장시킬 수 있음.  2. Object detection with R-CNN세개의 모듈로 이뤄져있음1. category-independent regision proposals 생산* proposals는 set of candidate detections available to our detector2. 각 region에서 일정한 길이의 feature vector를 추출해내는 large convolutional neural network3. a set of class-specific linear SVMs.​2.1. Module designRegion proposals. 최근 많은 논문들이 category-independet region proposals를 생산해내는 방법들을 제공함. R-CNN은 selective search 사용할 것. to enable a controlled comparison with prior detection work.​Feature extractionCaffe를 사용하여 각 region proposal에서 4096 차원의 feature vector를 추출.Feature들은 5개의 convolutional layer와 2개의 FC를 거쳐나온 mean-substracted 227 x 227 RGB 이미지를 순전파하여 구해진 것. ​region proposal을 위해 feature를 계산하기 위해, 먼저 그 region의 이미지 데이터를 CNN에서 사용할 수 있는 형태로 바꾸어줘야 함. (227 x 227 pixel size)candidated region의 size나 aspect ratio와 상관없이 모든 pixel들이 요구된 사이즈에 맞춰 bounding box에 들어갈 수 있도록 해야 함.  -> 왜곡된 training regions의 random sampling임​2.2. Test-time detectionselective search를 사용하여 약 2000개의 region proposals를 추출해냄. 그리고 각 proposal을 박스 크기에 맞춰 왜곡시키고, feature를 계산하기 위해 CNN으로 순전파시킴. -> 그 클래스를 학습한 SVM을 활용하여 추출된 feature vector들의 점수를 계산.  이미지에서 점수매겨진 region에 대하여 greedy non-maximum suppression을 적용. (학습된 임계치보다 큰 점수를 얻은 intersection-over-union 부분 제거)​Run-time analysis.detection을 효율적으로 만드는 두가지 properties(1) all CNN parameters are shared across all categories.-> region proposals와 features를 계산하는 시간을 단축시킴.(2) feature vectors computed by the CNN are low-dimensional compared to other common approaches.​2.3. TrainingSupervised pre-trainingCNN을 large auxiliary dataset (ILSVRC2012 classification)으로 pre-trained.image-level annotations only. (bounding-box labels x)​Domain-specific fine-tuningstochastic gradient descent(SGD) training of CNN parameters using only warped region proposals.​CNN의 ImageNet-specific 1000-way classification layer를 randomly initialized (N+1)-way classification layer로 바꾼 것 외에 CNN 구조는 변함 없음.​Object category classifierspartially overlap된 부분은 어떻게 label해야 할까?feature가 추출되고 training label이 적용되면, 하나의 linear SVM을 optimize.​2.4. Results on PASCAL VOC 2010-12​2.5. Results on ILSVRC2013 detection  3. Visualization, ablation, and modes of error3.1. Visualizing learned features첫번째 층 필터는 직관적이고 이해하기 쉽다.  oriented edge나 opponent color를 포착.다음 층을 이해하는 것이 어렵.-> 간단한 non-parametric method 제안.: single out a particular unit (feature) in the network and use it as if it were an object detector in its own right. (네트워크의 특정 단위를 object detector로 사용)즉, region proposals에 대해 점수를 매기고, 차례로 정렬한 뒤 non-maximum suppression 수행. 그리고나서 가장 높은 영역을 display.* 다른 visual mode를 보기 위해 그리고 그 단위로 계산된 분산에 대한 인사이트를 얻기 위해 averaging X. ​5번째 층이자 마지막 convolutional layer의 maxpooled output인 layer pool_5을 살펴보면, 이 feature map은 6 x 6 x 256 = 9216 차원임.boundary effect를 무시하며, 각 단위는 원본 227 x 227 픽셀 입력에서 195 x 195 receptive filed를 가진다. 중앙 pool_5 단위는 가장자리에 가까운 것은 더 작고, clipped 되어있지만 거의 global view를 보인다.​ figure 4의 각 행은 pooling 단계에서 나온 가장 높은 16개의 activation을 보여준다.이 단위들은 네트워크가 무엇을 학습했는지에 대한 대표적인 샘플을 보여준다. ​3.2. Ablation studiesPerformance layer-by-layer, without fine-tuningdetection에서 어떤 층이 중요한 역할을 하는지 알아내기 위해 VOC 2007 데이터셋으로 학습된 각 cnn의 마지막 세 층을 분석함. 3-1에서 마지막 pooling 층은 봤음.​층 fc_6은 pool_5와 fully connected되어 있음.feature를 계산하기 위해, pool_5 feature map에 4096 x 9216 가중치 벡터를 곱하고, 편향 벡터를 더함.이 중간 벡터는 component-wise half-wave rectified됨. (x <- max(0,x))​층 fc_7은 네트워크의 마지막 층.fc_6에서 계산된 feature에 4096 x 4096 가중치 행렬을 곱하고, 편향 벡터를 더한 뒤 half-wave rectification을 적용하여 값을 변환.​ 1-3행) PASCAL에 fine-tuning 없는 CNN 결과.(모든 CNN 파라미터들이 오직 ILSVRC 2012로만 pre-trained됨)​fc_7 feature 없이 fc_6 feature를 사용하는 것이 좋으며, 둘 다 없애는 것이 꽤 좋은 결과를 보임.pool_5 feature가 오직 CNN parameter의 6%만 사용함에도 불구하고.-> CNN의 representational power가 densely connected layer보다 convolutional layer로부터 나옴을 의미.-> dense feature map 계산의 잠재적 utility를 보여줌. ​Performance layer-by-layer, with fine-tuning4-6행) fine-tuning한 모델굉장한 성능 향상.fc_6, fc_7 층보다 pool_5 층에서 훨씬 큰 효과.-> ImageNet으로 학습된 pool_5 feature가 보편적이며, 대부분의 향상은 domain-specific non-linear classifiers를 학습하는 과정에서 얻어졌음을 의미. ​Comparison to recent feature learning methods8-10행) All R-CNN variants strongly outperform the three DPM baselines.​3.3. Network architectures구조를 선택하는 것이 R-CNN detection performance에 큰 영향을 줌. 16 층의 deep network를 이용한 VOC 2007 test 결과.이 네트워크는 ILSVRC 2014 classification challenge에서 최고의 성능을 보인 네트워크 중 하나. -> 이 논문에서 O-Net이라 부름. (VGG16임) T-Net을 baseline이라 부름. (AlexNet임)​R-CNN에서 O-Net을 사용하기 위해, VGG_ILSVRC_16_layers 모델의 pre-trained된 네트워크 가중치를 사용.그리고나서 T-Net과 같은 방식으로 네트워크를 fine-tuning함.-> 차이: GPU 메모리에 맞추어 더 작은 mini batch를 사용. (24 examples)​O-Net을 사용한 R-CNN이 T-Net을 사용한 R-CNN보다 훨씬 좋은 성과. 그러나 계산 시간이 7배 더 걸린다는 단점.​3.4. Detection error analysiserror mode를 찾아내고, fine-tuning이 어떻게 error modes를 바꾸는지, 그리고 DPM과 error type을 비교하기 위해 Hoiem et al.에서 사용한 detection 분석 tool 사용.  각 그래프는 false positive(FP)의 분포가 어떻게 변화하는지를 보여줌. 각 FP는 4개의 종류가 있음.Loc - poor localization (잘못된 localization)Sim - confusion with a similar category (비슷한 카테고리와의 혼동)Oth - confusion with a dissimilar object category (비슷하지않은 카테고리와의 혼동)BG - a FP that fired on background (배경에서 발생한 에러)​DPM과 비교했을 때, 배경이나 다른 클래스와의 혼동보다 잘못된 localization으로 인한 에러가 훨씬 많았음.CNN feature가 HOG보다 훨씬 잘 분류됨을 의미.​그러나 Loose-localization은 bottom-up region proposals의 사용과 전체 이미지 분류를 위한 CNN을 pre-training할 때 학습되는 positional invariance로부터 발생한 것 같음.​3번째 열은 간단한 bounding-box 회귀 방법이 많은 localization error를 어떻게 변화시키는지를 보여줌.​ 각 그래프는 6개의 object characteristics 중 가장 높은, 그리고 가장 낮은 performing subsets의 mean normalized AP를 보여줌.* object characteristics: occlusion, truncation, bounding-box area, aspect ratio, viewpoint, part visibility)​fine-tuning은 민감도를 감소시키지 않으며, 거의 모든 characteristics의 가장 높은 성능과 가장 낮은 성능을 향상시킴. 대신 모든 characterisitic에서 robustness를 높임.​3.5. Bounding-box regression에러 분석을 바탕으로, localization error를 줄일 간단한 방법을 구상해냄.DPM에서 사용된 bounding-box 회귀에서 영감을 얻어 selective search region proposal을 위한 pool_5 feature를 바탕으로 새로운 detection window를 예측하는 선형 회귀 모델을 학습시킴.​3.6. Qualitative results ​ -> ILSVRC2013의 정성적 detection 결과these are not curated and give a realistic impression of the detectors in actino. -> these are curated. ​all detections at precision greater than 0.5 are shown.  4. The ILSVRC2013 detection datasetILSVRC2013 detection dataset의 결과를 보여줌.이 dataset은 PASCAL VOC보다 덜 homogeneous함.​4.1. Dataset overviewILSVRC2013은 3개로 나눠져있음.train (395,918)val (20,121)test (40,152)​val, test set은 같은 이미지 분포로부터 뽑힘.이 이미지들은 scene-like하고 PASCAL VOC 이미지와 complexity(복잡도)가 비슷함.​val, test set은 모든 이미지가 bounding-box와 함께 200개의 클래스가 표시되어 있음.그러나 train 이미지는 그렇지 않음. (되어있을 수도 안 되어 있을 수도)게다가 각 클래스에는 negative 이미지 세트가 추가적으로 있음. 그들과 관련된 클래스에 포함되지 않도록 확인함. 이 실험에서는 negative image set은 사용되지 않음.​이러한 데이터셋 분리는 R-CNN을 학습시키는 데 많은 선택을 하게 됨.train 이미지들이 hard negative mining에 사용되지 않음. 왜냐하면 annotation이 소모적이기 때문(exhaustive)​=> Where should negative examples come from?train 이미지들은 val과 test set과 다른 통계량을 가지고 있어야 함.​=> Should the train images be used at all, and if so, to what extent?​general strategy: to rely heavily on the val set and use some of the train images as an auxiliary source of positive examples. val set을 주로 이용하되 train set에서는 positive 이미지만 가져와 보조 데이터로 사용val set을 training과 validation에 둘 다 사용하기 위해 val set을 반으로 나눔. * class-balanced되게 *​4.2. Region proposalsPASCAL detection에서 사용된 region proposal 방법 사용.-> selective search. ""fast mode""로selective serach가 scale invariant하지 않고, 따라서 생산되는 region이 이미지 해상도에 많은 영향을 받기 때문에 ILSVRC 이미지를 500 픽셀의 고정된 넓이로 resize함.​val set에서 이미지당 평균 2403개의 region proposals. PASCAL보다 낮은 recall -> significant room for improvement in the region proposal stage.​4.3. Training data이미지와 selective search로부터 나온 box와 ground-truth box의 집합.​R-CNN에서 training data는 3가지 절차가 필요:(1) CNN fine-tuning50k SGD 반복​(2) detector SVM training​(3) bounding-box regressor training​4.4. Validation and evaluationAll system hyperparameters were fixed at the same values used for PASCAL.​val_2를 통해 validation을 했을 때, extensive한 데이터셋 tuning없이 R-CNN이 좋은 성능을 보임.​4.5. Ablation study​ 다른 양의 training data와 fine-tuning, bounding-box regression의 효과에 대한 ablation study를 보여줌.​val_2에서의 mAP가 test에서의 mAP와 매우 일치. -> val_2에서의 mAP가 test set 성능을 보여주는 좋은 지표로 사용될 수 있음을 의미.​4.6. Relationship to OverFeatR-CNN과 OverFeat와의 흥미로운 관계:OverFeat는 R-CNN의 특별한 케이스라고 볼 수 있음.selective search region proposals -> multi-scale pyramid of regular square regions per-class bounding-box regressors -> single bounding-box regressor로 하면매우 비슷해짐.​OverFeat가 R-CNN보다 엄청난 속도 이점이 있음. (9배 정도 빠름)OverFeat의 sliding window가 이미지를 변형하지 않으므로 겹치는 window에 대해 계산을 공유할 수 있어서.  5. Semantic segmentationRegion classification은 semantic segmentation의 표준 기법.-> R-CNN을 PASCAL VOC segmentation challenge에 쉽게 적용할 수 있도록 함.​CNN features for segmentation.CPMC region의 feature를 계산하는 3가지 방법 평가.* 직사각형의 window를 227 x 227 정사각형으로 변형시키는(1) full: 영역의 모양을 무시하고 왜곡된 window에서 CNN feature를 직접 비교.영역의 직사각형이 아닌 모양을 무시. 매우 작게 겹침에도 불구하고 매우 비슷한 바운딩 박스를 가질지도 모름. ->(2) fg: 영역의 foreground mask에서만 CNN feature를 계산. 배경은 input의 평균값으로 대체. -> 0으로 됨. ->(3) full + fg: full과 fg feature를 합침.​Results on VOC 2011 full+fg R-CNN은 O2P보다 더 좋은 성능을 보임. single core에서도 더 빠른 속도.​ 가장 좋은 성능을 보였던 R-CNN(full + fg)과 다른 basline과의 비교. (R&P, O2P)21개의 카테고리 중 11개에서 가장 높은 segmentation 정확도.​여전히 fine-tuning을 거치면 더 좋은 성능이 보일 것 같긴 함.  6. Conclusion더 성능 좋고, 간단하고 scalable 객체 탐지 알고리즘을 제시함.-> 두 가지 인사이트로 달성해냄(1) apply high-capacity convolutional neural networks to bottom-up region proposals in order to localize and segment objects.(2) pre-train the network (with supervision) and then fine-tune the network for the target task where data is scarce.* train large CNNs when labeled training data is scarce.​classification tools from computer vision + deep learning (bottom-up region proposals ans CNN)으로 이루어낸 결과.​​나중에[이론]Region of Convolution.. : 네이버블로그 (naver.com)[논문리뷰] R-CNN (Regions wit.. : 네이버블로그 (naver.com)참고해서 더 수정하자!​​​​​​​ "
"Object Detection을 위한 Image Annotation, A to Z (1) ",https://blog.naver.com/panzer05/222258290626,20210227,"Custom Data를 이용해서, 자신이 만든 Neural Network을 구성해서 해 보려고 하니까, 여러가지 제약이 많이 있었습니다. 그래서 다른 사람이 한 것을 참조 하려고, 검색을 해 보았지만, 제 입맛이 맞는 것은 없었고, 대부분이 기존에 만들어진 유명한 Network을 이용하는 것이 많이 있더군요.​이렇게 하루를 소비하고, 스스로 해 보려고 했으나, 역시 자료가 많지 않았습니다. 제가 못 찾았을 수도 있지만, 누구라도 찾기에는 쉽지 않았을 것으로 생각 됩니다. ​뭐, 그래서 직접 한번 해 보았고, 혹시나 필요한 사람이 있을까 싶어 제가 했던 과정과 참조한 자료를 소개 해 볼까 합니다. ​순서1. 이미지 수집2. 이미지 Annotation - Bounding Box 작업3. Neural Network 이 알아 먹을 수 있도록 Data 변환     - Annotation한 xml 파일을 List형태로 변환 하는 것4. Annotation Data 확인​참조 자료1. 점프 투 파이선 - ebook2. https://keras.io/guides/ - 한글 사이트에는 Object Detection에 대한 내용이 없어요 ㅜㅜ Keras documentation: Developer guidesDeveloper guides Our developer guides are deep-dives into specific topics such as layer sublassing, fine-tuning, or model saving. They're one of the best ways to become a Keras expert. Most of our guides are written as Jupyter notebooks and can be run in one click in Google Colab , a hosted notebook...keras.io 3. https://www.tensorflow.org/guide?hl=ko - 한글로 잘 정리되어 있습니다.  TensorFlow Core즉시 실행, Keras 상위 수준 API 및 유연한 모델 빌드와 같은 TensorFlow의 기본 및 고급 개념을 알아보세요.www.tensorflow.org 4. https://github.com/tzutalin/labelImg#labelimg - 사용된 bounding Box작업하는 Tool tzutalin/labelImg🖍️ LabelImg is a graphical image annotation tool and label object bounding boxes in images - tzutalin/labelImggithub.com 5. https://www.kaggle.com/search?q=object+detection - Kaggle 에서 Object Detection 검색한 것 Search | Kagglearrow_back search closewww.kaggle.com ​다음 회 부터, 실재 작업을 시작 해 보겠습니다.​- panzer -​ "
[ML] Detectron2: A PyTorch-based modular object detection library ,https://blog.naver.com/horajjan/221680101729,20191017,"Detectron2 is flexible and extensible, and able to provide fast training on single or multiple GPU servers. Detectron2 includes high-quality implementations of state-of-the-art object detection algorithms, including DensePose, panoptic feature pyramid networks, and numerous variants of the pioneering Mask R-CNN model family also developed by FAIR. Its extensible design makes it easy to implement cutting-edge research projects without having to fork the entire codebase.출처 : https://ai.facebook.com/blog/-detectron2-a-pytorch-based-modular-object-detection-library-/Previous imageNext imageThis video shows different types of object detection tasks done with Detectron2. Detectron2: A PyTorch-based modular object detection libraryWe are open-sourcing Detectron2, the second-generation of our widely used object-recognition platform. Detectron2 has been rewritten from the ground up in PyTorch to enable faster model iteration and deployment.ai.facebook.com "
Tensorflow를 활용한 물체감지(Object Detection)(2) ,https://blog.naver.com/nangsit/222342642686,20210508,"Using Tool · LabelImg.py · download_images.py [ 설치 참조 ] https://blog.naver.com/nangsit/222340531396 Object Detection :: Android Studio by Tensorflow-Lite (1)Object detection in Android using TensorFlow Lite (2020) 예제 : https://www.youtube.com/watch?v...blog.naver.com 학습 목표  이전 과정에서 실행한 Object Detection 예제를 응용하여, 자신이 원하는 물체를 학습하여 Object Detection을 Customizing을 한다.​Step 1. 이미지 준비 학습에 사용할 이미지를 준비해야 한다. 딥러닝을 학습시키는 과정에서 성공적인 학습모델은 학습시간과 양질의 데이터 셋이 중요하다.  양질의 데이터 셋에는 Labeling 작업에서 Bounding Box를 정확히 지정해 주고, 너무 작은 양의 이미지를 사용하지 않음으로 좋은 학습모델을 생성할 수 있다. 결국 학습에 사용할 이미지가 다소 많이 필요하게 될 텐데, 이를 구글링을 진행하면서 일일이 다운을 받는 것은 시간과 의미 없는 노력의 낭비를 도출한다. 편리한 환경에서 이미지를 준비할 수 있도록 관련 도구를 검색하다 영상을 발견했고, 유튜버가 제공해 주는 Tool을 사용하면 편리하게 구글링 이미지를 받을 수 있다. [참조] https://www.youtube.com/watch?v=m7jwlDhsfQ4 첨부파일googleImageDownloader-master.zip파일 다운로드 ​ 설치가 완료되면 설치 디렉터리로 이동하여 가장 먼저 js_console.js를 notepad에 읽어온다. 읽어왔다면, ctrl+A를 눌러 전체 영역을 선택하고 내용을 복사한다.  이제 Chrome을 열어서 Google에 자신이 원하는 Object를 검색하여 이미지로 이동한 후에 Ctrl+Shift+J를 눌러 웹 콘솔창을 열어준다. 콘솔창이 열렸다면, 이전에 복사해둔 js_console.js의 내용을 입력해 준다. 주의사항은 콘솔의 내용을 실행하면 url을 txt로 저장하는데, 자신이 구글에서 스크롤 되어  로딩이 된 이미지의 url 정보만 저장이 된다. 그럼으로 자신이 원하는 만큼의 이미지 개수만큼 스크롤을 내린 이후에 Enter를 눌러 콘솔의 내용을 실행해야 한다. urls 수가 부족하다면 영어로 검색해보고 한글로도 검색해서 url을 받으면 충분한 이미지를 받을 수 있을 것이다. ( 중복 이미지 존재 가능 ) ​ 아래를 보면 urls.txt가 다운이 된걸 확인할 수 있다. 스크롤을 하지 않은 경우와 한 경우의 urls.txt의 line 수를 비교하면 아래와 같이 결과가 나온다. ​ 최종 urls.txt  음식을 총 4개를 구별하기 위해서 urls의 내용을 하나로 합쳐서 따로 저장하였다. 총 2752개의 url을 저장  원하는 url을 모두 수집한 이후에 cmd를 열어 download_images.py를 실행한다. no named module이라는 에러가 뜨면 해당 모듈을 pip install 해주면 된다.​ > cd googleImageDownloader-master 저장 디렉터리 > download_images.py     --urls 경로/urls.txt     --output 경로/image 저장 위치​ 실행 결과 ​  이미지를 받기를 전부 진행이 된다면, 이 이미지를 Labeling 작업을 거쳐야 하는데 사전 작업으로 사진의 크기를 줄여주는 작업을 거쳐야 한다.   이미지의 크기가 작을수록 화질은 안 좋아지겠지만, 연산에 소모되는 시간이 적어짐으로 사진의 크기를 줄이는 게 효율적이다. 이전에 이미지의 크기를 줄이지 않고 실습을 진행하면서 사진을 변환할 수 없다는 에러가 계속 발생하였는데, 이는 이미지의 크기를 변환해 주고 훼손된 jpg를 걸러주니  해결이 되었다. 만약 사전 작업 없이 Labeling을 진행한 후에 학습을 진행할 때 변환할 수 없다는 에러가 뜬다면, 사진 전부의 크기 변환과 훼손 jpg 제외 작업을 진행 해야 하며, Labeling 작업을 다시 해야 하니 꼭 미리 사전 작업을 진행하는 걸 추천한다. 사진 크기 변환작업은 아래의 링크에서 진행하면 된다. 해당 사이트에서는 비율에  맞춰 Resize 작업을 진행해 주어서 좋다. 무료로 이용한다면 최대 30개만 사진 변환이 가능하여 30개씩 이미지 변환작업을 거쳐주어야 한다.  ( Resize는 200x200으로 진행하였다 )​ 에러 내용 tensorflow.python.framework.errors_impl.InvalidArgumentError: {{function_node Dataset_map_TfExampleDecoder.decode_56}} assertion failed:    [Unable to decode bytes as JPEG, PNG, GIF, or BMP] [[{{node case/cond/cond_jpeg/decode_image/cond_jpeg/cond_png/cond_gif/Assert_1/Assert}}]] [[IteratorGetNext]]​ [참조] https://www.iloveimg.com/ko/resize-image/resize-jpg 한 번에 여러 JPG 크기를 조절하여 시간을 절약하세요!JPG 이미지 크기를 신속하게 조절하세요. 픽셀과 퍼센트를 지정하여 크기를 변경하세요.www.iloveimg.com 학습을 진행하다 보면 아래와 같이 손상된 파일은 .zip에 포함하지 않음으로 훼손된 이미지를 제외하고 학습을 진행할 수 있다.​  총 1506개의 image의 크기를 변환하였다. 데이터의 사이즈를 전부 변경한 이후에, 이미지의 이름을 음식명으로 분류를 하였다. 이름은 Shift를 누른 채로 원하는 영역 까지 스크롤 한 이후에 처엄 사진의 이름을 원하는 이름으로 변경하면 (n)으로 이름이 순차 정렬된다. 폴더의 정렬 기준을 '크기'와 '내림차순'을 선택하여 사진의  크기가 큰 순으로 정렬하여 맨 앞의 사진 크기를 확인함으로, 크기가 가장 큰 사진의 크기를 확인함으로 알 수 있다.     사진의 크기가 확인이 되었다면, 이미지 디렉터리에 train과 test 디렉터리를 추가해 주고 train 디렉터리에 80%~90%가량의 이미지를 넣어주고  나머지는 test 디렉터리에 넣어준다.​Step 2. Labeling 작업​ 학습을 위해서는 사진이 무슨 사진이며, 정답의 위치가 어디에 존재하는지 알려주어야 학습이 가능하다. 그러므로 사진에 정답을 새긴 annotation을 생성해야 한다. annotation을 생성하기 위해 LabelImg.py를 사용한다. 설치를 진행하지 않았다면, 이전 블로그에서 설치를 참조하여 진행한다. 처음에는 편리를 위해 predefined_classes.txt의 내용을 자신의 클래스를 입력해 준다. 이는 Bounding_Box를 생성하고 어떤 내용이 들어있는지 입력해 줄 때 사전에  정의된 class에서 항목을 선택할 수 있도록 도와준다. 경로는 설치경로/labelImg-master/data에 존재한다. 다음으로 labelImg.py를 실행한다. ​ > cd 경로/labelImg-master > python setup.py > python labelImg.py   실행 시 labeling 작업을 진행한 이미지 디렉터리를 열기 위해서 좌측의 메뉴에서 Open Dir에서 Image를 저장한 경로를 지정한다.​ ​ 지정 이후에 labeling 작업의 출력물인 annotation을 저장할 디렉터리를 좌측의 Change Save Dir 메뉴를 선택하여 지정해 준다. train/annotations 형식으로  디렉터리를 생성하고 지정한다. ​ labeling은 w 단축키를 이용하여 Bounding_Box를 생성할 수 있다. Bounding_Box를 원하는 영역을 선택하여 지정 후 저장하고 다음 이미지로 넘겨준다. 저장한 후에 자신이 원하는 디렉터리에 제대로 저장이 되는지 한번 확인을 하고 작업을 진행한다.  팁을 주자면 키 패드보다는 마우스를 사용하는 것이 훨씬 빠르며, 단축키를 손에 익혀서 사용해야 빠르게 annotations을 생성할 수 있다.  W : Bounding_Box 생성 Ctrl+S : 저장 A : 다음 이미지로 넘김 D : 이전 이미지로 넘김 Bounding_Box 과정 중에 사진을 걸러내는 작업도 진행한다. 본인이 보기에 학습에 적당한 사진이 아닌 것 같으면 사진을 삭제하고 다음 사진으로 넘긴다. 작업이 끝났다면 annotations 디렉터리와 images 디렉터리의 항목수가 동일한지 확인한다. 만약 다르다면 annotations 디렉터리의 항목을 전부 복사해서  images 디렉터리에 붙여 넣어 확인한다. 확인 이후에 Ctrl+z 혹은 Del을 입력하여 undo를 진행한다. 작업이 완료되면 test 디렉터리도 진행한다.  test 디렉터리를 열고 반드시 annotaion의 경로도 test/annotations로 변경을 해주어야 한다.  annotations을 이용 이후의 과정은 이전 내용과 동일함으로 생략한다.​  [ 참조 ] https://blog.naver.com/nangsit/222340531396 Object Detection :: Android Studio by Tensorflow-Lite (1)Object detection in Android using TensorFlow Lite (2020) 예제 : https://www.youtube.com/watch?v...blog.naver.com 약 6000번의 학습 이후 Android Studio에 적용 ​[ 최종 결과 ]  ​실습 간 에러코드 정리​  1.  no module named 'tensorflow.contrib' tensorflow 2.0 버전 이상에서는 tensorflow.contrib가 제거되었다고 한다.  tensorflow 버전을 확인하여 다시 실행 시 해결  > pip3 install tensorflow==1.15.0 [참조] https://maluchisoft.com/wp/tag/ugatit/ #UGATIT Archives - 적극적인 개발자 (PASSIONATE DEVELOPER)적극적인개발자, 마루치소프트, 마루치, Passionate developer, maluchi, maluchisoft.maluchisoft.com  2.  Could not load dynamic library 'cudart64_100.dll'; dlerror: cudart64_100.dll not found  cuda v10을 다운로드해서 실행 시 해결  [설치] https://developer.nvidia.com/cuda-10.1-download-archive-base?target_os=Windows&target_arch=x86_64&target_version=10&target_type=exelocal CUDA Toolkit 10.1 original ArchiveSelect Target Platform Click on the green buttons that describe your target platform. Only supported platforms will be shown. Operating System Architecture Distribution Version Installer Type Do you want to cross-compile? Yes No Select Host Platform Click on the green buttons that describe your host...developer.nvidia.com   [참조] https://stynxh.github.io/2020-07-16-solve-not-found-dll-tensorflow-cuda-korean/ CUDA Tensorflow 실행 시 “Could not load dynamic library cudart64_100.dll; dlerror: cudart64_100.dll not found” 오류 해결상황stynxh.github.io  3.  no module named 'object_detection'  환경 변수 경로의 문제 문제가 생긴 실행파일을 notepad 혹은 문서 편집 도구로 열어 import를 함으로 해결 import sys  sys.path.append(""C:경로/object_detection이전의 디렉터리 경로"") [참조] https://www.bangseongbeom.com/sys-path-pythonpath.html sys.path, PYTHONPATH: 파이썬 파일 탐색 경로import 문을 통해 다른 파이썬 파일을 불러올 때, 파이썬은 내부적으로 파일을 찾기 위해 sys.path와 PYTHONPATH에 있는 경로를 탐색합니다. 이 두 변수를 적절히 수정해 임의의 디렉터리에 있는 파이썬 파일을 손쉽게 불러올 수 있습니다.www.bangseongbeom.com  4.  NotImplementedError: Cannot convert a symbolic Tensor 5. A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy  numpy 버전 변경 시 해결 > pip3 install numpy==1.19.5  [참조] https://stackoverflow.com/questions/58479556/notimplementederror-cannot-convert-a-symbolic-tensor-2nd-target0-to-a-numpy NotImplementedError: Cannot convert a symbolic Tensor (2nd_target:0) to a numpy arrayI try to pass 2 loss functions to a model as Keras allows that. loss: String (name of objective function) or objective function or Loss instance. See losses. If the model has multiple outputs,...stackoverflow.com  6. Could not load dynamic library 'cudnn64_7.dll'; dlerror: cudnn64_7.dll not found  cuDNN이 v7 인 버전의 CUDA 10.1을 설치해야 하는 것으로 확인 cudnn64_8.dll 을 CUDA/10.1v/bin/ 폴더에 삽입하여 해결  [설치] https://developer.nvidia.com/rdp/cudnn-archive cuDNN ArchiveNVIDIA cuDNN is a GPU-accelerated library of primitives for deep neural networks.developer.nvidia.com   [참조] https://deep-deep-deep.tistory.com/5 Tensorflow Error 해결: Could not load 'cudnn64_7.dll'; dlerror: cudnn64_7.dll not foundWindow 10 CUDA 10.1 환경에서, tensorflow == 2.3.1를 설치한 후, Could not load dynamic library 'cudnn64_7.dll', dlerror: cudnn64_7.dll not found 에러가 발생하여, GPU가 잡히지 않는 에러가 발생한 경우..deep-deep-deep.tistory.com  7. Specified output array ""'TFLite_Detection_PostProcess'"" is not produced by any op in this graph  --output_arrays='TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1',   'TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3'​  --output_arrays=TFLite_Detection_PostProcess,TFLite_Detection_PostProcess:1,   TFLite_Detection_PostProcess:2,TFLite_Detection_PostProcess:3  따옴표 표기를 제거하고 실행하여 해결  [참조] https://github.com/tensorflow/tensorflow/issues/41829 Specified output array ""'TFLite_Detection_PostProcess'"" is not produced by any op in this graph. · Issue #41829 · tensorflow/tensorflowSystem information OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 TensorFlow installed from (source or binary): pip installed TensorFlow version (or github SHA if from source):...github.com  8. Could not load dynamic library 'nvcuda.dll'; dlerror: nvcuda.dll not found  nvcuda.dll를 설치하여 해결  [설치] https://ko.dll-files.com/nvcuda.dll.html nvcuda.dll 무료 다운로드 | DLL‑files.comnvcuda.dll, 파일 설명 : NVIDIA CUDA Driver nvcuda.dll와(과) 관련된 오류는 몇 가지 다른 이유로 발생할 수 있습니다. 예를 들어, 응용 프로그램에 결함이 있거나, nvcuda.dll이(가) PC에 존재하는 악성 소프트웨어에 의해 삭제, 잘못된 장소에 위치, 손상되었거나, Windows 레지스트리가 손상되었을 수 있습니다. 가장 일반적으로 발생하는 오류 메시지는 다음과 같습니다: nvcuda.dll을(를) 컴퓨터에서 찾을 수 없어서 해당 프로그램을 시작할 수 없습니다. 해당 프로그램을 재설치하여 이...ko.dll-files.com  9. Windows fatal exception: access violation  실행파일의 인자 값이 다 들어가 있는지 확인이 필요하다. 매치가 되지 않은 파일을 인자 값으로 넣을 시 발생한다. [참조] https://github.com/tensorflow/models/issues/7522 Windows fatal exception: access violation error (generate_tfrecord.py ) · Issue #7522 · tensorflow/modelsHi all I need your help a lot. I've been dealing with this problem for about 3 days. Moreover, I have to solve this problem in 1-2 days at the latest. ""(RobotaksiTespit) D:\RobotaksiTespit...github.com  10. failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error >pip3 install nvidia-modprobe [참조] https://developia.tistory.com/16 [Issue] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown errorubuntu 20.04 lts 환경에서 cuda, cudnn을 설치하고 난 후 텐서플로우에서 gpu를 사용할 수 있는지 확인하고자 했습니다. 확인하려고 한 명령어는 다음과 같습니다. import tensorflow as tf tf.config.list_physi..developia.tistory.com  11. assertion failed: [Unable to decode bytes as JPEG, PNG, GIF, or BMP] 데이터를 변환할 수 없다는 에러이다. jpg 중에 훼손되어 있는 이미지가 포함되어 있을 때 발생한다. 이미지 축소 및 걸러내기 작업을 진행하여 해결  [참조] https://www.iloveimg.com/ko/resize-image/resize-jpg 한 번에 여러 JPG 크기를 조절하여 시간을 절약하세요!JPG 이미지 크기를 신속하게 조절하세요. 픽셀과 퍼센트를 지정하여 크기를 변경하세요.www.iloveimg.com ​ "
Feature Pyramid Networks for Object Detection ,https://blog.naver.com/dr_moms/221768061037,20200111,"  안녕하세요 오늘은 Computer Vision에서 다양한 분야에 사용되는 Feature Pyramid Network에 대해서 포스팅해보겠습니다. 'Feature Pyramid Networks for Object Detection'이라는 제목으로 CVPR 2017에 발표된 논문이 제안한 방법으로, Object Detection Upsampling과 Skip Connection으로 Scale-Invariant 문제를 해결하고 이미지의 해상도를 높여 정확도를 올리는 방식입니다.​이 글을 보기에 앞서, R-CNN 계열 알고리즘의 이해가 필요합니다. 따라서 R-CNN, Fast R-CNN, Faster R-CNN에 대하여 먼저 이해하시고 이 글을 보는 것을 추천드립니다.​https://blog.naver.com/dr_moms/221631504020 Faster R-CNN안녕하세요 오늘은 지난 포스팅에서 살펴보았던 Fast R-CNN의 발전된 형태인 Faster R-CNN에 대...blog.naver.com  Introduction​기존의 Object Detection 알고리즘에서 사용되고 있는 방법들과 논문에서 사용된 방법을 비교합니다.​​1) Featurized Image Pyramid​   hand-engineered feature를 사용하여 다양한 크기에서 feature map을 추출합니다. 각 level에서 독립적으로 Feature를 추출한 후 예측하는 방식이기 때문에 광범위한 객체를 탐지할 수 있으나, 연산량과 시간 관점에서 매우 비효율적이며, 현실에 적용하기가 어렵습니다.​2) Single feature map​   Convolution Layer가 scale 변화에 강건하여, 이를 이용해 feature를 압축하여 예측하는 방식입니다. 그러나 고정된 크기의 입력만을 받을 수 있고, 다양한 크기에서 feature를 나타낼 수 없기 때문에(multi-scale feature representation) 성능이 부족합니다.​3) Pyramid feature hierarchy​   SSD(Single Shot multibox Detector)에서 사용하는 방식입니다. Convolution 연산을 수행할수록 feature map size가 작아진다는 점을 이용하여 서로 다른 크기의 feature map을 이용하여 예측을 수행하게 됩니다. 각 Pyramid level에서 독립적으로 feature를 추출하여 예측하지만, Convolution 연산을 수행할수록 Semantic feature는 더욱 강해지지만 이미지의 해상도가 떨어져 작은 Object를 감지하기가 어렵다는 특징이 있습니다.​4) Feature Pyramid Network​   논문에서 사용하는 방식으로 Top-down pathway와 lateral connection을 이용하여 저해상도의 이미지와 고해상도의 이미지를 더하여 압축된 feature map에 해상도를 보완한다는 방식입니다. Top-down pathway와 lateral connection에 대해서는 밑에서 다시 설명하도록 하겠습니다. 역시나 3)과 마찬가지로 독립된 level에서 독립적으로 feature를 추출하여 예측합니다. 더하여 multi-scale feature representation을 효율적으로 사용한다는 장점이 있습니다.  Feature Pyramid Networks​임의의 크기의 단일 scale 이미지를 입력으로 하며, backbone은 ResNets를 사용하였습니다.    Bottom-Up pathway​backbone CNN의 feed foward 계산입니다. 여기에서는 각 Layer마다 하나의 Pyramid level을 정의하고, 동일한 크기의 output feature map을 생성하는 Layer들이 있다면, 가장 마지막 Layer의 출력을 reference feature map으로 선택합니다. 가장 깊은즉, 가장 마지막의 Layer가 가장 강한 feature를 가지기 때문입니다. 입력 이미지의 메모리 공간이 커서 Pyramid에 conv1은 포함시키지 않았습니다. 각 level의 마지막 layer를 C2, C3, C4, C5라고 부릅니다.  Top-down pathway and lateral connections​   ​Sematic이 강한 feature map을 2배로 Upsampling 하고 Bottom-Up pathway에서 선택한 reference feature map을 더하여 해상도를 보완하는 것이 핵심입니다. 채널 수(논문에서 d=256)를 맞춰주기 위해 reference feature map에 1 × 1 Convolution 연산을 수행합니다. 마지막으로 병합한 feature map에 3 × 3 Convolution 연산을 추가하여 최종적인 feature map(P2, P3, P4, P5)을 생성합니다. 이는 Upsampling의 aliasing(신호가 변형되어 일그러짐이 생기는 현상)을 제거하기 위함입니다.   Application​Feature Pyramid Networks for RPN​   RPN에  적용하기 위하여 feature pyramid의 각 level에 RPN head를 부착시킵니다. RPN head는 3 × 3 conv 및 2개의 형제(sibling) 1 × 1 conv로 이루어져 있습니다. 각 level(P2, P3, P4, P5, P6)에서 {1 : 2, 1 : 1, 2 : 2} 3개의 aspect ratio의 anchor(크기는 각 level 별로 322, 642, 1282, 2562, 5122)를 사용하여 총 15개의 anchor를 사용했습니다. Ground Truth에 대해서 IoU가 0.7보다 크면 positive label로 지정하고, 0.3보다 낮으면 negative label로 지정했습니다. head의 parameter는 모든 feature pyramid level에서 공유됩니다.   Feature Pyramid Networks for Fast R-CNN​   FPN을 통하여 다양한 크기의 feature map을 추출하고, 가장 해상도가 높은 feature map을 이용하여 Region Proposal 단계를 수행합니다. 이때 RPN을 사용했습니다. 각 제안된 영역을 적당한 크기의 feature map P_k에 할당하였습니다.   그 후, RoI Pooling 단계를 거쳐 Bounding Box regression과 Classification을 수행합니다.  Experiments & Result​  Bounding Box proposal results using RPN ​  Object detection results using Fast R-CNN ​  Object detection results using Faster R-CNN  Conclusion​Top-down pathway 없이 Bottom-Up pathway 만으로 Network를 설계하면 Layer 별 Semantic 수준이 다르기 때문에 성능이 좋지 않습니다. 깊으면 깊을수록 이러한 현상이 두드러집니다. 또한 lateral connection이 없다면 Top-down pathway를 가짐으로써 높은 sematic feature를 가지고 있을 순 있지만 이미지 해상도가 크게 떨어져 지역 정보가 부정확하여 정확도가 떨어집니다. 따라서 이전 연산에서 계층별 Output feature map을 재사용하는 것이 Multi-scale feature representation에 효율적이며 정확도 향상에 좋습니다. 결론적으로 Multi-scale feature representation이 매우 중요함을 알 수 있습니다.  참고 문헌 및 이미지 출처​https://arxiv.org/pdf/1612.03144.pdf​https://medium.com/@lsrock125/feature-pyramid-networks-for-object-detection-%EB%85%BC%EB%AC%B8%EC%9D%BD%EA%B8%B0-e4e577c4b423 Feature Pyramid Networks for Object Detection 논문읽기Scale-Invariant는 object detection에서 아주 중요한 과제다. 예전에는 다양한 크기의 물체를 탐지하기 위해 이미지 크기를 리사이즈 하면서 물체를 찾았다(a). 하지만 이런 작업은 메모리와 시간을 아주 많이 잡아먹는다.medium.com https://seongkyun.github.io/papers/2019/12/06/fpn/ Feature Pyramid Networks for Object Detection · Seongkyun Han's blogFeature Pyramid Networks for Object Detection Feature Pyramid Networks for Object Detection Original paper: https://arxiv.org/abs/1612.03144 Authors: Tsung-Yi Lin, Piotr Dollár, Ross Girshick, Kaiming He, Bharath Hariharan, Serge Belongie (Facebook, Cornell Univ.) 참고 글 https://eehoeskrap.tistory.com...seongkyun.github.io ​ "
Rich feature hierarchies for accurate object detection and semantic segmentation ,https://blog.naver.com/ethan26/222417460144,20210702,"R-CNN 논문은 본 논문과 저자가 쓴 보조 자료가 있다. 논문에서 보조 자료를 언급한다. (Supplementary Material) 본 포스팅에서는 보조 자료 내용까지 포함해서 썼다.​​Abstract 지금까지는 Low level feature와 High level feature의 Ensemble이었다. 여기서는 mAP를 올리는 방법을 소개한다. 1) Localization에 CNN을 쓴다. 2) Labeled data가 부족하면 supervised-pre training을 한다.  Region proposal에 CNN을 적용하므로 R-CNN이라고 부른다.​1. Introduction HOG, SIFT 등으로 해왔지만 큰 진전이 없었다. 그리고 이 논문에서 처음으로? CNN을 object detection에 이용해서 HOG 비슷한 성능을 냈다. Classification과 달리 Detection은 Location이 필요하다. 그래서 Localizatoin을 Regression으로 취급하기도 한다. C. Szegedy [Deep neural networks for object detection]에서도 이렇게 했는데 우리 게 더 잘났다.​​[Neural network based face detection][Original approach for the localisation of objects in images][Pedestrian detection with unsupervised multi-stage feature learning]이 논문들은 Sliding Window를 썼는데 Sliding Window의 Resolution을 높이기 위해서 CNN은 Convolution 2 개, Pooling 2 개만 썼다. ​이 논문에서도 Sliding Window를 썼고, CNN은 receptive field가 195×195, Convolution layer 5개, Stride 32로 구성해서 Localization의 정확도를 높였다.​Category independent한 2000개의 Region proposal에 CNN을 적용하였으며, CNN의 결과로 정해진 길이의 output을 뽑고 이걸 다시 SVM으로 분류한다.​또 하나의 문제는 지금의 Dataset이 적어서 Large scale의 CNN을 Training할 수 없다는 것이다. 이에 대한 최근의 해결 방법은 Unsupervised Pre-training을 먼저 하고 Supervised fine-tuning을 뒤에 하는 것이다. [Pedestrian detection with unsupervised multi-stage feature learning]에서 이렇게 했다. 이 논문에서는 ILSVRD Data를 이용해서 Supervised Pre-training을 하고 PASCAL의 small Data로 fine tunning한 것이다. ​Pre-trainingFine Tuning참고 논문UnsupevisedSupervised이 논문SupervisedSupervised ​ 이 논문의 방법은 HOG나 DPM보다 좋았다. 그리고 효과적이기도 한다. Category 별로 필요한 연산은 matrix, vector 간 곱과 greey non-maximum suppression 뿐이다.  ( NMS : Non-Maximum Suppression은 Bounding box들에 점수를 매겨서 최고점을 기록한 Bounding Box만 선택하는 거라고 보면 된다.)  HOG의 장점은 이해하기가 쉽다는 것이다. CNN 내부를 이해할 수 있을까? CNN이 복잡하면 잘될까? CNN 내에서 94%를 제거해도 detection accuracy는 그닥 떨어지지 않는다. 대신 CNN 내부를 살펴봤을 때 아래 그림처럼 여러 가지 특징을 뽑아내기는 한다.​ [Diagnosing error in object detectors]에서의 detection analysis tool을 썼다. Bouding Box를 Regression해서 Mis-localization의 가장 큰 원인을 줄일 수 있다.(요건 [Object detection with discriminatively trained part-based models]에서! 이 논문에서 중요함! 다른 포스트에서 따로 정리!)​​​2. Object detection with R-CNN Module 1 : category independent region proposal Module 2 : CNN extracts fixed length feature vector Module 3 : SVM​2.1. Module Design Region Proposals : [Selective Search for Object Recognition]의 Selective search를 이용한다.​ Feature Extractions : CNN 입력으로 mean-subtracted 227×227 Input을 5개 층 Convolution Map을 통과시키고, 2개의 fc layer를 추가로 통과시켜서 4,096 길이의 feature를 뽑는다. [ImageNet classification with Deep CNN]에서 Net의 전체 구조를 보라. (지금은 안 봐도 되겠다. Alex Net임.) CNN Input으로 쓰기 위해서 모든 Region을 Warping한다. Warping 결과는 227×227이며, 테두리에 배경 화면을 16Pixel씩 포함하도록 한다(이거 무슨 말인지 잘 모르겠다. 일단 패스). 아래 그림.​ ​​​2.2. Test-time detection Selective search를 fast-mode로 써서 2000개 정도의 region proposal을 찾는다. CNN과 SVM을 통과한 후 각 region들의 각 Class에 대한 점수를 매긴다. 그리고 각 Class에 대해서 독립적으로 NMS(Non-Maximum Suppression)을 적용해서 Score가 높고 IoU(Intersection over Union)이 Threshold 이상인 region을 제거한다. (겹치는 거 제거)​Run-time analysis :  두 가지 특징으로 Detection의 효율이 좋다. 1) CNN의 Parameter들을 모든 Category들이 공유한다. 2) CNN의 결과로 나온 feature vector가 Low level이다.​The feature matrix is typically2000×4096 and the SVM weight matrix is 4096N,where N is the number of classes.​​​​2.3. Training Supervised Pre-training : Bounding box 없이 ILSVRC 2012 Alex Net의 결과를 Transfer Lerning 함. Learning rate는 0.01​ Domain-specific fine tunning :  Input을 Warping한 VOC Image로 바꿔서 Fine tunning 한다. ImageNet의 1000개 Class를 Random setting한 VOC 21개 class (1개는 background)로 바꿔서 진행한다. Ground-truth box와의 IoU가 50% 이상이면 Positive, 아니면 Negative로 나눈다. SGD Learning rate는 0.001이고, 이 값은 Fine tunning을 진행하면서도 CNN의 초기값을 크게 망가뜨리지 않는다. 각 Epoch에서는 32개의 positive와 96개의 negative로 총 128개의 input을 batch로 돌린다. ​Object category clasifiers : IoU가 0.3 이하이면 negative로 한다. 0.3이라는 값은 신중하게 찾은 것이다. [Selective serach...]에서처럼 0.5로 해버리면 mAP가 나쁘다.  각 Class 별로 linear SVM을 따로 설계한다. 그리고 데이터 양이 많아서 Memory에 모두넣을 수 없으므로 Hard Negative Mining Method를 쓴다. [Object detection with discriminatively trained part based models], [Exampl-based learning for view-based human face detection]​※Supplementary Meterial에서 설명한 CNN, SVM Training CNN :      1) fine tunning에서는 각각의 object proposal을 maximum-IoU를 보이는           ground-truth에 match     2) 1)에 해당하는 Ground truth가 있으면 그 region proposal은 해당          ground-truth로 labeling한다.        3) 해당하는 Ground truth가 없으면 모든 Class에서의 Negative​    SVM :     1) 각 Class의 Ground truth만 Positive     2) 모든 class에서 0.3 IoU 이하인 proposal만 negative로 본다.   WHY? :     - CNN fine tunning 대로 했더니 SVM이 잘 안되더라.        왜 그럴까? 사실은 pos neg 정의는 중요한 게 아니고         fine tunning data가 적은 게 문제인 것 같다.        0.5~1.0 IoU의 example이 positive를 30배나 늘린다.        이건 overfitting 없는 전체 network를 train하는 데는 도움이 되지만        network이 최적화되어있는 것이 아니기 때문에 좋은 선택이 아닌 것 같다.​  WHY SVM? :    - 그럼 SVM은 왜 또 따로 쓰나?      CNN에 softmax를 연결했을 때 결과가 나빴다.      몇 가지 원인이 있을 건데      CNN에서 정의한 positive의 정의가 localization을      정확하 하는 것에 초점을 맞춘 것이 아니고      (0.5가 너무 낮다??)      softmax classifier에서 쓴 것이 random negative sample이고      SVM에서는 hard negative를 썼기 때문이다.      CNN을 좀 더 fine tunning하면 SVM과 비슷한 성능을 보여줄 것이다.​ Supplementary Material 1. Object propoal transformations     : 227×227로 맞추는 이야기. (대충 그렇다...)​  2. Positive vs. negative examples and softmax     : 위에서 설명 했음.​ 3. Bounding box regression     : 아래에서 설명 했음.  ​2.4. Results in PASCAL VOC 2010-12 다른 알고리즘들과 비교. 특히 [Selective search for object recognition]. Region proposal은 똑같이 하고 대신 SS에서  non-linear kelnel SVM을 쓴 것에 반해 여기서는 mAP도 높이고 속도도 높였다.(2.2)​​3. Visualization, ablation, modes for error3.1. Visualizing learned features 이 논문에서는 쉽고 완벽하게 Visualizing 한다. 방법은 1) 1,000만개의 region proposal들의 activation을 계산한다.  2) Activation을 정렬한다. 3) Non-maximum suppression으로 region을 제거한다. 4) 최고점 region을 표시한다. 5) 4)의 결과를 해당 layer가 뽑아내는 특징으로 본다.  이 논문에서는 마지막 Layer(5번째)인 Pool5를 썼다. Poo5의 feature map은 6×6×256 =  9,216차원. Receptive field는 195×195(orignial image 227×227)이다. ​아래 그림. 가로 16은 top-16, 세로 6은 6개 class. 2번째 row에서 개와 구멍이 섞인 것을 볼 수 있다. 이 Visualization 결과로 볼 때 Net은 물체의 형태, 질감, 색, 물성 등을 학습하는 것으로 보인다. ​3.2. Ablation studiesPerformance layer-by-layer, without fine-tunning. 어떤 Layer가 성능에 영향을 주는지 보기 위해 마지막 3개 Layer 분석. 1개는 위에 3.1.에서 보여줬고, 여기서는 나머지 두 개. 1) fc6 : pool5의 output : 9216 dim(6 × 6 × 256을 9216 × 1로 reshape)               fc6의 matrix : 4096 × 9216               fc6의 bias : 4096 × 1               fc6 activation : half-wave rectify ( = max(0, 4096 × 1c6) )               fc6의 output : 4096 × 1 2) fc7 : fc6의 output : 4096 × 1               fc7의 matrix : 4096 × 4096               fc7의 biase : 4096 × 1               fc7 activation : half-wave rectify ( = max(0, 4096 × 1c6) )               fc7의 output : 4096 × 1​​ PASCAL fine-tunning을 하지 않은 Net을 먼저 평가해보자.​ fc6의 출력이 더 좋다 .(mAP가 높다.) 16.8백만 parameter를 줄일 수 있다는 의미. 게다가 pool5만으로도 충분히 좋은 결과가 나온다. 이 CNN의 6%만으로도 좋은 결과를 뽑을 수 있다. ​Performance layer-by-layer, with fine-tunning 이번엔 fine-tunning 한 것을 평가. fc7의 성능 boost가 fc6보다 좋다. (fc6 : 46.2→53.1, fc7 : 44.7→54.2) 이것은 ImageNet으로부터 일반적인 학습을 하고, domain-specific classifier에서 성능 향상이 이루어졌음을 의미한다.​Comparison to recent feature learning methods. 두 개를 비교하겠다. (HOG-based DPM도 reference로 써서.)​  DPM들 33.7, 29.1, 34.3 등으로 R-CNN (최소 40)보다 안 좋은 거 보이지? R-CNN win~​3.3. Detection error analysis [Diagnosing error in object detectors]의 방법으로 fine tunning 등의 성능을 평가 했다. 평가 Tool에 대해서는 여기서 다 소개할 수 없으니 직접 찾아보라 (...고 씌여있음.) 각각의 FP(False Positive의 비율이 전체 false positive가 늘어남에 따라 어떻게 변하는지에 대한 그림이다.  Loc : Localization이 잘못된 것.(poor localiztion) Sim : 유사한 Class로 틀린 것. Oth : 전혀 다른 Class로 틀린 것. BG : Background인 것으로 판정했으니 실제로는 Foreground였던 것.​직관적으로 당연해보인다. 전체 False Positive가 늘어날수록 Back ground를 Foreground로 오인하는 수가 늘어난다. (많이 Detect하니까.)​또 전혀 다른 Class로 오인하는 경우도 늘어난다.(Oth)​비슷한 Class 간 오인 비율은 별로 변하지 않는다.(Sim)​Localization이 잘못되는 비율은 줄어든다. (많이 검출하니까?) ​​다른 그림..각 지표 별 성능을 평가  occ : occlusion trn : truncation size : bounding bos area asp : aspect ratio view : view point part : part visivility([Diagnosing error in object detectors]의 평가)  FT는 Fine tunning. FT 결과가 (모든 지표에서는 아니지만 대부분의 지표에서 성능을 향상 시킨다.) 특히 Highedt 성능은 전체적으로 다 좋아졌다. 가로 점선이 robustness인가본데 이것도 좋아졌다. ​3.4. Bounding box regression   Bounding box의 정합성을 향상시키기 위해 Regression    Training 대상 :     Validation set 중에서 Ground truth와의 IoU가 0.6 이상인 Proposal​  Training 방법 :​    1) 아래의 P, G 정의              : i번째 Ground truth Bounding box와 IoU 0.6 이상인 Proposal의 x, y              center 좌표, width, height​             : i번째 Ground truth Bounding box의 x, y center 좌표, width,               height​  2) x, y center 좌표의 Proposal to Prediction 변환식은 Shift일    것이다. 또한 Shift 하는 정도가 center 좌표에서 나오는 것은     non-sense이고 (같은 Object라도 Image의 여러 위치에서 나올 수 있으므로)     width와 height의 함수로 보는 것이 타당하다.      (Object의 크기가 클 때의 Shift와 작을 때의 Shift는 크기에 비례할 것으로     가정하면)     Shift 정도의 정답 값을 Tx, Ty라고 하면          로 쓸 수 있다.​  3) 비슷하게 width와 height는 width와 heigth의 scaling으로 가정한다. 정값 scale을      Tw, Th라고 하면  4) Tx, Ty, Tw, Th의 prediction을 각각 dx(P), dy(P), dw(P), dh(P)로 하고,      prediction에 의해 나온 Ground truth G의 prediction을 Ghat이라고 하면   5) 이 Regression이 목표는 Dx, y, h, w를 최적화하는 것이다. 정답이 Tx, y, w, h이므로 먼저      2)와 3)이 수식에서 Tx, y, w, h를 Explicit하게 표현한다.   6) 또한 Pool5의 출력 phi5 (6×6×256 = 9,216)의 출력 vector에 대응하는 w를 최적화하여     regression을 하며, 우측 Term은 Regularization이다.​  Lamda는 1,000이다.​​​4. Semantic segmentation Regin classificaiton으로 Semantic segmentation을 할 수 있다.​ 결론... R-CNN이 O2P보다 겨우 좋더라??? "
Overcoming Catastrophic Forgetting in Incremental Object Detection via Elastic Response Distillation ,https://blog.naver.com/jjunsss/222891884042,20221004,"ERDERSElastic Response DistillationElastic Response SelectionCatastrophic forgettingContinual LearningIncremental Learning2022 CVPRCNN을 기반으로 하는 Detection 모델에 Catastrophic forgetting을 방지하기 위한 방법으로 ERD + ERS 라는 새로운 전략을 제시하는 논문입니다. 기존에 사용하던 Distillation방법과 유사하지만 다른 Head의 개념에 맞게 변경해서 사용하였고, All Response 말고 High Confidence response만을 사용해서 좋은 성능을 냈다고 이야기 하고 있습니다.내용은 간단하게 이해할 정도로 소개합니다.​IntroductionCL = Continual Learning은 우리 동물들에게는 정말 당연한 일이지만 딥러닝 모델에서는 그렇지 않습니다. 현재까지의 딥러닝은 지속적으로 들어오는 정보를 처리하지 못하며, 새롭게 들어오는 정보가 있으면 이전의 정보에 합쳐서 다시금 훈련을 진행하는 방식으로 모델을 훈련하였습니다. 새로운 정보를 훈련할 때 이전의 정보를 잃는 것을 딥러닝에서는 Catastropic forgetting이라고 하며, 해당 방식을 줄이는 방법이 지속적으로 연구되어지고 있습니다.또한 해당 연구는 Class, Task, Domain 이라는 3가지 방법으로 나누어져 연구가 진행되고 있는데 ERD+ERS에서는 하나씩 Class를 늘리는 방식으로 훈련을 진행하는 그러니까 즉 가장 현실과 동일하게 훈련을 하는 방법을 채택하였습니다. Class 방법은 만약 이전까지 훈련한 Class가 50개면 연속해서 5개의 Class를 추가로 훈련하는 등의 Class를 기준으로 훈련을 진행하는 것을 말합니다.또한, 해당 논문 이전까지는 Feature를 조절하여 이전의 정보를 담은 Feature는 고정하고 새로운 정보를 담을 수 있는 Feature를 만드는 형태로 연구가 진행되었다면, ERD 논문에서는 이전의 Class들을 훈련한 모델의 Response. 즉, 이전에 훈련을 완료한 모델을 통해 새로운 데이터를 넣었을 때의 출력. 을 사용해서 새롭게 훈련하는 모델의 Catastropic forgetting을 방지하는 방법을 제시한다고 합니다.또한, 이러한 Response를 사용한 방법을 이전까지는 모든 Head(Classfication, Regression)에서 동일하게 사용했다면 해당 논문에서는 Regression과 Classification의 Head를 분할하여 다른 Loss를 사용해며, 여기서 사용되는 Response는 Positive + Negative한 정보를 모두 담고 있기 때문에 모든 Response를 전달하는 것보다는 통계적 방법을 통해 양질의 Response만을 사용한다고 합니다.​ Selected response의 우수한 성능해당 논문에서는 이렇게 각각의 Head에 Distillation방법을 적용하는 것을 Elastic Response Distillation이라고 하여 ERD라고 줄여부르며, Distillation 방법을 적용함에 있어서 위의 도표에서 볼 수 있는 것처럼 모든 응답을 증류하는 방법은 좋지 못한데 그렇다면 이때 어떠한 방법을 사용해서 증류할 응답들을 고를지에 대해 정의 한 것이 Elastic Response Selection ( ERS )라고 부르며 논문을 전개합니다.​Related WORK​Incrementa Learning 당연함, 이전부터 Catastropic Forgetting을 방지하기 위한 방법들은 엄청나게 많이 소개되었습니다. 하지만, Detection 방법 중 Response를 사용해서 Catastropic Forgetting을 방지하는 것은 드뭅니다.또한, Head 각각에 중요성을 다르게 주어서 Distillation을 적용합니다. ( ERD + ERS )Detection with CNNDistillation​Method Overall Structure전체적인 구조는 위의 Fig2 와 동일합니다. 하나씩 뜯어서 간단하게 소개하자면 아래와 같습니다. ERD + ERSClassifciation Head / Regression Head 의 정보를 각각 받아서 Student 모델을 학습시킵니다.Teacher Model의 Localization information을 받아서 Student 모델의 Localization 능력을 향상시킵니다.ERS 방법을 통해 의미있는 응답만을 가져와 ERD 전략을 진행합니다. 이렇게 했을 때 위에서 소개한 이미지처럼 더 좋은 성능을 기록할 수 있다고 합니다.위에 설명한 것을 모두 적용하여 Loss를 만들었을 때 전체적인 Loss 식은 위와 동일합니다. 람다는 각 Loss의 Balance를 위해서 만들어 두었으며 논문에서는 1로 설정하였다고 합니다.( 위에서 T: Teacher, S : Student 를 의미합니다. )또한, 위에서 cls, bbox는 전부 old class outputs을 위해 존재합니다.  한마디로 이야기하면, 새로운 정보는 L_model을 통해서 학습하고, 이전 분류에 대한 정보는 증류를 통해 이전모델과의 비교를 받아오는 것으로 이해할 수 있습니다. ( 거의 Distillation이 사용되는 곳에서는 Old Model에 중요성을 부여합니다. ) ​​ERD at Classification Head이전에 계속해서 말한 것처럼 ERD의 차별 포인트는 repectively Head에 각각 적용하는 Distillation Method라는 점입니다. 여기서 Classification 에 대한 방법을 소개합니다.이전에도 작성했었던 Knowledge Distillation 방법에서도 나왔지만, Knowledge Distillation은 모델의 Output을 Hard하게 [ 0, 1, 0, 0 ] 과 같은 형태로 나타내면 하나의 클래스에만 정보를 가지게 되기 때문에 한정적이라고 합니다. 따라서 정보를 전달하기위한 방법으로 Soft Label 방법을 선택했는데 여기서도 해당 방법을 사용합니다.그렇게 사용하는 방법이 Softmax와 Temperature를 사용해서 확률 P를 만들고 이를 Teacher와 Student의 비교를 KL_div를 사용해서 진행하곤 했었습니다. 하지만, 이러한 방법은 모든 응답을 동등하게 받아들이면서 foreground와 Background의 불균형 마저도 그대로 가져오게 됩니다. 따라서 개선의 필요성이 존재했는데 이것을 해당 저자들이 바꾸게 되었습니다. Softmax를 적용한 확률을 KL-DIV로 지식증류하는 것이 아닌, Softmax를 적용하기 이전의 Class Logit을 그대로 Teacher-Student의 비교를 통해 전다랗는 것입니다. 이는 아래의 식에서 확실하게 이해할 수 있습니다. 얼핏보면 비슷해보입니다. 하지만 위에서는 확률분포를 증류하고 아래에서는 Softmax를 지나기 이전인 하나의 Class의 가능성을 나타내는 Logits이 비교되는 것을 확인할 수 있습니다. 또한 비교를 L2 Loss를 통해 계산하고, 이전에는 KL-DIV를 통해 분포를 비교한 것을 확인할 수 있습니다. 이러한 차이는 P_t, P_s 가 어떻게 구해지는 지에 대한 아래의 식을 보면 더 확실하게 이해할 수 있습니다. 이러한 방법은 Knowledge Distillation에서 사용되었고, 모델의 출력이 더 많은 정보를 담을 수 있도록 Soft하게 만들어주는 T : Temperature를 사용하고 있습니다. ( KD 에서는 이렇게 Temerature를 적용한 Softmax information을 전달하는 작업을 Dark Knowledge 라고 말하기도 하였습니다. 이부분에 대해서는 제 블로그 Knowledge Distillation에서 이야기 합니다. )참고로 이렇게 비교를 진행하게 되는 응답을 고르는 방법을 여기서는 ERS 라고 해서 새롭게 전략을 제시하고 있습니다. ( Head 마다의 Response 선택 방법에 약간의 차이가 존재합니다. )​​ERD at Regression HeadRegression은 Classifcation과는 다르게 이미지 내에 이전에 사용된 클래스 객체가 존재하지 않더라도 낮은 신뢰도를 가지며 계속해서 위치를 예측하게 됩니다. 이것은 지식을 증류하는 데에 있어서 문제를 만들어내게 됩니다. 또한 이전의 방법들은 높은 신뢰도를 가지는 방법에 대해서만 Regressions distillation 을 진행했고, 이러한 방법은 Regression head의 Knowledge를 무시하게 되는 효과를 만들었습니다. 따라서 저자들은 GFLV1 에 사용된 일반적으로 BB를 예측하는 방법을 가져왔고, 이는 각각의 좌표 [Top, Bottom, Left, Right]를 Softmax를 사용해 확률로 나타내었습니다. ( 모든 엣지에 대한 정보 ) 위와 같이 Bounding Box를 나타낸 다음 KL Div를 사용해서 Teacher-Student의 차이를 증류합니다. 또한 J는 새로운 데이터를 사용함에 따라 선택되어지는 BB 응답이라고 합니다. ( ERS ) 이러한 정보는 Localization에서 추가적인 정보를 전달해주어, 객체가 존재하는 위치를 더욱 정확하게 파악할수 있도록 하는것에 도움을 주었다고 합니다.그러니까 요약해보면, 이전 모델의 Lcalization 에측 정보를 가져와 새로운 데이터를 할당했을 때 새로운 데이터를 받을 수 있는 모델이 출력하는 Localization 정보와 이전 모델이 출력하는 정보를 KL-DIV를 통해 전달받습니다. 아마도 이러한 정보는 이전 정보의 BB를 예측하는데의 정보를 주지 않을까 싶습니다.​​Elastic Response SelectionERD에서 계속해서 서로의 응답을 비교한다고 말하고 있는데, 해당 방법만 사용하게 되면 All Responses 즉, 이전에 증류 기법에서 약간만의 변화가 일어난 셈입니다. 따라서 저자들은 해당 기법을 제시하였고 이는 곧, All Responses에 비해 통계적 기법을 적용한 일부분의 Responses를 Distillation에 사용하는 것이 이득이라는 것을 밝히는 것에 초기 주장을 뒷바침하게 되었습니다.기존에 Selection 방법들도 있었기는 하지만 이전에는 하이퍼 파라미터로 제공되는 Threshold or Top-K에 의존해서 Selection을 제공하였고, 이러한 방법들은 적절한 하이퍼 파라미터를 설정하면 엄청나게 모델 훈련에 도움이 되었지만, 최종적으로는 잘못 선택했을 경우 Negative한 정보가 성능을 떨어트리는 주요한 요인이 되었다고 합니다.따라서 저자들은 새로운 방법인 통계적으로 Selection하는 방법인 ERS를 만들어 제시하게 되었습니다. 또한 계속해서 설명한 바에 따라 각각의 Head에 따라 응답을 선택하는 기준이 다릅니다. ( Classification Head, Regression Head ) ​ClassificationTeacher 모델의 Classification Score 계산 ( 각 노드에서 Confidence 들을 계산해서 하나로 모은다고 되어있는데 위의 Overall architecture에서 살펴보면, Classfication의 Class 개수만큼 존재하는 출력의 가능성을 나타내는 Logits에서의 Confidence score를 말하는 것 같습니다. 즉, Class 개수 만큼의 Output이 있다면, 이를 각각 node라고 보고 Logits을 계산하는 것입니다. )i개 이미지에 대한 신뢰도의 평균 및 표준 폍차위에서 구한 평균 + 표준편차를 사용해서 통계적인 분포상 우수하다고 생각할 수 있는 일정한 Threshold 값 설정.threshold를 넘는 응답 노드들을 출력 set으로 추가 ( ERS classification의 출력 )​​​RegressionGFLV1의 BB 구조를 사용하는 만큼 GFLV의 기본 배경을 많이가져오게되는데 GFLV1의 BB 예측중에 모호하지 않고, 정확한 애들을 살펴보면 분포가 항상 Sharp하다고 합니다. 따라서 BB의 분포가 확실한 경우 Top-1 이 상대적으로 크기 때문에 Top-1 값을 사용한다고 합니다. 또한 Top-1 value는 각 BB의 신뢰도를 기준으로 진행한다고 합니다.Teacher 모델의 Top-1 Value 선택J 개의 분포에 대한 대한 신뢰도의 평균 및 표준 편차 계산위에서 구한 평균 + 표준편차를 사용해서 통계적인 분포상 우수하다고 생각할 수 있는 일정한 Threshold 값 설정. → 통계적 방법threshold를 넘는 응답 노드들을 출력 set으로 추가 ( ERS classification의 출력 )추가된 B set 들과 새로운 모델의 Top-1 BB 분포를 모은 그룹의 NMS( Non Maximum Suppression 을 통해 모델을 구성합니다.​​이러한 방법을 통해 ERS는 좋은 응답들을 선택한다고 합니다. 또한 이렇게 ERS를 사용해서 좋은 응답들을 고르게 되면 장점이 존재하는데 다른 응답들 사이에서 공평함을 유지할 수 있다고 합니다. 하나의 이미지에서 긍정적인 응답은 100 - 1000까지 존재하게 되는데 그 사이에서 표준편차와 평균을 사용해서 가져오는 것이기 때문에 일정한 응답을 가져오게 되고 이것이 곧 Selection에 공평함을 부여하는 것이라고 말할 수 있는 것 같습니다.​ 각 샘플에 대한 ERS를 거쳤을 때와 거치지 않았을 때의 응답들을 나타내는 것입니다. 대부분의 값들이 Low Confidence를 가지는 것을 볼 수 있습니다. All Response를 전달할 때에는 이러한 정보를 전달하다보니 보다 높은 성능을 내기 힘들었던 것 같습니다.  추가로, 위의 Table에서는 Response가 어느정도 되었을 때 가장 좋은 성능을 내는지를 나타냅니다. 해당 값들은 위에서 소개되었던 Figure 1과 관련있으며 정확히 100개 정도의 Positive Response를 전달했을 경우에 좋은 결과를 보인다고 합니다.​Experiments and discussionImplementationGFLV1 Detector 기반Resnet - 50 ( Backbone ) + Feature Pyramid Network ( Neck )batch_size 8alpha1 = 2, alpha2 = 2 해당 알파 값은 긍정적인 응답에 대한 계수로써 표준편차에 사용됩니다. 어느 정도의 긍정 응답을 가져올지를 설정하게 됩니다. 하지만 어떤 값을 설정하든 크게 차이가 나지 않는 다는 것을 저자들은 강조하며 ERS가 하이퍼 파라미터에 많은 영향을 받지 않는 다는 것을 서술합니다. DiscussionFCOS 모델로 추가 사용해본 결과 ERD, ERS 방법이 잘 적용되었다고 합니다. FCOS에서는 LD Loss 대신해서 GIOU를 사용해야 한다고 합니다.또 다른 모델에서도 약간의 헤드에 맞게 기법을 조절해주면 문제없이 사용이 가능했고 이는 곧 일반화가 잘되었음을 의미하므로 어떠한 모델에서든 괜찮은 성능을 보인다고 합니다. ​Conclusion저자들은 긍정적인 Response를 ERS라는 전략을 사용해서 Classification Regression Head 각각에서 적용하고 선택된 Response들을 통해 만들어지는 정보들을 Teacher → Student로 증류하는 방법인 ERD를 만들어 제시하였습니다.모든 응답에는 부정적인 영향도 있으므로 ERS를 통해 긍정적인 응답만을 가져오는 방법은 모델 개선에 큰 영향을 주었으며, 각각의 Head에 따라 다른 Distillation방법을 적용한 ERD 또한 이전 모델의 지식을 전달하는 것에 큰 도움을 주어 최종적으로 New class가 들어오더라도 이전의 정보를 잃지 않도록 하는 것을 성공적으로 진행하였습니다.또한, 제시한 방법이 특정 모델에서만 동작하는 것이 아닌 다양한 모델에 약간의 변화만 주면 잘 적용하는 것으로 일반화적인 특성까지 검증하였습니다. "
Object Detection - Deep approach ,https://blog.naver.com/hasukmin12/222156501075,20201127,"Detection = Classification + Localization + Multi object​​ Image가 NN를 통해 나온 결과를 마지막 SoftMax를 통해 classification 구현​​​​​ Object detection target label y- localization 예측을 위해서는 위치정보까지 (x,y,h,w) 학습시켜줘야 한다.- 보행자, 차, 오토바이, 배경을 예측하는 모델의 y값은 위와 같다.​- localization 문제를 같이 풀기 위해선 우리는 학습하기 전에 db 만들때 부터 object의 종류와 object의 위치 정보까지 db에 넣어줘야 한다.- 여기서 (x,y,h,w)는 패치(window)의 크기를 나타낸다. (x,y의 좌표 + 박스의 높이와 너비) 이는 localization을 하기 위해 필수적인 정보다. 그렇기에 학습할때 이러한 위치 정보까지 학습시켜줘야 한다.- train을 하고 나면 위의 y처럼 결과가 나와야한다.classification만 한다면 output이 class 4개만 나오면 되는데localization까지 같이 하다보니 class 정보(Pc,c1,c2,c3) 외에도 위치정보(x,y,h,w)까지 출력하게 된다.=> 여기서 Pc는 background와 관련이 있다. 이 Pc는 object가 있다 없다를 알려준다.c1 = pedstrainc2 = carc3 = motorcycle - 이 처럼 Pc의 값이 1로 출력된다면, 이 network안에 object가 존재한다는 뜻이 된다.object가 없으면 0을 출력하고, 이를 background라고 인식하게 된다.- 위의 패치 결과는 c2(=car)의 object를 인식했음을 알려준다.​- 그런데 막상 NN을 돌려보면 pc값이 0,1처럼 discrete한 값을 가지지 않고 0~1사이의 continuous한 값을 가진다.- pc값이 threshold(보통 0.6) 보다 클 때 object가 있다고 판단한다.​ binary classification 문제를 풀 때 cross entropy loss를 사용하면 좋다.- 위의 경우도 classification 부분은 cross entropy loss를 사용한다.- pc,c1,c2,c2는 cross entropy loss를 사용(분류 문제)하고 bx,by,bh,bw는 MES loss(Euclidean distance)를 사용(regression 문제= continuous할때 주로 사용)해준다.​- 위의 공식에서 y는 정답지(GT), y^은 추정치- 그래서 GT와 추정치의 거리가 최소화가 되게도록 loss를 설정해준다.- 만약 Pc값이 0이면, y1와 y^1만 고려한다.(object가 아닌데 굳이 다 계산해줄 필요가 없다)Pc값이 1이라면(object가 있다면) 8개의 loss를 전부 다 계산해준다=> y1은 pc값으로 y1이 1일 때 (객체가 있을 때) 전체 loss를 계산해주고, 0 일 때는 y1만 계산한다.​​​자 이제 예제를 통해 정리해보자면 이러한 접근법(NN을 사용한 접근법, cnn말고 그 전에)을 통해 우리는 학습을 시키게 되는데이러한 접근법의 문제점1. 만약 training과정에서 정면만 바라본 자동차만 학습하면 가로로 세워진 자동차를 제대로 찾아내기 힘들다. (이를 classifier가 잘 학습이 안됬다고 표현한다)2. sliding window를 통해 image의 모든 면을 뜯어볼텐데, 만약 그 window안에 object가 제대로 들어오지 않으면 학습에 지장이 생긴다 => 이는 Image의 scale을 변화시켜 해결 가능 3. 우리가 다양한 scale의 window로 object를 제대로 찾아낼 수 있게 된다면,candidate가 너무 많아진다. 계산량이 엄청나진다.인접한 패치는 결국 거의 비슷한 정보를 가지고 있을텐데 이를 하나하나 독립적으로 다 따로 classifier에 넣어준다는게 매우 비효율적인 방법이다.이러한 방식이 아무래도 좀 운에 따라 성능이 좌지우지 되는면이 없잖아 있다.​​​​​​FC layer를 Conv layer로 전환​- 밑의 그림에서 맨 위의 그림은 VGG같은 classifier의 구조이다.Image를 Conv layer를 통해 feature를 잡아내고 이후 FC(dense) layer를 통해 fully connect 시켜서 mix해준다.- FC layer의 장점은, 가지고 있는 element를 weighted sum해서 output을 낼 때 feature의 모든 요소를 고려해서 결과를 낸다. 단점은, dense layer에 들어오는 입력의 모양이 모두 동일해야한다. 아래 그림에서 5*5*16으로 한번 들어왔다면 모든 input은 이제 앞으로 5*5*16으로 들어와야한다!! (test image도 모두 이 구조여야만 classification이 가능하다. 그렇기에 이미지의 해상도가 달라지거나 scale이 달라지면 학습된 classifier를 사용할 수 없다.)=> 왜냐하면, FC layer가 결국 하는일이 들어오는 input인 x를 wx+1로 선형변환해서 relu 등을 통해 비선형으로 바꿔주는 것인데, 이때 사용된 w 값을 FC layer가 가지고 있는데..이 w의 shape이 input의 shape에 dependency를 가지고 있다(input의 모양에 딱 맞는 w를 가지게 된다.) - layer에 FC layer가 마지막 layer에 포함되면 input 이미지의 크기가 동일해야한다.즉, 다양한 종류의 image를 넣어주지 못한다.- 따라서 input에 상관없는 conv layer(1*1 conv layer)를 Dense layer 대신 사용한다. (only Conv layer만 존재하게 된다)이를 통해 임의의 input size를 받을 수 있지만(사실 굉장히 큰 장점) FC에 비해 parameter수가 적어지기 때문에 representation이 적어진다.- 이를 통해 classification network의 input candiate를 다양한 형태로 넣어줄 수 있다.​​​- 자 이제 dense 대신 conv로 대체를 해 놨다고 가정하면- 14*14*3 이미지를 input으로 잡는 기준으로 네트워크를 만들었을 때16*16*3 이미지를 input으로 넣으면 결과값은 14*14*3의 커널을 통과해 conv된 결과 값 2*2를 가지게 된다.- 16*16*3 이미지가 input으로 들어갔을 때 (0,0)값은 x=0, y=0, w=14, h=14 영역에 대한 분류 결과이고 (0,1)값은 x=2,y=2,w=14,h=14 영역에 대한 분류 결과이다. (마치 conv를 지나간것 같이 나눠지게 된다)-이는 마치 Sliding window를 통과시킨것 같은 결과물을 가지게 된다. 그렇기에 마지막 layer를 conv layer로 한다면 굳이 Sliding window를 사용할 필요가 없다. - 하지만 이 방법은 window의 위치가 정해져있기 때문에 객체가 window에 애매하게 끼인 상황에서 결과가 좋지 않다.  ​- 여기서 말하고자 하는것 : 기존에는 패치를 하나하나 다 뜯어내서 classfier한테 들고가서 이거 맞냐고 확인받았다면, 위의 접근 방식으로 인해 사진 하나만 넣어주면 알아서 계산되어 결과를 내어준다.​deep에서 conv의 역할은 마치 Sliding window를 하는것과 같다. 해당 window 만큼(kernel의 크기만큼) feature를 뽑아내서 vector화 시키고 이를 기반으로 classify 구현https://egg-money.tistory.com/92 [Deep Learning] Convolutional Neural Networks(CNN) 2 - Padding, Strided, RGBWhat is Padding Convolution 전 게시물에서 3x3 짜리 필터를 걸었을 때, 4x4짜리가 나왔다. 어떻게 보면 당연할 결과이다. 이미지의 한 변의 크기를 n, 필터의 한 변의 크기를 f라고 한다면, 출력의 한 변의 크기..egg-money.tistory.com ​** 정리하자면 **- 기존의 Sliding window 방식을 이용한다면, detection 문제를 classification 문제로 바꾸어서 생각할 수 있다. input 이미지에서 여러가지 candidate patch들을 뜯어내서 classifier에게 보여주면, 이를 통해 object가 있는지 + 뭐인지 알아낼 수 있다.하지만 이 방식의 단점은, 너무 많은 candidate를 뽑아내기 때문에 연산량이 많아서 느리다. 물론 되긴 된다.(나름 훌륭한 방법이다 이것도) 그리고 다양한 scale의 object를 handling 하려면 image의 scale을 바꾸던지 patch의 크기를 바꾸던지 해야한다. 또한 임의의 size의 이미지를 input으로 받을 수 없다. 처음 정해놓은 규격만 classify 할 수 있다.(dense layer 때문에)​- 여기서 dense layer만 conv layer로 바꿔주면 상당히 많은 문제가 해결된다.이 네트워크는 input 이미지 하나만 넣어주면 전체 feature를 한번에 다 계산해준다. (sliding window가 필요없다.) 옛날에는 인접한 패치들을 다 뜯어서 보여주다보니 똑같은 정보가 또 들어가고 똑같은 feature가 반복적으로 들어가는 쓸때없는 일이 많았는데, 이 방식을 사용하면 이런 부분을 보완할 수 있다.(패치 재사용)+ 임의의 size 규격의 input도 학습 가능하지만 이 방법은 window의 위치가 정해져있기 때문에 객체가 window에 애매하게 끼인 상황에서 결과가 좋지 않다.​​​​​​​​​위의 방식은 window의 위치가 정해져있기 때문에 객체가 window에 애매하게 끼인 상황에서 결과가 좋지 않다. 이러한 단점을 보완하기 위해 나온게 (object가 patch위에 정확히 안들어 오는 문제를 해결한게) YOLO의 PID​​YOLO- 위에서 언급한대로 패치 안에 object가 걸치거나 삐져나오는 문제를 해결하려는게 YOLO의 핵심 기술- 기본적인 아이디어는 전체 이미지를 grid로 나누고, 각 grid 별로 localization과 classification을 진행- 각각의 grid 마다 y라는 output를 뽑아내는게 basic idea - grid cell 하나 하나를 독립적인 이미지로 보고 object의 위치를 찾는다. (object의 center값을 기준)- pc는 grid의 object center의 포함 여부이고 c2는 차량이다. (정 가운데 grid의 pc = 0)- x,y,w,h는 기준점을 기준으로 grid밖으로 나간다면 음수도 허용한다.- box의 크기(object의 크기)가 grid보다 클 수도 있다. 이 경우 -300 이런식으로 큰 음수도 허용한다. - detection을 판단할 때는 IoU를 사용한다.- IoU는 window와 GT값과 겹치는 범위의 비율이다. (일반적으로 0.5보다 크면 'correct' 잘 찾았다고 판단한다.)- 근데 왜 IoU로 loss로 만들지 않고 Euclidean distance로 loss를 만들었는가?=> IoU는 미분이 안되기 때문에 loss로 사용할 수 없다. (loss와 IoU는 별개이다. loss가 떨어져도 IoU는 그대로일수 있다. 어느정도 correlation은 있지만 항상 비례하진 않는다.)- IoU는 그저 편의를 위해 사용되는 metric  - Object 주변의 candiates들은 높은 pc값을 나타내기 때문에, NMS(Non-max supperession)을 사용하여 처리해준다. (NMS = 제일 점수 좋은것만 남기고 나머지는 제거) - NMS에 대해 좀 더 설명하자면yolo에서 19*19 grid를 사용한다고 가정하면 각 grid별로 [pc,bx,by,bh,bw] 값이 나온다. pc값이 0.6보다 작으면 모두 제거한다.남은 grid중에 max pc값을 선택하고, 겹치면서 pc가 0.6보다 큰 grid들을 제거해준다.다음으로 높은 pc를 선택하고 겹치는 pc를 제거해준다.이 과정을 반복한다. - 하지만 위의 방법을 사용한다면, 만약 한 grid의 두개의 object가 존재한다면 이를 처리할 수 없다.- 그렇기에 이런 경우 anchor box를 사용해서 처리해준다. - 위의 사진은 2개의 anchor box를 가진다고 가정했을때anchor box 1은 가로로 길쭉한 object를 찾아내고anchor box 2는 세로로 길쭉한 object를 찾아낸다.그렇다면 output인 y는 오른쪽과 같이 출력된다.(각 anchor box의 정보가 모두 담겨있다.)- 실제 YOLOv3에서는 anchor box 8개를 사용한다.​- 하지만 이 방법을 사용할지라도.. 모양이 비슷한 object가 한 grid cell에 포함되어 있으면 찾기 어렵다.실제로 YOLO에서 이를 인식 못한다;; (예를 들어 키가 비슷한 사람이 옆에 서있으면 하나만 인식함)- 사람과 차를 분류하는 문제라면 학습시에 각 grid에 대한 값과 사람과 차에 대한 분류, 위치 정보를 학습시켜줘야한다. (anchor box 2개 사용) 3*3*2*8인 이유     :     3*3 image를 2개의 anchor box로 적용 (8은 archor box당 metric)- anchor box가 겹치면 한 grid에 대한 2개의 object에 대해 점수가 낮은 object를 삭제할 수 있으므로(위의 anchor box example 사진에서 사람과 차 중에 Pc가 낮은 놈은 지워버릴수도 있으니) 클래스 별로 NMS를 사용한다.​​​​R-CNN- YOLO가 속도에 초점을 둔다면R-CNN은 정확도에 초점​1. R-CNN - Region-based CNN그림이 들어오면 YOLO처럼 바로 신경망에 넣는게 아니라object가 있을만한 위치를 먼저 찾아내서(옛날 방식을 통해 objectness가 높은 부분을 찾고)candidate를 뽑아내서 네트워크에 밀어 넣어준다.- 옛날에는 patch 하나하나씩 NN에 넣어줬다면, 여기선 region (object가 있는 candidate) 하나하나씩 넣어준다.=> selective search 방법을 사용해서 obejct가 있을 만한 region을 추출한다. (yolo같이 regular한 box를 안잡기 때문에 정확도가 높다.)- 위의 사진을 예를 들어서 노란색 부분을 NN에 넣는다고 하면, 우선 image scale을 정사각형으로 맞춰준다. (뒤에 dense layer가 있어서 고정된 image size만 받을수 있기 때문 ㅠ)이렇게 모든 region candidate들을 정사각형으로 만들어서 NN에 넣어준다.- 그렇게 region이 NN에 들어가면, 이를 1) object다 아니다로 구별해주고, 2) object의 box 위치를 더 정확하게 수정해준다.(옛날 방식의 box치는 정확도가 낮기 때문에)- 4에서는 SVM을 통해 object의 유무를 판단하고 5에서는 bounding box의 위치를 조정해준다.- 얘의 문제는 옛날의 문제점을 고스란히 가지고 있다. 노란색 영역과 초록색 보라색 영역이 겹치는데 이런 경우 노란색을 이미 한번 봤을지라도 초록색 보라색 계산할때 한번 더 계산한다.(feature 재사용 불가)​​​2. Fast R-CNN- 위의 단점을 커버해서, feature 재사용을 하겠다는게 Fast R-CNN - FCN을 사용해서 feature를 뽑아서 저장해둔다.- 예전에는 region을 규격에 맞춰 정사각형으로 NN에 넣었다면여기선 해당 region에 맞는 feature map을 잘라서 NN에 넣어준다.(그림을 넣는게 아니라 이미 계산된 feature map을 넣어주기에 한번만 계산한다.)- classifier에는 같은 feature를 들고 여러번 찾아가겠지만, feature를 매번 반복해서 뽑을 필요가 없기에 계산량을 확실히 줄여준다. ​​​3. Faster R-CNN- 옛날 고전 방식을 통해 objectness를 계산하여 region을 뽑아낸 부분을 네트워크(RPN)를 사용하여 object의 region을 찾아내는 방식이다.  ​​​4. Faster R-CNN + Feature Pyramid Network- feature를 multi scale에서 뽑는다.- object scale 문제를 해결해줌 ​[참고] [CV] Object detection|작성자 Jun "
딥러닝 컴퓨터 비전 완벽 가이드 | 2. Object Detection 주요 구성 요소와 난제 ,https://blog.naver.com/kimsamuel351/222739682730,20220520,"Object Detection의 주요 구성 요소​ 1. 영역 추정 - Region Proposal ​ 2. Detection을 위한 딥러닝 네트웍 구성  - Back Bone : Feature Extraction  - Neck : FPN  - Head : Network Prediction  ​(Classfication, Regression)​ 3. Detection을 구성하는 기타 요소 - IOU - NMS - mAP - Anchor box  Object Detection의 난제​- 여러개의 물체를 Classfication함과 동시에 위치를 찾아야 함(Regression) - 다양한 크기와 유형의 오브젝트- 시간 (성능과 시간의 반비례 관계)- 명확하지 않은 이미지, 배경- 데이터 세트의 부족  Image Resolution, FPS, Detection 성능 상관 관계 * FPS : Frame Per Second "
"딥러닝 컴퓨터 비전 완벽 가이드 | 4.  Object Detection 성능 평가 Metric (IoU, NMS, mAP) ",https://blog.naver.com/kimsamuel351/222739770904,20220520,"IoU (Intersection over Union)- 모델이 예측한 결과와 실측(Ground Truth) Box가 얼마나 정확하게 겹치는가를 나타내는 지표  NMS (Non Max Suppression)- Detected된 Object의 Bounding box중에 비슷한 위치에 있는 box를 제거하고 가장 적합한 box를 선택하는 기법 * Confidence score가 높을수록, IOU Threshold가 낮을수록 많은 Box가 제거됨  mAP (mean Average Precision) - 실제 Object가 Detected된 재현율(Recall)의 변화에 따른 정밀도(Precision)의 값을 평균한 성능 수치 - 각각의 클래스에 대한 AP의 평균​- Precision-Recall Curve, Average Precision 오차행렬 (Confusion Matrix)정밀도 (Precision)재현율 (Recall)TP / (FP + TP)TP / (FN + TP)정밀도가 상대적으로 더 중요한 지표인 경우는 실제 Negative 음성인 데이터 예측을 Positive 양성으로 잘못 판단하게 되면 업무상 큰 영향이 발생하는 경우재현율이 상대적으로 더 중요한 지표인 경우는 실제 Positive 양성인 데이터 예측을 Negative로 잘못 판단하게 되면 업무상 큰 영향이 발생하는 경우스펨 메일 암 진단, 금융사기 판별정밀도 100% =>확실한 기준이 되는 경우만 Positive로 예측재현율 100% =>모든 환자를 Positive로 예측  - Confidence threshold  * Confidence 임계값이 낮을수록 더 많은 예측 Bbox를 만들게 되어 정밀도는 낮아지고 재현율은 높아짐  * Confidence 임계값이 높을수록 예측 bbox를 만드는데 매우 신중하게 되어 정밀도는 높아지고 재현율은 낮이짐  Precision Recall Trade-off, Precision-Recall Curve​AP  AP= 1/11 * (mP(r=0) + mP(r=0.1) + ... + mP(r=1))= 1/11 * (1.0 + 1.0 + 1.0 + 1.0 + 1.0 + 0.6 + 0.6 + 0.57 + 0.57 + 0.5 + 0.5)= 0.758​ ​ "
"3D Object Detection Survey - 4. 3D Object Detection Methods, B Point Cloud-based Methods ",https://blog.naver.com/mikangel/222476467112,20210819,"B. 포인트 클라우드 방식​CNN의 본질은 희박한 상호 작용과 가중치 공유이며, 이 커널은 정규 영역(유클리드 구조에서 중심 픽셀과 그 인접 픽셀로 되어 있는) 에서는 효과적이라고 증명되어 왔습니다. 반면 CNN은 데이터가 불규칙한 영역(예: 소셜 네트워크, 포인트 클라우드 등)으로 표현되는 경우에는 적합하지 않습니다. Li et al. 의 메모에 따르면 포인트 클라우드는 불규칙하고 무질서하기 때문에 이에 직접 컨볼루션을 하면 ""형상 정보의 이탈 및 포인트 정렬에 대한 편차""가 발생합니다.그림 8과 같이 (i)-(iv)에서 집합 F = fi ∈ RF : i = a, b, c, d가 4개의 점 특징을 나타낸다고 가정합니다. (i)를 제외하고 (ii)-(iv)의 각 점은 순서 인덱스, 좌표 및 특성과 연관됩니다.  K = ki ∈ RF라고 하면 i = α, β, γ, δ는 컨볼루션 커널이고, Conv(·, ·)는 가중된 요소별 합계입니다. 전통적으로 불규칙한 도메인에 대해 컨볼루션하는 것은 다음과 같이 나타낼 수 있습니다. fii ≡ fiii는 모양 정보를 무시하는 모든 경우에 적용되는 반면 fiii ≠ fiv는 점 순서에 대한 분산을 나타내는 대부분의 경우에 해당됩니다. 따라서 불규칙 영역에서 특징 학습 방법이 포인트 클라우드 기반 방법의 핵심입니다. 구체적으로, 이러한 방법은 포인트 클라우드로부터의 표현 학습을 기반으로 하는 멀티뷰 기반, 복셀 기반, 포인트 기반 및 포인트 복셀 기반의 4가지 범주로 더 나눌 수 있습니다.​멀티 뷰 기반 방법. ​이 방법은 먼저 희소 포인트 클라우드를 그리드에 조밀한 전면 뷰(알고리즘 3 참조) 또는 조감도(BEV)(알고리즘 2 참조) 표현으로 변환합니다. 이 아이디어는 CNN과 표준 2D 탐지 파이프라인을 활용하기 위해 직관적이고 간단합니다.  정면에서 본 포인트 클라우드 시각화. 3 × W × H 모양의 정면도는 높이, 거리 및 반사 강도를 인코딩합니다. 전면 뷰는 LiDAR 좌표에서 알고리즘 3로 얻을 수 있습니다. 조감도에서 포인트 클라우드 시각화. 높이 맵은 z축의 높이 정보를 인코딩하고, 강도 맵은 각 LiDAR 포인트의 LiDAR 반사 강도를 인코딩하고, 밀도 맵은 각 그리드 셀 내의 통계를 인코딩합니다. 이 맵의 모양은 1×W×H입니다. 3 × W × H 모양의 BEV 맵은 앞서 언급한 3개의 맵을 연결한 것입니다. 이 모든 지도는 LiDAR 좌표에서 알고리즘 2로 얻을 수 있습니다.Li et al. 제안된 VeloFCN[36]은 포인트 클라우드를 전면 뷰 2D 피처 맵으로 변환한 다음 기성 2D 검출기에 의존합니다. 중첩으로 인한 교합 문제를 완화하기 위해 Yang et al. 제안된 PIXOR[37]는 포인트 클라우드를 보다 컴팩트한 2D BEV 표현으로 래스터화합니다. 이산화되면 표준 2D 감지 파이프라인이 적용됩니다. PIXOR는 BEV 관점에서 포인트 클라우드를 변환하여 스케일 모호성이 적고 폐색을 최소화하는 이점을 제공합니다. 반면, 알고리즘 2에 따르면 BEV 맵을 생성할 때 세로축에 대한 상당한 정보가 무시될 수 있습니다. 따라서 보행자, 도로 표지판 및 육교 아래의 물체 등에 대해서는 실현 가능한 선택이 아닐 수 있습니다. 이러한 관점에서 열거된 사례는 특정 높이(알고리즘 2 참조)에서 샘플링한 후 몇 지점에 불과할 수 있기 때문입니다. 분명히 네트워크 피처 추출에 도움이 되지 않습니다.​복셀 기반 방법. ​이 방법은 일반적으로 3D CNN(3D Convolutional Neural Networks)을 통해 3D 감지를 위한 점 특징을 효율적으로 추출하기 위해 불규칙한 포인트 클라우드를 컴팩트한 형태의 체적 표현으로 변환합니다. 복셀 기반 방법은 이산화 중 정보 손실로 인해 세분화된 위치 정확도 저하를비용으로 계산할 경우 효율적이라고 믿어집니다[23, 40].특정 방법을 소개하기 전에 먼저 몇 가지 기본 개념부터 시작해야 합니다. 복셀 기반 방법의 전체 프로세스를 알아야 하기 때문입니다. ​p를 3D 좌표(x, y, z)와 반사 강도 r을 갖는 포인트 클라우드 P의 한 점이라고 합니다. 여기서 ｛P = pi = [xi , yi , zi , ri ] ∈ R4 : i = 1, . .., N｝ 포인트 클라우드 P는 먼저 L × W × H의 공간 해상도로 균등하게 나뉩니다. v = {pi = [xi , yi , zi , ri ] T ∈ R4 : i = 1, ..., t}는 t 점(t ≤ T ) 및 [vL, vW , vH] ∈ R3 을 포함하는 비어 있지 않은 복셀은 복셀의 공간 볼륨이고, P의 각 점 pi의 복셀 인덱스는 여기서  는 floor function을 나타내고, 연관된 인덱스에 따라 특정 포인트 pi가 속하는 복셀을 반복적으로 결정합니다. . 각 복셀 내의 점이 복셀의 점유 T에 도달하면 pi를 직접 버리고 그렇지 않으면 점 pi를 복셀에 할당합니다. 이러한 다대일 매핑 알고리즘(이후 하드 복셀화라고 함)은 SECOND[41], Pointpillars[43] 및 모든 후속 제품에서 널리 채택되며 본질적으로 아래 세 가지 본질적인 제한[42]을 나타냅니다.​1) 주어진 포인트와 복셀이 할당된 용량을 초과하면 폐기되므로 탐지에 유용한 정보가 포기될 수 있습니다. 2) 포인트와 복셀이 확률적으로 삭제되는 비결정적 복셀 임베딩은 감지 모델의 지터를 유발할 수 있습니다. 3) 제로 패딩을 사용하지 않으면 불필요한 계산이 다소 낭비됩니다. ​단순화를 위해 SA-SSD[40]는 각 pi를 0이 아닌 항목으로 간주하여 각 포인트 pi를 텐서 인덱스로 직접 양자화합니다. 여러 포인트의 공유 인덱스는 최신 포인트로 덮어씁니다. 최근에 Zhou et al. [42]는 이전의 한계를 극복하기 위해 새로운 동적 복셀화를 제안했으며, 하드 복셀화와 동적 복셀화의 차이점은 아래 그림에 나와 있습니다. 하드 복셀화와 동적 복셀화 비교 [42]. 구체적으로, 3D 공간이 v1, v2, v3, v4로 인덱싱된 4개의 복셀로 균등하게 분할되었다고 가정합니다. 하드 복셀화는 메모리 비용 절감을 위해 전체 포인트 클라우드에 대해 복셀 T의 점유를 5로 설정하고 최대 유지 복셀 K를 3으로 설정하면 v1에서 3포인트를 무작위로 드롭하고 15f 메모리 사용량으로 v2를 폐기할 수 있습니다. 반면, 동적 복셀화는 메모리 사용량 15f로 복셀화하는 동안 모든 포인트를 유지하여 불안정한 복셀 임베딩을 제거합니다.Voxel-wise 표현은 pointwise feature를 voxel-wise feature로 집계하는 것에 불과합니다. 아래 그림과 같이 현재 3개의 operator가 있습니다.  세 가지 aggregation 연산자를 통한 복셀 방식 표현1) 평균 operator : 3D 좌표와 특정 복셀의 반사 강도가 있는 모든 내부 포인트 피처는 (cx, cy, cz)로 표시된 평균(즉, 중심)에 대해 직접 계산됩니다. (예: SECOND[41]) 2) 무작위 샘플링 : 복셀 내 포인트은 복셀 특징을 대신하여 무작위로 선택됩니다. (예: SA-SSD[40])3) MLP operator : v 내의 point-wise feature pˆi는 PointNet [5] 블록으로 변환되어 다음과 같이 voxel-wise high level semantic feature를 생성합니다. 선택적으로 초기 복셀 표현은 다음과 같이 중심(cx, cy, cz)의 상대 오프셋에 의해 증가될 수 있습니다.v = {pˆi = [xi , yi , zi , ri , xi − cx, yi − cy, zi − ci ]T ∈ R7 : i = 1, ..., t},여기서 G(·)는 각 복셀 내의 포인트 수를 동일하게 유지하기 위해 최대 T 포인트에서 무작위 샘플링을 나타냅니다. MLP(·)는 선형 레이어, BN(Batch Normalization) 레이어 및 ReLU(Rectified Linear Unit) 레이어로 구성된 적층형 다층 퍼셉트론 네트워크를 나타냅니다. 채널을 따라 최대 풀링 연산 max(·)는 모든 내부 포인트별 피쳐를 복셀별 피쳐 f(예: Voxelnet[45], Pointpillar[43], F-ConvNet[46] 등)로 집계합니다.​Zhou et al. 그림 15(a)와 같이 VoxelNet[45]이라는 이름의 종단 간 학습 가능한 네트워크를 제안하는 데 처음으로 앞장섰습니다.  대부분의 이전 작업에서와 같이 수동 피처 엔지니어링 대신 VoxelNet은 (1) 피처 학습 네트워크, (2) 컨볼루션 중간 계층, (3) 지역 제안 네트워크의 세 가지 구성요소로 유익한 피처 표현을 학습합니다.피처 학습 네트워크는 포인트 클라우드를 동일한 간격의 3D 복셀로 나누고 PointNet과 같은 VFE(Voxel Feature Encoding) 레이어를 새로 도입하여 포인트 세트를 각 복셀 내 표면의 모양을 인코딩하는 벡터로 변환합니다. 컨볼루션 중간 레이어는 확장 수용 필드 내에서 복셀 방식의 피처를 집계하여 보다 상황에 맞는 모양 설명을 도입합니다. 마지막으로 RPN(Region Proposal Network)은 3D CNN 피쳐 볼륨을 입력으로 받아 고무적인 3D 감지 결과를 출력합니다. VoxelNet은 3D 객체 감지에서 중요한 작업이지만 단점은 다음과 같습니다. 3D 컨볼루션 네트워크의 3차 계산 복잡성은 컴퓨팅 플랫폼에 메모리 사용량과 효율성 부담을 증가시킵니다. ​나중에 Yan et al에 제안된 SECOND[41]는 희소 합성곱 연산을 통해 메모리 소비를 줄이고 계산 속도를 가속화하기 위해 나오게 되었습니다. 아래 그림 15(b)와 같이 먼저 원본 희소 데이터를 해당 좌표가 기록된 순서대로 직접 수집한 다음 GEMM(General Matrix Multiplication) 알고리즘을 수행하여 수집된 데이터에 대해 컨벌루션을 수행한 다음 데이터를 분산시킵니다.  희소 컨볼루션 연산은 전통적인 3D 컨볼루션처럼 모든 복셀에 대해 수행하는 대신 비어 있지 않은 복셀에 대해서만 컨볼루션하기 위해 포인트 클라우드의 희소성을 활용합니다. SECOND는 VoxelNet[45]에 비해 제로 패딩 복셀을 사용하지 않아 낭비되는 불필요한 계산을 제거하기 위해 희소 컨볼루션 연산을 활용하지만 값비싼 3D 컨볼루션이 남아 있어 추가 속도 향상을 방해합니다. 그 후 이 병목 현상을 제거하기 위해 Pointpillars가 제안되었습니다. ​H. Lang et al.이 제안한 PointPillars[43]는 표준 2D 컨볼루션 탐지 파이프라인의 이점을 활용하기 위해 본질적으로 복셀의 특수 파티션인 수직 열, 즉 기둥으로 포인트 클라우드를 인코딩하는 PointPillars를 제안했습니다. PointPillars는 VoxelNet과 같이 (1) 피처 인코더 네트워크, (2) 2D 컨볼루션 백본 및 (3) 감지 헤더의 세 가지 주요 구성 요소로 구성됩니다. 피처 인코더 네트워크는 수직 기둥에 포인트 클라우드를 구성한 다음 기둥을 원래 위치로 다시 분산시켜 BEV 관점에서 의사 이미지를 생성합니다. 2D 컨볼루션 백본은 두 개의 하위 네트워크로 구성됩니다. 하나는 의사 이미지를 다운샘플링하고 다른 하나는 다운샘플링된 피처를 업샘플링하고 연결합니다. 감지 헤더는 2D 대응 헤더와 동일한 3D 상자를 회귀합니다. Pointpillars는 VoxelNet[45]에서 사용되는 기존 3D 컨볼루션을 제거하여 62FPS1에서 2-4배의 런타임 개선을 달성했지만 기둥 파티션에서 열악한 정보 인식 문제를 겪고 있습니다. 결과적으로 후속 문헌은 효율성과 정확도 사이의 균형을 고려하여 포인트 클라우드를 기둥 대신 복셀로 인코딩하는 경향이 있습니다[44].​Shiet al.가  제안한 Part-A2 [24]. 개체 내 부품 위치가 3D 경계 상자 주석에 의해 폐색 없이 정확하게 제공될 수 있다는 관찰에 동기를 받아 부품 인식 단계와 부품 집계 단계의 두 단계가 섬세하게 설계되었습니다. 특히, Part-A2는 UNet [72] 아키텍처를 활용하여 전경 점 분할 및 부품 예측을 위해 비어 있지 않은 복셀을 컨벌루션 및 디컨볼루션합니다. 동시에 추가 RPN 헤더를 통해 거친 3D 제안이 생성됩니다. 부품 집계 단계에서는 3D 경계 상자의 모호성을 제거하고 3D 제안 내 점의 공간적 상관 관계를 학습하기 위해 미묘한 RoI-grid 풀링 모듈이 제공됩니다. 마지막으로, 3D sparse convolution[41]은 스코어링 및 정제 위치를 위한 부품 정보를 집계하기 위해 적용됩니다.​Yeet al.에 의해 제안된 HVNet[47]은 3가지 구성 요소로 구성됩니다. 1) 다중 스케일 복셀화 및 특징 추출 2) 다중 스케일 특징 융합 및 동적 특징 투영 3) 탐지 헤더. ​특히, HVNet은 먼저 서로 다른 축척에서 포인트 클라우드를 복셀화한 다음 AVFE(Attentive Voxel Feature Encoder)에 의존하여 복셀 내 각 포인트의 정보를 집계하여 각 스케일에 대해 voxel-wize 피처를 계산합니다. 그런 다음 이러한 높은 수준의 의미론적 특징은 원래 위치로 다시 흩어져 해당 인덱스 레코드에 따라 의사 이미지를 형성합니다. 마지막으로 HVNet은 FPN을 백본으로 사용하여 최종 인스턴스를 예측합니다. 이러한 전략을 통해 HVNet은 31FPS의 실시간 추론 속도로 KITTI 벤치마크의 Cyclist 측면에서 기존의 모든 LiDAR 기반 1단계 방법을 능가할 수 있습니다. HVNet이 Cyclist 카테고리와 관련된 KITTI 테스트 서버에서 1위를 차지했지만 자동차 리더보드의 최신 LiDAR 기반 방법과는 여전히 거리가 멀습니다. 자동차 카테고리가 전체 KITTI 데이터 세트[19, 51]를 지배한다는 점을 감안할 때 자동차 카테고리에서 비교하는 것이 더 설득력이 있을 것입니다.​포인트 기반 방법. ​이 방법은 일반적으로 PointNet(++) 및 그 변형 또는 GNN(그래프 신경망)의 두 가지 백본 유형을 활용하는 원시 포인트 클라우드를 사용합니다. 일반적으로 원래 포인트 클라우드의 형상을 최대한 유지합니다. 그럼에도 불구하고 3D 공간에서 포인트 검색은 체적 그리드에 비해 효율적인 하드웨어 구현에 적대적입니다[73].2017년에 Qi et al. 포인트 세트에 대한 학습의 선구자인 PointNet[5]을 제안했습니다. PointNet을 사용하면 네트워크에서 원시 포인트 클라우드를 직접 사용하고 포인트 클라우드를 체적 그리드 또는 기타 형식으로 변환하지 않고도 분류 및 세분화를 위해 포인트 클라우드에서 3D 표현을 학습할 수 있습니다. 13에서 보는 바와 같이 개선된 버전인 PointNet(++)[74]은 PointNet을 계층적 방식으로 재귀적으로 적용한다.  PointNet(++)[74]은 PointNet[5]을 계층적 방식으로 재귀적으로 적용합니다. 특히, MSG(Multi-Scale Grouping) 및 MRG(Multi-Resolution Grouping)와 같은 밀도 적응 계층을 통해 PointNet 및 그 변종은 포인트 클라우드의 로컬 구조와 세분화된 패턴을 적응적으로 캡처할 수 있습니다.특히 MSG(Multi-Scale Grouping) 및 MRG(Multi-Resolution Grouping)와 같은 밀도 적응 계층을 통해 PointNet 및 그 변종은 포인트 클라우드의 로컬 구조와 세분화된 패턴을 적응적으로 캡처할 수 있습니다. 나중에 복잡한 장면에 대한 일반화 가능성으로 인해 PointNet을 기반으로 하는 3D 객체 감지를 위해 일련의 포인트 기반 방법이 등장했습니다. 따라서 포인트 기반 방법(예: PointRCNN[22], 3DSSD[39])은 PointNet(++)[5, 74] 및 그 변형에 의해 구동되어 원시 포인트 클라우드에서 식별 기능을 직접 추출합니다. 이 패러다임에서 일반적으로 SA(Set Abstraction) 레이어는 다운샘플링 지점에 사용되며 FP(Feature Propagation) 레이어는 업샘플링을 통해 전체 장면에 피처를 브로드캐스트하는 데 적용됩니다. 그런 다음 3D 지역 제안 네트워크를 활용하여 최종 단계에서 추가 개선을 위해 각 지점을 중심으로 고품질 제안을 생성합니다. 이러한 방법은 복셀 기반 방법에 비해 더 높은 계산 비용의 대가로 스택된 SA 모듈에 의해 유연한 수용 필드를 얻을 수 있습니다.​Shi et al. 이 제안한 PointRCNN은 전형적인 포인트 기반 2단계 탐지 프레임워크로, 고전적인 2D 탐지기인 Faster RCNN[13]의 아이디어를 3D 탐지 작업에 우아하게 이식하여 포인트 클라우드를 입력으로만 사용합니다.  (1) 포인트 클라우드의 3D 객체 인스턴스 이후 주석이 달린 3D 경계 상자로 잘 구분되어 있는 PointRCNN은 PointNet(++) 아키텍처를 통해 전체 장면에서 의미론적 분할을 직접 수행하여 전경 지점을 달성하고 상향식 방식으로 고품질 3D 제안 세트를 생성합니다. (2) PointRCNN은 3D 관심 영역 풀링 작업을 활용하여 포인트를 풀링하고 추가 상자 세분화 및 신뢰도 예측을 위해 각 제안의 상응하는 의미론적 기능을 활용합니다. PointRCNN의 한계는 추론 시간 이다, 2단계의 백본 인, PointNet(++) 및 개선 모듈 모두 시간이 많이 걸립니다. ​Yang et al. 제안된 3DSSD[39]. 3DSSD의 주요 기여는 다음과 같습니다. (1) 유클리드 메트릭(3DSSD [39]에서 D-FPS* 참조)을 융합하여 포인트 기반 방법의 속도의 주요 병목으로 간주되는 FP 계층을 안전하게 제거합니다. 다운샘플링 중 서로 다른 전경 인스턴스의 내부 포인트 손실을 보충하기 위해 알고리즘 1에 지정된 FPS*(Furthest Point Sampling)2를 수행할 때 메트릭(3DSSD[39]에서 F-FPS* 참조)을 함께 제공합니다. (2) 메모리 소비를 줄이고 정확도를 더욱 높이기 위해 앵커가 없는 회귀 헤더가 개발되었습니다. 3DSSD는 25FPS 이상, PoinRCNN보다 2배, 약 13FPS 빨라 실시간 시스템에서 널리 활용될 수 있습니다.​다른 중요한 논문 [75–79]은 포인트 클라우드에서 분류 및 의미론적 세분화를 위해 GNN을 활용하는 것을 조사했습니다. GNN은 NonEuclidean 데이터(예: Point Cloud)에 대해 강력한 추론 능력을 가지고 있기 때문입니다[80]. 포인트 클라우드 기반 3D 객체 감지는 그래프 기반 방법이 향후 몇 년 동안 큰 영향을 미칠 준비가 되어 있는 또 다른 영역입니다. 그럼에도 불구하고 현재 3D 객체 감지에 GNN을 사용하는 것은 여전히 ​​많이 연구되지 않았습니다[81, 82]. Shiet al. 제안된 Point-GNN[25]은 양자화 없이 포인트 클라우드의 불규칙성을 보존하기 위해 GNN을 사용합니다. PointGNN은 그래프 신경망을 활용하여 포인트 클라우드 내의 각 포인트를 그래프 정점으로 간주하여 포인트 클라우드를 인코딩합니다. 주어진 절단 거리 내에 있는 점을 연결하여 그래프 모서리를 형성합니다. Point-GNN은 (1) 다운샘플링된 포인트 클라우드에서 그래프 구성, (2) 카테고리 및 지역화를 감지하기 위해 이웃 간에 메시지가 흐를 수 있도록 각 정점 업데이트, (3) 여러 정점의 3D 경계 상자 병합 . Point-GNN이 3D 객체 감지를 위한 새로운 스트림 라인을 개척했지만 그 도전은 까다롭습니다. 그래프를 구성하고 반복 내에서 추론하는 시간은 일반적으로 쓸 수 없으며 논문에서 주장하는 모델을 완전히 훈련하는 데 거의 일주일이 걸립니다. 포인트 기반 방법은 번역 변동에 민감합니다. 딜레마를 완화하기 위해 Point-GNN은 중심 정점의 구조적 특징을 통해 이웃의 상대 좌표에 추가되는 정렬 오프셋을 예측하는 자동 등록 메커니즘을 제안했습니다. 3DSSD는 내부 지점과 인스턴스 내 해당 중심 간의 상대적 위치에 의해 감독되는 전경 지점의 이동을 예측합니다.​포인트 복셀 기반 방법. ​포인트 복셀 기반 방법은 포인트 클라우드에서 표현을 학습하는 새로운 추세를 나타냅니다. 2019년에는 Liu et al. 제안된 PVConv[73], PVConv는 복셀과 포인트의 장점을 융합합니다. 한편으로, 복셀 기반 방법은 복셀의 매개변수에 취약합니다. 예를 들어, 저해상도는 거친 위치 정확도를 초래하는 반면, 고해상도는 입방체 계산 비용을 증가시킵니다. 반면에, 포인트 기반 방법은 추상화 또는 PointNetlike 블록을 설정하도록 선택하여 포인트 클라우드의 불규칙성과 지역성을 쉽게 보존하여 세분화된 이웃 정보를 제공할 수 있습니다. ​실제로 이러한 통합은 실제로 여러 문헌[23, 38, 40, 63]에 의해 효과적임이 입증되었습니다. Chen et al. 제안된 Fast Point R-CNN[63]은 각각 체적 표현과 원시 조밀 좌표의 이점을 제공합니다. VoxelRPN이라는 첫 번째 단계에서는 전체 장면을 상향식 방식으로 일반 그리드로 복셀화하여 고품질 제안의 작은 집합을 생성합니다. 초기 예측을 감안할 때 경량 PointNet인 RefinerNet은 주의 메커니즘을 통해 내부 포인트와 해당 컨볼루션 피처을 효과적으로 융합하고 첫 번째 단계의 현지화 정보 손실을 보완하도록 밀접하게 설계되었습니다. Fast Point R-CNN은 15FPS로 실행됩니다. Yang et al. 그림 15(d)와 같이 제안된 STD[38]. STD의 파이프라인은 PointRCNN과 매우 유사합니다. STD의 주요 혁신은 헤딩 각도에 관계없이 직사각형 앵커보다 높은 재현율을 달성하기 위해 더 일반적인 구형 앵커입니다. STD는 특히 하드 세트에서 뛰어난 성능을 가지고 있습니다. Shiet al. 제안된 PV-RCNN[23]은 그림 15(e)와 같이 3D Sparse Convolution[41]의 효율성과 PointNet 기반 집합 추상화의 유연한 수용 필드를 깊이 통합하여 보다 차별적인 포인트 클라우드 피처를 학습합니다. 특히 PV-RCNN은 SECOND[41]와 같이 전체 장면을 인코딩하는 백본으로 3D sparse convolution을 사용합니다. 그런 다음 두 가지 혁신적인 작업인 복셀-키포인트 장면 인코딩 및 키-포인트-그리드 RoI 피처 추상화가 계산 비용 절감 및 현지화 개선을 위해 적용됩니다. 특히 VSA(Voxel Set Abstraction) 모듈을 채택하여 3D CNN의 다중 스케일 시맨틱 복셀별 피처를 키포인트 피처로 집계합니다. 키포인트는 원래 포인트 클라우드에서 FPS* 알고리즘 1을 통해 선택됩니다. PV-RCNN은 2020년 7월 10일 현재 Car 3D 감지 리더보드에서 1위를 하고 2위를 크게 앞질렀습니다.​​ ​​​​​#3D #LiDAR #ObjectDetection #ROS​ "
Object Detection Algorithm ,https://blog.naver.com/gn03237/222624754583,20220118,"인공지능 기반 기술 중 하나인 딥러닝텍스트 번역이나 이미지 분류 등을 할 수 있음​객체 탐지 Object Detection이미지에서 관심 객체를 배경과 구분해 식별하는 자동화 기법컴퓨터 비전 기술 중 하나​객체 탐지를 할 때 경계박스(Bouncing Box)를 설정해서 객체를 나타냄​CNN Convolutional Neural Network컴퓨터 비전의 핵심 기술제한된 객체 탐지에 대한 회귀와 분류가 가능- Image Recognition-Key Points Detection-Semantic Segmentation기존 컴퓨터 비전 업무와 마찬가지로 정해진 수의 대상을 처리​진정한 객체 탐지 기술은 제한되지 않고 N개의 객체를 탐지해 분류할 수 있어야 함CNN의 한계가 바로 이것다수의 사각형 상자 위치와 크기를 가정해 컨볼루션 신경망을 변형한 후 객체 분류에 활용할 수 있음이러한 사각형 상자들을 윈도우 Windows라 부름윈도우 가설은 이미지 상의 가능한 모든 위치와 크기를 포함해야 한다.​윈도우를 활용해 객체를 찾는 효율적인 방법은?​첫번째 객체 탐지 알고리즘: 영역 제안 Region Proposal객체를 포함할 가능성이 높은 영역을 선택적 탐색 Selective Search같은 컴퓨터 비전 기술을 활용하거나 딥러닝 기반의 영역 제안 네트워크 RPN Region Proposal Network를 통해 선택하는 것이 카테고리에는 Faster R-CNN, R_RCN, FPN-FRCN과 같은 알고리즘이 포함된다.Two-Stage Methods : 높은 정확도 but 단일 단계 방식보다 느린 처리 속도​두번째 객체 탐지 알고리즘: 정해진 위치와 정해진 크기의 객체만 찾는 것여기서는 위치와 크기가 대부분의 시나리오에 적용할 수 있도록 전략적으로 선택됨이 카테고리의 알고리즘은 보통 원본 이미지를 고정된 사이즈 그리드 영역으로 나눔각 영역에 대해 형태와 크기가 미리 결정된 객체의 고정 개수를 예측단일 단계 방식 One(Single)-Stage Methods라고도 불림YOLO, SSD, RetinaNet과 같은 알고리즘이 있음정확도는 떨어지지만 빠른 처리 속도빠른 속도가 중요한 실시간 탐지와 같은 기능을 요구하는 애플리케이션에 활용됨​referencehttps://blogs.sas.com/content/saskorea/2018/12/21/%EB%94%A5%EB%9F%AC%EB%8B%9D%EC%9D%84-%ED%99%9C%EC%9A%A9%ED%95%9C-%EA%B0%9D%EC%B2%B4-%ED%83%90%EC%A7%80-%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98-%EC%9D%B4%ED%95%B4%ED%95%98%EA%B8%B0/ 딥러닝을 활용한 객체 탐지 알고리즘 이해하기인공지능의 기반 기술 중 하나인 딥러닝은 눈부신 혁신을 거듭하고 있습니다.blogs.sas.com ​ "
"OpenCV, Object Detection - CascadeClassifier(), Face & Eyes Detection(얼굴 & 눈 검출) ",https://blog.naver.com/handuelly/221833440240,20200302," # CascadeClassifier() - 얼굴 검출 지난 포스팅에서 다룬 Object Detection 알고리즘 중 Cascade를 사용해서 영상 속 얼굴 검출 실습을 해본다.먼저, OpenCV로 얼굴 검출을 위해서는 XML 파일이 필요한데, 아래 파일을 다운받고 코드를 작성해보자.그리고 사용할 이미지는 본인이 원하는 어떤 사진이어도 되지만, XML 파일 이름에서 알 수 있듯이 '얼굴의 전면' 인식이 가능한 사진이어야 한다. 첨부파일haarcascade_frontalface_default.xml파일 다운로드   사용할 원본 이미지   # 4번 라인 : 얼굴 전면을 검출하는 분류기 객체를 생성한다.# 6번 라인 : 사용할 이미지를 불러온다.# 7번 라인 : 원본 이미지에서 얼굴이라고 판단되는 부분의 좌표 값이 저장된다.   # 9~10번 라인 : for loop를 돌면서 얼굴 영역에 사각형을 그리는데, 입력 영상의 컬러/흑백은 상관 없고 내부적으로 알아서 GrayScale로 입력한다. rectangle()의 파라미터는 순서대로 (이미지, 시작점, 끝점, 컬러, 두께)이다.# 12번 라인 : 결과 이미지를 저장한다.   얼굴 검출 결과 이미지 하지만 아래줄 맨 오른쪽 선수의 팔 부분에 얼굴이 아닌데 사각형이 그려진 것을 볼 수 있다.이는 Pattern을 이용한 분류기의 단점이자 한계점이라고 볼 수 있다.또한, 앞서 언급한대로 이 알고리즘은 얼굴 정면만 가능하고 회전하거나 측면은잘 찾아내지 못한다.   # CascadeClassifier() - 눈 검출 이번에는 얼굴과 눈을 같이 검출해보는 코드를 작성해보자.얼굴은 검출하는 패턴화보다 눈을 찾아내는 패턴화가 더 단순하고, 동시에 '패턴이 단순하다'는 것은 모델을 학습시키기 어렵다는 것을 의미하기도 한다.눈 영역 검출을 위한 XML 파일을 다운 받고, 실습 코드를 작성해본다. 첨부파일haarcascade_eye.xml파일 다운로드   사용할 원본 이미지(출처 : https://www.arsenal.com/news/arteta-reacting-saka-mari-full-transcript)   # 1~2번 라인 : 얼굴 전면을 검출하는 분류기 객체와 눈을 검출하는 분류기 객체를 생성한다.# 4번 라인 : 사용할 이미지를 불러온다.# 5번 라인 : 원본 이미지에서 얼굴이라고 판단되는 부분의 좌표 값이 저장된다.# 8번 라인 : 얼굴 영역에 사각형을 그린다.# 10~11번 라인 : 얼굴 영역 안에서 눈 검출하고 좌표 값을 저장한다.# 13~15번 라인 : 눈의 중앙 좌표 지정하고  눈을 그린다.# 17번 라인 : 결과 이미지를 저장한다.   ​전체적인 코드 프로세스는 동일하고, 눈을 검출하는 과정만 추가됐다고 이해하면 된다.여기서도 얼굴과 눈의 좌표에 대한 정보를 확인해보면 아래와 같다.    ​ "
object_detection/protos/*.proto: No such file or directory ,https://blog.naver.com/7804542/221892511560,20200405,"object_detection/protos/flexible_grid_anchor_generator.proto: File not found. object_detection/protos/grid_anchor_generator.proto: File not found. object_detection/protos/multiscale_anchor_generator.proto: File not found. object_detection/protos/ssd_anchor_generator.proto: File not found. models/research/object_detection/protos/anchor_generator.proto:5:1: Import ""object_detection/protos/flexible_grid_anchor_generator.proto"" was not found or had errors. models/research/object_detection/protos/anchor_generator.proto:6:1: Import ""object_detection/protos/grid_anchor_generator.proto"" was not found or had errors. models/research/object_detection/protos/anchor_generator.proto:7:1: Import ""object_detection/protos/multiscale_anchor_generator.proto"" was not found or had errors. models/research/object_detection/protos/anchor_generator.proto:8:1: Import ""object_detection/protos/ssd_anchor_generator.proto"" was not found or had errors. models/research/object_detection/protos/anchor_generator.proto:14:5: ""GridAnchorGenerator"" is not defined. models/research/object_detection/protos/anchor_generator.proto:15:5: ""SsdAnchorGenerator"" is not defined. models/research/object_detection/protos/anchor_generator.proto:16:5: ""MultiscaleAnchorGenerator"" is not defined. models/research/object_detection/protos/anchor_generator.proto:17:5: ""FlexibleGridAnchorGenerator"" is not defined.​​나는 proto 파일을 models/research 위치에서 다시 컴파일해주었다​https://github.com/tensorflow/models/issues/2930 object_detection/protos/*.proto: No such file or directory · Issue #2930 · tensorflow/modelsAs mentioned above the error takes place in while executing the command in the windows cmd prompt. D:/BB/bin/protoc object_detection/protos/*.proto --python_out=. as for the reference in the instal...github.com ​ "
[object_detection] 전동킥보드 Custom Data ,https://blog.naver.com/yhjo029/222643355383,20220209,"오늘은 custom data로 직접 labelling한 후 yolov5모델에 전이학습하여 전동킥보드를 detection 해보도록 하겠습니다.​일단 전동킥보드 data를 가져오기 위해 google에서 전동킥보드 사진을 26개 저장하였습니다.  (데이터는 많으면 많을수록 좋습니다)​​​​​다음으로 labelling 할 툴을 사용해서 정답 bounding box를 구현해보겠습니다.https://supervise.ly/ Supervisely - Web platform for computer vision. Annotation, training and deployFirst available ecosystem to cover all aspects of training data development. Manage, annotate, validate and experiment with your data without coding.supervise.ly 위 사이트에 가셔서 회원가입 후 프로젝트를 만들고 이미지를 업로드 하세요 업로드 한 후 이미지를 클릭해주시면왼쪽에 이미지에 따라 bounding box를 찍을 수 있는 tool이 있습니다.(사각형) 오른쪽 상단에 이미지를 바꿔가며 box를 찍어보세요모든 사진을 완료하셨다면​​​​​다시 전 projects에 들어가셔서 YOLO v5 format으로 다운받으시면 됩니다.​​​​이제 colab을 들어가셔서 다운받은 데이터셋을 업로드 해주세요업로드 하셨다면 아래 코드로 압축을 풀어보겠습니다 !tar -xvf 파일명.tar 그러면 images, labels 폴더와 data_config.yaml 파일이 나옵니다.data폴더를 생성한 후 넣어주세요 ​​​​​이제 yolov5 git을 다운받겠습니다. !git clone https://github.com/ultralytics/yolov5.git 다운 완료되셨다면 다음으로 필요 패키지들을 설치해줄게요 %cd /content/yolov5/!pip install -r requirements.txt ​​​​​이제 저희 Cutsom 데이터를 yolov5 모델에 맞게 바꿔보도록하겠습니다. 먼저 data 폴더에 data_config.yaml 파일을 열어보세요열어보시면 colors, names, nc, train, val을 보실 수 있습니다.colors는 bounding box의 r,g,b 색name은 class labelnc는 class 수train과 val은  이미지 데이터 경로.txt파일입니다.​​​​​이미지 데이터 경로.txt파일(텍스트파일)을 만들어야합니다. %cd /from glob import globimport ostrain_img_list = glob('/content/data/images/train/*')val_img_list = glob('/content/data/images/val/*')print(len(train_img_list)) 먼저 glob함수로 이미지 파일 경로를 각 list로 저장합니다.​​​​​그 후 list를 'train.txt', 'val.txt'의 텍스트 파일이름으로 한줄씩 쓰고 저장하면 됩니다.  data_dir = '/content/data'yaml_dir = os.path.join(data_dir, 'data_config.yaml')with open(os.path.join(data_dir, 'train.txt'), 'w') as f:  f.write('\n'.join(train_img_list)+'\n')with open(os.path.join(data_dir, 'val.txt'), 'w') as f:  f.write('\n'.join(val_img_list)+'\n')   ​​​​​​이제 먼저 data 폴더에 data_config.yaml 파일을 열어서방금 작성한 train.txt와 val.txt경로를 적고 ctrl + s 로 저장해주시면 됩니다. yolo5를 사용할 데이터 세팅은 끝났습니다.​​​​​​이제 train.py를 통해 yolo5 학습을 진행해보겠습니다. %cd /content/yolov5/!python train.py --img 300 --batch 16 --epochs 25 --data /content/data/data_config.yaml --cfg ./models/yolov5s.yaml --weights yolov5s.pt --name vehicle_yolov5s_results train.py 사용 방법은 yolov5 페이지나 혹은 train.py 파일을 열어보시면 나와있습니다.​저는 GPU 할당이 안되기 때문에 이미지 사이즈 300, batch는 16, epochs는 25로 설정하였습니다.data는 data_config.yaml경로, cfg는 yolov5에 small size config파일, weights yolov5에 small size 가중치로 주었습니다.​ 훈련이 완료되면 best.pt로 best 파라미터를 저장합니다.mAP0.5가 0.931로 너무 높게 나왔습니다. (데이터가 적어서..)파라미터가 편향되어 overfitting이 된 것 같습니다...하지만 구현만 해보기 때문에 이대로 진행해보겠습니다. ​​​​​​​마지막으로 val_img_list에 이미지 경로를 통해 전동킥보드를 잘 detecting하는지 확인해보겠습니다. val_img_path = val_img_list[1]!python detect.py --weights /content/yolov5/runs/train/vehicle_yolov5s_results/weights/best.pt --img 300 --conf 0.5 --source ""{val_img_path}""  아래 경로로 저장이 되는 것 같습니다. 결과 8장 정도를 살펴보겠습니다.(bg bounding box는 없다고 봐주세요! 초기설정에 label를 2개로 해서 그렇습니다,,,)​ 8개 중 2개를 제외하고는 잘 인식하는 것을 볼 수 있었습니다.NMS임에도 그리 높지 않은 confidence들이 나오는 것을 보아 데이터를 보충해야 할 것으로 보입니다.​오늘은 구현을 토대로 전체적인 흐름을 포스팅해보았습니다!! "
"object detection, mask rcnn(detectron) ",https://blog.naver.com/kimsjpk/222138416972,20201107,"tensorflow 2.0 사이트를 보면 object detection 내용이 있고  mask rcnn 내용을 소개하고 있다. 이 내용을 편집해서 google colab에서 실행한 내용을 정리해서 적어본다. 참고로 facebook(FAIR)에서 제안을 하였고, faster rcnn에서 image segmentation을 첨가하여 object detection 대회를 우승할 수 있었다 라고 기술해 놓았다.facebook github에서는 detectron이라고 명명하여 내용을 올려두었고 논문에선 mask rcnn이라고 올려두었다.논문의 내용은 https://www.youtube.com/watch?v=RtSZALC9DlU&t=3s 여기를 참고하도록 하자 import osimport pathlibimport matplotlibimport matplotlib.pyplot as pltimport ioimport scipy.miscimport numpy as npfrom six import BytesIOfrom PIL import Image, ImageDraw, ImageFontfrom six.moves.urllib.request import urlopenimport tensorflow as tfimport tensorflow_hub as hubtf.get_logger().setLevel('ERROR')# Clone the tensorflow models repositorydef load_image_into_numpy_array(path):  """"""Load an image from file into a numpy array.  Puts image into numpy array to feed into tensorflow graph.  Note that by convention we put it into a numpy array with shape  (height, width, channels), where channels=3 for RGB.  Args:    path: the file path to the image  Returns:    uint8 numpy array with shape (img_height, img_width, 3)  """"""  image = None  if(path.startswith('http')):    response = urlopen(path)    image_data = response.read()    image_data = BytesIO(image_data)    image = Image.open(image_data)  else:    image_data = tf.io.gfile.GFile(path, 'rb').read()    image = Image.open(BytesIO(image_data))  (im_width, im_height) = image.size  return np.array(image.getdata()).reshape(      (1, im_height, im_width, 3)).astype(np.uint8)ALL_MODELS = {'CenterNet HourGlass104 512x512' : 'https://tfhub.dev/tensorflow/centernet/hourglass_512x512/1','CenterNet HourGlass104 Keypoints 512x512' : 'https://tfhub.dev/tensorflow/centernet/hourglass_512x512_kpts/1','CenterNet HourGlass104 1024x1024' : 'https://tfhub.dev/tensorflow/centernet/hourglass_1024x1024/1','CenterNet HourGlass104 Keypoints 1024x1024' : 'https://tfhub.dev/tensorflow/centernet/hourglass_1024x1024_kpts/1','CenterNet Resnet50 V1 FPN 512x512' : 'https://tfhub.dev/tensorflow/centernet/resnet50v1_fpn_512x512/1','CenterNet Resnet50 V1 FPN Keypoints 512x512' : 'https://tfhub.dev/tensorflow/centernet/resnet50v1_fpn_512x512_kpts/1','CenterNet Resnet101 V1 FPN 512x512' : 'https://tfhub.dev/tensorflow/centernet/resnet101v1_fpn_512x512/1','CenterNet Resnet50 V2 512x512' : 'https://tfhub.dev/tensorflow/centernet/resnet50v2_512x512/1','CenterNet Resnet50 V2 Keypoints 512x512' : 'https://tfhub.dev/tensorflow/centernet/resnet50v2_512x512_kpts/1','EfficientDet D0 512x512' : 'https://tfhub.dev/tensorflow/efficientdet/d0/1','EfficientDet D1 640x640' : 'https://tfhub.dev/tensorflow/efficientdet/d1/1','EfficientDet D2 768x768' : 'https://tfhub.dev/tensorflow/efficientdet/d2/1','EfficientDet D3 896x896' : 'https://tfhub.dev/tensorflow/efficientdet/d3/1','EfficientDet D4 1024x1024' : 'https://tfhub.dev/tensorflow/efficientdet/d4/1','EfficientDet D5 1280x1280' : 'https://tfhub.dev/tensorflow/efficientdet/d5/1','EfficientDet D6 1280x1280' : 'https://tfhub.dev/tensorflow/efficientdet/d6/1','EfficientDet D7 1536x1536' : 'https://tfhub.dev/tensorflow/efficientdet/d7/1','SSD MobileNet v2 320x320' : 'https://tfhub.dev/tensorflow/ssd_mobilenet_v2/2','SSD MobileNet V1 FPN 640x640' : 'https://tfhub.dev/tensorflow/ssd_mobilenet_v1/fpn_640x640/1','SSD MobileNet V2 FPNLite 320x320' : 'https://tfhub.dev/tensorflow/ssd_mobilenet_v2/fpnlite_320x320/1','SSD MobileNet V2 FPNLite 640x640' : 'https://tfhub.dev/tensorflow/ssd_mobilenet_v2/fpnlite_640x640/1','SSD ResNet50 V1 FPN 640x640 (RetinaNet50)' : 'https://tfhub.dev/tensorflow/retinanet/resnet50_v1_fpn_640x640/1','SSD ResNet50 V1 FPN 1024x1024 (RetinaNet50)' : 'https://tfhub.dev/tensorflow/retinanet/resnet50_v1_fpn_1024x1024/1','SSD ResNet101 V1 FPN 640x640 (RetinaNet101)' : 'https://tfhub.dev/tensorflow/retinanet/resnet101_v1_fpn_640x640/1','SSD ResNet101 V1 FPN 1024x1024 (RetinaNet101)' : 'https://tfhub.dev/tensorflow/retinanet/resnet101_v1_fpn_1024x1024/1','SSD ResNet152 V1 FPN 640x640 (RetinaNet152)' : 'https://tfhub.dev/tensorflow/retinanet/resnet152_v1_fpn_640x640/1','SSD ResNet152 V1 FPN 1024x1024 (RetinaNet152)' : 'https://tfhub.dev/tensorflow/retinanet/resnet152_v1_fpn_1024x1024/1','Faster R-CNN ResNet50 V1 640x640' : 'https://tfhub.dev/tensorflow/faster_rcnn/resnet50_v1_640x640/1','Faster R-CNN ResNet50 V1 1024x1024' : 'https://tfhub.dev/tensorflow/faster_rcnn/resnet50_v1_1024x1024/1','Faster R-CNN ResNet50 V1 800x1333' : 'https://tfhub.dev/tensorflow/faster_rcnn/resnet50_v1_800x1333/1','Faster R-CNN ResNet101 V1 640x640' : 'https://tfhub.dev/tensorflow/faster_rcnn/resnet101_v1_640x640/1','Faster R-CNN ResNet101 V1 1024x1024' : 'https://tfhub.dev/tensorflow/faster_rcnn/resnet101_v1_1024x1024/1','Faster R-CNN ResNet101 V1 800x1333' : 'https://tfhub.dev/tensorflow/faster_rcnn/resnet101_v1_800x1333/1','Faster R-CNN ResNet152 V1 640x640' : 'https://tfhub.dev/tensorflow/faster_rcnn/resnet152_v1_640x640/1','Faster R-CNN ResNet152 V1 1024x1024' : 'https://tfhub.dev/tensorflow/faster_rcnn/resnet152_v1_1024x1024/1','Faster R-CNN ResNet152 V1 800x1333' : 'https://tfhub.dev/tensorflow/faster_rcnn/resnet152_v1_800x1333/1','Faster R-CNN Inception ResNet V2 640x640' : 'https://tfhub.dev/tensorflow/faster_rcnn/inception_resnet_v2_640x640/1', 'Faster R-CNN Inception ResNet V2 1024x1024' : 'https://tfhub.dev/tensorflow/faster_rcnn/inception_resnet_v2_1024x1024/1','Mask R-CNN Inception ResNet V2 1024x1024' : 'https://tfhub.dev/tensorflow/mask_rcnn/inception_resnet_v2_1024x1024/1'}IMAGES_FOR_TEST = {  'Beach' : 'models/research/object_detection/test_images/image2.jpg',  'Dogs' : 'models/research/object_detection/test_images/image1.jpg',  # By Heiko Gorski, Source: https://commons.wikimedia.org/wiki/File:Naxos_Taverna.jpg  'Naxos Taverna' : 'https://upload.wikimedia.org/wikipedia/commons/6/60/Naxos_Taverna.jpg',  # Source: https://commons.wikimedia.org/wiki/File:The_Coleoptera_of_the_British_islands_(Plate_125)_(8592917784).jpg  'Beatles' : 'https://upload.wikimedia.org/wikipedia/commons/1/1b/The_Coleoptera_of_the_British_islands_%28Plate_125%29_%288592917784%29.jpg',  # By Américo Toledano, Source: https://commons.wikimedia.org/wiki/File:Biblioteca_Maim%C3%B3nides,_Campus_Universitario_de_Rabanales_007.jpg  'Phones' : 'https://upload.wikimedia.org/wikipedia/commons/thumb/0/0d/Biblioteca_Maim%C3%B3nides%2C_Campus_Universitario_de_Rabanales_007.jpg/1024px-Biblioteca_Maim%C3%B3nides%2C_Campus_Universitario_de_Rabanales_007.jpg',  # Source: https://commons.wikimedia.org/wiki/File:The_smaller_British_birds_(8053836633).jpg  'Birds' : 'https://upload.wikimedia.org/wikipedia/commons/0/09/The_smaller_British_birds_%288053836633%29.jpg',}COCO17_HUMAN_POSE_KEYPOINTS = [(0, 1), (0, 2), (1, 3), (2, 4), (0, 5), (0, 6), (5, 7), (7, 9), (6, 8), (8, 10), (5, 6), (5, 11), (6, 12), (11, 12), (11, 13), (13, 15), (12, 14), (14, 16)]# Clone the tensorflow models repository!git clone --depth 1 https://github.com/tensorflow/models%%bashsudo apt install -y protobuf-compilercd models/research/protoc object_detection/protos/*.proto --python_out=.cp object_detection/packages/tf2/setup.py .python -m pip install .from object_detection.utils import label_map_utilfrom object_detection.utils import visualization_utils as viz_utilsfrom object_detection.utils import ops as utils_ops%matplotlib inlinePATH_TO_LABELS = './models/research/object_detection/data/mscoco_label_map.pbtxt'category_index = label_map_util.create_category_index_from_labelmap(PATH_TO_LABELS, use_display_name=True) model_display_name = 'Mask R-CNN Inception ResNet V2 1024x1024'model_handle = ALL_MODELS[model_display_name]print('Selected model:'+ model_display_name)print('loading model...')hub_model = hub.load(model_handle)print('model loaded!')selected_image = 'Birds' # @param ['Beach', 'Dogs', 'Naxos Taverna', 'Beatles', 'Phones', 'Birds']flip_image_horizontally = False convert_image_to_grayscale = False image_path = IMAGES_FOR_TEST[selected_image]image_np = load_image_into_numpy_array(image_path)# Flip horizontallyif(flip_image_horizontally):  image_np[0] = np.fliplr(image_np[0]).copy()# Convert image to grayscaleif(convert_image_to_grayscale):  image_np[0] = np.tile(    np.mean(image_np[0], 2, keepdims=True), (1, 1, 3)).astype(np.uint8)plt.figure(figsize=(24,32))plt.imshow(image_np[0])plt.show()#image_path = '/gdrive/My Drive/data/hyundai_car.jpg'#image_np = load_image_into_numpy_array(image_path)# running inferenceresults = hub_model(image_np)# different object detection models have additional results# all of them are explained in the documentationresult = {key:value.numpy() for key,value in results.items()}print(result.keys())label_id_offset = 0image_np_with_detections = image_np.copy()# Use keypoints if available in detectionskeypoints, keypoint_scores = None, Noneif 'detection_keypoints' in result:  keypoints = result['detection_keypoints'][0]  keypoint_scores = result['detection_keypoint_scores'][0]viz_utils.visualize_boxes_and_labels_on_image_array(      image_np_with_detections[0],      result['detection_boxes'][0],      (result['detection_classes'][0] + label_id_offset).astype(int),      result['detection_scores'][0],      category_index,      use_normalized_coordinates=True,      max_boxes_to_draw=200,      min_score_thresh=.30,      agnostic_mode=False,      keypoints=keypoints,      keypoint_scores=keypoint_scores,      keypoint_edges=COCO17_HUMAN_POSE_KEYPOINTS)plt.figure(figsize=(24,32))plt.imshow(image_np_with_detections[0])plt.show() 여기서 model_display_name = 'Mask R-CNN Inception ResNet V2 1024x1024'model_handle = ALL_MODELS[model_display_name]을 바꿔주면 사람의 특징점을 추출한 모델을 사용할 수도 있고 그렇지 않은 모델을 사용할 수도 있다. 결과는 특징점 추출의 차이가 있고 모델에 따른 추론 속도 차이가 있다. 그리고 viz_utils.visualize_boxes_and_labels_on_image_array 의 인풋으로 어떤 걸 넣었냐에 따라서 segmetation 결과를 같이 출력해 줄 수 있다. # Handle models with masks:image_np_with_mask = image_np.copy()if 'detection_masks' in result:  # we need to convert np.arrays to tensors  detection_masks = tf.convert_to_tensor(result['detection_masks'][0])  detection_boxes = tf.convert_to_tensor(result['detection_boxes'][0])  # Reframe the the bbox mask to the image size.  detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(            detection_masks, detection_boxes,              image_np.shape[1], image_np.shape[2])  detection_masks_reframed = tf.cast(detection_masks_reframed > 0.5,                                      tf.uint8)  result['detection_masks_reframed'] = detection_masks_reframed.numpy()viz_utils.visualize_boxes_and_labels_on_image_array(      image_np_with_mask[0],      result['detection_boxes'][0],      (result['detection_classes'][0] + label_id_offset).astype(int),      result['detection_scores'][0],      category_index,      use_normalized_coordinates=True,      max_boxes_to_draw=200,      min_score_thresh=.30,      agnostic_mode=False,      instance_masks=result.get('detection_masks_reframed', None),      line_thickness=8)plt.figure(figsize=(24,32))plt.imshow(image_np_with_mask[0])plt.show() 소스코드를 붙여넣기 했는데 실제로 볼때 가독성이 떨어져서 보기에 불편하기 때문에 colab에서 실행할 때에https://github.com/tensorflow/hub/blob/master/examples/colab/tf2_object_detection.ipynb tensorflow/hubA library for transfer learning by reusing parts of TensorFlow models. - tensorflow/hubgithub.com 여기를 먼저 참고한 후 블로그의 글을 보고 조금씩 고쳐보자. 잘 실행될 것이다.나는 시도해보진 않았는데 custom training data를 사용해서 학습할 때에는 https://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/training.html#training-the-model Training Custom Object Detector — TensorFlow 2 Object Detection API tutorial documentationHow to organise your workspace/training files How to prepare/annotate image datasets How to generate tf records from such datasets How to configure a simple training pipeline How to train a model and monitor it’s progress How to export the resulting model and use it to detect objects. Preparing the ...tensorflow-object-detection-api-tutorial.readthedocs.io 이걸 시도해 보도록 하자. 난 전 회사 서버로 실행시켜 본 적 있는데 실행한 뒤 컴퓨터가 느려진 적이 있어서 더 이상실행해 보진 않았다. 덤으로 cache data를 생성하기 때문에 저장 용량이 넉넉하게 필요하다.  "
"Neural Network, Object Detection 알고리즘 정리 ",https://blog.naver.com/ha6kim/222654585760,20220222,"□ CNN   - 일반적 CNN은 Conv Layer와 Fully-Connected Layer로 구성   - Conv Layer : Feature Extracting 수행 (Spatial Information 유지)   - Fully-Connected Layer : Weight들을 일렬로 펼치기 때문에 Spatial Information 소실.     고정된 Dimension으로 앞단 Layer에서 임의크기의 Feature를 받아들일 수 없다   0) Perceptron : 사람의 뇌 동작 모방                      N개 입력들의 linear Combination + Activation function으로 확률값 생성(0~1) - Single-layer perceptron : 간단한 XOR 문제조차 분류 불가 - Multi-layer perceptron : Input – Output layer 사이에 1개 이상의 중간 layer를 추가하여 학습 ‘local minima’ 문제  1) LeNet (‘98) : 최초의 CNN. 필기체 숫자 인식. ’Gradient-based learning applied to document recognition △ 기존 딥러닝 모델(Fully-connected Layer로 구성. 연산량 기하급수적 증가,       역전파시 미분계수 0 수렴 ‘Gradient Vanishing’으로 가중치 갱신 및 학습내용 미반영)     - 필터, Pooling을 통해 Input Data 축소, Feature 추출 – Fully-Connected Layer에 연결(Classification) (1) Conv Layer (2) : 이미지의 Feature를 추출한 Feature map * 32*32 -> 6@28*28 (2) Pooling Layer (2) : Subsampling.(feature 개수는 유지) * 6 @ 14*14 (3) Fully-Connected Layer (1) : Classification을 위해 필요 ※ 가장 기본적인 CNN 구조. 구현이 간단. 작은 데이터셋(MNIST) 사용. 활성함수(tanh)   2) AlexNet (‘12) : ’ImageNet Classification with Deep Convolutional Neural Networks’ - Alex Krizhevsky     - Overlapping Pooling을 사용, 활성함수로 ReLU 적용(Sigmoid x), Local Response Normalization 추가     - GPU 연산을 고려, 두 개의 병렬 네트워크로 구성. 병렬 Layer들은 각각 독립적으로 특징 추출,       필터 학습 8개의 Layer : Convolutional Layer(5) + Fully-Connected Layer(3) (1) Overlapping Pooling Layer 적용 : Stride를 좁혀 Overlapping (소폭 정확도 향상 / 연산량 증가) (2) 활성함수 ReLU 적용 : Epoch 증가시 빠른속도로 정확도 향상. (3) Local Response Normalization 사용 : 활성함수 적용 전에 (4) Dropout 사용 : F-C Layer에 Dropout (빠른 학습, overfitting 감소) ※ 딥러닝에 GPU를 고려해서 디자인된 최초 모델. 깊은 Conv Layer, F-C Layer로 많은 파라미터 수 필요.    3) GoogLeNet (‘14) : Inception.v1 ~ v4. ’Going Deeper with Convolutions’     - Alexnet에 비해 12배 적은 파라미터 사용. 더 깊은 네트워크 구성     - 성능 향상을 위해 큰 네트워크를 Sparse하게 구성 / 모듈내부는 Dense △ GPU 등 하드웨어가 Non-uniform한 연산에 취약 -> Non-uniform Sparse Matrix를 Dense한 부분행렬로 치환하여 처리     - 처음에는 전통적인 Conv Layer 사용 – Inception Module / Auxiliary Layer 사용​ (1) Inception 모듈(청) : 하나의 Input에 대해 여러 종류의 크리를 가진 Filter를 병렬로 적용한뒤 하나로 출력 * 부담이 적은 작은 Layer들을 여러번 적용. 작은 필터(좁은 영역 특징) / 큰 필터(넓은 영역 특징)​ (2) Auxiliary Classifier(노) : Gradient Vanishing 문제 해소.        순전파시 중간중간 Softmax 거친 결과 저장 / 역전파시 일정 확률로 미리 저장해둔 결과 사용 ※ Sparsity, Density를 적절히 적용. 이후 여러 Inception 모델 발표  4) VGG (‘14) : Oxford. GoogLeNet에 비해 간단한 구조 / 높은 성능. 신경망의 깊이가 정확도에 큰 영향     - 입력 : 224*224의 Fixed RGB Image     - Conv Layer (8~16) + F-C Layer (3) (1) 모든 Conv Layer에 3*3 filter 적용 : 최소 크기의 필터 사용(3*3) - 중심 및 상하/좌우 표현 (2) 1*1 Conv Layer 사용 : 의사결정함수에 Non-linearity 부여, 연산량 감소 (3) Max-Pooling Layer 사용 (5) : 고정된 수(5)의 Pooling Layer ※ 신경망 깊이의 유효성 증명. 비효율 문제(학습시간 과다소요) ​ 5) OverFeat(’13) : ‘13 ImageNet Challenge Localization/Classification 영역.     - ’OverFeat : Integrated Recognition, Localization and Detection using Convolutional Networks’     - Conv Layer (6) + F-C Layer 93), Max-Pooling Layer (#1,2,6)     - Normalization 미사용, Not Overlapping Pooling Layer, 1,2번째 Layer에 더 큰 Feature map 구성     - #1~5 Conv Layer에서 Feature Map 추출. #5 Pooling Layer는 ‘Dense’하게 적용.     - 20*20의 Feature Map에, 3*3 Max-Pooling Layer 적용(9장)     - #6~8 F-C Layer에서 9장의 Feature map들을 통합, 1000개 class로 분류 ※ CNN이 localization이나 Detection 같은 다른 작업에도 유용하게 사용 가능 ​ 6) ResNet (‘16) : Microsoft. Classification/Localization/Detection. ’Deep Residual Learning for Image recognition’  △ CNN은 오히려 망이 깊어질수록 성능 저하(Gradient Vanishing Problem, 연산량 과도 증가 등)     - ResNet은 Output에 간단한 수정, Degradation 문제 완화.     - Residual Block 도입 : H(x)를 최소화시키는 대신, Residual function인 F(x) (H(x) - x)를 최소화     - H(x)를 x로 mapping시키는 것이 목표. F(x)의 경우 H(x)=x라는 최적의 목표값이 Pre-conditioning으로 제공     - 최적화 용이. 기존 네트워크 구조를 크게 변경하지 않고 학습 진행. 파라미터 수나 연산량/복잡도 증가 적음     - 깊은 신경망을 구성해도 Vanishing 문제 발생하지 않음       (2~3개 짧은 Layer들로 이루어진 Residual Block마다 부분적 학습)     - 총 34개 Layer. Conv Layer(3*3 Filter 사용), Pooling Layer(2s의 downsampling, F-C Layer 이전 Avg Pooling),  Residual Network(Shortcut 삽입, Zero-padding, 1*1 Conv Layer 적용) ※ Shortcut을 사용한 Identity Mapping으로 Degradation 문제 해결. 비교적 간단한 구조. ※ Clean한 루트인 Shortcut 덕분에 결과갑을 간단히 Residual Unit의 덧셈으로 표현     * Identity Mapping in ResNet(link)   7) R-CNN(2014) :    - 이미지 하나에서 수많은 Proposal 추출 – CNN model – SVM - (Classification) - Regressor – Bounding Box Prediction 1) Region Proposals : 이미지에서 모든 ROI 추출 (ex: Selective search 알고리즘) * ROI (2000) 2) Feature Extraction : Warp(Fixed size로 Resize) - CNN에 각각 통과 (ex: ILSVRC2012 Dataset, Pre-training시킨 AlexNet) * Warping은 데이터 왜곡 가능성 증대. 많은 시간 소요 * feature (4096) 3) SVM : 4096 * (N+1)의 Weight를 가진 SVM에 넣어 Classification 수행 * Feature map dimension : 2000*4096 * 최종 Dimension : 2000*(N+1) 4) Regressor : Rough한 기존의 Bounding Box를 GT의 그것에 일치시키는 방향으로 Regressor를 학습 / 보정 * Bounding Box (x, y, w, h) ※ Limitation : Multistage Algorithm(CNN,SVM,Regressor 따로 학습), High Training Cost(많은 Proposal을 전부 CNN 통과), Slow Testing Speed(실시간 x) ​​ 8) SPPNet(2015) : Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition    - 기존 CNN이 고정크기의 입력을 요구 (Fully Connected Layer)      임의 크기의 입력을 받는 새로운 모델구조 고안 – Classification, Detection Task에 적용.    - Conv 계층의 마지막 단계에 적용된 Pooling Layer를 Spatial Pyramid Pooling Layer로 변환 ※ R-CNN 대비 빠른 속도 향상 (160배) / 이미지에서 최종 Feature Map을 한번 뽑아 여기에 Selective Search 적용  (Conv Layer 병목 회피). ※ SVM을 따로 학습시켜야 함 ​ 9) Fast R-CNN (‘15) : Microsoft Object Detection model. R-CNN과 SPPNet을 편리하게 개선     - 알고리즘 변경, Fine Tuning / Multi-task Loss Function 도입 (End-to-end 모델)     - Feature Extractor -> ROI Pooling -> Classifier & Regressor (1) Feature Extractor : CNN에는 이미지 한 장만 들어가 공통적인 Feature Map 추출 – 각 ROI들은 모델을 통과하며 줄어든 크기의 비율을 따져 좌표만 변경 (ROI projection : Selective Search 적용시 이미지를 잘라내지 않고, 그 좌표/크기정보만을 (r,c,h,w) 꼴로 메모리에 홀딩) (2) ROI Pooling Layer : 1개 Feature Map + ROI 좌표 -> ROI Pooling Layer 통과 -> FCL에 연결 (다양한 크기의 ROI들을 적절히 처리, 고정 크기로 변환) * SPPNet 접근법 차용 : ROI Pooling Layer = 1층짜리 Spatial Pyramid Pooling Layer (3) Classifier & Regressor : FCL은 각각 Classifier(Classification), Regressor(Detection)로 연결되는 병렬 FCL로 연결. 모든 Task를 한번에 최적화시키는 Multi-task Loss function 고안      - Lcls(classification을 위한 softmax func), Lloc(localization을 위한 L1 loss func) * L1 : 덜 민감한 함수로 Fine Tuning 용이 ※ R-CNN, SPPNet 대비 Training Time (18.3배), Testing time(213배) ​ 10) Faster R-CNN(‘) : 새로운 Region Proposal Networks(RPN) 도입 ○ RPN을 딥러닝 모델 중간에 이식, GPU를 활용한 속도 향상 ○ fast R-CNN의 selective search가 RPN으로 대체 ○ Feature Map 위에 N*N Conv Layer를 Sliding Window 식으로 옮겨가며 적용 -> 연산마다 k개의 Anchor (1) Anchor : 9개 Anchor 사용(3종 Ratio * 3종 Scale) /                   한 지점에 대해 다양한 크기와 비율의 Object 확인, 이미지 하나당 W*H*k개의 Proposal (2) Translation-Invariant Anchors : 물체 이동여부와 상관없이 똑같은 Region 제안 (3) Loss Function  □ YOLO   - 상업적 활용. 학문적 가치ㆍ유용성 낮음. 작은 물체들이 모여있는 이미지 검출 제한(Grid마다 1개 검출)   - 속도가 R-CNN 계열에 비해 빠름 / 준수한 정밀도(45~155fps / 52~63 mAP) "
Yolov3-Object Detection + BoundingBox ,https://blog.naver.com/polpolie95/221816973461,20200220,"#Yolov3 #Yolo #Windows #ObjectDetection #CPU #GPU​안녕하세요, 오늘은 yolov3에 대해 설명하겠습니다. Object Detection 알고리즘 Yolov3간단하게 말하면 사진 혹은 영상에 있는 물체를 찾아내는 Object Detection 알고리즘이라고 볼 수 있습니다.​그러면 어떤 이미지에 어떤 사물이 있는지 알 수 있겠죠?​CNN과 작동 원리는 비슷합니다. 용어를 살펴보자Joseph Redmon, et al., “YOLOv3: An Incremental Improvement”Yolov3 공식 홈페이지(https://pjreddie.com/darknet/yolo/)를 참고하면서 용어를 적어보았습니다. GT(Ground truth)사물의 실제 위치를 나타내며, 바운딩 박스 정보가 이미지 레이블 상에 포함되어 있음IoU(intersection over union)각 예측 바운딩 박스(Bp)와 GT 바운딩 박스(Bgt)에 대해 Bp와 Bgt가 서로 얼마나 '겹치는지' 평가Bp와 Bgt의 IoU = Bp ∩ Bgt 영역 넓이 / Bp ∪ Bgt 영역 넓이Confidence score해당 모델이 해당 자신 box안에 object가 있을 확률이 얼마나 되는지,그리고 해당 object가 예측한 object와 맞을 확률이 얼마나 되는지에 대한 scoremAP(mean Average Precision)여러개 Precision에서의 평균에서 평균값을 구한 것Precision참으로 판단한 것 중에 실제로 참인 것의 비율batch한번에 처리할 이미지의 개수(1iteration 당 처리할 이미지의 수)subdivision(mini-batch)batch를 몇 개의 단위로 분할해 GPU로 보내 프로세스를 진행하는 것(ex. batch = 64, subdivision = 8 / 64개의 batch를 8개의 그룹으로 나누어 순차적으로 보냄)iteration & epochbatch사이즈가 64고 학습할 이미지가 128개라면 두번의 iteration(반복)을 해야하며,1세대의 epoch이 만들어진다 좀 더 구체적으로 접근해보겠습니다.​    네모난 테두리를 바운딩박스라고 부릅니다. ​빨간색 박스를 정답박스 녹색 박스를 사용자가 정의한 detection 알고리즘의 예측한 값이라 하면, ​두 박스의 교집합/두 박스 합집합의 면적 = IoU라고 합니다.​이러한 IoU 수치가 기준 몇프로가 되면 이것을 맞는 바운딩 박스라고 인정해줍니다.   IoU 수치를 사용할때 mAP라는 개념을 사용하는데 위 그래프를 보시면 ​x축 Recall 그래프, y축 Precision 그래프로 표현이 되어 있습니다.​해석하자면 만약 y축 Precision이 0.9라면,​ 99%확률로 물체가 바운딩박스 안에 있는 것을 말합니다. ​x축은 물체 갯수라고 생각하시면 됩니다.​즉, Precision이 많아질수록 Recall은 감소하게되는 Trade-off 관계를 갖으며 그래프  아래의 면적을 AP라고 합니다.​이것을 클래스 별로 따로따로 구해서 클래스간 평균을 낸 것을 mAP라고 합니다.​다시말하면, 물체가 많아질 수록 물체를 인식하는 정확성은 떨어집니다.  바운딩 박스는 어떻게 예측할까?   Bounding Box Prediction은​Anchor박스(미리 정의된 바운딩 박스)를 정의해놓고 그 Anchor박스가 ​regression에서 얼마만큼 움직일 건지에 대한 부분을 예측하는것을 의미합니다.​그림을 보면 원래 Anchor박스 가운데 중심이 cx, cy라고 주어지고 ​그때 x, y 방향으로의 이동은 (가로세로방향) 시그모이드를 취해서, 보통 tx값에다가 시그모이드를 취해서 이동을 하게 됩니다.​중심은 A점 으로 왔을 때 B 테두리 안을 벗어나지 못하게 되는데, 그 이유는​1칸의 길이를 1이라고 보고 시그모이드를 할 경우 0에서 1사이의 값이 나오기 때문에 B박스를 벗어나지 못하기 때문입니다.​이 말은 결국 바운딩 박스의 중심(bx, by)은 다른 곳에 갈수 가 없는 것입니다.​bw, bh은 원래anchor박스 width, height에다가 얼만큼 움직이어야 하는지 tw, th의 expedition 만큼 움직이게 되어있습니다.​대략적으로 아 이렇구나라고 이해하시면 좋을 것 같아요 바운딩 박스는 Objectness score(confidence)를 각각 가지고 있습니다.  그림에 나와있듯이 umbrella 99%, person 99%와 같이  몇프로 일치하느냐 라는 것이죠​GT(Ground truth)랑 가장 많이 오버랩되는 것은 confidence값이 1이(100%) 되겠죠.​yolov3에서는 GT에 대해서 내가 예측한 바운딩 박스가 1개씩 할당이 된다고 합니다.(다른 알고리즘은 1 + α 개수 할당됨 - GT랑 IoU가 큰 것 + IoU가 0.7이상인 것들 전부다)​​오늘은 여기까지 포스팅 하겠습니다. ​다음에는 Class Prediction에 대해 설명하도록 하겠습니다.​​ "
ModuleNotFoundError: No module named 'object_detection' ,https://blog.naver.com/role___play/221953640625,20200509,윈도우10 환경에서 TensorFlow object_detection을 사용하려다가 제목과같은 오류가 발생했다.​검색해보니 PYTHON_HOME을 환경변수하라나 뭐라나... 하지만 나는 아래처럼 해결했다.​1. https://github.com/tensorflow/models 에서 프로젝트를 다운로드받는다. tensorflow/modelsModels and examples built with TensorFlow. Contribute to tensorflow/models development by creating an account on GitHub.github.com 2. 압축을 푼 후 models\research 로 들어간다.​3. 주소창에 cmd를 쳐서 해당 경로에서 cmd를 연다.    또는 cmd를 켜고 [cd 경로\models\research] 를 입력한다.​4. python setup.py install 를 입력한다.​설치 완료하면 사용이 가능하다.​하지만 해결 후 오류로 ModuleNotFoundError: No module named 'nets' 가 뜬다...슬슬 짜증이 났지만 또 검색해봤다.이 오류는 아래와같이 해결했다.​1. models\research\slim 로 들어간다.​2. 주소창에 cmd를 쳐서 해당 경로에서 cmd를 연다.    또는 cmd를 켜고 [cd 경로\models\research\slim] 를 입력한다.​3. python setup.py install 를 입력한다.​입력했을 때 간혹 [error: could not create 'build': 파일이 이미 있으므로 만들 수 없습니다] 같은 오류가 뜬다.​이럴경우 slim폴더 내에 생성되어있는  BUILD 파일을 삭제해주고 다시 설치하면 된다. 
Process of Object Detection ,https://blog.naver.com/sesme100/222182496837,20201223,#MicrosoftPowerPlatfrom#MicrsoftPowerApps#MicrosoftPowerAutomate#MicrosoftRPA#CitizenDeveloper#MSRPABeginners​#ObjectDetection#CocaCola #코카콜라#Pepsi #펩시​안녕하세요 Microsoft Citizen Developer 진미나 입니다. 이번엔 AI Builder로 내가 좋아하는 코카콜라와 펩시를 개체감지(Object Detection) 하기위한 AI Modeling과정을 담아보았어요! 코딩 한줄 없이 AI를 Modeling 한다는 점이 너무나 매력적인것 같습니다!​  ​이전 포스팅에서 개체인식을 AI Modeling 하여 Power Apps에 구현한 앱을 사용하는 비디오를 녹화하였습니다. 꼭 들러서 재미나게 봐주세요!!  Object Detection + Power Apps _ Demo Video#MicrosoftPowerPlatfrom#MicrsoftPowerApps#MicrosoftPowerAutomate#MicrosoftRPA#Citizen...blog.naver.com ​​#EdgeofCitizenDeveloper#나도하면너도할수있어#YoucandoitasIdo#LowCoding#마이크로소프트#사랑해~ 
0310 Object Detection ,https://blog.naver.com/smithfrancis313/222271370191,20210310,"Object Detection어제는 Image to semantic segmentation 오늘은 instance segmentation, panoptic segmentationsemantic 보다 더 고도화 된 것은 instance별로 인식가능!차. 로 끝내는게 아니라, 어떤건 세단, 어떤건 suv 등 구분panoptic 은 instance 보다 더 고급기술​object detection 은 곧 classification + box localization 이다.object 에 box 하고 해당 box 의 object 가 어떤 class 인지 구분하는 문제.주로, [class, x_min, y_min, x_max, y_max] (min 은 좌상단 점, max 는 점으로부터의 w,h)​autonomous driving, Optical Character Recognition 등에 주로 활용​two-stage detector / single-stage detectorTraditional OD 를 살펴보자.CVPR 2005사람사진평균내봄 경계선비슷한 사진이 나와서 사람의 경계선을 찾기 위한 노력.HOG, SVM 같은 걸로 해결.(Histogram of oriented gradients, support vector machine)HOG 개념에서는 사진 local area 마다 기울기를 보고 기울기가 비슷하면 사람으로 인식.​selective search다양한 object candidate 에 대해 boundary box proposal.1. 영상을 비슷한 색끼리 분할(over-segmentation)2. 반복적으로 비슷한 색끼리 다시 합침.(iteratively merging similar regions)3. 남은 segmentation 에 대해 box 치기 (extracting candidate boxes from all remaining segmentations)​R-CNNCVRP 2014regions with CNN featuresselective search 등을 사용하여 2k 이하의 region proposals 를 구함,input 으로 쓰기위해 적절한 크기로 warped region (affine transform 을 통해 비율무시 크기 변경을 말하는 듯)pretrained CNN결과로 나오는 dense layer 의 결과를 SVM 등을 사용하여 object detection단점 : object 검사 하나하나하나 하면서 느림, region proposals 은 selective search 를 썼기 때문에 이부분은 더 이상 학습이 안됨.​Fast R-CNNICCV 2015영상으로부터 CONV feature map 추출RoI pooling 을 통해 feature map 으로부터 RoI feature (region of interest) extraction후 fixed dimension 이 되어야 하므로resampling이를 dense layer 로 flatten 하게 만들고, 이걸 다른 dense 를 통해 softmax 나 bounding box regressor 를 사용​Faster R-CNNNeurIPS 2015region proposal 을 selective search 보단 NN 으로 대체드디어 end-to-end NN다.IoU(Intersection over Union) = A&B / A|B 따라서 IoU 가 1일수록 좋은 것, A 와 B 는 좌표상에 있는 areaanchor boxes 를 통한 region proposal각 위치에서 발생할 것 같은 box 들을 미리 정의해놓은 candidate 1:1, 1:2, 2:1 을 크기 별로 각 3개씩 9개 이런식Anchor box, Ground truth 의 IoU 가 0.7 이상이면 positive sample, 0.3 이하면 negative sample이를 Region Proposal Network(RPN) 이라 한다. 즉 time-consuming selective search 에서 바뀐 것원본 -> feature map -> RPN 으로 뽑아낸 anchor box -> 이를 이용한 RoI pooling -> 후에 classificationCONV feature map 에서 sliding window 만큼 보게 되는데 window 마다 k 개의 anchor box 를 고려함.각 window에서 256D 의 feature vector 하나 추출. 2k 의 classification score(is it object or not) 를 뱉어냄.k 개의 anchor box 의 위치를 미세하게 조정하기 위해 regression 하는 4k coordinates 를 뱉어냄. (x_min, y_min, x_max, y_max)k 가 너무 크면 연산이 너무느려져서 rough 한 anchor box 만 만들어서 대충 검사하고, 그 뒤에 window 마다 정교한 regression 을 통한 predict boundary box 를 만들기 위해 이렇게 설계함.classification score 를 뱉는 layer 는 cross entropy 같은 classifier 용 loss 를 사용, regression 은 regression 용 loss 를 사용 요 둘은 결국 RPN 을 위한 것이다. 전체 end-to-end 를 측정하는 loss 는 각 RoI category classification loss 를 추가하여 학습을 한다.문제는 RPN 을 딱 하고나면 중복된 박스들이 어마어마하게 나온다. 이렇게 치나 저렇게 치나 계속 object 가 걸쳐져서 그런듯.filtering, screening 이 필요함. 그것이 NMSNon-Maximum Suppression (NMS)1. objectiveness score 가 가장 높은 box 선택2. 해당 box, 다른 box 로 IoU 계산3. 0.5> 넘으면 삭제4. 다 지워졌으면 두번째로 highest score box 찾아서 반복​이번엔 single stage정확성보단 속도에 중점을 둬서 realtime detectionRegion proposal 을 기본으로 한 RoI pooling 이 없어 구조가 간단, 빠름RPN 로 box regression 후 바로 해당 box 에 대해 classification​YOLO(You Only Look Once)S by S grid 로 나눔, grid 별로 (b 개의 box(x_min, y_min, x_max, y_max) +1 confidence score) + (1 class score)bounding box + condifence 와 class probability map 이 동시에 일어남 -> NMS 을 통해 Boundary box + box 별 class 출력여기서는 Faster R-CNN 에서 썼던 NMS 에 class 를 추가해야할 듯? 겹쳐졌지만 class 가 다르면 다른 것이니..GT 매치되는 anchor box 를 positive 로 간주, 학습 label 을 positive448*448*3 image 를448*448*3 -> (7*7*64-s-2) = 224*224*(64*3=192) ->(2*2-s-2 max) = 112*112*192-> 3*3*256 = 112*112*256= 112*112*192 -> (3*3*256) = 56*56*256 -> (3*3*512) = 28*28*512 -> (3*3*1024) = 14*14*1024-> (3*3*1024) = 7*7*1024를 dense net 하여 4096 -> 7*7*30 으로 변형 참고로 그림이 오타인 것 같다.448*448*3 의 그림을 7*7*64-s-2 CONV 및 2*2-s-2 Maxpool 한 결과가 112*112*192 인데64로 돌렸으면 64가 나와야 한다.또 448->224->112 로 변화되는데 계산해보니까 padding=same(7*7 이므로 3) 이여야 하고, 분명히 소수점으로 나오는데 이를 버리는 것 같다.(W+2P-F)/S + 1 = (W+2P-F)/S + S/S = (W+2P-F+S)/S =  (448+6-7+2)/2 =224.5 = 224​최종적으로 7*7*30 이 나오는데, 왜 w,h 가 7이고 filter 를 30을 써서 만들었을까?grid S = 7 이기 때문, filter dimension 이 5B+C 여야 하기 때문.B 는 아까 b 개의 box 의 B 이고, YOLO 에서는 anchor box 를 2개 썼음. 따라서 B는 2, 5B 인 이유는 box 마다 5개의 params,C 는 class one-hot vector. YOLO 에서는 20개의 classification, 따라서 30확실히 같은 backbone NN 사용시 R-CNN 보다 빠르다!단점 : 맨 마지막 layer 에서만 prediction 하기 때문에 localization 성능이 좀 낮음.​Single Shot MultiBox Detector(SSD)multi-scale object 를 더 잘 처리하기 위해서 중간 feature map 을 해상도에 적절한 boundary box 구조를 생성VGG 가 backbone 이고, conv4_3, conv7, 8_2, 9_2, 10_2, 11_2 딱봐도 중간 layer 들에서 최종 결과를 출력하도록 output layer 쪽까지 중간결과값들이 날아간다. 이를 multi-scaleconv4_3 -> classifier 3*3*4*(classes+4)conv7, conv8_2, conv9_2 -> classifier 3*3*6*(classes+4)conv10_2 -> classifier 3*3*4*(classes+4)conv11_2 는 다음 layer 가 원래 classifier 임(classes+4) 앞의 숫자는 각 layer 에서 나오는 anchor box 의 개수 classes 는 class score, 4 는 (x_min, y_min, x_max, y_max)이 classifier 뒤로 바로 detections : 8732 / class 인 layer 가 붙는,이는 중간 feature map 들의 anchor boxes 의 총합과 같다.38*38*4 + 19*19*6 + 10*10*6 + 5*5*6 + 3*3*4 + 1*1*4 = 8732, W*H*N(anchor boxes for each feature map)이 큰 숫자를 NMS 로 적당히 제거하고 사용.R-CNN 보다 빠르면서 성능도 더 뛰어남!​RetinaNetICCV 2017Feature Pyramid Networks(FPN) Unet 같은 모습ResNet 의 NN 을 통과하면서 나온 feature 들을 복사한 뒤, 최종(제일 작은 feature map) + 중간 + 처음을 더함.최종 feature map -> class+box subnets, 최종+중간 feature map -> class+box subnets, 처음+중간+최종 feature map -> class+box subnets 함.class subnets 과 box subnets 를 dense 로 위치마다 수행하게 됨. 성능도 빠르고 속도도 빨랐다.​focal loss일반적으로 사진에서 object 는 background 보다 area 가 적음. positive sample 보다 negative sample 이 압도적으로 많음. one stage 에서는 background 도 loss 계산을 하면서 neg 만 압도적으로 많은 class imbalance 를 야기함.이를 해결하기 위해 focal loss 가 제안, cross-entropy 의 확장.CE(pt) = -ln(pt) 에서 FL(p_t) = -(1-pt)γ ln(pt), γ 에 따라 function shape 이 바뀜(1-pt)γ 때문에 0으로 가면(오답일 때) CE 보다 loss 가 급격히 상승오답일 경우, derivative 가 훨씬 커짐. detection with transformerDETR OD 를 Transformer 로 해결하려고 노력ViT by Google, Deit, DETR by FacebookCNN feature 와 positional encoding 을 쌍으로 해서 input embedding vector 를 생성 그 후 transformer encoder -> decoder​추가 중... "
AttentionNet(1): single class object detection ,https://blog.naver.com/hojun060534/222669088647,20220310,"본 논문이 발표되었을 당시는 (2015년) single stage detector는 아직 발표되지 않았고 (yolo가 2016년에 발표됨.) two stage detector가 막 발표되기 시작했었을 때이다. 본 논문의 relative work에서 R-CNN 비중이 높은것을 보아 아마 R-CNN의 개선 알고리즘 정로라 생각하면 될 것 같다.​해당 논문은 아래 사이트에서 확인할 수 있다.https://www.cv-foundation.org/openaccess/content_iccv_2015/html/Yoo_AttentionNet_Aggregating_Weak_ICCV_2015_paper.html ICCV 2015 Open Access RepositoryICCV 2015 open access These ICCV 2015 papers are the Open Access versions, provided by the Computer Vision Foundation . Except for the watermark, they are identical to the accepted versions; the final published version of the proceedings is available on IEEE Xplore. This material is presented to ens...www.cv-foundation.org   2015년경 Region-CNN (R-CNN)은 하나의 큰 문제점을 갖고 있었다. 바로 detection performance가 region proposal의 성능에 너무 지나치게 영향을 받아 R-CNN의 성능을 개선하기 위해서는 1. region proposal 알고리즘의 정확도 향상과 2. object classifier의 성능까지 챙겨야한다는 점이다.​ 이때 등장한 AttentionNet은 region proposal 알고리즘과 object classifier가 하나로 합쳐진 일종의 one stage detector의 구조를 지닌 일종의 CNN알고리즘이다. 다만, 논문이 출시되었을 당시에는 이미지상에서 단일 객체 검출에 대한 성능만 확인되었고, 다중 객체 검출에 대한 알고리즘은 신뢰도가 높지 않았는지 논문 초반부에 크게 언급하지 않았다. 물론 chapter. 4에서 다중 객체 검출에 대한 알고리즘을 서술하기는 한다.​AttentionNet의 특징은 바로 itrative하다는 점이다. 다른 CNN알고리즘과는 다르게 AttentionNet은 이미지상에서 bounding box의 크기와 위치를 조정해가며 물체의 위치를 검출해낸다. 논문의 Figure.1에서 알 수 있듯이 bounding box의 크기가 작아지면서 IOU를 증가시킨다. (bounding box와 물체의 경계가 거의 일치해짐)​​<Network Architecture>AttentionNet의 layer구성은 VGG-M network[1]의 구조와 유사하다고 하다. 이때 두번째 convolution layer은 M. D. Zeiler and R. Fergus의 ""Visualizing and understanding convolutional networks""[2]와 유사하다고 한다. 이때 AttentionNet만의 특징은 1회 실행 결과치를 다시 input image size로 resize하여 재실행시킨다는 점이다. 이러한 과정을 반복해 특정 임계치에 도달하면 반복을 멈추고 결과물을 return한다.​AttentionNet은 다음과 같은 구조로 반복을 진행한다. 먼저 AttentionNet에서 bounding box의 위치와 크기는 오로지 왼쪽 위 꼭짓점과 오른쪽 아래 꼭짓점의 이동으로만 조절된다. AttentionNet은 이를 구현하기 위해서 8번째 convolution layer를 두가지 종류로 분리시켰다. 이때 왼쪽 위, 오른쪽 아래 부분을 나타내기 위해 각각의 layer마다 Conv8-TL(Top Left), Conv8-BR(Bottom Right)라는 이름을 붙였다.​각각의 반복 횟수마다 검출된 2개의 layer (Conv8-TL, Conv8-BR)는 이동을 하거나, 정지하거나, False(no instance)를 return한다. 이때 각각의 레이어는 bounding box의 크기를 줄이는 방향으로 밖에 이동을 하지못한다. (방향은 8가지이다.) 즉 왼쪽 위 꼭짓점은 동쪽, 남쪽, 남동쪽으로밖에 이동하지 못하고, 오른쪽 아래 꼭짓점은 서쪽, 북쪽, 북서쪽으로 이동이 가능하다.​AttentionNet이 반복을 멈추는 근거는 다음과 같다. 1. 두 꼭짓점이 모두 False를 return하는 경우, 2. 두 꼭짓점이 모두 정지하는 경우이다. 1의 경우 해당 bounding box안에 instance가 없다는 뜻, 즉 검출하고자하는 객체가 없다는 뜻이기 때문에 bounding box를 소멸시킨다. 2의 경우는 두 꼭짓점이 정지, 즉 학습된 이상치만큼 bounding box가 작아졌다는 뜻이므로 (= bounding box가 물체를 어느정도 정확하게 잡았다. = IOU가 가능한한 최대로 높다.) 검출된 bounding box를 return한다. ​​<Training Method>AttentionNet이 training을 하기 위해 특정 조건에 맞춰친 임의의 bounding box를 생성하고 해당 bounding box가 실제 참 값에 도달하기 위해 취해야하는 움직임을 사용자가 지정해 준 데이터셋이 필요하다. 즉 한 사진마다 가능한 데이터셋의 종류는 각각의 레이어(Conv8-TL, Conv8-BR)이 움직일수 있는 방향의 모든 조합(4 * 4 = 16) + Negative sample (bounding box내에 instance가 없음) 총 17가지이다. ​학습이 원할히 이루어지기 위해 연구진이 training과정에서 bounding box를 생성할때의 3가지 규칙을 만들었는데 1. positive region(initial bounding box)내에 필히 target instance의 50%이상이 포함되어야 한다. 2. positive region은 여러가지 instance를 포함할 수 있다. 단, target instance가 반드시 가장 큰 영역에 포함되어야 하며, 다른 instance보다 적어도 1.5배정도 커야한다. 3. positive region은 여러가지 종횡비를 가져야하며, 그 크기도 다양해야한다. 이때 2번 조건을 반드시 지켜야 학습 결과 정확한 데이터를 얻을수 있다고 한다. (2가지 이상의 instance가 있을 경우)​train과정에서 positive sample : negative sample의 비율은 1대 1로 지정되었다. 이때 Negative sample을 제외한 나머지 positive sample은 총 16개이므로 1회의 batch속에     positive sample의 각각의 종류마다 1/(2*16)만큼의 dataset이 할당된다. ((1/(2*16)) * 16 + 1/2) 이때 AttentionNet은 train과정에서 loss는 Conv8-TL, Conv8-BR의 soft-max loss의 평균값이라 하는데 soft-max loss가 정확히 어떤 것을 뜻하는지 잘 모르겠다. (내가 아는건 MSE, SSE밖에 없어서..) ​이후 내용은 다중 객체 검출기로  확장하기 전에 단일 객체 검출기로서의 성능을 확인하는 단계이다. R-CNN과 같은Region proposal 알고리즘의 경우 객체의 전체가 아닌 부분을 검출하기 위한 알고리즘이기에 객체 전채를 검출하는데 있어서 퍼포먼스가 나쁠수 있다. 따라서 본 연구진은 약간의 실험을 통해 AttentionNet에서도 해당 단점이 있었는지 확인을 했고, 사람을 검출하는 실험 결과 큰 문제는 없었다고 한다. (기존의 알고리즘 : 79.4%, AttentionNet : 89.5%)​​여기까지가 단일 객체 검출기로서의 AttentionNet이다. AttentionNet은 현존하는 알고리즘, 특히 yolov5와 같은 알고리즘에 비해 성능이 미약한 것으로 보인다. 하지만, 아직 two stage detector밖에 존재하지 않던 시절에 one stage detector와 유사한 Network를 발명해냈다는 것이 좀 신기했다. (어찌되었든 AttentionNet 안에는 Region proposal과 object classifer가 혼재하기 때문에...)​논문의 길이가 길기도 하고 여러모로 적을것이 많아서 다중 개체 검출용 AttentionNet은 다음 글에 이어서 다뤄보도록 하겠다..  참고지식- Deformable Part Model (DPM) : R-CNN기법의 원조격 모델, selective search를 사용해 region proposal을 진행하는 R-CNN과는 다르게 이쪽은 사전에 이미지를 격자처럼 쪼개고, 그 격자간의 상호관계를 통해 object detecting을 진행한다. ensemble기법을 사용한다고 하는데.. 연산량이 엄청 많아 속도가 느리다고 한다.- HOG detector : HOG Feature를 생성해 검출을 진행한다고 한다. edge detection -> Histogram -> HOG Feature 순으로 알고리즘이 진행된다.- ImageNet : detector는 아니고 dataset의 일종. 본 논문이 출시되었을때 가장 큰 dataset이었나보다.- Region-CNN (R-CNN) : 앞서 언급한 DPM의 발전 버전, selective search를 이용해 region proposal을 하기 때문에 DPM에 비해 더 다양한 환경에서 Localization안정성을 보장받을 수 있다.- (selective search) : 이미지 상에서 객체가 있을만한 부분을 검출하는 알고리즘이다. 구현 방법이 다양하고, 최근에는 two stage detector (R-CNN계열 detector, one stage detector도 있다(yolo, SSD계열).)에 사용된다.​더 알아봐야하는 것.- Region-CNN : 본 논문의 선행 연구격- ensemble 기법 : 본 논문 뿐만 아니라 relative work에도 자주 등장​Ref.[1, 논문에서는 4] https://arxiv.org/abs/1405.3531 Return of the Devil in the Details: Delving Deep into Convolutional NetsThe latest generation of Convolutional Neural Networks (CNN) have achieved impressive results in challenging benchmarks on image recognition and object detection, significantly raising the interest of the community in these methods. Nevertheless, it is still unclear how different CNN methods compare...arxiv.org ​[2, 논문에서는 28] https://link.springer.com/chapter/10.1007/978-3-319-10590-1_53 Visualizing and Understanding Convolutional NetworksLarge Convolutional Network models have recently demonstrated impressive classification performance on the ImageNet benchmark Krizhevsky et al. [18]. However there is no clear understanding of why...link.springer.com ​ "
"[Object Detection] 비-최대 억제(NMS, Non-maximum) metric 이해하기 ",https://blog.naver.com/qkrdnjsrl0628/222815369995,20220717,"*해당 포스팅은 IOU(Intersection Over Union)을 알아야 하므로, 이전 포스팅 참고 바랍니당!https://blog.naver.com/qkrdnjsrl0628/222815344155 [Object Detection] Intersection over Union Metric 이해하기* R-CNN 논문을 리뷰하기 전 Object Detection Metric 내용 포스팅 합니다. IOU(Intersection o...blog.naver.com 비-최대 억제(NMS, Non-maximum Suppression)      - 비-최대 억제(Non-maximum Suprresion)은 Object Detector가 예측한 bounding box 중에서        정확한 bounding Box를 선택하도록 하는 기법입다.​      - Image내에는 다양한 크기와 형태의 Object들이 존재합니다. Object를 완벽하게 검출하기 위해,        여러 개의 bounding box를 생성해야 하는데, 이중 하나의 bounding box만을 선택해야 하는데,        이때 적용하는 기법이 비-최대 억제(NMS , Non-Maximum Suppression) 이다.​비-최대 억제(NMS, Non-maximum Suppression) 알고리즘 원리      - 하나의 Class에 대한 bounding boxes 목록에서 가장 높은 점수를 가지고 있는 bounding box를 선택하고,         목록에서 제거한다. 그리고, final box에 추가합니다.       - 선택된 bounding box를 bounding box 목록에 있는 모든 bounding box와 IOU를 계산 및 비교한다.        IOU가 threshold보다 높으면 bounding boxes 목록에서 제거한다.​      -  bounding boxes 목록에 남아있는 bounding box에서 가장 높은 점수를 갖고 있는 것은 선택하고          목록에서 제거한다. 그리고 final box에 추가한다.       - 다시 선택된 bounding box를 목록에 있는 box들과 IOU를 비교한다. threshold보다 높으면 목록에서 제거​      - 위의 과정을 반복한다.​[NMS를 실행하기 전] [NMS를 실행한 후] ​​* 해당 자료 48p를 참고하시면 됩니다.https://docs.google.com/presentation/d/1aeRvtKG21KHdD5lg6Hgyhx5rPq_ZOsGjG5rJ1HP7BbA/pub?start=false&loop=false&delayms=3000&slide=id.g137784ab86_4_5115 YOLOYOLO You Only Look Once: Unified, Real-Time Object Detection Joseph Redmon, Santosh Divvala, Ross Girshick, Ali Farhadidocs.google.com import torchdef nms(bboxes, iou_threshold, threshold, box_format = ""corners""):    # bboxes가 list인지 확인합니다.  assert type(bboxes) == list  # box 점수가 threshold보다 높은 것을 선별한다.  # box shape는 [class, score, x1, y1, x2, h2] 이다  bboxes = [box for box in bboxes if box[1] > threshold]  # 정렬  bboxes = sorted(bboxes, key = lambda x : x[1], reverse = True)  bboxes_after_num = []  while bboxes:    chosen_box = bboxes.pop(0)        # box가 선택된 box와의 iou가 임계치보다 낮거나        # class가 다르다면 bboxes에 남기고, 그 이외는 다 없앱니다.    bboxes = [box for box in bboxes if box[0] != chosen_box[0] \          or intersection_over_union(torch.tensor(chosen_box[2:]),                                     torch.tensor(box[2:]),                                     box_format=box_format) < iou_threshold]        # 선택된 박스를 추가합니다.    bboxes_after_nmn.append(chosen_box)    return bboxes_after_nmn Referencehttps://deep-learning-study.tistory.com/403 [Object Detection] 비-최대 억제(NMS, Non-maximum Suppression)를 이해하고 파이토치로 구현하기 안녕하세요! 이번 포스팅에서는 비-최대 억제(NMS,Non-maximum Suppression)을 알아보도록 하겠습니다.  비최대 억제를 이해하기 위해서는 IoU(intersection over unio)에 대한 개념을 알아야합니다.  IoU에..deep-learning-study.tistory.com https://dyndy.tistory.com/275 NMS (non-maximum-suppression)오래간만의 포스팅. 요즘 딥러닝을 이용한 여러 Object Detection 알고리즘을 구경하는데, 대부분 NMS (non-maximum suppression)을 사용하여 연산량을 줄이고, mAP도 올리는 효과를 본다고 한다. 물론 필수로 필..dyndy.tistory.com https://hongl.tistory.com/180 Non-maximum Suppression (NMS)YOLO와 같은 딥러닝을 이용한 object detection 은 입력 이미지를 몇 개의 grid 구역으로 나누고 각 grid 별 bounding box 를 제안하여 각 bound box의 위치, 크기, 물체가 담길 확률을 계산하는 방식으로 이루어..hongl.tistory.com ​ "
[python] YOLO V3와 MobileNet V2를 활용한 축구 영상 Object Detection Model (openCV & tensorflow) ,https://blog.naver.com/chunsa0127/222439908488,20210721,"0. 들어가기 전에겨울 방학부터 한 2달 간 Computer Vision 관련 공부를 해왔는데, CNN을 활용한 이미지 분류 외에는 따로 구현해본 적이 없었다. 그중 가장 재밌게 공부했던 파트인 object detection 관련 모델을 활용하여 작은 프로젝트를 해보고 싶었다. 그래서 방학을 맞아 조금 여유로워진 겸 YOLO를 활용한 object detection 모델을 구현해보기로 했다. ​어떤 영상에서 구현하면 좋을까 고민하다가 나의 유일한 취미인 축구 관련 프로젝트면 더 좋겠다 싶어서 SPOTV에서 제공하는 축구 하이라이트 영상을 활용하기로 했고, PL에서 제일 좋아하는 팀인 맨체스터 유나이티드의 선수들과 다른 팀 선수, 심판을 구분하는 모델을 만들기로 했다. ​전체 코드는 아래에 첨부하니 다운로드받아 확인 부탁드린다.  첨부파일football_detection.ipynb파일 다운로드 유투브에서 영상을 다운로드하는 것부터 전체 분석 과정이 담겨 있다. ​1. 접근 방식YOLO를 활용하기로 한 뒤, 훈련 데이터를 만들기 위해 축구 영상을 일정 시간 단위별로 캡쳐해 직접 박스를 그려주고,  라벨링을 하려고 했다. 그러나 이는 시간이 매우 오래 걸린다는 점에서 나에게 그리 좋은 방식은 아니었다. 그래서 어차피 유니폼의 색을 구별하면 된다는 점에서 YOLO모델로 사람 이미지를 잡아낸 다음, 해당 이미지에서 빨간색 등의 맨유 유니폼 색 비율이 높다면 맨유 선수로 분류하는 방식을 선택했다.  def detect_red(img):    red_range = ([10, 10, 70], [80, 80, 255])    lower = np.array(red_range[0], dtype = ""uint8"")    upper = np.array(red_range[1], dtype = ""uint8"")    cvt = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)    mask = cv2.inRange(cvt, lowerb = lower, upperb = upper)    result = cv2.bitwise_and(cvt, cvt, mask = mask)    return resultresult = detect_red(imgs[3])result = np.hstack([imgs[3], result])plt.imshow(result)print('proportion of red : {}'.format(round(len(result[result!=0])/(result.shape[0]*result.shape[1]*3),2))) 다음의 opencv의 간단한 함수를 통해 원하는 색의 범위를 지정해주어 해당 범위에 속하는 부분만 위와 같이 탐지할 수 있고, 설정한 threshold를 넘는다면 맨유 선수라고 인식하는 방식이다. 그러나 위 방식은 매 유니폼마다 색의 범위를 찾아내야 하며, 또한 threshold도 수차례 실험을 통해 경험적으로 알아낼 수 밖에 없다.  맨유의 Away 유니폼 색인 검정색에서도 문제가 발생하는데, 만약 상대팀에 흑인 선수가 있다면 검정색 비율이 높아지므로 제대로 된 분류가 되지 않을 수 있다. 결국 위 방식보단 맨유 선수를 구분해내는 CNN 모델을 만드는 것이 훨씬 나을 것이라고 판단했다.​2. YOLO by openCV arunponnusamy/object-detection-opencvYOLO Object detection with OpenCV and Python. Contribute to arunponnusamy/object-detection-opencv development by creating an account on GitHub.github.com YOLO 모델은 tensorflow 기반 darkflow나 openCV에서 제공하는 deep learning framework 등을 활용하여 사용할 수 있다. 나는 openCV를 활용했는데, openCV는 CPU만 할 수 있어 속도가 느리긴 하지만 내 기준 설치과정이나 사용방법이 어렵지 않기 때문이었다. 위 github 주소에서 yolov3.weights와 yolov3.cfg 파일을 다운로드하여 opencv에서 불러오면 사전학습된 YOLO모델을 사용할 수 있다.  축구 영상 중 한 장면을 캡쳐해 YOLO에 넣어본 결과, 사람을 잘 잡아내는 것을 확인했다. ​저 박스들별로 이미지를 다 저장한 뒤, 맨유의 Home, Away, Third kit별로 분류하고 심판도 검정색, 하늘색, 노란색 상의별로 분류하여 데이터를 정리했다. 6개의 하이라이트 영상에서 총 3000여개의 데이터셋을 구축했고 이를 분류하는 CNN 모델을 훈련시키기로 했다. ​3. MobileNet V2 fine-tuning by tensorflow훈련데이터가 많지 않으므로 전이학습을 하기로 했고, 모델은 inference 속도가 빠른 MobileNet V2를 사용했다. 모델 훈련 코드는 아래와 같다. 아래의 코드는 KUBIG에서 작년에 face mask detection project를 했던 팀의 코드를 참고했다.  KUBIG_2020_FALL/2분기 프로젝트/Face Mask Detection Project at master · KU-BIG/KUBIG_2020_FALLRepository for KUBIG 2021 Spring's slides, study, and projects - KUBIG_2020_FALL/2분기 프로젝트/Face Mask Detection Project at master · KU-BIG/KUBIG_2020_FALLgithub.com mobilenet = MobileNetV2(input_shape = (128, 128, 3), include_top = False, weights = 'imagenet') mobilenet.trainable = True## freeze base networkfor i in range(len(mobilenet.layers)):    mobilenet.layers[i].trainable = Falsemodel = Sequential()model.add(mobilenet)model.add(MaxPooling2D(3))model.add(Flatten())model.add(Dense(512, activation = 'relu'))model.add(BatchNormalization())model.add(Dense(256, activation = 'relu'))model.add(BatchNormalization())model.add(Dense(6, activation = 'softmax')) # number of classes = 6generator = ImageDataGenerator(horizontal_flip=True, zoom_range=0.1, fill_mode=""nearest"")INIT_LR = 1e-4EPOCHS = 30opt = Adam(lr=INIT_LR, decay=INIT_LR / EPOCHS)model.compile(loss=""categorical_crossentropy"", optimizer=opt, metrics=[""accuracy""])ckp = ModelCheckpoint('./best_model.h5', save_best_only=True, monitor = 'val_accuracy')history = model.fit(        generator.flow(x_train, y_train, batch_size=32),        steps_per_epoch = len(x_train)//32,        epochs=EPOCHS,        validation_data=(x_test, y_test),        validation_steps=len(x_test)//32,        callbacks = [ckp]) tensorflow의 Sequential API를 활용해 구성했고, augmentation은 zoom과 horizontal flip만 사용했는데, vertical flip 시 유니폼 상의와 하의의 색이 뒤바뀌며 다르게 인식할 확률이 높아질 것이라 판단했기 때문이다.  위 그래프를 보면 accuracy나 Loss 측면에서 모두 약간의 과적합이 있다고 보이나, model checkpoint를 통해 validation set 기준 accuracy가 가장 높은 모델만 저장하여 사용했다. ​4. object detection 관련 함수 정의 # return the final output of YOLOv3     def get_output_layers(net):       layer_names = net.getLayerNames()        output_layers = [layer_names[i[0] - 1] for i in net.getUnconnectedOutLayers()]    return output_layers# return the coordinate of boxesdef get_boxes_coordinate(boxes, confidence, conf_t, nms_t):    indices = cv2.dnn.NMSBoxes(boxes, confidence, conf_t, nms_t)    boxes = [boxes[i[0]] for i in indices]    return [[x, y, x+w, y+h] for [x, y, w, h] in boxes]# return the cropped person imagesdef get_crop_img(img, boxes):    return [img[round(y1):round(y2), round(x1):round(x2)] for [x1, y1, x2, y2] in boxes]# draw the bounding box in the imagedef draw_bounding_box(img, class_str, box, color):    cv2.rectangle(img, (round(box[0]), round(box[1])), (round(box[2]), round(box[3])), color, 2)    cv2.putText(img, class_str, (round(box[0])-10,round(box[1])-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2) 첫번째 함수는 inference를 위해 필요한 YOLO의 output layer들의 이름을 가져오는 코드이다. YOLO에서 반환한 bounding box는 x, y, w(가로 길이), h(세로 길이)를 반환하는데, 이를 박스 좌표로 변환하기 위한 함수가 두번째 함수이다. 두번째 함수에서 반환한 좌표들을 활용해 이미지에서 사람이 탐지된 부분만 잘라 저장하는 함수가 세번째 함수이며, 마지막 함수는 이미지에 박스를 그려주어 반환해주는 함수이다.  ​이렇게 필요한 함수들을 미리 정의해준 뒤, 실제로 맨유 선수들을 탐지하기 위한 함수를 아래와 같이 정의했다.  def detect_MU(img, net, pred_t_player, pred_t_referee ,classifier, conf_t, nms_t, mu_color = 'home', referee_color = 'black'):    img_rescale = cv2.resize(img, (224, 224))    scale = 0.00392    ## preprocessing the image by blob    blob = cv2.dnn.blobFromImage(img_rescale, scale, (416,416), (0,0,0), True, crop=False)    net.setInput(blob)        ## detect the objects    outs = net.forward(get_output_layers(net))    Width = img.shape[1]    Height = img.shape[0]    confidences = []    boxes = []        for out in outs:        for detection in out:            scores = detection[5:] ## probabilities of 80 classes            class_id = np.argmax(scores)            if class_id == 0:  ## get only class = 'person'                       confidence = scores[class_id]                if confidence > conf_t: ## get only images over the confidence threshold                    center_x = int(detection[0] * Width)                    center_y = int(detection[1] * Height)                    w = int(detection[2] * Width)                    h = int(detection[3] * Height)                    x = center_x - w / 2                    y = center_y - h / 2                     confidences.append(float(confidence))                    boxes.append([x, y, w, h])            box_fin = get_boxes_coordinate(boxes, confidences, conf_t, nms_t)    imgs = get_crop_img(img, box_fin)    try:        imgs_arr = np.array([])                for i in range(len(imgs)):            image = cv2.resize(imgs[i], (128, 128))            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)            image = preprocess_input(image)                    image = image.reshape(1, 128, 128, 3)            imgs_arr = np.append(imgs_arr, image)                imgs_arr = imgs_arr.reshape(-1, 128, 128 ,3)        classes = classifier.predict(imgs_arr) # classification by MobileNet v2                ## remove ohter classes        if mu_color == 'home':            classes[:, [0,2]] = 0        elif mu_color == 'away':            classes[:, [1,2]] = 0        else:            classes[:, [0,1]] = 0                if referee_color == 'black':            classes[:, [4,5]] = 0        elif referee_color == 'blue':            classes[:, [3,5]] = 0        else:            classes[:, [3,4]] = 0                prob = np.max(classes, axis = 1)        classes = np.argmax(classes, axis = 1)        class_list = []        ## compare the probability & threshold        for i in range(len(classes)):            if (prob[i] > pred_t_player) & (classes[i] < 3):                class_list.append('MU')            elif (prob[i] > pred_t_referee) & (classes[i] >=3):                class_list.append('referee')            else:                class_list.append('other')        ## drawing bounding boxes        for i in range(len(class_list)):            if class_list[i] == 'MU': # red box for MU players                draw_bounding_box(img, class_str = class_list[i], box = box_fin[i], color = [0, 0, 255])            elif class_list[i] == 'referee': # skyblue box for referee                draw_bounding_box(img, class_str = class_list[i], box = box_fin[i], color = [150, 150, 0])             else: # blue box for others                draw_bounding_box(img, class_str = class_list[i], box = box_fin[i], color = [255, 0, 0])    except:        pass        return img  위 함수가 조금 길어서 보기 힘들지만, 잘라서 보면 앞서 설명했던 그대로이다.  YOLOv3 모델에서 사람이라고 탐지한 부분만 잘라낸 뒤, MobileNet에서 총 6개의 class로 구분한다. 여기서 6개의 class는 맨유의 home, away, third kit, 심판의 검은색, 하늘색, 노란색을 구분하는 것이다. 그러나 한 경기에서 세 유니폼을 입는 것이 아니므로, 입지 않는 나머지 확률에 대해서는 다 0으로 만들어주어야 한다. 예컨대 맨유가 Away 유니폼을 입고, 상대팀이 빨간색 유니폼을 입을 경우 상대팀에 대해선 모두 맨유 Home으로 분류할 것이다. 또한 심판이 검정색 유니폼을 입을 경우, 맨유의 Away 유니폼(검정색)과 확률을 나눠가질 것이다. 이런 식으로 잘못 분류하는 것을 방지하기 위해 맨유 선수 유니폼 중 2개의 확률, 심판의 유니폼 중 2개의 확률은 모두 0으로 만들어주어야 한다. ​이후, 지정한 확률의 threshold보다 높은 경우엔 ""MU"" 또는 ""referee""로 분류하고 확률값이 threshold보다 낮을 경우 ""others""로 분류하도록 한다.​실제 경기 영상 중 한 장면에 대해 아래와 같이 잘 분류하고 있음을 확인했다.  MU Away(검정색) vs S.햄튼(빨간색) / referee(하늘색)실제로 사우스햄튼 선수들이 빨간색 유니폼을 입고 있으므로 맨유의 Home 선수들이라고 분류하지만, 후처리를 통해 해당 확률을 0으로 만들었기 때문에 others로 분류가 되고 있다. ​5. Writing Video 동영상은 사진의 연속이다. Frames Per Second(FPS)는 1초당 몇 프레임으로 구성되는지에 대한 단위로, FPS가 높을수록 더 부드러운 영상이 된다. 일반적인 방송 화면 등에서는 30FPS를 사용한다고 하는데, 나는 조금 더 빠른 속도를 위해 20FPS로 영상을 만들었다. 이 이야기는 4번에서 만든 detect_MU 함수가 초당 20장을 처리할만큼 빨라야 real time으로 구동이 된다는 말이다. 그러나 실제 영상 제작하면서 본 결과, 20초의 영상을 만드는데에 4분 정도의 속도가 걸릴만큼 느린 구동속도를 보였다. 이는 YOLO가 CPU에서 구동이 된다는 점도 있겠지만 YOLO의 최대 장점이라 할 수 있는 one-stage detector(분류와 bounding box를 한번에 처리)를 살리지 못하고 detection -> classification이 되어버렸기 때문이다. 또한 한 화면에서 분류해야할 객체들이 많다는 점도 문제일 것이다.  def getFrame(sec, video_path):    video_name = video_path.split('/')[-1].split('.')[0]    video.set(cv2.CAP_PROP_POS_MSEC,sec*1000)    success,image = video.read()    return success, image 위 함수는 초와 비디오 경로를 입력하면 해당 초의 비디오 화면을 읽어오는 함수이다. 반환하는 인자가 두개인데, 만약 입력된 초가 비디오의 길이에 벗어나지 않는다면 True, 벗어나서  화면을 읽어올 수 없다면 False를 반환하는 것이 success이며 image는 실제 읽어온 화면이다. ​이 함수를 활용하여 아래와 같이 정해진 범위의 화면을 읽어와 detection을 진행하고, 다시 동영상으로 만드는 함수를 정의했다.  def write_video(video_path, save_path, length,start_sec, conf_t,pred_t_player, pred_t_referee, mu_color = 'home', referee_color = 'black'):    cap = cv2.VideoCapture(video_path)     fourcc = cv2.VideoWriter_fourcc(*'DIVX')    out = cv2.VideoWriter(save_path, fourcc, 20.0, (1280,720)) # 20FPS    sec = start_sec    success, img = getFrame(sec, video_path)    count = 0    while success:        count += 1        sec += 1/20        sec = round(sec, 2)        success, img = getFrame(sec, video_path)        result_img = detect_MU(img, net, conf_t = conf_t, nms_t = 0.3, pred_t_player = pred_t_player, pred_t_referee = pred_t_referee ,classifier = best_model, mu_color = mu_color, referee_color = referee_color)              out.write(result_img)     cap.release()    out.release()    cv2.destroyAllWindows() write_video(video1, './MU_home.avi',             start_sec = 70, conf_t = 0.7,             pred_t_player =0.8,pred_t_referee =0.2,             mu_color = 'home', referee_color = 'black',             length = 20) 위 함수에서 눈여겨볼 부분은, 맨유 선수 분류 확률에 대한 threshold와 심판 분류 확률에 대한 threshold가 다르다는 것이다. 훈련 데이터 내에서 맨유 선수 이미지가 거의 90%를 차지하기 때문에 심판에 대한 분류 정확도가 떨어질 수밖에 없었기 때문에 심판에 대한 threshold를 낮게 설정했다. ​이렇게 만든 최종 결과물은 아래와 같다.     훈련에 사용된 영상이 아닌 다른 축구 영상에 적용해보았을 때에도,  잘 구동되는 것을 확인할 수 있었다. ​  ​약 20초의 영상을 만드는데 4분 정도의 시간이 소요되었는데, 20초 동안 총 400장의 영상을 처리하기 때문에 초당 1~2 프레임 정도를 처리하는 속도를 보인다. 즉, 실시간으로 적용하기는 불가능하다. 짧은 영상을 만드는 것까지는 어렵지 않게 처리할 수 있지만 실시간으로 영상을 처리하기 위해선 이 방식 대신 YOLO를 fine tuning하여 분류까지 한번에 진행하는 것이 맞을 것이다.​아무튼 재미삼아 해본 프로젝트인데, 한 2주 정도 많은 시행착오를 겪어가며 많이 배울 수 있었다. openCV와 조금은 친해질 수 있어 좋았다. 나름 절반의 성공 정도는 거뒀다고 생각하며 football detection project를 이렇게 마무리한다. ​​​ "
데이터 모델링 - 물체인식 (Object Detection) ,https://blog.naver.com/dslab_global/222219052585,20210125,"1. 물체인식이란?   물체인식(Object Detection)은 어떠한 이미지 데이터에서 찾아내고자 하는 물체를 인식하여 추출하는 방식의 기술입니다. 데이터 라벨링( 라벨링 )을 통해 미리 학습된 클래스들을 바탕으로, 이미지 속의 클래스를 찾아 추출해 내는 것입니다. 이는 단순 이미지를 카테고리로 분류하는 이미지 분류(Image Classification)와 이미지의 영역을 찾아내는 물체 위치 인식(Object Localization)보다 한 단계 더 발전된 기술로, 이미지 전체를 한 카테고리로 분류하지 않고 이미지 내의 다양한 물체를 분류하고 물체의 영역을 찾아줍니다. 딥러닝의 물체인식은 주로 R-CNN(Region based Convolutional Nueral Network) 또는 YOLO(You Only Look Once) 방식 등의 접근을 통해 이루어집니다.​​2.물체인식 예시​ 물체인식 모델을 통해 인식하고자 하는 클래스의 라벨링이 완료된 데이터셋를 인공지능 모델에게 학습시키면, 모델을 통해 한 이미지 데이터 속의 클래스 인식이 가능해집니다.  위 사진처럼 개와 고양이가 각각 'dog'과 'cat'이라는 클래스로 라벨링되어 있는 데이터셋을 학습시켜 물체인식 모델을 생성합니다.  위와 같이, 생성된 모델을 이용하여 다른 이미지에서도 이미 학습한 개와 고양이를 분류하여 인식하지만, 모델에게 학습을 시키지 않은 토끼나 기니피그는 분류가 불가능합니다.​​3. 물체인식 사용  물체인식이 이미지 또는 영상 내 물체를 분류하고 위치를 파악할 수 있는 기능은 최근 사회의 많은 분야에서 각광받고 있습니다. 그 중에서도 물체인식의 중요한 응용 분야는 자동차 자율주행입니다. 차량 또는 도로 시설물 등을 통해 카메라로 촬영되는 이미지와 영상 속의 차량, 신호등, 표지판, 행인 등을 물체인식을 통해 끊임없이 식별함과 동시에 주행해야 하기 때문입니다. 물체인식은 자율주행 외의 다양한 분야에서도 지속적으로 사용/개발되고 있습니다.​ - 얼굴 인식 - 불량품 검사 - 공항 검색대​​4. CLICK AI에서의 물체인식​ CLICK AI는 자동화 기계 학습 플랫폼을 통해 코딩 없이도 가능한 물체인식 모델 개발을 지원합니다. 이미지 데이터 전처리부터 모델 생성을 통한 예측까지 모든 과정을 CLICK AI 플랫폼을 통해 데이터 과학 전문 지식에 관계없이 편리하게 이용하실 수 있습니다. 플랫폼 내 라벨링 툴을 이용한 간단한 이미지 라벨링 작업과, 라벨링이 완료된 데이터의 연동을 통해 편리하게 물체인식 모델을 개발할 수 있습니다. ​ 개발이 완료된 인공지능 모델의 이미지 또는 영상 분석 기능을 이용한 물체인식 결과를 확인할 수 있습니다.​CLICK AI의 메인 페이지의 개발 시작하기 버튼을 클릭하여 물체인식 모델 개발을 시작하실 수 있습니다. CLICK AI 자동화 기계 학습 플랫폼을 이용한 물체인식 모델 개발에 대한 세부 내용은 물체 인식를 참고 바랍니다.​​ CLICK AICLICK AI는 AI 도입의 전 과정을 자동화하여 신속하고 간편하게 AI를 활용하도록 합니다. 최신 딥러닝 알고리즘을 활용하여 100여 개의 모델을 생성 및 학습하고,전사적 도입을 통한 부서별 협업으로 새로운 비즈니스 가치를 창출할 수 있습니다.clickai.ai ​ "
[컴퓨터비전]  Object detection ,https://blog.naver.com/navehag/222459294936,20210806,"1. [Object Detection] Faster R-CNN, YOLO, SSD, CornerNet, CenterNet 논문 소개 [Object Detection] Faster R-CNN, YOLO, SSD, CornerNet, CenterNet 논문 소개object detection에 대한 개념 정리 및 해당하는 딥러닝 논문들을 소개한 글입니다.nuguziii.github.io 2. 딥러닝 용어 classification, localization, detection, segmentation  딥러닝에서 이미지 classification, localization, detection, segmentation지난번 YOLO 소개에 이어서 ... 오늘은 이미지 classification, localization, detection, segmentation 이 어떤건지 알아보겠습니다.  복잡하고 머리 아픈 알고리즘은 저도 잘 모르는 관계로 집어치우고 위 용어들이 뭘 하는것인지만 알고 넘어가시죠.  비슷비슷 헛갈리는 용어들인egloos.zum.com 3. 딥러닝 분야별 State-of-the-art (SOTA) 브라우저 Papers with Code - Browse the State-of-the-Art in Machine Learning5177 leaderboards • 2361 tasks • 4500 datasets • 51437 papers with code.paperswithcode.com 4. Object Detection Part 4: Fast Detection Models​ Object Detection Part 4: Fast Detection ModelsPart 4 of the “Object Detection for Dummies” series focuses on one-stage models for fast detection, including SSD, RetinaNet, and models in the YOLO family. These models skip the explicit region proposal stage but apply the detection directly on dense sampled areas.lilianweng.github.io  YoLo(You only Look Once)​논문 사이트 : https://arxiv.org/pdf/1506.02640.pdf논문 리뷰 :   논문 리뷰 - YOLO(You Only Look Once) 톺아보기본 글은 YOLO 논문 전체를 번역 및 설명해놓은 글입니다. 크게 중요하지 않은 부분을 제외하고는 대부분의 글을 번역했고 필요하다면 부가적인 설명도 추가했습니다. 내용이 긴 섹션 끝에는 요약도 추가했습니다...bkshin.tistory.com [2] 왜 YOLO를 사용하는가?개발배경은 앞서 보았듯이 막대한 농작물의 피해를 Deep Learning기술을 사용함으로써, 방지하자 입니다. 그러면 어떤 Algorithm을 사용할 것인가? 또 그 Algorithm은 어떻게 설치하고 사용하는지 알아보도록 하겠..pgmrlsh.tistory.com  ​​Mask RCNN - COCO - instance segmentationhttps://youtu.be/OOT3UIXZztE https://machinelearningmastery.com/how-to-perform-object-detection-in-photographs-with-mask-r-cnn-in-keras/ How to Use Mask R-CNN in Keras for Object Detection in PhotographsObject detection is a task in computer vision that involves identifying the presence, location, and type of one or more […]machinelearningmastery.com sudo pip install --no-deps tensorflow==1.15.3sudo pip install --no-deps keras==2.2.4 "
Object Detection VS Image Segmentation ,https://blog.naver.com/kimsamuel351/222704042489,20220418,Object Detection ​Image Segmentation ​cf. https://towardsdatascience.com/what-is-the-difference-between-object-detection-and-image-segmentation-ee746a935cc1 What is the difference between Object Detection and Image Segmentation?And when to use which?towardsdatascience.com 
[object detection] 0강 - Intro ,https://blog.naver.com/jys3136/221680546037,20191017,"일단 지금까지 해서 1) CNN의 기본 개념과 2) 과거 LeNet과 AlexNet을 시작으로 ImageNet 대회에서 주목받은 CNN 구조들을 살펴 보았고 지금부터는 단지 classification이 아닌 더 심화된 Computer vision 기술을 배워본다.​1. Recognition   ​ Recognition1) classification지금까지 배운 이미지 전체를 보고 class별 분류를 하는 과정입니다.2) classification + localization1> object의 위치를 직사각형으로 localize하고 2> localize 된 이미지를 classification(단, 이미지 내에 object는 1개라는 전제가 있다.)3) object detection1> 이미지 내부에 있는 모든 object를 찾고2> localize 된 이미지 모두를 각각 classification 한다.(object가 여러 개라는 점에서 2)와 다르다.)4) Instance segmentation직사각형이 아니라 object의 모든 점을 localize 한다. ​2. ImageNet 대회   (1) 2개의 label1) box: object의 위치2) class: box안의 object의 class​(2) correct의 기준correct: 5개의 (class, box) 예측 중 1개 맞음1) box: IoU 0.5이하2) class: class 일치 여부​​​cf> IoU란? Intersection over union (합집합 중 교집합)    ​​참고: 1) CS231n (2016 winter)2) https://ballentain.tistory.com/12 "
SAN: Learning Relationship between Convolutional Features  for Multi-Scale Object Detection 논문 리뷰 ,https://blog.naver.com/tomatian/222086943148,20200911,"SAN: Learning Relationship between Convolutional Features  for Multi-Scale Object DetectionECCV 2018https://arxiv.org/abs/1808.04974​​ Introduction1. previous methods to fix multi-scale variant issues   2. Conventional Solutions for CNN Based NEtworks   3. Overview of SAN(multi scale aware network)① previous methods to fix multi-scale variant issues· 미리 알면 좋은 개념 : scale normalization 이라는 단어가 빈출하는데 나는 여러 갖은 scale을 하나로 인식할 수 있도록 만듦 이라고 해석하였다.​· image pyramid image pyramid : 한 이미지를 여러 scale로 이미지 피라미드+ scale normalization를 이용해 single detector 로 객체를 인식하는 것입니다. 이미지 사이즈를 변경하면서 이미지를 여러 scale 에서 분석하는 방법입니다. 이런 이미지 피라미드에 scale normalization을 적용하면, 한 이미지의 특정 물체에 대한 여러 scale이 나오고, 그 scale들을 하나의 scale로 정규화 하는 것입니다. 이미지 피라미드로는 multi scale feature 를 계산할 수 있지만 시간과 메모리를 많이 사용해서 end to end learning(딥러닝)에는 적합하지 않은 기법입니다.​· pooling & using grid cell (that grid cell indicates information of scale) ROI pooling을 통해 실제로 이미지에서는 크기가 다른 물체이지만 관심 object를 모두 같은 scale로 만들어 줍니다. 이 방법은 CNN 기반의 net에 많이 사용됩니다. 역시나 하나의 scale로 만들어 준다는 것에서 scale normalization이 적용되었습니다.다음은 주변 영역과 그 크기를 나타내는 grid cell 을 학습하여 객체 인식을 추론하는 것입니다.​· distinctive detector for each scale 이미지 피라미드와 비슷하지만 각 scale 별로 다른 detector 를 사용하는 것입니다. ​· 위의 방법들의 문제점 image pyramid 와 RoI pooling, grid cell 이용 기법에 사용되는 scale normalization 은 해상도와 관련성이 높습니다.scale normalization 이 multi scale invariant 를 유지하는 데는 도움이 되지만, 큰 이미지와 작은 이미지의 해상도가 그 크기에 비례해야만 효과가 있습니다. (scale normalization 이 이용되는 방법 : image pyramid, pooling)큰 이미지가 줄어들었을 때 resolution 이 줄지 않을 정도를 유지하고, 그 해상도를 가진 sample 들을 subspace로 묶고, gradient feature 을 조정하면 detection accuracy 향상에 도움을 줍니다.​​​② Conventional Solutions for CNN Based NEtworks CNN 기반의 네트워크에서 주로 사용되는 방법 (파란 엑스 : 배경, 초록 삼각형 : 객체)· CNN 기반의 네트워크에서는 주로 scale normalization 을 사용하지 않습니다.그렇다면 resolution에 대한 문제는 발생하지 않고 multi scale invariant 에 대한 문제만 남게 됩니다. 이러한 scale variation 은 resolution 문제보다 더 안좋은 detection accuracy 를 만듭니다.따라서 CNN 기반 네트워크에서는 주로 어떤 기법을 사용하는지 아래에 기술하겠습니다.​· 그림 a 전체 sample 들이 혼재되어 있는 상태에서 single detector 를 사용하는 경우가 있습니다. 가장 간단한 구조를 가지고 있어서 scale space 별로 학습하지 못합니다.​· 그림 b 각기 다른 scale space, 즉 크기 별로 detector 가 여러 개 존재하는 경우입니다. 이런 경우는 single detector 보다 scale space 학습을 하는데 용이하지만, 각각의 detector 가 학습시키는 sample 들이 적으면 효과가 없습니다.​· 그림 c 이 논문에서 제시한 san(scale aware network) 인데요, 다른 scale 들에서 발생한 feature 들을 scale invariant 한 subspace 에 모으고,  그림 c  에서와 같이 하나의 detector 를 이용해 classification 하는 것입니다.san 은 scale space 에서의 feature 들의 관계를 파악하고, feature 간의 차이를 줄입니다. ​​​③ Overview of SAN (Multi-scale Aware Network) overview of SAN· san 을 정의하는 큰 세가지는 scale – invariant, channel – routing mechanism, 실제로 train 과 같은 실험을 할 때 사용되는 learning trick 으로 구성됩니다.​·san은 계산메모리와 시간은 더 차지하는 경향이 있지만, detection accuracy를 높인다는 점에서 장점이 있습니다.​· scale – invariant 앞 전의 그림 c 에서 보았듯이 scale 별로 생성된 feature 들의 subspace 를 만들어서 san 이라는 하나의 detector 안에 sub – network 로 각각 보내지게 됩니다. scale 별로 공통된 subspace 를 지니게 되고, 이는 sample 들 간의 scale 차이를 줄이는 것이라 detection accuracy를 높여줍니다.이러한 과정에서 결론적으로는 하나의 detector 가 사용되는 것이고, 여러 각도에서 다양한 object 의 크기를 모두 잘 인식하므로 scale – invariant 한 시스템이 생기는 것입니다.​​· channel routing mechanism spatial information 은 무시하고, channel 간의 관계만을 고려하는 것인데요, scale 변화에 따른 channel activation 의 관계에 관한 내용입니다. 구체적은 설명은 뒤에서 하겠습니다. ​​· learning trick 은 siemese architecture 를 사용하는 것입니다. 이에 관한 구체적인 내용도 뒤에서 설명하겠습니다.san은 계산메모리와 시간은 더 차지하는 경향이 있지만, detection accuracy를 높인다는 점에서 장점이 있습니다.​​​ Method1. Channel activation matrix2. Architecture of SAN3. Chanel routing mechanism4. loss function① chanel activation matrix · channel activation 정보를 얻는 방법 중에 하나로 channel activation matrix가 있습니다. 줄여서 cam 이라고 불립니다.​· cam 은 backbone 으로 사용된 resnet 101 의 residual block 5번째에서 나오는 channel wise 결과를 시각적으로 비교할 수 있게 해줍니다. ​· 그림을 보시면 a는 scale normalization 된 feature 들을 사용한 cam 의 결과인데요, x 방향은 scale , y 방향은 channel activation 결과를 의미합니다. 하얀색이 channel activation 된 부분으로 대부분의 resolution이 크게 손상되어 있지 않는 한 scale space와 무관하게 일정한 channel activation 을 볼 수 있습니다.하지만 scale normalization이 없이 cam 을 사용하면 scale에 따라 channel activation 의 결과가 크게 달라지는 것을 볼 수 있습니다. 이는 scale 에 따라  값이 달라지는 것이고, scale space 간의 non –uniformity 는 detection accuracy 를 저하합니다.​​​​② Architecture 갈색 박스가 쳐진 부분을 설명하려한다.· Architecture파란 박스가 쳐진 부분이 san의 아키텍쳐 인데요, 각 object 의 scale 정보를 잃지 않게 roi pooling을 먼저 진행합니다.그런 뒤 mini-batch 를 세가지 사이즈로 나누어서 scale 별로 object의 subspace 를 형성합니다. 빨간색 동그라미가 subspace라고 생각하면 됩니다. 그 다음 1X1 convolution 과 ReLU를 각각 진행합니다.​​​​③ Channel routing mechanism 갈색 박스가 쳐진 부분을 설명하려한다.· 그 이후의 과정은 channel routing mechanism 입니다.​·이 메커니즘을 알기 위해서는 우선 spatial information 의 의미를 알아야하는데요. spatial information 에서는 2D input 이미지와 같이 local base 정보를 가진 것을 의미합니다. 만약 이미지 픽셀을 한 줄로 늘여 놓으면 그것은 local 정보가 없기 때문에 spatial information 이 아닙니다.​·다양한 scale 의 receptive field 는 spatial information 의 불일치를 만들고, detection accuracy 를 낮춥니다. 그래서 san 에서는 roi pooling 을 이용하여 channel 로부터 정보를 학습하고, 불일치 하는 spatial resolution 은 배제하는 방법을 사용합니다.​· channel routing mechanism 의 결과로는 두 가지 scale별 subspace 의 feature 에서 /  하나의 reference scale 의 feature 로 만드는 결과가 나옵니다. ​​spatial information 의 의미에 대해 더 자세히 알고 싶으면 아래 링크로!https://cs.stackexchange.com/questions/96672/what-is-the-spatial-information-in-convolutional-neural-network What is the ""spatial information"" in convolutional neural networkdeep learning research papers always claim that deeper layers of CNN have good ""semantic information"" but poor ""spatial information"". What is the spatial information exactly. Is that some activatio...cs.stackexchange.com ​​​ · channel routing mechanism가장 왼쪽의 이미지는 spatial information 을 담은 2d 이미지 입니다. 크기가 다른 이미지들은 roi pooling을 통해 spatial information 을 잃게 합니다. 그게 가장 오른쪽에 한 줄로 펼쳐진 박스들의 그림입니다. 이는 feature space 를 나타내기도 합니다.​여기서 왜 큰 인풋 이미지는 feature space 에서 작은 주황색 박스 영역으로 표시되고 반대로 작은 인풋 이미지는 크게 표시되는지 의문이 들었었는데요. 작은 이미지는 resolution 이 낮은 경향이 있기 때문입니다.이는 object 가 이미지 상에 다른 배경이나 객체와 존재해도, 같은 값으로 인지될 가능성이 크다는 것입니다. 그래서 작은 인풋 이미지가 feature space 에서 큰 공간을 차지하는 것입니다.​이 두 가지 주황색 네모로 표시된 부분이 reference scale 즉, 중간 정도 scale 로 통합되어 하나의 feature space 만 남게 됩니다.  그리고 이 feature space 는 global average pooling 과정을 거칩니다.​​​④ Loss function  · san 이 사용되는 loss function 은 classification , box regression , scale – aware loss 로 세가지로 분류됩니다. 왼쪽 그림에서도 세 가지 loss 가 사용되는 것을 확인 가능합니다. 오른쪽 공식에서 전체 loss 는 세 가지 loss 를 더한 것으로 계산됩니다. ​· scale aware lossscale 별로 feature 차이를 적게하도록 설계되었습니다. scale aware loss 에서 틸뎃 r은 scale normalizae 된 RoI의 channel wise feature 을 의미합니다. loss function으로 smooth L1을 사용하고, 이는 ground truth 와 결과 값의 error 가 작을 경우 거의 맞는 것으로 판단하여, loss 값을 빠르게 줄이는 loss function 입니다.​· classification lossclassification loss 에서 p 는 K 번째 카테고리에 object 가 포함될 확률이고, u는 ground truth 값 입니다.  classification loss 는 logarithmic  loss를 사용합니다.​· box regression lossbox regression loss 에서 t.k 는 k 번째 class의 box regression 결과이고, v는 ground truth 값 입니다. loss function 으로는  smooth L1을 사용하고, 이는 ground truth 와 결과 값의 error 가 작을 경우 거의 맞는 것으로 판단하여, loss 값을 빠르게 줄이는 loss function 입니다.​​​​​ Experiments1. Structure2. learning parameters3. effectiveness of SAN① Structure · how they implemented SANbackbone 과 detector 사이에 san을 넣어 마치 fpn이 구조에 적용되는 것과 같은 형태입니다.​· backbone networkbackbone으로는 resnet 101을 사용하였는데, 이는 1000개의 class 를 가진 voc 데이터로 pretrained 된 백본 네트워크 입니다. 100가지의 convolutional layer와 그에 따라오는 global pooling과 1000개 클래스의 fully connected layer로 구성되어 있습니다. 하지만 이 실험에서는 feature map 을 계산하기 위해서 average pooling layer와 fully connected layer를 제거하였습니다. 그리고 resnet 101 에서 마지막 residual module 에 1X1 convolutional layer 를 feature extract layer 로 사용하고, Gaussian distribution 으로 초기화를 진행하였습니다.​· SAN as feature extractor다름으로 san 은 각기 다른 RoI 를 sub-network 에 넣고, 1x1 convolutional layer 와 relu를 순차적으로 진행합니다. 그 결과가 detector 인 R-FPN 으로 들어가게 됩니다.​· R-FPNR-FPN 에 대해 간략히 설명 드리면 region based fully connected 네트워크로, 구조를 RoI 별 sub-network 를 제외하고 fully convolutional network 구조를 통해 계산 속도를 빠르게 하는 디텍터입니다. san 에서 memory 와 시간 소모가 있으므로 R-FPN을 사용한 것으로 보입니다.​​​② Learning Parameter ​​​​③ Effectiveness of SAN san 의 유용성은 RMSE 로 측정됩니다. zi는, so는 i번째 sample 에 대한 feature 를 의미하고, so는 reference scale 을 의미합니다.Nc 는 channel 의 수를 의미하고, fs 는 san 의 sub-network 를 의미합니다.  loss 와 같이 RMSE 의 값이 줄어들면 san 이 scale 간 차이를 줄여 성공적으로 scale invariant 한 시스템을 만들었다는 것입니다.위의 공식은 san이 없을 경우의, 아래 공식은 san이 있을 경우의 RSME 를 의미합니다.​​​​ Results ​· 각각의 표는 voc 데이터에서 특정 class의 RMSE 수치를 나타내는 그래프입니다.  빨간선은 san 미포함, 파란선은 san 포함시의 RMSE 수치 입니다. · san 을 도입했을 경우 RMSE 가 더 낮은 것으로 보아 scale invariant 한 시스템을 만들었고, detection accuracy 도 높을 것이라 예상할 수 있습니다.​​​​​​​  이상 동산이었습니다!정상에서 만나요 :)​ "
          Classification / Localization / Object Detection / Segmentation ,https://blog.naver.com/dr_moms/221577007985,20190703,"안녕하세요 오늘은 Computer Vision 분야에서 Deep Learning을 사용한 대표적인 기법들인 Classification  / Localization / Object Detection / Segmentation에 대해 정리를 하려고합니다.블로그를 시작하고 처음으로 작성하는 글이라 미숙하더라도 양해 부탁드립니다.   자, 위의 사진은 각 Method에 대해 한눈에 알아볼 수 있는 유명한 사진입니다.  저 사진을 잘 보고 다음 글을 읽어주시길 바랍니다.  1. Classification​Computer Vision에 관련된 분야의 Deep Learning에서 Image Classification 이란, 입력으로 주어진 이미지 안의 객체를 분류하는 것입니다. 위의 사진처럼 고양이 이미지를 컴퓨터가 이 이미지는 고양이 사진이라고 분류를 하는 것입니다. 마찬가지고 사자 사진이 들어오면 이 이미지는 사자라고 분류를 하는 것이죠. 여기서 컴퓨터가 분류한 사자, 또는 고양이를 Class, Label이라는 용어를 사용합니다. 대표적으로 AlexNet, VGG, ResNet 등의 Convolution Neural Network(CNN)가 유명한 이미지 분류 모델입니다.  2. Localization​단순한 분류에서 그치지 않고 이미지 안의 객체가 어느 위치에 있는지를 알아내는 것을 Localization이라고 합니다. 위의 사진처럼 고양이에 빨간색 네모 박스가 쳐져 있는 걸 확인할 수 있죠. 이 박스를 Bounding Box라고 하는데, Bounding Box는 대부분 박스의 중심 좌표 center_x, center_y와 너비와 높이인 width와 height의 정보를 가지고 이미지 위에 그려주게 됩니다.  (x, y, w, h) 혹은 (cx, cy, w, h)로 표현되는 경우가 많습니다.  3. Object Detection​Object Detection은 Localization의 의미로만 가지고 말하는 경우도 종종 있지만, 딥러닝에서 Object Detection은 위의 사진처럼 Multi Class에 대해 Classification과 Localization이 동시에 수행되는 것을 말합니다. 사진에서 고양이, 개, 오리에 대한 Class와 Bounding Box를 모두 출력하고 있는 것처럼 말이죠.하지만 필요에 따라 특정 Object만 학습을 시켜 검출하는 경우도 있습니다.​      Class를 고려하지 않고 객체가 있을법한 위치에 Localization만 수행하는 경우는 Region Proposal이라고 합니다. Object Detection은 보통 2가지로 크게 나누어지는데 Region Proposal을 수행하고 그것을 기반으로 Object Detection을 하는 2-stage Object Detection 방식과 Region Proposal 단계를 수행하지 않고 바로 Object Detection을 수행하는 것을 1-stage Object Detection이라고 합니다. ​2-stage 방식은 느리지만 정확하다는 특징이 있고 1-stage 방식은 빠르지만 정확성이 2-stage 방식보다 떨어진다는 특징이 있습니다. 1-stage와 2-stage 방식에 대해선 다음에 자세히 알아보도록 합시다.  4. Image Segmentation​위의 사진에는 Instance Segmantation이라고 되어있지만 100% 맞는 설명은 아닙니다. Object Detection을 통해 검출한 객체를 Object의 형상에 따라서 픽셀 별로 분류를 하는 것을 Image Segmentation이라고 합니다. ​Segmentation에는 Semantic Segmentation과 Instance Segmentation이 있는데 Semantic Segmentation은 단순히 이미지의 모든 픽셀을 Class 별로 구분하는 것입니다. 이미지를 보시면 고양이는 빨간색 테두리로 강아지는 파란색, 오리는 초록색으로 되어있습니다. 강아지와 오리는 객체가 Class 별로 하나기 때문에 문제가 없지만 고양이는 객체가 2개이지만, 단순히 고양이라는 Class로 분류를 할 뿐 고양이 2마리를 따로 고양이 1, 고양이 2로 분류하고 있지는 않습니다. 명백히 말하면 위 이미지는 Semantic Segmantation이라고 할 수 있죠. ​Instance Segmantation은 저 고양이 2마리를 고양이 1, 고양이 2로 도 분류를 하는 것을 의미합니다. 같은 Class라도 서로 다른 Instance를 구분해 주는 것이죠. 밑에 사진을 보시면 이해가 더 빠르실 겁니다.    이상으로 Classification / Localization / Object Detection / Segmentation에 대하여 알아보았습니다. ​감사합니다. "
무이메이커스_딥러닝을 활용한 객체 인식 프로젝트 (Object Detection) (1/2) ,https://blog.naver.com/honeycomb-tech/221566397529,20190620,"프로젝트 진행 순서 (1/2)1. 객체 인식 (Object Detection) 개요2. 커스텀 이미지 데이터셋 만들기 (Annotation)3. 이미지 데이터 전처리 4. 데이터에 적합한 딥러닝 모델 생성5. 모델 평가 및 시각화 (Evaluation and Visualization)6. 실생활 적용Honeycomb-무이메이커스안녕하세요 헬스케어 제품 개발회사 허니컴의 무이메이커스페이스 입니다.저희는 4차산업혁명을 맞이하여 딥러닝을 접목시킨 제품 개발을 위해 다양한 프로젝트를 수행하고,이를 활용하여 인공지능 (AI) 을 지닌 다양한 헬스케어 제품을 생산하는데 그 목적이 있습니다.​이번 시간에는 딥러닝을 활용한 객체 인식 (Object Detection) 프로젝트를 소개하고자 합니다.목표는 5가지 종류의 꽃에 대한 이미지 (Flower Image) 를 분류 (Classification) 하고,동시에 해당 꽃들의 위치까지 추정해내는 것으로데이터셋 (Dataset) 으로는 캐글 (Kaggle) 에서 제공되는 Flower Recognition Dataset 을 사용하였습니다.​개발 환경은 윈도우 (Window), 언어는 파이썬 (Python), 라이브러리는 파이토치 (Pytorch) 를 사용합니다.​https://www.kaggle.com/alxmamaev/flowers-recognition​ Flowers RecognitionThis dataset contains labeled 4242 images of flowers.www.kaggle.com  총 4242 장으로 구성된 5가지 종류의 꽃 이미지 (Flower Image) ​1. 객체 인식 (Object Detection) 개요​이번주 저희의 목표는 컴퓨터 비전 (Computer Vision) 분야에 속하는 이미지에 관련된 문제입니다.4차산업혁명의 핵심인 딥러닝 기술은 현재까지 해당 분야에서 가장 큰 발전을 이뤘다고 볼 수 있으며,크게 아래의 3가지 분야로 연구가 진행되고 있습니다.  CS231n 에 소개된 Computer Vision Task ​목표는 해당 사진 속 고양이를 컴퓨터가 인식하는 것입니다.가장 기본적으론 고양이라는 사물의 존재 여부를 분류하는 Classification,고양이라는 사물의 위치를 찾아냄과 동시에 여러가지 사물을 인식하는 Detection,고양이라는 사물의 형태를 찾아내는 Segmentation 이 존재하며,​이번 프로젝트는 4차산업혁명과 딥러닝의 가능성을 처음으로 대두시킨 CNN (Convolution Neural Network) 모델 기반의 Object Detection 을 파이썬 (Python) 과 파이토치 (Pytorch) 로 진행하였습니다.​  CS231n 에 소개된 Object Detection Object Detection 을 통해 컴퓨터는 주어진 이미지 속에 존재하는 객체의 위치를 파악할 수 있습니다.동시에 분류 (Classification) 로는 해결할 수 없는 하나의 이미지 속 다양한 객체들을 인식할 수 있는Multi Label Classification 을 할 수 있는 방법이기도 합니다.​기본적인 Object Detection 의 컨셉은 Selective Search 와 같은 Region Proposal 알고리즘을 통해주어진 이미지 안에서 4가지 요소들의 가중치로 계산된 유사 픽셀들을 모아 Region 을 만듭니다.(http://www.huppelen.nl/publications/selectiveSearchDraft.pdf)  Region 을 생성해 나가는 Selective Search 과정 처음에는 수많은 작은 Region 들을 생성하였으나, Greedy 알고리즘을 통해 점차 유사도에 따라 합쳐져최종적으로 오른쪽과 같이 하나의 객체에 하나의 Region 이 형성되는 Bottom-up 방식으로 진행됩니다.​이후 생성된 Region 들에 분류 (Classification) 를 진행 해당 객체가 무엇인지 판별합니다.이 과정에서 딥러닝을 도입하며 Region-based CNN (R-CNN, https://arxiv.org/abs/1311.2524), OverFeat(https://arxiv.org/abs/1312.6229) 등이 탄생하고 발전하기 시작했습니다.  딥러닝이 도입된 Object Detection 의 시초라 볼 수 있는 R-CNN 과정 마지막으로 객체에 대한 정확한 위치를 구하기 위해 Annotation 된 Bounding Box 정보를 이용합니다.​Bounding Box 는 다양한 Annotation Tool 을 이용해 만들어 낸 위치 정보를 지닌 Label 이며,Training Dataset 에 존재하는 Ground Truth 를 통해 위에서 구한 Region 정보를 Mapping 시키도록Regression 을 통해 학습시켜 보다 정확한 Intersection Over Union (IoU) 성능을 구하도록 도와줍니다.아래 그림에서 검은색 Bounding Box 를 True, 빨간색 Bouding Box 를 Pred 라고 가정했을 때,  다음과 같은 수식으로 구성되어 1에 가까울수록 좋은 성능임을 나타냅니다. Bouding Box Mapping (좌) 및 IoU 성능 평가 방법 (우)  IoU 수치에 따른 Bounding Box 성능 결과 Object Detection 은 이렇게 Region 을 선별해내는 Region Proposal,선별된 Region 으로부터 객체를 분류하는 Classification,Annotation 된 Bounding Box 의 위치정보를 통한 Regression 으로 구성되며,​이를 동시에 수행할 것인지 단계별로 수행할 것인지에 따른 Trade-off 가 존재합니다.​주로 정확도를 우선시 해야 하는 경우, 단계별로 수행하는 R-CNN 계열 알고리즘들이 사용되고속도를 우선시 해야 하는 경우, 동시에 수행하는 Yolo, SSD 와 같은 알고리즘들이 사용됩니다.​그렇기에 실무에서는 주어진 데이터와 목적에 맞는 딥러닝 모델을 선정하여 사용하는 것이 중요하고,저희는 R-CNN 계열과 Yolo, 두 가지 방법 모두 접근하여 프로젝트를 진행해보았습니다.​​​2. 커스텀 이미지 데이터셋 만들기 (Annotation)​R-CNN 과 Yolo 같은 딥러닝 모델에 적용하기 앞서 객체의 위치 정보인 Bounding Box 가 필요합니다.그러나 대부분 일반적인 이미지들에는 그러한 정보가 표시되어있지 않습니다.따라서 이미지 속 객체의 Bounding Box 를 표시해주는 Annotation 과정을 수행합니다.저희는 파이썬 (Python) 기반의 툴인 labelImg 를 사용하였습니다. pip install labelImg https://github.com/tzutalin/labelImg tzutalin/labelImg:metal: LabelImg is a graphical image annotation tool and label object bounding boxes in images - tzutalin/labelImggithub.com 해당 파이썬 (Python) 툴은 .xml 파일인 PASCAL VOC 포맷과 .txt 파일은 Yolo 포맷을 지원합니다.파일 형태들은 다른 파이썬 (Python) 툴을 통해 간단하게 바꿀 수 있으므로,기본 형태인 Pascal VOC 형태로 Annotation 을 수행하였습니다.이미 가공된 데이터셋이 아닌 Annotation 을 통해 커스텀 데이터셋을 구축하는것이 목표이므로,labelImg/data/predefined_classes.txt 파일을 삭제합니다. 이후 파이썬 (Python) 을 통해 labelImg 를 실행시킵니다.  labelImg Tool 화면 Opem Dir 를 통해 Flower 데이터셋이 있는 폴더를 선택합니다.  labelImg Tool : 이미지가 담긴 폴더 불러오기 이미지들이 불러졌다면 수작업을 통해 Annotation 작업을 진행합니다.다소 시간 소요가 되므로 각 Class 를 잘 대표하는 20가지의 사진을 선별해 작업을 진행했습니다.아래와 같은 단축키를 이용하면 보다 편하게 작업을 수행할 수 있습니다.​W : Bounding Box 지정D : 다음 이미지로 이동A : 이전 이미지로 이동Ctrl+s : 지정된 Bounding Box 정보 저장​  labelImg Tool : Bounding Box 를 그려 Label 지정 (Annotation) ​3. 데이터 전처리 (Preprocessing)​딥러닝 모델이 잘 학습하기 위해선 준비된 데이터들을 모델에 맞게 정제하는 과정이 필요합니다.이를 데이터 전처리 과정 (Preprocessing) 이라 부르며, 이미지 데이터 의 경우 Scaling, Normalizing, Augmentation 등의 과정을 거칩니다.​Detection 의 경우 이미지 뿐 아니라 Annotation 된 이미지의 Bounding Box 까지 함께 처리가 되야하므로해당 작업을 수행해줄 코드가 필요합니다.​따라서 저희는 파이썬 (Python) 라이브러리 imgaug 를 사용하여 전처리를 진행하였습니다.​※ 설치 과정에서 파이썬 (Python) 관련 Shapley 문제가 발생할 수 있습니다.해당 문제는 https://www.lfd.uci.edu/~gohlke/pythonlibs/#shapely 링크의 Shapely 부분에서자신의 운영체제에 맞는 버전을 선택하여 다운받고 설치하신 후,다시 파이썬 (Python) 을 통해 imgaug 를 Install 하시면 됩니다. pip install imgaug https://github.com/aleju/imgaug aleju/imgaugImage augmentation for machine learning experiments. - aleju/imgauggithub.com imgaug 의 경우 분류 (Classification) 프로젝트에서 적용한 Augmentor 와 유사한 방식으로전처리된 이미지 파일과 해당 Bounding Box 파일을 생성해줍니다. 원본 Daisy Image (좌) 와 Preprocessing 을 거친 Image (우) Augmentation 결과 이미지와 Bounding Box 가 함께 전처리 됐음을 확인할 수 있습니다.​이번주에는 파이썬 (Python) 의 툴박스들을 이용해 커스텀 데이터셋 구축 및 전처리를 진행하였습니다.다음주는 실제 Detection 을 하기 위한 딥러닝 모델들에 적용하여 성능 평가를 진행하겠습니다.​........시제품 제작 문의​   ​ "
"[Darknet , Object Detection] 사용법 ",https://blog.naver.com/steve6238/221999518543,20200613,"​Darknet 은 다양한 환경에서 빌드가 가능합니다. 다만 속도차이가 심하니 주의해주세요Darknet 을 이용하면 누구나 간단하게 Object Detection 알고리즘 개발이 가능합니다.​​1. Darknet 다운로드​저는 리눅스 환경에서 진행했습니다. git clone https://github.com/pjreddie/darknetcd darknet 2. Make 파일 수정​ vi Makefile--자신에게 맞게 수정--Make  Makefile을 열었을때 가장 상단에 다섯개의 옵션이 있습니다.만약 NVIDIA GPU사용 , CUDA 와 Cudnn 경로설정이 제대로 되 있으면 GPU=1 , CUDNN=1 을 설정하고 빌드해줍니다.​만약 OPENCV사용시 OPENCV=1 을 체크해주시면 됩니다.나는 GPU도없고 Opencv도 없으면 그냥 Make해주면 됩니다.​그리고 Make해줍니다.​다음으로 미리 학습된 weight을 한번 다운받겠습니다. (Test를 위해)​https://pjreddie.com/darknet/yolo/ YOLO: Real-Time Object DetectionYOLO: Real-Time Object Detection You only look once (YOLO) is a state-of-the-art, real-time object detection system. On a Pascal Titan X it processes images at 30 FPS and has a mAP of 57.9% on COCO test-dev. Comparison to Other Detectors YOLOv3 is extremely fast and accurate. In mAP measured at .5 I...pjreddie.com 제 서버에서는 이상하게 wget이 중간에 계속 끊겨서 직접 사이트에서 다운 후 서버로 옮겼습니다.​​ ./darknet detect cfg/yolov3-tiny.cfg yolov3-tiny.weights ./data/horses.jpg 그리고 명령어로 탐지를 시작해보겠습니다.가운데있는 weight이 여러분이 다운받은 weight을 넣으면 됩니다/data/horses.jpg는 예시 파일입니다.​컴파일 버전에 따라  Yolo V3, Yolo V3-tiny 의 속도차이를 한번 비교해보겠습니다.​1.CUDA + Cudnn 사용컴파일 시 GPU와 CUDNN을 1로 바꿨을 경우입니다.Cudnn 은 행렬연산 등 딥 러닝 연산을 빠르게 수행 할 수 있는 라이브러리를 제공해주기 때문에 훨씬 빠르게 계산 가능합니다.​1) YOLO_V3 tiny  + GPU  실행결과입니다.  결과를 보면 일부 말은 마킹이 안되거나 COW로  인식하네요.​​​2) YOLO v3 + GPU다음은 YOLO v3 입니다속도차이 어마무시합니다. 물론 둘다 빠르지만 비율로 따지면 10배가량 더 느립니다.   확실히 좀 더 높은 인식률을 가지고 있습니다. 정확한 마킹 + 높은 %를 보여주네요​​​다음은 CPU 버전입니다. Makefile에서 아무것도 1로 바꾸지 않은 상태로 진행하겠습니다.1) YOLOv3 -tiny + CPU  tiny 모델입니다. 성능은 동일하지만 속도차이가 느껴집니다..​​2) YOLO v3 + GPU  24초가 걸립니다 ..​​여기서 GPU의 힘을 느낄 수 있네요 .. ​다음에는 직접 데이터셋을 구축해 학습시켜보겠습니다. "
[AWS] Sagemaker Image Object Detection 예제(Python) ,https://blog.naver.com/nan17a/221966795926,20200517,"Sagemaker를 활용한 Object Detection 예제입니다.​Python 개발방식은 ""Amazon SageMaker Python SDK""를 이용하는 방법과 "" AWS SDK(Boto3)""를 이용하는 방법 두가지가 있는듯 하다. 이중 ""Amazon SageMaker Python SDK"" 가 코드가 더 간단하다.​​1. import import sagemakerfrom sagemaker import get_execution_rolefrom sagemaker.amazon.amazon_estimator import get_image_uri 2. role 설정​ * PC에서 직접 실행할 때는 IAM의 role 이름을 직접 적어주면 됩니다. role = get_execution_role() 3. 학습 이미지/모델 설정- 학습 이미지로 object-detection 을 선택하고, 관련 설정을 지정합니다.- train_instance_type 에 ml  타입을 사용하기 위해서는 limit을 해제해야 합니다.​https://blog.naver.com/nan17a/221951132373 [AWS] Sagemaker notebook instance limit increasesagemaker notebook에서 학습용 ml.* 인스턴스 타입을 사용하고자 할 때 limit을 해제해야 했다.​몇가지 ...blog.naver.com sess = sagemaker.Session()training_image = get_image_uri(sess.boto_region_name, 'object-detection', repo_version=""latest"")od_model = sagemaker.estimator.Estimator(training_image,                                         role,                                          train_instance_count=1,                                          train_instance_type='ml.p2.xlarge',                                         train_volume_size = 50,                                         train_max_run = 360000,                                         input_mode= 'File',                                         output_path=s3_output_location,                                         sagemaker_session=sess) 4.  s3 설정학습이미지와 annotation json을 생성/업로드 후학습 이미지, 검증 이미지, annotation 정보, 결과 파일의 경로를 설정합니다.​https://blog.naver.com/nan17a/221966706220 [AWS] SageMaker Object Detection 데이터 소스 만들기 - PASCAL VOC format을 이미지 형식 annotation JSON 으로 변환Sagemaker 의 오브젝트 디텍션 알고리즘을 간단하게 테스트 한 기록을 남깁니다.​https://docs.aws.amazo...blog.naver.com bucket = sess.default_bucket() # sagemaker-[region]-[id number]prefix = 'DEMO-ObjectDetection'train_channel = prefix + '/train'validation_channel = prefix + '/validation'train_annotation_channel = prefix + '/train_annotation'validation_annotation_channel = prefix + '/validation_annotation's3_train_data = 's3://{}/{}'.format(bucket, train_channel)s3_validation_data = 's3://{}/{}'.format(bucket, validation_channel)s3_train_annotation = 's3://{}/{}'.format(bucket, train_annotation_channel)s3_validation_annotation = 's3://{}/{}'.format(bucket, validation_annotation_channel)s3_output_location = 's3://{}/{}/output'.format(bucket, prefix) 5. 하이퍼파라메터 설정- base_network는 VGG-16 혹은 resnet-50- num_classes 는 학습용 데이터의 카테고리 숫자에 적합하게 수정,- epochs, learning_rate 는 적당하게...- num_training_samples 는 학습 이미지 갯수 od_model.set_hyperparameters(base_network='resnet-50',                             use_pretrained_model=1,                             num_classes=2,                                mini_batch_size=16,                             epochs=100,                             learning_rate=0.01,                             lr_scheduler_step='3,6',                             lr_scheduler_factor=0.1,                             optimizer='sgd',                             momentum=0.9,                             weight_decay=0.0005,                             overlap_threshold=0.5,                             nms_threshold=0.45,                             image_shape=300,                             label_width=350,                             num_training_samples=100) 6. 학습- 실행하면 Sagemaker 웹콘솔의 Train 탭에서 신규 생성된 배치가 실행됨을 확인할 수 있음.(학습중으로 표시됨) train_data = sagemaker.session.s3_input(s3_train_data, distribution='FullyReplicated',                         content_type='image/jpeg', s3_data_type='S3Prefix')validation_data = sagemaker.session.s3_input(s3_validation_data, distribution='FullyReplicated',                              content_type='image/jpeg', s3_data_type='S3Prefix')train_annotation = sagemaker.session.s3_input(s3_train_annotation, distribution='FullyReplicated',                              content_type='image/jpeg', s3_data_type='S3Prefix')validation_annotation = sagemaker.session.s3_input(s3_validation_annotation, distribution='FullyReplicated',                              content_type='image/jpeg', s3_data_type='S3Prefix')data_channels = {'train': train_data, 'validation': validation_data,                  'train_annotation': train_annotation, 'validation_annotation':validation_annotation}od_model.fit(inputs=data_channels, logs=True)   7. Deploy- 실행하면 Sagemaker 웹콘솔의 Model 및 endpoint가 자동 생성된다.  object_detector = od_model.deploy(initial_instance_count = 1,                                 instance_type = 'ml.m4.xlarge') ​​ "
Tensorflow 를 활용한 Object detection ,https://blog.naver.com/kkang9901/221799206790,20200207,"몇일동안 Mac OS에 Object detection을 설치하느라 3일을 고생했다. 알수도 없는 에러 덩어리와 오타, 터미널에서의 rm 실수로 인해 가상환경을 몇번을 밀어버렸는지 모르겠다.​바로 프로젝트 환경 구성으로 넘어가겠다.​프로젝트 환경 구성을 위해 anaconda3 navigatior는 거의 필수적이라고 생각한다. 가장 깔끔하고 충돌이 없고, 문제 발생시 삭제 후 다시 만들면 되니 이만한 프로그램은 없다고 생각한다.​conda terminal에서 아래와 같이 명령어를 작성한다.​ conda create -n [가상환경이름] python=3.5conda activate [가상환경이름]exam)conda create -n tensorflow python=3.5conda activate tensorflow 위 명령어를 실행하면 터미널 맨 앞에 있던 bash가 tensorflow로 활성화 되는것을 볼 수 있다. 다시 기존으로 돌아오려면  conda deactivate 를 입력하면, tensorflow(가상환경이름) --> bash 로 돌아오는 것을 볼 수 있다.​이제 필요한 pip를 설치해보자. 이제 앞에서 작동되는 모든 명령어들은 가상환경이 작동된 상태에서 실행되어야한다. pip install --upgrade tensorflow==1.13.1conda install -c menpo opencvconda install -c conda-forge imageiopip install tqdmpip install moviepypip install numpypip install pillowpip install matplotlibpip install Cythonpip install contextlib2pip install lxml tensorflow는 1.13.1 버전이 가장 안정적이라고 들었다. 다른 버전을 요구하는 사용자는 다른버전을 설치해도 괜찮지만, tensorflow가 버전 2를 넘어가면 오류가 발생하기 때문에 2버전 이하로 설치하긴 바란다.​이제 새로운 폴더에 개발환경을 만들어보자. 원하는 폴더로 terminal 에서 이동한다. mkdir [원하는 폴더 이름]cd [원하는 폴더 이름]git clone https://github.com/tensorflow/models ​이후 protobuf를 설치해야한다. mac os의 터미널에서 brew install protobuf 를 설치한다.​그 이후 protobuf를 컴파일 해야한다.​이때 주의할 사항은, github에서 다운로드 받은 models의 폴더에 가보면, research라는 폴더가 있을 것이다.​터미널의 디렉토리를 [원하는 폴더 이름]/models/research 로 이동하고 실행해야 한다.​ protoc object_detection/protos/*.proto --python_out=. 위 명령어를 실행하고, 디렉토리를 [원하는 폴더 이름]/models/research/object_detection/protos 로 이동해보면, pb2 라는 파일들이 생성되어 있으면 성공이다.​이 후 다시 디렉토리를 [원하는 폴더 이름]/models/research 로 이동한다.​ export PYTHONPATH=$PYTHONPATH:`pwd`:`pwd`/slim 의 명령어를 작성해야하는데. 이때 꼭 주의해야 할 사항은 'pwd'를 그대로 입력하는 것이 아니라, ​전자의 'pwd'는 [원하는 폴더 이름]/models/research후자의 'pwd'는 [원하는 폴더 이름]/models/research/slim 으로 설정한다.​그러면 전체 명령어는​ export PYTHONPATH=$PYTHONPATH:`[원하는 폴더 이름]/models/research`:`[원하는 폴더 이름]/models/research/slim' 과 같이 작성된다. 이 PYTHONPATH는 터미널을 새로 킬때마다 작성해줘야 하니, 한 곳에 메모했다가 작성하는 방법도 좋다.​이게 귀찮다면, bashrc 파일에 작성하는 법이 있다. 구글링하면 바로 나오니 한번 시도해보길 바란다.​이제 설치과정은 끝났으니 정상적으로 작동하는지 테스트해보자.​[원하는 폴더 이름]/models/research 디렉토리로 이동해 다음과 같은 명령어를 쳐보자.​ python object_detection/builders/model_builder_test.py   위 사진과 같이 OK가 뜨면 정상적으로 설치가 되었다.​혹시라도 오류가 발생한다면 댓글 달아주시면 빠르게 피드백하겠습니다. "
밀집환경에 대한 Object Detection 결과 ,https://blog.naver.com/mikangel/222607624744,20211229,"환경tensorflow-gpu==1.15.4numpy==1.19.2opencv-python==3.1.0.5tqdm==4.50.2pandas==0.23.4​윈도우에서 실행할 경우signal 관련 library 함수 pass 처리pandas 관련 에러 .as_matrix(BOX_CONSTANTS) 는 [BOX_CONSTANTS].values 로 처리​​Inference 결과 (이미지 사이즈 2448x3264, 12MB) ​ "
YOLO v3 : Object Detection (한양대 에리카) ,https://blog.naver.com/rldhks6899/221519399816,20190422,"해당 포스팅은 아래 YOLO v3 Github 실습 예제 코드를 기반으로 작성되었습니다.> https://github.com/ayooshkathuria/pytorch-yolo-v3#on-video ayooshkathuria/pytorch-yolo-v3A PyTorch implementation of the YOLO v3 object detection algorithm - ayooshkathuria/pytorch-yolo-v3github.com ​* 본인의 컴퓨터 스펙은 다음과 같다. GPUNVIDIA GeForce GTX 1050ProcessorIntel(R) Core(TM i5-7300HQ CPU @ 2.50 GHz (4 CPUs)RAM8GB (8192MB) RAM * YOLO Argument 명령어 이름 및 역할들은 아래와 같다.1. --images : 검출을 시행할 이미지 파일의 경로 및 이름을 적어줍니다.→ ex) python detect.py --images dog-cycle-car.png --det det​1-1. --video : 검출을 시행할 비디오 파일의 경로 및 이름을 적어줍니다.→ ex) python video_demo.py --video test.avi --det det​2. --det : 검출한 객체 정보를 저장할 공간으로 정의한 디렉터리가 없으면 새로 생성한다.→ ex) python detect.py --images dog-cycle-car.png --det det을 최초 실행하면 det 폴더가 생성됨​3. --bs : Batch 크기 정하기 (Default : 1)​4. --confidence : 객체의 신뢰도를 설정 (Default : 0.5)​5. --nms_thresh : Non-Maximum Suppression(NMS) for Object Detection 임계치 설정 (Default : 0.4)​6. --cfg : 신경망 config file 설정 (Default 경로 : cfg/yolov3.cfg)​7. --weights : 미리 학습된 weights 파일의 경로 (Default 경로 : yolov3.weights)→ Default 값인 경우 동일한 위치에 weights 파일이 있어야 인식됩니다. ​8. --repo : Input resolution of the network. Increase to increase accuracy. Decrease to increase speed.→ (OpenCV 기본적인 이론은 생략합니다.)​9. --scales : Scales to use for detection→ (OpenCV 기본적인 이론은 생략합니다.)​* video_demo.py : Video Input은 OpenCV 특성상 반드시 .avi 포맷을 가져야한다.→ .avi가 아닌 .mp4같은 포맷인 경우 다음과 같은 오류가 뜬다 : ​<AssertionError: Cannot capture source​>→ 아쉬운 점이 있는데 영상의 이름을 video.avi으로 해둬야 인식이 가능하게 만들어놨다.   ​*detect.py에서 이미지의 확장자는 .png / .jpeg / .jpg 만 인식된다.   ​* video_demo.py를 돌려 한양대 에리카 캠퍼스 도로변을 인식하는 모습 → 기본 검출 임계점 0.25로 되어있는 경우 인식률 자체는 좋지못한 편이였다. 나무를 Cow, Sheep으로 인식하거나 교통 표지판을 무조건 Stop sign으로 판단하는 등 실제 상황이였으면 매우 위험한 Detection을 감행하기도 했으니. 이런 문제점이 발생한 이유는 검출 임계점도 높지않았고 영상의 화질도 낮았던 것이 한 몫했다고 생각한다.            - ​영상 참고 : https://youtu.be/V0Rj1fyZtM0​* Detection Threshold를 높힌 후 video_demo.py를 재실행 (Detection Threshold : 0.6)Object가 엄청 많은 경우 영상 자체에 렉이 걸리기도했고 내 GPU는 전투기 소리를 내며 더욱 비명을 질러댔다.하지만 좀 더 정확한 검출을 위해 Detection Threshold를 높혀 다시 테스트해보기로 했다. ㅋㅋ우선 Detection Threshold를 변경하기 위해서는 YOLO 예제로 나와있는 아래 코드를 분석해봤다.> ./darknet detect cfg/yolov3.cfg yolov3.weights data/dog.jpg -thresh 0 ​일단 결론적으로 thresh값을 찾아야하는 것이니 YOLO_v3 폴더안에 위치한 파일들 중 thresh라는 이름의 변수를 가진 파일들을 찾아 내가 찾고자하는 값과 유사한 지를 생각해본다. 일단 명령어 안에 cfg/yolov3.cfg 파일이 언급되어있으니 cfg 파일부터 에디터를 이용해 ""thresh""라는 이름의 변수를 search했다.​역시 cfg파일 내에 ""thresh""라는 변수명을 사용하고 있었다. 근데 이게 무슨 일인가? 이미 0.6으로 설정돼있었다.이해를 하도록 머리를 굴려봤을 때 Default thresh값은 0.25가 아닌 0.6이 아니였나라고 납득을 하였다.   ​그래도 모르니 cfg파일이 아닌 bbox.py / cam_demo.py / darknet.py 등 다른 파일에서도 검색을 시도했다.근데 내가 원했던 Detection Threshold값은 없었고 nms_threshold 변수명만 존재했었다.내 결론상 yolov3.cfg 파일에서 찾은 thresh = .6 값이 내가 찾던 Detection Threshold가 맞는 듯 하다.​→ 따라서 나는 에리카 도로변에서 Object Detection을 시행했을 때 정확도가 다소 떨어지는 것은 YOLO v3의 Detection Threshold가 낮아서 생긴 것이 아니라 단지 영상의 화질이 좋지못해 생긴 문제라고 판단했다. "
YOLOv5로 내가 원하는 물체 학습시키기 (Object detection using custum dataset) ,https://blog.naver.com/cyberxirex/222582409508,20211130,"#YOLOv5 #YOLO #custum_dataset #custum #python #pytorch #파이썬 #파이토치 #욜로 #object_detection #물체인식 #커스텀 #windows10 #윈도우 #google_colab #colab #내_데이터로_학습 #machine_learning #머신러닝 #두_종류 #두가지 #강아지 #개 #동물​이전 블로그에서는 Google Colab에 YOLOv5를 설치하고, YOLO에서 제공해주는 dataset으로 학습을 시키는 것까지 진행하였다.​오늘은 나만의 데이터를 이용하여, 내가 원하는 물체를 dectection할 수 있는 방법을 알아보자. (더불어 Google Colab이 아닌 local PC에 YOLO를 설치하여 학습시키는 것도 진행해보자) ​ Dataset 준비내가 원하는 물체를 학습시키기 위해서는, 컴퓨터에게 정답지를 만들어줘야한다.  여기서는 우리집 강아지 2마리를 구분하는 것을 예제로 하겠다.우선, 아래 사진에서처럼 강아지 얼굴 위치를 체크해주고, label을 붙여줘야한다. 우리집 강아지(옥자), 인상쓰면 무서움그렇다면 어떻게 위치와 lebeling을 할 수 있을까?​여러 방법이 있지만, 여기서는 makesense.ai 라는 웹툴을 이용하도록 한다. ​https://makesense.ai Make SenseOpen source and free to use under GPLv3 license No advanced installation required, just open up your browser We don't store your images, because we don't send them anywhere Support multiple label types - rects, lines, points and polygons Support output file formats like YOLO, VOC XML, VGG JSON, CSV ...makesense.ai 아래 그림에서와 같이 Get started를 클릭한다. 그러면 이미지를 업로드하라고 박스가 하나 나올텐데, 윈도우에서 파일을 옮기는 것과 동일하게 여러 파일을 드래그해서 저 상자위로 가져다 놓는다.  그림파일을 올리면 아래와 같이 object detection을 클릭한다. ​그러면 다음과 같은 조그만 창이 하나 뜨게 되는데, 여기서 label을 먼저 지정해주고, start project를 누른다.(Label을 추가할때는 엔터를 누르면 추가된다) 여기서 우리는 강아지 얼굴을 구분할 것이므로, 다음과 같이 지정하도록 하겠다. (옥자와 예쁜이) 자, 이제 실제로 아래처럼 이미지에서 영역을 지정하고 labeling을 진행해보자. (여기서는 이미지가 하나이지만 라벨링이 끝나면 다른 사진을 클릭하여 이 작업을 반복한다.) 자 모든 이미지에 labeling이 완료되었으면, 아래 그림과 같이 Actions >> Expoert Annotations를 클릭하자. 그러면 아래와 같이 창이하나 뜰텐데, 우리는 YOLO를 이용할 것이니 YOLO format으로 export하자. 자, 이제 export를 누르면 자동으로 압축파일이 다운로드 될 것이다.​이제, 우리는 학습에 사용할 dataset을 완성하였다!!!​ Google Colab을 이용하여, 나만의 custum dataset으로 학습하기이전에 진행했던 것처럼 Google Colab에서 제공해주는 YOLO tutorial 페이지로 가보자.https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb YOLOv5 TutorialRun, share, and edit Python notebookscolab.research.google.com 자, 여기서 우리는 custum dataset으로 학습을 할 것이기 때문에 몇가지 수정을 해주어야한다.1) Training dataset upload2) 나만의 모델 설정​우선 training dataset을 업로드해보자.train_data라는 폴더에 images와 labels라는 폴더를 만들고, 각각의 폴더안에는 train과 val이라는 폴더를 만들어보자. 말은 복잡한데 아래와 같은 구조로 폴더를 만들면 된다 여기서 image에는 labeling을 마친 이미지들을 넣어주고, labels에는 아까 makesense.ai에서 다운받은 압축파일을 풀어서 집어넣으면 된다. (*참고로 val은 학습결과를 validate 하는데 쓰이는 dataset이다. Training에 사용된 이미지로 test하면 당연히 좋은 결과를 얻을 수 밖에 없으므로 training에 사용되지 않은 이미지로 학습결과를 test한다. 여기서는 전체 이미지 갯수가 많이 않아서 전체 이미지중 대략 5% 정도만을 val로 할당하였다.)​ 자, 폴더를 만들고 안에 파일까지 집어넣었다면, 이제 이 파일을 Google Colab에 업로드해보자.가장 쉬운 방법은 1개의 압축파일로 만들어 드래그 앤 드롭으로 파일을 업로드하고, Colab에서 압축을 푸는 것이다. ​아래와 같이 드래그 앤 드롭으로 파일을 옮기고 업로드가 다 될때까지 기다리자. (생각보다 속도가 느리다) 완료되면 train_data.zip 파일이 위의 그림처럼 보일 것이다. ​이제 압축을 풀고 YOLO를 git으로 설치하자. !unzip -q ./train_data.zip -d ./ #unzip train_data!git clone https://github.com/ultralytics/yolov5  # clone%cd yolov5%pip install -qr requirements.txt  # installfrom yolov5 import utilsdisplay = utils.notebook_init()  # checks train_data를 upzip하는 script를 맨 위에 한 줄 추가하고 실행시켜보자 (shift+enter) 위의 그림처럼 train_data 폴더가 생성되었고, YOLOv5도 잘 다운 되었음을 확인할 수 있다. ​자 이제 우리가 업로드한 데이터를 이용해 training 할 수 있도록 YOLOv5 학습 모델을 조금 수정해보자.​ 위의 그림처럼 yolov5 >> data 폴더의 coco128.yaml을 다운로드해보자. (사실 어떤 yaml파일이라도 상관없다. 여기서 다운로드하는 이유는 Colab 창에서 저 파일을 수정할 수 없기때문이다.)​파일을 다운로드하여 메모장으로 열어보면 다음과 같다. 여기서 수정후, 필요없는 부분을 좀 떼어낼 것이다. ​※ 우선 우리가 업로드한 이미지파일들에 대한 path를 새로 지정해주자. (train_data라는 폴더를 업로드하였다.) # Train/val/test sets as 1) dir: path/to/imgs, 2) file: path/to/imgs.txt, or 3) list: [path/to/imgs1, path/to/imgs2, ..]path: ../train_data  # dataset root dirtrain: images/train  # train images (relative to 'path') 128 imagesval: images/val  # val images (relative to 'path') 128 imagestest:  # test images (optional) root dir path는 ../train_data로 train과 val에 해당하는 path는 아까 지정해둔 대로 위와 같이 바꾸어준다. ※ 두번째로 우리는 두 마리의 강아지 얼굴만을 구별할 것이므로 Class의 갯수를 2개로 바꾸어준다.(train_data가 yolov5보다 상위 폴더에 위치하므로 ../train_data로 지정해준다)  # Classesnc: 2  # number of classesnames: ['OKJA', 'YEBBENI']  # class names 마지막 download script 부분은 필요없으니 지워준다.​[최종 파일] # YOLOv5 🚀 by Ultralytics, GPL-3.0 license# COCO128 dataset https://www.kaggle.com/ultralytics/coco128 (first 128 images from COCO train2017)# Example usage: python train.py --data coco128.yaml# parent# ├── yolov5# └── datasets#     └── coco128  ← downloads here# Train/val/test sets as 1) dir: path/to/imgs, 2) file: path/to/imgs.txt, or 3) list: [path/to/imgs1, path/to/imgs2, ..]path: ../train_data  # dataset root dirtrain: images/train  # train images (relative to 'path') 128 imagesval: images/val  # val images (relative to 'path') 128 imagestest:  # test images (optional)# Classesnc: 2  # number of classesnames: ['OKJA', 'YEBBENI']  # class names 자 이제 위를 다른 이름으로 저장하여 업로드해보자. (여기서는 okye.yaml로 정했다)아까 압축 파일을 옮긴 것 처럼 yolov5>>data 폴더에 드래그 앤 드롭으로 업로드한다.​ 자 위와 같이 okye.yaml 파일이 잘 업로드 되었다. ​​​이제 모든 준비는 끝났다. Training을 시켜보자!!!​스크롤을 내리다보면 train이라는 부분이 아래와 같이 보일 것이다. 붉은 색으로 체크된 명령어를 이용하면 training을 시킬 수 있다.단, 우리가 업로드한 데이터를 이용하도록 아래처럼 살짝 명령어를 바꾸어 수행한다.(coco128.yaml를 우리가 아까 수정한 okye.yaml로 교체) # Train YOLOv5s on COCO128 for 3 epochs!python train.py --img 640 --batch 16 --epochs 3 --data okye.yaml --weights yolov5s.pt --cache 무엇인가 쭉 글자가 나오면서 돌아가기 시작한다면 성공이다.​학습결과는 yolov5 >> runs >> train >> exp 폴더에 저장된다. (여러번 학습을 반복하면 저장폴더는 exp1, exp2, exp3 ... 이렇게 숫자가 붙게된다. 블로그 쓰면서 하다가 명령어 잘못 입력해서 여기서는 exp1이다.) 생성된 폴더에 들어가보면, _labels와 _pred로 끝나는 이미지 파일이 있는데, _label은 우리가 지정한 정답지이고, _pred는 학습 기반으로 컴퓨터가 제출한 시험지이다. ​사실 3번의 epoch만으로는 원하는 결과를 얻기가 불가능하니, 시간이 좀 걸리지만 200회 epoch를 실행해보자. (대략 10분 정도 소요된다.)​아까 training하는 script에서 epoch 부분만 200으로 늘려주면 된다. # Train YOLOv5s on COCO128 for 200 epochs!python train.py --img 640 --batch 16 --epochs 200 --data okye.yaml --weights yolov5s.pt --cache 자 이제 _label과 _pred를 한번 살펴보자. 아래 그림에서 왼쪽이 정답지, 오른쪽이 컴퓨터가 풀어낸 시험지이다. Confidence level 을 보면 > 0.7 로 적은 dataset에 비하면 제법 훌륭한 결과인 것 같다.  ​자, 그러면 이제 학습에 사용하지 않은 동영상을 이용해서 옥자와 예쁜이 얼굴을 detection 해보는 것으로 이번 posting을 마무리하겠다. (쓰다보니 너무 길어져서, windows local PC에 YOLOv5를 설치하는 것은 다음에 진행하려한다.)​ 동영상에서 object detection우리는 training을 통해 컴퓨터가 두 강아지의 얼굴을 구별할 수 있게 하였다. (이 학습정보는 weight라는 폴더안에 저장되는데, best.pt는 val 데이터를 이용했을때 제일 정확도가 높았던 것이고, last.pt는 200회 epoch를 모두 진행했을때의 학습정보이다.)​자 이제 학습된 정보를 가지고 학습에 사용하지 않은 동영상에서, 강아지 얼굴을 구별하는 것은 확인해보자.​우선 여기에 사용될 동영상을 아까와 같은 방법으로 업로드한다. (여기서는 ok_ye.mp4를 업로드했다)​ Inference 부분을 보면 detect.py라는 코드를 이용하는 것을 볼 수 있는데, 이를 이용하겠다.(단, 여기서 weight는 아까 학습후 얻어진 best.pt를 사용하기 위해 아래와 같이 코드를 살짝 수정하겠다)​ !python detect.py --weights ./runs/train/exp3/weights/best.pt --img 640 --conf 0.5 --source ../ok_ye.mp4 여기서 --conf는 0.5로 지정하겠다. (아까 그림에서 박스 옆에 나오는 숫자를 의미하는데, 0.5 이상만 표시하겠다는 의미이다)​결과는 아까와 비슷하게 yolov5 >> runs >> dectect 아래 exp 폴더에 저장된다.​아래 동영상 결과를 보면 알 수 있듯이, 매우 성공적으로 두 강아지 얼굴을 구별하고 있다.​  Written by FlatEarth "
[Object detection 시리즈] Fast/Faster RCNN ,https://blog.naver.com/koreadeep/222661706323,20220302,"이전 포스팅에서 two-stage detector의 시초 격인 RCNN 모델에 대해 살펴보았는데요.이 모델은 Selective Search라는 Region proposal 알고리즘을 통해 후보 영역을 약 2000여 개 선정하고, 이 영역을 입력으로 classification 및 boundbox regresssion을 진행한다고 설명했습니다. 꽤 상식적인 방법이긴 하지만 이 모델은 상용화하기에는 어려운 특징이 있었습니다.바로 모든 후보 영역들에 대해 CNN 모델 Forward propagation 연산을 해야 하기 때문에,연산량이 매우 많고, 따라서 inference time이 매우 길었기 때문입니다.  R-CNN Problems​아래 그림을 통해 RCNN 모델의 문제점에 대해 좀 더 자세하게 살펴보겠습니다. 모델 구조적으로 모든 ConvNet 뒤에 각각 Bound box regression 과 classifier(SVM)가 따라붙고,각각 고유의 loss function이 정의됩니다.학습시간은 84시간 정도 소요되었고, 저장 공간이 많이 필요했다고 합니다.특히, CNN 모델을 VGG16 모델로 하고 inference time을 측정해 봤을 때, 이미지다 47초 정도 소요되었다고 설명하고 있습니다.​이렇게 inference time이 오래 걸리는 문제를 개선한 모델이 바로 Fast R-CNN입니다.아래 그림은 Fast R-CNN 모델의 구조를 나타낸  것입니다.가장 눈에 띄는 한 가지 차이점은 입력 이미지가 들어와서 하나의 ConvNet만을 거친다는 것입니다.이렇게 함으로써 이미지의 모든 후보 영역들마다 ConvNet 연산을 하던 computation load가 줄어들 것이라는 것을 예상해 볼 수 있습니다.그렇다면 region에 대한 정보는 어떻게 학습할 수 있을지 의문이 생깁니다.이 부분은 ROI pooling이라는 테크닉을 이용하며,이 기법을 ConvNet을 거친 Feature map에 반영해 줍니다.  ROI pooling을 통해 원본 이미지에서의 각 후보 영역들에 대해 Feature map 상 대응되는 영역을 추출하여 Fully connected layer를 거친 후 Classifier(Softmax) 와 Bound box regression을 진행하게 되는 것입니다.  ​ Fast R-CNN Architecture아래 그림은 Fast RCNN 논문에서 표현한 모델의 아키텍처입니다.입력 이미지를 ConvNet 연산을 해서 Featre map을 만드는 동시에,Region proposal 알고리즘을 통해 나온 후보 영역 추출합니다.이 후보 영역을 feature map 상에 매핑(ROI projection) 한 후 ROI pooling + FC(fully connected)를 거치고각각 Softmax , bound box regressor로 분리되어 연산됩니다.  ​ROI Pooling 이란 Feature map을 미리 정해놓은 H x W 크기에 맞게 grid를 설정하여 max pooling 하는 것을 말합니다.이렇게 함으로써, Feature map의 크기에 상관없이 고정된 vector 값으로 표현이 가능해집니다.​아래의 예시를 통해 좀 더 자세히 알아보겠습니다.아래 그림에서 첫 번째 그림은8x8 (3channel) 크기의 feature map을 ROI pooling을 통해 3x1 크기의 vector로 representation 한 결과입니다.즉, Feature map의 각 채널에서 가장 큰 값을 sampling 한 것입니다.두 번째는 8x8 feature map을 네 개의 구간으로 나눈 뒤, 각 영역별로 max 값을 sampling 한 것입니다.  결과적으로 한 채널당 4개의 값이 나오므로, 4x3 = 12개의 숫자로 representation 되게 됩니다.마지막으로 세 번째 feature map은 16개의 구간으로 나누어서 각 영역마다 max pooling을 하는 것입니다.마찬가지로 16x3 = 48개의 숫자로 represenation 되게 됩니다.이게 무슨 의미이냐면,마지막 그림의 경우 8x8 feature map 말고도, 8x12 크기의 feature map 그림도 표현되어 있습니다.이렇게 서로 다른 크기의 feature map이 주어지더라도 구분할 영역의 개수가 16으로 고정되어 있다면,결과적으로 representation 되는 벡터의 크기는 48로 변함이 없다는 것을 알 수 있습니다.이것이 바로 ROI pooling을 통해 feature map 과는 무관하게 고정 크기의 vector를 얻을 수 있다는 의미가 되는 것입니다. 이렇게 ROI pooling을 적용함으로써 Fast RCNN은 연산량을 대폭 낮출 수 있었습니다.​ Fast R-CNN Performance아래의 표를 통해 기존 모델인 RCNN과 비교를 해보겠습니다. 학습시간이 9배 가까이 줄었고, inference 시간은 150배 가까이 빨라진 것을 확인할 수 있습니다.심지어 성능은 오히려 더 개선되었고요.(정보를 더 함축적으로 표현하기 때문에 accuracy가 감소할 수 있다는 우려가 있었음에도,오히려 일반화 / 추상화가 적절하게 되어 accuracy를 유지하면서도 연산량을 크게 단축하는 결과를 얻었습니다.)  ​하지만,,함정이 있었는데요.그것은 바로 inference time 계산 시, region proposal에 소요되는 시간을 고려하지 않았다는 것이었습니다.region proposal까지 고려한 실제 end-to-end 소요시간은 2초가량이었다고 합니다.즉, selective search 알고리즘에 1.5초 이상이 소요된다는 의미입니다.​아래 그림은 RCNN , SPP, Fast RCNN 모델의 학습시간과 테스트 시간을 그래프로 나타낸 것입니다.RCNN에 비해 눈에 띄게 준 것을 확인할 수 있으며,아래 그림의 test time은 region proposal은 고려되지 않을 것이라는 것을 확인할 수 있습니다. 다음에 다루게 될 Faster RCNN은 test time에서 많은 시간을 잡아먹는region propsal 알고리즘을 Neural Network 형태로 대체함으로써수행 시간을 대폭 줄이는데 기여한 모델입니다.다음에 다루게 될 Faster RCNN은 test time에서 많은 시간을 잡아먹는region propsal 알고리즘을 Neural Network 형태로 대체함으로써수행 시간을 대폭 줄이는데 기여한 모델입니다. Faster R-CNN Architecture앞서 언급했듯, Faster RCNN은 region proposal 알고리즘을 selective search에서Region proposal Network(이하 RPN)이라고 하는 Neural Network 구조의 모델로 대체했습니다.즉, CPU를 사용하는 selective search 알고리즘 대신GPU를 활용한 RPN 네트워크를 사용할 수 있게 됨으로써 연산력을 증가시켜수행 시간을 비약적으로 줄일 수 있었습니다.​ 아래 그림을 통해 Faster R-CNN 모델의 구조에 대해 살펴보겠습니다.RPN 모델은 ConvNet 연산 다음에 위치하며,object 가 존재할 것으로 생각되는 후보 영역을 좌표로 출력하는 것을 목적으로 하는 모델입니다.후보 영역이 추출되면, 그다음 과정은 Fast RCNN과 동일하게 ROI pooling 과정을 통해고정 크기의 vector로 표현하고,각각 classifer 와 bound box regression 모델을 통해 classification 및 localization을 수행합니다. 그러면 RPN 네트워크는 어떻게 동작하는지 좀 더 자세히 알아보도록 하겠습니다.RPN은 ConvNet을 거친 Feature map을 입력으로 받아 추가적으로 Convolution 연산을 거친 후,1x1 convolution 연산을 통해 두 종류의 결과를 도출하게 됩니다.하나는 영역 내에 object가 있는 지지 여부이고다른 하나는 해당 영역의 좌표를 의미하게 됩니다.  추가적인 convolution 연산에서 sliding window 연산을 수행하면서localization 정보를 학습할 수 있기 때문에, bound box 좌표값 도출이 가능할 수 있습니다. ​그리고 convolution 연산시 다양한 형태의 anchor box를 사용하게 되는데요이 anchor box는 object의 대략적인 형태에 맞게 offset을 설정하여 학습 되게 함으로써좀 더 학습이 잘 이루어질 수 있도록 하는 역할을 하게 됩니다. 이 anchor box는 여러 형태를 갖지만 모두 sliding window 방식으로 연산이 이루어지기 때문에,translation invariant 하다는 특성이 있습니다. Faster RCNN에서는 9개 종류의 anchor box를 사용했습니다3개의 서로 다른 크기(128, 256, 512) 와서로 다른 비율 (1:1, 1:2, 2:1 )을 가진 9개의 anchor box를 정의하였습니다. ​아래 그림은 800x600 크기의 이미지가 입력으로 주어지고,위에서 언급한 9가지 종류의 anchor box가 어떻게 적용되는지 나타낸 것입니다.좌측 첫 번째 그림이 파란색, 녹색, 빨간색이 각각 크기와 비율에 따라 anchor box를 나타낸 것이고,그 아래에는 backbone을 resnet으로 할 경우, feature map이 1/16이 되므로1900개의 포인트(800/16 * 600/16)를 갖는 feature map으로 줄어들게 되며,각 포인트를 original 이미지 상에서 grid를 나타내보았을 때 stride length가 16이라는 것을 표현하고 있습니다.우측의 그림은 9개의 anchor 박스를 각 grid 포인트마다 표시해 봤을 때를 나타낸 것인데,중앙에 가까울수록 anchor box들이 겹치는 부분이 많아지므로 진하게 표현되고 있는 것을 볼 수 있습니다. 아래 그림은 grid point들을 원본 이미지상에 표시해 본 것입니다.  Faster RCNN 모델은 Neural Network가 두 개인 만큼 training 도 이 점을 고려해서 각각 학습이 필요합니다.그래서 Faster RCNN은 Alternative training  이라는 기법을 통해 학습을 진행했습니다. alternative training 이란 loss의 종류에 따라 각각의 optimizer를 통해 독립적으로 학습하는 것을 말합니다. Faster R-CNN PerformanceFaser RCNN의 성능입니다.아래 표는 이전 모델인 RCNN 및 Fast RCNN 모델과 이미지상 test time , mAP에 대해 비교해 본 결과입니다.Fast RCNN 대비하여 Faster RCNN 모델은selective search 알고리즘을 포함한 수행 시간인 2초보다 10배 가까이 줄어들었으면서도,MAP는 유지한 것을 확인할 수 있습니다. 그럼 이상으로 포스팅을 마치겠습니다.감사합니다:)  한국딥러닝(주)는 인공지능, 머신러닝, 딥러닝 솔루션을 개발하여 세상이 해결할 수 없는 문제를 해결하고 제안하고 있습니다. 끊임없이 발생되는 새로운 빅데이터와 응용 기술의 홍수 속에서 최신 인공지능 기술 동향을 예측하고 빠르게 반영하는 한국딥러닝을 만나보세요.​ ​ "
2. How to Run TensorFlow Lite Object Detection Models on the RaspberryPi(Coral Usb Accelerator) ,https://blog.naver.com/sameallday/222505789250,20210914,"출처는 연결된 깃허브에 있습니다​Part 2 - How to Run TensorFlow Lite Object Detection Models on the Raspberry Pi (with Optional Coral USB Accelerator) IntroductionThis guide provides step-by-step instructions for how to set up TensorFlow Lite on the Raspberry Pi and use it to run object detection models. It also shows how to set up the Coral USB Accelerator on the Pi and run Edge TPU detection models. It works for the Raspberry Pi 3 and Raspberry Pi 4 running either Rasbpian Buster or Rasbpian Stretch.This guide is the second part of my larger TensorFlow Lite tutorial series:How to Train, Convert, and Run Custom TensorFlow Lite Object Detection Models on Windows 10How to Run TensorFlow Lite Object Detection Models on the Raspberry Pi (with Optional Coral USB Accelerator) <--- You are here!How to Run TensorFlow Lite Object Detection Models on Android DevicesTensorFlow Lite (TFLite) models run much faster than regular TensorFlow models on the Raspberry Pi. You can see a comparison of framerates obtained using regular TensorFlow, TensorFlow Lite, and Coral USB Accelerator models in my TensorFlow Lite Performance Comparison YouTube video.This portion of the guide is split in to three sections:Section 1. Run TensorFlow Lite Object Detection Models on the Raspberry PiSection 2. Run Edge TPU Object Detection Models on the Raspberry Pi Using the Coral USB AcceleratorSection 3. Compile Custom Edge TPU Object Detection ModelsThis repository also includes scripts for running the TFLite and Edge TPU models on images, videos, or webcam/Picamera feeds.Section 1 - How to Set Up and Run TensorFlow Lite Object Detection Models on the Raspberry PiSetting up TensorFlow Lite on the Raspberry Pi is much easier than regular TensorFlow! These are the steps needed to set up TensorFlow Lite:1a. Update the Raspberry Pi1b. Download this repository and create virtual environment1c. Install TensorFlow and OpenCV1d. Set up TensorFlow Lite detection model1e. Run TensorFlow Lite model!I also made a YouTube video that walks through this guide: Step 1a. Update the Raspberry PiFirst, the Raspberry Pi needs to be fully updated. Open a terminal and issue:sudo apt-get update sudo apt-get dist-upgrade Depending on how long it’s been since you’ve updated your Pi, the update could take anywhere between a minute and an hour.While we're at it, let's make sure the camera interface is enabled in the Raspberry Pi Configuration menu. Click the Pi icon in the top left corner of the screen, select Preferences -> Raspberry Pi Configuration, and go to the Interfaces tab and verify Camera is set to Enabled. If it isn't, enable it now, and reboot the Raspberry Pi. Step 1b. Download this repository and create virtual environmentNext, clone this GitHub repository by issuing the following command. The repository contains the scripts we'll use to run TensorFlow Lite, as well as a shell script that will make installing everything easier. Issue:git clone https://github.com/EdjeElectronics/TensorFlow-Lite-Object-Detection-on-Android-and-Raspberry-Pi.git This downloads everything into a folder called TensorFlow-Lite-Object-Detection-on-Android-and-Raspberry-Pi. That's a little long to work with, so rename the folder to ""tflite1"" and then cd into it:mv TensorFlow-Lite-Object-Detection-on-Android-and-Raspberry-Pi tflite1 cd tflite1 We'll work in this /home/pi/tflite1 directory for the rest of the guide. Next up is to create a virtual environment called ""tflite1-env"".I'm using a virtual environment for this guide because it prevents any conflicts between versions of package libraries that may already be installed on your Pi. Keeping TensorFlow installed in its own environment allows us to avoid version conflicts. For example, if you've already installed TensorFlow v1.8 on the Pi using my other guide, you can leave that installation as-is without having to worry about overriding it.Install virtualenv by issuing:sudo pip3 install virtualenv Then, create the ""tflite1-env"" virtual environment by issuing:python3 -m venv tflite1-env This will create a folder called tflite1-env inside the tflite1 directory. The tflite1-env folder will hold all the package libraries for this environment. Next, activate the environment by issuing:source tflite1-env/bin/activate You'll need to issue the source tflite1-env/bin/activate command from inside the /home/pi/tflite1 directory to reactivate the environment every time you open a new terminal window. You can tell when the environment is active by checking if (tflite1-env) appears before the path in your command prompt, as shown in the screenshot below.At this point, here's what your tflite1 directory should look like if you issue ls. If your directory looks good, it's time to move on to Step 1c!Step 1c. Install TensorFlow Lite dependencies and OpenCVNext, we'll install TensorFlow, OpenCV, and all the dependencies needed for both packages. OpenCV is not needed to run TensorFlow Lite, but the object detection scripts in this repository use it to grab images and draw detection results on them.To make things easier, I wrote a shell script that will automatically download and install all the packages and dependencies. Run it by issuing:bash get_pi_requirements.sh This downloads about 400MB worth of installation files, so it will take a while. Go grab a cup of coffee while it's working! If you'd like to see everything that gets installed, simply open get_pi_dependencies.sh to view the list of packages.NOTE: If you get an error while running the bash get_pi_requirements.sh command, it's likely because your internet connection timed out, or because the downloaded package data was corrupted. If you get an error, try re-running the command a few more times.ANOTHER NOTE: The shell script automatically installs the latest version of TensorFlow. If you'd like to install a specific version, issue pip3 install tensorflow==X.XX (where X.XX is replaced with the version you want to install) after running the script. This will override the existing installation with the specified version.That was easy! On to the next step.Step 1d. Set up TensorFlow Lite detection modelNext, we'll set up the detection model that will be used with TensorFlow Lite. This guide shows how to either download a sample TFLite model provided by Google, or how to use a model that you've trained yourself by following Part 1 of my TensorFlow Lite tutorial series.A detection model has two files associated with it: a detect.tflite file (which is the model itself) and a labelmap.txt file (which provides a labelmap for the model). My preferred way to organize the model files is to create a folder (such as ""BirdSquirrelRaccoon_TFLite_model"") and keep both the detect.tflite and labelmap.txt in that folder. This is also how Google's downloadable sample TFLite model is organized.Option 1. Using Google's sample TFLite modelGoogle provides a sample quantized SSDLite-MobileNet-v2 object detection model which is trained off the MSCOCO dataset and converted to run on TensorFlow Lite. It can detect and identify 80 different common objects, such as people, cars, cups, etc.Download the sample model (which can be found on the Object Detection page of the official TensorFlow website) by issuing:wget https://storage.googleapis.com/download.tensorflow.org/models/tflite/coco_ssd_mobilenet_v1_1.0_quant_2018_06_29.zip Unzip it to a folder called ""Sample_TFLite_model"" by issuing (this command automatically creates the folder):unzip coco_ssd_mobilenet_v1_1.0_quant_2018_06_29.zip -d Sample_TFLite_model Okay, the sample model is all ready to go!Option 2: Using your own custom-trained modelYou can also use a custom object detection model by moving the model folder into the /home/pi/tflite directory. If you followed Part 1 of my TensorFlow Lite guide to train and convert a TFLite model on your PC, you should have a folder named ""TFLite_model"" with a detect.tflite and labelmap.txt file. (It will also have a tflite_graph.pb and tflite_graph.pbtxt file, which are not needed by TensorFlow Lite but can be left in the folder.)You can simply copy that folder to a USB drive, insert the USB drive in your Raspberry Pi, and move the folder into the /home/pi/tflite1 directory. (Or you can email it to yourself, or put it on Google Drive, or do whatever your preferred method of file transfer is.) Here's an example of what my ""BirdSquirrelRaccoon_TFLite_model"" folder looks like in my /home/pi/tflite1 directory: Now your custom model is ready to go!Step 1e. Run the TensorFlow Lite model!It's time to see the TFLite object detection model in action! First, free up memory and processing power by closing any applications you aren't using. Also, make sure you have your webcam or Picamera plugged in.Run the real-time webcam detection script by issuing the following command from inside the /home/pi/tflite1 directory. (Before running the command, make sure the tflite1-env environment is active by checking that (tflite1-env) appears in front of the command prompt.) The TFLite_detection_webcam.py script will work with either a Picamera or a USB webcam.python3 TFLite_detection_webcam.py --modeldir=Sample_TFLite_model If your model folder has a different name than ""Sample_TFLite_model"", use that name instead. For example, I would use --modeldir=BirdSquirrelRaccoon_TFLite_model to run my custom bird, squirrel, and raccoon detection model.After a few moments of initializing, a window will appear showing the webcam feed. Detected objects will have bounding boxes and labels displayed on them in real time.Part 3 of my TensorFlow Lite training guide gives instructions for using the TFLite_detection_image.py and TFLite_detection_video.py scripts. Make sure to use python3 rather than python when running the scripts.Section 2 - Run Edge TPU Object Detection Models on the Raspberry Pi Using the Coral USB Accelerator The Coral USB Accelerator is a USB hardware accessory for speeding up TensorFlow models. You can buy one here (Amazon Associate link).The USB Accelerator uses the Edge TPU (tensor processing unit), which is an ASIC (application-specific integrated circuit) chip specially designed with highly parallelized ALUs (arithmetic logic units). While GPUs (graphics processing units) also have many parallelized ALUs, the TPU has one key difference: the ALUs are directly connected to eachother. The output of one ALU can be directly passed to the input of the next ALU without having to be stored and retrieved from a memory buffer. The extreme paralellization and removal of the memory bottleneck means the TPU can perform up to 4 trillion arithmetic operations per second! This is perfect for running deep neural networks, which require millions of multiply-accumulate operations to generate outputs from a single batch of input data. My Master's degree was in ASIC design, so the Edge TPU is very interesting to me! If you're a computer architecture nerd like me and want to learn more about the Edge TPU, here is a great article that explains how it works.It makes object detection models run WAY faster, and it's easy to set up. These are the steps we'll go through to set up the Coral USB Accelerator:2a. Install libedgetpu library2b. Set up Edge TPU detection model2c. Run super-speed detection!This section of the guide assumes you have already completed Section 1 for setting up TFLite object detection on the Pi. If you haven't done that portion, scroll back up and work through it first.Step 2a. Install libedgetpu libraryFirst, we'll download and install the Edge TPU runtime, which is the library needed to interface with the USB Acccelerator. These instructions follow the USB Accelerator setup guide from official Coral website.Open a command terminal and move into the /home/pi/tflite1 directory and activate the tflite1-env virtual environment by issuing:cd /home/pi/tflite1 source tflite1-env/bin/activate Add the Coral package repository to your apt-get distribution list by issuing the following commands:echo ""deb https://packages.cloud.google.com/apt coral-edgetpu-stable main"" | sudo tee /etc/apt/sources.list.d/coral-edgetpu.list curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add - sudo apt-get update Install the libedgetpu library by issuing:sudo apt-get install libedgetpu1-std You can also install the libedgetpu1-max library, which runs the USB Accelerator at an overclocked frequency, allowing it to achieve even faster framerates. However, it also causes the USB Accelerator to get hotter. Here are the framerates I get when running TFLite_detection_webcam.py with 1280x720 resolution for each option with a Raspberry Pi 4 4GB model:libedgetpu1-std: 22.6 FPSlibedgetpu1-max: 26.1 FPSI didn't measure the temperature of the USB Accelerator, but it does get a little hotter to the touch with the libedgetpu1-max version. However, it didn't seem hot enough to be unsafe or harmful to the electronics.If you want to use the libedgetpu-max library, install it by using sudo apt-get install libedgetpu1-max. (You can't have both the -std and the -max libraries installed. If you install the -max library, the -std library will automatically be uninstalled.)Alright! Now that the libedgetpu runtime is installed, it's time to set up an Edge TPU detection model to use it with.Step 2b. Set up Edge TPU detection modelEdge TPU models are TensorFlow Lite models that have been compiled specifically to run on Edge TPU devices like the Coral USB Accelerator. They reside in a .tflite file and are used the same way as a regular TF Lite model. My preferred method is to keep the Edge TPU file in the same model folder as the TFLite model it was compiled from, and name it as ""edgetpu.tflite"".I'll show two options for setting up an Edge TPU model: using the sample model from Google, or using a custom model you compiled yourself.Option 1. Using Google's sample EdgeTPU modelGoogle provides a sample Edge TPU model that is compiled from the quantized SSDLite-MobileNet-v2 we used in Step 1e. Download it and move it into the Sample_TFLite_model folder (while simultaneously renaming it to ""edgetpu.tflite"") by issuing these commands:wget https://dl.google.com/coral/canned_models/mobilenet_ssd_v2_coco_quant_postprocess_edgetpu.tflite mv mobilenet_ssd_v2_coco_quant_postprocess_edgetpu.tflite Sample_TFLite_model/edgetpu.tflite Now the sample Edge TPU model is all ready to go. It will use the same labelmap.txt file as the TFLite model, which should already be located in the Sample_TFLite_model folder.Option 2. Using your own custom EdgeTPU modelIf you trained a custom TFLite detection model, you can compile it for use with the Edge TPU. Unfortunately, the edgetpu-compiler package doesn't work on the Raspberry Pi: you need a Linux PC to use it on. Section 3 of this guide will give a couple options for compiling your own model if you don't have a Linux box. While I'm working on writing it, here are the official instructions that show how to compile an Edge TPU model from a TFLite model.Assuming you've been able to compile your TFLite model into an EdgeTPU model, you can simply copy the .tflite file onto a USB and transfer it to the model folder on your Raspberry Pi. For my ""BirdSquirrelRaccoon_TFLite_model"" example from Step 1e, I can compile my ""BirdSquirrelRaccoon_TFLite_model"" on a Linux PC, put the resulting edgetpu.tflite file on a USB, transfer the USB to my Pi, and move the edgetpu.tflite file into the /home/pi/tflite1/BirdSquirrelRaccoon_TFLite_model folder. It will use the same labelmap.txt file that already exists in the folder to get its labels.Once the edgetpu.tflite file has been moved into the model folder, it's ready to go!Step 2c. Run detection with Edge TPU!Now that everything is set up, it's time to test out the Coral's ultra-fast detection speed! Make sure to free up memory and processing power by closing any programs you aren't using. Make sure you have a webcam plugged in.Plug in your Coral USB Accelerator into one of the USB ports on the Raspberry Pi. If you're using a Pi 4, make sure to plug it in to one of the blue USB 3.0 ports.Insert picture of Coral USB Accelerator plugged into Raspberry Pi here!Make sure the tflite1-env environment is activate by checking that (tflite1-env) appears in front of the command prompt in your terminal. Then, run the real-time webcam detection script with the --edgetpu argument:python3 TFLite_detection_webcam.py --modeldir=Sample_TFLite_model --edgetpu The --edgetpu argument tells the script to use the Coral USB Accelerator and the EdgeTPU-compiled .tflite file. If your model folder has a different name than ""Sample_TFLite_model"", use that name instead.After a brief initialization period, a window will appear showing the webcam feed with detections drawn on each from. The detection will run SIGNIFICANTLY faster with the Coral USB Accelerator.If you'd like to run the video or image detection scripts with the Accelerator, use these commands:python3 TFLite_detection_video.py --modeldir=Sample_TFLite_model --edgetpu python3 TFLite_detection_image.py --modeldir=Sample_TFLite_model --edgetpu Have fun with the blazing detection speeds of the Coral USB Accelerator!Section 3 - Compile Custom Edge TPU Object Detection ModelsTo use a custom model on the Coral USB Accelerator, you have to run it through Coral's Edge TPU Compiler tool. Unfortunately, the compiler only works on Linux operating systems, and only on certain CPU architectures.The easiest way to compile the Edge TPU model is to use a Google Colab session. I created a Colab page specifically for compiling Edge TPU models. Please click the link below and follow the instructions in the Colab notebook.https://colab.research.google.com/drive/1o6cNNNgGhoT7_DR4jhpMKpq3mZZ6Of4N?usp=sharingAppendix: Common ErrorsThis appendix lists common errors that have been encountered by users following this guide, and solutions showing how to resolve them.Feel free to create Pull Requests to add your own errors and resolutions! I'd appreciate any help.1. TypeError: int() argument must be a string, a bytes-like object or a number, not 'NoneType'The 'NoneType' error means that the program received an empty array from the webcam, which typically means something is wrong with the webcam or the interface to the webcam. Try plugging and re-plugging the webcam in a few times, and/or power cycling the Raspberry Pi, and see if that works. If not, you may need to try using a new webcam.2. ImportError: No module named 'cv2'This error occurs when you try to run any of the TFLite_detection scripts without activating the 'tflite1-env' first. It happens because Python cannot find the path to the OpenCV library (cv2) to import it.Resolve the issue by closing your terminal window, re-opening it, and issuing:cd tflite1 source tflite1-env/bin/activate Then, try re-running the script as described in Step 1e.3. THESE PACKAGES DO NOT MATCH THE HASHES FROM THE REQUIREMENTS FILEThis error can occur when you run the bash get_pi_requirements.sh command in Step 1c. It occurs because the package data got corrupted while downloading. You can resolve the error by re-running the bash get_pi_requirements.sh command a few more times until it successfully completes without reporting that error.4. Unsupported data type in custom op handler: 6488064Node number 2 (EdgeTpuDelegateForCustomOp) failed to prepare.This error occurs when trying to use a newer version of the libedgetpu library (v13.0 or greater) with an older version of TensorFlow (v2.0 or older). It can be resolved by uninstalling your current version of TensorFlow and installing the latest version of the tflite_runtime package. Issue these commands (make sure you are inside the tflite1-env virtual environment):pip3 uninstall tensorflow pip3 install https://dl.google.com/coral/python/tflite_runtime-2.1.0.post1-cp37-cp37m-linux_armv7l.whl (Or, if you're using Python 3.5, use pip3 install https://dl.google.com/coral/python/tflite_runtime-2.1.0.post1-cp35-cp35m-linux_armv7l.whl instead.)Then, re-run the TFLite detection script. It should work now!Note: the URLs provided in these commands may change as newer versions of tflite_runtime are released. Check the TFLite Python Quickstart page for download URLs to the latest version of tflite_runtime.5. IndexError: list index out of rangeThis error usually occurs when you try using an ""image classification"" model rather than an ""object detection"" model. Image classification models apply a single label to an image, while object detection models locate and label multiple objects in an image. The code in this repository is written for object detection models.Many people run in to this error when using models from Teachable Machine. This is because Teachable Machine creates image classification models rather than object detection models. To create an object detection model for TensorFow Lite, you'll have to follow the guide in this repository.If you'd like to see how to use an image classification model on the Raspberry Pi, please see this example: https://github.com/tensorflow/examples/tree/master/lite/examples/image_classification/raspberry_pi "
AugFPN: Improving Multi-scale Feature Learning for Object Detection 간단 논문 리뷰 ,https://blog.naver.com/tomatian/222093022779,20200918,"AugFPN: Improving Multi-scale Feature Learning for Object DetectionCVPR 2020https://arxiv.org/abs/1912.05384 AugFPN의 구성1. Consistent Supervision2. Residual Feature Augmentation3. Soft RoI Selection​ Consistent Supervision ?fxing sementic gaps between feature mapsFigure 1. Architecture of AugFPN· Consistent Supervision 을 적용하는 목적-  기존 fpn 에서는 다른 feature level 간에 semantic gap이 존재하여, multi - scale 특성이 미약한 점을 수정하려함.​· Consistent Supervision 은 무엇인가?-  PSPNet 에서는 intermediate layer에 pixel-level loss를 추가함으로써 구현함.-  Nas-FPN 에서는 classifier 와 regression heads를 모든 intermediate pyramid nets에 적용함.→  따라서 network의 중간 layer에서 loss를 구하는 과정이 추가되는 것이 공통점!​· AugFPN 에서는 어떻게 Consistent Supervision 을 적용했는가?-  동일한 supervision signals 를 여러 feature에 동일하게 적용하였다.-  supervision signals를 적용하려면 intermediate layer ( 즉 M layers )에  RoI 들이 필요한데 이는 RoI align을 통해서 만들수 있다.​·  적용 procedure-  M2,3,4,5에 여러 classification 과 box regression head 를 추가한다.→  auxillary (보조적인) loss 생성​-  그리고 다른 featur level 간에 head의 parameter들을 공유한다.→  여러 level 간에 서로 sementic information을 배울 수 있다.​- final loss 에서는 원래 P layers 에서 생성된 loss와 auxillary loss 를 더해주는 형태다. →  여기서 lamda는 original loss 와 auxillary loss 의 밸런스를 맞추는 weight다. ​· 그외 특이사항-  test phase 에서는 auxillary branch를 생성하지 않는다. →  consistent supervision 은 더이상 parameter를 생성하지 않는다.​​​​ Residual Feature Augmentation ?preventing information loss in upper level in pyramid ·  Residual Feature Augmentation 을 적용하는 목적​-  FPN에서 highest pyramid level에서 information loss 가 발생한다. ( feature channel의 감소 때문 )-  AugFPN에서는 M5 level에서 정보 손실이 일어난다.​·  Residual Feature Augmentation 적용 방법-  residual branch 를 사용하여 diverse 한 context information 을 주입한다. ​· 적용 procedure-  우선 C5 level 의 feature map을 사용해야한다. ​①  이 C5 level feature map에 ratio-invariant pooling을 통해 C5의 a1배, a2배, a3배… an 크기의 context feature을 만들어준다.→ 즉 n개의 다른 scale의 context feature 가 생성된 것이다.  (단 a1, a2, a3, … <1)​※ ratio - invariant pooling ?·  output size를 정해 놓고, 그에 따라 kernel, stride 등을 결정하는 pooling→  image size 를 고려한 pooling 진행​②  1X1 conv 를 적용한다.​③  contect feature 들을 다시 upsample을 하여, C5와 동일한 사이즈로 맞춰준다.→  bilinear interpolation 을 사용한다. (  하지만 aliasing 문제가 발생할 수 있음  )​④  aliasing 문제를 해결하고자 context feature 들을 fusion할 때는 ASF ( Adaptive channel fusion )을 사용한다.→  각 feature map 에 spatial weight 를 생성한다.→  weight를 모두 모아 M6라는 feature level을 생성한다.→  M6를 M5 와 summation→ M6, M5를 summation 한 layer를 다른 feature map들과 fuse→ 그리고 나서 3X3 conv를 적용하여 P level 들을 생성한다. ​※ ASF ?-  concatenation 을 대체하는 fusion tool​​​​ Soft RoI selectionpreventing information loss in upper level in pyramid ·  Soft RoI selectiion 을 적용하는 목적-  Rol 들은 각자의 scale에 맞는 level로 배정이 되는데 크기 차이가 미미한 RoI들은 그 scale에 맞지 않는 level로 연결될 수 있다.→  어떤 feature level이 중요한 information을 가지고 있는지 판단할 수 없다.​· Soft RoI Selection 이란?-  Rol pooing 절차를 parameterizing 하는 것-  다른 level feature 안에 RoI 중 어떤 것이 중요한 것인지 측정하고자, adaptive weights 를 만든다.-  RoI 들은 adaptive weight 들을 기반으로 만들어진다. ​· 적용 procedure-  모든 feature level 에 있는 feature 들에서 RoI를 pooling으로 만들어낸다.-  ASF 를 적용하여 feature 들을 adaptively fuse 한다.-  각기 다른 level에 존재하는 RoI들을 위해 각기 다른 spatial weight map을 만든다.→ fuse RoIs​​​​  이상 동산이었습니다.정상에서 만나요 ! :)​ "
Thank you for OBJECT DETECTION explanation! ,https://blog.naver.com/ointilj/222358873352,20210520,"​​https://www.bogotobogo.com/python/OpenCV_Python/python_opencv3_Image_Object_Detection_Face_Detection_Haar_Cascade_Classifiers.php OpenCV 3 Object Detection : Face Detection using Haar Cascade Classfiers - 2020Object Detection : Face Detection using Haar Cascade Classfiers bogotobogo.com site search: Face Detection ""Face detection is a computer technology that determines the locations and sizes of human faces in arbitrary (digital) images. It detects facial features and ignores anything else, such as buil...www.bogotobogo.com ​OBJECT DETECTION : FACE DETECTION USING HAAR CASCADE CLASSFIERS ​​ ​​ ​  ​​​bogotobogo.com site search:   Face Detection""Face detection is a computer technology that determines the locations and sizes of human faces in arbitrary (digital) images. It detects facial features and ignores anything else, such as buildings, trees and bodies. Face detection can be regarded as a more general case of face localization. In face localization, the task is to find the locations and sizes of a known number of faces (usually one)."" - wiki - Face detection​​​​​Haar featuresOpenCV's algorithm is currently using the following Haar-like features which are the input to the basic classifiers:​ ​​ ​Picture source: How Face Detection Works​​​​Cascade of Classifiers""Instead of applying all the 6000 features on a window, group the features into different stages of classifiers and apply one-by-one. (Normally first few stages will contain very less number of features). If a window fails the first stage, discard it. We don't consider remaining features on it. If it passes, apply the second stage of features and continue the process. The window which passes all stages is a face region."" - Face Detection using Haar Cascades.​​ Picture source: How Face Detection Works​​​​OpenCV's pre-trained classifiersOpenCV already contains many pre-trained classifiers for face, eyes, smile etc. Those XML files are stored in opencv/data/haarcascades/ folder: ~/OpenCV/opencv/data/haarcascades$ ls haarcascade_eye_tree_eyeglasses.xml haarcascade_mcs_leftear.xml haarcascade_eye.xml haarcascade_mcs_lefteye.xml haarcascade_frontalface_alt2.xml haarcascade_mcs_mouth.xml haarcascade_frontalface_alt_tree.xml haarcascade_mcs_nose.xml haarcascade_frontalface_alt.xml haarcascade_mcs_rightear.xml haarcascade_frontalface_default.xml haarcascade_mcs_righteye.xml haarcascade_fullbody.xml haarcascade_mcs_upperbody.xml haarcascade_lefteye_2splits.xml haarcascade_profileface.xml haarcascade_lowerbody.xml haarcascade_righteye_2splits.xml haarcascade_mcs_eyepair_big.xml haarcascade_smile.xml haarcascade_mcs_eyepair_small.xml haarcascade_upperbody.xml  ​​​​OpenCV's face detectionLet's load the required XML classifiers. face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml') eye_cascade = cv2.CascadeClassifier('haarcascade_eye.xml')  Then, we need to load input image in grayscale mode: img = cv2.imread('xfiles4.jpg') gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)  We use v2.CascadeClassifier.detectMultiScale() to find faces or eyes, and it is defined like this: cv2.CascadeClassifier.detectMultiScale(image[, scaleFactor[, minNeighbors[, flags[, minSize[, maxSize]]]]])  Where the parameters are:image : Matrix of the type CV_8U containing an image where objects are detected.scaleFactor : Parameter specifying how much the image size is reduced at each image scale. ​Picture source: Viola-Jones Face DetectionThis scale factor is used to create scale pyramid as shown in the picture. Suppose, the scale factor is 1.03, it means we're using a small step for resizing, i.e. reduce size by 3 %, we increase the chance of a matching size with the model for detection is found, while it's expensive.minNeighbors : Parameter specifying how many neighbors each candidate rectangle should have to retain it. This parameter will affect the quality of the detected faces: higher value results in less detections but with higher quality. We're using 5 in the code.flags : Parameter with the same meaning for an old cascade as in the function cvHaarDetectObjects. It is not used for a new cascade.minSize : Minimum possible object size. Objects smaller than that are ignored.maxSize : Maximum possible object size. Objects larger than that are ignored.If faces are found, it returns the positions of detected faces as Rect(x,y,w,h). faces = face_cascade.detectMultiScale(gray, 1.3, 5)  Once we get these locations, we can create a ROI for the face and apply eye detection on this ROI.​​​​The Code import numpy as np import cv2 from matplotlib import pyplot as plt face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml') eye_cascade = cv2.CascadeClassifier('haarcascade_eye.xml') img = cv2.imread('xfiles4.jpg') gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) faces = face_cascade.detectMultiScale(gray, 1.3, 5) for (x,y,w,h) in faces: cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2) roi_gray = gray[y:y+h, x:x+w] roi_color = img[y:y+h, x:x+w] eyes = eye_cascade.detectMultiScale(roi_gray) for (ex,ey,ew,eh) in eyes: cv2.rectangle(roi_color,(ex,ey),(ex+ew,ey+eh),(0,255,0),2) cv2.imshow('img',img) cv2.waitKey(0) cv2.destroyAllWindows()  ​​​​Output 1​ ​We're almost there except we got an additional eye.​​​​Output 2​ I got the image using: faces = face_cascade.detectMultiScale(gray, 1.03, 3)  ​But if with scaleFactor = 3 and minNeighbors = 5, I got this:​​ ​ "
"3D Object Detection Survey - 4. 3D Object Detection Methods, A Monocular/Stereo Image-based Methods ",https://blog.naver.com/mikangel/222476423003,20210819,"2D 객체 감지는 3D 객체 감지의 연구를 어느 정도 촉진시키는 촉매 효과가 있습니다. 아래 그림과 같이 3차원 물체 검출 방법은 입력 데이터의 양상에 따라 단안/입체 영상 기반 IV-A, 포인트 클라우드 기반 IV-B 및 다중 모드 융합 기반 방법 IV-C로 분류할 수 있습니다.  3차원 객체 감지에서 지배적인 포인트 클라우드 기반 방법은 표현 학습을 기반으로 다시 점 기반, 복셀 기반, 포인트 기반 및 포인트 복셀 기반 방법으로 더 분류할 수 있습니다. ​다중 모드 융합 기반 방법이 요즘 인기를 얻고 있지만 서로 다른 영역(예: 이미지 및 포인트 클라우드)의 시너지 효과를 활용하는 것은 쉬운 일이 아닙니다. 다양한 다중 모드 융합 기반 방법을 명시적으로 구별하기 위해 순차 융합 기반 방법과 병렬 융합 기반 방법이라는 두 가지 새로운 융합 전략 범주를 정의합니다. ​단안/입체 영상 기반 방법, 포인트 클라우드 기반 방법, 다중 모드 융합 기반 방법을 각각의 방법이 등장하는 시간 순서대로 소개합니다. 다른 방법을 기반으로 하기 때문에 마지막에 멀티모달 융합 기반 방법을 넣습니다. 이제 다음 하위 섹션에서 각 범주를 개별적으로 자세히 분석합니다.​A. 단안/스테레오 이미지 기반 방식 ​3D 방식 중 2D 객체 감지와 가장 유사한 스트림 방식인 이러한 방식은 단안/스테레오 이미지만 입력으로 받아 3D 객체 인스턴스를 예측합니다. 일반적으로 템플릿 매칭 기반 방법과 기하학적 속성 기반 방법의 두 주류가 존재합니다. ​전자의 경우 지역 제안이 이 라인의 필수 요소입니다. 사실, 2D 객체 감지에서 객체성이 존재할 수 있는 고품질 영역을 제안하는 방법은 전통적인 수작업 그룹화 (예: SelectiveSearch) 및 윈도우 스코어링(예:EdgeBoxes) 에서 CNN의 성공과 함께 최근 RPN(Region Proposal Network) [13]에 대한 방법까지 다양하게 광범위하게 연구되었습니다[59]. ]) 지역 제안을 얻는 방법은 철저하게 연구되었습니다. 그럼에도 불구하고 3D 대표 템플릿을 철저하게 점수화하고 일치시키기 위해 이미지의 지역적 대응 사전(즉, 분할, 모양 및 여유 공간 등)을 추출하는 목적은 항상 일관성있게 사용되어 왔습니다[27, 28, 35, 55]. ​반면, 후자는 실증적 관찰에서 기하학적 속성을 통해 3D 포즈를 대략적으로 추정하기 위해 원근 n-점 문제(PnP)[62]로 축소될 수 있으며, 기존 2D 검출기를 통해 고품질 2D 경계 상자가 제공됩니다. [11–18]. ​최근에 포인트 클라우드 기반 방법의 고무적인 성공에 영감을 받아 LiDAR 신호를 모방(이후 유사 LiDAR라고 함)하여 이미지 좌표를 다시 3D 공간으로 재투영한 차이 계산하고, 이를  고성능 포인트 클라우드 기반 방법으로 재정렬 하는 또 다른 시도가 발생합니다. 우리는 그것을 Pseudo(의사) LiDAR 기반 방법이라고 명명했습니다.​A-1 템플릿 매칭 기반 방법​이 방법은 3D 제안을 대표 템플릿으로 철저하게 샘플링하고 채점하여 2D/3D 매칭을 수행하는 경향이 있습니다. 템플릿 매칭 기반 방법은 일반적으로 Chen et al.이 제안한 초기의 잘 알려진 3DOP[55]로 예시되며, 스테레오 이미지 쌍을 입력으로 사용하여 깊이를 추정하고 이미지 평면에서 픽셀 단위 좌표를 재투영하여 포인트 클라우드를 계산합니다. 다시 3D 공간으로. 3DOP는 신중하게 설계된 가능 요소(예: 객체 크기 사전, 접지 평면 및 점 구름 밀도 등)와 관련하여 Markov Random Field(MRF)의 에너지 최소화로 제안 생성 문제를 공식화합니다. 다양한 3D 객체 제안을 얻으면서 3DOP는 객체 위치를 공동으로 회귀하기 위해 Fast R-CNN[11] 파이프라인을 사용합니다. 그 후, 자동차에 단일 카메라만 장착될 가능성이 있는 경우 Chen et al 은 스테레오 카메라 대신 단안 카메라만 사용하여 동등한 성능을 달성하기 위해 Mono3D[27]를 제안했습니다. 3DOP와 달리 Mono3D는 깊이 정보를 계산하지 않고 슬라이딩 창을 사용하여 3D 공간에서 3D 객체 후보를 직접 샘플링합니다. 단, 검색 노력을 줄이기 위해 객체가 있는 접지면이 이미지 평면과 직교해야 한다고 가정합니다. 예를 들어, 의미론적 분할, 인스턴스 수준 분할 및 위치 우선순위는 3DOP[55]에 지정된 Fast R-CNN[11] 파이프라인을 통해 탐지를 수행하기 전에 가장 유망한 후보를 선택하기 위해 이미지 평면의 후보에 철저하게 점수를 매기기 위해 활용됩니다. 3DOP[55] 또는 Mono3D[27]는 클래스별 제안을 출력합니다. 즉, 잠재력은 카테고리별로 별도로 새로 설계되어야 합니다. 그럼에도 불구하고 신중한 엔지니어링 및 상당한 도메인 전문 지식에 대한 과도한 의존은 복잡한 시나리오에 대한 이러한 모델의 일반화를 제한합니다. 또 다른 시도는 Chabot 등이 제안한 Deep MANTA[28]에서 볼 수 있습니다. 이는 2D 부품 좌표, 부품 가시성 및 3D 템플릿 유사성과 관련된 2D 경계 상자를 출력하기 위해 맞춤형 2D 검출기를 사용하여 3D 모델의 대규모 CAD 데이터베이스에서 감독합니다. . 예측된 3D 템플릿 유사도에 따라 템플릿 데이터베이스에서 가장 적합한 3D 템플릿을 선택하여 [68]의 2D/3D 매칭을 수행하여 3D 기하학을 복구합니다. 단점은 다음과 같습니다. Deep MANTA는 데이터베이스에 없는 범주로 일반화하기에는 부적합한 3D 모델의 거대한 CAD 데이터베이스를 유지해야 한다는 것입니다.​A-2 기하학적 속성 기반 방법. ​높은 재현율을 달성하기 위해 광범위한 제안을 요구하는 대신 이러한 방법은 정확한 2D 경계 상자에서 직접 시작하여 경험적 관찰을 통해 얻은 기하학적 속성에서 3D 포즈를 대략적으로 추정합니다. Mousavianet al. 가 제안한Deep3DBox[29]는 3D 모서리의 투시 투영이 2D 경계 상자의 적어도 한 면에 단단히 닿아야 하는 기하학적 속성을 활용합니다. Li et al. 가 제안한 GS3D[31]는 추가 데이터나 레이블이 도입되지 않고 단안 RGB 이미지만을 통해 완전한 3D 인스턴스를 감지합니다. 특히 GS3D는 Faster RCNN[13] 프레임워크를 기반으로 하는 추가 방향 예측 분기를 추가하여 2D+O 서브넷이라고 하는 신뢰할 수 있는 2D 경계 상자 및 관찰 방향을 예측합니다. 그런 다음 GS3D는 자율 주행의 맥락에서 3D 상자 상단 중심이 해당 2D 경계 상자의 상단 중간점에 거의 가깝다는 경험적 증거를 기반으로 안내라고 하는 거친 3D 상자를 얻습니다. 마지막으로, 2D 상자와 3D 상자의 3개의 가시적 표면 모두에서 추출된 기능을 융합하여 표현 모호성 문제를 제거한 후 추가 개선을 위해 3D 서브넷에 공급합니다. GS3D는 기존의 단안 이미지 기반 방법에 비해 상당한 성능 향상을 보여주지만 GS3D는 부정확하고 물체의 범위와 크기에 취약한 경험적 지식에 의존합니다. 2D와 3D 사이의 투영 관계를 활용하는 또 다른 예는 스테레오 R-CNN[33]입니다. Li et al. 가 제안한 스테레오 RCNN[33]은 스테레오 이미지에서 의미론적 속성과 조밀한 제약 조건을 완전히 활용합니다. 특히, 스테레오 R-CNN은 가중치 공유 네트워크인 ResNet-101[69]과 FPN[70]을 백본으로 채택하여 각각 왼쪽 및 오른쪽 이미지 특징을 추출한 다음 관심 영역(RoI) 정렬 작업을 적용한 제안된 RoI 영역을 잘라냅니다. 다음으로, 이러한 선택된 RoI 기능은 스테레오 회귀 분기에 공급되기 전에 연결을 통해 융합됨과 동시에 왼쪽 가지의 RoI 기능을 활용하여 Mask R-CNN[18]을 모방하여 4개의 시맨틱 키포인트를 예측합니다. 마지막으로, 3D 상자 추정은 기하학적 제약, 즉 3D 모서리와 2D 상자 사이의 투영 관계 및 예측된 키포인트에 의존하여 정교하게 처리될 수 있습니다.​A-3 의사 LiDAR 기반 방법. ​이 방법은 먼저 깊이 추정을 수행한 다음 기존 포인트 클라우드 기반 방법에 의존합니다. Xu et al.은 이미지 특징과 의사 LiDAR에 대한 다중 레벨 융합을 수행하는 MF3D[30]를 제안했습니다. 특히, MF3D는 먼저 독립형 단안 깊이 추정 모듈을 통해 차이점을 계산하여 유사 LiDAR를 얻습니다. 동시에 표준 2D 영역 제안 네트워크가 차이점 맵에서 얻은 변환된 전면 뷰(알고리즘 3 참조) 기능과 융합된 RGB 이미지를 입력으로 사용합니다. 2D 영역 제안을 얻으면 RGB 이미지와 유사 LiDAR의 기능이 연결에 의해 융합되어 추가 개선이 이루어집니다. 최근에 Weng et al.에 의해 제안된 Mono3D-PLiDAR[32]는 단안 깊이 추정(예: DORN[71])을 통해 입력 이미지를 3D 카메라 좌표, 즉 의사 LiDAR 점으로 적용합니다. 그런 다음 Frustum PointNets[56]라고 하는 3D 물체 감지기가 의사 LiDAR와 함께 적용됩니다. Weng et al. Pseudo LiDAR는 단안 깊이 추정의 오류로 인해 많은 양의 노이즈가 있음을 보여줍니다. 이 오류는 LiDAR 지점과의 로컬 오정렬과 깊이 인공물의 문제라는 두 가지 측면을 반영합니다. 전자를 극복하기 위해 Mono3D-PLiDAR는 2D-3D 경계 상자 일관성 손실(BBCL)을 사용하여 교육을 감독합니다. 후자를 완화하기 위해 Mono3D-PLiDAR는 절단체 내에서 관련 없는 점을 줄이기 위해 2D 경계 상자 대신 Mask-RCNN[18]에서 예측한 인스턴스 마스크를 채택합니다. 절단체 아키텍처에 대한 자세한 내용은 관심 있는 독자에게 그림 15(f)를 참조하십시오.  Pseudo LiDAR 기반 방법은 실제로 두 가지 방식, 즉 이미지와 포인트 클라우드의 시너지 효과를 탐구하기 위한 인식 수준의 정확도 성능 향상을 얻습니다.​요약하면, 단안/입체 영상 기반 방법은 장단점이 있는데. 이러한 방법은 색상 속성과 텍스처 정보를 제공하는 입력으로만 이미지를 사용합니다. 일반적으로 벡터 표현을 설계하기 위해 상당한 양의 도메인 전문 지식에 의존합니다. 깊이 정보가 없기 때문에 가능한 해결책 중 하나는 깊이 추정 알고리즘을 조사하는 것입니다. 자율 시스템의 경우 경제적 문제와 별개로 안전을 보장하기 위해 이중화가 필수적이므로 이미지 기반 방법은 향후 몇 년 동안 지속적인 영향을 미칠 것입니다. #3D #ObjectDetection #LiDAR​​ "
Obeject Detection(객체 탐지) - YOLO: Real-Time Object Detection 및 문제점 ,https://blog.naver.com/85honesty/222759309609,20220604,"YOLO: Real-Time Object Detection dailyman@dailyman-desktop:~$ df -HFilesystem      Size  Used Avail Use% Mounted on/dev/mmcblk0p1   62G   15G   45G  25% /none            1.9G     0  1.9G   0% /devtmpfs           2.1G   91k  2.1G   1% /dev/shmtmpfs           2.1G   21M  2.1G   1% /runtmpfs           5.3M  4.1k  5.3M   1% /run/locktmpfs           2.1G     0  2.1G   0% /sys/fs/cgrouptmpfs           415M  123k  415M   1% /run/user/1000 dailyman@dailyman-desktop:~$ lsdarknet  Documents  examples.desktop  Pictures  TemplatesDesktop  Downloads  Music             Public    Videosdailyman@dailyman-desktop:~$ rm -rf darknet/dailyman@dailyman-desktop:~$ lsDesktop    Downloads         Music     Public     VideosDocuments  examples.desktop  Pictures  Templates dailyman@dailyman-desktop:~$ git clone https://github.com/pjreddie/darknetCloning into 'darknet'...remote: Enumerating objects: 5946, done.remote: Total 5946 (delta 0), reused 0 (delta 0), pack-reused 5946Receiving objects: 100% (5946/5946), 6.37 MiB | 8.51 MiB/s, done.Resolving deltas: 100% (3928/3928), done. dailyman@dailyman-desktop:~$ cd darknet dailyman@dailyman-desktop:~/darknet$ makemkdir -p objmkdir -p backupmkdir -p resultsgcc -Iinclude/ -Isrc/ -Wall -Wno-unused-result -Wno-unknown-pragmas -Wfatal-errors -fPIC -Ofast -c ./src/gemm.c -o obj/gemm.ogcc -Iinclude/ -Isrc/ -Wall -Wno-unused-result -Wno-unknown-pragmas -Wfatal-errors -fPIC -Ofast -c ./src/utils.c -o obj/utils.ogcc -Iinclude/ -Isrc/ -Wall -Wno-unused-result -Wno-unknown-pragmas -Wfatal-errors -fPIC -Ofast -c ./src/cuda.c -o obj/cuda.ogcc -Iinclude/ -Isrc/ -Wall -Wno-unused-result -Wno-unknown-pragmas -Wfatal-errors -fPIC -Ofast -c ./src/deconvolutional_layer.c -o obj/deconvolutional_layer.ogcc -Iinclude/ -Isrc/ -Wall -Wno-unused-result -Wno-unknown-pragmas -Wfatal-errors -fPIC -Ofast -c ./src/convolutional_layer.c -o obj/convolutional_layer.ogcc -Iinclude/ -Isrc/ -Wall -Wno-unused-result -Wno-unknown-pragmas -Wfatal-errors -fPIC -Ofast -c ./src/list.c -o obj/list.ogcc -Iinclude/ -Isrc/ -Wall -Wno-unused-result -Wno-unknown-pragmas -Wfatal-errors -fPIC -Ofast -c ./src/image.c -o obj/image.ogcc -Iinclude/ -Isrc/ -Wall -Wno-unused-result -Wno-unknown-pragmas -Wfatal-errors -fPIC -Ofast -c ./src/activations.c -o obj/activations.ogcc -Iinclude/ -Isrc/ -Wall -Wno-unused-result -Wno-unknown-pragmas -Wfatal-errors -fPIC -Ofast -c ./src/im2col.c -o obj/im2col.ogcc -Iinclude/ -Isrc/ -Wall -Wno-unused-result -Wno-unknown-pragmas -Wfatal-errors -fPIC -Ofast -c ./src/col2im.c -o obj/col2im.ogcc -Iinclude/ -Isrc/ -Wall -Wno-unused-result -Wno-unknown-pragmas -Wfatal-errors -fPIC -Ofast -c ./src/blas.c -o obj/blas.ogcc -Iinclude/ -Isrc/ -Wall -Wno-unused-result -Wno-unknown-pragmas -Wfatal-errors -fPIC -Ofast -c ./src/crop_layer.c -o obj/crop_layer.ogcc -Iinclude/ -Isrc/ -Wall -Wno-unused-result -Wno-unknown-pragmas -Wfatal-errors -fPIC -Ofast -c ./src/dropout_layer.c -o obj/dropout_layer.ogcc -Iinclude/ -Isrc/ -Wall -Wno-unused-result -Wno-unknown-pragmas -Wfatal-errors -fPIC -Ofast -c ./src/maxpool_layer.c -o obj/maxpool_layer.(중략).o obj/lstm_layer.o obj/l2norm_layer.o obj/yolo_layer.o obj/iseg_layer.o obj/image_opencv.ogcc -Iinclude/ -Isrc/ -Wall -Wno-unused-result -Wno-unknown-pragmas -Wfatal-errors -fPIC -Ofast -c ./examples/captcha.c -o obj/captcha.ogcc -Iinclude/ -Isrc/ -Wall -Wno-unused-result -Wno-unknown-pragmas -Wfatal-errors -fPIC -Ofast -c ./examples/lsd.c -o obj/lsd.ogcc -Iinclude/ -Isrc/ -Wall -Wno-unused-result -Wno-unknown-pragmas -Wfatal-errors -fPIC -Ofast -c ./examples/super.c -o obj/super.ogcc -Iinclude/ -Isrc/ -Wall -Wno-unused-result -Wno-unknown-pragmas -Wfatal-errors -fPIC -Ofast -c ./examples/art.c -o obj/art.ogcc -Iinclude/ -Isrc/ -Wall -Wno-unused-result -Wno-unknown-pragmas -Wfatal-errors -fPIC -Ofast -c ./examples/tag.c -o obj/tag.ogcc -Iinclude/ -Isrc/ -Wall -Wno-unused-result -Wno-unknown-pragmas -Wfatal-errors -fPIC -Ofast -c ./examples/cifar.c -o obj/cifar.ogcc -Iinclude/ -Isrc/ -Wall -Wno-unused-result -Wno-unknown-pragmas -Wfatal-errors -fPIC -Ofast -c ./examples/go.c -o obj/go.ogcc -Iinclude/ -Isrc/ -Wall -Wno-unused-result -Wno-unknown-pragmas -Wfatal-errors -fPIC -Ofast -c ./examples/rnn.c -o obj/rnn.ogcc -Iinclude/ -Isrc/ -Wall -Wno-unused-result -Wno-unknown-pragmas -Wfatal-errors -fPIC -Ofast -c ./examples/segmenter.c -o obj/segmenter.ogcc -Iinclude/ -Isrc/ -Wall -Wno-unused-result -Wno-unknown-pragmas -Wfatal-errors -fPIC -Ofast -c ./examples/regressor.c -o obj/regressor.ogcc -Iinclude/ -Isrc/ -Wall -Wno-unused-result -Wno-unknown-pragmas -Wfatal-errors -fPIC -Ofast -c ./examples/classifier.c -o obj/classifier.ogcc -Iinclude/ -Isrc/ -Wall -Wno-unused-result -Wno-unknown-pragmas -Wfatal-errors -fPIC -Ofast -c ./examples/coco.c -o obj/coco.ogcc -Iinclude/ -Isrc/ -Wall -Wno-unused-result -Wno-unknown-pragmas -Wfatal-errors -fPIC -Ofast -c ./examples/yolo.c -o obj/yolo.ogcc -Iinclude/ -Isrc/ -Wall -Wno-unused-result -Wno-unknown-pragmas -Wfatal-errors -fPIC -Ofast -c ./examples/detector.c -o obj/detector.ogcc -Iinclude/ -Isrc/ -Wall -Wno-unused-result -Wno-unknown-pragmas -Wfatal-errors -fPIC -Ofast -c ./examples/nightmare.c -o obj/nightmare.ogcc -Iinclude/ -Isrc/ -Wall -Wno-unused-result -Wno-unknown-pragmas -Wfatal-errors -fPIC -Ofast -c ./examples/instance-segmenter.c -o obj/instance-segmenter.ogcc -Iinclude/ -Isrc/ -Wall -Wno-unused-result -Wno-unknown-pragmas -Wfatal-errors -fPIC -Ofast -c ./examples/darknet.c -o obj/darknet.ogcc -Iinclude/ -Isrc/ -Wall -Wno-unused-result -Wno-unknown-pragmas -Wfatal-errors -fPIC -Ofast obj/captcha.o obj/lsd.o obj/super.o obj/art.o obj/tag.o obj/cifar.o obj/go.o obj/rnn.o obj/segmenter.o obj/regressor.o obj/classifier.o obj/coco.o obj/yolo.o obj/detector.o obj/nightmare.o obj/instance-segmenter.o obj/darknet.o libdarknet.a -o darknet -lm -pthread  libdarknet.a dailyman@dailyman-desktop:~/darknet$ wget https://pjreddie.com/media/files/yolov3.weights--2022-06-04 04:20:09--  https://pjreddie.com/media/files/yolov3.weightsResolving pjreddie.com (pjreddie.com)... 128.208.4.108Connecting to pjreddie.com (pjreddie.com)|128.208.4.108|:443... connected.HTTP request sent, awaiting response... 200 OKLength: 248007048 (237M) [application/octet-stream]Saving to: ‘yolov3.weights’yolov3.weights      100%[===================>] 236.52M  4.02MB/s    in 68s     2022-06-04 04:21:18 (3.49 MB/s) - ‘yolov3.weights’ saved [248007048/248007048] dailyman@dailyman-desktop:~/darknet$ ./darknet detect cfg/yolov3.cfg yolov3.weights data/dog.jpglayer     filters    size              input                output    0 conv     32  3 x 3 / 1   608 x 608 x   3   ->   608 x 608 x  32  0.639 BFLOPs    1 conv     64  3 x 3 / 2   608 x 608 x  32   ->   304 x 304 x  64  3.407 BFLOPs    2 conv     32  1 x 1 / 1   304 x 304 x  64   ->   304 x 304 x  32  0.379 BFLOPs    3 conv     64  3 x 3 / 1   304 x 304 x  32   ->   304 x 304 x  64  3.407 BFLOPs    4 res    1                 304 x 304 x  64   ->   304 x 304 x  64    5 conv    128  3 x 3 / 2   304 x 304 x  64   ->   152 x 152 x 128  3.407 BFLOPs    6 conv     64  1 x 1 / 1   152 x 152 x 128   ->   152 x 152 x  64  0.379 BFLOPs    7 conv    128  3 x 3 / 1   152 x 152 x  64   ->   152 x 152 x 128  3.407 BFLOPs    8 res    5                 152 x 152 x 128   ->   152 x 152 x 128    9 conv     64  1 x 1 / 1   152 x 152 x 128   ->   152 x 152 x  64  0.379 BFLOPs   10 conv    128  3 x 3 / 1   152 x 152 x  64   ->   152 x 152 x 128  3.407 BFLOPs   11 res    8                 152 x 152 x 128   ->   152 x 152 x 128   12 conv    256  3 x 3 / 2   152 x 152 x 128   ->    76 x  76 x 256  3.407 BFLOPs   13 conv    128  1 x 1 / 1    76 x  76 x 256   ->    76 x  76 x 128  0.379 BFLOPs   14 conv    256  3 x 3 / 1    76 x  76 x 128   ->    76 x  76 x 256  3.407 BFLOPs   15 res   12                  76 x  76 x 256   ->    76 x  76 x 256   16 conv    128  1 x 1 / 1    76 x  76 x 256   ->    76 x  76 x 128  0.379 BFLOPs(중략)   94 yolo   95 route  91   96 conv    128  1 x 1 / 1    38 x  38 x 256   ->    38 x  38 x 128  0.095 BFLOPs   97 upsample            2x    38 x  38 x 128   ->    76 x  76 x 128   98 route  97 36   99 conv    128  1 x 1 / 1    76 x  76 x 384   ->    76 x  76 x 128  0.568 BFLOPs  100 conv    256  3 x 3 / 1    76 x  76 x 128   ->    76 x  76 x 256  3.407 BFLOPs  101 conv    128  1 x 1 / 1    76 x  76 x 256   ->    76 x  76 x 128  0.379 BFLOPs  102 conv    256  3 x 3 / 1    76 x  76 x 128   ->    76 x  76 x 256  3.407 BFLOPs  103 conv    128  1 x 1 / 1    76 x  76 x 256   ->    76 x  76 x 128  0.379 BFLOPs  104 conv    256  3 x 3 / 1    76 x  76 x 128   ->    76 x  76 x 256  3.407 BFLOPs  105 conv    255  1 x 1 / 1    76 x  76 x 256   ->    76 x  76 x 255  0.754 BFLOPs  106 yoloLoading weights from yolov3.weights...Done!data/dog.jpg: Predicted in 83.144477 seconds.dog: 100%truck: 92%bicycle: 99% 기존 레퍼런스보다 인식률이 더 높게 나왔다dog 99% -> 100%  dailyman@dailyman-desktop:~/darknet$ ./darknet detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights data/dog.jpglayer     filters    size              input                output    0 conv     32  3 x 3 / 1   608 x 608 x   3   ->   608 x 608 x  32  0.639 BFLOPs    1 conv     64  3 x 3 / 2   608 x 608 x  32   ->   304 x 304 x  64  3.407 BFLOPs    2 conv     32  1 x 1 / 1   304 x 304 x  64   ->   304 x 304 x  32  0.379 BFLOPs    3 conv     64  3 x 3 / 1   304 x 304 x  32   ->   304 x 304 x  64  3.407 BFLOPs    4 res    1                 304 x 304 x  64   ->   304 x 304 x  64    5 conv    128  3 x 3 / 2   304 x 304 x  64   ->   152 x 152 x 128  3.407 BFLOPs    6 conv     64  1 x 1 / 1   152 x 152 x 128   ->   152 x 152 x  64  0.379 BFLOPs    7 conv    128  3 x 3 / 1   152 x 152 x  64   ->   152 x 152 x 128  3.407 BFLOPs    8 res    5                 152 x 152 x 128   ->   152 x 152 x 128    9 conv     64  1 x 1 / 1   152 x 152 x 128   ->   152 x 152 x  64  0.379 BFLOPs   10 conv    128  3 x 3 / 1   152 x 152 x  64   ->   152 x 152 x 128  3.407 BFLOPs   94 yolo   95 route  91   96 conv    128  1 x 1 / 1    38 x  38 x 256   ->    38 x  38 x 128  0.095 BFLOPs   97 upsample            2x    38 x  38 x 128   ->    76 x  76 x 128   98 route  97 36   99 conv    128  1 x 1 / 1    76 x  76 x 384   ->    76 x  76 x 128  0.568 BFLOPs  100 conv    256  3 x 3 / 1    76 x  76 x 128   ->    76 x  76 x 256  3.407 BFLOPs  101 conv    128  1 x 1 / 1    76 x  76 x 256   ->    76 x  76 x 128  0.379 BFLOPs  102 conv    256  3 x 3 / 1    76 x  76 x 128   ->    76 x  76 x 256  3.407 BFLOPs  103 conv    128  1 x 1 / 1    76 x  76 x 256   ->    76 x  76 x 128  0.379 BFLOPs  104 conv    256  3 x 3 / 1    76 x  76 x 128   ->    76 x  76 x 256  3.407 BFLOPs  105 conv    255  1 x 1 / 1    76 x  76 x 256   ->    76 x  76 x 255  0.754 BFLOPs  106 yoloLoading weights from yolov3.weights...Done!data/dog.jpg: Predicted in 82.429671 seconds.dog: 100%truck: 92%bicycle: 99% 기존 레퍼런스보다 인식률이 더 높게 나왔다​dog 99% -> 100%   [Reference]https://pjreddie.com/darknet/yolo/ YOLO: Real-Time Object DetectionYOLO: Real-Time Object Detection You only look once (YOLO) is a state-of-the-art, real-time object detection system. On a Pascal Titan X it processes images at 30 FPS and has a mAP of 57.9% on COCO test-dev. Comparison to Other Detectors YOLOv3 is extremely fast and accurate. In mAP measured at .5 I...pjreddie.com https://ultrakid.tistory.com/11 [YOLO] Jetson nano 에 YOLO 설치 및 실시간 객체 인식 예제 구동현재 Jetson nano에 깔려있는 CUDA 10.0 , JetPack 4.3 , OpenCV 3.4 버전을 기준으로 작성하였습니다. YOLO ? YOLO(You Only Look Once)는 이미지 내의 bounding box와 class probability를 single regression p..ultrakid.tistory.com ​ Demo needs OpenCV for webcam images.(opencv is installed and set opencv4=1)I am trying to do object detection from a video file by using https://github.com/pjreddie/darknet. I've installed libopencv-dev for opencv. I've set opencv4=1 in Makefile. And run this code. ./dark...stackoverflow.com ​ Survival Strategies for the Robot RebellionJoseph Chet Redmon Welcome to my website! I am a graduate student advised by Ali Farhadi . I work on computer vision. I maintain the Darknet Neural Network Framework , a primer on tactics in Coq , occasionally work on research , and try to stay off twitter . Outside of computer science, I enjoy skii...pjreddie.com GitHub - jugfk/buildYOLO: 젯슨나노에서 YOLO v3 (GPU + CUDNN + OpneCV 전용) 설치하기젯슨나노에서 YOLO v3 (GPU + CUDNN + OpneCV 전용) 설치하기. Contribute to jugfk/buildYOLO development by creating an account on GitHub.github.com ​욜로의 문제점- 셀마다 2개만 인식- 겹치는 객체는 동시에 인식불너무 작은 객체는 크기 위치 인식 어려움학습 데이터 기반이라서 없는 것만 바운딩 박스 일반화 안됨​바운딩 박스를 높이면 인식률을 높일 수 있다.(큰차이는 없다?)7X7에서  11X11​장점계산량이 적어서 빠르다  dailyman데일리남매일매일 좋은 추억 한두개씩 함께 만들어요😀 선한영향력 긍정적인변화 즐거운하루 같이 가치있게 만들어 가요😄www.youtube.com  구독과 공감 좋아요는 콘텐츠를 만드는 데 큰 힘이 됩니다. "
[Object Detection] mAP(mean Average Precision) metric 이해하기 ,https://blog.naver.com/qkrdnjsrl0628/222815918455,20220717,"mAP(mean Average Precision)     - Faster R-CNN, SSD와 같은 object detection의 정확도를 측정하는 유명한 평가 지표입니다.     * mAP를 이해하기 위해서는, (precision, recall, AP(Average Precision))에 대한 사전지식이 필요합니다.​Precision(정밀도) 와 Recall(재현율)* 해당 포스팅을 참고하시기 바랍니다.https://blog.naver.com/qkrdnjsrl0628/222521564717 3단원 ""분류를 위한 평가""분류의 성능 평가 지표 - 분류는 2개의 결괏값을 가지는, 이진 분류와 여러개의 결정 클래스 값을 가지는 ...blog.naver.com mAP(mean Average Precision) 구하는 공식 구현 import torchfrom collections import Counterdef mean_average_precision(pred_boxes, true_boxes, iou_threshold = 0.5, box_format = 'corners', num_classes = 20):  average_precision = []  epsilon = 1e-6  # 각각의 클래스에 대한 AP를 구합니다.  for c in range(num_classes):    detections = []    ground_truths = []    # 모델이 c를 검출한 bounding box를 detections에 추가합니다.    for detection in pred_boxes:      if detection[1] == c:        detections.append(detection)        # 실제 c인 bounding box를 ground_truths에 추가합니다.    for true_box in true_boxes:      if true_box[1] == c:        ground_truths.append(ture_box)        # amount_bboxes에 class에 대한 bounding box 개수를 저장합니다.    # 예를 들어, img 0은 3개의 bboxe를 갖고 있고, img 1은 5개의 bboxes를 갖고 있으면    # dictionary 형태로 개수가 지정됩니다.    amount_bboxes = Counter([gt[0] for gt in ground_truths])    # class에 대한 bounding boc 개수 만큼 0을 추가한다.    # amount_boxes     for key, val in amount_bboxes.items():      amount_bboxes[key] = torch.zeros(val)        # detections를 정확도 높은 순으로 정렬합니다.    detections.sort(key = lambda x : x[2], reverse = True)    TP = torch.zeros((len(detections)))    FP = torch.zeros((len(detections)))    total_true_bboxes = len(ground_truths)    # Tp, fp 를 구합니다.    for detection_idx, detection in enumerate(detections):      ground_truth_img = [bbox for bbox in ground_truths if bbox[0] == detection[0]]      num_gts = len(ground_truth_img)      best_iou = 0      for idx, gt in enumerate(ground_truth_img):        iou = intersection_over_union(torch.tensor(detection[3:]),                                      torch.tensor(gt[3:]),                                      box_format = box_format)                if iou > best_iou:          best_iou = iou          best_gt_idx = idx            if best_iou > iou_threshold:        if amount_bboxes[detection[0]][best_gt_idx] == 0:          TP[detection_idx] = 1          amount_bboxes[detection[0]][best_gt_idx] = 1        else:          FP[detection_idx] = 1    # 누적합    TP_cumsum = torch.cumsim(TP, dim = 0)    FP_cumsum = torch.cumsum(FP, dim = 0)    recalls = TP_cumsum / (total_true_bboxes + epsilon)    precisions = torch.divide(TP_cumsum, (TP_cumsum + FP_cumsum + epsilon))    precisions = torch.cat((torch.tensor([1]), precisions))    recalls = torch.cat((torch.tensor([0]), recalls))        # 그래프 적분    average_precision.append(torch.trapz(precisions, recalls))    return sum(average_precisions) / len(average_precisions) "
Stereo R-CNN based 3D Object Detection for Autonomous Driving(작성 中) ,https://blog.naver.com/dhson825/222116685403,20201015,"Peiliang Li, Xiaozhi Chen, and Shaojie Shen, Stereo R-CNN based 3D Object Detection for Autonomous Driving, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2019, pp. 7644-7652 ​[0. Abstract]​- 자율주행을 위한 3D 물체 인식 기법을 제시함.- Faster R-CNN을 Stereo-input으로 확장하여 양 이미지에서 물체를 인식하고 연관시킴.- Region Proposal Nework(RPN)에 추가로 branch를 추가하여 sparse keypoints, viewpoint, object dimension을 예측하고, 양 이미지의 2D box와 결합하여 coarse한 3D bounding box를 계산함.(Coarse : 정확한 2D projection, but 부정확할 수도 있는 3D position)- 그 후, 양 이미지의 RoI를 이용한 region-based photometric alignment 를 이용해 정확한 3D bounding box 위치를 계산함- depth나 3D position 정보를 요구하지 않음에도, 현존하는 거의 모든 image-based method에서 뛰어난 성능을 보임(KITTI dataset 기준으로 약 30%정도 더 뛰어난 성능)- Code는 공개되어있음(https://github.com/HKUST-Aerial-Robotics/Stereo-CNN)​​[1. Introduction]​- 대부분의 자율주행을 위한 3D 물체 인식 기법은 정확한 depth informatino을 제공하는 LiDAR를 이용함.- 하지만 ,LiDAR는 가격이 비싸고, 인식범위가 100m이내로 짧으며, 데이터가 sparse함.- monocular camera는 가격이 저렴하여 대안이 될 수 있음. 하지만, depth 정보 알기 힘듬.- stereo camera는 left-right photometric alignment를 통해 더 정확한 detph 정보를 주고, LiDAR에 비해 저렴하기 때문에 저자는 이를 이용한 3D 물체 인식 기법을 제시함.- stereo camera는 focal length와 baseline에 따라 인지 범위가 달라지기 때문에 더 넓은 범위를 인지하기 위해서 서로 다른 focal length와 baseline을 가진 stereo module들을 결합해야함.​- 저자는 이 연구에서 stereo 영상의 의미와 기하학적 정보(semantic and geometry information)를 충분히 활용하여 3D 물체에 대한 sparse하고 dense한 제약조건을 연구하여 정확한 3D 물체 인식 기법을 제안함. 그림 1. Network Architecture of The Proposed Stereo R-CNN- Network Architecture 는 위 사진과 같고 크게 세 부분으로 나눔. 1. Stereo RPN module  : 서로 대응하는 좌우 이미지의 RoI 제안을 출력함. 좌우 feature map에 RoIAlign을 적용한 후, 대응하는 RoI feature을 연결하여 물체의 카테고리를 분류하고, Stereo 회귀 분석(regression) branch에서 정확한 2D stereo box와 viewpoint, dimension을 역 추적함. ​ 2. Stereo regression  : Keypoint branch는 오직 왼쪽의 RoI feature만 사용하여 object keypoint를 예측함. 이 출력값은 3D box estimation을 위한 희박한(sparse) 제약조건(2D box, keypoints)을 형성함.​ 3. 3D box estimation  : 3D box의 코너와 좌우 이미지에서의 2D box + keypoint 사이의 projection관계를 공식화함.​- 3D localization 성능을 보장하기 위해 가장 중요한 것은 정밀한 3D box alignment.- 저자는 3D 물체 localization을 종단간 회귀 문제(end-to-end regression problem)가 아니라 학습 보조 기하학 문제(learning-aided geometry problem)로 간주함.- 물체의 특성을 명시적으로 활용하지 않는 depth input을 직접 사용하는 대신, 저자는 물체의 RoI를 독립 픽셀이 아닌 전체로 취급함.- Regular-shaped 물체의 경우,  coaser 3D bounding box를 통해 각 픽셀과 3D center 사이의 depth 관계를 유추해 낼 수 있음.- 저자는 좌측 이미지 RoI의 고밀도 픽셀을 3D 물체 center와의 depth 관계에 따라 우측 이미지에 배치하여 전체 photometric error를 최소화하는 최적의 center depth를 찾으려 함.- 그러므로, 모든 물체의 ROI들은 3D 물체 depth estimation을 위한 고밀도(dense) 제약조건을 형성함.- 3D box는 aligned depth 및 2D 측정에 따라 3d box 추정기(estimator)를 사용하여 추가로 rectify함.- 본 논문의 contribution은 다음과 같음 1. Stereo 영상에서 물체를 동시에 인지하고 연관시키는 stereo R-CNN approach 2.  keypoint 및 stereo 구속 조건을 이용하는(exploit) 3D box estimator 3. 3D 물체 localization 정확도를 보장하는 dense region-based photometric alignment method 4. KITTI dataset을 이용한 평가는 모든 현 수준의 이미지 기반 기법을 능가하며, LiDAR 기반 기법과도 비교가 충분히 가능함.​[2. Related Work] ​ 2.1. LiDAR-based 3D Object Detection​- 대부분의 현재 기술은 정확한 3D information을 얻기위해 LiDAR 사용함- 어떤 연구에서는 Point Cloud들을 BEV나 FV로 projection하고 이를, 구조화된 convolution network에 input으로 제공한다. 그 중 일부는 여기서 다양한 LiDAR representation(BEV or FV)들을 RGB 이미지와 퓨전하여 보다 고밀도(dense)의 정보를 얻음.- 어떤 연구에서는 구조화된 복셀 그리드 repesentiation을 활용하여 raw point cloud 데이터를 정량화 한 후, 2D/3D CNN을 이용해 3D 물체를 인지함. 그 중 일부 연구는 여러개의 frame을 입력값으로 활용하여 3D 물체인식(detection), 추적(tracking), 모션 예측(motion forecasting)을 동시에 진행함.- 어떤 연구는 point cloud들을 정량화하는 대신에 raw point cloud를 입력값으로 직접 사용하여 2D 물체 인식(detection)과 PointNet에서 도출된 frustum region을 기반으로 3D 물체의 위치를 추정함.​ 2.2. Monocular-based 3D Object Detection​- 어떤 연구는 monocular 이미지에서의 ground plane assumption, shape prior, contextual feature, instance segmentation을  이용한 3D 물체 proposal generation에 초점을 맞춤.- 어떤 연구는 2D box의 가장자리(edge)와 3D box의 코너 사이의 기하학적 관계(geometry relation)를 이용하여 3D box를 추정함. - 어떤 연구는 regular shape 차량의 keypoints들을 예측하여 희박한(sparse) 정보를 명시적으로 활용함. 3D 물체의 pose는 wireframe template fitting에 의해 제약조건이 형성됨(constrained).- 어떤 연구는 RGB 이미지와 monocular 카메라에서 생성된 depth map을 결합(concatenate)하여 3D 물체를 인지(detect)하는 end-to-end multi-level fusion을 제안함.- 최근에는, 그래픽 렌더링과 비교를 통해 3D 물체의 pose와 instance level의 segmentation을 모두 예측하는 inverse-graphic framework가 제안됨. - 그러나, monocular방식은 정확한 depth 정보를 얻을 수 없어 불가피한 어려움을 겪음.​ 2.3. Stereo-based 3D Object Detection ​- 3D 물체 인지를 위한 스테레오 비전의 사용은 극소수에 불과함.- 어떤 연구에서는 object size prior, ground-plane prior, depth 정보(예를 들면 free space나 point cloud 밀도)를 energy function으로 인코딩하여 3D 물체 인지 후보를 제안하는데에 초점을 맞춤. 그 3D 물체 후보는 R-CNN approach를 이용하여 물체의 pose와 2D box를 regression함.- 어떤 연구에서는 dynamic object case까지 Structure from Motion(SfM) approach를 확장하고, 공간정보와 시간정보를 퓨전하여 3D 물체와 ego-camera pose를 지속적으로 추적함.- 그러나, 그 어떤 연구도 raw 스테레오 이미지의 고밀도(dense) 물체 제약조건을 이용하지는 않음.​[3. Stereo R-CNN Network]​- 이번 섹션에서는 Stereo R-CNN Network architecture을 소개함.- Faster R-CNN과 비교하면, stereo R-CNN은 좌우 이미지의 2D bounding box를 동시에 검출하고 minor modification을 통해 연관시킴.- 저자는 backbone network로 weight-share ResNet-101과 FPN을 사용하여 좌우 이미지에서 일치하는 feature을 찾아냄.- 저자의 학습 타켓 디자인(training target design)의 이점을 활용하니 데이터 연관을 위한 추가 계산이 필요가 없음.  [참고 자료]​What is Anchor?​첫 등장anchor라는 개념은 Faster R-CNN에서 처음으로 제안되었습니다.​주요 사용처anchor는 대부분의 one-stage, two-stage detector에서 사용하며 대표적으로는 RetinaNet(one-stage)와 Faster R-CNN(two-stage)가 존재합니다.​anchor를 사용하는 목적object detection 문제에서 우리는 이미지 상에 물체 (object)가 있는 영역을 예측하여 Bounding box(BBox)를 그려야 합니다. 이 때, 이미지 전체를 한꺼번에 보고 특정 위치를 예측하는 것보다 특정 영역 BBox만을 보고 이 안에 물체가 있는지를 예측하는 편이 더 쉽습니다.따라서 이미지 상에 균일하게 많은 BBox(즉, anchor)를 그린 뒤에 이 anchor들 중 gt와 서로 겹치는 영역의 넓이를 기준으로 선별된 Anchor를 학습에 활용하게 됩니다.각 object detector 모델은 위 과정을 통해 선별된 anchor를 이용하여 anchor와 정답(ground-truth)과의 차이에 대해서 예측하도록 학습하게 됩니다. (BBox regression)(이 때, anchor의 크기가 적절하지 못한 경우에는 차이의 편차가 커지게 될 것이므로 학습이 어려워질 수 있어서 적절한 크기를 선정하는게 중요합니다.)anchor는 균일한 간격, 일정한 규칙으로 생성 하여, 물체가 특정 위치에 존재할 때만 탐지가 잘 되거나, 혹은 특정 위치에서는 탐지가 잘 되지 않는 현상을 줄입니다. 이러한 특성을 translation-Invariance라고 하며, 대부분의 object detector 모델이 개선하려고 노력하는 특징입니다.   3.1. Stereo RPN​- RPN(Region Proposal Network)은 sliding window 기반의 foreground detector임.- feature 추출 후에 3x3 convoution layer를 활용하여 channel을 감소시키고, sibling fully-connected layer 2개를 활용하여 사전에 정의된 multiple-scale box들로 고정된(anchored) 각 입력 위치(input location)에 대한 objectness와 regress box offset을 분류함.- FPN과 유사하게 multiple-scale feature map에서 anchor를 평가하여 피라미드 feature의 origin RPN을 수정함. 차이점으로는 저자가 각각의 scale에서 좌/우 feature map을 concatenate한 다음, stereo RPN network에 concatenated feature을 공급한다는 것임.​- 물체 detection과 association이 동시에 일어나는 것을 가능하게 하는 핵심은 objectness classifer와 stereo box regressor에 다른 ground truth(GT)를 할당하는 것임. 그림2. Different targets assignment for RPN classfication and regression- 위 그림에서와 같이, 저자는 objectness classification의 타겟으로 좌/우 GT박스(union GT box라고도 불림)의 union을 할당함.- Anchor는 하나의 union GT box와 IoU ratio가 0.7이상이면 positive label을, 어떤 union GT box와의 IoU ratio도 0.3 이하일 경우 negative label을 부여받음.- 이런 design의 이점으로 the positive anchor는 좌/우 object region을 모두 포함하는 경향이 있음.- 저자는 타겟 union GT box에 포함된 좌/우 GT box에 대한 positive anchor의 offset을 계산하고, 좌/우 이미지의 회귀분석(regression)에 각각 offset을 할당함.- Stereo regressor에는 6개의 term이 존재함 :  (u, v), (u', v') : 좌/우 Image space에서 2D box center의 x,y좌표(w, h) = 2D box의 너비와 높이- rectified image라서 좌/우 y좌표 및 높이의 offset은 같음.- 따라서, origin RPN implementation에서 4개의 output channel 대신에 stereo RPN regressor를 위한 6개의 output channel을 가짐.- 좌/우 proposal은 동일한 anchor에서 생선되어, objectness score을 공유하기 때문에 자연스럽게 one by one으로 연관시킬 수 있음.- 좌/우 RoI에서 NMS(Non-Maximum Suppression)을 통해 중복성(redundancy)를 줄인 후, 학습을 위해 좌/우 NMS에 보관된 항목에서 상위 2000개의 후보를 선정함. 테스트를 위해서는 상위 300개의 후보를 선정함.​ 3.2. Stereo R-CNN​  3.2.1 Stereo Regression​- Stereo RPN 이후에, 서로 대응되는 좌/우 이미지의 proposal pair가 생김- 저자는 RoI Align을 각각 좌/우 feature map에 적절한 pyramid level로 적용함.- 좌/우 RoI feature는 concatenate되고 두 개의 sequential fully-connected layer의 입력값으로 들어가 의미 정보(semantic information)를 추출함.- 저자는 물체의 class, stereo bounding box, dimension, viewpoint angle을 각각 예측하기 위해 4개의 sub-branch를 사용함.- box regression term은 3.1.에서 정의한 것과 동일함.- Note : viewpoint angle은 잘린(cropped) 이미지의 RoI에서 관측할 수 없는 object orientation과 다름. 그림 3. Relations between object orientation, azimuth, and viewpoint- Only Same viewpoints lead to same projection- 위 그림에서와 같이 카메라 프레임을 기준으로 차량 방향(vehicle orientation)을 theta, 카메라 센터를 기준으로 물체 방위각(object azimuth)을 beta로 나타냄.- 세 차량은 방향이 다르지만, 잘라낸(cropped) 이미지에서 그들의 projection은 정확하게 일치함.- 따라서, viewpoint angle = vehicle orientation + object azimuth (alpha = theta + beta)- 불연속성(discontinutiy)을 피하기 위해, 학습 타겟은 raw angle 값이 아닌 [sine alpha, cosine alpha]임.- stereo box와 object dimension을 사용하면 depth정보를 직관적으로 알 수 있으며, 3D position과 viewpoint angle 사이의 관계를 decouple하여 vehicle orientation을 알 수 있음.​- RoI를 샘플링할 때, 좌측 이미지의 RoI와 좌측 GT box사이의 최대 IoU 값이 0.5 이상이면 좌/우 RoI pair를 전경(foreground)로 간주함. 이 때, 우측 이미지의 RoI와 우측 GT box사이의 최대 IoU 값도 0.5보다 큼.- 좌 or 우 RoI의 최대 IoU 값이 [0.1, 0.5) 범위 내 있을 경우 배경(background)으로 간주함.- 전경(foreground)으로 간주된 RoI pair의 경우, 좌/우 RoI와 좌/우 GT box간의 offset을 계산하여 회귀 타겟(regression target)을 할당한다. - 여전히 좌/우 RoI의 y좌표 및 높이의 offset은 동일함.- dimension 예측을 위해, 저자는 ground truth와 미리 설정된 dimension(pre-set diemension prior)을 사이의 offset을 regression함   3.2.2 Keypoint Prediction​- Stereo box와 viewpoint angle 이외에도 저자는 box 가운데에 project된 3D box corner는 3D box estimation에 더 엄격한 제약조건(constraint)들을 제공함. 그림 4. Different targets assignment for RPN classification and regression- 위 그림에서와 같이 저자는 3D bounding box 하단의 네 모서리를 나타내는 4개의 3D semantic keypoint를 정의함.-  box 중간에 가시적으로 projected될 수 있는 3D semantic keypoint는 오직 하나 뿐임.- 저자는 이 semantic keypoint의 projection을 perspective keypoint로 정의함.- 4장과 Table 5에서 perspective keypoint가 3D box estimation에 기여하는 것을 볼 수 있음.- 저자는 regular-shaped object에 대한 instance mask의 단순한 대안(simple alternative)으로 기능하는 두개의 boundary keypoints를 예측함.- 두 boundary keypoints 사이의 영역만 current object에 속하며, 이는 5장에서 dense alignment에 사용됨.​- 저자는 Mask R-CNN에서 제안된 방식대로 keypoint를 예측함.- keypoint prediction에는 좌측 feature map만 사용함.- 그림1에서 본 것과 같이 14x14 RoI aligned feature map을 6개의 sequential 256d 3x3 convolution layer에 입력함 .그 후에는 ReLU layer에 입력됨.- 2x2 deconvolution layer는 출력 스케일을 28x28로 upsample 하는데 사용됨.- 2D box 이외에는 keypoint  中 u 좌표만이 추가적인 정보를 제공함.- 6x28x28 출력에서 height channel을 합하여 6x28의 예측(prediction)을 생성함. 그 결과로, RoI feature의 각 column이 합해져(aggregate) keypoint prediction에 기여함.- 첫 네개의 channel은 4개의 semantic keypoint가 해당되는 u location에 project될 확률을 나타냄.- 나머지 두 채널은 u가 좌/우측 boundary에 있을 확률을 나타냄.- 4개의 3D keypoint 중 오직 하나만 2D 박스 가운데에 visibly projected될 수 있으므로 Softmax함수가 4x28 output에 적용되어 오직 하나의 exclusive한 semantic keypoint가 하나의 location에 projected되게 함.- 이 전략은 semantic keypoints에 해당하는 perspective keypoint type의 발생가능한 confusion을 방지함.- 좌/우 boundary keypoints의 경우, 1x28 output에 softmax를 각각 적용함.​- 학습 中, perspective keypoint prediction을 위해 4x28 softmax output에 대한 cross-entropy loss를 최소화함.  - 4x28 output에서 오직 한 location만 perspective keypoint target으로 라벨링 된다.- box 가운데에 3D semantic keypoint가 visibly projected되지 않는 경우(truncation and orthogonal projection case)는 생략함.- boundary keypoints의 경우, 독립적으로 두개의 1x28 softmax output에서 crsso-entropy loss를 최소화함.- 각 foreground RoI는 GT box의 occlution 관계에 따라 좌우 boundary keypoints가 배정된다.​[4. 3D Box Estimation]​- 여기서는 sparse keypoints와 2D box를 이용해서 coarse 3D bounding box를 찾음.- 3D bounding box의 state는 다음과 같이 나타낼 수 있다. x,y,z, = center position, theta = horizontal orientation- 좌/우 2D box, perspective keypoints, regressed diemsnion을 고려할 때, 2D box와 keypoint의 reprojection error을 최소화하여 3D box 문제를 해결할 수 있음. 그림 5. Sparse constraints for the 3D box estimation- 위 그림에서와 같이 stereo box들과 perspective keypoint에서 7개의 measurement를 추출함. 첫 4개는 좌측 이미지에서의 left, top, right, bottom좌표를, 그 다음 2개는 우측 이미지에서의 left, right좌표를, 마지막은 perspective keypoint의 u좌표를 나타냄.- 각각의 measurement는 표현을 단순화하기위해 camera intrinsic에 의해 normalized됨.- perspective point를 고려할 때, 3D box corner와 2D box edge 사이의 correspondences를 유추가능.- 저자는 다른 논문에서 영감을 받아 3D-2D 관계를 아래처럼 공식화함. b는 stereo 카메라의 baseline을, w,h,l은 3D box의 dimension을 나타냄.총 7개의 방정식이 존재하고, w/2와 l/2의 부호는 해당하는 3D box corner를 기준으로 적절히 변경- 잘린 가장자리(truncated edges)는 7개의 방정식으로 표현가능함.- 이런 multivarate equation은 Gauss-Newton method를 이용해 해결함.- 저자는 다른 논문에서처럼 3D position과 orientation을 해결하기 위해 하나의 2D box와 size를 사용하기보다는, stereo box와 regressed된 dimension을 이용해 3d depth information을 더 robust하게 구함.- 관찰된 side-surface가 2개 미만이거나 perspective keypoint가 없는 경우(잘려나가거나, 정사영같은 경우), oreintation과 dimension은 단순히 기하학적인 제약조건으로 관측할 수 없음.(위 식으로 구할 수 없다는 의미)- 그래서 관찰이 불가능한 상태를 관찰하기 위해 그림3에서처럼 viewpoint angle alpha를 사용함.alpha = theta + arctan(-x/z)- 2D box와 perspective를 이용해 구한 coarse 3D box는 정확한 projection을 가지고 있으며, image에 잘 맞춰져 있어 후에 dense alignment를 가능케함.​[5. Dense 3D Box Alignment]​- 좌/우 bounding box는 객체 수준의 disparity information을 제공하므로 3D box bounding box를 대략적으로는 해결할 수 있음.- 그러나, stereo box는 7x7 RoI feature map의 high level information을 종합하여 회귀분석 됨.- 원본 image의 corner나 edge 같은 pixel-level information은 convolution filter을 거치면서 손실 됨.- sub-pixel 매칭 정확도를 얻기 위해, 저자는 pixel level의 고 해상도 정보를 활용하기 위해 raw image를 retrieve함.- 저자가 하려고 하는 것은 결과가 좋지 않은 부위에서 불연속성(SGM), 또는 edge에서 지나치게 부드러움(CNN 기반의 기법들)에 직면할 수 있는 pixe-wise disparity estimation 문제와 다름.- 저자는 오직 dense obejct patch만을 이용하여 3D bounding box의 disparity를 해결하려함. 즉, 하나의 변수를 해결하기 위해 많은 pixel 측정값들을 이용함.​- object를 정육면체로 취급하면서, 4장에서 해결한 3D bounding box의 중심과 각 픽셀 사이의 depth 관계에 대해 알고 있음.- background나 다른 object로 판명난 pixel을 제외하기 위해, 저자는 해당 region이 좌/우 boundary keypoint 사이에 있고, 그 region이 3D box의 아래쪽 절반에 있다는 이유로 유효한 RoI를 정의함.(차량의 아래쪽 절반은 3D box에 좀더 tightly하게 fit하기 때문)- 좌측 image의 유효한 RoI에서 normalized coordinate (u,v)에 위치한 pixel의 경우, photometric error은 다음과 같이 정의 됨. - I는 좌/우 image의 3채널 RGB vector을 나타냄.- delta z(= z(i) - z)는 3D 박스 중심pixel의 depth 차이임.- b는 baseline의 길이.- z가 저자가 해결하려고 하는 변수임.- bilinear interpolation을 이용해 우측 이미지에서 sub-pixel value를 얻음.- total matching cost는 valid RoI의 모든 pixel에 대해 SSD를 이용해서 다음과 같이 정의 됨. - 중심의 depth z는 matching cost E를 최소화하여 찾아냄, 저자는 효율적으로 depth를 enumerate하여 cost를 최소화하는 depth를 찾음.-  처음에는 대략적인 depth를 찾기 위해 0.5m 간격으로 initial value 주변의 depth값 50개를 열거하고, 최종적으로 정확한 aligned depth를 구하기 위해 대략적으로 구한 depth의 주변값 20개를 0.05m 간격으로 열거함.- 이후에 저자는 aligned depth를 fixed하여 전체 3D box를 교정해냄(오차를 확실하게 잡아냄).- 물체의 RoI를 기하학적 제약조건의 전체로 간주하면, 저자의 dense alignment 기법은 stereo depth estimation에서의 discontinuity, ill-posed problem을 겪지 않고, valid RoI의 각 픽셀이 물체의 depth estimation에 기여하기 때문에 intensity변화나 밝기 변화에도 robust함.- 이 방법은 효율적이며, depth 교정을 위한 모든 image 기반의 3D detection 기법에 light-weight plug-in module이 됨.- 비록 3D object가 3D cube에 정확하게 fit한건 아니지만, shape variation으로 인해 생기는 relative depth error는 global depth보다 훨씬 더 사소함(작음).- 그러므로, 저자의 geometry-constraint dense alignment는 object center의 depth estimaion을 정확하게 하게함.​[6. Implementation Details]​6.1 Network​​- 다른 논문처럼, anchor는 {32, 64, 128, 126, 512} 5개의 scale과 {0.5, 1, 2}의 3개의 ratio를 사용함.- original image는 짧은 쪽 기준 600 pixel로 resized됨.- stereo RPN의 경우, 좌/우 feature map의 concatenation으로 인해 512개의 layer가 아닌 1024 input channel을 final classification과 regression layer에 갖게됨.- 마찬가지로, R-CNN regress head에 512 input channel있음.- Titan Xp GPU기준 하나의 stereo pair 추론 시간은 0.28s임.​6.2. Training- multi-task loss를 다음과 같이 정의함. - p, r은 RPN과 R-CNN을 나타내고, box는 stereo box의 loss를, alpha는 viewpoint의 loss를, dim은 dimension의 loss를, key는 keypoint의 loss를 나타냄.- 각각의 loss에 weight를 곱해줌(다른 논문에서 가져온 식)- 좌/우 image를 뒤집고, 서로 교환함. viewpoint angle과 keypoint를 각각 반영하여 새로운 stereo image를 만듬. 이 과정을 통해 traing target은 두배로 늘어남.- 학습 중에는, 각각의 mini-batch에 1개의 stereo pair와 512개의 sample RoI를 보관함.- 저자는 weight decay을 0.0005, momentum을 0.9로 설정한 SGD를 이용한 네트워크로 학습함.- learning rate는 초기에 0.001이었다가 매 5개의 epoch마다 0.1씩 감소함.- 저자는 총 이틀에 걸쳐 20 epochs를 학습함.​[7. Experiments]​​​​​​ "
[Tensorflow]Mask Rcnn (object detection) ,https://blog.naver.com/rosijin/221599959953,20190730,"== tensorflow faster rcnn과 동일하게 세팅 후==다운 받아 압축 풀고, 복사 ( /tensorflow/models/research/object_detection/multi_object_mask)https://github.com/vijendra1125/Custom-Mask-RCNN-using-Tensorfow-Object-detection-API vijendra1125/Custom-Mask-RCNN-using-Tensorfow-Object-detection-APICustom Mask RCNN using Tensorflow object detection API - vijendra1125/Custom-Mask-RCNN-using-Tensorfow-Object-detection-APIgithub.com ===mask_rcnn_inception_v2_coco==pip install Cythonpip install contextlib2​​학습파일만들기python multi_object_mask/extra/create_mask_rcnn_tf_record.py --data_dir=multi_object_mask/dataset --annotations_dir=Annotations --image_dir=JPEGImages --output_dir=multi_object_mask/dataset/train.record --label_map_path=multi_object_mask/dataset/label.pbtxt​학습하기python legacy/train.py --train_dir=multi_object_mask/CP --pipeline_config_path=multi_object_mask/mask_rcnn_inception_v2_coco.config​그래프 배포python export_inference_graph.py --input_type=image_tensor --pipeline_config_path=multi_object_mask/mask_rcnn_inception_v2_coco.config --trained_checkpoint_prefix=multi_object_mask/CP/model.ckpt-2000 --output_directory=multi_object_mask/IG "
deep learning object detection 계보 ,https://blog.naver.com/phj8498/221772316253,20200115," deep learning object detection https://blog.editor.naver.com/editor?deviceType=mobile&returnUrl=https%3A%2F%2Fm.blog.naver.com%2Ftlqordl89%2F221757604373​A paper list of object detection using deep learning. I wrote this page with reference to this survey paper and searching and searching..Last updated: 2020/01/13  Update log ​​2018/9/18 - update all of recent papers and make some diagram about history of object detection using deep learning. 2018/9/26 - update codes of papers. (official and unofficial)2018/october - update 5 papers and performance table.2018/november - update 9 papers.2018/december - update 8 papers and and performance table and add new diagram(2019 version!!).2019/january - update 4 papers and and add commonly used datasets.2019/february - update 3 papers.2019/march - update figure and code links.2019/april - remove author's names and update ICLR 2019 & CVPR 2019 papers.2019/may - update CVPR 2019 papers.2019/june - update CVPR 2019 papers and dataset paper.2019/july - update BMVC 2019 papers and some of ICCV 2019 papers.2019/september - update NeurIPS 2019 papers and ICCV 2019 papers.2019/november - update some of AAAI 2020 papers and other papers.2020/january - update ICLR 2020 papers and other papers.  Table of Contents  Paper list from 2014 to now(2019) ​​The part highlighted with red characters means papers that i think ""must-read"". However, it is my personal opinion and other papers are important too, so I recommend to read them if you have time.   ​    2014 ​​[R-CNN] Rich feature hierarchies for accurate object detection and semantic segmentation | [CVPR' 14] |[pdf] [official code - caffe][OverFeat] OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks | [ICLR' 14] |[pdf] [official code - torch][MultiBox] Scalable Object Detection using Deep Neural Networks | [CVPR' 14] |[pdf][SPP-Net] Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition | [ECCV' 14] |[pdf] [official code - caffe] [unofficial code - keras] [unofficial code - tensorflow]  2015 ​​Improving Object Detection with Deep Convolutional Networks via Bayesian Optimization and Structured Prediction | [CVPR' 15] |[pdf] [official code - matlab][MR-CNN] Object detection via a multi-region & semantic segmentation-aware CNN model | [ICCV' 15] |[pdf] [official code - caffe][DeepBox] DeepBox: Learning Objectness with Convolutional Networks | [ICCV' 15] |[pdf] [official code - caffe][AttentionNet] AttentionNet: Aggregating Weak Directions for Accurate Object Detection | [ICCV' 15] |[pdf][Fast R-CNN] Fast R-CNN | [ICCV' 15] |[pdf] [official code - caffe][DeepProposal] DeepProposal: Hunting Objects by Cascading Deep Convolutional Layers | [ICCV' 15] |[pdf] [official code - matconvnet][Faster R-CNN, RPN] Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks | [NIPS' 15] |[pdf] [official code - caffe] [unofficial code - tensorflow] [unofficial code - pytorch]  2016 ​​[YOLO v1] You Only Look Once: Unified, Real-Time Object Detection | [CVPR' 16] |[pdf] [official code - c][G-CNN] G-CNN: an Iterative Grid Based Object Detector | [CVPR' 16] |[pdf][AZNet] Adaptive Object Detection Using Adjacency and Zoom Prediction | [CVPR' 16] |[pdf][ION] Inside-Outside Net: Detecting Objects in Context with Skip Pooling and Recurrent Neural Networks | [CVPR' 16] |[pdf][HyperNet] HyperNet: Towards Accurate Region Proposal Generation and Joint Object Detection | [CVPR' 16] |[pdf][OHEM] Training Region-based Object Detectors with Online Hard Example Mining | [CVPR' 16] |[pdf] [official code - caffe][CRAPF] CRAFT Objects from Images | [CVPR' 16] |[pdf] [official code - caffe][MPN] A MultiPath Network for Object Detection | [BMVC' 16] |[pdf] [official code - torch][SSD] SSD: Single Shot MultiBox Detector | [ECCV' 16] |[pdf] [official code - caffe] [unofficial code - tensorflow] [unofficial code - pytorch][GBDNet] Crafting GBD-Net for Object Detection | [ECCV' 16] |[pdf] [official code - caffe][CPF] Contextual Priming and Feedback for Faster R-CNN | [ECCV' 16] |[pdf][MS-CNN] A Unified Multi-scale Deep Convolutional Neural Network for Fast Object Detection | [ECCV' 16] |[pdf] [official code - caffe][R-FCN] R-FCN: Object Detection via Region-based Fully Convolutional Networks | [NIPS' 16] |[pdf] [official code - caffe] [unofficial code - caffe][PVANET] PVANET: Deep but Lightweight Neural Networks for Real-time Object Detection | [NIPSW' 16] |[pdf] [official code - caffe][DeepID-Net] DeepID-Net: Deformable Deep Convolutional Neural Networks for Object Detection | [PAMI' 16] |[pdf][NoC] Object Detection Networks on Convolutional Feature Maps | [TPAMI' 16] |[pdf]  2017 ​​[DSSD] DSSD : Deconvolutional Single Shot Detector | [arXiv' 17] |[pdf] [official code - caffe][TDM] Beyond Skip Connections: Top-Down Modulation for Object Detection | [CVPR' 17] |[pdf][FPN] Feature Pyramid Networks for Object Detection | [CVPR' 17] |[pdf] [unofficial code - caffe][YOLO v2] YOLO9000: Better, Faster, Stronger | [CVPR' 17] |[pdf] [official code - c] [unofficial code - caffe] [unofficial code - tensorflow] [unofficial code - tensorflow] [unofficial code - pytorch][RON] RON: Reverse Connection with Objectness Prior Networks for Object Detection | [CVPR' 17] |[pdf] [official code - caffe] [unofficial code - tensorflow][RSA] Recurrent Scale Approximation for Object Detection in CNN | | [ICCV' 17] |[pdf] [official code - caffe][DCN] Deformable Convolutional Networks | [ICCV' 17] |[pdf] [official code - mxnet] [unofficial code - tensorflow] [unofficial code - pytorch][DeNet] DeNet: Scalable Real-time Object Detection with Directed Sparse Sampling | [ICCV' 17] |[pdf] [official code - theano][CoupleNet] CoupleNet: Coupling Global Structure with Local Parts for Object Detection | [ICCV' 17] |[pdf] [official code - caffe][RetinaNet] Focal Loss for Dense Object Detection | [ICCV' 17] |[pdf] [official code - keras] [unofficial code - pytorch] [unofficial code - mxnet] [unofficial code - tensorflow][Mask R-CNN] Mask R-CNN | [ICCV' 17] |[pdf] [official code - caffe2] [unofficial code - tensorflow] [unofficial code - tensorflow] [unofficial code - pytorch][DSOD] DSOD: Learning Deeply Supervised Object Detectors from Scratch | [ICCV' 17] |[pdf] [official code - caffe] [unofficial code - pytorch][SMN] Spatial Memory for Context Reasoning in Object Detection | [ICCV' 17] |[pdf][Light-Head R-CNN] Light-Head R-CNN: In Defense of Two-Stage Object Detector | [arXiv' 17] |[pdf] [official code - tensorflow][Soft-NMS] Improving Object Detection With One Line of Code | [ICCV' 17] |[pdf] [official code - caffe]  2018 ​​[YOLO v3] YOLOv3: An Incremental Improvement | [arXiv' 18] |[pdf] [official code - c] [unofficial code - pytorch] [unofficial code - pytorch] [unofficial code - keras] [unofficial code - tensorflow][ZIP] Zoom Out-and-In Network with Recursive Training for Object Proposal | [IJCV' 18] |[pdf] [official code - caffe][SIN] Structure Inference Net: Object Detection Using Scene-Level Context and Instance-Level Relationships | [CVPR' 18] |[pdf] [official code - tensorflow][STDN] Scale-Transferrable Object Detection | [CVPR' 18] |[pdf][RefineDet] Single-Shot Refinement Neural Network for Object Detection | [CVPR' 18] |[pdf] [official code - caffe] [unofficial code - chainer] [unofficial code - pytorch][MegDet] MegDet: A Large Mini-Batch Object Detector | [CVPR' 18] |[pdf][DA Faster R-CNN] Domain Adaptive Faster R-CNN for Object Detection in the Wild | [CVPR' 18] |[pdf] [official code - caffe][SNIP] An Analysis of Scale Invariance in Object Detection – SNIP | [CVPR' 18] |[pdf][Relation-Network] Relation Networks for Object Detection | [CVPR' 18] |[pdf] [official code - mxnet][Cascade R-CNN] Cascade R-CNN: Delving into High Quality Object Detection | [CVPR' 18] |[pdf] [official code - caffe]Finding Tiny Faces in the Wild with Generative Adversarial Network | [CVPR' 18] |[pdf][MLKP] Multi-scale Location-aware Kernel Representation for Object Detection | [CVPR' 18] |[pdf] [official code - caffe]Cross-Domain Weakly-Supervised Object Detection through Progressive Domain Adaptation | [CVPR' 18] |[pdf] [official code - chainer][Fitness NMS] Improving Object Localization with Fitness NMS and Bounded IoU Loss | [CVPR' 18] |[pdf][STDnet] STDnet: A ConvNet for Small Target Detection | [BMVC' 18] |[pdf][RFBNet] Receptive Field Block Net for Accurate and Fast Object Detection | [ECCV' 18] |[pdf] [official code - pytorch]Zero-Annotation Object Detection with Web Knowledge Transfer | [ECCV' 18] |[pdf][CornerNet] CornerNet: Detecting Objects as Paired Keypoints | [ECCV' 18] |[pdf] [official code - pytorch][PFPNet] Parallel Feature Pyramid Network for Object Detection | [ECCV' 18] |[pdf][Softer-NMS] Softer-NMS: Rethinking Bounding Box Regression for Accurate Object Detection | [arXiv' 18] |[pdf][ShapeShifter] ShapeShifter: Robust Physical Adversarial Attack on Faster R-CNN Object Detector | [ECML-PKDD' 18] |[pdf] [official code - tensorflow][Pelee] Pelee: A Real-Time Object Detection System on Mobile Devices | [NIPS' 18] |[pdf] [official code - caffe][HKRM] Hybrid Knowledge Routed Modules for Large-scale Object Detection | [NIPS' 18] |[pdf][MetaAnchor] MetaAnchor: Learning to Detect Objects with Customized Anchors | [NIPS' 18] |[pdf][SNIPER] SNIPER: Efficient Multi-Scale Training | [NIPS' 18] |[pdf]  2019 ​​[M2Det] M2Det: A Single-Shot Object Detector based on Multi-Level Feature Pyramid Network | [AAAI' 19] |[pdf] [official code - pytorch][R-DAD] Object Detection based on Region Decomposition and Assembly | [AAAI' 19] |[pdf][CAMOU] CAMOU: Learning Physical Vehicle Camouflages to Adversarially Attack Detectors in the Wild | [ICLR' 19] |[pdf]Feature Intertwiner for Object Detection | [ICLR' 19] |[pdf][GIoU] Generalized Intersection over Union: A Metric and A Loss for Bounding Box Regression | [CVPR' 19] |[pdf]Automatic adaptation of object detectors to new domains using self-training | [CVPR' 19] |[pdf][Libra R-CNN] Libra R-CNN: Balanced Learning for Object Detection | [CVPR' 19] |[pdf][FSAF] Feature Selective Anchor-Free Module for Single-Shot Object Detection | [CVPR' 19] |[pdf][ExtremeNet] Bottom-up Object Detection by Grouping Extreme and Center Points | [CVPR' 19] |[pdf] | [official code - pytorch][C-MIL] C-MIL: Continuation Multiple Instance Learning for Weakly Supervised Object Detection | [CVPR' 19] |[pdf] | [official code - torch][ScratchDet] ScratchDet: Training Single-Shot Object Detectors from Scratch | [CVPR' 19] |[pdf]Bounding Box Regression with Uncertainty for Accurate Object Detection | [CVPR' 19] |[pdf] | [official code - caffe2]Activity Driven Weakly Supervised Object Detection | [CVPR' 19] |[pdf]Towards Accurate One-Stage Object Detection with AP-Loss | [CVPR' 19] |[pdf]Strong-Weak Distribution Alignment for Adaptive Object Detection | [CVPR' 19] |[pdf] | [official code - pytorch][NAS-FPN] NAS-FPN: Learning Scalable Feature Pyramid Architecture for Object Detection | [CVPR' 19] |[pdf][Adaptive NMS] Adaptive NMS: Refining Pedestrian Detection in a Crowd | [CVPR' 19] |[pdf]Point in, Box out: Beyond Counting Persons in Crowds | [CVPR' 19] |[pdf]Locating Objects Without Bounding Boxes | [CVPR' 19] |[pdf]Sampling Techniques for Large-Scale Object Detection from Sparsely Annotated Objects | [CVPR' 19] |[pdf]Towards Universal Object Detection by Domain Attention | [CVPR' 19] |[pdf]Exploring the Bounds of the Utility of Context for Object Detection | [CVPR' 19] |[pdf]What Object Should I Use? - Task Driven Object Detection | [CVPR' 19] |[pdf]Dissimilarity Coefficient based Weakly Supervised Object Detection | [CVPR' 19] |[pdf]Adapting Object Detectors via Selective Cross-Domain Alignment | [CVPR' 19] |[pdf]Fully Quantized Network for Object Detection | [CVPR' 19] |[pdf]Distilling Object Detectors with Fine-grained Feature Imitation | [CVPR' 19] |[pdf]Multi-task Self-Supervised Object Detection via Recycling of Bounding Box Annotations | [CVPR' 19] |[pdf][Reasoning-RCNN] Reasoning-RCNN: Unifying Adaptive Global Reasoning into Large-scale Object Detection | [CVPR' 19] |[pdf]Arbitrary Shape Scene Text Detection with Adaptive Text Region Representation | [CVPR' 19] |[pdf]Assisted Excitation of Activations: A Learning Technique to Improve Object Detectors | [CVPR' 19] |[pdf]Spatial-aware Graph Relation Network for Large-scale Object Detection | [CVPR' 19] |[pdf][MaxpoolNMS] MaxpoolNMS: Getting Rid of NMS Bottlenecks in Two-Stage Object Detectors | [CVPR' 19] |[pdf]You reap what you sow: Generating High Precision Object Proposals for Weakly-supervised Object Detection | [CVPR' 19] |[pdf]Object detection with location-aware deformable convolution and backward attention filtering | [CVPR' 19] |[pdf]Diversify and Match: A Domain Adaptive Representation Learning Paradigm for Object Detection | [CVPR' 19] |[pdf][GFR] Improving Object Detection from Scratch via Gated Feature Reuse | [BMVC' 19] |[pdf] | [official code - pytorch][Cascade RetinaNet] Cascade RetinaNet: Maintaining Consistency for Single-Stage Object Detection | [BMVC' 19] |[pdf]Soft Sampling for Robust Object Detection | [BMVC' 19] |[pdf]Multi-adversarial Faster-RCNN for Unrestricted Object Detection | [ICCV' 19] |[pdf]Towards Adversarially Robust Object Detection | [ICCV' 19] |[pdf]A Robust Learning Approach to Domain Adaptive Object Detection | [ICCV' 19] |[pdf]A Delay Metric for Video Object Detection: What Average Precision Fails to Tell | [ICCV' 19] |[pdf]Delving Into Robust Object Detection From Unmanned Aerial Vehicles: A Deep Nuisance Disentanglement Approach | [ICCV' 19] |[pdf]Employing Deep Part-Object Relationships for Salient Object Detection | [ICCV' 19] |[pdf]Learning Rich Features at High-Speed for Single-Shot Object Detection | [ICCV' 19] |[pdf]Structured Modeling of Joint Deep Feature and Prediction Refinement for Salient Object Detection | [ICCV' 19] |[pdf]Selectivity or Invariance: Boundary-Aware Salient Object Detection | [ICCV' 19] |[pdf]Progressive Sparse Local Attention for Video Object Detection | [ICCV' 19] |[pdf]Minimum Delay Object Detection From Video | [ICCV' 19] |[pdf]Towards Interpretable Object Detection by Unfolding Latent Structures | [ICCV' 19] |[pdf]Scaling Object Detection by Transferring Classification Weights | [ICCV' 19] |[pdf][TridentNet] Scale-Aware Trident Networks for Object Detection | [ICCV' 19] |[pdf]Generative Modeling for Small-Data Object Detection | [ICCV' 19] |[pdf]Transductive Learning for Zero-Shot Object Detection | [ICCV' 19] |[pdf]Self-Training and Adversarial Background Regularization for Unsupervised Domain Adaptive One-Stage Object Detection | [ICCV' 19] |[pdf][CenterNet] CenterNet: Keypoint Triplets for Object Detection | [ICCV' 19] |[pdf][DAFS] Dynamic Anchor Feature Selection for Single-Shot Object Detection | [ICCV' 19] |[pdf][Auto-FPN] Auto-FPN: Automatic Network Architecture Adaptation for Object Detection Beyond Classification | [ICCV' 19] |[pdf]Multi-Adversarial Faster-RCNN for Unrestricted Object Detection | [ICCV' 19] |[pdf]Object Guided External Memory Network for Video Object Detection | [ICCV' 19] |[pdf][ThunderNet] ThunderNet: Towards Real-Time Generic Object Detection on Mobile Devices | [ICCV' 19] |[pdf][RDN] Relation Distillation Networks for Video Object Detection | [ICCV' 19] |[pdf][MMNet] Fast Object Detection in Compressed Video | [ICCV' 19] |[pdf]Towards High-Resolution Salient Object Detection | [ICCV' 19] |[pdf][SCAN] Stacked Cross Refinement Network for Edge-Aware Salient Object Detection | [ICCV' 19] |[official code] |[pdf]Motion Guided Attention for Video Salient Object Detection | [ICCV' 19] |[pdf]Semi-Supervised Video Salient Object Detection Using Pseudo-Labels | [ICCV' 19] |[pdf]Learning to Rank Proposals for Object Detection | [ICCV' 19] |[pdf][WSOD2] WSOD2: Learning Bottom-Up and Top-Down Objectness Distillation for Weakly-Supervised Object Detection | [ICCV' 19] |[pdf][ClusDet] Clustered Object Detection in Aerial Images | [ICCV' 19] |[pdf]Towards Precise End-to-End Weakly Supervised Object Detection Network | [ICCV' 19] |[pdf]Few-Shot Object Detection via Feature Reweighting | [ICCV' 19] |[pdf][Objects365] Objects365: A Large-Scale, High-Quality Dataset for Object Detection | [ICCV' 19] |[pdf][EGNet] EGNet: Edge Guidance Network for Salient Object Detection | [ICCV' 19] |[pdf]Optimizing the F-Measure for Threshold-Free Salient Object Detection | [ICCV' 19] |[pdf]Sequence Level Semantics Aggregation for Video Object Detection | [ICCV' 19] |[pdf][NOTE-RCNN] NOTE-RCNN: NOise Tolerant Ensemble RCNN for Semi-Supervised Object Detection | [ICCV' 19] |[pdf]Enriched Feature Guided Refinement Network for Object Detection | [ICCV' 19] |[pdf][POD] POD: Practical Object Detection With Scale-Sensitive Network | [ICCV' 19] |[pdf][FCOS] FCOS: Fully Convolutional One-Stage Object Detection | [ICCV' 19] |[pdf][RepPoints] RepPoints: Point Set Representation for Object Detection | [ICCV' 19] |[pdf]Better to Follow, Follow to Be Better: Towards Precise Supervision of Feature Super-Resolution for Small Object Detection | [ICCV' 19] |[pdf]Weakly Supervised Object Detection With Segmentation Collaboration | [ICCV' 19] |[pdf]Leveraging Long-Range Temporal Relationships Between Proposals for Video Object Detection | [ICCV' 19] |[pdf]Detecting 11K Classes: Large Scale Object Detection Without Fine-Grained Bounding Boxes | [ICCV' 19] |[pdf][C-MIDN] C-MIDN: Coupled Multiple Instance Detection Network With Segmentation Guidance for Weakly Supervised Object Detection | [ICCV' 19] |[pdf]Meta-Learning to Detect Rare Objects | [ICCV' 19] |[pdf][Cap2Det] Cap2Det: Learning to Amplify Weak Caption Supervision for Object Detection | [ICCV' 19] |[pdf][Gaussian YOLOv3] Gaussian YOLOv3: An Accurate and Fast Object Detector using Localization Uncertainty for Autonomous Driving | [ICCV' 19] |[pdf] [official code - c][FreeAnchor] FreeAnchor: Learning to Match Anchors for Visual Object Detection | [NeurIPS' 19] |[pdf]Memory-oriented Decoder for Light Field Salient Object Detection | [NeurIPS' 19] |[pdf]One-Shot Object Detection with Co-Attention and Co-Excitation | [NeurIPS' 19] |[pdf][DetNAS] DetNAS: Backbone Search for Object Detection | [NeurIPS' 19] |[pdf]Consistency-based Semi-supervised Learning for Object detection | [NeurIPS' 19] |[pdf][NATS] Efficient Neural Architecture Transformation Searchin Channel-Level for Object Detection | [NeurIPS' 19] |[pdf][AA] Learning Data Augmentation Strategies for Object Detection | [arXiv' 19] |[pdf][EfficientDet] EfficientDet: Scalable and Efficient Object Detection | [arXiv' 19] |[pdf]  2020 ​​[Spiking-YOLO] Spiking-YOLO: Spiking Neural Network for Real-time Object Detection | [AAAI' 20] |[pdf]Tell Me What They're Holding: Weakly-supervised Object Detection with Transferable Knowledge from Human-object Interaction | [AAAI' 20] |[pdf]Distance-IoU Loss: Faster and Better Learning for Bounding Box Regression | [AAAI' 20] |[pdf]Computation Reallocation for Object Detection | [ICLR' 20] |[pdf]​ "
Object Detection이란? ,https://blog.naver.com/eun9659/221680452219,20191017,"출처 : https://deepbaksuvision.github.io/Modu_ObjectDetection/posts/01_00_What_is_Object_Detection.html 01. Object Detection 이란? · GitBookObject Detection이란? ​ Object Detection은 컴퓨터비전(Computer vision), 영상처리(image processing)와 관계가 깊은 컴퓨터 기술입니다. Computer Vision 대회에서 주로 다루는 Task들의 카테고리를 확인해보면 크게 다음과 같이 3가지로 구분될 수 있습니다. Classification Single Classification & Localization & Detection Multiple Object Detection & Localization & Classificatio...deepbaksuvision.github.io ​Object Detection이란?   ​Object Detection은 컴퓨터비전(Computer vision), 영상처리(image processing)와 관계가 깊은 컴퓨터 기술입니다.Computer Vision 대회에서 주로 다루는 Task들의 카테고리를 확인해보면 크게 다음과 같이 3가지로 구분될 수 있습니다.ClassificationSingle Classification & Localization & DetectionMultiple Object Detection & Localization & Classification   [컴퓨터 비전에서 다루는 3가지 주요 Task]​Computer Vision에서는 객체 검출(Object Detection), 객체 인식(Object Recognition), 객체 추적(Object Tracking) 세 가지 용어가 혼재되어 사용됩니다.​RecognitionObject가 어떤 것인지 구분합니다Object DetectionRecognition보다 더 작은 범위로써 Object의 존재 유무만 판단합니다.​Object Recognition을 하기 위해서는 해당 이미지 혹은 영상에 Object가 있고, 그것이 무엇이냐를 찾는 문제이기 때문에, Object Detection이 선행되어야 합니다.​일반적으로 Object Detection 알고리즘은 찾고자 하는 Object의 특징(feature)을 사전에 추출하고 주어진 영상 내에서 해당 특징를 검출(detection)하는 접근을 주로 사용합니다.전통적으로 영상처리에서 사용했던 Object Detection 알고리즘은 특징 엔지니어링(Feature Engineering)기법을 통하여 수학적으로 혹은 실용적으로 검증된 특징을 추출(Feature Extraction)하여 특징들의 분포(Distribution)에서 경계 결정(Boundary Decision)을 찾는 방법을 주로 사용했습니다. 전통적인 특징 추출(Feature Extraction) 방법은 Haar-like feature, HOG(Histogram of Oriented Gradient), SIFT(Scale Invariant Feature Transform), LBP(Local Binary Pattern), MCT(Modified Census Transform) 등이 있습니다.특징 추출(Feature Extraction) 후, 특징들의 분포(Distribution)에서 경계 결정(Boundary Decision)하는 알고리즘은 SVM(Support Vector Machine), Adaboost 등의 같은 검출 알고리즘(Classifier)을 사용하여 어떤 특징(Feature)의 분포가 객체(Object)를 표현하는지 그렇지 않은지를 구분하는 것을 통해서 객체(Object)를 검출하였습니다.   [기존 컴퓨터 비전에서 사용되오던 특징(Feature)들]​결론적으로 Object Detection Algorithms은 영상에서 전처리 등을 통해서 노이즈를 제거하거나, 이미지를 선명하게 만든 후에 해당 이미지에서 특징들을 추출하고, 이 특징들을 이용하여 Object Detection에 대해 분류(Classifier)하는 파이프라인(pipe line)을 따릅니다.​Object Detection Algorithms에 대해서 대략적으로 요약하면 다음과 같이 작동합니다.전처리 (Pre-processing)특징 추출 (Feature Extraction)분류 (Classifier)​최근에는 딥 러닝 중 CNN(Convolutional Neural Network)을 기반으로 한 다양한 Detection 및 Recognition 알고리즘이 발전되어왔습니다. 최근 딥러닝 알고리즘에서의 Object Detection 알고리즘은 Object Detection과 Recognition을 통합하여 처리하고 있습니다. 본 ebook에서는 고전의 Object Detection Algorithms을 다루는 것보다, 현재 트렌드인 딥러닝을 이용한 Object Detection을 소개할 예정이며, 해당 Object Detection Algorithm의 구현체를 공개할 예정입니다.   [경계 결정(Boundary Decision) 알고리즘]​​   [딥러닝 기반의 Object Detection 알고리즘 (R-CNN)]​ "
Object Detection - Haar filter를 이용한 얼굴 인식 실습 (2)  ,https://blog.naver.com/dongju0531hb/222442629880,20210723,"#시각지능 #객체검출 #AI #비전인식 #인공지능​0. Intro...저번시간에는 Haar filter에 대한 개념을 살펴보았다.핵심은 1. haar-like filter를 사용하여 feature를 추출하는 것2. integral image를 사용하여 window안에 pixel값을  매번 더하고 빼는 게 아니라 특정 위치 pixel 값만 빼고 더해도 됨4. adaboost알고리즘으로 feature 6000개를 찾아낸 것5. Cascading 구조로 얼굴이 아닌 부분을 빨리 skip할 수 있다는 것​이 정도로 요약할 수 있을 것 같다.이걸 다 일일히 구현할 필요 없고, 미리 구현된 cascading structure안에는 이미 haar-like filter가 체계적으로 set되어 있는openCV 라이브러리를 이용하면 함수에 이미지와 값만 넣으면 자동으로 된다.​  1. CascadingClassifier 클래스 정의 class CascadeClassifier{  public:     CascadeClassfier(); // 클래스 생성자     CascadeClassfier(const String& filename); // 클래스를 생성하는 동시에 분류기 로드할 수 있음     ~CascadeClassfier();     bool load(const String& filename); // 클래스 생성 후 분류기 로드할 때     bool empty() const; // 클래스에 분류기가 잘 설정됐는지 체크     // image를 주면 얼굴이 있는 부위에 <Rect> 형식의 object에 저장할 수 있다.     void detectMultiScale(InputArray image,                           std::vector<Rect>& objects,                           double scaleFactor = 1.1,                           int minNeighbors =3, int flag=0,                           Size minSize = Size(),                           Size maxSize = Size());    ... 생략}; 이 클래스는미리 훈련된 Object Detection Classifier(객체 검출 분류기) XML 파일을 불러오는 기능과 주어진 영상에서 객체를 검출하는 기능으로 이루어져 있다.​​  2. XML 파일을 불러오는 기능먼저 CascadeClassfiier 객체를 생성한 후에는 미리 훈련된 분류기 정보를 불러올 수 있다.이 Classifier(분류기) 정보는 XML 파일 형식으로 저장되어 있다.​자 아래와 같이 classifier 이름으로 선언을 해주고,classifier.load(""여기에는 xml형식의 파일 "")을 해주면 된다.참고로 xml 파일은 자신의 opencv를 설치한 폴더에 build → etc → haarcascades에 있다.  haarcascade xml 파일 위치ex)  C:\opencv\build\etc\haarcascades여기에 사용할 xml파일을 복사해서 나의 project가 있는 폴더 위치안에 붙어넣어주면 된다.아래는 기본적으로 안면인식하는 haarcascade_frontalface_default.xml 파일을 사용한다. CascadeClassifier classifier;classifier.load(""haarcascade_frontalface_default.xml""); ​위와 같이 class를 선언한 후 load 함수로 불러올 수도 있고, 아래처럼 class 선언 동시에 호출할 수도 있다. 아래가 더 편함 ㅇㅇ CascadeClassifier classifier(""haarcascade_frontalface_default.xml""); ​여기서 궁금점 xml 파일 안은 어떻게 생겨먹었을까?안에서 처럼 비올라-존스 알고리즘에 필요한 parameter들이 정말 많이 담겨 있고,xml(html과 유사한) 구문으로 저장되어 있는 것을 볼 수 있다. haarcascade_frontalface_default.xml 파일 코드​​​​  3. XML 파일 잘 불러왔는 지 check​아래와 같이 선언한 class이름의 empty() 멤버 함수를 사용하면 선언된 클래스에 xml파일을 잘 불러왔는 지 확인할 수 있다.만약 empty()가 true를 반환하면 return되서 프로그램이 종료되겠지? if(classifier.empty()) {  cerr << ""XML load failed"" << endl;  return;} ​  4. detectMultiScale 멤버함수를 이용하여 객체 검출을 실행할 수 있다. void detectMultiScale(InputArray image,                           std::vector<Rect>& objects,                           double scaleFactor = 1.1,                           int minNeighbors =3, int flag=0,                           Size minSize = Size(),                           Size maxSize = Size()); image입력영상, CV_8U (픽셀 값 0~ 255) 깊이의 행렬objects검출된 객체의 사각형 좌표 정보 (이 함수의 실질적 출력이고, objects 변수에 함수의 결과값이 담긴다)scaleFactor검색 윈도우 확대 비율(설정 안하면 기본 값 1.1). 1보다 커야 한다.다양한 크기의 얼굴을 검출하기 위해 처음에는 작은 크기의 검색 윈도우를 이용하여 객체를 검출하고,이후 scaleFactor 값의 비율로 검색 윈도우 크기를 확대시키면서 여러 번 객체를 검출합니다.minNeighbors검출 영역으로 선택하기 위한 최소 검출 횟수 (설정 안하면 기본값 3). 검출할 object 영역에서 얼마나 많은 사각형이 중복되어 검출되어야 최종적으로 object 영역으로 설정할지를 지정한다.​즉 비올라-존스를 알고리즘으로 통과하더라도, 그 부분 주변 부위도 역시 비올라-존스 알고리즘을 통과하면,얼굴 특징을 가질 확률이 더 높다는 것이다. 근데 이 최소 검출횟수가 너무 커버리면, 검출 민감도가 떨어지는 반비례관계이다.flags현재 사용되지 않음minSize검출할 객체의 최소 크기maxSize검출할 객체의 최대 크기 <Rect> 객체 type의 vector(직사각형 object를 저장할 수 있는 stack 구조라고 생각하면 됨!)를 faces라고 선언classifier 멤버함수 detectMultiScale(src, faces); image와 objects 인자만 전달 나머지는 기본값을 사용이러면 faces에 얼굴이 검출된 부위에 직사각형 정보가 faces(stack 구조)에 차곡차고 쌓인다. vector<Rect> faces;classifier.detectMultiScale(src,faces); ​​​  5. 시각화​그럼 이렇게 쌓인 faces의 rect 구조에서원소 하나를 Rect rc 로 갖고와서 rc를 src에 다시 표시(시각화)한다.반복문을 이용해서 rectangle( ) 함수로 그려준다.그 후 imshow하면 됨! for(Rect rc : faces){   rectangle(src,rc,Scalar(255,0,255),2);}imshow(""src"",src);waitKey(0);destoryAllWindows(); ​6. 컴퓨터 웹캠을 이용하여 얼굴 인식하는 소스 코드 #include ""opencv2/opencv.hpp""#include <iostream>using namespace cv;using namespace std;int main(){	VideoCapture cap(0);   // cap(0) 웹 캠 이미지를 얻기 위한 class 생성    /* camera가 연결됐는지 확인 */	if (!cap.isOpened()) { 		cerr << ""Camer open failed!"" << endl;		return;	}    	Mat frame; // 카메라로부터 받아오기 위한 Mat 객체 생성    /* cascade class 앞면 얼굴 생성 */	CascadeClassifier classifier(""haarcascade_frontalface_default.xml"");	    /* cascade class eye 생성 */    CascadeClassifier eye_classifier(""haarcascade_eye.xml"");	    /* classifier가 xml 파일 제일 로드 됐는지 확인 */    if (classifier.empty() || eye_classifier.empty()) {		cerr << "" XML load failed!"" << endl;		return;	}    // while 문을 통해 매번 frame 받아오기	while (true) {        // cap에서 frame 보내기		cap >> frame;        // frame이 비어있는지 check		if (frame.empty())			break;        // 탐지했을 때 나타낼 직사각형의 정보를 담을 변수 생성		vector<Rect> faces;        // frame에서 비올라-존스 알고리즘으로 faces에 탐지된 영역 정보 저장		classifier.detectMultiScale(frame, faces);               // C++11 문법임. faces에서 face이름의 Rect 객체 하나하나 개별적으로 가져옴)		for (Rect face : faces) {            // rectangle( ) 함수는 frame에 face 직사각형을 cyan색깔로 두께 2로 그려줌			rectangle(frame, face, Scalar(255, 0, 255), 2);            // frame에서 (face) 영역만 추출해서 faceROI에 저장			Mat faceROI = frame(face);            // 그리고 face영역 이미지에서만 eye를 탐지했을 때 정보저장을 위한 변수 선언			vector<Rect> eyes;            // faceROI에서 eyes만 인식			eye_classifier.detectMultiScale(faceROI, eyes);			for (Rect eye : eyes) {				Point center(eye.x + eye.width / 2, eye.y + eye.height / 2);				circle(faceROI, center, eye.width / 2, Scalar(255, 0, 0), 2, LINE_AA); //얼굴 인식영역, 중심, 눈 너비, 파란색, 2 LINE_aa			}		}		imshow(""frame"", frame);		if (waitKey(10) == 27)			break;	}	destroyAllWindows();} ​위에는 웹캠을 이용한 얼굴감지, 아래는 이미지 한장을 얼굴감지 했을 때 결과이다. 이렇게 비올라- 존스 얼굴 감지 알고리즘을 이용한 결과는 다음과 같다. ​  함수에 대해서 모르는 부분이 있으면 질문 주세용OpenCV 4로 배우는 컴퓨터 비전과 머신러닝​ "
[TensorFlow] - YOLO(Object Detection) ,https://blog.naver.com/minsu_jj/221606375660,20190805,"현재 ETRI 연구소에서 인턴생활을 하고 있습니다. 하지만, 제가 많이 부족하여 Object Detection 부분에 대해서 짧게 나마 공부하고더 나아가 Object Detection에 관련된 대한 논문들을 읽고 공부하는 글을 작성할 예정입니다.아직 많이 부족하지만, 제가 공부했던 내용들을 한번 읽어봐주시고 부족한 부분 있으면 피드백 해주시면 감사하겠습니다.​이번주는 Object Detection에 관해서 작성하고 다음에는 YOLO에 대해서 공부하여 작성할 예정입니다.​출처 - https://nuggy875.tistory.com/20​0. Backgournd  먼저 YOLO를 알아보기 전에, Object Detection이 어떠한 것일까?Object Detection 이란 여러 물체에 대해 어떤 물체인지 분류하는 Classification 문제와그 물체가 어디 있는지 네모 박스를 통해서(Bounding Box) 위치 정보를 나타내는 Localization 문제를 둘 다 해내야 하는 분야를 뜻한다.​한마디로, Object Detection = 여러 가지 물체에 대한 Classification + 물체의 위치정보를 파악하는 Localization이라고 할 수 있다.​Object Detection = Multi-labeled Classification + Bounding Box Regression(Localization)​Object Detection은 자율 주행 자동차, CCTV Suveilllance, 스포츠 경기, 무인점포 등등 많은 곳에 쓰인다.​Deep Learning을 이용한 Object Detection 분야를 관련 최신 논문들의 흐름을 통해 알아보도록 하자.   2014년부터 2018년도까지 Object Detection에 관련된 논문을 정리한 그림이다. 이 논문들은 크게 2가지로 분류할 수 있는데,1-Stage Detector와 2-stage Detector로 나눌 수 있다.Object Detection 문제는 앞에서 말했듯이 물체의 위치를 찾는 Localization 문제와, 물체를 식별하는 Classification 문제를 한한 것인데,​  출처 - http://zoey4ai.com/2018/05/12/deep-learning-object-detection/ ​1-stage Detector는 이 두 문제를 동시에 행하는 방법이고,2-stage Detector는 이 두 문제를 순차적으로 행하는 방법이다.​따라서 1-stage Detector는 가 비교적으로 빠르지만 정확도가 낮고2-stage Detector가 비교적으로 느리지만 정확도가 높다.​2-Stage Detector은 CNN을 처음으로 적용시킨 R-CNN부터 Fast R-CNN, Faster R-CNN 등의 R-CNN 계열이 대표적이고,1-Stage Detector는 YOLO(You Lock Only Once) 계열과 SSD 계열 등이 포함된다.​​Object Detection의 간략한 개요를 공부하여 작성해봤습니다.다음주에는, 1-Stage Detector인 YOLO(You Lock Only Once)에 대해 논문을 읽고 정리해서블로그에 작성할 예정입니다.혹시나 틀린 부분이나 좀 더 보충해야 되는 부분들이 있다면 댓글로 남겨주시면 정말 감사하겠습니다.​​#tensorflow #ObjectDetection #YOLO "
6. TensorFlow2 Object Detection API설치_(2) Training Custom Object Detector   ,https://blog.naver.com/alsdhr0155/222583807470,20211201,"안녕하세요?지금까지 1) TensorFlow 설치, 2) TensorFlow Object Detection API설치했습니다. 앞으로 우리가 해야 할 것은 다음과 같습니다.​1. How to organise your workspace/training files2. How to prepare/annotate image datasets3. How to generate tf records from such datasets4. How to *configure a simple training pipeline5. How to train a model and monitor it’s progress6.How to export the resulting model and use it to detect objects.​* configure 스크립트는 개발 중인 프로그램을 각기 다른 수많은 컴퓨터들에서 실행하고 도와주도록 설계된 실행 스크립트이다. 소스 코드로부터 컴파일하기 직전에 사용자 컴퓨터의 라이브러리의 존재 여부를 확인하고 연결시킨다  1. Preparing the Workspace학습모델을 설치하기 전에, 이를 작업할 파일을 미리 만들어줄겁니다. 자세한 과정은 링크 참조 부탁드립니다.(너무 잘 설명되어있어요!!!) 아래는 각 파일의 용도 설명입니다! annotations : This folder will be used to store all *.csv files and the respective TensorFlow *.record files, which contain the list of annotations for our dataset images.exported-models : This folder will be used to store exported versions of our trained model(s).images : This folder contains a copy of all the images in our dataset, as well as the respective *.xml files produced for each one, once labelImg is used to annotate objects.images/train : This folder contains a copy of all images, and the respective *.xml files, which will be used to train our model.images/test : This folder contains a copy of all images, and the respective *.xml files, which will be used to test our model.models : This folder will contain a sub-folder for each of training job. Each subfolder will contain the training pipeline configuration file *.config, as well as all files generated during the training and evaluation of our model.pre-trained-models : This folder will contain the downloaded pre-trained models, which shall be used as a starting checkpoint for our training jobs.README.md : This is an optional file which provides some general information regarding the training conditions of our model. It is not used by TensorFlow in any way, but it generally helps when you have a few training folders and/or you are revisiting a trained model after some time.  2. Preparing the Dataset1) Annotate the Dataset: labelimg를 설치합니다! 저는 데이터 라벨링 작업을 미리 해놓았기 때문에 다시 설치할 필요는 없었어요. 설치 하셨다면 아나콘다 프롬프트에서 [labelimg]만 치시면 나옵니다.링크 내용 중 Use precompiled binaries (Easy) 부분 보시고 설치하시면 됩니다! 2) Annotate Images: 라벨링 작업 하시면 됩니다. [Open Dir]을 눌러서 작업하실 이미지 파일이 들어있는 폴더를 불러오세요. 그 후 [Change Save Dir]로 작업하실 이미지 폴더를 한번 더 선택해주세요. 작업하시면 해당 폴더에 이미지파일과 함께 라벨링 정보가 담긴 xml파일이 함께 저장됩니다. - [W] : 박스를 만들 수 있습니다.- ctrl + shift + s : 고정되어있는 박스 규격 자유형태로 수정use default label : 반복적으로 사용하는 키워드를 설정 가능...등 여러 기능이 있습니다. 자세한 사항은 구글링 참고해주세요!  3. Partition the Dataset: 자, 지금까지 우리는 설치에 필요한 기본적인 파일을 설치했고, 학습시킬 데이터까지 만들었어요! 이제 데이터셋을 나눠볼까요? 일반적으로 학습시킬(train)할 데이터와 test할 데이터의 비율을 9:1이라고해요. 이게 무슨 소리냐면 90만큼 공부시켜서 그 공부한 것을 토대로 수능을 본다는 것입니다. 얼마나 정확하게, 많이 공부했는지 나머지 ‘1’만큼의 시험지에 test하는 것이죠.  해당 링크에 직접 9:1로 나누기 귀찮은 사람을 위한 py파일을 만들주었어요. 저희는 tt/scripts/preprocessing 폴더에 py 파일을 넣고, python partition_dataset.py -x -i [PATH_TO_IMAGES_FOLDER] -r 0.1 python partition_dataset.py -x -i [PATH_TO_IMAGES_FOLDER] -r 0.1을 실행시키면 [training_demo/images] 에 작업했던 이미지파일과 xml 파일이 9:1(train:test 비율)로 나뉘면서 각 train, test 파일이 생깁니다! ﻿(tt) C:\tf_1\scripts\preprocessing>python partition_dataset.py -x -i C:\tf_1\workspace\training_demo\images -r 0.1  4. Create Label Map: 텐서플로는 각 라벨의 정수값을 담고있는 label map을 필요로해요. 따라서 우리는 이것을 만들어줄겁니다! item {id: 1name: 'exclude'}item {id: 2name: 'id_ko'} 위의 형식으로 자신이 만든 라벨 만큼 쭉 적으시면 됩니다. 제가 작성한 것 중 일부만 가져와봤어요! 그리고 이 때 라벨링 xml 파일에 한글이 있으면 안됩니다. 다 영어로 바꿔주셔야해요. 저는 pbtxt라는 파일 형식은 처음 봐서 도대체 label map을 어떻게 만들어야하는지 감이 안오더라고요ㅠ 진짜 별것도 아닌 것 때문에 구글링에만 몇시간 쓴 것 같아요.. 의미있는 삽질이랄까요? 아 몰라요 진짜 짜증났어요 사실.. 여러분은 꼭 편한 길만 걸으세요.. 종류 상관없이 IDE에 저렇게 적으신 후에 (저는 가장 유명한 python idle에 작성했어요!) txt 파일로 저장하시면 돼요! 사실 그냥 메모장에 적으셔도 상관없어요..! 파일 이름만 [label_map.pbtxt]로 저장해주세요! 그리고 이 파일을[./training_demo/annotations] 폴더에 위치해주세요!   5. Create TensorFlow Records: 개인적으로 오류 천국이라 너무 속상했던 부분...! 오류를 찾다가 정작 내가 뭘 하고 있었는지, 어떤 오류를 해결하려했는지 방황했답니다.. 지금와서 보니 별것도 아닌데 ㅠㅠ 어쨌든, 지금까지 어노테이션 파일을 만들었고, 데이터셋을 train용과 test용을 나눴습니다. 이제 우리의 annotations들을 [TFRecord]로 바꿀 때에요!! 1) Convert *.xml to *.record: 해당 목차 내용에서 파이썬 스크립트를 저장하신 후에 TensorFlow/scripts/preprocessing파일로 옮기세요! : conda install pandas (자신의 환경에 맞춰서 pandas를 설치하세요! 저는 conda) : 그 후 cd 키워드로 TensorFlow/scripts/preprocessing 연결 후 1) train data 2) test data를 만들어주세요! 아래 형식에 맞춰 실행시키시면 됩니다. # Create train data:python generate_tfrecord.py -x [PATH_TO_IMAGES_FOLDER]/train -l [PATH_TO_ANNOTATIONS_FOLDER]/label_map.pbtxt -o [PATH_TO_ANNOTATIONS_FOLDER]/train.record # Create test data:python generate_tfrecord.py -x [PATH_TO_IMAGES_FOLDER]/test -l [PATH_TO_ANNOTATIONS_FOLDER]/label_map.pbtxt -o [PATH_TO_ANNOTATIONS_FOLDER]/test.record #예제 #(ten) C:\tf_1125\scripts\preprocessing>python generate_tfrecord.py –x C:\tf_1125\workspace\training_demo\images\train -l C:\tf_1125\workspace\training_demo\annotations\label_map.pbtxt –o C:\tf_1125\workspace\training_demo\annotations\train.record//Successfully created the TFRecord file: C:\tf_1\workspace\training_demo\annotations\train.record#(ten) C:\tf_1125\scripts\preprocessing>python generate_tfrecord.py -x C:\tf_1125\workspace\training_demo\images\test –l C:\tf_1125\workspace\training_demo\annotations\label_map.pbtxt –o C:\tf_1125\workspace\training_demo\annotations\test.record//Successfully created the TFRecord file: C:\tf_1\workspace\training_demo\annotations\test.record 위 코드 실행시 test.record와 train.record 파일이 [C:\tf_1\workspace\training_demo\annotations]에 저장됩니다.  6. Configuring a Training Job: 저희는 학습을 위해 TensorFlow에서 이미 제공하고 있는 pre-trained models을 사용할거에요! 따라서 새로운 모델을 train 하시려면 TensorFlow 튜토리얼에서 확인하실 수 있습니다. 저희가 사용할 모델은 SSD ResNet50 V1 FPN 640x640입니다. 해당 모델이 성능과 속도가 적절히 균형 맞춰 좋기 때문이라네요!! 자신의 학습 용도에 맞춰 원하시는 모델을 다운 받으면 될 것 같습니다.  1) Download Pre-Trained Modelhttps://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md 여기에서 다운받으세요!  다운 받았을 때 파일 형식은 *.tar.gz file 이어야 합니다. 저는 이 파일 형식은 처음이라 열리지도 않고 그래서 많이 헤맸는데요.. 이게 압축파일 형태로 되어있어서 반디집 등으로 zip파일을 풀어줘야해요! 대부분의 경우 이상하게 열리거나 다운이 제대로 안되는 파일을 zip파일일 확률이 높으니! 반디집으로 풀어봅시다!! training_demo/├─ ...├─ pre-trained-models/│ └─ ssd_resnet50_v1_fpn_640x640_coco17_tpu-8/│ ├─ checkpoint/│ ├─ saved_model/│ └─ pipeline.config└─ ... zip파일 해제 후, 위와 같이 폴더를 구성하시면 됩니다! 2) Configure the Training Pipeline: 이제 우리의 training 작업에 필요한 파일을 만들어봅시다!./training demo/models 에 위에서 다운받은 pipeline.config파일을 복붙해줍니다. training_demo/├─ ...├─ models/│ └─ my_ssd_resnet50_v1_fpn/│ └─ pipeline.config└─ ... 그 후 우리의 입맛에 맞게 pipeline.config 파일을 수정해야합니다!! 수정해야하는 코드는 본문 해당 목차에서 확인해주세요.  저는 notepad++로 해당 파일을 열어 코드를 수정했어요!! (한번 배워뒀더니 여러모로 많이 쓰이는 notepad++!! https://blog.naver.com/alsdhr0155/222482104142 에서 사용법 확인 가능합니다!)  7. Training the ModelTensorFlow/models/research/object_detection/model_main_tf2.py 파일을 training_demo로 복사 붙여넣게 해주세요!그 후 cd ./training_demo 파일로 연결해 아래 코드를 실행시켜주세요.  (tf_3.9) C:\tf_1\workspace\training_demo>python model_main_tf2.py --model_dir=models/my_ssd_resnet50_v1_fpn --pipeline_config_path=models/my_ssd_resnet50_v1_fpn/pipeline.config 이를 시행했을 때 설치 되지 않고 frozen 된 것처럼 보이지만, 원래 설치에 오랜 시간이 걸리는 것이므로 인내심을 갖고 기다리시면 됩니다. 이렇게 기다리다 성공적으로 된 분도 있으시겠지만, 저는 계속 오류가 떴습니다. [ UnicodeDecodeError: 'utf-8' codec can't decode byte 0xc1 in position 91: invalid start byte ]​notepad++로 들어가 한글을 다 수정하고(상대경로로 수정해서), 코덱도 ANCI로 바꾸고, 뭐 딱히 한것도 없는데 현구님이랑 같이 처음부터 설치하고 돌려보니 됐다.. 억울해~!!! 환경변수때문인가..?? 더 추가하시긴했다...  인내와 고통과 배움의 시간이었습니다. 다들 성공하시길 바랍니다:)다음 포스팅에서는 실제로 인식률을 확인해보도록 할게요! "
페이스북 AI 논문 분석. 이미지에서 물체를 쉽게 인식하는 기술. DETR. end to end object detection with transformers [김용덕 변리사] ,https://blog.naver.com/rlaydrla/222465712869,20210821,"안녕하세요. 김용덕 변리사입니다.​오늘은 ECCV(European Conference on Computer Vision) 2020에서 출간되면서 많은 이목을 끈 논문(end to end object detection with transformers)에 대해서 분석한 내용을 설명드릴게요.  2005.12872.pdf2005.12872.pdf Sign Indrive.google.com ​이 논문은 Facebook AI 팀에서 출간한 논문으로, 기존의 객체인식(object detection) 알고리즘보다 simple 하면서 경쟁력 있는 architecture를 제안했다는 점에서 많은 사람들에게 이목을 받았습니다. ​본 논문이 제안한 것은 DETR(DEtection TRansformer)입니다. DETR의 특징은 1) 이분매칭손실함수를 이용했다는 것과 2) 자연어 처리에서 많이 사용되는 Transformer를 이용했다는 것입니다.   본 논문이 해결한 문제는?기존 객체 탐지 방법들은 너무 복잡하고 다양한 라이브러리를 사용하고 있습니다. 따라서, 기존 방법을 이용하는 경우 활용하기 상당히 까다로웠습니다. 예를 들어, 기존 객체 탐지 방법들을 활용해서 객체를 탐지하려면, bounding box의 형태나 bounding box가 겹칠 때의 처리 방법 등을 전체적으로 고려해야 했습니다. 이렇게 전체적으로 고려해서 setting을 해놔야 더 좋은 성능이 나오기 때문이었죠. 즉, 기존 객체 탐지 방법들을 활용하기 상당히 까다롭다는 문제점을 해결하기 위해 보다 간단한 architecture가 필요했었고, 이를 본 논문에서 해결하였습니다.   DETR의 전체 구조1) 이미지에 대해서 피처(feature)를 추출하기 위한 목적으로 backbone network(CNN)가 사용되고, 2) 이미지에서 상대적 위치 정보를 획득하기 위해 positional encoding이 사용됩니다.   백본 네트워크에서 추출된 feature와 positional encoding에서 획득된 상대적 위치 정보는 인코더에 입력됩니다. 여기서, 각각의 픽셀 정보들은 순차적 데이터(sequential data)에 해당합니다. 따라서, 순차적 데이터를 처리하기 적합한 트랜스포머(Transformer)가 인코더로 사용됩니다.   한편, 인코더의 출력과 오브젝트 쿼리(고정된 수의 학습된 위치 임베딩)가 디코더에 입력되고, 디코더의 출력은 최종 output을 생성하는 피드 포워드 네트워크(FFN)에 전달됩니다. 최종 결괏값은, 1) 특정 오브젝트가 존재하는 경우 해당 오브젝트에 대한 클래스와 해당 오브젝트가 어디 존재하는지와 관련된 바운딩 박스에 대한 정보를 포함하게 되고, 2) 특정 오브젝트가 존재하지 않는 경우 해당 오브젝트가 없다는 의미의 no object가 class로 표현됩니다.  그리고, 최종 결괏값과 실제 값 사이의 매칭이 성공적으로 이루어질 수 있도록 이분 매칭 손실 함수를 이용하게 되고, 이분 매칭 손실 함수를 통해서 중복되지 않은 instance를 각각 탐지할 수 있도록 만들어주게 됩니다. ​결과적으로, 이미지가 들어갔을 때 그 이미지에서 피쳐를 추출해서 트랜스포머를 거쳐서 실제 오브젝트 디텍션 결과가 나올 수 있도록 하는 게 전체 구조입니다.    이분 매칭(bipartite matching)기존에는 NMS(non-maximum suppression)를 이용하여 간접적으로 set prediction problem을 해결했었습니다. 구체적으로, 전체 이미지에서 몇 개의 오브젝트 instance가 존재하는지 미리 정해져있지 않고 무작위로 존재할 법한 위치를 찾은 다음에 그것들을 한 개로 압축하는 과정을 거쳐서 간접적으로 set prediction problem을 해결했었습니다. ​하지만, 본 논문에 따르면, set prediction problem이 이분 매칭을 통해 직접적으로 해결되었습니다. 구체적으로, ""N 개만큼만 이미지에 object instance가 존재할 수 있을 것이다""라고 고정시키고, 이분 매칭을 수행해서 set prediction problem을 해결하였습니다. 즉, N 개만큼 output dimension이 나올 수 있게 만들어 주고 실제 ground truth(실제 값)랑 비교할 때 이분 매칭을 수행해서 하나씩 1:1 매칭이 되도록 해주어 set prediction problem을 해결하였습니다. 예를 들어, 전체 이미지에 6개까지만 object instance가 존재할 수 있다고 N을 6으로 고정을 하면, 인스턴스가 중복되지 않도록 학습 과정에서 이분 매칭을 수행합니다. 이 경우, 인스턴스가 중복되지 않도록 유도됩니다.  학습 과정에서 2마리의 새가 촬영된 이미지를 이용하여 object detection을 수행한 예측 값이 다음과 같다고 가정해볼게요.  C = class b = bounding box 정보 (x 좌표, y 좌표, 너비, 높이)N=6으로 설정되어 있기 때문에 예측 값은 C0~C5 총 6개가 생성됩니다. 여기서, 예측 값 C2에 잘못된 결과가 포함되었는데요. 학습 과정에서 Class가 같고 bounding box 정보가 유사할 때 더 낮은 loss 값을 갖도록 매칭 loss 값을 구성합니다. 그리고, 전체 매칭이 끝났을 때 전체 loss 값이 가장 줄어든 방향으로 이분 매칭을 수행합니다. ​Class와 bounding box 정보에 기초하여 매칭을 수행합니다. (예측 값 C1이 실제 값 C0에 매칭이 되고, 예측 값 C3가 실제 값 C3에 매칭이 됨) 그리고, class가 없는 나머지 예측 값들은 나머지 실측 값들에 랜덤하게 매칭됩니다. ​학습 과정에서 class가 맞는 것 끼리는 bounding box 정보의 차이가 작아지도록 학습을 시키게 됩니다. 그리고, 예측 값에 잘못된 예측 값(예측 값 C2)이 존재하는 경우 이는 아무 데이터가 없는 실제 값 (실제 값 C2)과 매칭되기 때문에 no object로 구분될 수 있도록 학습이 진행됩니다. ​이런 식으로 학습이 잘 되면, 오브젝트가 존재하는 위치 및 개수만큼만 prediction이 수행될 수 있게 됩니다. 여기서, 핵심은 set prediction problem을 해결했다는 것인데, 즉 이분 매칭을 수행하게 되면 prediction 결과에서 각각의 output 위치가 바뀐다고 하더라도(예를 들어, C1과 C3의 위치가 바뀐 경우) 이분 매칭만 잘 되면 전혀 문제가 발생하지 않기 때문에 순서가 바뀜에 따라서 발생하는 문제가 없고, 2개의 바운딩 박스만 새를 detection 하도록 학습이 되기 때문에 자연스럽게 instance의 중복을 피할 수도 있게 됩니다.   Transformer을 왜 사용했는가? 트랜스포머는 순차적인 데이터가 나열되어 있을 때 그 데이터에 내재된 embedding을 잘 학습시키기 위해서 사용하는 architecture 중 하나임.Transforemer는 1) Attention을 통해 전체 이미지의 문맥 정보를 이해하고, 2) 이미지 내 각 인스턴스의 interaction을 용이하게 파악할 수 있도록 만들기 위해서 사용됩니다. 구체적으로, attention 메커니즘을 통해 각각의 픽셀들이 서로 attention score를 매겨서 attention을 수행하게 되는데 그런 과정에서 instance가 분리가 되고, 각각의 instance에 대해서 context(상호작용 값들)를 파악할 수 있게 됩니다. ​또한, Transformer는 3) 큰 바운딩 박스에서 멀리 떨어진 픽셀 간의 연관성을 용이하게 파악하기 위해 사용됩니다. 자연어 처리에서도, 문장의 길이가 멀어지게 되면 맨 앞에 나왔던 단어와 맨 뒤에 나온 단어 사이의 연관 관계를 파악하기 쉽지 않을 수 있습니다. 트랜스포머는 이런 경우에 연관 관계를 쉽게 파악해 줍니다. 이미지에서도 자연어 처리와 마찬가지로 큰 바운딩 박스에서 멀리 떨어진 픽셀 간의 연관성을 용이하게 파악하기 어려운 문제가 있는데 이를 트랜스포머로 해결하였습니다.   인코더의 기능은?인코더는 이미지의 특징 정보를 포함하고 있는 각 픽셀의 위치 데이터를 입력받아서 인코딩을 수행하는 작업을 합니다. 구체적으로, 인코더는 d*HW 크기의 연속성을 띠는 feature map을 입력으로 받습니다. (d=image feature, HW = 각각의 픽셀 위치 정보) 뿐만 아니라, positional encoding 정보를 입력받아 2D로 존재하는 실제 이미지 데이터에 대한 위치 정보들을 적절히 인코딩하여 처리할 수 있도록 해줍니다. ​한편, 인코더에 이미지의 feature 정보가 입력된 경우 self attention을 수행하면서 각각 픽셀에 대한 상호 작용, 연관성 정보를 학습하게 됩니다. 그리고, 인코딩된 정보가 디코딩 파트에서 사용이 됩니다. ​인코딩이 전부 수행된 후 self-attention map을 시각화해보면 개별 인스턴스가 다음과 같이 적절히 분리되어 있는 것을 확인할 수 있습니다. 즉, 특정 픽셀(빨간색 점)마다 연관성 있는 부분이 어느 부분이라고 적절하게 분리되는 것을 확인할 수 있습니다.   self-attention map을 시각화한 것. 4마리의 소가 적절하게 분리되어 있음.  디코더의 기능은?디코더는 N 개의 object query를 초기 입력으로 받고, 인코딩된 정보를 인코더로부터 받으면서 전체 이미지에 대한 context를 파악할 수 있게 됩니다. 여기서, 인코더로부터 인코딩된 정보를 전달받을 때 positional encoding 정보도 입력받아 2D로 존재하는 실제 이미지 데이터에 대한 위치 정보들을 적절히 인코딩하여 처리할 수 있도록 해줍니다. ​디코더는 각각의 object query가 하나의 instance에 대한 class와 bounding box 정보를 출력할 수 있도록 해줍니다. 즉, N 개의 서로 다른 고유한 instance를 구별할 수 있도록 architecture가 구성되어 있습니다. ​결과적으로, 인코더가 global attention을 통해 인스턴스를 분리한 경우, 디코더는 각 인스턴스의 클래스와 경계선을 추출해 주게 됩니다. ​아래 이미지는 디코더 파트에서 N 개 각각의 오브젝트에 대해서 attention 맵을 뽑아서 표현한 건데요. 각 인스턴스의 끝부분에서 attention socre 값이 높게 형성(하늘색과 주황색으로 표현됨) 된 것을 확인할 수 있습니다. 여기서, 끝부분은 bounding box가 잘 쳐질 수 있도록 만들어주는 요인이 됩니다.   DETR은 큰 오브젝트에 대해서는 좋은 퍼포먼스를 보이지만, 작은 오브젝트에 대해서는 낮은 퍼포먼스를 보입니다. 그리고, DETR의 경우 학습 시간이 상당히 오래 소요된다는 문제점이 존재합니다. 따라서, DETR의 학습 시간을 줄이면서 작은 오브젝트에 대해서도 잘 처리할 수 있는 그러한 기술이 개발될 필요가 있는데요. 이러한 기술이 개발된다면 아마 또다시 큰 이목을 끌게 되겠죠. ​오늘 살펴본 바와 같이 인공지능 기술은 상당히 난이도가 높은 기술입니다. 만약, 변리사를 통해 인공지능 관련 특허 출원을 진행하신다면 반드시 미팅을 진행하여 인공지능에 대한 이해도가 깊은지, 내가 개발 중인 인공지능 기술에 대해 잘 이해하고 있는지 확인하는 것을 추천 드립니다. ​​ "
Object Detection ,https://blog.naver.com/iadslba/222451823104,20210731,-  CNN은 EfficientNet으로 어느 정도 완성이 되어가는 형태고 RNN은  BERT가 나오고 BERT 파생 모델이 등장하면서 어느정도 목적한 바가 완성이 되어 가고 있다면 현재 속도와 정확도를 모두 잡아야 하는 Object detection으로 관심이 집중되고 있는 것같다. - 현재 3가지 방향으로 모델이 전개되고 있는데       - YOLO      - SSD      - EfficientDet 앞으로 이 3가지 모델을 구현하고 비교해서 학습에 도움이 되는 정보를 정리하고자 한다.  
"YOLO v1 ""You Only Look Once:Unified, Real-Time Object Detection"" ",https://blog.naver.com/sungshikbaik/223108733701,20230522,출처 : https://youtu.be/CXAhGoDphJc​ ​ 
[Object detection] 다트니오 선별기 ,https://blog.naver.com/ojkk371/222051690638,20200805,"필자의 취미 중 하나인 열대어 키우기와현재 공부하고 있는 딥러닝 [Object detection] 을 컬래버레이션 하여결과물을 내보았다.​http://datnio-detection-ojs.ngrok.io​위 링크로 접속하면아래의 화면이 출력되는데 처음 화면위 화면에서 파일 선택 버튼을 클릭하고다트니오 사진을 첨부한다음( 이 때, 사진의 파일명은 한글이면 안됩니다. 영문이나 숫자여야합니다. )Upload 버튼을 클릭하면​Object detection 을 수행하는인공지능 모델에다트니오 사진이 통과되어 박주현님의 다트니오 합사어항​이 다트니오가샴타이거로 불리는 'Pulcher'인지​플러스원으로 불리는'Microlepis'인지​ (좌) 거제아재님의 Microlepis / (우) 꽃순이아빠님의 Pulcher ​객체인식 되어 결과사진이 출력된다.​ 여기서 사용한 아키텍쳐는Faster R-CNN 이고모델은 구글의 Inception V2 이다. "
"[논문 리뷰] You Only Look Once (YOLO) : Unified, Real-Time Object Detection ",https://blog.naver.com/bosongmoon/222239958128,20210211,"논문 원본을 읽고 정리한 내용입니다. 스스로 공부를 위한 번역 위주임을 알려드립니다.​​​SummaryYOLO는 이미지를 한번만 보고, Object Detection과 Classification 통합(Unified)하여 진행하며, 속도가 빠르다(Real-Time)는 특성을 갖는다. 2-Stage Object Detector(기존 DPM, R-CNN)는 bounding box를 구한 후, class probability를 구하기 때문에, 그 속도가 느리다. 이와 달리, YOLO는 이 모든 과정을 A Single Neural Network로 만들어, 빠른 속도를 갖는 특징이 있다.​​​0. AbstractYOLO는 object detection의 새로운 접근 방법으로, Single neural network가 bounding box와 class probabilities를 예측한다. 또한, 1초당 45 frames 처리 가능하다(45fps). ​​​​1. Introduction기존 Object Detection에 활용되던 DPM(Deformable Parts Model)과 R-CNN은 window sliding을 이용하기 때문에, 느리고 최적화하는데 어려움이 있다. 즉, potential bounding box를 구한 후, post-processing을 통해 bounding box를 재조정하고, 중복 제거, 재측정하는 과정을 거친다.​*window sliding : 다양한 scale의 window를 이용해 이미지 내에서 score를 계산​ 이와 달리, YOLO는 A Single Regression Problem을 통해 이 모든 과정을 한번에 진행한다. 즉, Bounding box를 구하고(Where They Are), Class Probabilities(what objects are present)를 계산한다.​이러한 YOLO의 장점은 다음과 같다.1. Fast- 기존의 DPM, R-CNN은 복잡한 파이프라인을 갖지만, YOLO는 simple한 파이프라인을 갖는다.​2. Reasons Globally- globally 하게 판단한다. training and test time동안에 전체 이미지를 보기 때문에, Contextual Information을 놓치지 않게된다. cf) R-CNN은 larger context를 볼 수 없기 때문에, background patches에서 문제가 있다.​3. Learns Generalizable Representation- object의 일반화된 표현을 학습- 새로운 도메인이나 예상치 못한 input에 대해 세분화되지 않고 일반화 가능데모 : https://pjreddie.com/darknet/yolo/​​​2. Unified Detection여러 component들을 single neural network로 합침(unfiy)bounding box를 예측하기 위해 전체 이미지의 feature를 사용이미지에 대한 모든 클래스와 bounding box를 동시에 예측input image를 SxS grid로 나눔각각의 grid cell은 B bounding box와 각 bounding box에 대한 confidence score 예측confidence score는 bounding box가 object를 포험하는게 얼마나 믿을만한지와 예측한 bounding box가 얼마나 정확한지를 의미하며 식으로 나타내면 아래와 같다. IOU(Intersection over union)는 (ground truth와 예측 bounding box의 교집합)/ (ground truth와 예측 bounding box의 합집합) 을 의미한다. 셀에 어떤 객체도 없으면, confidence score는 0 이다(Pr(object)이 0 이므로). 그리드 셀에 어떤 객체가 확실히 있다고 예측했을 때, 즉 Pr(Object)=1일 때가 가장 이상적이다. 결과적으로, confidence score와 IOU가 같은게 가장 이상적인 score이다. ​각각의 bounding box는 x, y, w, h confindenct의 예측치를 갖는다. (x,y)는 그리드 셀의 상대적인 중심을 나타내며, width와 height는 전체 이미지에 대해 상대적으로 예측된다. 또한, 각각의 그리드 셀은 C conditional class probabilities를 예측하며, 이것은 그리드 셀 안에 object가 있다는 조건 하에 그 객체가 어떤 class인지에 대한 조건부 확률로, Pr(Classi |Object)와 같이 표현된다. ​​bounding box의 개수와 상관없이, 그리드 셀 당 오직 하나의 class에 대한 확률만 구한다. 테스트를 할 때는, C conditional class probabilites와 개별 bounding box의 confidence를 곱해준다. 이는 각 bounding box에 대한 class-specific confidence score라고 하며, 식은 아래와 같다.​ 이 score는 bounding box에 특정 클래스 객체가 나타날 확률(=Pr(Class_i))과 예측된 bounding box가 그 클래스객체에 얼마나 잘 들어맞는지(=IOU_pred^truth)를 나타낸다.​​ ​PASCAL VOC 데이터셋을 이용했고, S = 7로, B = 2로 설정했다. 클래스는 20개로 C =20이다. 다시말해 인풋 이미지는 7x7 그리드 셀로 나눠지며, 2개의 bounding box를 예측할 것이다. 이러면 최종 예측 텐서의 차원은 (7 x 7 x 30)이 된다.​2.1 Network Design YOLO Model 구조# GoogleNet 모델 기반24 Convolution layer, 2 Fully Connected layer초기 convolution layers에서 이미지의 feature 추출하고, fully connected layer에서는 class probablities 와 coordiantes 예측1 x 1 reduction layers , 3x3 convolutional layers​​​​2.2 Training1000개의 클래스를 갖는 ImageNet 데이터셋으로 YOLO의 convolutional layer을 pretrain 했다. 20개의 컨볼루션 layer만 사용했고, 이어서 fully connected layer 사용했다. 훈련과 추론을 위해서는 Darknet 프레임워크를 사용​object detection 모델로 전환하기 위해서, 4개의 convolutional layers와 2개의 fully connected layers를 추가했으며, 가중치는 무작위로 초기화했다. 객체 검출을 위해서는 이미지 해상도가 높을 필요가 있기 대문에, 224 x 224 에서 448 x 448로 이미지 해상도를 높였다. final layer는 클래스 확률과 bounding box의 좌표를 예측한다. bounding box의 width와 height를 0과 1사이의 값으로 정규화했다. 도한, x,y 좌표 역시 0과 1사이로 조정했다. 마지막 layer에서는 linear activation function을 적용하고, 나머지 모든 layer에서는 leaky ReLU를 적용했다. ReLU는 0 이하의 값이 모두 0인데 비해, leaky ReLU는 0 이하의 값도 작은 음수 값을 갖는다. leaky ReLU의 식은 다음과 같다.​​ ​YOLO의 loss는 최적화하기에 적합한 SSE(sum-squared error)를 기반으로 한다. 하지만 SSE를 최적화하는 것과 YOLO의 최종 목적인 mAP(평균 정확도)를 높이는 것이 일치하지는 않는다. YOLO의 loss에는 bounding box의 위치를 얼마나 잘 예측했는지에 대한 loss인 localization loss와 클래스를 얼마나 잘 예측했는지에 대한 loss인 classification loss가 있다. localization loss와 classification loss의 가중치를 동일하게 주는 것은 좋은 방법이 아니다. 하지만 SSE를 최적화하는 방식은 이 두 loss의 가중치를 동일하게 취급한다.​또, 각 그리드 셀이 모두 객체를 포함하고 있는 것은 아니다. 그리드 셀에 객체가 없을 때는, confidence score 값은 0이다. 이때는 모델의 불균형이 발생한다. 이를 위해, 객체가 존재하는 bounding box coordinate에 대한 loss의 가중치를 증가시키고, 객체가 존재하지 않는 bounding box의 confidence loss에 대한 가중치는 감소시킨다. 즉, 두 개의 파라미터를 사용하여, λ_coord=5, λ_noobj=.5로 가중치를 준다.​​또 다른 문제는 큰 bounding box와 작은 bounding box에 동일한 가중치를 주는 것이다. 작은 bounding box가 큰 bounding box보다 small deviations에 더 민감하게 반응한다. 이를 해결하기 위해, bounding box의 너비(widht)와 높이(hegith)에 square root를 취해준다.​YOLO는 하나의 그리드 셀당 여러개의 bounding box를 예측한다. 훈련을 할 때는, 한개의 bounding box predictor는 한개의 object만 나타내야 한다. 이를 위해서는 여러개의 bounding box 중에서 ground truth와 가장 높은 IOU를 가진 객체를 선택한다. 각각의 bounding box predictor는 특정 크기(size), 종횡비(aspect ratios), 객체의 클래스를 잘 예측하게 된다.​​훈련 단계에서는 다음과 같은 loss function을 사용한다.​ ​여기서 1_ij^obj는 i번째 그리드 셀에 j번째 bounding box에 객체가 존재하는지 여부를 의미한다. 위 loss function의 5개 식은 차례대로 아래와 같은 의미를 갖습니다.​(1) Object가 존재하는 그리드 셀 i의 bounding box predictor j에 대해, x와 y의 loss를 계산.(2) Object가 존재하는 그리드 셀 i의 bounding box predictor j에 대해, w와 h의 loss를 계산. 큰 box에 대해서는 작은 분산(small deviation)을 반영하기 위해 제곱근을 취한 후, sum-squared error를 구합니다. (같은 error라도 큰 box의 경우 상대적으로 IOU에 영향을 적게 준다.)(3) Object가 존재하는 그리드 셀 i의 bounding box predictor j에 대해, confidence score의 loss를 계산. (Ci = 1)(4) Object가 존재하지 않는 그리드 셀 i의 bounding box predictor j에 대해, confidence score의 loss를 계산. (Ci = 0)(5) Object가 존재하는 그리드 셀 i에 대해, conditional class probability의 loss를 계산. (p_i(c)=1 if class c is correct, otherwise: p_i(c)=0)​λ_coord: coordinates(x, y, w, h)에 대한 loss와 다른 loss들과의 균형을 위한 balancing parameter.λ_noobj: 객체가 있는 box와 없는 box 간에 균형을 위한 balancing parameter. (일반적으로 image내에는 객체가 있는 그리드 셀보다는 없는 셀이 훨씬 많으므로)​출처: curt-park.github.io/2017-03-26/yolo/​Batch size는 64로, momentum은 0.9로, decay는 0.0005로 설정했다. 초반 learning rate를 0.001에서 0.01로 상승시켰다. learning rate를 낮은 값 부터 시작한 이유는, 높은 learning rate에서 기울기 explosion이 발생할 수 있기 때문이다. 또한, 과적합을 막기 위해 dropout은 0.5로 설정하고 data augmentation을 적용했다.​​​2.3 Inference ​YOLO는 single network evaluation을 사용하기 때문에, 빠르다는 장점이 있다. 하지만 큰 객체가 여러 셀의 경계에 있을 경우, 동시에 검출될 수 있다. 이러한 문제를 해결하기 위해서는 Non-maximal suprression(비 최대 억제)이 사용된다. YOLO는 비 최대 억제를 통해 mAP를 약 2~3%로 향상시켰다.​​​2.4 LimitationsYOLO는 각 셀이 2개의 bounding box와 1개의 class만 예측하기 때문에, 공간적 제약을 가지고 있다. 이것은 세대와 같이 작은 물체가 몰려 있는 경우, 검출을 잘 하지 못하는 것을 의미한다. (하나의 그리드 셀에 두 개 이상의 객체가 붙어있을 때, 잘 검출하지 못하는 특성) ​또한, 새로운 종횡비를 가진 객체에 일반화하는 데 어려움이 있다. 즉, 학습시 사용했던 bounding box의 형태가 아닐 경우 검출이 잘 안될 수 있다. 마지막으로, 큰 bounding box와 작은 bounding box에 동일한 가중치를 제공한다. 이는 작은 bounding box와 큰 bounding box의 error를 동일하게 처리하므로, 정확하지 않은 localization이 된다. 작은 bounding box와 큰 bounding box의 가중치를 다르게 주어야 하는 이유는 작은 bounding box의 위치가 조금만 달라져도 성능에 큰 영향을 미치기 때문이다.​​​​참고자료zzsza.github.io/data/2018/05/02/YOLO-You-only-look-once-review/bkshin.tistory.com/entry/%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0-YOLOYou-Only-Look-Once "
파이썬 ImportError: cannot import name 'graph_rewriter_pb2' from 'object_detection.protos' 에러 ,https://blog.naver.com/alaldi2006/222149564899,20201120,ImportError: cannot import name 'graph_rewriter_pb2' from 'object_detection.protos'​휴; 또 proto 에러나서 아나콘다 site-packages 에 있는 object_detection protos 안에 graph_rewriter_pb2 라는 파일이 없었음.그래서 models/research/object_detection/protos 에 있는 protos 파일을 protoc 해주고​protoc object_detection/protos/*.proto --python_out=.​파일 다시 재복사해서 넣어줌. ​ 
[yolov5] yolov5 모델의 Object Detection Custom Dataset 만들기! ,https://blog.naver.com/bsh1004664/222422137547,20210706,"미리 주어진 Dataset이 없다면?YOLOv5 모델의Object Detection Custom Dataset 만들기! 이전  포스팅에서는 'YOLOv5 모델'의 기존의 Dataset을 이용하여 Object Detection을 해보았습니다.하지만 미리 주어진 Dataset이 없다면?이번 포스팅에서는 'Object Detection Custom Dataset'을 직접 만들어 보겠습니다. ˙ᵕ˙​ 'Object Detection Custom Dataset'을 만드는 과정'Object Detection Custom Dataset'을 직접 만들고 'Object Detection'를 진행하는 과정은 크게 네 가지로 나눌 수 있습니다.1. Image Gathering웹 크롤링을 통한 학습시킬 데이터 수집 + 불필요한 이미지 제거(웹 크롤링은 다른 포스팅에서 조금 더 구체적으로 다뤄보도록 하겠습니다.)2. Image Resizing수집한 이미지의 사이즈를 동일한 크기로 맞추는 과정3. Image Labelingresize 한 이미지 속에서 Object의 위치를 정확히 표기하는 과정4. Object Detection 진행Object의 이미지와 해당 이미지의 라벨링 과정을 통해 구축한 Dataset을 활용하여 Object Detection 진행 1. Image Gathering이번 포스팅에서는 코카콜라 Dataset을 만들어보겠습니다.먼저 웹 크롤링을 통해 코카콜라의 다양한 사진을 수집해야 합니다.(chromedriver를 활용한 웹 크롤링은 다른 포스팅에서 조금 더 자세하게 다뤄보도록 하겠습니다.)조금 더 쉽게 이미지 수집을 할 수 있는 방법으로 크롬 확장 프로그램 중 Download All Images을 이용하는 방법이 있습니다.​https://chrome.google.com/webstore/detail/download-all-images/nnffbdeachhbpfapjklmpnmjcgamcdmm Download All ImagesEasily save images with a wide range of customization features, such as file size, dimensions, and image type.chrome.google.com 위의 링크를 타고 들어가면,Download All Images 크롬 확장 프로그램을 다운로드하실 수 있습니다.  해당 프로그램을 사용하여 위의 영상과 같이 학습시키고자 하는 이미지를 JP(E)G, PNG 형태로 다운로드해 줍니다.압축 폴더에 다양한 코카콜라 이미지 사진이 다운로드해진 것을 확인하실 수 있습니다.​다운로드한 코카콜라 이미지 중, 학습시킬 이미지학습시킬 이미지는 남겨놓고, 불필요한 이미지이렇게 불필요한 이미지는 삭제한 후, 학습시킬 이미지만 남겨놓은 폴더로컬 디스크(C:) 'guide' 폴더에 넣어줍니다. 2. Image Resizing다음으로 수집한 이미지의 사이즈를 동일한 크기로 맞추는 과정을 진행해보겠습니다.먼저, 로컬 디스크(C:)에 resize된 이미지를 저장할 'guide_resize' 폴더를 만들어줍니다.​이후, Anaconda Prompt 창에서 # ﻿selenium, opencv를 설치pip install seleniumpython -m pip install opencv-python selenium과 opencv 설치 과정selenium과 opencv를 설치해 줍니다.​다음으로 Anaconda Prompt 창에서 python을 실행한 후, import  cv2import  os import  numpy  as nppath = ""C:\guide"" # 원본 사진이 있는 폴더 경로 대입file_list = os.listdir(path)    for k in file_list:    img = cv2.imread(path + '\\' + k)    width, height = img.shape[:2]    resize_img = cv2.resize(img, (416 , 416), interpolation=cv2.INTER_CUBIC) # resize 사진 크기    cv2.imwrite('C:\guide_resize\\' + k, resize_img) # resize된 사진이 저장될 폴더 경로 대입 이미지를 일정한 크기로 resize 하는 과정Image Resize 코드를 입력합니다. 모든 이미지가 416 X 416 크기로 resize된 결과가 담긴 폴더다음과 같이 'guide_resize' 폴더에서 모든 이미지가 416 X 416 크기로 resize된 결과를 확인하실 수 있습니다. 3. Image Labeling라벨링을 진행하기 전!Anaconda에서 YOLOv5 모델의 환경설정을 먼저 해주도록 하겠습니다. conda create -n yolov5 python=3.8conda activate yolov5 Anaconda Prompt 창에서 위 코드를 실행한 후,​ ultralytics/yolov5YOLOv5 in PyTorch > ONNX > CoreML > TFLite. Contribute to ultralytics/yolov5 development by creating an account on GitHub.github.com GitHub 링크에 들어가 YOLOv5 모델을 다운받아줍니다. pip install matplotlib tqdm opencv-python pillow PyYAML scipy tensorboard# cuda 설치conda install pytorch==1.6.0 torchvision==0.7.0 cudatoolkit=10.1 -c pytorch# kernel 생성conda install ipykernelpython -m ipykernel install --user --name yolov5 --display-name yolov5Kenrel 이후, 다시 Anaconda Prompt 창에서 위 코드를 실행시켜주면 YOLOv5 모델 환경설정 완료!​다음으로는 'labelImg'라는 프로그램을 이용하여   resize 한 이미지 속에서 Object의 위치를 정확히 표기하는 과정을 진행해보겠습니다. tzutalin/labelImg🖍️ LabelImg is a graphical image annotation tool and label object bounding boxes in images - tzutalin/labelImggithub.com 위의 링크를 타고 가서, labelImg ZIP 파일 다운로드labelImg ZIP 파일을 다운받아 압축을 풀고 저장해 줍니다. cd C:\Users\반소희\OneDrive\바탕 화면\소히공부\labelImg-master\labelImg-master # cd 각자의 labelimg폴더 저장위치 대입conda install pyqt=5conda install -c anaconda lxmlpyrcc5 -o libs/resources.py resources.qrcpython labelImg.py 이후, Anaconda Prompt 창에 위의 코드를 입력해 주면,(저는 conda가 이미 설치되어 있어 conda install pyqt=5와 conda install -c anaconda lxml 과정을 생략하였습니다.) labelimg 프로그램 위와 같은 labelimg 창이 뜹니다.​로컬 디스크(C:)에 labeling 결과를 넣어줄 'guide_labels' 폴더를 만들어준 후, Open Dir을 클릭하여 resize 된 이미지가 저장되어있는 폴더를 선택해주고, Change Save Dir을 클릭하여 labeling 결과를 저장할 폴더를 선택해줍니다. Image Labeling 과정이후, 위의 그림에 나와있는 순서대로1. YOLO로 맞추기2. Create RectBox 선택 후 이미지 라벨링 진행3. Save 클릭4. 'cocacola' class 이름 작성 후 OK 클릭을 진행하면, 'guide_labels' 폴더에 아래와 같이 라벨링 결과들이 저장된 것을 확인할 수 있습니다. 여기서 classes.txt 파일은 지워줍니다. 라벨링 결과 파일을 하나 열어보면, 0 0.49087 0.526442 0.204327 0.278846 숫자가 나오는 것을 확인할 수 있는데,이 중 맨 앞의 숫자 0은 class 번호를 의미합니다.즉, 제가 라벨링한 cocacola의 class 번호는 0임을 알 수 있습니다. dir -Path ""C:\guide_labels""-Include *.txt -Recurse | %{$tmp = Get-Content $_; $tmp=$tmp -Replace (""0"",""1""); Set-Content $_ $tmp} 혹시 class 번호를 일괄적으로 0에서 1로 수정하고싶다면, Anaconda Powershell Prompt 창에서 위의 코드를 실행시키면 됩니다.(위 코드는 '숫자 0을 모두 1로 바꿔줌'을 의미함, 다른 정확한 코드가 있을 것 같음...............더 알아봐야겠습니다ㅜㅅㅜ) 4. Object Detection 진행드디어 Object Detection 진행을 위한 Dataset 준비가 끝이 났네요!본격적으로 Object Detection Inference 과정을 진행해보겠습니다.​로컬 디스크(C:)에 'guide_cocacola' 폴더를 만들어준 후, 그 안에 'export' 폴더를 만들고아래 코드가 입력된 메모장(파일 형식은 모든 파일로, 파일명은 'guide_cocacola.yaml'로 설정)도 만들어줍니다. train: ../train/imagesval: ../valid/imagesnc: 1names: ['cocacola'] 그럼 이렇게 총 두 개의 파일이 생성되겠죠! 그런 다음, export 폴더 안에 images 폴더와 labels 폴더를 만들어준 후, images 폴더 안에는 resize 한 이미지들이 들어있던 'guide_resize' 폴더(cocacola로 파일명 변경함)를 넣어주고,labels 폴더 안에는 labeling 한 이미지들이 들어있던 'guide_labels' 폴더(cocacola로 파일명 변경함)를 넣어줍니다.​Object Detection 모델을 만들기 위해서는 '학습시키고자 하는 물체가 담긴 이미지'와 '해당 물체의 종류 그리고 위치 정보가 기록된 label'이 필요한데, 이들 모두 위 과정을 통해 'guide_cocacola' 폴더 내에 넣어놨습니다.​자, 그럼 본격적으로 google Colab에서 YOLOv5 모델을 활용한 Object Detection 과정을 진행해보겠습니다.​먼저, 구글 드라이브에 앞서 만든 'guide_cocacola' 폴더를 업로드한 후, Colab 환경을 준비하고, # Mount Google Drivefrom google.colab import drivedrive.mount('/content/drive') Colab과 구글 드라이브를 연동해줍니다. # Clone YOLOv5 GitHub repository%cd /content!git clone https://github.com/ultralytics/yolov5.git 그다음 YOLOv5 모델을 GitHub에서 Clone 해서 받아옵니다.(참고: GitHub - ultralytics/yolov5: YOLOv5 in PyTorch > ONNX > CoreML > TFLite) # YOLOv5를 위한 패키지 설치%cd /content/yolov5/! pip install -r requirements.txt YOLOv5 모델을 사용하기 위한 패키지를 설치하고, %cd /from glob import glob# resize 한 이미지를 'img_list_co'에 넣어주기img_list_co = glob('/content/drive/MyDrive/guide_cocacola/export/images/cocacola/*')# resize 한 이미지 수 출력print(len(img_list_co)) resize 한 cocacola 이미지를 'img_list_co'에 넣어줍니다. from sklearn.model_selection import train_test_split# resize 이미지를 test(20%)/train(80%) dataset으로 나누기train_img_list_co, val_img_list_co = train_test_split(img_list_co, test_size=0.2, random_state=2000)# 각각의 test(20%)/train(80%) dataset 이미지 수 출력print(len(train_img_list_co), len(val_img_list_co)) with open('/content/drive/MyDrive/guide_cocacola/train.txt', 'w') as f:  f.write('\n'.join(train_img_list_co)+'\n')with open('/content/drive/MyDrive/guide_cocacola/val.txt', 'w') as f:  f.write('\n'.join(val_img_list_co)+'\n') 그다음, 'img_list_co'에 들어있는 resize 된 이미지를 test(20%)/train(80%) dataset으로 나눠주고, import yamlwith open('/content/drive/MyDrive/guide_cocacola/guide_cocacola.yaml', 'r') as f:  data = yaml.load(f)print(data)data['train'] = '/content/drive/MyDrive/guide_cocacola/train.txt'data['val'] = '/content/drive/MyDrive/guide_cocacola/val.txt'with open('/content/drive/MyDrive/guide_cocacola/guide_cocacola.yaml', 'w') as f:  yaml.dump(data, f)print(data) 'guide_cocacola' 폴더에 들어있는 'guide_cocacola.yaml' 파일을 이용하여,train에는 train.txt를, test에는 test.txt를 넣어줍니다.​위 코드를 실행시키면, /usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.  This is separate from the ipykernel package so we can avoid doing imports until{'train': '../train/images', 'val': '../valid/images', 'nc': 1, 'names': ['cocacola']}{'train': '/content/drive/MyDrive/guide_cocacola/train.txt', 'val': '/content/drive/MyDrive/guide_cocacola/val.txt', 'nc': 1, 'names': ['cocacola']} 이러한 결과를 얻을 수 있는데, 여기서 'nc': 1는 class의 개수가 한 개임을 뜻하고, 그 클래스의 이름은 'names': ['cocacola']를 통해 cocacola임을 알 수 있습니다.​드디어 이제 학습시킬 준비가 다 된 것 같습니다. %cd /content/yolov5/!python train.py --img 416 --batch 16 --epochs 50 --data /content/drive/MyDrive/guide_cocacola/guide_cocacola.yaml --cfg ./models/yolov5s.yaml --weights yolov5s.pt --name guide_yolov5s_results 416 X 416 크기로 resize 된 이미지들을 batch size 16, epochs 50으로 학습시켜보겠습니다.--img: 이미지 크기--batch: 전체 데이터셋 중 한 번에 몇 개까지 학습시킬 것인지--epochs: 학습시키는 데이터셋을 몇 번 반복시켜 학습시킬 것인지--data: yaml 파일--cfg: 모델 선택(yolov5l.yaml, yolov5m.yaml, yolov5s.yaml, yolov5x.yaml 등이 있음)--weights: 전이학습의 가중치 선택--name: 결과 저장 폴더 YOLOv5 모델의 Object Detection 결과 경로결과는 위 사진의 경로에서 확인할 수 있습니다. YOLOv5 모델의 Object Detection 결과YOLOv5 모델의 Object Detection 결과 데이터와 epochs 수가 부족해서 정확도가 많이 높지는 않은 것 같습니다.​다음 포스팅에서는 data의 배경과 물체의 촬영 각도를 더욱 다양화하고  데이터와 epochs 수를 늘린 후, 여러 object에 대한 train을 시켜보도록 하겠습니다! ˙ᵕ˙ "
이미지 프로세싱 & 컴퓨터 시각화 18부 - Object Detection (Template Matching) ,https://blog.naver.com/zeus05100/221655525721,20190922," 첨부파일love_sign_ny.png파일 다운로드 첨부파일love_sign_ny_partial.png파일 다운로드 여러분 안녕하세요. 오랜만에 이렇게 블로그로 인사를 드리게 됩니다. 이제 다시 활발한 활동을 재개할 예정이니 많은 기대 부탁할께요. 이번시간부터는 조금 흥미있는 주제를 다뤄볼까 합니다. 요즘 많이 떠돌아 다니고 있는 물체 감지, 즉 Object Detection이라는 주제입니다. 기계가 어떻게 사물을 인식하는지, 또 그 후에 우리가 어떤 Application들을 구현시킬 수 있는지에 대해 고민해보는 것도 좋겠군요. Object Detection에는 매우 다양한 기법들이 존재하기 때문에 우리는 가장 간단한 방법부터 살펴본 후 차차 난이도 있는 기법을 다뤄볼 예정입니다. 처음부터 음식을 급하게 먹으면 체하는법! 그러나 간단한 방법이라고 허접한 그런건 절대 아니니 무시 안해도 됩니다. :)  그럼 시작해볼까요?​Template Matching이 바로 그 주인공입니다. 단어 그대로 템플릿을 매칭시켜 가장 유사한 부분들을 색출해내는 방법입니다. 예를 한번 들어봅시다. 원본이미지를 A라고 하고 B라는 이미지가 있다고 합시다 (B는 A이미지의 일부분입니다). 그럼  B라는 녀석을 A에다 갖다대어 어느 부분이 가장 유사한지 찾아내는 방법입니다. 만약 똑같은 부분이 있다면 우리는 B가 A에서 감지되었다, 디택팅 되었다, B는 A에 존재한다 뭐 이렇게 설명할 수 있겠군요. 그러나 중요한 사항이 있습니다. B라는 이미지는 반드시 A이미지에 존재해야 한다는 것입니다. 정확한 matching을 요구하는 것이기 때문이죠. 따라서 B의 크기는 A보다 무조건 작거나 같아야 합니다. ​이렇게 유사한 부분을 찾아낼때 우리는 상관관계(Correlation)를 따져봅니다. 통계학을 공부해보신 분들이라면 이미 잘 아는 내용이겠군요. 그러나 상관관계가 무엇인지 모른다고 기죽지 않아도 됩니다. 1에 가까울수록 둘의 관계가 상관이 있다 이렇게 이해하면 됩니다. 즉 상관관계가 1이면 완벽한 매칭이라는 것입니다. 그러나 우리는 코딩을 집중 다룰예정이기에 수학적인 공식은 아예 패스할 껍니다.​그럼 바로 코딩을 통하여 Template Matching이 어떻게 작동되는지 살펴봅시다. ​우선 원본 이미지와 부분 이미지를 불러옵니다.  import cv2import pandas as pdimport numpy as npimport matplotlib.pyplot as plt%matplotlib inlinefull = cv2.imread(""love_sign_ny.png"")full = cv2.cvtColor(full, cv2.COLOR_BGR2RGB)plt.imshow(full)   ​그다음 부분 이미지를 불러옵니다. part = cv2.imread(""love_sign_ny_partial.png"")part = cv2.cvtColor(part, cv2.COLOR_BGR2RGB)plt.imshow(part)   ​뉴욕 시내 한복판에 있는 유명한 LOVE사인입니다. 위에서도 보이듯 부분이미지(B)는 원본이미지(A)에 포함되어 있는 사실은 우리가 눈으로 흘겨봐도 알 수 있는 내용입니다. 그럼 우리의 컴퓨터는 이를 어떻게 인식하는지 알아봅시다.​Template Matchin에는 다양한 방법이 있으며 그중 대표적으로 6가지가 존재합니다. 그리고 우리는 각각의 방법을 사용하여 어떤 결과를 보여주는지 비교를 해볼껍니다. 아래 그 방법들을 나열해봅니다. methods = ['cv2.TM_CCOEFF', 'cv2.TM_CCOEFF_NORMED', 'cv2.TM_CCORR', 'cv2.TM_CCORR_NORMED', 'cv2.TM_SQDIFF', 'cv2.TM_SQDIFF_NORMED'] 더 많은 정보를 알고 싶으시면 다음 홈페이지를 방문하시면 됩니다. -> https://docs.opencv.org/master/df/dfb/group__imgproc__object.html OpenCV: Object DetectionFile failed to load: https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/extensions/MathEvents.js Enumerations enum   cv::TemplateMatchModes { cv::TM_SQDIFF = 0, cv::TM_SQDIFF_NORMED = 1, cv::TM_CCORR = 2, cv::TM_CCORR_NORMED = 3, cv::TM_CCOEFF = 4, cv::TM_CCOEFF_NORMED = 5 } type of the template m...docs.opencv.org   다시 언급하지만 우리는 위 공식을 이해하지 못해도 됩니다. 그냥 아하! 저런것들이 있구나. 저것들이 Template Matching에서 사용되는구나 하고 쿨하게 넘겨버리세요. 지금 우리는 수학공부를 하러 온게 아니잖아요???​그럼 위에서 정의했던 methods를 이용하여 하나하나 돌려봅시다. 아니 그보다 먼저 하나만 돌려보고 결과가 어떻게 나타나는지 관찰해봅시다. full_copy = full.copy()method = eval('cv2.TM_CCOEFF')res = cv2.matchTemplate(full_copy, part, method)plt.imshow(res) 한줄한줄 살펴봅니다.  원본 이미지의 복사본을 우선 만듭니다. 그 이유는 우리가 Template Matching을 하면서 매칭되는 부분에 사각형을 그릴건데 원본 이미지에 피해가 가지 않기 위함입니다. 그다음 eval이라는 함수를 사용하여 문자열을 함수로 인식하게끔 해줍니다. 따라서 method는 실제 함수가 되는 것이죠 (cv2.TM_CCOEFF). 그다음 중요한 함수가 등장합니다. matchTemplate이라는 함수를 사용합니다. 파라미터는 3개가 존재하며 각각의 의미는 너무나 명백합니다. 우선 원본 이미지를 넣어주고 (복사본을 넣어줍니다) 부분 이미지를 넣어주고 마지막으로 method를 넣어줍니다. ​그럼 res에는 무엇이 들어가있을까요? 그래프로 출력해봅시다.​   오잉? 이게 무엇인가요? 이건 Heat Map이라고 합니다. 데이터 시각화부분에서 단골손님으로 등장하는 녀석이기 때문에 심오한 데이터분석 및 visualization을 해오셨던 분들은 Heat Map이 무엇인지 아실꺼에요. 하지만 이 개념이 생소한 분들을 위하여 아주 간단히 설명하고 넘길께요. 그래프를 보시면 우측 상단에 밝게 점 하나가 찍혀있는게 보이나요? (이 점을 우리는 X라고 정의합니다) 그 점의 위치를 기억해 둡시다. 그리고 우리의 원본이미지와 부분이미지를 대조한 후 부분이미지가 원본이미지의 어디쯤 위치해 있는지 눈여겨봅시다 (여기서 위치를 Y라고 정의합니다).  그럼 X와 Y의 위치는 거의 95%이상 비슷할 껍니다. 그렇습니다. Heat Map은 이처럼 correlation(상관관계)가 높을수록 그 부분은 HeatMap에 아주 밝게 표시됩니다. 아! 위의 Heat Map은 cv2.TM_CCOEFF일때만입니다. 다른 method를 넣으면 다른 결과가 나오겠죠?​res에는 매우 중요한 정보가 담겨있습니다. 상관관계와 관련된 정보뿐만 아니라 그 점의 위치정보까지 저장하고 있습니다. 따라서 우리는 이걸 이용하여 원본 이미지에 사각형을 그림으로 우리의 부분이미지가 원본이미지의 어디에 매칭되는지 보여줄 수 있는 것이죠.​그럼 계속 코딩을 해봅시다. 이제는 For loop을 돌려서 모든 method들을 돌려봅니다. for m in methods:    full_copy = full.copy()        method = eval(m)    res = cv2.matchTemplate(full_copy, part, method)        min_val, max_val, min_loc, max_loc = cv2.minMaxLoc(res)        if method in [cv2.TM_SQDIFF, cv2.TM_SQDIFF_NORMED]:        top_left = min_loc    else:        top_left = max_loc            height, width, channel = part.shape    bottom_right = (top_left[0] + width, top_left[1] + height)        cv2.rectangle(full_copy, top_left, bottom_right, color=(255,0,0),thickness=10)        # plot and show the image    plt.subplot(121)    plt.imshow(res)    plt.title('HEATMAP OF TEMPLATE MATCHING')        plt.subplot(122)    plt.imshow(full_copy)    plt.title('DETECTION OF TEMPLATE')        plt.suptitle(m)        plt.show()        print('\n') Tuple unpacking을 하여 4개의 variable을 정의합니다. 최소 최대 위치정보와 상관관계의 정보를 가져옵니다. 그다음 간단한 체크를 하나 합니다. method가  cv2.TM_SQDIFF 혹은 cv2.TM_SQDIFF_NORMED 일때만 top_left 는 min_loc가 됩니다. 그 이유는 우리가 수학에서 제곱을 할 때 음수는 모두 양수로 변하기 때문에 그 성질을 고려해야 합니다. ​그다음 부분이미지의 너비와 높이를 구합니다. 이제 사각형을 그리는 일만 남았군요. 잠깐! 우리는 위에서 top_left정보가 있지만 bottom_right에 대한 정보가 없군요. 따라서 부분이미지에서 얻은 정보를 이용하여 bottom_right의 위치를 찾아줍니다. ​마지막으로 matplotlib에 딸려나오는 subplot기법을 사용하여 여러개의 그래프를 한번에 보여줄 수 있게 해줍니다. 이부분은 설명을 생략할께요. ​그럼 이제 결과를 봅시다!!!! ​   6가지의 다른 방법을 사용하여 대동소이한 결과를 보여줍니다. 우리의 부분이미지가 전체이미지에 저렇게 매치가 된다는 것을 빨간 사각형으로 나타내줬습니다. 특이한 점은 중간에서 위에 있는 녀석은(cv2.TM_CCORR) 완전 엉뚱한 결과를 보여줍니다. 또한 오른쪽 상/하단에 있는 녀석들은(cv2.SQDIFF, cv2.SQDIFF_NORMED) 제곱이 됨으로서 최소값을 가지고 Template Matching을 하기 때문에 밝게 빛나는 부분이 아닌 검은 부분에서 매칭을 찾아낸다는 사실 또한 짚고 넘어가야 합니다. ​유의할 사항은 모든 방법이 다 올바른 결과를 나타내지 못한다는 것입니다. 따라서 우리가 우리에게 맞는 방법을 찾아내야 합니다. ​지금까지 우리는 Template Matching에 대해서 살펴보았습니다. 매우 간단한 방법이며 우리는 Exact matching을 찾아낼때 이 방법을 사용합니다. 다음시간에는 Corner Detection에 대해 살펴봅니다. Object Detection을 다루기 위한 또다른 중요한 내용이니 많은 기대 하셔도 괜찮습니다^^ 그럼 다같이 달려봅시다! 사이먼킴과 컴퓨터 시각화의 세계에 빠져봅시다!!! 많은 응원과 관심 감사해요. ​  ​ "
Object Detection 속도 비교 with raspberry pi 4 ,https://blog.naver.com/codingteacher/222980687698,20230110,라즈베리파이에서 객체 인식 알고리즘 별 속도 측정을 해보았습니다.​Opencv + haar : 20 frame은 가볍게 넘습니다.압도적으로 속도가 빠르네요. Opencv + mobilenet v3 small : 평균 6 frame정도딥러닝 주행하면 4frame정도 안정적으로 나와서자율주행하면서 쓸 수 있습니다.haar에 비해 분류 class가 많고 확장도 용이해서 요걸로 사용할 예정입니다  ​mobilenet v2 : 평균 2 frame정도 못 씁니다. 느려요. ​tensorflow light : efficientdet_lite0 : 초당 4frame정도 나옵니다. 쓰레드 4개 쓴건 안비밀자율 주행하면서 쓰면 초당 3frame정도로 주행 가능합니다. 간혹 차선 탈출이 일어 납니다. ​ 
[영상 처리] Adaboost를 이용한 얼굴 인식(object detection) ,https://blog.naver.com/bsw2428/221500916032,20190330,"Adaboost 알고리즘을 사용하여, 영상에서 관심 있는 물체를 검출(object detection)하는 classifier를 만드는 과정이다.  관심 있는 물체(object) = 정면 얼굴(frontal face) ​​Adaboost 사용을 위해 필요한 것들​① 먼저, 훈련/학습시킬 영상 데이터 (training images / samples)가 필요하다.    ▶ 찾고자 하는 '얼굴' 영상들 : positive (object) samples    ▶ '얼굴이 아닌' 영상들 : negative (non-object, background) samples        positive image는 모두 똑같은 크기여야 한다.        그리고 (얼굴인 경우) 눈, 코, 입, 위치를 비슷하게 맞추어야 한다.   학습 데이터는 라벨링(labelling) 정보(어떤 클래스에 속하는지? pos vs. neg)를 갖고 있다.​② 영상에서 얼굴/배경을 구분하기 위한 특징들 (feature)도 필요하다.    ▶ feature는 물체를 구분/분류하는 특징이면서 분류기(classifier)로 볼 수 있다. 따라서    ▶ feature들은 base classifier이자, weak classifier 후보이다.     - 각각의 성능이 매우 뛰어날 필요는 없지만 적어도 50% 이상의 성능이어야 한다.       (랜덤으로 맞추는 확률보다는 높아야 weak classifier 조합했을 때 최종 성능 향상됨.)    ▶ feature에는 여러 종류가 있다. : Haar-like, LBP, MCT, HOG, etc.​③ weak classifier를 선택하는 과정을 몇 번 반복할 것인가? iteration round T를 정한다.     ▶ 몇 개의 weak classifier를 조합하여 하나의 strong classifier를 만들지?      - 매 iteration round마다, training sample들을 class에 맞게 분류하는 성능이 제일 좋은 단 하나의 feature        선택하여, 해당 iteration round에서의 weak classifier로 한다.      - T번 반복한다면 ? -> T개의 weak classifier를 얻는 것.      ▶ 반복 횟수 = weak classifier의 개수​초기화 Initialization모든 training sample 들의 weight(가중치, 중요도)를 동일하게 초기화한다.​positive samples의 수가 N개 -> positive 의 initial weight은 각각 1/Nnegative samples의 수가 M개 -> negatives의 initial weight은 각각 1/M​​반복 Iteration아래 과정을 T회 반복한다 (t = 1, 2, ... , T) :​① 모든 feature에 대해 training sample 들을 얼마나 잘 분류하는지 성능 평가한다.     어떻게? 각 feature마다 weighted error를 계산한다.         ▶ positive를 negative로 분류하거나, negative를 positive로 분류 -> 잘못된 분류, error.             얼마나 많은 training sample들을 각자의 class에 맞게 제대로 분류하지 못했는지 ?        ▶ weighted error = sample들의 weight를 고려하여 각 feature의 error를 구한다.​② 분류 성능이 가장 좋은 하나의 feature를 해당 round t 에서의 weak classifer로 한다.        ▶ 분류 성능이 가장 좋은 = weighted error가 가장 작은.​③ 해당 weak classifier의 weight(가중치, 중요도)를 구한다.        ▶ weighted error를 이용하여 계산한다.         - 에러가 크면 중요도는 작아지고, 에러가 작으면 중요도는 커지도록 한다.           = 성능이 좋은 weak classifier의 판단을 더 신뢰하여 점수를 더 쳐준다.​④ training sample들의 weight를 업데이트 한다.   ▶ 잘못 분류된 sample의 weight는 증가, 잘 분류된 sample의 weight는 감소시킨다.          오 분류된 sample의 weight이 커지는 건 해당 sample의 중요도가 커지는 것과 같다.         다음 iteration에서 해당 sample을 잘못 분류할 때 weighted error가 커지게 되고,         weighted error가 큰 feature는 weak classifier가 될 수 없다.         -> 이전 단계에서 잘못 분류된 sample을 제대로 분류할 수 있는 feature가 다음 단계에서 weak classifier로              될 수 있다.​⑤ t = t + 1 (① 번으로 돌아가 알고리즘을 반복한다.)​​[정리] 매 iteration round마다 분류 성능이 가장 좋은 하나의 feature를 선택하여 해당 iteration round 에서의 weak classifier로 한다. ( T번 반복 - > T개의 weak classifiers )​​​최종 Output​T개의 weak classifier를 weighted linear combination하여 최종 strong classifier를 얻는다.​   ​​   ​[참고] T개의 weak classifier를 모두 찾이 않는 경우도 있다.매 iteration마다 weak classifier를 하나씩 추가해가며 strong classifier를 업데이트해서strong classifier의 분류 성능이 일정 수준 이상을 만족하면모든 weak classifier T개를 전부 구하지 않더라도 알고리즘을 종료하기도 한다.즉, T보다 적은 수의 weak classifiers 만으로도 일정 수준 이상의 성능을 보이는 strong classifier가 가능하다고 판단하기 때문이다.​   ​​ "
[연구] 라이다 공부 및 개발일지 (2) 관심영역(ROI) 설정 후 물체 감지하기(Object Detection) ,https://blog.naver.com/ycpiglet/222624952401,20220118,"​ ​임의로 데이터를 생성하는 것에 성공​ ​이것을 활용해 네모난 모양의 박스를 만들었다.​ ​박스의 위치를 좀 조정한 뒤, 박스 내부에 들어오는 것을 검출해보기로 했다.​   ​(혼자 움직이면서 촬영해서 초점도 흔들리고 영상이 좀...)​여차저차 많은 일이 있었지만 겨우겨우 Rviz 상에서임의로 관심영역 즉, ROI(Region of Interest)를 시각화하는데 성공했다.​그리고 해당 영역에 들어가면 감지가 되게끔 프로그래밍을 했는데,여기서 문제는 각 물체(Object)를 어떻게 구분(Classification)할 것인지이다.​여기서부터는 머신러닝 또는 딥러닝의 영역에 들어가야 하나 싶다...(아마 필터도 사용해야할 듯 하다)​물론 노가다로 그냥 point cluster를 만들어서 설정할 수도 있겠지만 흠...어떻게 진행할 지는 고민중이다.​  ​실험환경은 일반 사무실에서 라이다 높이만 설정했다.​기본적으로 전원만 켜도 책상 등의 물체들이 관심영역 안에 잡혀서 약간의 전처리가 필요했다.​  ​평균적으로 15~18 개의 점들이 잡혀서20개 이하는 무시하기로 했다.​ ​코드를 좀 더 개선시켜서 환경에 맞춰 알아서 처음에 잡히는 점들을 지워내게끔 짰다.​  ​* OS 및 Framework : Linux Ubuntu 18.04 LTS, ROS Melodic* 사용 제품 : YDLiDAR G4​​ ​ "
YOLOv4：Optimal Speed and Accuracy of Object Detection Review ,https://blog.naver.com/phj8498/222109945646,20201008,"​https://hoya012.github.io/blog/yolov4/ YOLOv4：Optimal Speed and Accuracy of Object Detection ReviewYOLOv4：Optimal Speed and Accuracy of Object Detection 논문을 리뷰하였습니다.hoya012.github.io 안녕하세요, 오늘은 지난 4월 23일 arXiv에 공개된 “YOLOv4：Optimal Speed and Accuracy of Object Detection” 논문을 자세히 리뷰할 예정입니다.YOLO는 Object Detection을 공부하시는 분들이라면 다들 들어 보셨을 것이라 생각합니다. Object Detection은 크게 2-Stage 와 1-Stage Detector로 분류가 되며, 정확도를 약간 포기하는 대신 속도를 챙긴 1-Stage Detector의 대표적인 모델이며 v1, v2, v3에 이어 이번에 4번째 버전이 공개가 되었습니다. (이번엔 YOLO의 아버지인 Joseph Redmon이 빠졌습니다..! ㅠ)You Only Look Once: Unified, Real-Time Object DetectionYOLO9000: Better, Faster, StrongerYOLOv3: An Incremental ImprovementObject Detection에 대한 기본적인 내용을 공부하고 싶으신 분들은 제 블로그에 총 9편의 Object Detection Tutorial 이 존재하니 같이 참고하셔도 좋을 것 같습니다! Introduction[Object Detection Model Zoo]Object Detection에 대한 연구는 R-CNN을 시작으로 굉장히 다양한 연구가 단기간에 진행되었고, 이 글을 작성하고 있는 시점인 2020년 중순에도 많은 논문이 나오고 있습니다.deep learning object detection repository제가 Object Detection에 딥러닝을 적용한 논문들을 시간 순서에 따라 정리를 하고 있는 repository에 2020 CVPR까지 정리를 하였는데, 최근에는 새로운 모델을 제안하는 논문은 예전만큼 많지는 않고, Object Detection의 임계 성능을 높이기 위해 AutoML, Semi-Supervised Learning 등을 적용하는 시도가 주를 이루고 있습니다. 저는 이제 단일 모델에 대한 연구는 어느 정도 포화가 되었다고 생각을 했었는데, 잠시 잊고 있었던 YOLO의 4번째 버전이 공개되어서 반가웠습니다.저 또한 회사에서 Object Detection 알고리즘을 구현하여 현업에 적용을 하고 있지만 아직까지 실생활에서 실시간 동작이 가능할 만큼 성능이 완벽하지 않은 것이 현실입니다. 저자들도 Object Detection은 대부분 수 초에 걸쳐 detection이 처리되는 주차장에서 빈 자리 찾기, 공항 X-ray image에서 위험 물질 검출과 같은 상황에서 대부분 사용이 될 뿐, 실시간 동작이 필수적인 자율 주행 자동차 등에서는 아직 완벽하게 사용이 되기 어렵다고 강조하고 있습니다.이러한 한계를 극복하기 위해 저자들은 원래의 전략처럼 매우 빠르고 꽤 정확한 모델을 설계하는 데 집중하였습니다. YOLO의 새로운 버전이 출시되는 사이에 학계에서도 굉장히 다양한 기법들이 제안이 되어왔는데요, 본 논문에서는 학계에서 좋은 성능을 보이는 여러가지 기법들을 YOLO에 적용하여 성능 향상을 이뤘습니다. 논문에서는 이러한 기법들을 Bag or Freebies, Bag of Specials라 부르고 있으며, 각 기법들의 효과를 분석하였고, 기존 방법을 개선하는 결과도 보였습니다. 본 논문의 Main Contribution은 다음과 같습니다.Develop an efficient and powerful object detection models. It makes everyone can use just single GPU ( 1080 Ti or 2080 Ti)Verify the influence of SOTA Bag of Freebies and Bag of Specials methodsModify SOTA methods and make them more efficient and suitable for single GPU training본문으로 들어가기에 앞서 YOLO의 v1~v3까지를 하나의 그림으로 요약하면 다음과 같습니다. [YOLO v1 ~ v3 요약]제가 현재 참여 중인 PR-12 논문 읽기 모임에서 YOLOv1 ~ v3을 한국어로 잘 설명해주신 발표 영상이 있으니 같이 참고하셔서 공부하시면 좋을 것 같습니다. 참고로 오늘 소개드릴 YOLOv4는 제가 PR-12 논문 읽기 모임에서 최근 발표했고, 발표 영상은 하단 링크에서 확인하실 수 있습니다.PR-016: You only look once: Unified, real-time object detectionPR-023: YOLO9000: Better, Faster, StrongerPR-207: YOLOv3: An Incremental ImprovementPR-249: YOLOv4: Optimal Speed and Accuracy of Object Detection Bag of Freebies본 논문에서는 YOLO에 적용한 기법들을 2가지 유형으로 나눠서 설명하고 있습니다. 우선 Bag of Freebies는 Data augmentation, Loss function, Regularization 등 학습에 관여하는 요소로, training cost를 증가시켜서 정확도를 높이는 방법들을 의미합니다. [Bag of Freebies]Data Augmentation으로는 image의 일부 영역에 box를 생성하고 해당 영역을 0~255의 random한 값으로 채우는 Random erase, 0으로 채우는 CutOut, 두 image와 label을 alpha blending하는 MixUp, CutOut과 MixUp을 응용한 CutMix, Style-transfer GAN 등의 기법을 사용하였습니다.Regularization 기법으로는 DropOut, DropPath, Spatial DropOut, DropBlock 등을 이용하였고, Bounding Box Regression에 사용되는 Loss function으로는 MSE, IoU, Generalized IoU, Complete IoU, Distance IoU 등 다양한 기법을 사용하였습니다.이번 포스팅에서는 이 다양한 기법들을 하나 하나 설명드리진 않을 예정이며, 관심있으신 분들은 각 방법론들의 논문을 참고하시기 바랍니다. Bag of SpecialsBag of Specials는 architecture 관점에서의 기법들이 주를 이루고, post processing도 포함이 되어 있으며, 오로지 inference cost만 증가시켜서 정확도를 높이는 기법들을 의미합니다. 앞의 Bag of Freebies는 학습과 관련된 요소였다면, Bag of Specials는 학습에서는 Forward pass만 영향을 주고, 학습된 모델에 대해 Inference를 하는 부분에 관여를 한다는 점이 차이입니다. (사실 왜 구분을 하였는지 잘 모르겠습니다 ㅎㅎ) [Bag of Specials]Receptive Field를 키워서 검출 성능을 높이기 위해 제안된 Spatial Pyramid Pooling (SPP), atrous convolution(dilated convolution)을 적용한 ASPP, Receptive Field Block (RFB) 등을 사용하였고, Feature를 중간에서 합쳐주는 Skip-connection, Feature Pyramid Network (FPN), Scale wise Feature Aggregation Module (SFAM), adaptively spatial feature fusion (ASFF), BiFPN 등을 사용하였습니다.Activation Function으로는 자주 이용되는 ReLU 계열의 activation function과 AutoML로 찾은 Swish, Swish를 개선시킨 Mish 등을 사용하였습니다.Attention module에는 Squeeze-and-Excitation Module (SE), Spatial Attention Module (SAM)을 사용하였고, Normalization은 가장 많이 사용되는 Batch Normalization 외에 Filter Response Normalization (FRN), Cross-Iterative Batch Normalization (CBN)을 사용하였고, 본 논문에서는 1개의 GPU로 YOLOv4를 사용하는 것을 목표로 하였기 때문에 Cross-GPU Batch Normalization은 사용하지 않았습니다.마지막으로 Post Processing에는 예측된 Bounding box들 중 중복된 Bounding box들을 하나로 합쳐주는 Non Maximum Suppression (NMS), Soft NMS, DIoU NMS 등을 사용하였습니다. YOLOv4이제 YOLOv4의 architecture에 대해 설명을 드리겠습니다. 우선 YOLO의 고질적인 문제로 작은 object에 취약한 점이 있는데, 다양한 작은 object들을 잘 검출하기 위해 input resolution을 크게 사용하였습니다. 기존에는 224, 256 등의 resolution을 이용하여 학습을 시켰다면, YOLOv4에서는 512을 사용하였습니다.또한 receptive field를 물리적으로 키워 주기 위해 layer 수를 늘렸으며, 하나의 image에서 다양한 종류, 다양한 크기의 object들을 동시에 검출하려면 높은 표현력이 필요하기 때문에 parameter 수를 키워주었습니다.다만 이렇게 정확도 관점에서 좋은 방향의 변화를 주면 당연히 속도 관점에서 손해를 보게 되겠죠? 그래서 본 논문에서는 2020년 CVPR Workshop에 발표될 예정인 CSPNet 기반의 backbone을 설계하여 사용하였습니다. [CSPNet]CSPNet은 굉장히 heavy한 inference cost를 완화시키며 정확도 손실을 최소로 할 수 있는 Cross Stage Partial Network 구조를 제안하였으며, 위의 그림과 같이 input feature map을 2개의 part로 나눈 뒤, 하나의 part는 연산에 참여시키지 않고 뒤에서 합쳐주는 방식을 기반으로 inference cost, memory cost 등을 줄일 수 있었습니다. 또한, 학습 관점에서는 gradient flow를 나눠줘서 학습에 좋은 영향을 줘서 정확도 손실이 적다고 주장하고 있습니다.YOLOv4에서는 CSPNet 기반의 CSPDarkNet53을 제안하였으며, CSPResNext50, EfficientNet-B3에 비해 parameter수와 FLOPS는 많았지만 실제 Inference Time (Throughput)은 가장 좋은 결과를 보이고 있습니다. [YOLOv4 architecture]처음 보여드렸던 그림 상에서 YOLOv4를 대입해보면 위와 같이 나타낼 수 있습니다. 대부분의 아이디어는 YOLOv3를 기반으로 하였고, 여기에 backbone을 CSPDarkNet53으로 바꾸고 Neck에는 SPP와 Path Aggregation Network(PAN)을 적용하였고, 위에서 설명 드렸던 Bag of Freebies, Bag of Specials를 적용하였다고 정리할 수 있습니다. Additional Improvements마지막으로, 위에서 설명한 Bag of Freebies, Bag of Specials 외에도 저자들이 자체적으로 제안한 기법들에 대해서도 설명하고 있습니다.Mosaic Augmentation [Mosaic Augmentation]Mosaic Augmentation은 논문의 마지막 부분에 Acknowledgments를 참고하면 Glenn Jocher라는 분의 도움을 받아서 추가한 아이디어이며 4개의 image를 하나로 합치는 방식을 의미합니다. 위의 그림과 같이 각기 다른 4개의 image와 bounding box를 하나의 512x512 image로 합쳐주며, 당연히 image의 모양의 변화에 따라 bounding box GT의 모양도 바뀌게 됩니다. 이를 통해 하나의 input으로 4개의 image를 배우는 효과를 얻을 수 있어서 저자의 주장에 따르면 Batch Normalization의 statistics 계산에 좋은 영향을 줄 수 있다고 합니다. 기존 Batch Normalization에서는 작은 batch size를 사용하면 학습 안정성이 떨어져서 이를 개선하기 위해 Group Normalization, Switchable Normalization 등이 제안되었는데, Mosaic Augmentation을 이용하면 batch size가 4배 커지는 것과 비슷한 효과를 볼 수 있어서 작은 batch size를 사용해도 학습이 잘된다는 설명을 하고 있습니다. 여기에 제 사견을 덧붙이자면, 4개의 image를 하나로 합치는 과정에서 자연스럽게 small object들이 많아지다 보니 small object를 학습에서 많이 배우게 돼서 small object에 대한 성능도 좋아지지 않을까 생각해봅니다.Self-Adversarial Training다음은 Self-Adversarial Training이라는 방법을 제안하였다고 합니다. 다만 논문에 이 방법에 대한 설명도 부실하고 무엇보다 실험 결과가 존재하지 않아서 왜 이러한 내용을 넣어 놨는지 잘 이해가 되지 않네요. [Self-Adversarial Training]위의 그림은 yolo의 official code repository의 코드와 issue들을 샅샅이 뒤져서 찾아낸 그림입니다. 그림을 바탕으로 이해한 바를 설명드리자면, 우선 input image에 저희가 잘 아는 FGSM과 같은 adversarial attack을 가해서 model이 예측하지 못하게 만듭니다. 그 뒤, perturbed image와 원래의 bounding box GT를 가지고 학습을 시키는 것을 Self-Adversarial Training이라 부르고 있습니다. 이는 보통 정해진 adversarial attack에 robustness를 높이기 위해 진행하는 defense 방식인데, 이러한 기법을 통해 model이 detail한 부분에 더 집중하는 효과를 보고 있다고 설명하고 있는데, 실험 결과가 없어서 저는 잘 이해가 되지 않았습니다.Modified SAM, PAN, CmBN마지막으로 기존의 Spatial Attention Module(SAM), Path Aggregation Network(PAN), Cross iteration-Batch Normalization(CBN)을 본인들이 약간의 수정을 해서 사용을 하였다고 합니다. 다만 왜 수정을 하였는지는 잘 설명이 되어있지 않습니다. [Modified SAM, PAN, CmBN]Experiments and Results이제 실험 진행 방식과 실험 결과에 대해 알아보겠습니다. 앞서 굉장히 많은 Bag of Freebies, Bag of Specials 들을 다뤘었는데요, 모든 기법들을 다 실험하진 않고, 일부는 제외하였다고 합니다.Experimental Setup [Selection of Bag of Freebies, Bag of Specials]우선 학습이 어려워서 PReLU, SELU를 제외하였고, ReLU6도 quantized network에 특화된 함수여서 제외하였다고 합니다. Regularization 방법으로는 DropBlock 하나만 사용하였고, 그 이유로는 DropBlock의 저자들이 YOLO에 대해서 본인들의 방법이 다른 방법보다 우수함을 보여서 DropBlock만 사용하였다고 합니다. 이 외에도, YOLOv4의 철학이 Single GPU만으로 사용이 가능해야 한다! 이므로 Multi-GPU 학습 환경에서 사용되는 SyncBN도 제외하였습니다. [Experimental Setup Details]이 외에 ImageNet 데이터셋을 이용한 classification 실험, MS COCO 데이터셋을 이용한 object detection 실험에 대한 구체적인 실험 셋팅은 위와 같습니다.Experimental Result우선 실험에서 사용한 CSPResNeXt, CSPDarknet-53의 ImageNet Classification 실험 결과는 다음과 같습니다. [Classification 실험 결과]CutMix, Mosaic Augmentation과 Label Smoothing을 적용하여 성능이 약간 향상이 되었고, Activation Function으로는 ReLU대비 Swish는 오히려 안 좋은 결과를 보였고 Mish를 사용할 때 큰 폭의 성능 향상이 있었습니다. [Object Detection 실험 결과]Object Detection 실험에도 다양한 테크닉들을 적용하였는데, 9가지의 기법들에 대한 설명과 각각을 적용하였을 때의 성능 표는 위에 그림에서 확인하실 수 있습니다. 적용하여서 성능이 떨어진 기법들에 대해선 따로 설명을 하지 않을 예정이니, 혹시 궁금하신 분들은 논문을 참고하시기 바랍니다.우선 M은 앞서 설명드린 Mosaic Augmentation을 의미하고, GA는 Mosaic를 제안한 Glenn Jocher의 도움을 받아서 진행한 방식이며 유전 알고리즘을 이용한 hyper parameter search를 의미합니다. CBN은 Cross mini-Batch Normalization, CA는 Cosine Annealing Learning Rate scheduling을 의미하고, 마지막으로 OA는 YOLOv2에서 제안된 k-means 기반의 사전에 정해 놓은 anchor 크기를 사용하는 것을 의미하며, 512x512 input resolution에 최적화된 anchor들을 사용하였다고 합니다. 이 외에도 loss도 MSE, GIoU, DIoU, CIoU 등을 사용하였고, 각각을 적용하였을 때의 Ablation study 결과도 제시되어 있습니다.여담이지만 제 블로그에 리뷰가 되어있는 2019 ICCV에 발표된 Gaussian YOLO 기법을 YOLOv4에 적용하였을 때 오히려 정확도가 떨어지는 결과도 논문에 제시되어 있습니다. 하하.. 아쉽네요..이 외에도 추가적인 실험 결과들이 더 제시되어 있는데, 별로 중요한 것 같지 않아서 설명은 생략하도록 하겠습니다. Batch size를 4로 줄였을 때, 본인들이 제안한 방법을 쓰면 batch size가 8일 때랑 성능이 거의 비슷하다는 결과 정도만 짚고 넘어가면 될 것 같습니다. 결론이번 포스팅에서는 YOLO의 4번째 버전에 대해 자세히 알아보았습니다. YOLOv3 대비 정확도(AP)를 거의 10% 포인트나 끌어 올린 점이 인상깊으며, 실시간으로 동작해야 하는 조건에서는 고려해 봄직한 모델을 만든 것 같습니다. 물론 연구적인 관점에서 봤을 때는 다소 주먹구구식으로 이 방법 저 방법을 가져다 쓴 느낌이 들긴 하지만, 실용적인 관점에서 봤을 때는 Single GPU로 학습과 테스트, 모델 배포가 가능하다는 점이 가장 큰 장점이라고 생각합니다. 또한 Object Detection 관련 다양한 기법들을 제시하고 있어서, Object Detection을 공부하시는 분들, Kaggle 등 Challenge를 준비하시는 분들이라면 아이디어를 얻어 가시기 좋을 것 같습니다. 공부하시는데 도움이 되셨으면 좋겠습니다! 감사합니다! "
[ML] Tutorials of Object Detection using Deep Learning ,https://blog.naver.com/horajjan/221432941063,20190103,1. What is object detection?​ “Tutorials of Object Detection using Deep Learning [1] What is object detection?”Deep Learning을 이용한 Object detection Tutorial - [1] What is object detection?hoya012.github.io 2. First Object Detection using Deep Learning “Tutorials of Object Detection using Deep Learning [2] First Object Detection using Deep Learning”Deep Learning을 이용한 Object detection Tutorial - [2] First Object Detection using Deep Learninghoya012.github.io 3. The application of Object Detection “Tutorials of Object Detection using Deep Learning [3] The application of Object Detection”Deep Learning을 이용한 Object detection Tutorial - [3] The application of Object Detectionhoya012.github.io 4. How to measure performance of object detection “Tutorials of Object Detection using Deep Learning [4] How to measure performance of object detection”Deep Learning을 이용한 Object detection Tutorial - [4] How to measure performance of object detectionhoya012.github.io 5. Training Deep Networks with Synthetic Data Bridging the Reality Gap by Domain Randomization Review “Tutorials of Object Detection using Deep Learning [5] Training Deep Networks with Synthetic Data Bridging the Reality Gap by Domain Randomization Review”Deep Learning을 이용한 Object detection Tutorial - [5] Training Deep Networks with Synthetic Data Bridging the Reality Gap by Domain Randomization Reviewhoya012.github.io 6. Object Detection Multi Scale Testing Method Review “Tutorials of Object Detection using Deep Learning [6] Object Detection Multi Scale Testing Method Review”Deep Learning을 이용한 Object detection Tutorial - [6] Object Detection Multi Scale Testing Method Reviewhoya012.github.io 7. Object Detection 최신 논문 Review “Tutorials of Object Detection using Deep Learning [7] Object Detection 최신 논문 Review”Deep Learning을 이용한 Object detection Tutorial - [7] Object Detection 최신 논문들을 간단하게 Review하였습니다.hoya012.github.io 8. Object Detection Labeling Guide “Tutorials of Object Detection using Deep Learning [8] Object Detection Labeling Guide”Deep Learning을 이용한 Object detection Tutorial - [8] Object Detection Labeling에 대한 설명과 Tool 사용법 등을 소개드립니다.hoya012.github.io 
(딥러닝 12주차) Object Detection ,https://blog.naver.com/z1z11009/222440835758,20210722," Object Localization : 단순 이미지 인식/분류에서 더 나아가 위치를 포착하려면 b_x, b_y, b_w, b_h의 4가지 변수가 더 필요하다. Defining the target label yy = 8*1 vector (containing p_c, b_x, b_y, b_h, b_w, c_1, c_2, c_3): 위치를 탐지하고자 하는 물체가 3종류이고, 그 중 한 개만 사진에 드러난다면 출력층은 위와 같이 구성된다.이 때 p_c는 물체가 있는지 없는지 유무(0/1)를 나타내고, p_c가 0이라면 이외의 성분들은 아예 무시하게 된다(?로 처리). : 손실함수는 위와 같다. : 얼굴/신체의 특정 랜드마크를 찾는 것도 유사한 방법을 따른다. 오늘날 특수효과를 적용하는 보정 어플은 이러한 기술을 활용한다.  Sliding Window : 이러한 감지기를 만들기 위해서는 label된 데이터를 가지고 ConvNet을 학습시킨다.  : ConvNet이 하는 일은 Sliding window를 활용하여 사진의 각 부분에 물체가 있는지 유무를 판단한다. : 한 번 실행한 이후에는 더 큰 window를 활용하여 반복한다.=> 그렇지만 이는 독립적으로 수행되어 계산 비용이 많이 든다. stride가 작을수록, 계산 비용은 기하급수적으로 늘어난다.=> 이를 개선하기 위해 Convolutional layer을 활용한다.  Convolutional Implementation of Sliding Windows : 지금까지 배운 ConvNet은 위와 같다. : Sliding window의 연산비용을 낮추기 위해서는 이처럼 일반적인 ConvNet에서 모든 FC를 Convolution layer로 바꿔준다. 이 때 기존의 FC 벡터의 유닛의 수는 Volume으로 바뀌어, 수학적으로 아무런 문제가 없다.  : Convolution implementation의 출력층은 Sliding window의 결과물을 모두 담고 있다. 위의 예시에서는 16*16*3의 test set에 14*14*3의 window를 적용한 것이다. => 굳이 window를 slide하는 연산을 수행하지 않아도 그 결과물이 알아서 저장된다. : 인풋 데이터의 크기가 커지더라도 마찬가지이다. 출력층의 크기만 더 커질 뿐, 굳이 slide는 하지 않아도 된다.cf) Boundary box의 위치에 대한 세밀한 조정이 불가능하다는 단점이 있긴 하다.(box 안에 물체가 정확히 위치해야만 감지할 수 있다) "
ST-HOI: A Spatial-Temporal Baseline for Human-Object Interaction Detection in Videos 논문 리뷰(ICMR2021) ,https://blog.naver.com/kingjykim/222996901564,20230128,"Paper : https://dl.acm.org/doi/pdf/10.1145/3463944.3469097Git : https://github.com/coldmanck/VidHOIAuthor : Meng-Jiun Chiou, Chun-Yu Liao, Li-Wei Wang, Roger Zimmermann and Jiashi Feng​​1. Introduction​  딥 러닝의 급속한 발전 덕분에 기계는 언어 작업, 음향 작업 및 비전 작업에서 이미 인간 수준의 성능을 근접하거나 능가하고 있다. 따라서 연구자들은 기계가 물체를 인식하는 것뿐만 아니라 그들의 관계와 맥락을 이해하는 것을 배울 수 있도록 이러한 성공을 의미적으로 더 높은 수준의 비전 작업과 비전 언어 작업으로 복제하는 방법에 초점을 맞추기 시작했다. 특히 이미지/비디오에서 인간과 두드러진 물체 사이의 행동과 공간적 관계를 감지하는 것을 목표로 하는 human-object interaction(HOI)은 때때로 보행자 감지 및 무인 상점 시스템과 같은 인간의 행동을 이해하기 위해 기계를 작업하기 때문에 점점 더 많은 관심을 끌고 있다. Figure 1: An illustrative comparison between conventional HOI methods and our ST-HOI when inferencing on videos.  정적 이미지에서 HOI를 탐지하는 데 성공한 연구가 풍부하지만, 비디오 데이터에서 수행될 때 시간 정보(즉, 대상 프레임 전후의 이웃 프레임)를 고려하는 연구가 거의 없다는 것은 실제로 동시에 발생한 행동(이미지)만으로 시간 관련 HOI를 ""추측""하고 있음을 의미한다. 기존의 이미지 기반 HOI 방법은 비디오에 대한 추론에 사용될 수 있지만 입력 프레임을 독립적이고 동일하게 분포된(i.i.d.) 데이터로 취급하고 인접 프레임에 대해 각각 예측을 한다. 그러나 비디오 데이터는 순차적이고 본질적으로 구조화되어 있기 때문에 I.I.D.가 아니다. 더 큰 한계점은 이러한 방법들이 시간적 맥락 없이는 인간을 밀어 당기는 것과 문을 여는 것과 닫는 것과 같은 (특히 반대의) 시간적 상호 작용을 구별할 수 없다는 것이다. 그림 1(a)에서 보는 바와 같이, 비디오 세그먼트가 주어지면, 전통적인 HOI 모델은 한 번에 하나의 프레임에서 작동하고 2D-CNN 시각적 특징을 기반으로 예측한다. 이러한 모델은 본질적으로 정적 이미지에서 시각적으로 유사한 밀기, 당기기, 기대기, 쫓기와 같은 두 사람 사이의 상호 작용을 구별할 수 없었다. 비디오 기반 HOI 연구가 아직 적은 이유는 적절한 비디오 기반 벤치마크와 실현 가능한 설정이 없기 때문이다. 이 격차를 해소하기 위해 먼저 VidOR에서 VidHOI라는 비디오 기반 HOI 벤치마크를 구성한다. VidHOI는 비디오의 공통 프로토콜과 HOI 작업을 따라 키 프레임 중심 전략을 사용한다. VidHOI와 함께, 저자는 비디오 데이터의 사용을 촉구하고 비디오를 제안한다. VidHOI는 훈련과 추론 모두에서 비디오로 HOI 검출을 수행한다. Figure 2: (a) Relative performance change (in percentage), on different video tasks by replacing 2D-CNN backbones with 3D ones (blue bars) [2, 7, 36], and on VideoHOI by adding trajectory feature (tangerine bar).  (b) An illustration of temporal-RoI pooling in 3D baselines .  Spatial-Temporal Action Detection (STAD)는 인간을 localize하고 비디오에서 수행되는 행동을 감지해야 한다는 점에서 VideoHOI와 유사한 또 다른 작업이다. STAD는 인간이 상호작용하는 객체를 고려하지 않는다. STAD는 일반적으로 먼저 3D-CNN을 백본으로 사용하여 시간 정보를 특징 맵으로 인코딩함으로써 해결된다. 이어서 RoI 풀링을 객체 제안과 함께 수행하여 행위자 특징을 얻으며, 이는 선형 계층에 의해 분류된다. 본질적으로, 이 접근법은 Figure 1(a)에 설명된 일반적인 HOI 기준선과 유사하며 3D 백본의 사용과 상호 작용하는 물체의 부재에서만 다르다. 기존 HOI 및 STAD 방법을 기반으로 2D 백본을 3D 백본으로 대체하고 상호 작용하는 물체의 시각적 특징을 활용하여 두 세계의 최고를 누릴 수 있는가라는 순진하면서도 직관적인 아이디어가 발생한다. 그러나 이 아이디어는 VideoHOI를 수행하기 위해 2D 기준선의 백본을 3D(예: SlowFast)로 교체한 예비 실험에서 직접적으로 작동하지 않았다. 백본 교체 후 성능의 상대적 변화는 Figure 2(a)의 가장 왼쪽 항목에 파란색 막대로 표시된다. VideoHOI 실험에서 3D 기준선은 상대적으로 제한된 개선(~2%)만 제공하며, 이는 추가적인 temporal context을 고려할 때 만족스럽지 않다. 실제로, 이 현상은 유사한 설정 하에서 두 개의 기존 연구에서도 관찰되었으며, 여기서 STAD와 다른 비디오 작업 Spatial-Temporal Scene Graph Generation (STSGG)에서의 실험은 훨씬 더 나쁘고 직관적이지 않은 결과를 제시한다. 백본을 교체하는 것은 실제로 좋지 않다. (Figure 2(a)에서도 파란색 막대로 표시됨) 저자는 이러한 3D 기준선의 아키텍처를 분석하여 근본적인 이유를 조사했고 놀랍게도 RoI pooling과 함께 temporal pooling이 합리적으로 작동하지 않는다는 것을 발견했다. Figure 2(b)에서 설명한 바와 같이, 기존의 STAD 방법에서 일반적인 관행인 RoI pooling에 이은 temporal pooling은 객체가 움직이는 방식을 고려하지 않고 전체 비디오 세그먼트에 걸쳐 동일한 영역의 기능을 자르는 것과 같다. 이웃 프레임에서 움직이는 인간과 객체가 대상 keyframe의 위치에 없는 것은 드문 일이 아니다. 동일한 위치에 있는 Temporal-and-RoI-pooling 기능은 다른 사람/객체 또는 무의미한 배경과 같은 잘못된 기능을 얻을 수 있다. 이러한 불일치를 처리하면서, VideoHOI에서 인간과 객체 궤적을 고려하여 누락된 공간-시간 정보를 복구할 것을 제안한다. VideoHOI에 대한 이 시간적 증강 3D 기준선의 성능 변화는 Figure 2(a)의 주황색 막대로 표현되며, 여기서 원래 3D 기준선의 ~2%와 뚜렷한 대조를 이루며 ~23% 향상을 달성한다. 이 실험은 ""correctly-localized"" 시간 정보를 통합하는 것의 중요성을 보여준다.  앞서 언급한 아이디어를 염두에 두고, 본 논문에서는 비디오에서 Human-Object Interaction 탐지를 위한 Spatial-Temporal baseline, 즉 궤적을 기반으로 인스턴스별 공간-시간적 특징으로 정확한 HOI 예측을 하는 ST-HOI를 제안한다. Figure 1(b)에 설명된 바와 같이, ST-HOI에서는 세 가지 종류의 그러한 특징이 이용된다. (a) 궤도 특징(이동 경계 상자; 빨간색 화살표로 표시됨)(b) 정확하게 지역화된 시각적 특징(노란색 화살표로 표시됨)(c) 공간-시간적인 사람의 자세(녹색 화살표로 표시됨).​저자의 연구 기여도는 다음의 3가지이다.첫째, 간단하지만 “correct” spatial-temporal feature pooling으로 해결하는 3D 모델에 존재했던 특징 불일치 문제를 최초로 식별했다.둘째, 비디오 기반 HOI를 효과적으로 감지하기 위해 정확하게 지역화된 시각적 특징, 프레임 상자별 좌표 및 새로운 시간 인식 masking pose module을 활용하는 공간-시간 모델을 제안한다.셋째, 공간-시간 인식 상호 작용을 탐지하는 연구에 동기 부여하고 비디오 프레임, 텍스트 (의미 객체/관계 레이블) 및 오디오와 같은 다중 양식 데이터를 활용하는 VidHOI 접근 방식에 영감을 주기 위해 keyframe 기반 VideoHOI 벤치마크를 설정한다.​​2 RELATED WORK ​2.1 Human-Object Interaction (HOI)  HOI 탐지는 인간과 대상 물체 간의 상호 작용을 추론하는 것을 목표로 한다. HOI는 시각적 관계 감지 및 장면 그래프 생성과 밀접한 관련이 있으며, 여기서 (subject-predicate-object)의 주제는 인간으로 제한되지 않는다. 정적 이미지의 HOI는 최근 집중적으로 연구되고 있다. 기존 방법의 대부분은 human-object 쌍 제안과 상호 작용 분류의 순서에 따라 두 가지 범주로 나눌 수 있다. 첫 번째 그룹은 human-object 쌍 생성 후 상호 작용 분류를 수행하는 반면, 두 번째 그룹은 먼저 사람이 수행하는 가장 가능성이 높은 상호 작용을 예측한 후 가장 가능성이 높은 개체와 연결한다. 우리의 ST-HOI는 궤적(연속 객체 제안)을 기반으로 시간 모델을 설정함에 따라 첫 번째 그룹에 속한다.  이미지 기반 HOI의 인기와 대조적으로 VideoHOI에는 몇 가지 연구만 있으며, 우리가 아는 한 CAD-120 데이터 세트에 대한 실험을 수행했다. CAD-120에서 상호작용은 120개의 RGB-D 비디오에서 10개의 높은 수준의 활동(예: 시리얼 또는 전자레인지 음식 만들기)으로 정의된다. 이 설정은 컴퓨터가 더 세밀한 행동을 이해하도록 요청할 수 있는 현실에는 적합하지 않다. 또한, 이전 방법은 심층 신경망에서 능가하는 SIFT와 같은 사전 계산된 수작업 기능과 현실에서는 사용할 수 없을 것 같은 RGB-D 비디오의 3D 포즈 및 depth 정보를 포함한 ground truth 정보 기능을 채택했다. ResNet을 백본으로 채택했지만, 𝑀 인간과 𝑁 객체의 특징을 추출하기 위해 𝑀 × 𝑁 계산이 필요하기 때문에 그들의 방법은 비효율적이다. 이러한 기존 방법과 달리 VidHOI는 수천 개의 비디오에 50개의 predicates 주석을 포함하여 더 크고 다양한 비디오 HOI 벤치마크에서 평가한다. 그런 다음 RGB 비디오에서 작동하고 추가 정보를 활용하지 않는 spatial-temporal HOI 기준선을 제안한다.​2.2 Spatial-Temporal Action Detection (STAD)  STAD는 행동하는 사람을 localize하고 (상호 작용하는 객체를 고려하지 않고) 관련 동작을 감지하는 것을 목표로 한다. STAD의 가장 인기 있는 벤치마크 중 하나는 AVA이며, 여기서 주석은 1Hz의 샘플링 주파수에서 수행되고 프레임별 평균 AP로 성능을 측정한다. VidHOI를 구성할 때 이 주석 및 평가 스타일을 따라 원래 레이블을 동일한 형식으로 변환했다.  Section 1에서 설명한 바와 같이, STAD에 대한 표준 접근 방식은 3D-CNN을 사용하여 spatial-temporal feature map을 추출한 후 RoI pooling을 통해 인간 특징을 잘라내고, 이를 선형 레이어로 분류한다. Figure 2(a)에 나온 것처럼, RoI pooling된 인간/객체 기능을 통합하는 단순한 수정은 VideoHOI에게 효과가 없다. 대조적으로, ST-HOI는 궤적, 정확하게 지역화된 시각적 특징 및 공간-시간적 마스킹 포즈 특징을 포함한 여러 시간적 특징을 통합하여 VideoHOI를 처리한다.​2.3 Spatial-Temporal Scene Graph Generation  Spatial-Temporal Scene Graph Generation (STSGG)은 비디오 프레임에서 쌍별 시각적 관계를 나타내는 symbolic graphs를 생성하는 것을 목표로 한다. STSGG.Ji 등의 연구를 용이하게 하기 위해 새로운 벤치마크인 Action Genome도 제안되었다. STSGG은 기성 장면 그래프 생성 모델을 2D 또는 3D-CNN 위의 장기 피처 뱅크와 결합하여 STSGG를 다루었는데, 여기서 3D-CNN이 실제로 성능을 저하시킨다는 것을 발견했다. VidHOI에서 유사한 결과를 관찰하면서(Figure 2(a)), 저자는 한 걸음 더 나아가 프레임에 걸친 RoI 기능이 잘못 풀링되었기 때문이라는 근본적인 이유를 알아낸다. 저자는 객체 궤적을 활용하고 생성된 궤적에 관심 튜브(ToI) 풀링을 적용하여 비디오 세그먼트 전체에 걸쳐 정확하게 지역화된 위치 정보와 기능 맵을 얻음으로써 이를 수정한다.​​3 METHODOLOGY ​3.1 Overview  keyframe 중심 전략의 VideoHOI를 감지하기 위해 STAD 접근 방식을 따른다. 샘플링 주파수가 1Hz인 𝑇 keyframe을 {𝐼𝑡 }, 𝑡 = {1, ..., 𝑇 }으로 하는 비디오로 𝑉를 나타내고, 미리 정의된 상호작용 클래스의 개수로 𝐶을 나타냅니다. 대상 프레임을 중심으로 한 비디오 세그먼트에서 인간 궤적(𝑀 ≤ 𝑁)을 포함한 𝑀 인스턴스 궤적을 고려할 때, keyframe 𝐼𝑡의 인간 𝑚 ∈ {1, ..., 𝑀}과 객체 𝑛 ∈ {1, ..., 𝑁}에 대해,  저자는  pairwise human-object interactions 𝑟𝑡 = {0, 1}C을 탐지하는 것을 목표로 하며, 여기서 각 항목 𝑟𝑡,𝑐, 𝑐 ∈ {1, ...,𝐶}은 상호 작용 𝑐의 존재 여부를 의미합니다.  ST-HOI에 대한 설명은 그림 1(b)에 나와있다. 우리 모델은 𝐼𝑡 를 중심으로 한 비디오 세그먼트(𝑇 프레임 시퀀스)를 받아들이고 3D-CNN을 백본으로 사용하여 전체 세그먼트의 공간-시간 피처 맵을 추출한다. 시간적 폴링-RoI 풀링으로 인한 불일치를 수정하기 위해 𝑁 객체(인간 포함) 궤적 { 𝑗𝑖 }, 𝑖 = {1, .., 𝑁}, 𝑗𝑖 ∈ R𝑇×4를 기반으로 정확하게 지역화된 기능과 공간-시간 masking pose 특징을 포함한 시간 인식 기능을 생성한다. 궤적과 함께 이러한 특징은 선형 레이어에 의해 연결되고 분류된다. 저자는 non-local block 또는 long-term feature bank와 같은 STAD 및 이미지 기반 HOI와 같은 상호 작용에서 트릭을 활용하지 않도록 VideoHOI에 대한 간단하지만 효과적인 시간 인식 기준선을 목표로 한다는 점에 주목한다.​ Figure 3: An illustration of the two proposed spatial-temporal features.3.2 Correctly-localized Visual Features  부적절하게 풀링된 RoI 기능에 대해 이전 섹션에서 논의했고 시간적 풀링과 RoI 풀링의 순서를 반대로 하여 이 문제를 해결할 것을 제안한다. 이 접근법은 최근 ""Tube convolutional neural network (T - CNN) for action detection in videos."" 에서 제안되었으며 tube-of-interest pooling(ToIPool)으로 명명되었다. 그림 3(a)을 참고하면 3D-CNN 백본의 끝에서 두 번째 레이어의 출력으로 𝑣 ∈ R𝑑×𝑇×𝐻×𝑤를 나타내고, 시간 축을 따라 𝑡번째 피쳐 맵으로 𝑣𝑡 ∈ R𝑑×𝐻×𝑤을 나타낸다. keyframe에 중심을 둔 N 궤도가 존재한다. 기존 방식에 따라, 저자는 상호 작용을 예측할 때 시각적 맥락을 활용하는데, 이는 인간과 객체의 결합 경계 상자 기능을 활용하여 이루어진다. 예를 들어, human과 kite 사이의 하늘은 정확한 상호작용 fly을 추론하는 데 도움이 될 수 있다. 𝑗𝑖는 물체 𝑖의 궤적을 나타내며, 여기서 𝑗𝑖,𝑡를 시간 𝑡의 2D 경계 상자로 더 나타낸다. 그런 다음 공간-시간 인스턴스 특징 {𝑣¯𝑖}는 식1에 의해 RoI lign을 가진 ToIPool을 사용하여 얻는다.  여기서 𝑣¯𝑖 ∈ R𝑑×ℎ×𝑤, ℎ, 𝑤는 풀링된 피쳐 맵의 높이와 폭을 의미한다. 𝑣¯𝑖는 다른 기능과 연결하기 전에 납작해집니다.​3.3 Spatial-Temporal Masking Pose Features  사람의 특징적인 자세를 활용하여 특별한 행동를 추론하는 이미지 기반 HOI 방법에 널리 사용되어 왔다. 또한 일부 기존 연구에서는 공간 정보가 상호 작용을 식별하는 데 사용될 수 있다는 것을 발견했다. 예를 들어, 인간이 타는 말의 경우, 말을 탄 사람의 골격은 (말의 측면에서) 다리를 넓게 벌린 것으로 상상할 수 있으며, 인간의 바운딩 박스 중심은 보통 말의 그것 위에 있다. 그러나 기존 연구 중 어떤 것도 시간 영역에서 말을 탈 때 인간은 말 전체와 함께 움직여야 한다는 메커니즘을 고려하지 않는다. 저자는 이 시간성이 중요한 속성이며 또한 활용되어야 한다고 주장한다.  spatial-temporal masking pose module은 그림 3(b)에 제시되어 있다. 𝑀개의 인간 궤적이 주어지면, 먼저 훈련된 인간 자세 예측 모델로 𝑀개의 공간-시간적 자세 특징을 생성한다. 프레임 𝑡에서 예측된 인간 자제 ℎ𝑖,𝑡 ∈ R17×2, 𝑖 = {1, .., 𝑀}, 𝑡 = {1, ..,𝑇 }는 원래 이미지에 매핑된 17개의 관절 지점으로 정의된다. 우리는 각 선이 고유한 값 𝑥 ∈ [0, 1]를 갖는 선을 사용하여 관절을 연결함으로써 𝑓ℎ : {ℎ𝑖,𝑡 } ∈ R17×2 → { ℎ¯𝑖,𝑡 } ∈ R1×𝐻×𝑊가 있는 이진 마스크에서 ℎ𝑖,𝑡를 골격으로 변환한다. 이를 통해 모델이 다양한 자세를 인식하고 차별화할 수 있다.  frame 𝑡의 𝑀 × (𝑁 − 1)개의 유효한 human-object 쌍 각각에 대해, 각각 인간과 객체에 해당하는 두 개의 공간 마스크 𝑠𝑖,𝑡 ∈ R2×𝐻×𝑊, 𝑖 = {1, ..., 𝑀 × (𝑁 − 1)}를 생성한다. 여기서 각 경계 상자 내부의 값은 1이고 외부는 0으로 표시된다. 이러한 마스크를 통해 모델은 중요한 공간 정보를 참조하여 HOI를 예측할 수 있다.  각 쌍에 대해, 저자는 첫 번째 차원을 따라 골격 마스크 ℎ¯𝑖,𝑡 및 공간 마스크 𝑠𝑖,𝑡를 연결하여 초기 공간 마스킹 포즈 기능 𝑝𝑖,𝑡 ∈ R3×𝐻×𝑊을 얻는다. 그런 다음 {𝑝𝑖,𝑡}를 다운샘플링하고 공간 및 시간 풀링이 있는 두 개의 3D 컨볼루션 레이어로 공급한 다음 평평하게 하여 최종 spatial-temporal masking pose 특징 {𝑝¯𝑖,𝑡}를 얻는다.​3.4 Prediction  저자는 정확하게 지역화된 시각적 특징 𝑣¯, 공간-시간적 마스킹 포즈 특징 𝑝 및 인스턴스 궤적 𝑗를 마지막 축인 식3에 따라 연결하여 앞서 언급한 특징을 융합한다. 여기서 구독 𝑠을 주체로, 𝑜는 객체로, 𝑢는 결합 영역으로 나타내기 위해 표기법을 약간 남용한다. 그런 다음 𝑣𝑠𝑜는 데이터 세트의 상호 작용 클래스 수가 최종 출력 크기인 두 개의 선형 계층으로 공급된다. VideoHOI는 본질적으로 다중 레이블 학습 작업이기 때문에 클래스별 이진 교차 엔트로피 손실로 모델을 훈련시킨다.  추론하는 동안, 저자는 가능한 모든 쌍을 softmax 점수로 정렬하고 상위 100개 예측에 대해서만 평가하기 위해 이미지 기반 HOI의 휴리스틱을 따른다.​​4 EXPERIMENTS ​ Table 1: A comparison of our benchmark VidHOI with existing STAD (AVA), image-based (HICO-DET and V-COCO) and video-based (CAD-120 and Action Genome) HOI datasets.4.1 Dataset and Performance Metric  CAD-120을 분석을 통해 적합한 VideoHOI 데이터 세트가 부족하다는 문제에 대해 앞에서 논의했지만, 여기서는 Action Genome도 실현 가능한 선택이 아닌 이유를 추가로 설명한다. 첫째, 저자들은 데이터 세트가 여전히 불완전하고 잘못된 레이블을 포함하고 있음을 인정했다. 둘째, Action Genome은 원래 각 클립이 사전 정의된 작업을 수행하는 하나의 ""actor""만 포함하는 활동 분류를 위해 설계된 Charades에 주석을 달아 생성된다. 다른 사람이 나타나면 경계 상자나 상호 작용 레이블이 없다. 마지막으로, 그 비디오들은 volunteers들에 의해 의도적으로 만들어지는데, 이것은 다소 부자연스럽다. 대조적으로, VidHOI는 모든 인간과 각 프레임에 나타나는 사전 정의된 개체로 빽빽하게 주석이 달린 VidOR를 기반으로 한다. VidOR는 또한 비디오가 자원 봉사가 아닌 사용자 생성이므로 때때로 떨리기 때문에 더 어렵다. VidHOI와 기존 STAD 및 HOI 데이터 세트의 비교는 Table 1에 제시되어 있다. Figure 4: Predicate distribution of the VidHOI benchmark shows that most of the predicates are non-temporal-related.  VidOR는 원래 궤적 기반으로 비디오 시각적 관계 감지를 위해 수집된다. 궤적과 ground truth 사이의 체적 상호 작용(vIOU)은 관계 예측을 고려하기 전에 0.5 이상이어야 한다. 그러나 올바른 시작 및 종료 스탬프로 정확한 궤적을 얻는 방법은 여전히 어렵다. 일부 이미지 기반 HOI 데이터 세트(예: HICO-DET 및 V-COCO)와 STA 데이터 세트(예: AVA)는 앞서 언급한 문제를 우회하는 키 프레임 중심 평가 전략을 사용하고 있음을 알 수 있다. 따라서 우리는 동일한 것을 채택하고 AVA를 따라 1 FPS 주파수에서 키 프레임을 샘플링한다. 여기서 타임스탬프 𝑡의 키 프레임에 대한 주석은 𝑡 ± 0.5초 동안 고정된 것으로 가정한다. 세부적으로, 먼저 적어도 하나의 유효한 인간-객체 쌍을 제시하지 않고 이러한 키 프레임을 필터링한 다음 레이블을 비디오 클립 기반에서 키 프레임 기반으로 변환하여 일반적인 HOI matric(즉, 프레임 mAP)과 일치시킨다. 원래 VidOR 분할을 따라 VidHOI를 6,366개 비디오의 193,911개 키 프레임으로 구성된 교육 세트와 756개 비디오의 22,808개 키 프레임으로 구성된 검증 set2로 나눈다. Fig 4와 같이 동작(예: 밀기, 당기기, 들기 등)과 공간 관계(예: 옆, 뒤 등)를 포함한 50개의 관계 등급이 있다. 예측 클래스의 절반(25개)은 시간과 관련이 있지만 데이터 세트의 ~5%에 불과하다.  HICO-DET의 평가 지표에 따라, 저자는 평균 정밀도(mAP)를 채택하는데, 여기서 진정한 양의 HOI는 아래의 세 가지 기준을 충족해야 한다. (a) 예측된 인간 및 객체 경계 상자는 모두 0.5 이상의 IOU를 가진 ground truth box와 겹쳐야 하며, (b) 예측된 대상 범주가 일치해야 하며 (c) 예측된 상호 작용이 정확해야 한다. 50개 이상의 술어에서, 저자는 HICO-DET를 따라 평균 AP를 계산하는 557개의 삼중항으로 HOI 범주를 정의한다. 삼중항으로 HOI 범주를 정의함으로써 폴리세미 문제를 우회할 수 있다. 즉, 동일한 예측 단어가 person-fly-kite 및 person-fly-airplane와 같은 별개의 물체와 쌍을 이룰 때 매우 다른 의미를 나타낼 수 있다. 저자는 세 가지 범주에 걸쳐 평균 AP를 보고한다. (a) Full: 모든 557개 범주가 평가된다. (b) Rare: 데이터 세트에서 인스턴스가 25개 미만인 315개 categories, (c) Non-rare: 데이터 세트에서 인스턴스가 25개 이상인 242개 categories. 저자는 또한 두 가지 평가 모드에서 모델을 조사한다. Oracle 모델은 실측 궤적으로 훈련 및 테스트되고, Detection 모드의 모델은 예측 궤적으로 테스트된다.​4.2 Implementation Details  저자는 예비 실험을 위해 Resnet-50을 2D 백본으로 채택하고, 다른 모든 실험을 위해 Resnet-50 기반 SlowFast를 3D 백본으로 활용한다. SlowFast는 서로 다른 주파수의 비디오 프레임을 샘플링하여 텍스처 세부 정보와 시간 정보에 해당하는 느린 경로와 빠른 경로를 포함한다. keyframe을 중심으로 한 64 프레임 세그먼트의 경우 𝑇 =32 프레임이 교대로 샘플링되어 느린 경로를 통해 공급된다; 𝑇/𝛼 프레임만 고속 경로를 통해 공급된다. 여기서 𝛼 =8. FastPose를 사용하여 비디오 객체 감지, 시간적 NMS 및 추적 알고리즘의 자세 그리고 cascaded model을 예측한다. 물체 감지가 2D HOI 감지와 마찬가지로 궤적 생성은 필수 모듈이지만 이 작업의 주요 초점은 아니다. 인접 프레임에서 경계 상자를 사용할 수 없는 경우(즉, 궤적이 𝑇보다 짧거나 세그먼트 전체에 걸쳐 연속되지 않는 경우), 전체 이미지를 상자로 채운다. 저자는 초기 학습률 1 × 10-2로 20 epochs 동안 모든 모델을 처음부터 훈련하는데, 여기서 특정 epoch에 학습률 감소하는 방법을 사용하여 10번째 및 15번째 epochs에서 학습률을 10배 감소시킨다. 저자는 0.9의 모멘텀과 10-7의 가중치 감소를 가진 동기화된 SGD를 사용하여 모델을 최적화한다. 메모리 제한으로 인해 배치 크기를 112로 설정한 전체 모델을 제외하고 배치 크기가 128(i.e. GPU당 16개)인 8개의 NVIDIA Tesla V100 GPU로 각 3D video mode을 교육한다. 또한 배치 크기가 128인 단일 V100으로 2D 모델을 훈련한다.  교육 중에 SlowFast의 전략에 따라 비디오의 짧은 쪽을 픽셀 단위로 무작위로 스케일링한 다음 무작위 수평 플립 및 무작위 자르기를 224 x 224 픽셀로 조정한다. 추론하는 동안, 비디오 세그먼트의 짧은 쪽의 크기만 224픽셀로 조정한다.​4.3 Quantitative Results  a) 2D HOI 방법의 시간 인식 기능 부족, b) 일반적인 3D HOI 방법의 기능 불일치 문제, c) VideoHOI 벤치마크 부족을 다루는 것을 목표로 하기 때문에, 저자는 주로 2D 모델과 VidHOI의 naive 3D 변형과 비교하여 ST-HOI가 이러한 문제를 효과적으로 해결하는지 이해한다.  전체 ST-HOI 모델(Ours-T+V+P)과 기준선(2D 모델, 3D 모델) 간의 성능 비교는 Table 2에 제시되어 있으며, 여기서 궤적 특징(T), 정확한 지역화된 시각적 특징(V) 및 공간-시간적 마스킹 포즈 특징(P)을 포함한 서로 다른 특징(모듈)에 대한 절제 연구도 제시한다. Table 2는 두 평가 모드의 모든 설정에서 3D 모델이 2D 모델에 비해 (전체적으로 ~2%) 미미한 개선만 있음을 보여준다. 대조적으로 궤적 기능(Ours-T)을 추가하면 Oracle 모드에서 23% 또는 탐지 모드에서 15%가 훨씬 더 크게 향상되어 올바른 공간-시간 정보의 중요성을 보여준다. 저자는 또한 추가적인 시간 인식 기능(즉, V 및 P)을 추가함으로써 점점 더 높은 mAP가 달성되고, 전체 모델(Ours-T+V+P)은 오라클 모드에서 최고의 mAP를 보고하여 최대 ~25%의 상대적 개선을 달성한다는 것을 발견했다. 우리는 오라클 설정에서 Ours-T+V의 성능이 Ours-T의 성능에 가깝다는 것을 알아차렸는데, 이는 ground truth 궤적(T)이 올바른 기능이 크게 도움이 되지 않을 정도로 ""정확하게 현지화된"" 정보를 충분히 제공했기 때문일 수 있다. 저자는 또한 검출 모드에서 Ours-T+P의 성능이 Ours-T+V+P의 성능보다 약간 더 높다는 것에 주목하는데, 이는 앞서 언급한 동일한 이유와 예측된 궤적으로 인한 성능 저하 때문일 수 있다. 탐지 모델과 오라클 모델 간의 전반적인 성능 차이가 상당하여 궤적 생성을 개선할 여지가 있다. 또 다른 흥미로운 관찰은 Full mAP가 특히 Oracle 모드에서 Rare mAP에 매우 가깝다는 것으로 HOI에 대한 long-tail 효과가 강하다는 것을 보여준다.(그러나 일반적이고 자연스럽다.)  개별 속성에 대한 시간적 특징의 영향을 이해하기 위해 Figure 5에 나와 있는 속성별 AP(pAP)와 비교한다. 다시, 저자는 대부분의 상황에서 2D 백본을 3D 백본으로 순진하게 교체하는 것이 비디오 HOI 탐지에 도움이 되지 않는다는 것을 관찰한다. 시간적 속성(예: 방향, 멀어짐, 당겨짐)와 공간적 속성(예: 다음_뒤, 아래) 모두 ST-HOI의 추가 시간 인식 기능으로부터 이익을 얻는다. 이러한 발견은 궤적 및 궤적 기반 기능의 필수 사용에 대한 우리의 주요 아이디어를 검증한다. 게다가, 각각의 추가적인 특징들은 다른 속성들에 대해 동일하게 기여하지 않는 것처럼 보인다. 예를 들어, Ours-T+V+P가 일부 속성(예: 뒤와 아래)에서 가장 잘 수행되는 반면, 우리의 하위 모델은 다른 속성(예: 시청 및 승차)에서 가장 높은 mAP를 달성한다는 것을 알 수 있다. 이는 속성별 성능이 예시의 수에 따라 크게 좌우되기 때문으로 추정되며, 여기서 주요 속성는 부 속성보다 10-10000배 더 많은 예를 갖는다(그림 4 참조).  HOI 예제의 대부분은 공간과 관련이 있기 때문에(~95%), 위의 결과는 제안된 모델의 시간적 모델링 능력을 입증하는 데 적합하지 않을 수 있다. 따라서 저자는 Figure 6의 시간 관련 속성에 대한 성능에만 초점을 맞추는데, 이는 Ours-T+V+P가 최상위 빈번한 시간 속성에 대한 기준선을 크게 능가한다는 것을 보여준다. Table 3은 공간 또는 시간 전용 속성의 삼중항 mAP를 보여주며, Ours-T는 Oracle 모드에서 3D 모델의 -7.1%와 대조적으로 시간 전용 mAP에서 2D 모델을 상대적으로 +73.9% 향상시킨다.Table 2의 관찰과 유사하게, Ours-T는 시간 전용 속성에 대해 Ours-T+V+P와 동등하게 수행하지만, 공간 전용 속성에는 미치지 못해 공간/자세 정보가 여전히 공간 속성를 탐지하는 데 필수적이라는 것을 보여준다. 전반적으로, 이러한 결과는 연구 접근 방식의 뛰어난 공간-시간 모델링 능력을 보여준다.  저자는 또한 Figure 7의 일부 HOI 세 쌍둥이와 관련된 성능을 비교한다. 속성별 mAP의 결과와 유사하게, 또한 naive 2D/3D 모델과 시간적 특징을 가진 우리의 모델 사이의 큰 격차를 관찰한다. ST-HOI 변형은 특히 시간 인식 HOI(hug/lean_onperson and push/pull-baby_walker)를 예측하는 데 더 정확하다. 저자는 또한 일부 예시에서 Ours-T+V+P가 모든 변형 중에서 최고의 성능을 발휘하지 않는다는 것을 알 수 있다.(e.g, lean_on-person)은 Figure 5에서 관찰한 현상과 유사하다.​4.4 Qualitative Results   제안된 방법의 효과를 이해하기 위해, Figure 8 처럼 2D 모델과 Ours-T+V+P(둘 다 Oracle 모드)에 의해 예측된 VidHOI의 두 가지 비디오 HOI 예를 시각화한다. 각 (upper and lower) 예는 HOI 예측 테이블이 있는 5초 비디오 세그먼트(즉, 5개의 keyframe)이며, 각 항목은 두 모델 모두에 대해 True Positive(TP), False Positive(FP), False Negative(FN) 또는 True Negative(TN)를 의미한다. 위의 예는 2D 모델과 비교하여 Ours-T+V+P가 𝑇4 및 𝑇5에서 hold_hand_of를 성공적으로 예측하여 보다 정확한 HOI 탐지를 수행한다는 것을 보여준다. 더욱이, Ours-T+V+P는 하위 예에서 𝑇1에서의 리프트와 같이 시간적 정보를 필요로 하는 상호 작용을 예측할 수 있다. 그러나, 다음 𝑇2~𝑇4 프레임에서 리프트가 감지되지 않는 동일한 예에서 Ours-T+V+P에 대한 개선의 여지가 여전히 있음을 알 수 있다. 전체적으로, 모델은 데이터 세트 전체에서 잘못된 positives을 줄였으며, 이는 결국 mAP와 pAP의 증가에 기여한다.​​​2023.01.28 "
[Object Detection] validation loss fluctuating ,https://blog.naver.com/da1396/222906567859,20221021,"현재 상황 1. val loss fluctuating issue 발생epoch 조정 : 40 -> 10 -> 10train/val set 데이터 재구성 후 훈련(1회) 1) epoch40 2) 1)모델을 epoch 10 학습 3) train/val set 재구성 후 훈련2. 인식 및 추적이전보다 조금 더 개선 되었지만, 거의 안 되었다고 보면 된다.id switching의 빈도는 줄었지만, fragmentation 시간이 길어지면 새로운 ID를 부여한다. 이렇게 life cycle이 이렇게 짧나 싶을 정도.​​ 앞으로의 해결 방향 모델을 가져다 쓰는 경우에는 구글링과 fine tunning 밖에 답이 없다고 하셨다.다양한 데이터 샘플을 구성하여 계속 학습시킨다. + cross validation / version 구분 필요 : 데이터셋을 나눌 때 7:3 혹은 8:2 두 개로 해보기데이터 스플릿할 때 랜덤하게 여러 개를 구성 : 1,2의 trial-error 과정이 필요함위의 과정에도 해결되지 않은 문제라면 데이터셋 문제가 아니라는 건 확실해진다.파라미터 조절 "
Object detection(3) _Two-stage 2D object detectors: SPPNet[TPAMI'15] ,https://blog.naver.com/summa911/222988454299,20230119,"R- CNN의 한계점​1) 이미지 하나당 2000번의 CNN 과정이 필요하다. 2) input data를 warping 해야한다.  ​--> CNN을 한 번만 학습시킬 수 있다면 시간을 줄일 수 있음. --> 임의 사이즈 데이터를 input 할 수 있다면 wrapping을 안해도 됨. --> accuracy증가 SPPNet​​1) BB on a CNN feature map​원본 이미지에서 bounding box를 찾는 것이 아니라,  CNN feature map(before FC layers) 상태에서 box를 찾는다.  원본 이미지 상에서 selective search방법으로 유의미한 box들을 자르지 않고 저장만 해둔다. box정보가 저장된 전체 이미지를  한 번 CNN 학습한번의 CNN학습을 거친 feature vector(before FC layers)에서 bounding box를 잘라낸다.​트레이닝이 R-CNN보다 3배나 빨라지고, 10~100배 빨라진 inference를 보인다. ​마지막 FC를 거치기 전에는 warpping과 같이 size를 조절해주는 작업을 해줘야 한다.SPPNet은 warpping 대신에 Spatial pyramid pooling(SPP) 이라는 층을 거친다. ​​2) SPP(Spatial Pyramid Pooling) layer for scale invariance​ SPP layer는 input 데이터를 원하는 사이즈의 spatial bins로 쪼갠다. 그리고 max pooling같은 방법을 거쳐서 또 다른 사이즈의 데이터로 resizing을 한다.다양한 사이즈의 데이터를 flatten 후, concat 하면 FC가 만들어 진다.bean을 나눠서 pooling하는 것이 데이터를 warping하는 것보다 효과적이라고 한다. spatial pyramid pooling인 이유는, bean을 나눠서 pooling하는 과정을 쌓아가면서 점진적으로 사이즈를 조절했다는 이야기이다. ​ SPPNetCNN 이후에 box를 쪼갠다는 아이디어, bean갯수를 fix하는 것이 성능향상의 요인!이미지 사이즈에 구애받지 않고 CNN사용 가능. ​ [출처] Kaiming He et al., ""Spatial pyramid pooling in deep convolutional networks for visual recognition."" https://arxiv.org/abs/1406.4729 Spatial Pyramid Pooling in Deep Convolutional Networks for Visual RecognitionExisting deep convolutional neural networks (CNNs) require a fixed-size (e.g., 224x224) input image. This requirement is ""artificial"" and may reduce the recognition accuracy for the images or sub-images of an arbitrary size/scale. In this work, we equip the networks with another pooling strategy, ""s...arxiv.org [출처] https://youtu.be/yGZVibqaPJY​ "
[인공지능] 객체 탐지(Object Detection) with YOLO & R-CNN ,https://blog.naver.com/chrisjae508/221968909960,20200518,"현재 이미지에서 객체를 탐지하기 위한 많은 연구는 정말 많이 이루어지고 있다. 이는 여러 분야에 적용이 되는 기술인데 실생활 예로 자동차 번호판 검출(LPR : license plate recognition)을 들 수 있다. 카메라의 실시간 촬영으로 얻은 이미지나 영상으로 차량의 크기, 각도, 조명, 주변 환경 등에 제한을 받지 않고, 빠르고 정확하게 번호판을 추출하고 문자를 인식하는 것이 LPR 성능의 척도이며 번호판 검출의 핵심 과제이다.   https://bigdata-madesimple.com/wp-content/uploads/2019/02/numberplate.jpg(사진 출처)​ 객체를 탐지하는 기존의 기법들에는 여러 가지가 있는데 이 중에는 인공지능을 기반으로 객체 탐지(Object detection) 방법이 매우 효율적이다. 인공지능 기반인 CNN(convolutional neuron network)을 이용한 방법으로는 R-CNN, fast R-CNN, faster R-CNN, YOLO, YOLOv2, YOLOv, SSD 등이 있다. ​​CNN 기반 객체 검출 분야는 one stage method와 two stage method 2가지로 나눠져 있다. Two stage method가 먼저 나왔고 최근에 one stage method가 나왔으며 one stage method가 더 깔끔하고 구현도 쉽다. 그래서 실무에서 속도가 중요하므로 one stage method를 더 많이 쓴다고 한다. 특히, 자율 주행 자동차같이 실시간으로 예측이 필요한 영역은 실행 속도가 중요하여 one stage method를 많이 쓰고 현재도 계속 발전 중이라고 한다. 참고로 Two stage method인 이유는 쉽게 말해  기존 알고리즘으로 먼저 찾고, 딥러닝으로 다시 찾는 두 가지 단계로 이루어져 있기 때문이다.​​One stage Method ex) YOLO, SSD속도가 빠름Two stage Method ex) R-CNN, fast R-CNN, faster R-CNN정확도가 좋음​​최근에 발표된 YOLO v3 알고리즘은 현재 학계와 산업 분야 둘 다에서 가장 빠르고 효율적인, CNN을 기반으로 한 객체 감지 기법 중 하나로 여겨지고 있다. Fast R-CNN이나 Faster R-CNN보다 훨씬 높은 FPS(frames per second) 수치를 가진다. Yolo v3는 사용이 용이하고 객체 인식 성능이 뛰어나 비교적 빠른 처리시간이 필요한 real time  시스템에 최적화되어 있다. 기존 R-CNN이 느린 이유는 Proposal 수가 많아 그로 인한 오버헤드가 많기 때문에 영상이나 이미지를 처리함에 있어 상당히 긴 시간이 소요된다. Yolo v3는 이러한 문제를 개선하기 위해 만들어졌다. 이는 one stage method으로서 처리 과정이 타 CNN 알고리즘에 비해 간단하고 그리드(Grid) 방식으로 구분(Classification)이 이루어진다. 따라서 높은 mAP(mean Average Precision)을 보인다. 참고로 mAP는 쉽게 말해 예측 정확도를 의미한다. 하지만 기존 타 CNN 알고리즘에 비해 정확도 면에서 떨어진다는 단점이 존재한다. 예를 들어 멀리 있거나 작은 물체는 잘 찾지 못한다. 따라서 YOLO와 R-CNN 둘 다 장단점이 존재한다.​​​<Reference> https://pjreddie.com/darknet/yolo/ YOLO: Real-Time Object DetectionYOLO: Real-Time Object Detection You only look once (YOLO) is a state-of-the-art, real-time object detection system. On a Pascal Titan X it processes images at 30 FPS and has a mAP of 57.9% on COCO test-dev. Comparison to Other Detectors YOLOv3 is extremely fast and accurate. In mAP measured at .5 I...pjreddie.com J. Redmon, S. Divvala, R. Girshick, and A. Farhadi, “You only look once: Unified, real-time object detection,” in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.​J. Redmon and A. Farhadi, “YOLO9000: Better, faster, stronger,” in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.​J. Redmon and A. Farhadi, “YOLOv3: An incremental improvement,” CoRR, vol. abs/1804.02767, 2018.​Ross Girshick. Fast r-cnn. In Proceedings of the IEEE International Conference on Computer Vision, pages 1440–1448, 2015. ​S. Ren, K. He, R. Girshick, and J. Sun. Faster R-CNN: Towards realtime object detection with region proposal networks. IEEE Trans. Pattern Anal. Mach. Intell., 2016​​​​​​​​​ ​  ​  ​ "
1. TensorFlow-Lite-Object-Detection-on-Android-and-Raspberry-Pi ,https://blog.naver.com/sameallday/222505787843,20210914,"​출처는 연결된 깃허브에 있습니다.​TensorFlow-Lite-Object-Detection-on-Android-and-Raspberry-PiA guide showing how to train TensorFlow Lite object detection models and run them on Android, the Raspberry Pi, and more! IntroductionTensorFlow Lite is an optimized framework for deploying lightweight deep learning models on resource-constrained edge devices. TensorFlow Lite models have faster inference time and require less processing power, so they can be used to obtain faster performance in realtime applications. This guide provides step-by-step instructions for how train a custom TensorFlow Object Detection model, convert it into an optimized format that can be used by TensorFlow Lite, and run it on Android phones or the Raspberry Pi.The guide is broken into three major portions. Each portion will have its own dedicated README file in this repository.How to Train, Convert, and Run Custom TensorFlow Lite Object Detection Models on Windows 10 <--- You are here!How to Run TensorFlow Lite Object Detection Models on the Raspberry Pi (with optional Coral USB Accelerator)How to Run TensorFlow Lite Object Detection Models on Android Devices (Still not complete)This repository also contains Python code for running the newly converted TensorFlow Lite model to perform detection on images, videos, or webcam feeds.A Note on VersionsI used TensorFlow v1.13 while creating this guide, because TF v1.13 is a stable version that has great support from Anaconda. I will periodically update the guide to make sure it works with newer versions of TensorFlow.The TensorFlow team is always hard at work releasing updated versions of TensorFlow. I recommend picking one version and sticking with it for all your TensorFlow projects. Every part of this guide should work with newer or older versions, but you may need to use different versions of the tools needed to run or build TensorFlow (CUDA, cuDNN, bazel, etc). Google has provided a list of build configurations for Linux, macOS, and Windows that show which tool versions were used to build and run each version of TensorFlow.Part 1 - How to Train, Convert, and Run Custom TensorFlow Lite Object Detection Models on Windows 10Part 1 of this guide gives instructions for training and deploying your own custom TensorFlow Lite object detection model on a Windows 10 PC. The guide is based off the tutorial in the TensorFlow Object Detection repository, but it gives more detailed instructions and is written specifically for Windows. (It will work on Linux too with some minor changes, which I leave as an exercise for the Linux user.)There are three primary steps to training and deploying a TensorFlow Lite model:Train a quantized SSD-MobileNet model using TensorFlow, and export frozen graph for TensorFlow LiteBuild TensorFlow from source on your PCUse TensorFlow Lite Optimizing Converter (TOCO) to create optimzed TensorFlow Lite modelThis portion is a continuation of my previous guide: How To Train an Object Detection Model Using TensorFlow on Windows 10. I'll assume you have already set up TensorFlow to train a custom object detection model as described in that guide, including:Setting up an Anaconda virtual environment for trainingSetting up TensorFlow directory structureGathering and labeling training imagesPreparing training data (generating TFRecords and label map)This tutorial uses the same Anaconda virtual environment, files, and directory structure that was set up in the previous one.Through the course of the guide, I'll use a bird, squirrel, and raccoon detector model I've been working on as an example. The intent of this detection model is to watch a bird feeder, and record videos of birds while triggering an alarm if a squirrel or raccoon is stealing from it! I'll show the steps needed to train, convert, and run a quantized TensorFlow Lite version of the bird/squirrel/raccoon detector.Parts 2 and 3 of this guide will go on to show how to deploy this newly trained TensorFlow Lite model on the Raspberry Pi or an Android device. If you're not feeling up to training and converting your own TensorFlow Lite model, you can skip Part 1 and use my custom-trained TFLite BSR detection model (which you can download from Dropbox here) or use the TF Lite starter detection model (taken from https://www.tensorflow.org/lite/models/object_detection/overview) for Part 2 or Part 3.Step 1: Train Quantized SSD-MobileNet Model and Export Frozen TensorFlow Lite GraphFirst, we’ll use transfer learning to train a “quantized” SSD-MobileNet model. Quantized models use 8-bit integer values instead of 32-bit floating values within the neural network, allowing them to run much more efficiently on GPUs or specialized TPUs (TensorFlow Processing Units).You can also use a standard SSD-MobileNet model (V1 or V2), but it will not run quite as fast as the quantized model. Also, you will not be able to run it on the Google Coral TPU Accelerator. If you’re using an SSD-MobileNet model that has already been trained, you can skip to Step 1d of this guide.If you get any errors during this process, please look at the FAQ section at the bottom of this guide! It gives solutions to common errors that occur.As I mentioned prevoiusly, this guide assumes you have already followed my previous TensorFlow tutorial and set up the Anaconda virtual environment and full directory structure needed for using the TensorFlow Object Detection API. If you've done so, you should have a folder at C:\tensorflow1\models\research\object_detection that has everything needed for training. (If you used a different base folder name than ""tensorflow1"", that's fine - just make sure you continue to use that name throughout this guide.)Here's what your \object_detection folder should look like: If you don't have this folder, please go to my previous tutorial and work through at least Steps 1 and 2. If you'd like to train your own model to detect custom objects, you'll also need to work through Steps 3, 4, and 5. If you don't want to train your own model but want to practice the process for converting a model to TensorFlow Lite, you can download the quantized MobileNet-SSD model (see next paragraph) and then skip to Step 1d.Step 1a. Download and extract quantized SSD-MobileNet modelGoogle provides several quantized object detection models in their detection model zoo. This tutorial will use the SSD-MobileNet-V2-Quantized-COCO model. Download the model here. Note: TensorFlow Lite does NOT support RCNN models such as Faster-RCNN! It only supports SSD models.Move the downloaded .tar.gz file to the C:\tensorflow1\models\research\object_detection folder. (Henceforth, this folder will be referred to as the “\object_detection” folder.) Unzip the .tar.gz file using a file archiver like WinZip or 7-Zip. After the file has been fully unzipped, you should have a folder called ""ssd_mobilenet_v2_quantized_300x300_coco_2019_01_03"" within the \object_detection folder.Step 1b. Configure trainingIf you're training your own TensorFlow Lite model, make sure the following items from my previous guide have been completed:Train and test images and their XML label files are placed in the \object_detection\images\train and \object_detection\images\test folderstrain_labels.csv and test_labels.csv have been generated and are located in the \object_detection\images foldertrain.record and test.record have been generated and are located in the \object_detection folderlabelmap.pbtxt file has been created and is located in the \object_detection\training folderproto files in \object_detection\protos have been generatedIf you have any questions about these files or don’t know how to generate them, Steps 2, 3, 4, and 5 of my previous tutorial show how they are all created.Copy the ssd_mobilenet_v2_quantized_300x300_coco.config file from the \object_detection\samples\configs folder to the \object_detection\training folder. Then, open the file using a text editor.Make the following changes to the ssd_mobilenet_v2_quantized_300x300_coco.config file. Note: The paths must be entered with single forward slashes (NOT backslashes), or TensorFlow will give a file path error when trying to train the model! Also, the paths must be in double quotation marks ( "" ), not single quotation marks ( ' ).Line 9. Change num_classes to the number of different objects you want the classifier to detect. For my bird/squirrel/raccoon detector example, there are three classes, so I set num_classes: 3Line 141. Change batch_size: 24 to batch_size: 6 . The smaller batch size will prevent OOM (Out of Memory) errors during training.Line 156. Change fine_tune_checkpoint to: ""C:/tensorflow1/models/research/object_detection/ ssd_mobilenet_v2_quantized_300x300_coco_2019_01_03/model.ckpt""Line 175. Change input_path to: ""C:/tensorflow1/models/research/object_detection/train.record""Line 177. Change label_map_path to: ""C:/tensorflow1/models/research/object_detection/training/labelmap.pbtxt""Line 181. Change num_examples to the number of images you have in the \images\test directory. For my bird/squirrel/raccoon detector example, there are 582 test images, so I set num_examples: 582.Line 189. Change input_path to: ""C:/tensorflow1/models/research/object_detection/test.record""Line 191. Change label_map_path to: ""C:/tensorflow1/models/research/object_detection/training/labelmap.pbtxt""Save and exit the training file after the changes have been made.Step 1c. Run training in Anaconda virtual environmentAll that's left to do is train the model! First, move the “train.py” file from the \object_detection\legacy folder into the main \object_detection folder. (See the FAQ for why I am using the legacy train.py script rather than model_main.py for training.)Then, open a new Anaconda Prompt window by searching for “Anaconda Prompt” in the Start menu and clicking on it. Activate the “tensorflow1” virtual environment (which was set up in my previous tutorial) by issuing:activate tensorflow1 Then, set the PYTHONPATH environment variable by issuing:set PYTHONPATH=C:\tensorflow1\models;C:\tensorflow1\models\research;C:\tensorflow1\models\research\slim Next, change directories to the \object_detection folder:cd C:\tensorflow1\models\research\object_detection Finally, train the model by issuing:python train.py --logtostderr –train_dir=training/ --pipeline_config_path=training/ssd_mobilenet_v2_quantized_300x300_coco.config If everything was set up correctly, the model will begin training after a couple minutes of initialization. Allow the model to train until the loss consistently drops below 2. For my bird/squirrel/raccoon detector model, this took about 9000 steps, or 8 hours of training. (Time will vary depending on how powerful your CPU and GPU are. Please see Step 6 of my previous tutorial for more information on training and an explanation of how to view the progress of the training job using TensorBoard.)Once training is complete (i.e. the loss has consistently dropped below 2), press Ctrl+C to stop training. The latest checkpoint will be saved in the \object_detection\training folder, and we will use that checkpoint to export the frozen TensorFlow Lite graph. Take note of the checkpoint number of the model.ckpt file in the training folder (i.e. model.ckpt-XXXX), as it will be used later.Step 1d. Export frozen inference graph for TensorFlow LiteNow that training has finished, the model can be exported for conversion to TensorFlow Lite using the export_tflite_ssd_graph.py script. First, create a folder in \object_detection called “TFLite_model” by issuing:mkdir TFLite_model Next, let’s set up some environment variables so the commands are easier to type out. Issue the following commands in Anaconda Prompt. (Note, the XXXX in the second command should be replaced with the highest-numbered model.ckpt file in the \object_detection\training folder.)set CONFIG_FILE=C:\\tensorflow1\models\research\object_detection\training\ssd_mobilenet_v2_quantized_300x300_coco.config set CHECKPOINT_PATH=C:\\tensorflow1\models\research\object_detection\training\model.ckpt-XXXX set OUTPUT_DIR=C:\\tensorflow1\models\research\object_detection\TFLite_model Now that those are set up, issue this command to export the model for TensorFlow Lite:python export_tflite_ssd_graph.py --pipeline_config_path=%CONFIG_FILE% --trained_checkpoint_prefix=%CHECKPOINT_PATH% --output_directory=%OUTPUT_DIR% --add_postprocessing_op=true After the command has executed, there should be two new files in the \object_detection\TFLite_model folder: tflite_graph.pb and tflite_graph.pbtxt.That’s it! The new inference graph has been trained and exported. This inference graph's architecture and network operations are compatible with TensorFlow Lite's framework. However, the graph still needs to be converted to an actual TensorFlow Lite model. We'll do that in Step 3. First, we have to build TensorFlow from source. On to Step 2!Step 2. Build TensorFlow From SourceTo convert the frozen graph we just exported into a model that can be used by TensorFlow Lite, it has to be run through the TensorFlow Lite Optimizing Converter (TOCO). Unfortunately, to use TOCO, we have to build TensorFlow from source on our computer. To do this, we’ll create a separate Anaconda virtual environment for building TensorFlow.This part of the tutorial breaks down step-by-step how to build TensorFlow from source on your Windows PC. It follows the Build TensorFlow From Source on Windows instructions given on the official TensorFlow website, with some slight modifications.This guide will show how to build either the CPU-only version of TensorFlow or the GPU-enabled version of TensorFlow v1.13. If you would like to build a version other than TF v1.13, you can still use this guide, but check the build configuration list and make sure you use the correct package versions.If you are only building TensorFlow to convert a TensorFlow Lite object detection model, I recommend building the CPU-only version! It takes very little computational effort to export the model, so your CPU can do it just fine without help from your GPU. If you’d like to build the GPU-enabled version anyway, then you need to have the appropriate version of CUDA and cuDNN installed. The TensorFlow installation guide explains how to install CUDA and cuDNN. Check the build configuration list to see which versions of CUDA and cuDNN are compatible with which versions of TensorFlow.If you get any errors during this process, please look at the FAQ section at the bottom of this guide! It gives solutions to common errors that occur.Step 2a. Install MSYS2MSYS2 has some binary tools needed for building TensorFlow. It also automatically converts Windows-style directory paths to Linux-style paths when using Bazel. The Bazel build won’t work without MSYS2 installed!First, install MSYS2 by following the instructions on the MSYS2 website. Download the msys2-x86_64 executable file and run it. Use the default options for installation. After installing, open MSYS2 and issue:pacman -Syu After it's completed, close the window, re-open it, and then issue the following two commands:pacman -Su pacman -S patch unzip  This updates MSYS2’s package manager and downloads the patch and unzip packages. Now, close the MSYS2 window. We'll add the MSYS2 binary to the PATH environment variable in Step 2c.Step 2b. Install Visual C++ Build Tools 2015Install Microsoft Build Tools 2015 and Microsoft Visual C++ 2015 Redistributable by visiting the Visual Studio older downloads page. Click the “Redistributables and Build Tools” dropdown at the bottom of the list. Download and install the following two packages:Microsoft Build Tools 2015 Update 3 - Use the default installation options in the install wizard. Once you begin installing, it goes through a fairly large download, so it will take a while if you have a slow internet connection. It may give you some warnings saying build tools or redistributables have already been installed. If so, that's fine; just click through them.Microsoft Visual C++ 2015 Redistributable Update 3 – This may give you an error saying the redistributable has already been installed. If so, that’s fine.Restart your PC after installation has finished.Step 2c. Update Anaconda and create tensorflow-build environmentNow that the Visual Studio tools are installed and your PC is freshly restarted, open a new Anaconda Prompt window. First, update Anaconda to make sure its package list is up to date. In the Anaconda Prompt window, issue these two commands:conda update -n base -c defaults conda conda update --all The update process may take up to an hour, depending on how it's been since you installed or updated Anaconda. Next, create a new Anaconda virtual environment called “tensorflow-build”. We’ll work in this environment for the rest of the build process. Create and activate the environment by issuing:conda create -n tensorflow-build pip python=3.6 conda activate tensorflow-build After the environment is activated, you should see (tensorflow-build) before the active path in the command window.Update pip by issuing:python -m pip install --upgrade pip We'll use Anaconda's git package to download the TensorFlow repository, so install git using:conda install -c anaconda git Next, add the MSYS2 binaries to this environment's PATH variable by issuing:set PATH=%PATH%;C:\msys64\usr\bin (If MSYS2 is installed in a different location than C:\msys64, use that location instead.) You’ll have to re-issue this PATH command if you ever close and re-open the Anaconda Prompt window.Step 2d. Download Bazel and Python package dependenciesNext, we’ll install Bazel and some other Python packages that are used for building TensorFlow. Install the necessary Python packages by issuing:pip install six numpy wheel pip install keras_applications==1.0.6 --no-deps pip install keras_preprocessing==1.0.5 --no-deps Then install Bazel v0.21.0 by issuing the following command. (If you are building a version of TensorFlow other than v1.13, you may need to use a different version of Bazel.)conda install -c conda-forge bazel=0.21.0 Step 2d. Download TensorFlow source and configure buildTime to download TensorFlow’s source code from GitHub! Issue the following commands to create a new folder directly in C:\ called “tensorflow-build” and cd into it:mkdir C:\tensorflow-build cd C:\tensorflow-build Then, clone the TensorFlow repository and cd into it by issuing:git clone https://github.com/tensorflow/tensorflow.git cd tensorflow Next, check out the branch for TensorFlow v1.13:git checkout r1.13 The version you check out should match the TensorFlow version you used to train your model in Step 1. If you used a different version than TF v1.13, then replace ""1.13"" with the version you used. See the FAQs section for instructions on how to check the TensorFlow version you used for training.Next, we’ll configure the TensorFlow build using the configure.py script. From the C:\tensorflow-build\tensorflow directory, issue:python ./configure.py This will initiate a Bazel session. As I mentioned before, you can build either the CPU-only version of TensorFlow or the GPU-enabled version of TensorFlow. If you're only using this TensorFlow build to convert your TensorFlow Lite model, I recommend building the CPU-only version. If you’d still like to build the GPU-enabled version for some other reason, then you need to have the appropriate version of CUDA and cuDNN installed. This guide doesn't cover building the GPU-enabled version of TensorFlow, but you can try following the official build instructions on the TensorFlow website.Here’s what the configuration session will look like if you are building for CPU only. Basically, press Enter to select the default option for each question.You have bazel 0.21.0- (@non-git) installed. Please specify the location of python. [Default is C:\ProgramData\Anaconda3\envs\tensorflow-build\python.exe]: Found possible Python library paths: C:\ProgramData\Anaconda3\envs\tensorflow-build\lib\site-packages Please input the desired Python library path to use. Default is [C:\ProgramData\Anaconda3\envs\tensorflow-build\lib\site-packages] Do you wish to build TensorFlow with XLA JIT support? [y/N]: N No XLA JIT support will be enabled for TensorFlow. Do you wish to build TensorFlow with ROCm support? [y/N]: N No ROCm support will be enabled for TensorFlow. Do you wish to build TensorFlow with CUDA support? [y/N]: N No CUDA support will be enabled for TensorFlow. Once the configuration is finished, TensorFlow is ready to be bulit!Step 2e. Build TensorFlow packageNext, use Bazel to create the package builder for TensorFlow. To create the CPU-only version, issue the following command. The build process took about 70 minutes on my computer.bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package Now that the package builder has been created, let’s use it to build the actual TensorFlow wheel file. Issue the following command (it took about 5 minutes to complete on my computer):bazel-bin\tensorflow\tools\pip_package\build_pip_package C:/tmp/tensorflow_pkg This creates the wheel file and places it in C:\tmp\tensorflow_pkg.Step 2f. Install TensorFlow and test it out!TensorFlow is finally ready to be installed! Open File Explorer and browse to the C:\tmp\tensorflow_pkg folder. Copy the full filename of the .whl file, and paste it in the following command:pip3 install C:/tmp/tensorflow_pkg/<Paste full .whl filename here> That's it! TensorFlow is installed! Let's make sure it installed correctly by opening a Python shell:python Once the shell is opened, issue these commands:>>> import tensorflow as tf >>> tf.__version__ If everything was installed properly, it will respond with the installed version of TensorFlow. Note: You may get some deprecation warnings after the ""import tensorflow as tf"" command. As long as they are warnings and not actual errors, you can ignore them! Exit the shell by issuing:exit() With TensorFlow installed, we can finally convert our trained model into a TensorFlow Lite model. On to the last step: Step 3!Step 3. Use TOCO to Create Optimzed TensorFlow Lite Model, Create Label Map, Run ModelAlthough we've already exported a frozen graph of our detection model for TensorFlow Lite, we still need run it through the TensorFlow Lite Optimizing Converter (TOCO) before it will work with the TensorFlow Lite interpreter. TOCO converts models into an optimized FlatBuffer format that allows them to run efficiently on TensorFlow Lite. We also need to create a new label map before running the model.Step 3a. Create optimized TensorFlow Lite modelFirst, we’ll run the model through TOCO to create an optimzed TensorFLow Lite model. The TOCO tool lives deep in the C:\tensorflow-build directory, and it will be run from the “tensorflow-build” Anaconda virtual environment that we created and used during Step 2. Meanwhile, the model we trained in Step 1 lives inside the C:\tensorflow1\models\research\object_detection\TFLite_model directory. We’ll create an environment variable called OUTPUT_DIR that points at the correct model directory to make it easier to enter the TOCO command.If you don't already have an Anaconda Prompt window open with the ""tensorflow-build"" environment active and working in C:\tensorflow-build, open a new Anaconda Prompt window and issue:activate tensorflow-build cd C:\tensorflow-build Create the OUTPUT_DIR environment variable by issuing:set OUTPUT_DIR=C:\\tensorflow1\models\research\object_detection\TFLite_model Next, use Bazel to run the model through the TOCO tool by issuing this command:bazel run --config=opt tensorflow/lite/toco:toco -- --input_file=%OUTPUT_DIR%/tflite_graph.pb --output_file=%OUTPUT_DIR%/detect.tflite --input_shapes=1,300,300,3 --input_arrays=normalized_input_image_tensor --output_arrays=TFLite_Detection_PostProcess,TFLite_Detection_PostProcess:1,TFLite_Detection_PostProcess:2,TFLite_Detection_PostProcess:3 --inference_type=QUANTIZED_UINT8 --mean_values=128 --std_values=128 --change_concat_input_ranges=false --allow_custom_ops Note: If you are using a floating, non-quantized SSD model (e.g. the ssdlite_mobilenet_v2_coco model rather than the ssd_mobilenet_v2_quantized_coco model), the Bazel TOCO command must be modified slightly:bazel run --config=opt tensorflow/lite/toco:toco -- --input_file=$OUTPUT_DIR/tflite_graph.pb --output_file=$OUTPUT_DIR/detect.tflite --input_shapes=1,300,300,3 --input_arrays=normalized_input_image_tensor --output_arrays=TFLite_Detection_PostProcess,TFLite_Detection_PostProcess:1,TFLite_Detection_PostProcess:2,TFLite_Detection_PostProcess:3 --inference_type=FLOAT --allow_custom_ops If you are using Linux, make sure to use the commands given in the official TensorFlow instructions here. I removed the ' characters from the command, because for some reason they cause errors on Windows!After the command finishes running, you should see a file called detect.tflite in the \object_detection\TFLite_model directory. This is the model that can be used with TensorFlow Lite!Step 3b. Create new label mapFor some reason, TensorFlow Lite uses a different label map format than classic TensorFlow. The classic TensorFlow label map format looks like this (you can see an example in the \object_detection\data\mscoco_label_map.pbtxt file):item { name: ""/m/01g317"" id: 1 display_name: ""person"" } item { name: ""/m/0199g"" id: 2 display_name: ""bicycle"" } item { name: ""/m/0k4j"" id: 3 display_name: ""car"" } item { name: ""/m/04_sv"" id: 4 display_name: ""motorcycle"" } And so on... However, the label map provided with the example TensorFlow Lite object detection model looks like this:person bicycle car motorcycle And so on... Basically, rather than explicitly stating the name and ID number for each class like the classic TensorFlow label map format does, the TensorFlow Lite format just lists each class. To stay consistent with the example provided by Google, I’m going to stick with the TensorFlow Lite label map format for this guide.Thus, we need to create a new label map that matches the TensorFlow Lite style. Open a text editor and list each class in order of their class number. Then, save the file as “labelmap.txt” in the TFLite_model folder. As an example, here's what the labelmap.txt file for my bird/squirrel/raccoon detector looks like: Now we’re ready to run the model!Step 3c. Run the TensorFlow Lite model!I wrote three Python scripts to run the TensorFlow Lite object detection model on an image, video, or webcam feed: TFLite_detection_image.py, TFLite_detection_video.py, and TFLite_detection_wecam.py. The scripts are based off the label_image.py example given in the TensorFlow Lite examples GitHub repository.We’ll download the Python scripts directly from this repository. First, install wget for Anaconda by issuing:conda install -c menpo wget Once it's installed, download the scripts by issuing:wget https://raw.githubusercontent.com/EdjeElectronics/TensorFlow-Lite-Object-Detection-on-Android-and-Raspberry-Pi/master/TFLite_detection_image.py --no-check-certificate wget https://raw.githubusercontent.com/EdjeElectronics/TensorFlow-Lite-Object-Detection-on-Android-and-Raspberry-Pi/master/TFLite_detection_video.py --no-check-certificate wget https://raw.githubusercontent.com/EdjeElectronics/TensorFlow-Lite-Object-Detection-on-Android-and-Raspberry-Pi/master/TFLite_detection_webcam.py --no-check-certificate The following instructions show how to run the webcam, video, and image scripts. These instructions assume your .tflite model file and labelmap.txt file are in the “TFLite_model” folder in your \object_detection directory as per the instructions given in this guide.If you’d like try using the sample TFLite object detection model provided by Google, simply download it here and unzip it into the \object_detection folder. Then, use --modeldir=coco_ssd_mobilenet_v1_1.0_quant_2018_06_29 rather than --modeldir=TFLite_model when running the script.For more information on options that can be used while running the scripts, use the -h option when calling the script. For example:python TFLite_detection_image.py -h WebcamMake sure you have a USB webcam plugged into your computer. If you’re on a laptop with a built-in camera, you don’t need to plug in a USB webcam.From the \object_detection directory, issue:python TFLite_detection_webcam.py --modeldir=TFLite_model After a few moments of initializing, a window will appear showing the webcam feed. Detected objects will have bounding boxes and labels displayed on them in real time.Video streamTo run the script to detect images in a video stream (e.g. a remote security camera), issue:python TFLite_detection_stream.py --modeldir=TFLite_model --streamurl=""http://ipaddress:port/stream/video.mjpeg"" After a few moments of initializing, a window will appear showing the video stream. Detected objects will have bounding boxes and labels displayed on them in real time.Make sure to update the URL parameter to the one that's being used by your security camera. It has to include authentication information in case the stream is secured.If the bounding boxes are not matching the detected objects, probably the stream resolution wasn't detected. In this case you can set it explicitly by using the --resolution parameter:python TFLite_detection_stream.py --modeldir=TFLite_model --streamurl=""http://ipaddress:port/stream/video.mjpeg"" --resolution=1920x1080 VideoTo run the video detection script, issue:python TFLite_detection_image.py --modeldir=TFLite_model A window will appear showing consecutive frames from the video, with each object in the frame labeled. Press 'q' to close the window and end the script. By default, the video detection script will open a video named 'test.mp4'. To open a specific video file, use the --video option:python TFLite_detection_image.py --modeldir=TFLite_model --video='birdy.mp4' Note: Video detection will run at a slower FPS than realtime webcam detection. This is mainly because loading a frame from a video file requires more processor I/O than receiving a frame from a webcam.ImageTo run the image detection script, issue:python TFLite_detection_image.py --modeldir=TFLite_model The image will appear with all objects labeled. Press 'q' to close the image and end the script. By default, the image detection script will open an image named 'test1.jpg'. To open a specific image file, use the --image option:python TFLite_detection_image.py --modeldir=TFLite_model --image=squirrel.jpg It can also open an entire folder full of images and perform detection on each image. There can only be images files in the folder, or errors will occur. To specify which folder has images to perform detection on, use the --imagedir option:python TFLite_detection_image.py --modeldir=TFLite_model --imagedir=squirrels Press any key (other than 'q') to advance to the next image. Do not use both the --image option and the --imagedir option when running the script, or it will throw an error. If you encounter errors while running these scripts, please check the FAQ section of this guide. It has a list of common errors and their solutions. If you can successfully run the script, but your object isn’t detected, it is most likely because your model isn’t accurate enough. The FAQ has further discussion on how to resolve this.Next StepsThis concludes Part 1 of my TensorFlow Lite guide! You now have a trained TensorFlow Lite model and the scripts needed to run it on a PC.But who cares about running it on a PC? The whole reason we’re using TensorFlow Lite is so we can run our models on lightweight devices that are more portable and less power-hungry than a PC! The next two parts of my guide show how to run this TFLite model on a Raspberry Pi or an Android Device.Links to be added when these are completed!Part 2. How to Run TensorFlow Lite Object Detection Models on the Raspberry Pi (with optional Coral USB Accelerator)Part 3. How to Run TensorFlow Lite Object Detection Models on Android DevicesFrequently Asked Questions and Common ErrorsWhy does this guide use train.py rather than model_main.py for training?This guide uses ""train.py"" to run training on the TFLite detection model. The train.py script is deprecated, but the model_main.py script that replaced it doesn't log training progress by default, and it requires pycocotools to be installed. Using model_main.py requires a few extra setup steps, and I want to keep this guide as simple as possible. Since there are no major differences between train.py and model_main.py that will affect training (see TensorFlow Issue #6100), I use train.py for this guide.How do I check which TensorFlow version I used to train my detection model?Here’s how you can check the version of TensorFlow you used for training.Open a new Anaconda Prompt window and issue activate tensorflow1 (or whichever environment name you used)Open a python shell by issuing pythonWithin the Python shell, import TensorFlow by issuing import tensorflow as tfCheck the TensorFlow version by issuing tf.__version__ . It will respond with the version of TensorFlow. This is the version that you used for training.Building TensorFlow from sourceIn case you run into error error C2100: illegal indirection during TensorFlow compilation, simply edit the file tensorflow-build\tensorflow\tensorflow\core\framework\op_kernel.h, go to line 405, and change reference operator*() { return (*list_)[i_]; } to reference operator*() const { return (*list_)[i_]; }. Credits go to: https://github.com/tensorflow/tensorflow/issues/15925#issuecomment-499569928 "
"REQ-YOLO: A Resource-Aware, Efficient Quantization Framework for Object Detection on FPGAs ",https://blog.naver.com/hanpanjjang/222676079782,20220318,"REQ... 무튼 리소스 생각하면서 효율적으로 quantization한 yolo... 딱 우리 대회에 잘 맞는 제목인 것 같아서 들고왔습니다. 2019년 2월/ FPGA 19​github: https://github.com/Anonymous788/heterogeneous_ADMM_YOLO.​ DNN의 inference단계에서의 성능과 에너지 효율 향상이 목적이고그 방법으로첫 번째는 모델 압축 기술이고 두 번째는 효율적인 하드웨어 구현입니다.​​the block-circulant matrix method???a heterogeneous weight quantization using Alternative Direction Method of Multipliers (ADMM),ADMM은 non-convex optimization problems에  잘 적용되는 기술이라네요​서론 자율주행 그런 기술에서 우리가 엣지 컴퓨팅 에서 목표하듯 에너지 효율, 추론 시간, 성능 같은 요소들에 같은 문제를 가지고 해결로 prunning하는데 이건 parallelism degree와 hardware performance를 낮췄다네요. 흥미로워서 [46, 10, 51] 인용된 부분​block-circulant matrices for weight representationefficient hardware implementations -> 한마디로 하기어려운지 계속 이렇게 말하네요Conv layer는 Fully-connected layer 보다 계산할게 많다.. 이건 이미 아는 것​Binarization은 memory bandwidth와 storage requirement를 줄일 뿐만아니라전통적인 floating point operations를 binary bit operations로 대체합니다.이는 LUT(look up table) 기반인 FPGA chip에 효율적으로 들어갈 수 있다고 하네요 근데 이제 weight의 정보 손실이 커서 대규모의 데이터 셋에서는 무시할수없는 정확도 손실이 발생한다고 합니다. 하드웨어적인 낭비 (multiplier 대체로 인한 DSP 자원 낭비 등)도 있고..​무튼 그래서 자기네꺼 쓰겠다는 거임. REQ - YOLO​block-circulant matrices를 이용한 압축, 하드웨어 리소스를 고려한 FFT 결과에 대한 ADMM을 이용한heterogeneous weight quantization​OBJECT DETECTION 관련 내용 뛰고시작!​​COMPRESSED CONVOLUTION LAYERS ​Block-Circulant MatricesBlock-Circulant Matrices... 블록 순환행렬..?원래의 가중치 행렬을 각 행/열은 다른 행/열들의 cyclic reformulation으로 쓰여지는 몇개의 순환행렬로 대체함으로 가중치 저장공간을 줄일 수 있다고 합니다. 여기서 Lb는 각각의 만들어지는 matrix의 행/열의 사이즈를 나타냅니다.(block 크기나 FFT크기)(+FFT는 Fast Fourier Transform으로 추정됩니다. 서론 마지막에서 두번째 문단에 언급되나 모름https://dl.acm.org/doi/10.1145/3394885.3431532 이런거 보면 맞는듯) Block-Circulant Neural Network Accelerator Featuring Fine-Grained Frequency-Domain Quantization and Reconfigurable FFT Modules | Proceedings of the 26th Asia and South Pacific Design Automation Conferenceresearch-article Block-Circulant Neural Network Accelerator Featuring Fine-Grained Frequency-Domain Quantization and Reconfigurable FFT Modules Share on Authors: Yifan He , Jinshan Yue , Yongpan Liu , Huazhong Yang Authors Info & Claims ASPDAC '21: Proceedings of the 26th Asia and South Pacific Desi...dl.acm.org Block-Circulant Matrices-Based CONV 10, 46에 인용된 CirCNN, C-LSTM에선 이 방식의 효과가 잘 입증되어있음. 하지만 conv는 철저히 논의되지는 않았다네요​input과 weight filters(tensor)가 들어왔을 때 이미지에 filter 곱한것들을 다 더하는 MAC(multiplication accumulation)이 가속의 초점!​ ⊛가 circular convolution을 의미 ◦는 elementwise multiplication​그림에서 볼 수 있듯이 weight 압축후 weight tensor의 형태는 r ×r ×C ×C ′ /Lb 가 됨.a에서 바깥쪽의 중괄호는 weight tensor의 output channel의 ID안쪽 중괄호는 input channels의 index vector이다. 여기서 각 벡터들의 길이는 Lb, 안쪽과 바깥쪽에서 C/Lb개가 있어 이득을 얻는것으로 보인다.​C/Lb개의 weight kernel. CONV kernel 의 결과가 나오면 중간 matrices를 더하고 하나의 matrix를 내놓는다.​이것이 바로 CONV layer의 결과 중 하나의 채널!​batch normalization이나 bias는 마지막에 계산된다고 합니다.​​​REQ-YOLO FRAMEWORKHeterogeneous Weight Quantization​Equal-distance quantization과 the powers-of-two quantization technique의 이점을 적절히 취하겠다는 것.​Equal-distance quantization는 정확도가 크게 안떨어지나 전력 소비량, 하드웨어 사용률이 높았고the powers-of-two quantization technique는 binary bit shift 기반의 곱셈연산으로 하드웨어 효율이 좋으나 간격이 고르지 못한 스케일링으로 정확도가 유의미하게 떨어졌음.​여기서는 Conv layer에 따라 두 방법중 하나를 선택하였음. 진짜 별거없네​ The mixed powers of-two-based weight representation은 부호 비트와 magnitude 비트로 구성되는데magnitude 비트는 primary powers-of-two 와 a secondary powers-of-two part의 조합이다. ADDITIVE POWERS-OF-TWO QUANTIZATION: A NONUNIFORM DISCRETIZATION FOR NEURAL NETWORKS 에서 가져옴. 추후 적용가능..?ADMM for Weight Quantizationblock circulant matrix Wij에는 index vector인 wij가 아니라 FFT(wij)가 담겨있는데 이것에 양자화는 어려움. ADMM을 FFT/IFFT와 통합하고 heterogeneous weight quantization에 사용하여 FFT친구들을 직접적으로 양자화함으로 압축비와 정확도를 모두 챙기고자함.​frequency(FFT) domain 에서 quantization후 weight domain에서 weight를 매핑하여 ADMM의 유연성을 활용하겠다고 한다. 양자화 문제를 최적화 문제로 접근을 하는데 최적화 도구를 사용하긴 어려워서ADMM 적용으로 원래의 양자화 문제를 수렴까지 반복적으로 해결될 2개의 하위문제로 분해한다.​첫 번째 하위 문제는 minx f(x) +q1(x)q1 (x) is a differentiatede, quadratic term-> combinatorial constraints 없음. sgd같은 전통적 optimization 방법 가능​두 번째 하위 문제는 minx g(x) + q2(x)g(x) corresponds to the original combinatorial constraints and q2 (x) is another quadratic term.->우리가 사용한 block -Circulant Matrices, 양자화 등의 특수한 combinatorial constraints는 최적분석가능.수학적 내용 잠깐 건너뜀. (최적분석이 가능함을 증명하는 내용)​​​​HARDWARE IMPLEMENTATION​  low-power and high-performance.. YOLO를 FPGA서 구현​제한된 FPGA on-chipBRAM이 host memory에서 가중치를 받아오는데 충분한 용량을 보장한다.(이유 block circulant matrices의 규칙성으로 압축후 가중치 등 추가적인 저장공간이 필요치않고 ADMM을 활용해  heterogeneous weight quantzation하니까 weight storage 더 줄이고 효율적으로 쓴다. )​무튼 시작FPGA Resource-Aware Design Flowheterogeneous weight quantization 때문에 PE 두개 디자인 했다고함.DSP기반의 equal distance quantization을 위한 PEShift 기반의 mixed powers of tow quantization을 위한 PE​ #CONV들은 DSP와 LUT에서의 Conv 연산수 DSP,LUT는 각각 conv연산서 필요한 자원인데 그 뒤에 붙는건 DSP based인지 shift based인지BRAM 부분은 우리 모델 체크해야 함곱셈을 bit shift 연산으로 대체하면 DSP 사용량이 대폭 감소해 전력소비가 감소한다는데 이때 LUT 오버헤드가 발생하니 하드웨어 효율을 위해 both the equal-distance quantization and the mixed powers-of-two-based quantization techniques 모두 적용하는걸 추천한다네요. layer마다 같긴하지만​ Equal distance quantization; 모드 1은 DSP를 곱셈에 활용해 정확도가 높다.정확도 감소가 많이 크지 않다면 모드 2로 간다는 거​여기서 가장 많이 리소스를 잡아먹어 병목현상이 생기는 곳은 bandwidth이라고 합니다. 기존 디자인과 자기 디자인의 차이라고 하네요.​​​Overall Hardware Architecture FPGA 디자인은 computation unit, on-chip memories/BRAM, 그리고 datapath control logic로 구성off chip memory에는 access하지 않는다고한다.​사전 훈련 네트워크 모델(conv weight filter 등)의 FFT 결과는 호스트 CPU 및 PCI-xepress(PCIe) 버스를 통해 호스트 메모리에서 FPGA BRAM으로 로드된다.​데이터 버퍼는 PE 연산 준비를 위해 이전 단계로부터 슬라이스의 입력 이미지, itermediate results와 레이어 출력을 슬라이스별로 cache하는데 사용된다.​computation unit 내부 PE는 곱셈, 덧셈(MAC) 및 normalization 등 기타 기능을 실행하는 기본 computing unit의 집합이다.​글로벌 컨트롤러는 FPGA fabric 상의 연산과 데이터 흐름을 조정한다.​라고 합니다.​PE Design​처음쪽에서 봤듯 FFT와 IFFT 는 쌍으로 실행되고 이를 FFT/IFFT 커널로 결합 구현해도 문제없다. N점 IFFT 계산은 나눗셈과 two conjugation이 추가된 N점 FFT를 이용해 실행될 수 있다​FFT와 IFFT 사이에는 N개의 곱셈기가 있으며, 이는 FFT의 intermediate results와 BRAM에 저장된 가중치 값의 곱셈을 담당한다. PE는 주로 matrix-vector multiplication을 위해 설계되었고 요소별 ""FFT→MAC→IFFT"" 계산을 사용하여 구현된다. 제안된 PE 아키텍처. Register bank, weight decoder, mode decoder, 2 multiplexers, 2 FFT/IFFT kernels​​ "
PyTorch 정복 _ 4. Object Detection Tutorial ,https://blog.naver.com/ab415/222210391791,20210117,"참고 링크 : PyTorch 공식 Documenthttps://pytorch.org/tutorials/intermediate/torchvision_tutorial.html TorchVision Object Detection Finetuning Tutorial — PyTorch Tutorials 1.7.1 documentationDefining the Dataset The reference scripts for training object detection, instance segmentation and person keypoint detection allows for easily supporting adding new custom datasets. The dataset should inherit from the standard torch.utils.data.Dataset class, and implement __len__ and __getitem__ .pytorch.org ​Colab에서 튜토리얼 진행​​1. pycocotools 설치 %%shellpip install cython# Install pycocotools, the version by default in Colab# has a bug fixed in https://github.com/cocodataset/cocoapi/pull/354pip install -U 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI' ​2. Defining the Dataset- Custom dataset을 만들어서 사용할 것- 사용하는 데이터는 Penn-Fudan dataset, 보행자들에 대한 이미지 데이터셋, Detection으로 사용할 수도 있고, Masking도 있어서 Segmentation도 활용할 수 있음​2-1) 데이터셋 받기 %%shell# download the Penn-Fudan datasetwget https://www.cis.upenn.edu/~jshi/ped_html/PennFudanPed.zip .# extract it in the current folderunzip PennFudanPed.zip - 데이터셋의 구성은 다음과 같이 Image 데이터와 mask이미지가 다음과 같은 경로에 있다. PennFudanPed/  PedMasks/    FudanPed00001_mask.png    FudanPed00002_mask.png    FudanPed00003_mask.png    FudanPed00004_mask.png    ...  PNGImages/    FudanPed00001.png    FudanPed00002.png    FudanPed00003.png    FudanPed00004.png ​3. 이미지 시각화​3-1) 일반 image 데이터 from PIL import ImageImage.open('PennFudanPed/PNGImages/FudanPed00001.png') ​3-2) mask 이미지- mask.putpalette()   → putpalette 메소드는 mask이미지에 해당하는 index(label)값을 어떠한 색으로 칠할 것인지를 정하는 것   → np.unique를 사용하면 해당 mask에서 몇개의 label이 있는지 확인 가능   → 일반적으로 배경은 0번, 이후에는 object들 import numpy as npmask = Image.open('PennFudanPed/PedMasks/FudanPed00001_mask.png')mask.putpalette([                 0, 0, 0,     # Black background                 255, 0, 0,   # index 1 is red                 255, 255, 0, # index 2 is yellow                #  255, 153, 0, # index 3 is orange                 # 3번째 instance에는 오렌지 색을 칠해줄 것이지만 여기선 두명의 사람만 있음])print(mask)print(np.unique(np.array(mask)))mask <PIL.PngImagePlugin.PngImageFile image mode=P size=559x536 at 0x7FC6F8F14630>[0 1 2] ​4. Custom Dataset 만들기- Custom Dataset에서 있어야할 메소드는?    → __getitem__() + __len__()    → 궁금한 점은... bbox 의 정보를 찾을때 np.where()를 사용하였는데, 일반적으로 인자로 조건과, 참일때의 값, 거짓일 때의 값을 넣는 것으로 알고 있는데... masks라는 bool값이 담긴 것을 넣어 줬을 뿐인데... 뭐지- 다음과 같은 프로세스가 수행된다.    → __init__ 에서 이미지들의 경로를 list로 받아와서 정렬, imgs, masks로 사용    → __getitem__ 에서는 이 경로들을 idx로 접근, 경로로부터 이미지를 PIL로 열고, 일반 이미지에 대해서만 ""RGB""로 convert    → 이후에 np.where를 사용해서 구하는 bbox정보, label, mask 정보, 그리고 area 정보도 tartget dictionary에 담아서 return한다 import osimport numpy as npimport torchimport torch.utils.datafrom PIL import Imageclass PennFudanDataset(torch.utils.data.Dataset) :    def __init__(self, root, transforms = None):        self.root = root        self.transforms = transforms        self.imgs = list(sorted(os.listdir(os.path.join(root, ""PNGImages""))))        self.masks = list(sorted(os.listdir(os.path.join(root, ""PedMasks""))))    def __getitem__(self, idx) :        # load images ad masks        img_path = os.path.join(self.root, ""PNGImages"", self.imgs[idx])        mask_path = os.path.join(self.root, ""PedMasks"", self.masks[idx])        img = Image.open(img_path).convert(""RGB"")        # mask는 RGB로 Convert하지 않는다.        # 각 색깔은 다른 인스턴스를 나타내기 때문, 0은 Background        mask = Image.open(mask_path)        mask = np.array(mask)        # instance들은 각각 다른 색으로 encode할 것        obj_ids = np.unique(mask)        # 0은 배경이므로 obj_ids에서는 뺀다        obj_ids = obj_ids[1:]        # 색으로 encode된 마스크를 binary masks로 바꾼다.        # masks의 shape은 기존 mask의 H, W를 그대로 사용하고 각 픽셀에 대한 label        # 즉, 위에서 시각화한 이미지를 예로들면 0, 1, 2에셔 배경을 제외한 1, 2,         # 2개에 대한 binary mask가 생성될 것        # masks.shape은 2, H, W가 될 것이다.        masks = mask == obj_ids[:, None, None]        # 각각의 마스크를 위한 bbox를 얻는다.        # pos에는 tuple 값이 들어가게 된다.        # 이때 두가지의 값이 담기게 되는데, pos[1]의 값에서 x, pos[0]에서 y값을 구한다.        # 이해가 잘 안간다.. np.where        num_objs =len(obj_ids)        boxes = []        for i in range(num_objs):            pos = np.where(masks[i])            xmin = np.min(pos[1])            xmax = np.max(pos[1])            ymin = np.min(pos[0])            ymax = np.max(pos[0])            boxes.append([xmin, ymin, xmax, ymax])        # box정보와 masks정보를 tensor로..        boxes = torch.as_tensor(boxes, dtype=torch.float32)        # 오직.. 하나의 class만 있음, (num_objs, )의 shape을 갖는 1이 있는 텐서        # 하나의 클래스라는 것은 사람을 뜻하는 것일 것 같다.        labels = torch.ones((num_objs,), dtype=torch.int64)        masks = torch.as_tensor(masks, dtype=torch.uint8)        image_id = torch.tensor([idx])        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])        # suppose all instances are not crowd        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)        target = {}        target[""boxes""] = boxes        target[""labels""] = labels        target[""masks""] = masks        target[""image_id""] = image_id        target[""area""] = area        target[""iscrowd""] = iscrowd        if self.transforms is not None:            img, target = self.transforms(img, target)                return img, target    def __len__(self):        return len(self.imgs) - 데이터 확인    → 기본적으로 image, 그리고 area, boxes, image_id, iscrowd, labels, masks를 return 하고 있다 dataset = PennFudanDataset('PennFudanPed/')dataset[np.random.randint(len(dataset))] (<PIL.Image.Image image mode=RGB size=455x414 at 0x7FC6F8F35AC8>, {'area': tensor([35301.]),  'boxes': tensor([[ 67.,  92., 190., 379.]]),  'image_id': tensor([1]),  'iscrowd': tensor([0]),  'labels': tensor([1]),  'masks': tensor([[[0, 0, 0,  ..., 0, 0, 0],           [0, 0, 0,  ..., 0, 0, 0],           [0, 0, 0,  ..., 0, 0, 0],           ...,           [0, 0, 0,  ..., 0, 0, 0],           [0, 0, 0,  ..., 0, 0, 0],           [0, 0, 0,  ..., 0, 0, 0]]], dtype=torch.uint8)}) ​5. Defineing Model- 이 Tutorial에서는 R-CNN 계열의 Mask R-CNN을 사용- Yolo에 비해서 좀 더 느리지만 정확도는 더 높다.- pre-trained된 모델을 fine-tuning해서 사용할 것이다.    → Fast R-CNN모델을 사용함과 동시에 Segmentation Mask도 계산할 것이기 때문에 Mask R-CNN도 사용 import torchvisionfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictorfrom torchvision.models.detection.mask_rcnn import MaskRCNNPredictordef get_instance_segmentation_model(num_classes):    # COCO 데이터에 사전학습된 instance segmentation 모델을 로드    model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)    # classifier의 입력으로 들어가는 feature의 수    in_features = model.roi_heads.box_predictor.cls_score.in_features    # 사전학습된 head를 교체    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)    # mask classifier의 입력 feature로 들어가는 수    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels    hidden_layer = 256    # mask predictor를 교체    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask,                                                       hidden_layer,                                                       num_classes)        return model ​6. 학습 전 준비​- 학습을 진행하기 전에 학습에 필요한 코드들을 가져온다 %%shell# Download TorchVision repo to use some files from# references/detectiongit clone https://github.com/pytorch/vision.gitcd visiongit checkout v0.3.0cp references/detection/utils.py ../cp references/detection/transforms.py ../cp references/detection/coco_eval.py ../cp references/detection/engine.py ../cp references/detection/coco_utils.py ../ ​- augmentation, transformation 적용    → mean/std nomalization과 image scaling을 따로 필요없다고 한다.    → Mask R-CNN 모델에 의해서 handled된다고 한다. from engine import train_one_epoch, evaluateimport utilsimport transforms as Tdef get_transform(train) :    transforms = []    # image > PIL Image > PyTorch Tensor    transforms.append(T.ToTensor())    if train:        # 학습하는 동안에는 랜덤하게 flip        transforms.append(T.RandomHorizontalFlip(0.5))    return T.Compose(transforms) ​7. DataLoader 만들기 # 데이터셋을 사용해서 transformation정의하기dataset = PennFudanDataset('PennFudanPed', get_transform(train=True))dataset_test = PennFudanDataset('PennFudanPed', get_transform(train=False))# train과 test 나누기torch.manual_seed(1)indices = torch.randperm(len(dataset)).tolist()dataset = torch.utils.data.Subset(dataset, indices[:-50])dataset_test = torch.utils.data.Subset(dataset_test, indices[-50:])# train과 valid loader만들기data_loader = torch.utils.data.DataLoader(    dataset, batch_size=2, shuffle=True, num_workers=4,    collate_fn=utils.collate_fn)data_loader_test = torch.utils.data.DataLoader(    dataset_test, batch_size=1, shuffle=False, num_workers=4,    collate_fn=utils.collate_fn) ​8. 모델 및 optimizer 설정 device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')num_classes = 2model = get_instance_segmentation_model(num_classes)model.to(device)params = [p for p in model.parameters() if p.requires_grad]optimizer = torch.optim.SGD(params, lr = 0.005,                            momentum=0.9, weight_decay=0.0005)# lr schedulelr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,                                               step_size=3,                                               gamma=0.1) ​9.  학습 num_epochs = 10for epoch in range(num_epochs):    train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)    lr_scheduler.step()    evaluate(model, data_loader_test, device=device) ​10.평가 img, _ = dataset_test[0]model.eval()with torch.no_grad():    prediction = model([img.to(device)]) Image.fromarray(img.mul(255).permute(1, 2, 0).byte().numpy()) Image.fromarray(prediction[0]['masks'][0, 0].mul(255).byte().cpu().numpy()) ​​+. 6번에서 사용되는 코드 깃허브https://github.com/pytorch/vision/tree/v0.3.0/references/detection pytorch/visionDatasets, Transforms and Models specific to Computer Vision - pytorch/visiongithub.com ​ "
Fruit Object Detection Result ,https://blog.naver.com/mikangel/221894039607,20200406,Fruit Object Detection Result  아마존 고도 안될거라고 확신함다.. 
[kinect camera] Kinect V2 python 개발/Object detection ,https://blog.naver.com/jkl1435/221542347122,20190520,"Tensorflow Object Detection으로 하려고 하면 libfreenect2로는 안 된다.​PyKinect2와 libfreenect2를 python으로 사용할 수 있게 해주는 py3freenect2를 쓰는 두 가지 방법이 있는데, py3freenect2는 인스톨 중에 libfreenect2.lib인가에서 계속 linking error가 나서 그냥 포기했다..​하라는 대로 다 해보고 했는데 되지 않아 pykinect2로 전향.​pykinect2는 tensorflow에 바로 적용할 수가 없다. 그냥 무식하게 복붙했더니 nonetype이라서 안 된다고 떳나 그랬다..​중간발표때 교수님들한테  혼나서^^.......열심히 찾아본 결과..pykinect2를 사용하려면 wrapperpykinect2를 사용해주면 된다.​https://github.com/gdubrg/wrapperPyKinect2 gdubrg/wrapperPyKinect2A wrapper of the famous wrapper called PyKinect2 to use Kinect One with Python - gdubrg/wrapperPyKinect2github.com kinect one을 위한 거라고 써있는데, v2에서도 잘 작동했다.​ import timeimport cv2import numpy as npimport ctypesimport _ctypesimport sysfrom acquisitionKinect import AcquisitionKinect as ackfrom frame import Frameimport cv2import pygamefrom pykinect2 import PyKinectV2from pykinect2.PyKinectV2 import *from pykinect2 import PyKinectRuntimekinect = ack()frame = Frame()with detection_graph.as_default():  with tf.Session(graph=detection_graph) as sess:        while True:        kinect.get_frame(frame)        kinect.get_color_frame()        image = kinect._frameRGB        kinect.get_depth_frame()        image2 = kinect._frameDepth               image_np = cv2.cvtColor(image, cv2.COLOR_RGBA2RGB)                 image_tensor = detection_graph.get_tensor_by_name('image_tensor:0')        detection_boxes = detection_graph.get_tensor_by_name('detection_boxes:0')        detection_scores = detection_graph.get_tensor_by_name('detection_scores:0')        detection_classes = detection_graph.get_tensor_by_name('detection_classes:0')        num_detections = detection_graph.get_tensor_by_name('num_detections:0')                       image_np_expanded = np.expand_dims(image_np, axis=0)        (boxes, scores, classes, num) = sess.run(              [detection_boxes, detection_scores, detection_classes, num_detections],              feed_dict={image_tensor: image_np_expanded})        vis_util.visualize_boxes_and_labels_on_image_array(              image_np,              np.squeeze(boxes),              np.squeeze(classes).astype(np.int32),              np.squeeze(scores),              category_index,              use_normalized_coordinates=True,              line_thickness=8)          cv2.imshow('live_detection',image_np)       #cv2.imshow('depth_image', image2) #depth        if cv2.waitKey(25) & 0xFF==ord('q'):            break            cv2.destroyAllWindows()            cap.release() 실행 결과 live에서도 잘 object detection이 되었다. "
Google Object Detection API 설치 및 구현/실시간 웹캠 detection ,https://blog.naver.com/jkl1435/221436542356,20190107,1. google tensorflow object detection api 다운받기(models/research/object_detection)​2. protoc 다운받기protoc-3.6.1-win32다운받아서 protoc.exe 경로 설정하기(SET PATH)(아니면 그냥 bin폴더에 있는 protoc.exe를 models/research폴더 안에 넣어주기)​3. object_detection의 protoc에서 다 protoc 설정하기윈도우의 경우 한 번에 하는 명령어가 안되기 때문에 일일해 해줘야 함(ex. protoc --python_out=. ./object_detection/protos/anchor_generator.proto)​4. 단순 실행을 원할 경우 미리 설치 된 object_detection/builders/model_builder_test.py 실행위의 protoc과정을 끝냈으면 builders라는 폴더는 생겨있음​5. 웹캠을 통해서 실시간 object_detection을 하려고 하면이 코드 참조https://github.com/ElephantHunters/Real_time_object_detection_using_tensorflow ElephantHunters/Real_time_object_detection_using_tensorflowcode for real time object detection using TensorFlow and openCV - ElephantHunters/Real_time_object_detection_using_tensorflowgithub.com ​신기하당@!!!!   
Fast R-CNN 모델(Object detection) 세부 설명 ,https://blog.naver.com/grow_bigger/222770993279,20220613,"[1-stage detector / 2-stage detector]1-Stage detector와 2-Stage detector란? :: 프라이데이 (tistory.com)위치 탐지와 객체 인식이 한번에 이루어지는 것을 1-stage, 위치탐지를 한 후, 이를 토대로 어떤 물체인지 판단하는 방식을 2-stage detector이라고 합니다. 1-stage방식은 빠르고 2-stage방식은 비교적 정확도가 높습니다.​[bounding box regression]Bounding box regression (tistory.com)https://lilianweng.github.io/lil-log/2017/12/31/object-recognition-for-dummies-part-3.htmlbounding box regression은 모델이 객체가 있다고 인식하여 그린 영역과 실제 객체가 있는 영역의 차이를 줄이기 위한 학습을 말한다. R-CNN에서는 IoU가 0.6이상인 경우 적용했다고 한다.아래 내용은 R-CNN에서 사용된 bounding box regression이다.P는 모델이 예측한 영역, Gsms ground truth이다. 목표는 P를 d라는 함수를 통해 G로 이동시키는 것이다. 그림 1그림2를 보면 d의 식이 나와있는데 w는 학습가능한 가중치, 파이는 pool5층에서 나온 feature vector이다. 그림 2사실 핵심적인 식은 그림3의 식이다. SSE loss를 정규화와 같이 쓴 형태이다. 즉 아래 식을 최소화하는 d함수를 찾아하는데 d함수에서 파이는 고정되어 있으니 결국 아래 식을 최소화하는 가중치 값을 찾는 과정이다. 그림3그림 4그림 4는 그림 3에 있는 t를 구하는 식인데, 얼마나 실제 영역과 차이가 나는지에 대한 값이다. 또는 이동해야하는 정도를 나타내는 것인데, 예를 들어 오차가 아예 없어 SSE가 가장 최소인 경우 wwTΦ(Pi) = dw = tw 이므로 그림1의 3번째 식에 대입하면 좌변과 같아지는 것을 확인할 수 있다.[SSE / MSE](R1,P1)제02강(01)모형 평가 지표 -SSE, MSE, MAE, MAPE :: 통컨(통계컨설팅) (tistory.com)아래 식을 보면 시그마 앞의 값에 1/n이 곱해지는지에 따라 다르다. SSE는  bounding box regression에서 사용된다. [mAP, mean average precision]mAP는 객체 탐지등에서 자주 사용되는 평가 지표이다. 모델 평가 시 정확도와 재현율을 동시에 고려해야 일반적으로 유용한 모델을 만들 수 있는데 재현율에 따른 정확도 그래프를 PR곡선이라한다. (일반적으로 재현율과 정확도는 반비례한다.)이 PR곡선의 면적으로 나타낸 값이 AP(average precision)이며 높을수록 전박적인 알고리즘 성능이 우수하다. ​[RoI Pooling]Fast R-CNN 논문 리뷰 (tistory.com)RoI Pooling은 사진을 합성곱 계층을 통과 시켜 특징 맵을 만들고 Selective search와 같은 방식으로 생성된 region proposal을 feature map에 투영합니다. 그 후 이 계층이 설정한 output 크기에 맞춰 output을 만들수 있도록 하기 위해 검은 영역을 출력하고자 하는 크기대로 나누어줍니다. 그러니까 이 RoI pooling층이 2x2 행렬을 출력으로 생성하게 하고 싶다면 검은 영역을 가로 세로 총 2번씩 나누어 4개의 칸을 만들어 주는 것입니다. 그러면 이 작게 나누어진 Grid하나에서 pooling을 하여 하나를 고르는 작업을 모든 셀에 대해 한다면 2x2행렬을 출력으로 산출할 수 있게됩니다. 자세한 그림은 링크를 참조하시면 이해가 더 쉬울 것같습니다.​[Mini-batch gradient descent(MSGD)]https://light-tree.tistory.com/133미니 배치 경사 하강법은 SGD와는 달리 하나가 아닌 일정 크기의 batch로 모델을 업데이트 하고 Batch gradient descent(BGD)와는 달리 한번의 업데이트에 전체 데이터셋을 다 이용하지는 않는다. 미니배치 경사하강법은 전체 데이터 셋에서 확률적으로 뽑은 데이터들로 작은 을 만들고 이 그룹에 대해 기울기를 구하여 모델을 업데이트 하는 방식을 사용한다. ​[L1, L2 loss]L1 & L2 loss/regularization · Seongkyun Han's blog L1 LossL2 loss[SVD, 특잇값 분해]머신러닝 - 20. 특이값 분해(SVD) (tistory.com)​[Truncated SVD]머신러닝 - 20. 특이값 분해(SVD) (tistory.com)Truncated SVD는 SVD에서 대각행렬의 일부를 추출하고 제거된 부분에 대응하는(행렬 연산시 제거된 부분과 곱해지는) 원소를 제거하여 크기를 줄인 것입니다. "
[머신러닝] Object detection: Introduction ,https://blog.naver.com/kyy0810/220973218296,20170402,"Object detection(물체 검출)은 주어진 영상에서 찾고자 하는 물체의 위치를 찾는것이다. 검출기의 입력은 이미지이고 출력은 찾고자 하는 물체를 감싸는 사각형 박스가 된다. 예를 들어, 주어진 이미지에서 사과를 찾는다고 가정해보자.    가장 일반적인 방법은 어떤 머신에게 사과가 어떤 특징을 가졌는지(어떻게 생겼는지) 알려주고 찾으라고 하는 방법이 될 것이다. 이 아이디어는 두 단계로 구체화될 수 있다. ​  1. 머신이 사과가 어떤 특징을 가졌는지 배우는 단계, 즉 학습 단계  2. 학습된 머신이 실제로 사과를 찾는 단계, 즉 검출 단계​ 학습 단계를 수행하기 위해서는 먼저 사과 이미지가 있어야 할것이다. 또, 이 이미지가 사과를 포함하고 있다는 정보도 주어야 될것이다. 아래 그림에 적용해 보면 input이 이미지가 되고 label 은 ""이 이미지가 사과를 포함하고 있다"" 라는 정보가 될 것이다.  그 다음, 사과의 특징을 잘 설명할 수 있는 요소들을 추출하여 이 값을 토대로 사과를 인식하면 된다. 사과만의 독특한 특징은 무엇을까? 바로 떠오르는 생각은 사과의 색감이다. 내가 지금까지 본 사과는 빨간색이거나 초록색이었다. 색깔정보를 추출하기 위해서는 단순히 이미지의 픽셀값(RGB)를 머신에게 주면 된다. 그럼 머신은 '색이 빨간색이나 초록색은 사과의 특징이구나'라고 생각한다. 이 판단 기준을 세우는 것이 머신의 역할이며, 판단 기준은 보통 '학습 모델'이라고 불린다. ​ 검출 단계에서는 입력된 이미지에 대해서 학습 모델이 사과의 존재유무와 위치를 대답을 해주는 과정이다. 그러므로, 학습 단계와 마찬가지로 검출 단계에서도 이미지에서 색깔정보를 추출하고 이 값을 학습 모델에게 넘겨준다. 그리고 모델은 이 이미지가 사과인지 아닌지 판별을 하게 된다. 즉, 이미지에 초록색이나 빨간색이 포함됬다면, '이미지에 사과가 포함됬어요'라고 출력을 할 것이다. 여기까지의 과정을 classification(분류)이라고 하며, 사과가 이미지 어디에 위치에 있는지 좌표 정보까지 출력한다면 detection(검출)이 된다. ​이 과정을 통해 머신은 사과를 찾을수 있는 것이다. 위에 예에서는 색깔이라는 아주 단순한 특징을 사용했다. 하지만, 실제로 색깔 정보만 사용한다면, 빨간 구두도 사과로 인식해버리는 터무니없는 결과가 나올 것이다. 그러므로, 많은 연구자들은 사물에 특징을 잘 추출할수 있는 특징 추출 알고리즘을 연구해왔다.   http://www.nltk.org/book/ch06.html ​ 가장 대표적인 특징 추출 알고리즘은 윤곽선 같은 식별이 용이한 특징 값을 사용하는 SIFT(Scale Invariant Feature Transform), HOG(Histogram of Gradient)가 많이 사용되었다. 하지만, 최근에는 딥러닝의 성능이 워낙 우수하기 때문에, 다른 특징 추출 알고리즘은  잘 사용되지 않는 추세인거 같다. ​또한, 판단 기준을 세우는 머신의 성능을 향상하기 위해 다양한 알고리즘이 제시되었다. 가장 간단하면서도 파워풀한 알고리즘은 KNN(K-nearest neigbhor)이 있다. 아래 그림은 특징 추출 알고리즘을 이용해 뽑힌 특징들이 특징 공간에 보여진 모습이다. 빨간색 세모는 사과의 특징값이고, 파란색 사각형은 바나나의 특징값이라고 가정해보자. 즉, 두개의 클래스가 존재한다. 초록색으로 보여지는 원은  우리가 어떤 사물인지 알고 싶은 특징값이다. KNN은 이 초록색 원을 기준으로 가장 가까운 K개의 특징값을 보고 어떤 사물인지 결정하는 역할을 한다. 예를 들어, K가 3일 경우 빨간색 세모가 더 많으므로, 초록색 원은 사과라고 판단하게 된다. K가 5일 겨우는 파란색이 우세하므로, 바나나라고 판단하게 된다.    SVM(support vector machine) 역시 가장 많이 사용되는 알고리즘 중에 하나이며, 특징 값을 구별하기 위해 경계면을 설정할때, 경계면에서 가장 인접한 특징값이 가장 멀리 떨어질수 있도록(최대 마진) 하는 경계면을 설정하는 방법이다. 경계면에 가장 가까운 특징값은 support vector라고 한다. 이전에는 클래스의 모든 특징값들의 평균값으로 경계면을 정해왔다는 점에서 SVM은 새로운 접근법을 가지고 있는 셈이다. 성능면에서도 매우 우수하다.   ​ "
Tensorflow를 활용한 물체감지(Object Detection)(1) ,https://blog.naver.com/nangsit/222340531396,20210506,"Object detection in Android using TensorFlow Lite (2020)예제 : https://www.youtube.com/watch?v=0miGC4qLros ​Using Tool· Python ( version 3.7.3 )  - tensorflow 공식 홈페이지에 따르면 Python 3.8 지원에는 TensorFlow 2.2 이상이 필요하다고 하며,     실습에서 진행할 때 tf.contrib가 필요함으로, Tensorflow 2.X 버전에서는 tf.contib가 삭제되어 tensorflow 1.15.0 버전을 사용한다. · Anaconda ( 선택사항 // Window의 cmd 환경에서도 진행이 가능합니다. ) · Tensorflow ( version 1.15.0 )· LabelImg.py  [설치] https://github.com/tzutalin/labelImg tzutalin/labelImg🖍️ LabelImg is a graphical image annotation tool and label object bounding boxes in images - tzutalin/labelImggithub.com · download_images.py  [설치] https://www.pyimagesearch.com/2017/12/04/how-to-create-a-deep-learning-dataset-using-google-images/ How to create a deep learning dataset using Google Images - PyImageSearchIn this post I'll show you how to use Google Images to (easily) grab more training data for your Deep Learning and Convolutional Neural Networks.www.pyimagesearch.com  [참조] https://youtu.be/m7jwlDhsfQ4· generate_tfrecord.py [설치] https://github.com/practical-learning/object-detection-on-android practical-learning/object-detection-on-androidContribute to practical-learning/object-detection-on-android development by creating an account on GitHub.github.com · xml_to_csv.py [설치] generate_tfrecord.py 설치경로에 함께 존재· ssd_mobilenet_v2 [설치] https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf1_detection_zoo.md  [설치] http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v2_quantized_300x300_coco_2019_01_03.tar.gz (url 입력) tensorflow/modelsModels and examples built with TensorFlow. Contribute to tensorflow/models development by creating an account on GitHub.github.com · Tensorflow Open API [설치] https://github.com/tensorflow/models tensorflow/modelsModels and examples built with TensorFlow. Contribute to tensorflow/models development by creating an account on GitHub.github.com  [참조] https://musma.github.io/2019/02/15/tensorflow-on-windows.html Windows에서 Tensorflow Object Detection API 설치하기!!Windows에서 Tensorflow Object Detection API 설치하기!musma.github.io ​​Step 0. Anaconda Prompt 가상환경 생성 및 환경설정 ( Window의 경우 cmd로 진행해도 상관없습니다. )​ 0-1. conda 환경 변수 설정​ conda 환경 변수  설치를 진행하면 자동으로 경로가 저장되는 경우가 대부분이지만 혹시 모르니 환경 변수가 제대로 설정돼있는지 확인 후 진행​ 0-2. conda 가상환경 생성 및 실행  [가상환경 생성]  > conda create -n python_3.6.5 python=3.6.5 -n : 가상환경 이름 python=python_version  : 가상환경에서 설치할 python 버전​ [가상환경 실행] > conda activate python_3.6.5​ [가상환경 리스트 확인] > conda env list​ [참조] https://sdc-james.gitbook.io/onebook/2./2.1./2.1.1./2-conda-virtual-environments​​Step 1. Tensorflow 설치 및 라이브러리 설치  1-1. tensorflow 1.15.0 설치  단순히 pip3 install tensorflow라고 입력한다면, tensorflow의 최신 버전이 설치됨으로 꼭 버전명을 입력하여 설치하여야 한다.​ 설치가 안 될 경우  > python -m pip install --upgrade pip 명령을 통해 pip 버전을 업데이트하고 다시 tensorflow 설치 명령을 진행 만약, pip 혹은 pip3 명령이 되지 않는다면, 이것도 환경 변수 문제가 대부분이므로 환경 변수를 확인해야 한다.​ [참조] https://oz-il.tistory.com/21 [Python] cmd 내 pip 실행 오류와 해결 방법 / VisualStudio 패키지 설치 방법최근 컴퓨터를 포맷한 후 비주얼 스튜디오를 깔며 python을 같이 설치했었다. 설치 후 간단한 프로젝트를 진행하다 외부 라이브러리를 필요로 하여 cmd에서 pip를 입력하였으나, 'pip'은(는) 내부 또는 외부 명령,..oz-il.tistory.com  설치 완료 시  > python   >>> import tensorflow as tf >>> print(tf.__version__)​ 설치된 tensorflow의 버전을 확인한다. 사진처럼 1.15.0이 출력된다면 성공한 것이다. 만약, 이 과정에서  Could not load dynamic library 'cudart64_100.dll' : dlerror: cudart64_100.dll not found 라는 에러가 출력이 되었다면, 아래의 링크에서 cuda 10버전을 운영체제에 맞게 설치한 후 아래의 이미지처럼 생성되면 해결이 될 것이다. 그래도 안된다면 명령 프롬프트가 관리자 모드가 맞는지 한 번 확인이 필요하다. [참조] https://mickael-k.tistory.com/29 [Windows] cudart64_100.dll 문제Tensoflow 깔다 아래와 같은 이슈가 발견 되었습니다. >>> import tensorflow as tf 2019-12-29 18:39:37.326960: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic libr..mickael-k.tistory.com  [참조] https://ayoteralab.tistory.com/entry/Tensorflow-20error-cudart64100dll-not-found [Tensorflow 2.0][error] cudart64_100.dll not foundAP 프로젝트에서 Tensorflow 2.0을 설치하다보니.... 이 카테고리에 첫 글이 공교롭게도 error가 되었습니다. 더 많은 내용으로 풍성해 진다면 첫 글이 뭐든 상관은 없겠죠?? 오늘은 Tensorflow 2.0은 정상적으로..ayoteralab.tistory.com  [설치 ] https://developer.nvidia.com/cuda-10.1-download-archive-base?target_os=Windows&target_arch=x86_64&target_version=10&target_type=exelocal CUDA Toolkit 10.1 original ArchiveSelect Target Platform Click on the green buttons that describe your target platform. Only supported platforms will be shown. Operating System Architecture Distribution Version Installer Type Do you want to cross-compile? Yes No Select Host Platform Click on the green buttons that describe your host...developer.nvidia.com ​ 1-2. 라이브러리 설치 > pip3 install Cython > pip3 install contextlib2 > pip3 install pillow > pip3 install lxml > pip3 install jupyter > pip3 install matplotlib > pip3 install tf_slim or  > pip3 install Cython contextlib2 pillow lxml jupyter matplotlib tf_slim​ 설치를 하나하나 확인하려면 위의 명령을 사용하고 한 번에 설치를 진행하려면 아래 명령을 사용하면 된다.​ 설치 완료 시  > pip list  명령을 사용하여 설치를 진행한 라이브러리가 pip list에 존재하는지 확인하자  ​Step 2. Apple Dataset 다운로드 ​ 원래는 직접 데이터를 받고 라벨링 작업을 진행해야 하지만, 데이터를 만드는 데 걸리는 시간이 생각보다   오래 걸리며, 많은 데이터가 필요하게 됨으로  우선적으로 제공하는 Dataset을 이용하여 실습을 진행한다. 첨부파일Apple_dataset.egg파일 다운로드    dataset 폴더의 모습은 이렇게 존재하면 된다. train , test 디렉터리 : Apple_dataset.egg generate_tfrecord.py , xml_to_csv.py는 설치를 진행하지 않았다면, 아래의 링크를 clone 하거나      code를 .zip으로 받아 dataset에 옮겨 넣으면 된다. [설치] https://github.com/practical-learning/object-detection-on-android practical-learning/object-detection-on-androidContribute to practical-learning/object-detection-on-android development by creating an account on GitHub.github.com Step 3. 학습하기​ 여기서부터 Tensorflow Open API를 사용한다. 설치를 진행하지 않았을 경우 아래의 경로에서 설치를 진행하자. [설치] https://github.com/tensorflow/models tensorflow/modelsModels and examples built with TensorFlow. Contribute to tensorflow/models development by creating an account on GitHub.github.com  > pip3 install protobuf==3.11.4  ​ > cd C:설치경로\models-master\models-master\research\object_detection > protoc object_detection/protos/*.proto --python_out=.​ 해당 명령을 사용해서 proto 파일을 python 실행파일로 생성한다.  .proto파일을 .py파일로 변환한 이후에 실행파일 테스트를 진행하여 변환에 문제가 없는지 테스트를 진행한다.  > python model_builder_tf1_test.py  동일한 결과가 도출된다면, 다음으로 annotations를 이용해서 csv 파일을 생성해야 하는데, 이는 이전에 dataset 디렉터리에 저장한   xml_to_csv.py를 이용한다. window 키+r 을 입력하고 notepad를 입력하여 메모장을 켜서 xml_to_csv.py를 넣어 내용을 받아온다.​  xml_to_csv.py의 내용에서는 빨간 네모를 친 부분만 수정을 진행하면 된다. annotations의 경로나 이름이 다르더라도 상관없고 절대 경로를 사용해 주어도 된다.  현재 위의 dataset 디렉터리대로 파일들이 존재한다면 해당 파일처럼 내용을 수정하면 된다.  > cd xml_to_csv 경로 > xml_to_csv.py​ Successfully converted xml to csv. 문구가 나오면 성공적으로 csv 파일이 생성된 것이다. test.csv를 생성하였으니 이전의 xml_to_csv.py에서 수정한 내용을   train/annotations 와 train.csv로 변경한 후 똑같이 실행한다. ​ 실행 결과   csv 파일이 생성되면 해당 csv 파일을 이용해 train.record 파일과 test.record 파일을 생성한다. xml_to_csv.py처럼 메모장에 generate_tfrecord.py 파일을 읽어온다. ( 기왕이면 Notepad++를 설치해서 사용하는 것이 편함으로 권장 )​ ​ 우선 첫 번째 박스부터 import sys sys.path.append(""C:경로/models-master/models-master/research/slim"")​ 해당 코드는 system이 읽을 수 있도록 경로를 추가해 주는 것인데, 환경 변수 추가랑 동일하게 생각하면  된다.  이상하게 환경 변수를 동일하게 추가해도 모듈을 읽지 못하는 no moudule named 에러가 발생하여 그냥 코드에 경로를 추가해 주었다. ​ 두 번째 박스에는 구별할 class의 이름을 명시해 주는 것인데, annotations을 진행하였을 때 자신이 지정한 class의 이름으로 지정하면 된다. 제공한 Apple_dataset의   경우 annotations의 label 값이 'apple' 과 'damaged_apple' 두 가지가 존재함으로     if row_label == 'apple':          return 1    elif row_label == 'damaged_apple':          return 2    else:           None 가 된다. 나중에 실습 이후에 Customizing을 하려고 더 많은 클래스를 사용할 경우에는 elif 문을 복사해서  추가하고 이름을 본인이 labeling 한 이름을 지정해 주고   return 값을 3,...,n 과같이 순차적으로 늘려주면     된다. 마지막의 else: None은 불안해서 0으로 지정하고 진행하였다.  이제 변경사항을 변경하였으면,   generate_tfrecord.py을 실행한다.​ > python generate_tfrecord.py --csv_input=train.csv --output_path=train.record     --image_dir=train/images > python generate_tfrecord.py --csv_input=test.csv --output_path=test.record     --image_dir=test/images​ --csv_input : 이전에  xml_to_csv.py를 이용하여 생성한 csv 파일의 경로 및 이름 --output_path : record 파일이 출력되는 경로 및 이름 --image_dir : csv_input의 이미지 경로  ex) train.csv의 경우 dataset/train/images 이전의 dataset 디렉터리와 환경이 같다면 명령을 그대로 수행하면 된다. ​ 가끔 test.record 나 train.record의 파일 크기가 0KB인 경우가 있는데, 이는 변환이 잘못된 것이니 본인의 annotations의 class name과 row_label 값,   generate_tfrecord.py의 인자 값이 제대로 되었는지 한번 확인해보거나 xml_to_csv.py에서부터 다시 시작해보는 걸 추천한다.​Step 4. Tensorflow Open API  이제 본격적인 학습을 시작하기 위해 ssd 모델과 Tensorflow에서 제공하는 Open API를 사용한다. [설치] http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v2_quantized_300x300_coco_2019_01_03.tar.gz (url 입력, ssd_mobilenet) [설치] https://github.com/tensorflow/models (Tensorflow Open API) tensorflow/modelsModels and examples built with TensorFlow. Contribute to tensorflow/models development by creating an account on GitHub.github.com  설치가 완료되면, 우선 ssd_mobilenet_v2 디렉터리의 pipline.config 와 label_map.pbtxt를 수정해야 한다.  pbtxt 파일을 생성할 줄 모른다면 아래의 파일을 받아서 ssd_mobienet_v2 디렉터리 내에 옮겨 넣으면 된다.  ​ 작성양식 item {​     id: 1     name: 'apple' } item {​     id: 2     name: 'damaged_apple' } 이며, 이전의 generate_tfrecord.py의 row_label과 순서, 이름이 꼭 동일하게 작성해야 하니 주의하길바란다. 그리고 동일하게 클래스를 추가하고 싶을 경우에,   item{...}의 내용을 복사하며, id도 순차적으로 부여하면 된다. 꼭 주의할 점은 generate_tfrecord.py의 내용과 동일해야 한다는 점이며, 변경할 시 둘 다 변경 첨부파일label_map.pbtxt파일 다운로드  line 3 : classes 수 = 구별할 이미지의 수 ( 'apple' , 'damaged_apple' => 2 ) line 157 : model.ckpt ( 체크포인트 위치, ssd_mobilenet_v2 설치 시 내부에 존재함 ) line 162 : label_map.pbtxt 경로 line 164 : train.record ( generate_tfrecord.py로 생성한 train.record의 위치 ) line 174 : line 162과 동일 line 178 : test.record ( generate_tfrecord.py로 생성한 test.record의 위치 )​ 학습 진행​ > train.py   --alsologtostderr   --train_dir= 학습 출력 경로 ( 원하는 저장 폴더 경로 입력하면 됨 ) --pipeline_config_path=pipeline.config 경로​ NotImplementedError: Cannot convert a symbolic Tensor 발생 시  numpy 버전 변경 시 해결 > pip3 install numpy==1.19.5​ [참조] https://stackoverflow.com/questions/58479556/notimplementederror-cannot-convert-a-symbolic-tensor-2nd-target0-to-a-numpy​ 학습 진행  loss = Number는 손실 함수의 결괏값으로 생각하면 된다. 출력값과 오차의 정도를 의미하며, 1 step에서는 대부분 20 이상의 loss 값이 도출되는 것 같다. 2000회  정도의 학습을 진행하니 loss가 3에서 2 정도를 유지된다. 이는 높은 오차 값은 아니지만, 프로그램에 사용하려면 보통 1이하를 사용하는 것을 추천한다. [참조] https://hyeonnii.tistory.com/228 손실함수(Loss function)[손실함수 (Loss function)] 출력값과 정답의 오차를 정의하는 함수. 딥러닝에서 아래 두가지 손실함수가 많이 쓰임. 1. 오차제곱합 (Sum of Squares for Error, SSE) 출력층의 모든 뉴런에서 출력값과 정담의 차..hyeonnii.tistory.com ​ 학습의 진행사항을 확인하고 싶다면, tensorboard에서 학습내용을 그래픽으로 확인시켜주므로 육안으로 확인할 수 있으니 한 번 확인하는 것이 도움이 된다. 새로운 CMD 창을 열어 해당 명령을 실행한다. > tensorboard --logdir=경로 --host=127.0.0.1 --logdir : train.py의 학습 저장 경로 ( train.py를 실행할 때 입력한 --train_dir의 경로 ) url을 복사하여 브라우저에 입력해 주면 해당 html이 생성되며, 학습에 대한 정보를 그래프로 나타내어준다.  Step 4. tflite 생성​ 어느 정도 학습한 model의 학습내용을 Android Studio에 적용을 하기 위해서 설치경로>models-master>research>object_detection로 이동한 후  훈련 체크 포인트 파일을 고정된 그래프로 내보내 나중에 학습을 전달하거나 직선 추론에 사용할 수 있도록 export_tflite_ssd_graph.py를 실행하여 변환한다.​ >python object_detection/export_tflite_ssd_graph.py --pipline_config_path= pipeline.config 경로 (ssd_mobilenet_v2 설치경로에 존재) --trained_checkpoint_prefix= checkpoint 경로 ( train.py의 --train_dir 경로에 존재 ) --output_directory= convert된 tflite 파일의 출력 경로 (저장 위치) --add_postprocession_op=true​[참조] https://www.python2.net/questions-1054328.htm tensorflow - export_tflite_ssd_graphpy 이해홈 > 2020-09-23 12:57 source tensorflow - export_tflite_ssd_graphpy 이해 여기 는 export_tflite_ssd_graph.py ,이 맞춤 스크립트는 tf.image.non_max_suppression 를 지원하는 데 사용됩니다.  작업. export CONFIG_FILE=gs://${YOUR_GCS_BUCKET}/data/pipeline.config export CHECKPOINT_PATH=gs://${YOUR_GCS_BUCKET}/train/model.ckpt-2000 ex...www.python2.net ​ ​ tflite_graph.pb가 생성이 되었다면, 안드로이드에서 사용할. tflite 파일로 변환만 하면 이제 안드로이드에서 사용할 Object_detection 학습 파일이 준비된다. .tflite로 변환하기 위해서는 tflite_convert.py를 실행한다.  > tflite_convert     --graph_def_file=tflite_graph.pb     --output_file=detect.tflite     --output_format=TFLITE     --input_shapes=1,300,300,3     --input_arrays=normalized_input_image_tensor     --output_arrays='TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1',                                           'TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3' (위에 가 안되면 아래의 --output_arrays를 입력)    // --output_arrays=TFLite_Detection_PostProcess, TFLite_Detection_PostProcess:1,                                          TFLite_Detection_PostProcess:2, TFLite_Detection_PostProcess:3     --inference_type=QUANTIZED_UINT8     --mean_values=128     --std_dev_values=127     --change_concat_input_ranges=false     --allow_custom_ops​ input_file : tflite_graph.pb의 위치 (이전 과정의 Output_path의 경로)  output_file : 변환될. tflite 파일의 저장 위치 output_format : tensorflow Lite 파일 형식으로 생성 input_shpate : [1, 300, 300, 3] shape를 명시해 줌. 맨 앞을 1로 한다는 것은 1장의 32x32 RGB(3채널) 이미지를 input으로 한다는 의미 intput_array : input node 이름  output_array : output node 이름 inference_type : FLOAT or QUANTIZED_UINT8 ​ ​Step 5. Android Studio 예제 사용​ [설치] https://github.com/tensorflow/examples tensorflow/examplesTensorFlow examples. Contribute to tensorflow/examples development by creating an account on GitHub.github.com ​ 설치를 진행한 이후에 examples-master>examples-master>lite>examples>object_detection 경로의 android 디렉터리를 열어준다. SDK와 Gradle 업데이트 내용이 나오면 업데이트를 진행한다.   [ build.gradle 수정 ]​ line 10 : tragetSdkVersion 28로 변경 line 52 : 주석 처리 수정 이후 상단의 Sync Now를 클릭​  [ labelmap.txt 수정 ]​ assets 폴더에 존재하는 labelmap.txt의 내용을 detect를 진행할 클래스명을 써주면 된다.  ​ [ .tflite 삽입 ]​ 변경 이후에 이전에 생성한 tflite 파일을 assets 폴더에 복사 붙여넣기를 진행한다. 만약 detect.tflite 파일을 선택할 때, 아래 사진과 같은 결과가 나온다면, tflite 파일에 metadata를 추가를 진행해야 한다.  아래의 파일을 설치하여 metadata를 추가한다. 첨부파일metadata_writer_for_object_detection.egg파일 다운로드  > metadata_writer_for_object_detection.py     --model_file=경로/detect.tflite     --label_file=경로/labelmap.txt     --export_directory=출력 경로  ​ Windows fatal exception: access violation  에러가 발생한다면 --export_directory 경로에 입력한 폴더가 생성한 후에 다시 실행하거나 labelmap.txt 내용 확인 metadata가 추가된 detect.tflite를 다시 Android project의 assets 폴더에 다시 넣는다.   Add meta data라는 내용이 사라지면 성공한 것이다. 이제 USB 케이블을 연결하여 앱을 휴대폰에 설치하면 끝이다.​ [참조] https://m.blog.naver.com/PostView.nhn?blogId=beaqon&logNo=221076390196&proxyReferer=https:%2F%2Fwww.google.com%2F 안드로이드 스튜디오 핸드폰 연결 방법안드로이드 스튜디오를 사용하여 앱을 만들다 보면 실제 핸드폰으로 테스트해보고 싶다는 생각이 들것이다....m.blog.naver.com [ 최종 결과 ]  ​ "
EfficientDet ： Scalable and Efficient Object Detection Review ,https://blog.naver.com/phj8498/222109941919,20201008,"https://hoya012.github.io/blog/EfficientDet-Review/ EfficientDet ： Scalable and Efficient Object Detection Review“EfficientDet： Scalable and Efficient Object Detectionhoya012.github.io ​안녕하세요, 이번 포스팅에서는 이틀 전 11월 20일에 공개된 논문인 “EfficientDet: Scalable and Efficient Object Detection” 논문에 대한 리뷰를 수행하려 합니다.이 논문은 제가 지난번에 리뷰했던 “EfficientNet: Improving Accuracy and Efficiency through AutoML and Model Scaling” 의 저자들이 속한 Google Brain 팀에서 쓴 논문이며, EfficientNet이 Image Classification 문제를 타겟으로 논문을 작성하였다면, 이번 EfficientDet은 제목에서 알 수 있듯이 Object Detection 문제를 타겟으로 논문을 작성하였습니다.EfficientNet의 내용과 겹치는 내용이 많기 때문에 원활한 이해를 위해서는 EfficientNet 리뷰 포스팅을 먼저 보고 오시는 것을 권장드립니다.마찬가지로, 논문을 소개 드리기 앞서 이 논문의 결과를 먼저 보여드리고 시작을 하도록 하겠습니다. [EfficientDet 실험 결과]Object Detection을 주제로 한 논문이 굉장히 많이 나왔는데, AutoML을 이용하여 찾은 Feature Pyramid Network 구조와, AutoML로 찾은 굉장히 큰 Backbone Architecture인 AmoebaNet을 섞어서 사용한 것이 COCO 데이터셋에서 가장 좋은 성능을 보이고 있었습니다. 이러한 모든 모델들의 성능을 크게 상회하는, 특히 연산량, 연산 속도 관점에서는 굉장히 효율적인 모델을 제안을 하였다는 점이 인상깊었습니다. 이제 어떻게 이러한 좋은 성능을 달성할 수 있었는지 설명 드리도록 하겠습니다. Main Challenge and SolutionSpeed와 Accuracy는 Trade-Off 관계를 가지기 때문에 높은 정확도와 좋은 효율을 동시에 잡기는 굉장히 어려운 일입니다. 이러한 두마리 토끼를 잡기 위해선 모델을 굉장히 잘 설계하여야 합니다. 본 논문에서는 Object Detection에서 이러한 모델을 설계하기 위해 고려하여야 할 점 중 크게 2가지를 Challenge로 삼았습니다.Challenge 1. Efficient multi-scale feature fusionFeature Pyramid Network(FPN)는 2017년 공개된 이후 대부분의 Object Detection 연구에서 사용되고 있습니다. One-Stage Detector의 대표격인 모델인 RetinaNet, M2Det, AutoML의 Neural Architecture Search를 FPN 구조에 적용한 NAS-FPN 등 FPN을 적용하고, 성능을 개선하고자 하는 연구들이 많이 진행이 되어왔습니다. 하지만 선행 연구들은 모두 서로 다른 input feature들을 합칠 때 구분없이 단순히 더하는 방식을 사용하고 있음을 지적하였습니다.서로 다른 input feature들은 해상도가 다르기 때문에 output feature에 기여하는 정도를 다르게 가져가야 함을 주장하며, (단순히 더하면 같은 weight로 기여하게 됨) 간단하지만 효과적인 weighted bi-directional FPN(BiFPN) 구조를 제안하였습니다. 이 구조를 사용하면 서로 다른 input feature들의 중요성을 학습을 통해 배울 수 있으며, 이를 통해 성능을 많이 향상시킬 수 있었습니다. 자세한 구조는 아래에서 다루도록 하겠습니다.Challenge 2. Model scalingEfficientNet에서 제안한 Compound Scaling 기법은 모델의 크기와 연산량를 결정하는 요소들(input resolution, depth, width)을 동시에 고려하여 증가시키는 방법을 의미하며, 이 방법을 통해 높은 성능을 달성할 수 있었습니다. 이러한 아이디어를 Object Detection에도 적용을 할 수 있으며, backbone, feature network, box/class prediction network 등 모든 곳에 적용을 하였습니다.Main Contribution즉 이 논문에서 제안하는 핵심 내용은 크게 2가지이며, BiFPN과 Model Scaling을 적용하여서 COCO 데이터셋에서 가장 높은 정확도를 달성하였고, 기존 연구들 대비 매우 적은 연산량(FLOPS)으로 비슷한 정확도를 달성할 수 있음을 보여주고 있습니다. 본 논문의 Contribution을 정리하면 다음과 같습니다.Weighted bidirectional feature network (BiFPN)을 제안하였다.Object Detection에도 Compound Scaling을 적용하는 방법을 제안하였다.BiFPN과 Compound Scaling을 접목하여 좋은 성능을 보이는 EfficientDet 구조를 제안하였다.이제 각 핵심 내용들을 자세히 설명드리도록 하겠습니다. BiFPNCross-Scale ConnectionsFeature Pyramid Network를 이용한 방법들을 모아둔 그림은 다음과 같습니다. [Feature Network Design]여기에서 (a) 방식이 전통적인 FPN 구조를 의미하고, (b) PANet은 추가로 bottom-up pathway를 FPN에 추가하는 방법을 제안하였습니다.(c)는 AutoML의 Neural Architecture Search를 FPN 구조에 적용하였고, 불규칙적인 FPN 구조를 보이는 것이 특징입니다. 또한 (a)와 (b) 구조는 같은 scale에서만 connection이 존재하지만, (c) 구조부터는 scale이 다른 경우에도 connection이 존재하는 Cross-Scale Connection 을 적용하고 있습니다.(d)와 (e)는 본 논문에서 추가로 제안하고 실험을 한 방식이고, 마지막 (f) 방식이 본 논문에서 제안하고 있는 BiFPN 구조를 의미합니다.(e) Simplified PANet 방식은 PANet에서 input edge가 1개인 node들은 기여도가 적을 것이라 생각하며 제거를 하여 얻은 Network 구조를 의미하고, 여기에 (f) 그림의 보라색 선처럼 같은 scale에서 edge를 추가하여 더 많은 feature들이 fusion되도록 구성을 한 방식이 BiFPN입니다. 또한 PANet은 top-down과 bottom-up path를 하나만 사용한 반면, 본 논문에서는 이러한 구조를 여러 번 반복하여 사용을 하였습니다. 이를 통해 더 high-level한 feature fusion을 할 수 있음을 주장하고 있습니다. [BiFPN의 성능 향상 분석]BiFPN의 성능 향상을 살펴보기 위해 2가지 ablation study를 하였습니다. Table 3에서 저자는 같은 backbone인 EfficientNet-B3에서 FPN을 BiFPN으로 바꿨을 때의 성능을 측정하였고, mAP는 약 4.1 증가하였고 parameter 수와 FLOPS도 적게 사용하고 있음을 보여주고 있습니다. Table 4에서는 위에서 보여드렸던 여러 Feature Network 방식들에 따라 성능이 어떻게 바뀌는지를 분석한 결과이며 BiFPN을 사용하였을 때 가장 좋은 성능을 보임을 확인할 수 있습니다.Weighted Feature FusionFPN에서 서로 다른 resolution의 input feature들을 합칠 때, 일반적으로는 같은 해상도가 되도록 resize를 시킨 뒤 합치는 방식을 사용합니다. 하지만 앞에서 말씀드렸듯이 모든 input feature들을 동등하게 처리를 하고 있는 점을 문제점으로 인식하고, 본 논문에서는 이 점을 개선하기 위해 각 input feature에 가중치를 주고, 학습을 통해 가중치를 배울 수 있는 방식을 제안하였습니다. 총 3가지 방식을 제안하고 있으며 각 방식을 하나의 그림으로 정리하면 다음과 같습니다. [Weighted Feature Fusion]우선 weight는 scalar (per-feature)로 줄 수 있고, vector (per-channel)로 줄 수 있고 multi-dimensional tensor (per-pixel)로 줄 수 있는데, 본 논문에서는 scalar를 사용하는 것이 정확도와 연산량 측면에서 효율적임을 실험을 통해 밝혔고, scalar weight를 사용하였습니다.Unbounded fusion은 말 그대로 unbounded 되어있기 때문에 학습에 불안정성을 유발할 수 있습니다. 그래서 weight normalization을 사용하였다고 합니다.SoftMax-based fusion은 저희가 익히 알고 있는 SoftMax를 사용한 것이지만, 이는 GPU 하드웨어에서 slowdown을 유발함을 실험을 통해 보여주고 있습니다.그래서 본 논문은 Fast normalized fusion 방식을 제안하였습니다. 우선 weight들은 ReLU를 거치기 때문에 non-zero임이 보장이 되고, 분모가 0이 되는 것을 막기 위해 0.0001 크기의 입실론을 넣어주었습니다. Weight 값이 0~1사이로 normalize가 되는 것은 SoftMax와 유사하며 ablation study를 통해 SoftMax-based fusion 방식보다 좋은 성능을 보임을 보여주고 있습니다. [Weighted Feature Fusion Ablation Study 결과]위의 표 5는 SoftMax fusion과 Fast Fusion을 비교한 결과이며, Fast Fusion을 사용하면 약간의 mAP 하락은 있지만 약 30%의 속도 향상을 달성할 수 있습니다. 또한 그림 5를 보시면, input 1과 input 2의 weight를 training step에 따라 plot한 결과인데, 학습을 거치면서 weight가 빠르게 변하는 것을 보여주고 있고, 이는 각 feature들이 동등하지 않게 output feature에 기여를 하고 있음을 보여주고 있으며, Fast fusion을 사용하여도 SoftMax fusion과 양상이 비슷함을 보여주고 있습니다. EfficientDet위에서 설명드린 BiFPN을 기반으로 EfficientDet 이라는 One-Stage Detector 구조를 제안하였습니다.EfficientDet ArchitectureEfficientDet의 backbone으로는 ImageNet-pretrained EfficientNet을 사용하였습니다. BiFPN을 Feature Network로 사용하였고, level 3-7 feature에 적용을 하였습니다. 또한 top-down, bottom-up bidirectional feature fusion을 반복적으로 사용하였습니다. [EfficientDet Architecture]EfficientDet Architecture는 위의 그림과 같습니다. 별다른 특이점이 없어서 그림만 봐도 쉽게 이해하실 수 있습니다.Compound ScalingBackbone network에는 EfficientNet-B0 부터 B6까지 사용을 하였으며 마찬가지로 ImageNet-pretrained network를 가져와서 사용을 하였습니다. 실험에 사용한 Compound Scaling configuration은 아래 그림에서 확인하실 수 있습니다. [Compound Scaling for EfficientDet]저희가 알던 Compound Scaling처럼 input의 resolution과 backbone network의 크기를 늘려주었고, BiFPN과 Box/class network 도 동시에 키워주는 것을 확인하실 수 있습니다. 각 network마다 어떻게 키워줬는지는 위의 그림의 (1) ~ (3)에서 확인하실 수 있습니다. 실험 결과이제 위에서 설명 드린 EfficientDet 구조가 기존 논문들의 방식 대비 얼마나 성능이 좋은지를 보여드릴 차례입니다. 사실 맨 처음 이미 결과를 보여드려서 다들 예상을 하고 계실 것이라 생각합니다. [EfficientDet COCO 데이터셋 성능]역시나 COCO 데이터셋에서 가장 높은 mAP를 달성하여, 2019년 11월 기준 State-of-the-art(SOTA) 성능을 보이고 있으며, 기존 방식들 대비 연산 효율이 압도적으로 좋음을 확인할 수 있습니다. [EfficientDet의 Model Size, Inference Latency 비교]모델의 크기, 즉 parameter 수와, Inference Latency를 GPU와 CPU에서 측정한 실험 결과도 이 논문의 위력을 잘 보여주고 있습니다. 단순히 FLOPS가 적다고 해서 항상 Inference Latency가 적다는 보장이 없기 때문에 실제 Inference Latency를 보여주고 있고, 이러한 결과는 참 유용한 것 같습니다. 그림을 보시면 아시겠지만 정확도도 높고, 모델 크기도 작고, Latency도 낮아서 빠르게 동작할 수 있음을 보여주고 있습니다. 논문에서는 정확히 어떤 셋팅에서 학습을 시켰는지에 대한 자세한 내용도 확인하실 수 있습니다. 결론이번 포스팅에서는 이틀 전 11월 20일에 공개된 논문인 “EfficientDet: Scalable and Efficient Object Detection” 에 대해 자세히 리뷰를 해보았습니다. Object Detection도 굉장히 다양한 논문들이 2018년을 기점으로 쏟아져 나왔었는데 단숨에 서열을 정리해버린 점도 인상깊었고, 성능도 성능이지만 Latency 관점에서도 너무 효율적이라 COCO 데이터셋같이 연구용 데이터셋 뿐만 아니라, 실제 데이터셋, 실제 제품, 실제 application에서도 충분히 적용해봄직한 논문이라고 생각이 들었습니다. Object Detection에도 EfficientNet을 적용하였으니, 조만간 Semantic Segmentation, Instance Segmentation에도 EfficientNet을 적용한 EfficientSeg? 같은 논문이 나오지 않을까 예상해보면서 글을 마무리짓겠습니다. 감사합니다! "
Object Detection(객체 탐지) - 오브젝트디텍션 ,https://blog.naver.com/85honesty/222734199790,20220516,2021년 12월말에 대구AI허브에 가서 잭슨나노 X 오브젝트디텍션을 경험해봤습니다.​ 너무나 신세계였던 오브젝트 디텍션물체를 인식한다는 것이 신기 했습니다. 놓여져 있는 물체가 아닌 내가 들어서 카메라에 인식 시켜 봤습니다. 카메라에 초점에 따라 안 인식되는 경우도 있고 아무래도 영역이 감지가 덜 되어서 그런 것 같습니다. 지금은 같은 사진의 각도 였지만 인식을 되는 것을 볼 수 있습니다.​* 데이터가 있는 물체만 인식되기 때문에 더 다양한 인식을 원한다면- 추가적인 데이터를 구축해야 인식이 가능합니다. 위와 같이 카메라를 움직여서 물체를 인식하는 모습입니다.​다음 포스팅도 기대해주세요  대구에서 하는 오프라인강의여서수원에서 왔다갔다 하는 시간이 많이 걸려서숙박을 하면서 3일간 추가적으로 2일간총5일에 걸쳐서 2개의 프로그램에 참가해서 오브젝트 디텍션을 해봤습니다.  [쿠키멘트]교수님께서 너무 잘 알려주셔서 많은 것을 배울 수 있었습니다.그리고 같이 배우는 분들의 많은 도움으로 더 많은 것을 배웠습니다.이 자리를 빌려서 다시 한번 감사의 말을 전합니다.  [Reference]대구ai허브  dailyman데일리남매일매일 좋은 추억 한두개씩 함께 만들어요😀 선한영향력 긍정적인변화 즐거운하루 같이 가치있게 만들어 가요😄www.youtube.com  구독과 공감 좋아요는 콘텐츠를 만드는 데 큰 힘이 됩니다. 
Object Detection 논문 흐름 ,https://blog.naver.com/keeping816/221637576598,20190903,"* Object Detection이란   여러 물체에 대해 어떤 물체인지 분류하는 classification + 그 물체가 어디 있는지 네모 박스를 통해 (Bounding box) 위치 정보를 나타내는 Localization 둘 다 해내야 한다.​즉, Object Detection = Multi-Labeled Classification + Bounding Box Regression(Localization)​​* Object Detection 활용분야자율주행자동차, CCTV Surveillance, 스포츠경기, 무인 점포 등​​* 1-stage Detector, 2-stage Detector- 2-stage Detector: Regional Proposal과 Classification이 순차적으로 이루어짐   Localization과 Classification을 순차적으로 행한다, 비교적 느리지만 정확도가 높다​- 1-stage Detector: Regional Proposal과 Classification이 동시에 이루어짐   Localization 문제와 Classification 문제를 동시에 행한다, 비교적 빠르지만 정확도가 낮다​​* Computer Vision Object Detection 논문 흐름(2019 기준)   ​* R-CNN: 딥러닝을 이용한 첫 2-stage detector   2-stage detector R-CNN계열의 선두주자, object detection 분야에 최초로 deep learning(CNN)을 적용시킨 R-CNN 논문:https://arxiv.org/pdf/1311.2524.pdf​​* R-CNN (CVPR 2014)Rich Feature hierarchies for accurate object detection and semantic segmentation​2012년 ILSVRC(Image Net 대회)에서 AlexNet이 세상에 공개된 이후 CNN은 이미지 분류(classification) 분야에 있어서 당연하게 사용되는 표준처럼 되었다. CNN이 이미지 분류(Classification) 분야에서 엄처난 성적을 거두었어도 Object detection 분야에 바로 적용되지는 못했다.​* Classification vs Object Detectionclassification은 한 개의 객체가 그려져 있는 이미지가 있을 때 이 객체가 무엇인지 알아내는 문제object detection 알고리즘은 이미지 내에 관심이 있는 객체의 위치(region of interest)에 물체의 위치를 알려주기 위한 bounding box를 그려주고, 다수의 bounding box를 다양한 객체 종류에 대해 찾아줘야 하기 때문에 classification보다 훨씬 복잡한 문제이다.​-> 2014년 R-CNN의 등장으로 CNN을 Object Detection 분야에 최초로 적용, Object detection 분야에서도 높은 수준의 성능을 이끌어 낼 수 있다고 보여줬다​   R-CNN 패밀리들, 모두 R-CNN 구조를 바탕으로 ​- 논문에서의 R-CNN 구조   1. 이미지를 input으로 집어넣는다 -> 2-1. 2000개의 영역(Bounding Box)를 Selective Search 알고리즘을 통해 추출하여 잘라낸다(Cropping) -> 2-2.이를 CNN 모델에 넣기 위해 같은 사이즈(277x277 pixel size)로 찌그러뜨린다(warping) -> 3. 2000개의 warped image를 각각 CNN 모델에 집어넣는다 -> 4. 각각 classification을 진행하여 결과를 도출한다​--> R-CNN은 2-stage detector로써 전체 task를 두 가지 단계로 나누어 진행한다: Region Proposal(물체의 위치 찾기) + Region Classification(물체 분류)​이 논문에서는 위 task들을 행하기 위해 구조를 총 세가지 모듈로 나누어놓았다.1. Regional proposal - 카테고리와 무관하게 물체의 영역 찾는 모듈2. CNN - 각각의 영역으로부터 고정된 크기의 feature vector를 뽑아내는 large convolutional neural network3. SVM - Classification을 위한 선형 지도학습 모델 Support Vector Machine(SVM)​1) Regional proposal   1. Regional proposal R-CNN은 Regional Proposal 단계에서 Selective Search라는 알고리즘을 이용한다. Selective Search 알고리즘은 Segmentation 분야에서 많이 쓰이는 알고리즘이며, 객체와 주변간의 색감(Color), 질감(Texture) 차이, 다른 물체에 애워쌓여있는지(Encolosed) 여부 등을 파악해서 다양한 전략으로 물체의 위치를 파악할 수 있도록 하는 알고리즘이다.​   Selective Search 알고리즘은 위의 그림처럼 Bounding box들을 random하게 많이 생성을 하고 이들을 조금씩 merge 해나가면서 물체를 인식해나가는 방식으로 되어있다.물체의 위치를 파악하기 위한 알고리즘으로, 비교되어 많이 설명되는 방법에는 sliding window 방법이 있다.(selective search 논문: http://www.huppelen.nl/publications/selectiveSearchDraft.pdf)​R-CNN에서는 Selective search 알고리즘을 통해 한 이미지에서 2000개의 region을 뽑아내고, 이들을 모두 CNN에 넣기 위해 같은 사이즈로 찌그려트려 통일시키는 작업을 거친다. (warping)​2) CNN (Convolutional Neural Network)   앞선 selective search를 통해 생성된 2000개의 227x227 pixel size로 warping된 이미지를 각각 CNN에 넣어준다.AlexNet Network의 마지막 부분을 Detection을 위한 Class 수 만큼 바꾸고 Object Detecton용 Dataset을 집어넣어 Fine-tuning 진행각각의 region proposal로부터 4096-dimensional feature vector를 뽑아내고, 이는 Fixed-length Feature Vector를 만들어낸다​​3) SVM(Support Vector Machine)   CNN 모델로부터 Feature가 추출이 되고 Training Label이 적용되고 나면, SVM을 이용하여 classification을 진행한다. (Category-Specific Linear SVMs)R-CNNdms Classifier로 Softmax를 쓰지 않고 SVM을 사용하였는데, 이는 Softmax에서 성능이 잘 나오지 않고 SVM에서의 성능이 더 나았기 때문이다.SVM(Support Vector Machine)은 CNN으로부터 추출된 각각의 Feature Vector들의 점수를 Class별로 매기고, 객체인지 아닌지, 객체라면 어떤 객체인지 등을 판별하는 역할을 하는 classifier이다.​​3-1) Bounding box regression논문에서의 bBox regselective search로 만들어낸 bounding box는 완전히 정확하지는 않기 때문에 물체를 정확히 감싸도록 조정해주는 선형회귀 모델(Bounding Box Regression)을 넣었다​​* R-CNN의 단점1. 오래걸린다R-CNN은 Selective Search에서 뽑아낸 2000개의 영역 이미지들에 대해 모두 CNN모델에 때려 박는다. 당연히 오래 걸릴 수 밖에 없다.Training time: 84 hours, Testing time GPU k40 사용 기준으로 frame당 13초, CPU를 사용하였을 때 frame당 53초가 걸린다.​2. 복잡하다Multi-satge training 실행한다​3. Back propagation이 안된다R-CNN은 Multi-satge training 수행, SVM, Bounding box egression에서 학습한 결과가 CNN를 업데이트 시키지 못한다​​* R-CNN 논문의 의미​최초로 Object Detection에 Deep Learning 방법인 CNN을 적용시켰다는 점과 이후 2-stage detector들의 구조에 막대한 영향을 끼쳤다​​* CenterNet - One-stage detectorReal-Time Task를 요구하는 Object Detection 문제를 해결할 때는 주로 YOLO(You Only Look Once)를 많이 사용하는데, 더 빠른 성능(FPS)를 가지는 Real-Time Detector로 CenterNet이라는 것이 있다고 한다.​   ​* CenterNet (Objects as Points)CenterNet 논문은 Objects as Points, CenterNet: Keypoint Triplets for Object Detection이 있는데 여기서 리뷰하는 논문은 Object as Points 이다.​논문, 코드링크: https://arxiv.org/pdf/1904.07850v2.pdf, https://github.com/xingyizhou/CenterNet xingyizhou/CenterNetObject detection, 3D detection, and pose estimation using center point detection:  - xingyizhou/CenterNetgithub.com CenterNetdms Anchor box를 사용하는 기존의 one-stage Detector(RetinaNet, SSD, YOLO) 들과 비슷한 접근방식을 보이지만 극명한 차이가 있다.​차이점 1) CenterNet은 box overlap이 아닌 오직 위치만 가지고 ""Anchor""를 할당한다차이점 2) CenterNet은 오직 하나의 ""Anchor""만을 사용한다차이점 3) CenterNet은 더 큰 output resolution (output stride of 4)를 가진다​이전의 one-stage detector들은 대부분 많은 수의 anchor box들을 사용하여 최종 bounding box들을 유추해 내었다   RetinaNet의 경우에는 100k개가 넘는 Anchor box들을 사용하였고, 이는 할당된 Anchor box가 실제값인 ground truth box와 충분히 overlap 될 수 있게 하기 위함이었다. (box overlap)​   CenterNet과 기존 Anchor-based detection과 다른점 하지만 이렇게 많은 anchor box를 사용하게 되면, 정확도는 높아지겠지만 positive anchor box와 negative anchor box 사이의 불균형을 만들게 되고 이는 training 속도를 늦추게 된다. 또한 많은 수의 anchor box는 어떤 사이즈로 할지, 어떤 비율로 할지, 등 많은 수의 hyperparameter와 많은 수의 선택지를 만들어 낸다.​CenterNet은 Key point estimation을 사용하여 고정적이지 않은 단 하나의 anchor를 사용하는 방법을 소개한 바 있다.​​* Keypoint Estimation for Object DetectionCenterNet은 단 하나의 Anchor를 Keypoint Estimation을 통해서 얻어낼 수 있고, 이러한 방법은 CornerNet에서 처음으로 소개된 바 있다​CoverNet(ECCV 2018), https://arxiv.org/pdf/1808.01244.pdfKeyPoints로 왼쪽 위, 오른쪽 아래, 두 개의 모서리를 Detect하여 Bounding box를 얻어낸다.   CornerNet 원리 ExtremeNet(CVPR 2019), https://arxiv.org/pdf/1901.08043.pdfKeyPoints로 top-most, left-most, bottom-most, right-most, center 점들을 Detect하여 Bounding box를 얻어낸다.   ExtremeNet 원리 CornetNet, ExtremeNet 둘 다 CenterNet과 같은 robust한 keypoint estimation network에서 build 된다.단, keypoint들에 대한 grouping 과정이 필요하며 이는 알고리즘 속도를 늦춘다.​CenterNet은 물체마다 단 하나의 keypoint인 중심점(Center Point)를 Estimate한다. 이로써 각 물체들은 모두 하나의 점(Key point)로 표현이 된다. 따라서 grouping 과정이나 post-processing 과정들이 필요없게 되고 단 하나의 anchor를 가지게 된다.​또한 예측된 중심점으로부터 CenterNet은 Object size, Dimension, 3D extent, Orientation, Pose 등 다양한 정보를 regress하여서 Object Detection 뿐만 아니라 3D object detection과 multi-person human pose estimation으로 쉽게 확장할 수 있다. ​   1행: Object Detection을 위한 정보, 2행: 3D Object Detection을 위한 정보, 3행: Pose Estimation을 위한 정보 ​​* Keypoint EstimationKey estimation은 pose estimation 분야에서 주로 많이 쓰이는 방법이다. CenterNet의 구체적인 원리는 마지막 레퍼런스에 잘 설명되어 있다 ...!(내용이 조금 복잡한건 안비밀...)​​​출처: https://nuggy875.tistory.com/20 [Object Detection]  1. Object Detection 논문 흐름 및 리뷰Deep Learning 을 이용한 Object Detection의 최신 논문 동향의 흐름을 살펴보면서 Object Detection 분야에 대해서 살펴보고, 구조가 어떤 방식으로 되어있으며 어떤 방식으로 발전되어 왔는지 살펴보고자 합니다..nuggy875.tistory.com https://nuggy875.tistory.com/21?category=860935 [Object Detection] 2. R-CNN : 딥러닝을 이용한 첫 2-stage Detector저번 포스팅에서는 Object Detection의 전체적인 흐름에 대해서 알아보았습니다. [Object Detection] 1. Object Detection 논문 흐름 및 리뷰 Deep Learning 을 이용한 Object Detection의 최신 논문 동향의 흐름..nuggy875.tistory.com https://nuggy875.tistory.com/34?category=860935 [Object Detection] CenterNet (Objects as Points) 논문 리뷰지금까지 Real-Time Task를 요구하는 Object Detection 문제를 해결할 때는 주로 YOLO(You Look Only Once)를 사용하였습니다. 최근 빠른 성능(FPS)을 가지는 Detector를 요구하는 프로젝트를 진행하게 되어 YOLO..nuggy875.tistory.com ​ "
"[논문 읽기] You Only Look Once: Unified, Real-Time Object Detection ",https://blog.naver.com/rbdus0715/222896334208,20221010," https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Redmon_You_Only_Look_CVPR_2016_paper.pdf 욜로 알고리즘 소개줄여서 Yolo라고 부르는 이 객체 탐지 알고리즘은 이미지 전체에 대해서 한 번의 계산만으로 객체의 위치와 클래스의 확률을 예측합니다.​객체를 탐지하기 위한 알고리즘 중에서 다른 알고리즘으로 다음 두 가지 알고리즘이 있습니다.deformable parts models(DPM) - sliding windowR-CNN sliding window위 단점(왼쪽) | R-CNN의 작동방식(오른쪽) 먼저 sliding window를 사용한 DPM 알고리즘입니다.이 알고리즘은 고정된 사이즈의 window를 움직이며 객체가 그 안에 있는지 확인하는 알고리즘입니다. 왼쪽 사진)하지만 이 방식은 그 윈도우가 객체를 잘 포함하지 않는 상황이 발생할 수 있습니다. 그렇다고 해서 윈도우 사이즈를 크게하고 stride를 작게 하여 더 많이 확인하자니 계산 수가 증가합니다.그 다음으로 R-CNN 알고리즘 입니다. (오른쪽 사진)이 알고리즘은 원래 사진에서 수천개 정도의 잘려진 사진을 Conv Net에 훈련시켜 분류하는 모델로 여러 과정이 있어 시간이 오래 걸릴 뿐더러, 분류하기 위해 잘라진 사진을 후처리도 해야하는 단점이 있습니다.이를 해결하기 위해 yolo라는 알고리즘이 대안으로 제시되었습니다. 욜로 알고리즘은 위에서 말했듯이 한 번의 연산으로 객체의 위치와 classification을 수행하는 one stage 객체 탐지 알고리즘입니다. 이 논문에서는 객체 탐지와 클래스 분류를 하나의 회귀 문제로 정의하여 간단한 프로세스로 개선하였습니다. 욜로 알고리즘의 장점 굉장히 빠릅니다.객체를 탐지하는 것은 원래 복잡한 과정이었지만 이것을 하나의 회귀 문제로 생각함으로서 단순한 프로세스가 되었습니다.​예측을 할 때 이미지 전체를 봅니다.이전 모델처럼 일부분을 잘라서 학습하는 방법과 달리 사진 한장을 전체적으로 보기 때문에 주변 정보도 같이 학습합니다.이로인해 background error가 예전 모델에 비해 훨씬 적었습니다.​욜로는 몰체의 general한 부분을 학습합니다.그래서 새로운 데이터에 더 좋은 성능이 뛰어납니다.​하지만 최근 모델인 SOTA에 비해 정확성이 떨어집니다.빠른 대신 작은 객체에 대해 정확성이 떨어지는 모습을 보였습니다.​​​ 세부 네트워크 원리와 구조, 훈련<원리>욜로는 인풋 이미지를 S*S 그리드로 나눕니다. 만약 어떤 객체의 중심점이 특정 그리드 셀에 위치하면, 그 그리드 셀이 해당 객체를 검출합니다. 각각의 그리드 셀은 B개의 bounding box와 그 bounding box에 대한 confidence scores를 예측합니다.여기에서 confidence scores는 박스가 객체를 포함한 다는 것이 얼마나 믿을만 한 것인지, 그리고 예측한 bounding box가 얼마나 정확한지를 알려줍니다. bounding box에 특정 클래스가 나타날 확률 * 바운딩 박스가 그 클래스에 얼마나 잘 맞는지 (IOU)IOU 예시) 두 개의 바운딩 박스일 때, IOU = A∩B/ AUB​각각의 바운딩 박스는 5개의 예측치로 구성되어있습니다. x, y, w, h, confidence(x, y) : 바운딩 박스 중심의 그리드 셀 내에서의 상대 위치로 0에서 1값을 가짐(w, h): 상대 너비와 상대 높이confidence : confidence scores​<구조>  하나의 CNN 구조로 디자인되어 있습니다.앞 단의 컨볼루션 계층은 이미지로부터 특징을 추출하고, 전결합 계층은 클래스 확률과 바운딩 박스의 좌표를 예측합니다.이 신경망의 구조는 이미지 분류에 사용되는 googleNet에서 따온 것입니다. (나중에 구글넷 논문도 읽어봐야겠다!)최종 아웃풋은 7*7*30의 예측 텐서입니다.​<훈련>leaky LeLU 신경망의 마지막 계층에는 선형 활성화 함수를 적용하였고, 나머지 모든 계층에는 leaky ReLU를 적용했습니다.​SSE (sum-squared error)SSE를 통해 최적화를 쉽게 할 수 있는 장점이 있지만 여러 문제점이 있습니다. 문제점세부 사항해결 방안SSE를 최적화하는 것이 YOLO의 최종 목적인 mAP를 높이는 것과 완벽하게 일치하지는 않음yolo의 loss에는 bounding box loss(localization loss)와 분류 loss 두가지가 있는데 이 두 가중치를 동일하게 두고 학습시키는 것은 좋은 방법이 아님객체가 존재하는 바운딩 박스 좌표에 대한 loss 가중치를 증가시키고, 객체가 존재하지 않으면 감소시켰다. 이로서 localization loss의 가중치를 증가시키고, 객체가 있는 그리드 셀에 조금 더 힘을 실을 수 있게 되었다.파라미터: λ_coord와 λ_noobj모델의 불균형배경 영역 > 전경 영역으로 대부분의 그리드 셀의 confidence score가 0으로 학습됨SSE는 큰 바운딩 박스와 작은 바운딩 박스에 대해 모두 동일한 가중치로 loss를 계산함바운딩 박스가 작으면 작은 움직임에도 작은 객체를 잘 벗어나게 됨너비와 높이에 루트를 취해줌: 너비와 높이가 커짐에 따라 그 증가율이 감소해 loss에 대한 가중치를 감소시켜줌 ​ 욜로의 한계점그리드 셀마다 오직 하나의 객체만 검출할 수 있습니다.훈련 단계에서 학습하지 못했던 새로운 가로 세로 비율을 마주하게 되면 잘 예측하지 못할것임큰 바운딩 박스와 작은 바운딩 박스의 loss에 대해 동일한 가중치를 둔다는 점    - 크기가 작은 박스는 위치 변화에 띠른 IOU 변화가 더 심하기 때문 "
"[TX2] Object detection (YOLOv3 , YOLOv3-tiny) based on darknet ros ",https://blog.naver.com/ehdgml9997/221997451770,20200611,Embedded : Jetson TX2camera : Intel D435based on ROSframework : darknet​TX2 Condition-MAXN 0 ($ nvpmodel -m 0)  Object detection Architecture : YOLOv3-tiny  image input (256x256)avg speed : 40 fps* camera has 30 fps. so i had the result of speed between 20~44 fps  ​image input (416x416)avg speed : 20 fps   Object detection Architecture : YOLOv3  image input (256x256)avg speed : 8 fps  ​image input (416x416)avg speed : 3 fps  ​ 
Object detection(9) _One-stage 2D object detectors: PANet[CVPR'18] ,https://blog.naver.com/summa911/223003721086,20230203,"PANet의 연구팀은 기존의 BB proposal 방법에는 크게 두 가지의 문제점이 있다고 생각했다.low level feature도 중요한데, conv layer를 거치면서 거의 사라진다는 점이 문제점이었다.그리고 upsampling한 4개의 layer도 각각 장단점이 존재하기 때문에 해당 layer하나만을 가지고 BB를 예측하는 것은 오류가 많았다. low level feature를 4개의 upsampling layer에 적용하기 위해서, shorcut route를 만들어서 적용시켜 주었다. 각 층이 기존 정보에 low level feature까지 추가되었기 때문에 정보가 augmentation 되었다. ​ ​각 층마다 BB를 도출하는 것은 불완전하기 때문에 4개의 계층을 feature pooling하여 하나의 층으로 합치고, BB를 예측하였다. 이를 adaptive feature pooling이라고 한다. ​​[출처] https://youtu.be/AfseiFiz9MI ​ "
[AWS] SageMaker Object Detection 데이터 소스 만들기 - PASCAL VOC format을 이미지 형식 annotation JSON 으로 변환 ,https://blog.naver.com/nan17a/221966706220,20200517,"Sagemaker 의 오브젝트 디텍션 알고리즘을 간단하게 테스트 한 기록을 남깁니다.​https://docs.aws.amazon.com/ko_kr/sagemaker/latest/dg/object-detection.html Object Detection 알고리즘 - Amazon SageMakerObject Detection 알고리즘 PDF Amazon SageMaker Object Detection 알고리즘은 단일 신경망을 사용하여 이미지의 객체를 감지 및 분류합니다. 이 알고리즘은                                    입력으로 이미지를 가져와 이미지 장면 내에서 객체의 모든 인스턴스를 식별하는 지도 학습 알고리즘입니다. 객체는 지정된 모음 내 클래스 중 하나로 범주화되는데,                                    이때 해당 클래스에 속하는 신뢰도 점수를 사용합니다. 이...docs.aws.amazon.com image classification 은 이미지 자체의 분류를 훈련시키지만, object detection은 이미지 내에 특정 오브젝트를 인식하도록 훈련시키는 방법입니다.​학습 데이터 또한 annotation 데이터로 원본 이미지 내에 어떤 카테고리의 이미지가 어떤 위치에 저장되어 있는지에 대한 정보가 이미지와 함께 제공되어야 합니다.​Sagemaker 내의 Ground Thruth 를 이용해서 학습용 데이터를 생성할 수도 있지만흔하게 사용되는 오픈 소스중에 LableImg를 이용한 경우 해당 annotation 정보의 기록 형태가 ""PascalVOC"" 혹은 ""YOLO"" 로 Sagemaker에서 사용하는 annotation 데이터 형태와 달라 이를 변환해서 사용해야 합니다.​[샘플 소스] import jsonimport xml.etree.ElementTree as ETdef xmlToJson(xmlname):        tree = ET.parse(xmlname)        root = tree.getroot()        filename = root.find('./filename').text        width = root.find('./size/width').text        height = root.find('./size/height').text        # print('{}/{}/{}'.format(filename,width,height))        line = {}        line['file'] = filename        line['image_size'] = [{            'width':int(width),            'height':int(height),            'depth':3        }]        line['annotations'] = []        line['categories'] = []        i=0        for object in root.findall('object'):            i += 1            class_name = object.find('name').text            class_index = img_class.index(class_name)            xmin = int(object.find('./bndbox/xmin').text)            ymin = int(object.find('./bndbox/ymin').text)            xmax = int(object.find('./bndbox/xmax').text)            ymax = int(object.find('./bndbox/ymax').text)            line['annotations'].append({                    'class_id':class_index,                    'top':ymin,                    'left':xmin,                    'width':xmax - xmin,                    'height':ymax - ymin                })            try:                line['categories'].index(class_name)            except ValueError :                line['categories'].append({                    'class_id':class_index,                    'name':class_name                })            # print(line)        return line 학습용 데이터는 S3에- train  : 학습용 이미지 저장용- validation  : 학습용 이미지 저장용- train_annotation  : 학습용 annotaion json 저장용- validation_annotation  : 학습용 annotaion json 저장용형태로 upload 후 학습 하면 됩니다.​참고로 Sagemaker는 Object-Detection용 네트워크로 VGG-16,  ResNet-50 두가지를 지원하고 있습니다.​[관련글]https://blog.naver.com/nan17a/221827935600 [머신러닝] Turi Create 를 이용한 이미지 오브젝트 인식Apple 에서 발표한 머신러닝 라이브러리인 Turi Create 를 활용하여, 이미지내 오브젝트 인식 기능 테스...blog.naver.com ​ "
object detection for evaluator in evaluators: TypeError: 'NoneType' object is not iterable ,https://blog.naver.com/alaldi2006/222223897521,20210129,"Object Detection evaluation 도중, 발생한 에러..왜...왜 호환이안돼서..object_detection/model_lib_v2.py 934번째 줄에서 에러가 났길래보니까 아래 구문이었음​     for evaluator in evaluators:      eval_metrics.update(evaluator.evaluate())    for loss_key in loss_metrics:      eval_metrics[loss_key] = tf.reduce_mean(loss_metrics[loss_key]) ​for 구문에서 None Type 에러라니..None 타입 못받게 막아보았다​   if evaluators is not None:     for evaluator in evaluators:      eval_metrics.update(evaluator.evaluate())    for loss_key in loss_metrics:      eval_metrics[loss_key] = tf.reduce_mean(loss_metrics[loss_key])    ~~~    ... 웨이팅 잘함 ㅇ_ㅇ "
50 object detection ,https://blog.naver.com/ooooo0o0oo/223106264211,20230519,오브젝ㅌ 디텤션사물검출 
[ Object Detection ] Training ,https://blog.naver.com/sgkim21/221933675881,20200428,"1. Using Tensorflowhttps://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/training.html Training Custom Object Detector — TensorFlow Object Detection API tutorial  documentationHow to organise your workspace/training files How to prepare/annotate image datasets How to generate tf records from such datasets How to configure a simple training pipeline How to train a model and monitor it’s progress How to export the resulting model and use it to detect objects. Preparing work...tensorflow-object-detection-api-tutorial.readthedocs.io ​2. Using (DIGITS + Tensorflow/Caffe)​1) Install DIGITSDIGITS (the Deep Learning GPU Training System) is a webapp for training deep learning models. The currently supported frameworks are: Caffe, Torch, and Tensorflow. DIGITS puts the power of deep learning into the hands of engineers and data scientists.DIGITS is not a framework. DIGITS is a wrapper for Caffe, Torch, and TensorFlow; which provides a graphical web interface to those frameworks rather than dealing with them directly on the command-line.DIGITS can be used to rapidly train highly accurate deep neural network (DNNs) for image classification, segmentation, object detection tasks, and more. https://docs.nvidia.com/deeplearning/digits/digits-installation/index.html Installing DIGITS :: Deep Learning DIGITS DocumentationInstalling DIGITS                                                                                          ( PDF )                  -                                                                        Last updated October 21, 2019                  - Abstract This guide provides a deta...docs.nvidia.com 2) Training with new imageshttps://github.com/dusty-nv/jetson-inference/blob/master/docs/detectnet-training.md dusty-nv/jetson-inferenceGuide to deploying deep-learning inference networks and deep vision primitives with TensorRT and NVIDIA Jetson. - dusty-nv/jetson-inferencegithub.com ​ "
Lidar Point Pilars 3D object detection ,https://blog.naver.com/brucks1217/222246356826,20210217,"Which is already presented in the Autoware docker, but this has some explanations. https://github.com/yukkysaito/autoware_perception/tree/master/lidar_point_pillars yukkysaito/autoware_perceptionContribute to yukkysaito/autoware_perception development by creating an account on GitHub.github.com Object detection model (*onnx)https://github.com/k0suke-murakami/kitti_pretrained_point_pillars k0suke-murakami/kitti_pretrained_point_pillarsContribute to k0suke-murakami/kitti_pretrained_point_pillars development by creating an account on GitHub.github.com ​https://www.youtube.com/watch?v=sOLCgRkWlWQ https://github.com/tier4/AutowareArchitectureProposal.proj tier4/AutowareArchitectureProposal.projThis is the source code of the feasibility study for Autoware architecture proposal. - tier4/AutowareArchitectureProposal.projgithub.com ​ "
[edureka!] Realtime Object Detection with TensorFlow | TensorFlow Python ,https://blog.naver.com/choeungjin/221628591234,20190826,#이미지처리 #image #processing #computer #vision #Object #Detection​- #TensorFlow #Python #OpenCV #Numpy- object detection #workflow​​ [edureka!] TensorFlow Object Detection | Realtime Object Detection with TensorFlow | TensorFlow Python** AI & Deep Learning Using TensorFlow - https://www.edureka.co/ai-deep-learni...  ** This Edureka video will provide you with a d...utokorea.blogspot.com ​ 
Useful github for object detection ,https://blog.naver.com/brucks1217/222249129614,20210219,"Super Fast and Accurate 3D Object Detection based on 3D LiDAR Point Cloudshttps://github.com/maudzung/SFA3D maudzung/SFA3DSuper Fast and Accurate 3D Object Detection based on 3D LiDAR Point Clouds (The PyTorch implementation) - maudzung/SFA3Dgithub.com Complex YOLOv4https://github.com/maudzung/Complex-YOLOv4-Pytorch maudzung/Complex-YOLOv4-PytorchThe PyTorch Implementation based on YOLOv4 of the paper: ""Complex-YOLO: Real-time 3D Object Detection on Point Clouds"" - maudzung/Complex-YOLOv4-Pytorchgithub.com ​https://github.com/ApolloAuto/apollo ApolloAuto/apolloAn open autonomous driving platform. Contribute to ApolloAuto/apollo development by creating an account on GitHub.github.com ONNX tutorial ,and training​https://github.com/k0suke-murakami/kitti_pretrained_point_pillars k0suke-murakami/kitti_pretrained_point_pillarsContribute to k0suke-murakami/kitti_pretrained_point_pillars development by creating an account on GitHub.github.com https://github.com/onnx/tutorials onnx/tutorialsTutorials for creating and using ONNX models. Contribute to onnx/tutorials development by creating an account on GitHub.github.com Autoware Architecture Proposal​https://github.com/tier4/AutowareArchitectureProposal.proj tier4/AutowareArchitectureProposal.projThis is the source code of the feasibility study for Autoware architecture proposal. - tier4/AutowareArchitectureProposal.projgithub.com https://github.com/kosuke55/train_baiducnn kosuke55/train_baiducnnTrain lidar apollo instance segmentation CNN. Contribute to kosuke55/train_baiducnn development by creating an account on GitHub.github.com ​ "
"[논문] YOLO v1 논문 원본 - You Only Look Once: Unified, Real-Time Object Detection ",https://blog.naver.com/ycpiglet/222555358878,20211101,"​ You Only Look Once: Unified, Real-Time Object Detection​ ​[ PDF 파일 ] 첨부파일YOLO v1.pdf파일 다운로드 ​​다음 영상은 YOLO 논문 발표 영상이다.참고하면 좋을 것 같다.​ ​​​ ​ "
"[K-Digital] 팀 프로젝트 (2) Object Detection 모델을 API 형식으로 제공하여, 딥러닝 모델 예측 결과를 보여줄 수 있는 Web 화면 개발 ",https://blog.naver.com/ycpiglet/222488910494,20210830,"​  ​원래 K-Digital 스마트 모빌리티 기반 과정 프로젝트 주제는 해양 AR기술 기반 항해 소프트웨어였는데,담당 현직자분들과 이야기를 나눠본 후에 기술력이나 실력이 너무 맞지 않는다고 판단하여서주제를 변경하게 되었다.​변경 주제 :  Object Detection 모델을 API 형식으로 제공하여,딥러닝 모델 예측 결과를 보여줄 수 있는 Web 화면 개발​​웹은 python 모듈인 Flask를 이용했고, AWS의 S3를 통해서도 진행했다.​결과적으로 이미지 처리는 YOLO를 통해서 하게 되었다.OpenCV는 사용하지 말라고 하셨고, 학습은 불가능해서 pretrain 하기로 했다.​​​ ​ "
object detection config 파일 ,https://blog.naver.com/skyfiower_/222040534348,20200724,"​ train_config: {  batch_size: 30  optimizer {    momentum_optimizer: {      learning_rate: {        manual_step_learning_rate {          initial_learning_rate: 0.0002          schedule {            step: 900000            learning_rate: .00002          }          schedule {            step: 1200000            learning_rate: .000002          }        }      }      momentum_optimizer_value: 0.9    }    use_moving_average: false  } train_input_reader: {  tf_record_input_reader {    input_path: ""/content/traffic_sign_object_detection/data/annotations/train.record""  }  label_map_path: ""/content/traffic_sign_object_detection/data/annotations/label_map.pbtxt""}eval_config: {  metrics_set: ""coco_detection_metrics""  num_examples: 1101}eval_input_reader: {  tf_record_input_reader {    input_path: ""/content/traffic_sign_object_detection/data/annotations/test.record""  }  label_map_path: ""/content/traffic_sign_object_detection/data/annotations/label_map.pbtxt""  shuffle: false  num_readers: 1} ​ "
"OpenCV, Object Detection - Video Detection ",https://blog.naver.com/handuelly/221835111866,20200303," # VideoCapture() 차량 운전을 생각해보면, 전방에 어떤 사물들이 있는지 실시간으로 판단하고 안전한 주행을 해야 한다.그리고 이러한 기술 구현은 자율주행 자동차의 핵심이다.지금까지 다뤘던 객체 검출 관련 내용은 정지된 영상, 사진 이미지에 한정됐는데, 이번에는 동영상 속에서 객체를 검출해보는 실습을 진행해보고자 한다.​영상 속에서 객체를 검출하기 위해서는, 동영상을 불러와서 프레임 단위로 정보를 읽어야 한다.   # 4번 라인 : 동영상 불러온다.# 6~7번 라인 : 영상을 불러와서 프레임 별 이미지 정보를 frame에 저장, ret은 다음 프레임이 있는지 없는지에 대한 T/F 값을 반환받아 저장한다.   # 9번 라인 : for loop 후 20번째 프레임의 값을 갖고 있기 때문에 20번 째 프레임 이미지를 출력한다.   영상에서 불러온 20번째 frame   # Video Detection 위에서 진행한 것은 하나의 프레임만 캡쳐해서 가져오기 때문에 결과 또한 이미지 하나이다.검출 결과 또한 영상처럼 만들기 위해 함수를 작성하고, 이를 활용한다.​ import numpy as npfrom bokeh.plotting import figurefrom bokeh.io import output_notebook, show, push_notebook    def create_win(frames, scale=1.0) :        global myImage        all = []    for f in frames :        if len(f.shape ) !=  3 : f = cv2.cvtColor(f, cv2.COLOR_GRAY2BGR)        all.append(f)    frame = np.vstack(all)        fr=cv2.cvtColor(frame, cv2.COLOR_BGR2RGBA)     fr=cv2.flip(fr, -1)    width=fr.shape[1]    height=fr.shape[0]        p = figure(x_range=(0,width), y_range=(0,height), output_backend=""webgl"",               width=int(width*scale), height=int(height*scale))        myImage = p.image_rgba(image=[fr], x=0, y=0, dw=width, dh=height)    show(p, notebook_handle=True)       def update_win(frames) :        all = []    for f in frames :        if len(f.shape ) !=  3 : f = cv2.cvtColor(f, cv2.COLOR_GRAY2BGR)        all.append(f)    frame = np.vstack(all)        fr=cv2.cvtColor(frame, cv2.COLOR_BGR2RGBA)    fr=cv2.flip(fr, -1)    myImage.data_source.data['image']=[fr]    push_notebook() # create_win() 함수는 출력을 위한 window를 생성하는 역할이다.# update_win() 함수를 반복 호출하면서 frame 정보를 계속 처리하는 역할이다.​이제 영상속에서 사람 객체를 검출해보자.   # 1~2번 라인 : 사람 검출하기 위한 객체 생성(지난 포스팅 참고)# 4번 라인 : 동영상 불러오기# 6번 라인 : 검출 결과를 볼 윈도우 생성   # 3번 라인 : 영상에서 총 2000 프레임을 읽고 사람 검출하도록 loop 수행# 4번 라인 : 2000 프레임을 모두 수행하기에는 프로그램이 너무 느려져서, 5번마다 수행했음# 5번 라인 : 프레임 정보를 저장# 7번 라인 : 해당 프레임에서 사람 객체 검출# 9~11번 라인 : 사람 객체가 있는 곳에 사각형을 그리는 loop# 13번 라인 : 결과 이미지를 update   ​​ "
Object detection(4) _Two-stage 2D object detectors: Fast R-CNN[CVPR'15] ,https://blog.naver.com/summa911/222990223371,20230121,"R- CNN의 한계점​1) 이미지 하나당 2000번의 CNN 과정이 필요하다. 2) input data를 warping 해야한다. +3) training stage가 너무 많다. --> disk에 저장할 데이터도 많았었음.​--> CNN을 한 번만 학습시킬 수 있다면 시간을 줄일 수 있음. --> 임의 사이즈 데이터를 input 할 수 있다면 wrapping을 안해도 됨. --> accuracy증가--> Single-stage training --> 시간과 메모리를 아낄 수 있음. Fast R-CNN​single-stage training(softmax not SVM, multi-task loss)​SVM training과정이 빠졌기 때문에 3x faster training than SPPnet.classification의 loss와 localization error를 Multi-task loss라는 하나의 loss로 training 가능해졌다.  Muti-task loss = softmax output loss + bbox regressor lossFC에는 반복되는 데이터가 많다는 특성을 이용해서 Truncated SVD방법???으로, FC layers와 softmax, single BBreg의 계산 과정을 줄였다. ​ Fast R-CNN10x faster inference than SPPnet ​[출처] Ross Girshick. Fast R-CNNhttps://arxiv.org/abs/1504.08083 Fast R-CNNThis paper proposes a Fast Region-based Convolutional Network method (Fast R-CNN) for object detection. Fast R-CNN builds on previous work to efficiently classify object proposals using deep convolutional networks. Compared to previous work, Fast R-CNN employs several innovations to improve training...arxiv.org [출처] https://youtu.be/yGZVibqaPJY ​ "
Google Material Design : Object detection - live camera ,https://blog.naver.com/salmon_s/222173754052,20201214,"물체 감지 : 라이브 카메라 (Object Detection : Live Camera)기기의 라이브 카메라는 머신러닝을 사용하는 환경에서 물체인식용으로 사용될 수 있습니다.​ ​​사용법 (Usage)​  카메라의 라이브 피드는 물질 세계의 물체를 식별하는데 사용될 수 있습니다.ML Kit의 물체 감지와 트래킹 API의  스트리밍 모드 사용 그리고 카메라 피드는 사물을 인식할 수 있고, 시각적 검색(입력처럼 이미지를 사용하는 검색쿼리)을 수행하는데 입력하도록 이들을 앱의 이미지 식별 모델과 함께 사용할 수 있습니다. ​라이브 카메라로 하는 검색은 사용자들이 물체와 관련된  더 많은 것들을 배우는데 도움을 주고, 그것이 박물관에 있는 전시품인지, 팔기위한 상품인지 알도록 해줍니다.​이러한 가이드라인들은  한 번에 단일 사물을 인식하는 것을 다룹니다.​ Object Detection and Tracking  |  ML Kit  |  Google DevelopersObject Detection and Tracking With ML Kit's on-device Object Detection and Tracking API, you can detect and track objects in an image or live camera feed. Optionally, you can classify detected objects, either by using the coarse classifier built into the API, or using your own custom image classific...developers.google.com FirebaseExtended/mlkit-material-androidML Kit Showcase App with Material Design. Contribute to FirebaseExtended/mlkit-material-android development by creating an account on GitHub.github.com FirebaseExtended/mlkit-material-iosThese apps demonstrate how to build an end-to-end user experience with Google ML Kit APIs and following the new Material for ML design guidelines. - FirebaseExtended/mlkit-material-iosgithub.com ​​원칙 (Principles)기능들의 설계는 아래의 원칙들에 기초하였습니다.​기기의 카메라로 탐색 (Navigare with a device camera) 에니메이트되는 온보딩 경험은 검색을 위해 그들의 카메라 기기를 어떻게 움직이고 배치해야 할지에 대한 사용자의 이해를 도울 수 있다. 기기의 카메라는 검색어를 타이핑하는 대신 시각적 컨텐츠를 검색하기 위한  ""원격 조정""으로 사용됩니다.사용자들이 카메라로 검색하는 방법에 대해 교육하려면 온보딩과 지속적인 지침을 제공하십시오.​​카메라를 깨끗하고 잘 보이도록 유지 (Keep the Camera Clear and Legible) 많은 요소들이 카메라의 피드의 조망을 최대화시키기 위해 의도적으로 화면의 상단과 하단 모서리에 배치된다.카메라의 가시영역을 최대화하기 위해서 앱의 UI 컴포넌트를 화면의 상.하단 모서리로 정렬하고, 카메라의 라이브피드가 앞에 위치했을 때 텍스트와 아이콘이 잘보이도록 보장하십시오.​라이브 카메라 피드의 앞쪽에 배치되는 어떤 실행불가능한 요소들도 카메라의 방해를 최소화하기 위해 반투명해야합니다.​​피드백 제공 (Provide Feedback)검색 도구로써의 카메라 사용은 고유한 사용 요구사항을 발생시킵니다. 적당한 이미지의 품질이 필요하고, 사용자들이 빛이나 사물로부터 너무 먼 거리등으로 야기되는 이슈를 수정하는 방법에 대해 알아야 합니다.​오류 상태는 아래와 같아야합니다.* (컴포넌트와 모션과 같은)다양한 디자인 단서들을 사용해 오류를 알려줄 것* 사용자들이 그들의 검색을 개선할 방법에 대한 설명 포함​ 배너는 그들의 검색이 잘못된 곳에 왔다는 것을 사용자가 알도록 눈에 잘 띄는 방법을 제공하고, 더 많은 정보를 위한 도움말 섹션으로 링크된 공간을 제공한다.​​​컴포넌트 (Components)라이브카메라 사물 감지 기능은  기존의 Material Design 컴포넌트와 특정한 카메라 상호작용을 위한 새로운 요소를 모두 사용합니다.코드 샘플과 새로운 요소(망원경과 같은)를 위해 ML Kit Material Design showcase앱의  iOS와 안드로이드 소스코드를 확인하십시오. 라이브 카메라 시각적 검색 경험의 여러 단계에 걸친 주요 요소 :1. 상단 앱 바 (Top App Bar)2. 레티클(Reticle)3. 객체 표시(Object Marker)4. 툴팁 (ToolTip)5. 감지된 이미지 (detected Image)6. 모달 하단 시트 (Modal Bottom Sheet)​상단 앱 바 (Top App Bar)​상단 앱 바는 아래의 액션들에 대한 영구적인 접근을 제공합니다.​* 검색 경험에서 나가기 위한 버튼* 밝기를 향상시키기 위한 토글 (카메라의 플래시를 사용)* 검색 이슈에 대한 문제해결을 위한 도움말 섹션​ 상단 앱 바의 컨테이너의 카메라 피드에서 이 작업이 잘 보이도록 그라디언트 스크림이나 단일 색상을 사용할 것.App bars: topThe top app bar displays information and actions relating to the current view.material.io ​​레티클  (Reticle)레티클은 시각적 카메라에서 물체가 감지 될 때 사용자가 집중할 수 있도록 타겟을 제공하는 안내입니다. (이름은 카메라 뷰파인더에서 영감을 얻었습니다.) 이것은 카메라가 활발하게 물체를 감지했을 때 사용자에게 알리기 위해 맥박이 뛰는듯한 애니메이션을 사용합니다.​카메라가 물체에 초점이 맞춰졌을 때, 레티클은 시각적 검색이 시작되었음을 나타내기 위해 결정 진행 표시기로 변합니다. 레티클은 예를들어 카메라가 물체를 찾을 때와 에러가 발생했을 때와 같이 앱의 중요 상태를 전달하기 위해서 다른 디자인과 애니메이션 효과를 가져야한다. ​ 레티클의 상태​1. 앱이 객체를 찾고 있음을 나타내기 위해 '감지'는 맥박 애니메이션을 사용한다.2. (개발자로부터 설정된) 사전 설정 딜레이 이후에 검색이 시작되면, 결정진행 표시기를 표시한다.3. 물체로 더 가까이 이동을 위한 필요는 레티클의 중앙 원이 최소 감지 크기에 도달할 때 까지 작아지는 것으로 표현된다.4. 어떤 감지 오류들은 레티클의 외곽선이 실선에서 점선으로 변한다. ​객체 표시  (Object Marker)ML Kit의 객체 감지와 추적 API는 ""중요 객체""를 감지하는 옵션을 포함합니다.이 옵션 감지와 단일 최대 객체 추적은 카메라의 중앙 바로 옆에 있습니다. 감지가 되면 당신은 객체를 사각형 실선에 표시해야합니다.​당신의 앱이 객체 감지를 위해 최소 이미지 사이즈를 요구한다면, 객체 표시가 부분적 사각형으로 바뀌어야 합니다(부분적 사각형 선은 모서리에만 존재합니다). 물체가 감지되었을 때 나타나는 이 표현들은 사용자가 더 가까이 가기 전까지는 검색하지 못합니다. 부분적 사각형 실선과 툴팁 메세지는 사용자에게 객체가 인식되었고, 검색 시작을 위해 더 가까이 이동해야한다는 것을 알려줄 수 있습니다. ​​툴팁  (Tooltip)툴팁들은 정보적인 문자를 사용자에게 보여줍니다.툴팁들은 상태(""검색중...""과 같은 문장이 적힌 메세지)와 다음 단계로 사용자를 제촉(""카메라를 식물에 비추세요""와 같은 문장이 적힌 메세지)하는 두가지 모두를 표현합니다.​ 적절한 말을 사용한 짧은 툴팁들을 적을 것​ ""Tap to search""와 같이 행동 단어가 적힌 툴팁을 사용하지 말 것.​ 오류 메세지를 툴팁에 배치하지 말 것. 오류는 강조를 더하고 액션을 보여줄 공간을 제공하기 위해서 배너에 배치되어야 합니다.​ TooltipsTooltips display informative text when users hover over, focus on, or tap an element.material.io ​​​감지된 이미지  (Detected Image)물체 감지시에, ML Kit의 객체 감지와 추적 API는 이미지 식별 모델을 사용한 시각적 검색을 실행하는데 사용되는 이미지의 잘린 버전을 생성합니다.​잘린 버전의 이미지는 아래를 위해 나타납니다 : * 감지된 객체를 확인하기 위해* 검색 결과의 이미지와 비교하기 위해* 이미지와 관련된 어떤 오류를 설명하기 위해(저화질 또는 다중 객체를 포함하는 등과 같은)​ 감지된 객체 이미지의 썸네일을 검색 결과 가까이에 배치하는 것은 사용자가 가장 좋은 일치를 식별하는데 도움이 된다.​​모달 하단 시트  (Modal Bottom Sheet)모달 하단 시트는 시각적 검색 결과로의 접근을 제공합니다. 이들의 레이아웃과 컨텐츠는 앱의 사용 사례와 결과의 수 그리고  그것들의 신뢰 수준에 의존합니다.​ 모달 하단 시트에서, 리스트나 이미지 그리드는 다중 시각적 검색 결과를 보여줄 수 있다. 시트는 조건적 결과를 보여주기 위해 화면의 최고 높이까지 열릴 수 있다.​ 하나의 모달 하단 시트는 단일 결과만을 보여줄 수 있고, 내용에 맞게 레이아웃을 적용시킨다.​ Sheets: bottomBottom sheets are surfaces containing supplementary content that are anchored to the bottom of the screen.material.io ​​​​경험 (Experience)라이브 카메라의 시각적 검색은 세단계를 거쳐 일어납니다.​1. 감지 : 입력을 보기 위해 카메라를 사용하십시오.2. 인지 : 사물을 감지하고 식별합니다.3. 전달 : 상응하는 물체를 찾게되면, 그것을 사용자에게 전달합니다.​​감지   (Sense)시각적 검색 기능이 활성화되면 사물인식이 시작됩니다. ""감지""는 라이브 카메라 안에서 카메라가 물체를 찾는 것을 말합니다.이 단계에서 앱은 아래와 같은 일들을 수행해야합니다.​* 기능이 작업하는 방법을 설명합니다.* 앱의 행동을 전달합니다.* 사용자가 카메라를 조절하고 조정을 제안하는 방법에 대해 가이드합니다.​기능 수행의 설명사용자에게           환경 내에서 사물을 검색하기 위해 ""원격제어""로 카메라를 사용하기 위한 지침을 제공하고, 온보딩과 도움말 컨텐츠를 통해 경험을 설명하십시오.​ 단일화면 온보딩 경험으로 상호작용을 설명 할 것.사용자의 기기에서 어떻게 움직이는지를 시연해주기 위해 애니메이션이 추천된다.​​ 감지 할 수 있는 여러 종류의 사물들과 최적의 결과를 얻는 방법,이미지가 어떻게 사용되는지를 포함한 정보가 있는 도움말 영역을 할애할 것.​​ 모든 가능한 오류상태를 설명하는 장황한 온보딩 과정은 필요하지 않다.사용자가 이슈에 직면할때까지 더 나아간 정보 제공를 제공하지 말고 기다릴 것.​​앱의 행동 전달   (Communicate the App's actions)카메라가 검색하는 동안, 카메라가 ""찾고있음""을 알려주기 위해  원이 진동하고, 사용자에게 툴팁을 띄워서 카메라가 물체를 가리키도록 하십시오.  이 UI가 사진을 찍는 UI와 다르다는 것을 알리기 위해 움직이는 원과 툴팁과 같은 단서를 사용자에게 제공할 것​​가이드 조정    (Guide Adjustment)가끔은 아래와 같은 환경적인 조건이 물체를 인식하기 어렵게 만듭니다.* 사물을 식별하기에 배경과 대비되어 너무 밝거나 어두울 때* 다른 것들로부터 구별하기 어려운 겹치는 사물일 때​사물이 인식되기 전에 중요한 시간이 지나갔다면, 원의 애니메이팅을 멈추고 사용자가 도움말 문서로 가도록 안내하십시오. 인식 시간이 지났다면, 사용자에게 무언가 잘못되었음을 알게할 것 : 원의 애니메이팅을 멈추고, 도움말 컨텐츠를 강조할 것. ​​인지    (Recognize)카메라에 의해 사물이 감지되었을 때, 앱은 아래와  같은 작업을 해야합니다.* 감지된 사물을 표시해야 합니다.* 사용자가 검색을 시작하도록 즉시 나타내야합니다.* 검색 진행을 나타내야합니다.​감지된 물체 식별카메라가 물체를 찾아냈을 때를 알리기 위해, 원의 애니메이팅을 멈추고 감지된 사물에 선을 만드십시오. 사물이 인지되었음을 알리기 위해 직사각형의 선은 감지된 물체 주변에 나타난다.​감지된 물체 식별감지된 사물이 카메라의 중앙에 유지되도록 사용자를 지도하십시오.검색이 시작되기 전, 짧은 시간동안을 지연시키고, 원 안에 결정 진행 지시자를 함께 제공합니다.이것은 사용자에게 아래의 두가지를 할 시간을 줍니다.​* 검색의향을 확인 (사용자가 기기에서 카메라를 계속 켜두게 합니다)* 검색 취소 (사용자가 사물로부터 카메라를 떨어지게 합니다)​지연시간은 사용자정의 할 수 있습니다. 지연시간은 사용자가 필요하다면 검색을 취소하도록 해준다. ​​검색 진행 표시검색이 시작되면, 사물인식은 중단되고 라이브 카메라가 일시중지됩니다.이것은 검색시 또 다른 새로운 검색을 예방하기 위함입니다.(또한 사용자들이 기기를 더 편하게 움직일 수 있게 하기 위함입니다)검색 진행은 로딩 애니메이팅과 툴팁 메시지를 통해 표시됩니다. 다중 디자인 단서들(모션,스크림 그리고 텍스트)은 검색이 시작되었고, 사용자들이 결과를 기다려야함을 알려주기 위해 사용된다. ​​수정인지단계에서 두가지 이슈가 검색 결과의 질에 영향을 미칠 수 있습니다.​작은 이미지 사이즈 : 인지되는 사물이 카메라로부터 너무 멀다면, 고화질의 이미지를 생산할 수 없습니다.(당신이 최소 이미지 사이즈를 어떻게 설정했냐에 따라 결정된다)​감지가 끝나지 않았음을 알려주기 위해서, (사각형 전체에 선이 표시되는 것 대신)모서리의 선들이 사물 주변에 표시되고, 사용자에게 더 가까이 이동하도록 요청하기 위해 툴팁 메시지를 표시하십시오. 모서리 선표시와 툴팁 메시지는 사용자에게 사물이 인식되었고 검색을 시작하기 위해 더 가까이 움직여야함을 알려줄 수 있다. ​네트워크 연결 : 이미지 분류 모델이 클라우드에 있다면 안정적인 네트워크 연결이 요구됩니다. 인터넷 연결이 실패하면, 진행을 위해 인터넷 연결이 필요하다는 배너를 표시합니다. 네트워크 문제들은 사용자들에게 배너로 전달될 수 있고, 이슈가 해결되었다면 재시도를 눌러 다시 요청 할 것.​​​전달     (Communicate)시각적 검색의 결과는 모달의 하단시트에 표시됩니다. 이 단계가 진행되는 동안, 앱은 아래의 작업을 진행해야합니다.* 결과를 표시* 감지 사물 표시* 빠른 네비게이션 표시​앱은 시각적 검색 결과를 표시해주기 위해 신뢰도의 한계를 설정해야합니다.신뢰도는 머신러닝 모델의 예측도의 정확성에 대한 평가를 의미합니다.시각적 검색에서 각 결과의 신뢰도는 제공하는 이미지와 모델의 유사한 정도를 믿을 수 있게 보여줍니다. 가장 일치하는 항목을 첫번째로 볼 수 있도록 시각적 검색의 결과를 신뢰도 순서대로 나열 할 것.​​ 사용 케이스 중 확실한 일치를 보장해야 할 때는 가장 신뢰도가 높은 결과만을 보여줄 것.​​ 검색 결과와 비교할 수 있도록 사물에 감지된 이미지를 포함 할 것.​​빠른 네비게이션 제공결과를 보여준 후, 사용자는 아래와 같이 다른 행동을 취할 수 있습니다.* 카메라로 돌아가기 위해, 사용자는 스크림이나 모달 하단시트의 헤더를 탭 할 수 있습니다.* 카메라에서 나가기 위해, 사용자는 결과와 상호작용하거나, 앱의 다른 곳으로 이동하거나, ""X"" 버튼을 눌러 카메라를 끄고 앱으로 돌아갈수도 있습니다.​ 다시 검색하기 위해서 사용자는 하단시트의 헤더나 스크림을 탭해 라이브 카메라로 돌아올 수 있다. ​ 사용 케이스 중 확실한 일치를 보장해야 할 때는 가장 신뢰도가 높은 결과만을 보여줄 것. ​​테마 (Theming)​슈라인 Material 테마 (Shrine Material Theme)라이브 카메라  시각적 검색은 슈라인 앱의 구매 플로우에서 사용됩니다.​ 슈라인 브랜드 내에서 사용되는 각진 모양은 십자선과 같은 주요 요소에 적용된다.​ 앱의 나머지 부분들의 일관성을 위해 슈라인의 색상과 폰트 스타일은 시각적 검색 결과에도 적용된다.​​십자선슈라인의 십자선은 슈라인의 스타일을 반영하기 위해 (각지게 잘린)다이아몬드 모양을 사용합니다. 1. 슈라인의 기하학 로고2. 4dp로 잘린 버튼 모서리3. 다이아몬드 모양의 십자선​슈라인의 툴팁은 사용자 맞춤 색상, 폰트 그리고 배치가 사용되어 강조된다.​ 1. 툴팁들은 전형적으로 저강조 컴포넌트이며, 넓이는 텍스트의 길이에 따른다.2. 슈라인의 툴팁은 사용자 맞춤 색상, 폰트 그리고 화면의 가장자리의 확장을 사용합니다.ShrineShrine is a lifestyle and fashion brand that demonstrates how Material Design can be used in e-commerce.material.io ​​​​​​ ML object detection: live cameraA device’s live camera can be used to detect objects in an environment using machine learningmaterial.io ​ "
Tensorflow Object Detection : 새로운 감지 된 이미지를 사용하여 모델 재교육  ,https://blog.naver.com/7804542/221713484448,20191120,머신 러닝의 주요 문제 (여기서 객체 감지에 사용하는 접근 방식)는 일반화 문제 입니다. 귀하의 경우 훈련 중에 사용되지 않은 이미지에서 훈련에 사용한 이미지와 동일한 유형의 객체를 인식하는 기능입니다.교육 중에 가능한 모든 이미지를 사용할 수 있었다면 시스템이 완벽 할 것입니다 (실제로는 정확한 이미지 일치 문제 일 것입니다). 보다 현실적인 설정에서 사용하는 훈련 이미지가 많을수록 더 나은 물체 탐지기를 얻을 수있는 가능성이 높아집니다.그러나 일반적으로 훈련 세트에 어려운 예 를 추가하는 것이 더 중요합니다 . 따라서 응용 프로그램에서 (특히 계산 시간 측면에서) 허용하면 데이터 세트에서 잘못 감지 된 모든 이미지를 정확한 레이블로 추가 할 수 있으며 더 나은 모델을 얻는 데 도움이 될 것입니다. 새로운 이미지에서 더 어려운 상태의 객체.그러나 그것은 실제로 당신이하는 일에 달려 있습니다. 시스템을 다른 시스템과 비교하려면 동일한 (훈련 및) 테스트 이미지를 사용하여 공정해야합니다. 벤치마킹의 경우 훈련 데이터 세트에 테스트 이미지를 포함시킬 수 없습니다 ! 여러 설정을 비교하기 위해 유효성 검사 (검증 / 테스트 데이터 세트에서)를 계산할 때이 비교가 공정한지 확인하십시오.​​​​정확도는 전체 훈련 이미지 수뿐만 아니라 올바르게 분류 된 이미지 수에 따라 달라집니다. https://developers.google.com/machine-learning/crash-course/classification/accuracy . 새 이미지를 교육에 사용한다고 생각하면 (올바른 레이블이 있음) 모델 재교육을 고려해야합니다. 이 게시물을보십시오 https://datascience.stackexchange.com/questions/12761/should-a-model-be-re-trained-if-new-observations-are-available​​​​그것은 향상시킬 수 있지만 까다 롭습니다. 과적 합으로 이어질 것입니다. 데이터 세트를 개선하면 실제로 도움이되지만 자체 모델로 감지 된 이미지에는 도움이되지 않습니다. 이러한 종류의 이미지는 모델이 이미 잘 수행되어 감지되므로 별 도움이되지 않습니다.실제로 필요한 것은 정반대입니다. 이전에는 인식하지 못한 이미지를 인식하도록 모델을 가르쳐야합니다.​​​https://stackoverflow.com/questions/57902980/tensorflow-object-detection-using-new-detected-images-to-retrain-model Tensorflow Object Detection: Using new detected images to retrain modelI have searched through this forum for similar questions but was unanswered (Updating Tensorflow Object detection model with new images). I have managed to create my custom train model (lets name itstackoverflow.com ​​ 
파이썬 ModuleNotFoundError: No module named 'object_detection' Object Detection 에러 ,https://blog.naver.com/alaldi2006/222149432308,20201120,ModuleNotFoundError: No module named 'object_detection'​https://github.com/tensorflow/models/tree/master/research/object_detection/g3docgit clone https://github.com/tensorflow/models/tree/master/research/object_detection/g3doc tensorflow/modelsModels and examples built with TensorFlow. Contribute to tensorflow/models development by creating an account on GitHub.github.com ​\models\research 이동python setup.py install​----------​update​pip install tensorflow-object-detection-api 
Google Colab에서 Object Detection Model 예제 실행해 보기 ,https://blog.naver.com/seodaewoo/222044962717,20200729,"참조 사이트 : https://tfhub.dev/google/openimages_v4/ssd/mobilenet_v2/1 Openimages V4 | Ssd | Mobilenet V2 | TensorFlow Hubmenu search Send feedback tfhub.dev 위 참조 사이트를 클릭한 후 우하단의 ""Open Colab Notebook""을 클릭한다. ​그러면 아래와 같은 Colab notebook 창이 열린다. 위에서부터 코드 부분 왼쪽의 ""[ ]""를 차례대로 클릭한다. 단, 각 코드의 실행이 끝난 후 아래의 코드를 클릭한다.​run_detector(detector, downloaded_image_path) 코드를 실행하면 아래 사진처럼 객체를 찾아 표시해 준다. 그 아래 있는 More images 영역의 코드도 차례대로 실행하면 다양한 사진의 객체를 찾아준다.​내 이미지로 테스트해 보려면 가장 아래쪽의 ""detect_img(image_urls[2])"" 코드 아래에 ""+ 코드"" 버튼을 눌러 새 코드창을 만든 후 아래와 같이 입력해 본다. ﻿detect_img(""나의 이미지 URL"") 다음 예와 같은 결과를 얻을 수 있다.  ​tfhub.dev 사이트에 다른 많은 예제가 있으니 한 번씩 살펴봐야겠다. "
[ Object Detection ] Training을 위한 데이타 준비 ,https://blog.naver.com/sgkim21/221918899915,20200420,"** Tensorflow Object Detection API uses the TFRecord file format​1) 관심있는(도로, 신호등, 고양이, 개, 등등) 이미지 파일(*.jpg 또는 *.png) 준비      --> 해상도가 너무 크면, Training 2) 경계사각형 리스트 준비 - ​a list of bounding boxes (xmin, ymin, xmax, ymax) for the image and the class of the object in the bounding box3) 이미지들을 라벨링함. ( 아래 Tool 활용 추천 )     - LabelImg tool     - FIAT(Fast Image Annotation Tool) tool : https://github.com/christopher5106/FastAnnotationTool christopher5106/FastAnnotationToolA tool using OpenCV to annotate images for image classification, optical character reading, ...  - christopher5106/FastAnnotationToolgithub.com 4) 많이 사용되는 아래 2개 타입과 유사한 구조로 준비함. PASCAL VOC dataset or the Oxford Pet dataset  --> 제공하는 Python 파일(create_pascal_tf_record.py and create_pet_tf_record.py  )을 실행해 입력파일 포맷에 맞게 변환함. 5) TFRecord file 준비  : XML --> CSV --> TFRecord file  --> Training 진행​https://towardsdatascience.com/how-to-train-your-own-object-detector-with-tensorflows-object-detector-api-bec72ecfe1d9 How to train your own Object Detector with TensorFlow’s Object Detector APIThis is a follow-up post on “Building a Real-Time Object Recognition App with Tensorflow and OpenCV” where I focus on training my own…towardsdatascience.com ​ "
[딥러닝] DETR(Detection with Transformer) 모델 개념 활용사례 ,https://blog.naver.com/totalcmd/223080711340,20230503,"DETR(DEtection with TRansformer)는 Facebook AI Research에서 발표한 Object Detection을 위한 Transformer 기반의 모델입니다. End-to-End Object Detection with TransformersWe present a new method that views object detection as a direct set prediction problem. Our approach streamlines the detection pipeline, effectively removing the need for many hand-designed components like a non-maximum suppression procedure or anchor generation that explicitly encode our prior know...arxiv.org 기존의 Object Detection 모델은 Region Proposal Network(RPN)과 같은 알고리즘을 사용하여 물체가 있을 가능성이 있는 영역(Region of Interest)을 먼저 찾은 다음 해당 영역에서 물체를 인식하는 방식이었는데, DETR은 이러한 과정을 간소화하기 위해 End-to-End 방식을 적용한 모델입니다.​Transformer Encoder-Decoder 구조를 기반으로 하며, 입력 이미지를 일렬로 나열하여 하나의 Sequence로 만들어 Encoder에 입력합니다. 이후 Encoder에서는 Self-Attention을 사용하여 입력 Sequence의 모든 위치를 적절히 연결하고 이를 하나의 벡터로 변환합니다. 이렇게 변환된 벡터를 Decoder에 입력하여 Object Detection을 수행합니다. ​DETR은 Object Detection을 위한 Loss 함수로 Hungarian Algorithm을 사용하여 Object Detection에서 발생하는 Matching 문제를 해결하고, 추가적인 분류와 bounding box regressor loss를 합친 Multi-Task Loss를 사용합니다. DETR은 기존 Object Detection 모델에서 필요한 hyper-parameter와 알고리즘 수를 크게 줄일 수 있으며, GPU 활용도도 높아 속도가 빠릅니다. ​이러한 특징들로 인해 DETR은 최근 대규모 데이터셋에서 다양한 Object Detection 문제에서 높은 성능을 보이며, 다른 분야에서도 활용될 수 있는 유연한 모델입니다. 활용사례DETR은 다음과 같은 활용 사례에 사용될 수 있습니다.​1. 상품 검색- DETR을 사용하여 이미지에서 상품을 감지하고, 이를 이용하여 쇼핑몰 등의 상품 검색 시스템을 구축할 수 있습니다.​2. 교통 흐름 모니터링- DETR을 사용하여 도로상의 차량이나 보행자를 감지하고, 이를 이용하여 교통 흐름을 모니터링하는 시스템을 구축할 수 있습니다.​3. 보안 시스템- DETR을 사용하여 CCTV 영상에서 이상 행동을 감지하고, 이를 이용하여 보안 시스템을 구축할 수 있습니다.​4. 의료 이미지 분석- DETR을 사용하여 의료 영상에서 종양 등의 이상을 감지하고, 이를 이용하여 의료 이미지 분석 시스템을 구축할 수 있습니다.​5. 자율주행 자동차- DETR을 사용하여 주행 중 차량 주변의 장애물을 감지하고, 이를 이용하여 자율주행 자동차의 안전성을 높일 수 있습니다. "
No image shown at the end of running object_detection_tutorial.ipynb  ,https://blog.naver.com/7804542/221628875691,20190826,No image shown at the end of running object_detection_tutorial.ipynb ​ object_detection_tutorial.ipynb 을 실행해도 이미지가 보이지않았다.....​해결>>from matplotlib import pyplot as plt 주석처리import matplotlib.pyplot as plt 추가​https://github.com/EdjeElectronics/TensorFlow-Object-Detection-API-Tutorial-Train-Multiple-Objects-Windows-10/issues/232​ 
Object Detection 정의 이미지 ,https://blog.naver.com/podong28/222933305824,20221120,#computer vision #DETR DETR:Query - based Object Detector PITWeighted Bipartite MatchingMulti-object TrackingTemporal Action Segmentation - TadTR(TIP '22)Instancee Segmentation - SOLQ(NeurlPS '21)Lane Shape Prediction - LSTR(WACV '21)Text Spotting - TESTR (CVPR '22)​​ 
"Objects are Different: Flexible Monocular 3D Object Detection (CVPR, 2021) ",https://blog.naver.com/016nyc/222387561490,20210606,"논문https://arxiv.org/abs/2104.02323Zhang, Yunpeng, Jiwen Lu, and Jie Zhou. ""Objects are Different: Flexible Monocular 3D Object Detection."" arXiv preprint arXiv:2104.02323 (2021).​코드(공개예정)https://github.com/zhangyp15/MonoFlex​​Abstract깊이 정보가없는 단일 이미지에서 3D 개체의 정확한 위치 파악은 매우 어려운 문제입니다. 대부분의 기존 방법은 다양한 분포에 관계없이 모든 개체에 대해 동일한 접근 방식을 채택하므로 잘린 개체의 성능이 제한됩니다. 이 논문에서는 잘린 물체를 명시 적으로 분리하고 물체 깊이 추정을 위한 여러 접근 방식을 적응 적으로 결합하는 단안 3D 물체 감지를 위한 유연한 프레임 워크를 제안합니다. 특히, 일반 객체의 최적화가 영향을 받지 않도록 길이가 길고 잘린 객체를 예측하기 위해 피쳐 맵의 가장자리를 분리합니다. 또한, 우리는 직접 회귀 된 객체 깊이와 여러 키포인트 그룹에서 해결 된 깊이의 불확실성 안내 앙상블로 객체 깊이 추정을 공식화합니다. 실험 결과 우리의 방법이 KITTI 벤치 마크의 테스트 세트에서 중간 수준의 경우 27 %, 어려운 수준의 경우 30 %까지 최첨단 방법을 능가하는 동시에 실시간 효율성을 유지하는 것으로 나타났습니다. 코드는 https://github.com/zhangyp15/MonoFlex 에서 제공됩니다.​​1. Introduction  3D 물체 검출은 기계가 물리적 환경을 인식하는 데 없어서는 안될 전제이며 자율 주행 및 로봇 내비게이션에 널리 사용되었습니다. 본 논문에서는 단안 이미지의 정보만으로 문제를 해결하는 데 중점을 둡니다. 대부분의 기존 3D 물체 감지 방법에는 정확한 깊이 측정을 위해 LiDAR 센서 [22, 33, 35, 40, 41, 49]가 필요합니다. 또는 깊이 추정을 위한 스테레오 카메라 [8, 24, 37, 45]는 실제 시스템의 구현 비용을 크게 증가시킵니다. 따라서 단안 3D 물체 감지는 유망한 솔루션이었으며 커뮤니티에서 많은 관심을 받았습니다 [2, 3, 7, 10, 13, 20, 27, 31, 34].  3D 객체의 까다로운 위치 인식을 위해 대부분의 기존 방법은 통합 접근 방식으로 다른 객체를 처리합니다. 예를 들어, [10, 25, 28, 52]는 공유 커널을 사용하여 다양한 분포의 객체를 예측하기 위해 fully convolution nets를 사용합니다. 그러나 모든 객체를 동등하게 처리하면 성능이 만족스럽지 않을 수 있습니다: (1) 그림 1에서 볼 수 있듯이 심하게 잘린 물체는 최첨단 방법으로 거의 감지 할 수 없지만 [3, 13] 자율 주행 차의 안전에 중요합니다. (2) 우리는 이러한 하드 샘플이 학습 부담을 증가시키고 일반 대상의 예측에 영향을 미칠 수 있음을 경험적으로 발견했습니다. 따라서 통합 접근 방식은 모든 물체를 찾고 정확한 3D 위치를 예측하는 데 실패 할 수 있습니다. 이를 위해 우리는 물체 간의 차이를 고려하고 적응 적으로 3D 위치를 추정하는 유연한 검출기를 제안합니다. 물체의 3D 위치 추정은 일반적으로 투영 된 3D 중심과 물체 깊이 [10, 28, 36, 52]를 찾는 것으로 구성되기 때문에 이 두 가지 측면의 유연성도 고려합니다. 그림 1: 이전 논문 [3, 13], 기준 및 제안된 방법 간의 정성적 비교. 청록색과 분홍색 경계 상자는 검출된 자동차와 보행자를 나타냅니다. 우리의 접근 방식은 빨간색 화살표로 강조 표시된 심하게 잘린 객체를 효과적으로 감지 할 수 있습니다.  투영 된 3D 중심의 위치를 찾기 위해 투영 된 중심이 이미지 ""내부""인지 ""외부""인지 여부에 따라 객체를 나눕니다. 그런 다음 내부 객체를 투영 된 중심으로 정확하게 표현하고, 외부 객체를 섬세하게 선택한 가장자리 점으로 표현하여, 두 객체 그룹이 각각 피쳐 맵의 내부 및 가장자리 영역에 의해 처리되도록 합니다. 컨볼루션 필터가 공간 변형 예측을 관리하는 것이 여전히 어렵다는 점을 고려하여, 외부 객체의 특징 학습 및 예측을 분리하기 위해 edge fusion 모듈이 추가로 제안됩니다.  물체의 깊이를 추정하기 위해 우리는 다른 깊이 추정기와 불확실성 추정을 결합 할 것을 제안합니다 [18, 19]. 추정기에는 직접 회귀 [10, 25, 36, 52]와 키포인트 [2, 5]의 기하학적 솔루션이 포함됩니다. 우리는 키포인트에서 깊이를 계산하는 것이 일반적으로 과도하게 결정된 문제이며, 다른 키포인트 [5]의 결과를 단순히 평균화하면 키포인트의 잘림 및 가려짐에 민감 할 수 있습니다. 그 결과 키포인트를 M 그룹으로 추가 분할했으며, 각 그룹은 깊이 문제를 해결하기에 충분합니다. M 개의 키포인트 기반 추정기와 직접 회귀를 결합하기 위해 불확실성을 모델링하고 최종 추정을 불확실 가중 평균으로 공식화합니다. 제안된 조합을 통해 모델은 강력하고 정확한 예측을 위해 더 적합한 추정기를 유연하게 선택할 수 있습니다.  KITTI [14] 데이터셋에 대한 실험 결과는 우리의 방법이 특히 중간 및 어려운 샘플에서 기존의 모든 방법보다 훨씬 뛰어난 성능을 보여줍니다. 이 논문의 주요 공헌은 두 가지 측면으로 요약 할 수 있습니다. (1) 단안 3D 물체 감지를 위한 물체 간의 차이를 고려하는 중요성을 드러내고 잘린 물체의 예측을 분리 할 것을 제안합니다. (2) 독립적인 추정치를 유연하게 결합하기 위해 불확실성을 활용하는 객체 깊이 추정을 위한 새로운 공식을 제안합니다.​​2. Related WorkMonocular 3D object Detection2D 이미지에서 3D 환경을 인식하는 데 어려움을 감안하면, 대부분의 기존 단안 3D 물체 감지 방법은 사전 훈련 된 깊이 추정 모듈 [30, 45, 46, 47], 키포인트 데이터 [2] 및 CAD모델 [32] 등의  추가 ​​정보를 활용하여 작업을 단순화합니다. Mono3D [7]는 먼저 지면 정보를 기반으로 후보를 샘플링하고 의미론적/인스턴스 분할, 컨텍스트 정보, 객체 모양 및 위치 정보로 점수를 매깁니다. MonoPSR [21]은 인스턴스 포인트 클라우드를 추정하고 제안 구체화를 위해 객체 모양과 투영 된 포인트 클라우드 간의 정렬을 적용합니다. Pseudo-LiDAR [45]는 추정된 깊이로 단안 이미지를 pseudo-LiDAR로 전환시킨 후, LiDAR 기반 검출기를 활용합니다. AM3D [31]는 색 정보로 pseudo-LiDAR를 향상시키기 위해 multi-modal fusion 모듈을 제안합니다. PatchNet [30]은 pseudo-LiDAR를 이미지 표현으로 구성하고 강력한 2D CNN을 활용하여 검출 성능을 향상시킵니다. 추가 정보가 있는 이러한 방법은 일반적으로 더 나은 성능을 달성하지만, 학습을 위해 더 많은 레이블링이 필요하며 보통 덜 일반화됩니다.  다른 순수 단안 방법 [3, 10, 27, 28, 34, 36]은 검출을 위해 단일 이미지만 사용합니다. Deep3DBox [34]는 방향 추정을위한 MultiBin 방법을 제시하고 2D 경계 상자의 기하학적 제약을 사용하여 3D 경계 상자를 도출합니다. FQNet [27]은 투영된 3D 제안과 객체 사이의 적합도를 측정하여 가장 적합한 제안을 선택합니다. MonoGRNet [36]은 sparse supervision으로 물체의 깊이를 직접 예측하고 초기 feature를 결합하여 위치 추정을 개선합니다. M3D-RPN [3]은 3D 영역 제안 네트워크의 문제를 해결하고 추출된 특징을 향상시키기 위해 깊이 인식 컨볼루션 레이어를 제안합니다. MonoPair [10]는 검출 결과를 최적화하기 위해 공간적 제약으로 활용되는 인접 물체 간의 쌍별 관계를 고려합니다. RTM3D [25]는 3D 경계 상자의 투영된 꼭지점을 예측하고 비선형 최소 제곱 최적화로 3D 속성을 해결합니다. 기존 방법은 대부분 객체 간의 차이를 무시하거나, 정상 분포에서 벗어난 객체는 고려하지 않고 일반적인 객체 크기 변화만 고려하여 성능이 저하 될 수있습니다. 대조적으로, 우리의 작업은 효율적인 학습을 위해 심하게 잘린 객체를 긴 길이의 객체 분포로 명시적으로 분리하고, 모든 객체에 대해 하나의 단일 방법을 사용하는 대신 여러 깊이 추정기를 적응적으로 결합하여 객체 깊이를 추정합니다.​Uncertainty Estimation  일반적으로 베이지안 모델링에서는 두 가지 주요 유형의 불확실성을 연구합니다 [18]. 인식론적 불확실성은 모델 매개 변수의 불확실성을 설명하는 반면, 환기적 불확실성은 관측의 노이즈를 포착 할 수 있으며, 물체 검출에 대한 응용은 [10, 11, 15]에서 탐구되었습니다. Gaussian YOLO [11]는 검출 점수를 수정하기 위해 예측된 2D 상자의 불확실성을 모델링합니다. [15]는 경계 상자를 가우스 분포로 예측하고 회귀 손실을 KL divergence로 공식화합니다. MonoPair [10]는 불확실성을 사용하여 예측된 3D 위치와 쌍별 제약 사이의 최적화 후 가중치를 제공합니다. 이 논문에서는 최종 결합 예측에 대한 기여도를 정량화하는 데 사용되는 다중 추정기로부터 추정된 깊이의 불확실성을 모델링합니다.​Ensemble Learning  앙상블 학습 [1, 12, 17, 23, 39]은 여러 모델을 전략적으로 생성하고 더 나은 성능을 위해 예측을 결합합니다. 전통적인 앙상블 방법에는 bagging, boosting, stacking, gating network 등이 포함됩니다. [17]은 분류를 위해 전문가 조합을 결합하는 gating network를 사용합니다. [1]은 얼굴 정렬을 위해 여러 전문가에게 계층적으로 가중치를 부여하는 트리 구조 게이트를 제안합니다. 앙상블 학습은 일반적으로 학습자가 동일한 구조를 가지고 있지만 다른 샘플 또는 초기화로 훈련되었다고 가정하는 반면, 다중 깊이 추정기는 각각 다른 방식으로 작동하며 실질적으로 다른 손실 함수에 의해 감독됩니다. 따라서 우리는 모든 예측의 불확실성 유도 평균으로 조합을 공식화 할 것을 제안합니다.​ 그림 2: 프레임 워크 개요. CNN backbone은 다중 예측 헤드에 대한 입력으로 단안 이미지에서 특징맵을 추출합니다. 이미지 레벨 localization에는 히트맵 및 오프셋이 포함되며, edge fusion 모듈은 잘린 객체의 특징 학습 및 예측을 분리하는 데 사용됩니다.적응형 깊이 앙상블은 깊이 추정을 위해 네 가지 방법을 채택하고 동시에 불확실성을 예측하여 불확실성 가중치 예측을 형성합니다.​3. Approach3.1. Problem Statement  물체의 3D 감지에는 3D 위치 (x; y; z), 크기 (h; w; l) 및 방향 θ 추정이 포함됩니다. 치수 및 방향은 모양 기반 단서에서 직접 추론 할 수 있으며, 3D 위치는 그림 3 (a) 및 (1)에 표시된 것처럼 투영된 3D 중심 xc = (uc, vc) 및 객체 깊이 z로 변환됩니다.   여기서 (cu, cv)는 주점(principal point)이고 f는 초점 거리입니다. 이를 위해 전체 문제는 4 개의 독립적 인 하위 작업으로 구성됩니다. 그림 3: (a) 3D 위치는 투영된 중심과 물체 깊이로 변환됩니다. (b) 2D 중심에서 투영 된 3D 중심으로 오프셋 δc의 분포. 내부 및 외부 개체는 완전히 다른 분포를 나타냅니다.3.2. Framework Overview  그림 2에서 볼 수 있듯이 우리의 프레임 워크는 CenterNet [52]에서 확장되었습니다. 여기서 객체는 대표 포인트로 식별되고 히트맵의 피크로 예측됩니다. 여러 예측 branch들이 공유 backbone에 배포되어 2D 경계 상자, 차원, 방향, 키포인트 및 깊이를 비롯한 객체의 속성을 회귀합니다. 최종 깊이 추정은, 회귀된 깊이와, 추정된 키포인트 및 차원에서 계산된 깊이의, 불확실성 유도 조합입니다. 3.3 절에서 일반 및 잘린 객체에 대한 분리된 대표 포인트의 디자인을 제시하고 3.4 절에서 시각적 속성의 회귀를 소개합니다. 마지막으로, 적응형 깊이 앙상블은 섹션 3.5에서 자세히 설명합니다.​3.3. Decoupled Representations of Objects  기존 방법 [10, 25, 52]은 모든 객체에 대해 2D 경계 상자 xb의 중심 인 통합 표현 xr을 사용합니다. 이러한 경우 오프셋 δc = xc − xb는 회귀하여 투영된 3D 중심 xc를 유도합니다. 투영된 3D 중심이 이미지 내부 또는 외부에 있는지 여부에 따라 객체를 두 그룹으로 나누고 그림 3 (b)에서 해당 오프셋 δc를 시각화합니다. 두 그룹의 실질적으로 다른 오프셋을 고려할 때, δc의 공동 학습은 긴 길이 오프셋으로 어려움을 겪을 수 있으므로 내부 및 외부 객체의 표현과 오프셋 학습을 분리 할 것을 제안합니다.​Inside Objects  투영된 3D 중심이 이미지 내부에있는 객체의 경우 [10, 25]와 같이 불규칙한 δc가 회귀하지 않도록 xc로 직접 식별됩니다. (2)에서와 같이 backbone CNN의 다운 샘플링 비율 S로 인해 이산화 오류 δin을 회귀해야하지만, 이는 δc보다 훨씬 작고 회귀하기 쉽습니다. 우리는 [52]에 따라 xc를 중심으로하는 원형 가우시안 커널을 가진 내부 객체에 대한 ground-truth 히트맵을 생성합니다.​Outside Objects  외부 객체의 표현을 분리하기 위해 그림 4 (a)와 같이 이미지 가장자리와 xb에서 xc까지의 선 사이의 교차점 xI로 식별 할 것을 제안합니다. 제안된 교차점 xI는 단순히 xb 또는 xc를 경계에 고정하는 것보다 물리적으로 더 의미가 있음을 알 수 있습니다. xI의 예측은 그림 4 (b)와 같이 1 차원 가우스 커널에서 생성된 edge 히트맵에 의해 달성됩니다. 또한 그림 4 (c)에서 xI와 일반적으로 사용되는 xb를 비교합니다. 2D 경계 상자는 객체의 내부 이미지 부분만 캡처하므로 xb의 시각적 위치는 혼란스럽고 다른 객체에서도 혼란스러울 수 있습니다. 반대로 교차점 xI는 히트맵의 edge 영역을 분리하여 외부 객체에 초점을 맞추고 위치 인식을 단순화하기 위해 강력한 경계 사전 정보를 제공합니다. 또한 (3)에서와 같이 xI에서 대상 xc로 오프셋을 회귀합니다. 그림 4: 외부 물체의 표현. (a) 이미지 가장자리와 xb에서 xc까지의 선 사이의 교차점 xI은 잘린 객체를 나타내는 데 사용됩니다. (b) edge 히트맵은 커널 크기가 2D 경계 상자의 크기에 비례하는 1D 가우스 분포로 생성됩니다. (c) 항상 가장자리에있는 교차 xI (청록색)은 심하게 잘린 물체의 경우 2D 중심 xb (녹색)보다 더 나은 표현입니다. 컬러로 봐야 잘 보입니다.Edge Fusion  내부 및 외부 객체의 표현이 출력 기능의 내부 및 주변 영역에서 분리되어 있지만 공유된 컨벌루션 커널이 공간 변형 예측을 처리하기는 여전히 어렵습니다. 따라서 우리는 외부 물체의 특징 학습과 예측을 더 분리하기 위해 edge fusion 모듈을 제안합니다. 그림 2의 오른쪽 부분에 표시된 것처럼 모듈은 먼저 feature map의 4개 경계를 추출하고 이를 시계 방향으로 edge feature 벡터로 연결한 다음 2 개의 1D 컨벌루션 레이어에서 처리하여 잘린 객체의 고유한 특징들을 학습합니다. 마지막으로 처리된 벡터는 4 개의 경계로 다시 맵핑되고 입력 feature map에 추가됩니다. 히트맵 예측에 적용 할 때 edge 특징은 외부 객체에 대한 edge 히트맵을 예측하는데 특화되어 내부 객체의 위치 인식이 혼동되지 않습니다. 오프셋을 회귀하기 위해 그림 3 (b)에 표시된 것처럼 δin과 δout 사이의 중요한 스케일 차이는 edge fusion 모듈을 사용하여 해결할 수 있습니다.​Loss Functions패널티 감소 focal loss [26]은 [10, 25, 28]에서와 같이 히트맵 예측에 사용됩니다. 회귀 δin에 대해 L1 loss를 채택하고, δout에 대해서는 극한 이상치에 더 강인한 로그 스케일 L1 loss를 채택합니다. 오프셋 loss는 (4)로 계산됩니다. 여기서 δin 및 δout은 예측값이고, δin * 및 δout *은 정답값입니다. Loff는 공식이 다르기 때문에 내부 및 외부 객체에 대해 개별적으로 평균화됩니다.​3.4. Visual Properties Regression  이 섹션에서는 2D 경계 상자, 크기, 방향 및 객체의 키포인트를 포함한 시각적 속성의 회귀에 대해 자세히 설명합니다.​2D Detection  객체를 2D 중심으로 나타내지 않기 때문에 FCOS [44]를 따라 내부 객체는 xb, 외부 객체는 xI로 나타내는 대표 포인트 xr = (ur, vr)에서 2D 경계 상자의 4면까지의 거리를 회귀합니다. 왼쪽 상단 모서리를 (u1, v1)로 표시하고 오른쪽 하단 모서리를 (u2, v2)로 표시하면 회귀 대상은 다음과 같습니다. GIoU loss [38]은 스케일 변경에 강하기 때문에 2D 검출에 채택됩니다.​Dimension Estimation  각 카테고리 내에서 객체 크기의 작은 분산을 고려하여 절대값 대신 통계적 평균에 대한 상대적 변화를 회귀합니다. 각 클래스 c에 대해 학습 데이터의 평균 차원은 (hc, wc, lc)로 표시됩니다. 회귀된 로그 스케일 차원의 오프셋이 (δh, δw, δl)이고 실측 차원이 (h*, w*, l*)라고 가정하면 차원 회귀에 대한 L1 loss는 다음과 같이 정의됩니다. Orientation Estimation  방향은 카메라 좌표계의 global 방향 또는 보는 방향에 대한 local 방향으로 나타낼 수 있습니다. (x, y, z)에 위치한 객체의 경우 global 방향 ry 및 local 방향 α는 (7)을 충족합니다.   그림 5에서 볼 수 있듯이 global 방향은 같지만 보이는 각도가 다른 객체는 local 방향과 시각적 모양이 다릅니다. 따라서 우리는 MultiBin loss [6]을 사용하여 로컬 방향을 추정합니다. 이 방법은 방향 범위를 No개의 겹치는 bin으로 분할하여 네트워크가 어떤 bin 안에 객체가 놓여있는지를 결정하고 bin 중심에 대해 남은 회전을 추정 할 수 있도록 합니다.​ 그림 5: ry는  global 방향, α는 local 방향, θ는 보는 각도입니다.Keypoint Estimation  그림 6에서 볼 수 있듯이, 8 개의 정점 {ki, i=1...8}, 3D 경계 상자의 하단 중앙 k9 및 상단 중앙 k10의 투영을 포함하는 각 객체에 대해 Nk = 10 개의 키포인트를 정의합니다. xr에서 Nk 키포인트로 local 오프셋 {δki = ki - xr, i = 1...Nk}을 L1 loss로 회귀합니다. 여기서 δki*는 정답값이고 Iin (ki)는 키포인트 ki가 이미지 내부에 있는지 여부를 나타냅니다. 그림 6: 키포인트에는 8 개의 꼭지점, 3D 경계 상자의 상단 중앙 및 하단 중앙 투영이 포함됩니다.​3.5. Adaptive Depth Ensemble  우리는 키포인트로부터 직접 회귀 및 M개의 기하학적 솔루션을 포함하여 M+1 개의 독립적인 추정기의 적응형 앙상블로 객체 깊이 추정을 공식화합니다. 먼저 이러한 깊이 추정기를 소개한 다음 이를 불확실성과 결합하는 방법을 제시합니다.​Direct Regression  객체 깊이를 직접 회귀하기 위해 [10, 52]에 따라 무제한 네트워크 출력 zo를 inverse sigmoid 변환을 사용하여 절대 깊이 zr로 변환합니다. 회귀된 깊이의 불확실성을 공동으로 모델링하기 위해 [11, 18, 19]를 따라 깊이 회귀를 위해 수정된 L1 loss를 활용합니다. 여기서 σdep는 회귀된 깊이의 불확실성입니다. 모델이 예측에 대한 신뢰가 부족하면 Ldep를 줄일 수 있도록 더 큰 σdep를 출력합니다. log (σdep)는 사소한 솔루션을 피하고 모델이 정확한 예측에 대해 낙관적이게 될 수 있도록 합니다.​Depth From Keypoints  알려진 카메라 매트릭스를 사용하면 픽셀 높이와 추정된 물체 높이 사이의 상대적 비율을 활용하여 물체 깊이를 계산할 수 있습니다. 이는 [5]와 유사합니다. 기준 모델에서 예측 치수의 상대적 오차는 높이, 너비 및 길이에 대해 5.2 %, 6.1 % 및 11.8 %입니다. 따라서 높이에서 깊이를 푸는 것은 방향 추정과 무관 할뿐만 아니라 치수 추정 오류의 영향을 덜받습니다. 그림 7과 같이 추정된 10 개의 키포인트는 3D 경계 상자의 5 개의 수직 지지선을 구성합니다. 각 수직선의 깊이 zl은 픽셀 높이 hl과 물체 높이 H에서 (11)과 같이 계산할 수 있습니다. 여기서 f는 카메라의 초점 거리입니다. 중심 수직선 zc의 깊이는 정확히 물체 깊이이며, 두 개의 대각선 수직 가장자리, 즉 z1과 z3 또는 z2와 z4의 깊이를 평균화하여 물체 깊이를 얻을 수도 있습니다. 따라서 추정 된 10 개의 키포인트는 3 개의 그룹으로 나누어지고 각각 중심 깊이 zc, diag1 깊이 zd1 및 diag2 깊이 zd2로 표시된 독립적 인 깊이를 생성합니다. 그림 7: 3D 경계 상자의 지지선 깊이는 개체 높이와 선의 픽셀 높이로 계산할 수 있습니다. 10 개의 키포인트를 3 개의 그룹으로 분할했으며, 각 그룹은 독립적으로 중심 깊이를 생성 할 수 있습니다.​  키포인트에서 계산된 깊이를 추가로 감독하고 불확실성을 모델링하기 위해 다음과 같이 불확실성이있는 L1 loss를 채택합니다. 여기서 z*는 정답값이고 Iin(zk)는 zk 계산에 사용되는 모든 키포인트가 이미지 내부에 있는지 여부를 나타냅니다. 보이지 않는 키포인트에서 계산된 ""유효하지 않은"" 깊이에 대한 로그 불확실성을 제거하면 모델이 완전히 비관적 일 수 있으므로 앙상블에서 이러한 깊이가 축소됩니다. 또한 불확실성을 업데이트하기 위해 이러한 유효하지 않은 깊이의 기울기를 제한합니다.​Uncertainty Guided Ensemble이제 M+1개의 독립적인 추정기로부터 M+1 개의 예측된 깊이 {zi, i=1...M+1}과 그들의 불확실성(σi, i=1... M + 1} 를 얻었습니다. 우리는 (13)에 나타낸 것 처럼, 불확실성 가중 평균, 이름하여 소프트 앙상블, 을 계산할 것을 제안합니다. 소프트 앙상블은 잠재적인 부정확한 불확실성에 대해 견고하면서 더 신뢰할 수있는 추정기에게 더 많은 가중치를 할당 할 수 있습니다. 또한 불확실성이 최소화 된 추정기가 최종 깊이 추정으로 선택되는 하드 앙상블을 고려합니다. 두 가지 앙상블 방식의 성능은 4.5 절에서 비교됩니다.​Integral Corner Loss[36, 42]에서 논의 된 바와 같이 여러 하위 작업의 개별 최적화는 서로 다른 구성 요소 간의 최적의 협력을 보장 할 수 없습니다. 따라서 L1 loss와 함께 추정된 차원, 방향, 오프셋, 그리고 소프트 깊이 zsoft에 의해 예측된 3D 경계 상자로부터, 우리는 8 개 모서리{vi=(xi,yi,zi),i=1,...,8}의 좌표를 감독합니다. 4. Experimentsㅌㅌ "
Object Detection YOLO 분석 ,https://blog.naver.com/sampoo00/222199813617,20210107,[참고]Object Detection / Yolo 관련 링크 정리 (tistory.com)​ 
customized 검출 모델 학습 코드_Object Detection Source Code for Learning ,https://blog.naver.com/kthchunjae/222713254899,20220427,"# 모델 학습 소스코드, 결과물은 가중치import tensorflow.keras.applications.mobilenet_v2import tensorflow.keras.layersimport tensorflow.keras.modelsimport tensorflow.keras.preprocessing.imageimport imutilsfrom imutils import pathsimport numpy as npbatch=100augmentation=Falseverbose=Falserecycle=Truemodelname=""tank""x_train=[]y_train=[]#One hot encoder. label 을 스트링 보다는 쓰기쉬운 정수값으로 바꿔서 수행. label 은 list of int#복잡한 label 구조 에서는 LabelBinarizer() 등을 사용 할 필요가 있지만,# 이 경우는, 표식의 유무만 따지는 것이므로 그냥 1/0 으로 직접 코딩함.data_directories=[[ [1],'../dataset/경로'],[ [0],'../dataset/경로']]for dir in data_directories:    n=0    filepaths=imutils.paths.list_images(dir[1]) #subdirectory 까지 찾아서 image file path/filename     for filename in list(filepaths):        # 학습 된 네트워크를 사용하므로, 그림 size가 맞아야 한다.        image=tensorflow.keras.preprocessing.image.load_img(filename,target_size=(224,224))        image=tensorflow.keras.preprocessing.image.img_to_array(image)        image=tensorflow.keras.applications.mobilenet_v2.preprocess_input(image)        x_train.append(image)         y_train.append(dir[0])        if(verbose):            print(dir[1])            print(dir[0])            print(image.shape)            break        if(n>batch): break        n+=1x_train=np.array(x_train)y_train=np.array(y_train)import sysmodel=Noneif recycle :    #우리의 모델이 학습한 것을 기반으로 계속 학습    model=tensorflow.keras.models.load_model(modelname)        for layer in model.layers:               if(layer.name==""mine""): break        layer.trainable=False #잊지마세요        if(verbose):            print(layer.name)            print(layer.trainable)      else:    BaseNN=tensorflow.keras.applications.mobilenet_v2.MobileNetV2(weights=""imagenet"",include_top=False,input_shape=(224, 224, 3))          mNN=BaseNN.output    mNN=tensorflow.keras.layers.AveragePooling2D(pool_size=(5,5),name=""mine"")(mNN)    mNN=tensorflow.keras.layers.Flatten()(mNN)    mNN=tensorflow.keras.layers.Dense(64,activation='relu')(mNN)    mNN=tensorflow.keras.layers.Dropout(0.5)(mNN)    mNN=tensorflow.keras.layers.Dense(2,activation='softmax')(mNN)    model=tensorflow.keras.models.Model(inputs=BaseNN.input,outputs=mNN)    for layer in BaseNN.layers: layer.trainable=False #잊지마세요    model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])history=Noneepoch=10if(augmentation): #우리 데이터가 적으므로, 데이터를 생성해서 학습한다.     aug=tensorflow.keras.preprocessing.image.ImageDataGenerator(        rotation_range=20)    history=model.fit(aug.flow(x_train,y_train),epochs=epoch,verbose=0)else:    history=model.fit(x_train,y_train,epochs=epoch,verbose=2)if history!=None:    import matplotlib    import matplotlib.pyplot as plt    matplotlib.use('Qt5Agg')    plt.plot(history.history['loss'],color=""b"")    plt.plot(history.history['accuracy'],color=""r"")    plt.xlim([0,epoch+1])    plt.show(block=False)    plt.pause(1)    plt.waitforbuttonpress(100)    plt.close()    #model.save(modelname) # 위에서 학습한 가중치를 통해 predict 하는 소스코드import numpy as npimport imutils.videoimport cv2import timeimport tensorflow.keras.modelsimport tensorflow.keras.preprocessingimport tensorflow.keras.applications.mobilenet_v2import sysmymodel=Falseglobal_confidence=0.8network_description='deploy.prototxt'weightfile=""res10_300x300_ssd_iter_140000.caffemodel"" facenet=cv2.dnn.readNet(network_description,weightfile)modelname=""mask_detector.model""if(mymodel): modelname=""mymask"" #우리가 학습시킨 표식을 사용함.masknet=tensorflow.keras.models.load_model(modelname)vs=imutils.video.VideoStream().start()#vs.release()time.sleep(2.0)frame=vs.read()(height,width)=frame.shape[0:2]print(frame.shape)#sys.exit()while True:    try:        frame=vs.read()    except:        ret=cv2.waitKey(1)         if(ret>30): break        continue         blob=cv2.dnn.blobFromImage(frame,1.0,(300,300))    facenet.setInput(blob)    faceout=facenet.forward()      for layers in faceout: # 1개        for layer in layers: #1 개                        for out in layer:                               object_probability=out[2]                if(object_probability<global_confidence): continue                #print(out[3:7])                box=out[3:7]*np.array([width,height,width,height])                (X1,Y1,X2,Y2)=box.astype(int) #여러 시도로 bounding box의 형식을 알아 내야함.                X1=max(0,X1);Y1=max(0,Y1);min(X2,width);min(Y2,height)                cropped=frame[Y1:Y2,X1:X2] # bounding box 안의 image만                 cropped=cv2.cvtColor(cropped,cv2.COLOR_BGR2RGB) #색 순서 변경                cropped=cv2.resize(cropped,(224,224)) # model 에 맞는 사이즈로                          cropped=tensorflow.keras.preprocessing.image.img_to_array(cropped)                            cropped=tensorflow.keras.applications.mobilenet_v2.preprocess_input(cropped)                             cropped=np.expand_dims(cropped,axis=0)                             predictions=masknet.predict(cropped)                if(predictions.shape==(1,2)):                    color=(255,0,0) # 잘 모를 때는 파란색                    if(predictions[0][0]>0.9): # (masked, nomasked)=predictions[0]                        color=(0,255,0) #마스크 초록색                    if(predictions[0][1]>0.9):                        color=(0,0,255) # 안 꼈을 때                    cv2.rectangle(frame, (X1,Y1),(X2,Y2),color,2)                         cv2.imshow(""Got IT"",frame)    ret=cv2.waitKey(1)         if(ret>30):        vs.stop()        breaktime.sleep(3.0)print(""A"")vs.stop()print(""B"")vs.stream.release()print(""c"")ret=cv2.waitKey(0) cv2.destroyAllWindows() ​ "
"Object Detection을 위한 Image Annotation, A to Z (3) ",https://blog.naver.com/panzer05/222259308401,20210228,"XML 파일에서 하기의 사항이 학습시키는데 필요할 것 같네요. ​이것을 파싱하려면, 과거에 했듯이 하나의 character를 읽어서 일일이 하는 방법도 있습니다. 그런데, 파이썬에서는 이미 이것을 Lib로 제공하고 있습니다. ​xml.etree.ElementTree 이것 인데요. 파이썬 2.6 부터 공식적으로 파이썬에 들어 왔다고 합니다. 지금은 대부분 3.x 대 버전이니까, 그냥 import 하면 될 것 같습니다. ​이것을 이용해서 파싱을 하면 된다는 것까지는 될 것 같고, 파싱한 후에 데이터를 저장해 놓아야 합니다. 학습할때 정답지로 사용해야 되기 때문입니다. ​그래서 하기와 같은 구조로 저장해 놓으려고 합니다.물론 개인의 입맛에 맞게 각자가 만들면 될 것 같은데, 전 이렇게 구상했습니다. ​[리스트 - xml 파일 하나의 정보        {딕셔너리 - width, height 정보 저장              [리스트  - 하나의 xml파일에 여러개의 object(bounding box)가 있을 수 있기 때문에 리스트로함                     {딕셔너리 - name과 xmin, ymin, xmax, ymax정보 저정                     }              ]        }]​이제 실제 코드로 들어가 보죠.​ 필요한 lib를 임포트 합니다. ​ 비행기 데이터가 있는 폴더를 지정하고, xml file과 image file list를 저장해 놓습니다. ​​ 위에서 설명한 파싱정보를 저장할 리스트에 for loop를 돌면서 각각 저장해 놓습니다. ElementTree 사용법은 위와 같이 사용하는 것 이외에 많은 방법이 있는데, 거는 이렇게 사용하는 것이 직관적이라 그냥 이것만 사용합니다. 그러나 더 좋은 스킬을 원하시는 분은 하기의 주소에서 확인 할 수 있습니다. ​https://docs.python.org/2/library/xml.etree.elementtree.html 19.7. xml.etree.ElementTree — The ElementTree XML API — Python 2.7.18 documentation19.7. xml.etree.ElementTree — The ElementTree XML API New in version 2.5. Source code: Lib/xml/etree/ElementTree.py The Element type is a flexible container object, designed to store hierarchical data structures in memory. The type can be described as a cross between a list and a dictionary. Warnin...docs.python.org 데이터가 잘 들어가 있는지 확인해 보겠습니다.  잘 들어가 있네요.​이미지 하나를 불러와서 data_dict에 있는 것으로 그려보겠습니다.  잘 나오네요.여기서는 matplotlib를 사용해서 그림을 그리고, 사각형을 overlay형태로 그린 것인데요. 파일을 오픈 할때는 opencv를 사용했습니다. 이유는, matplotlib 와 pillow는 이미지가 긴쪽을 무조건 가로로 생각해서 원본을 다르게 보이는 문제가 좀 있습니다.  사실 좀더 함수를 분석해 보면 방법이 있을 수도 있는데, 이것에 또 시간을 빼앗기긴 싫어서 그냥 3개 다 사용해 보니, opencv가 정상적으로 나와서 사용한 겁니다. ​하나가 정상적으로 출력이 되었으니, 이제 모두를 출력해 보겠습니다.  잘 나오는 군요. ​이것으로 annotation은 정리 될 것 같고, 이것으로 network 에 집어 넣어서 학습시킬 준비의 첫번째 단추를 끼운 셈이네요.​(너무 간단하고 직관적으로 짠것이라 필요는 없겠지만, 혹시나 해서 source file 첨부 합니다. )​- panzer - 첨부파일test.ipynb파일 다운로드 ​ "
object detection 셋팅/ yolo_mark/yolo/darknet ,https://blog.naver.com/kimmin2_/222173197742,20201214,"bounding box의 위치를 찾아주는 작업 : 로컬라이제이션​ ​데이터셋을 직접 만들어보자 [3] YOLO 데이터 학습필자는 컴퓨터 운영체제로서 'Linux Ubuntu 18.04.1 LTS'를 사용하고 그래픽 카드는 'GeForce GTX 970'을 사용한다. 이 글은 온전히 필자의 컴퓨터를 기준으로 작성했다. 이전 글에선 YOLO가 Detecting하는 방법들..writenkeep.tistory.com Yolo v3 custom 데이터 훈련하기Yolo v3 설치하기 2020/04/17 - [Computer Vision/Object detection] - Yolo v3 설치하기 Yolo v3 설치하기 object detection 분야에서 유명한 yolo의 설치법 및 사용법을 작성한다. 모든 딥러닝 라이브러리가 마..keyog.tistory.com ​ How to Rotate YOLO Bounding Boxes? · UR Machine Learning BlogHow to Rotate YOLO Bounding Boxes? 05 Mar 2020 • YOLO Rotate the image along with the bounding boxes. Image augmentation is a common technique in computer vision to increase the diversity of images present in a data set. One of the main challenges in computer ...usmanr149.github.io 참조​  Yolo_markgit clone https://github.com/AlexeyAB/Yolo_mark.git cmake가 없다고 오류가 나면  sudo apt install cmake open cv 오류가 나면 하기 명령어 sudo apt-get install libopencv-dev 기본적인 것들 깔아준다(gcc 같은것들) sudo apt-get install build-essential cmake .make./linux_mark.sh// 만약 실행이 안된다면chmod 777 linux_mark.sh 상기 경로에 이미지 파일을 올려놓으면 된다​​이미지를 올리고 ./linux_mark.sh 명령어를 실행하면 이렇게 창이 생기고 원하는 물체에 bounding box를 그려주면 된다그럼 이미지가 올라가있는 경로에 text파일이 생성되어있는데 내가 bounding box를 그려준 좌표값이 들어있다​  darknet git clone https://github.com/AlexeyAB/darknet.git ​Makefile 수정 후 저장 ​opencv 설치 sudo apt install libopencv-dev gpu가 설치가 되어있다면​gpu있는 컴퓨터인데 상기 경로에 cuda가 없다면 여기서 다운로드 CUDA Toolkit 11.1 Update 1 DownloadsPlease Note: We advise customers updating to Linux Kernel 5.9+ to use the latest NVIDIA Linux GPU driver R455 that will be available for download from NVIDIA website and repositories, starting today. Select Target Platform Click on the green buttons that describe your target platform. Only supported...developer.nvidia.com ​ make -j make를 하면 다크넷 폴더안에  다크넷폴더가 생긴 것을 볼 수 있다​​두개 설치를 하면  wget https://pjreddie.com/media/files/yolov3.weights./darknet detect cfg/yolov3.cfg yolov3.weights data/dog.jpg 귀여운 강아지 사진이 뜨는 것을 볼 수 있다  darknet/cfg 안에 yolov3.cfg라는 파일이 있다 --> 혹시 모르니 원본 카피해 주고 새로 진행​들어갔으면 yolo위에 convolutional의 filter 부분을 ↓↓ 이렇게 계산하여 적어준다classes는 훈련시키고자 하는 데이터셋의 class갯수, filter의 갯수는 (classes+5) * 3 으로 계산하여 적어준다 나는 진행 하는 품목이 바나나우유, (농심)육개장 라면, 참치캔, 가나초코렛, 라이터, 충전기(삼성,애플,LG..), 칫솔 이렇게 7개(7개의 클래스) 이기 때문에 filter 의 갯수가 (7+5)*3 = 36으로 바꿔줬다​yolo의 classes도 바꾸어 줘야한다  yolo_mark 폴더에 들어가면 이렇게 ​linux_mark.sh에 경로들이 써져있다  박스를 치고자하는 이미지 /     /yolo bounding box이름 ​이렇게 해주고 모두가 사진찍은 파일들을 모아서 데이터 전처리를 해보자​하는중 이름이 이렇게 된 파일이 들어와서 이름도 바꿔주고 (나는 class명_001.jpg 이런 형식을 쓰고 있다)   사이즈도 너무 큰 것들은 작게 만들어준다. 이렇게 하나하나씩 박스도 쳐준다 그럼 이렇게 텍스트 파일도 생기고 아까 git clone한 /darknet/data 에 들어가서 custom.data폴더를 만들자 classes = 7train = /home/mini/study/Yolo_mark/x64/Release/data/train.txtvalid = /home/mini/study/Yolo_mark/x64/Release/data/train.txtnames = /home/mini/study/Yolo_mark/x64/Release/obj.namesbackup = backup/ 학습할 모델 다운로드 wget https://pjreddie.com/media/files/darknet53.conv.74  훈련하기// 내가 custom.data / myyolov3.cfg 파일들을 만들어 놓은 곳의 경로를 적어준다./darknet detector train data/custom.data cfg/myyolov3.cfg darknet53.conv.74 | tee backup/train.log ​​훈련을 하려고 돌렸는데 에러가 났다.....;;;;; Cannot load image x64/Release/data/img/ramen004.jpg   Error in load_data_detection() - OpenCV 에러가 난이유를 구글링 해보니1. .data 파일에서 지정한 train.txt(또는 valid, test)의 파일 내부에 주소가 잘못된 경우YOLO_mark를 이용해서 Labeling을 해주면, train.txt에는 상대 경로로 이미지의 경로가 기록된다.   상대 경로를 사용하다 보니까 종종 발생하는 문제인데, 이럴 때는 경로만 잘 지정해주면 해결이 된다.​train.txt에 있는 이미지 경로를 절대경로로 바꿔주자파이썬코드를 짜서 간단하게 바꾸어주었다​욜로마크를 내가 쳤을때는 train.txt에 적혀져있어서 이 코드를 돌리면 되고 txt = open('/home/mini/study/Yolo_mark/x64/Release/data/train.txt','r')f = open('/home/mini/study/Yolo_mark/x64/Release/data/train2.txt','w')while True :    line = txt.readline()    if not line:        break    f.write('/home/mini/study/Yolo_mark/'+line)        txt.close()f.close() bounding box가 있는 파일들은 이렇게 폴더에서 읽어서 넣어준다 import ostxt = '/home/mini/study/Yolo_mark/x64/Release/data/img'filedir = os.listdir(txt)f = open('/home/mini/study/Yolo_mark/x64/Release/data/train.txt', 'w')for file in filedir:    if file.endswith('.jpg'):        writepath = os.path.join(txt,file)        print(writepath)        f.write(writepath+'\n')f.close() 나는 경로를 절대경로로 바꿔주니 학습을 제대로 시작하였다​  테스트하기컴퓨터로 사진을 찍으면 이름이 통일성 있지않은데 한번에 바꾸어 주는 코드 import ospath = '/home/ubuntu/Downloads/img'fileList = os.listdir(path)cnt = 5for filename in fileList:    nn = 'tuna_{}'.format(cnt)    newname = os.path.join(path, nn+'.jpg')    os.rename(path+'/'+filename,newname)    cnt+=1  darknet 폴더안에서 실행​test폴더를 만들고 그안에 이미지를 추가로 촬영하여 test폴더안에 넣어주고 그 안에있는 이미지를 사용한다​이미지를 전송할때 scp를 이용해서 전송해보았는데 무척이나 편리했다  이미지 인식 ./darknet detector test data/custom.data cfg/myyolov3.cfg backup/myyolov3_last.weights test/tuna.jpg 영상인식 ./darknet detector demo data/custom.data cfg/myyolov3.cfg backup/myyolov3_last.weights -ext_output test/TB.mp4 영상인식후 avi로 저장하고 싶을때 ./darknet detector demo data/custom.data cfg/myyolov3.cfg backup/myyolov3_last.weights -thresh 0.25 test/[test].mp4 -out_filename test/result.avi  ​training을 하다가 멈추다가 다시 restart하고싶을 때 ./darknet detector train cfg/coco.data cfg/yolov3.cfg backup/yolov3.backup | tee backup/train.log  mAP 값 보고싶을 때 ./darknet detector map data/custom.data cfg/myyolov3.cfg backup/myyolov3_final.weights  ​ "
딥러닝 객체검출 Object Detection 알고리즘 YOLO ,https://blog.naver.com/totalcmd/222738842408,20220519,"객체검출은 컴퓨터 비전 분야에서 가장 많이 활용되는 기술 중에 하나 인데요.​객체검출 알고리즘을 통해서 객체 분류, 검출, 인식을 하는데 주로 사용이 됩니다. 위와 같이 가장 기본적인 형태는 단일객체를 분류하는 것이고 진화된 형태에서는 위치까지 파악이 가능합니다.​여기에서 더 발전한다면 단일객체가 아닌 복수 객체가 있는 경우, 검출을 해내는 것이고 궁긍적으로는 모든 객체를 Segmentation까지 되는 단계가 최종 모습이라고 볼 수 있겠네요.​ ​객체 검출 알고리즘은 2001년 VJ Det를 시작으로 현재는 YOLO, SSD, RCNN, RFCN 등으로 발전되고 있습니다.​ 객체 탐지 알고리즘은 크게 두게로 나눌 수 있습니다.​첫번째는 Two-Stage / Region Proposal로 객체를 포함할 가능성이 높은 영역을 Selectice Serarch, RPN 등을 통해 영역제안을 통해 선택을 하게 됩니다. RCNN, Faster R-CNN, RFCN 등이 여기에 포함되며, 정확도가 높은 대신 Single Stage Mehods 대비 처리속도가 느립니다.​두번째는 Single-Stage 방식으로 정해진 위치와 정해진 크기의 객체만 찾는 방식입니다. 원본 이미지를 고정된 사이즈 그리드 영역으로 나누고, 각 영역에 대해 형태와 크기가 미리 결정된 객체의 고정 개수를 예측해내는 방식입니다.​YOLO, SSD 등의 알고리즘이 있으며, Two-Stage 방식 대비에서 정확도는 떨어지나 빠른 처리가 가능합니다. 빠른 속도를 이용하여 보통 실시간 탐지가 필요한 경우 많이 적용됩니다.​​1. YOLO (You Only Look Once)​2016년 YOLO가 나왔을 때 45 FPS의 속도를 기록했는데, 당시 Fast R-CNN이 0.5FPS, Faster R-CNN이 7FPS였기 때문에 속도면에서 월등한 성능을 보여줬죠. (R-CNN - Fast R-CNN - Faster R-CNN - YOLO 는 대략 10배씩 속도차이)​게다가 성능도 Faster R-CNN에 비해 크게 떨어지지 않았기 때문에 큰 주목을 받은 알고리즘입니다.​ ​YOLO는 CNN을 Base로 설계된 알고리즘 입니다.​YOLO는  IOU를 사용하여 개체를 완벽하게 둘러싸는 출력 상자를 제공합니다.​IOU(Intersection over Union)는 Bounding Box가 겹치는 방식을 활용한 개체 감지의 방법입니다.​ ​예측된 경계 상자가 실제 상자와 동일한 경우 IOU는 1 값을 갖게 되고, IOU를 활용하여 Threshold를 설정할 수 있습니다.​다음 이미지는 IOU 작동 방식에 대한 간단한 예시입니다. 파란색 박스가 Label 값이고, 노란색 박스가 예측값으로 두개의 박스가 겹치는 영역 비율에 따라 정확도를 판단할 수 있습니다.​ YOLO : S=7, B=2, C=20​네트워크 구조는 GoogleLeNet을 약간 변형시켜서 특징 추출기로 사용헸으며, Convloutin Layer 4회, FCN Layer 2회를 거쳐 7x7x30으로 Output를 나옵니다.​마지막 7x7x30이 예측 결과이며 이 안에 경계박스 및 클래스 정보가 들어있다고 볼 수 있습니다.​* 해당 포스팅에서는 간단한 개념만 살펴봅니다.​YOLO의 한계​YOLO는 강력한 장점이 있지만 몇가지 한계를 가지고 있습니다.​첫번째는 YOLO는 각 그리드 셀마다 오직 하나의 객체만을 검출이 가능합니다.​ ​YOLO는 특히 작은 물체에 대해 잘 수행되지 않습니다.​각 그리드 셀은 동일한 클래스의 B=2(Bounding Box 2개)로만 예측이 되기 때문입니다. 그럼에도 YOLO 알고리즘은 많은 분야에서 활용되고 있으며 현재도 계속 발전 진화되고 있습니다. ​ "
Android에서 Yolo detection 구현[참고] ,https://blog.naver.com/heennavi1004/222866932822,20220905,"안드로이드 기기, 모바일에서 YOLO 룰 구현하기 위한 참고​Android Yolo detection 구현 이곳  (2020. 6. 18) 안드로이드 기기, Java 코드 변경하여 YOLO 를 사용하는 방법이 자세히 코드와 함께 설명읽어보자Darkflow 설명도 있다: 이곳 Darkflow를 활용하여 YOLO 모델로 이미지 디텍션 구현 : 이곳 파이썬에서 YOLO v3 사용 예도 있다: 이곳 ​YOLOV3 - Android 이곳 (2020. 2. 3.) 안드로이드 설치 및 예를 보여줌간단한 사용팁을 알려줌​Real-Time Object Detection with Flutter, TensorFlow Lite and Yolo (2019, 5,19) Real-Time Object Detection with Flutter, TensorFlow Lite and Yolo -Part 1Implementing real time object detection with on device machine learning using Flutter, Tensorflow Liter and Yolo modal for an Android…blog.francium.tech Flutter 로 사물인식하는 안드로이드 앱을 FLutter로 제작하는 예 보여 줌 ​모바일 AI 프로젝트 모음 (2021, 5, 19)다양안 프로젝트를 리스트하였다. https://smilegate.ai/2021/03/19/awesome-tensorflow-lite/프로젝트를 알아보기 좋다.​Flutter realtime object detection with Tensorflow Lite (2021.12.27)Flutter로 실시간 탐지 앱 만들기​​​​ "
파이썬 ./object_detection/protos/anchor_generator_pb2.py: Permission denied 파이썬파일 권한 거부 에러  ,https://blog.naver.com/alaldi2006/222149491430,20201120,"./object_detection/protos/anchor_generator_pb2.py: Permission denied​https://github.com/protocolbuffers/protobuf/releases/tag/v3.4.0 Release Protocol Buffers v3.4.0 · protocolbuffers/protobufPlanned Future Changes Preserve unknown fields in proto3: We are going to bring unknown fields back into proto3. In this release, some languages start to support preserving unknown fields in prot...github.com Protoc 3.4 버전을 사용한다.그 이상의 버전을 사용할 경우 권한 거부 에러가 나기 때문임.가장 최근 버전인 3.14 깔았다가 자꾸 에러나서 겨우 알았네. "
Review — CornerNet: Detecting Objects as Paired Keypoints (Object Detection) ,https://blog.naver.com/phj8498/222345095231,20210510,"#CornerNet​CornerNet 짧은 리뷰 (velog.io)​CornerNet은 keypoint-based one-staged object detector이다. 왼쪽위, 오른쪽 아래 두 지점을 예측해서 경계박스를 만든다. MS-COCO 데이터셋에서 무려 42.2% AP라는 성능을 보여준다. ​Hourglass backbone을 써서 top-left, bottom-right 히트맵을 결과물로 뱉어낸다.Embedding은 left-top, right-bottom 코너들의 pair를 계산해서 경계 박스를 구성할때 쓰여진다.다른 network도 backbone으로 사용해보았으나 hourglass가 가장 성능이 우수하여 선택했다고 한다.Corner Pooling각 코너들의 히트맵을 예측할때 object의 가장자리에 있는 코너들은 local evidence나 pattern이 대부분 부족하다. CornerNet은 Corner Pooling이라는 컴포넌트를 이용하여, 더 코너를 잘 예측하겠끔 도와준다. Local evidence가 부족한 예​다 갈색이여서 어디가 어딘지 모를 수 있다... Hourglass에서 나온 feature map을 가로,세로로 이동하면서, max값을 채워버린다.   위 사진 처럼 feature map 가로, 세로 max pooling 한것을 더해서 output을 만든다.(그런데 여러 object가 많거나 겹칠경우 오류가 많을듯...) 전체적인 Overview​Heatmap으로 corner와 class를 찾고, embedding으로 pair를 찾는다. 성능은?  성능은 확실히 타 SOTA one-stage detector들 보다는 좋지만, 얘가 속도가 그렇게 빠른편이 아니다. 속도가 two-stage보다 더 느리다는 평이 있다.​ 코너넷은 왼쪽 상단 모서리와 오른쪽 하단 모서리를 감지합니다.Review — CornerNet: Detecting Objects as Paired Keypoints (Object Detection) | by Sik-Ho Tsang | Nerd For Tech | Apr, 2021 | Medium​코너넷: 미시간 대학의 페어링키포인트(코너넷)로 객체를 검색합니다. ​개체 경계 상자는 키포인트, 왼쪽 위 모서리 및 오른쪽 하단 모서리로감지되어 이전 단일 스테이지 검출기에서 일반적으로 사용되는 앵커 상자 세트를 설계할 필요가 없습니다.이것은 2018 년 ECCV에서 900 개 이상의 인용을 가진 논문입니다. ​개요코너넷: 네트워크 아키텍처코너 감지(히트맵 및 오프셋)코너 그룹화(포함)코너 풀링최첨단 검출기와의 비교​1. 코너넷: 네트워크 아키텍처 코너넷: 네트워크 아키텍처원래 인간의 포즈 추정에 사용되는 Newell ECCV'16에사용되는 모래 시계 네트워크는 백본으로사용됩니다.Newell ECCV'16을 백본으로 사용하는 것은 아마도 지금 CornerNet이 인간의 포즈 추정을 위한 네트워크의 목적과 유사한 키포인트를 감지할것이기 때문일 것입니다. 모서리의 예측 된 히트맵에 오버레이 된 경계 상자 예측의 두 가지 예. 왼쪽: 왼쪽 상단 코너 히트맵, 오른쪽: 오른쪽 하단 코너 히트 맵각 키포인트는 열지도로 예측됩니다.여기서 모서리는 키포인트로 처리됩니다.뉴웰 ECCV'16에서는머리, 어깨, 손 손바닥 등이 핵심 포인트로 취급됩니다.모래시계 네트워크에이어 두 개의 예측 모듈이 있습니다.하나의 모듈은 왼쪽 위 모서리에대한 것이고 다른 모듈은 오른쪽 아래 모서리용입니다.​ 히트맵, 포함 및 오프셋을 예측하기 위한 여러 가지.각 모듈에는 위와 같이 자체 코너 풀링 모듈이있으며 모래시계 네트워크에서 피처를 풀링한 다음 히트맵, 포함 및 오프셋을 예측합니다.모래 시계 네트워크의 깊이는 104입니다.다른 많은 최첨단 검출기와 달리 전체 네트워크의 마지막 레이어의 피처만 예측을 하는 데 사용됩니다.전체 훈련 손실은 다음과 입니다. Ldet가 히트맵의 검출 손실인 경우, Lpull 및 Lpush는 포함에 대한 손실이며 로프는 오프셋손실입니다. 이러한 손실은 아래 의 자세한 내용으로 설명됩니다.α β 0.1로 설정하고 γ 세트는 1로 설정됩니다.​2. 코너 감지 (히트맵 및 오프셋)​히트맵의 각 세트는 C가 범주의 수이며 H× W 크기인 C 채널이있습니다.pcij는 예측 된 히트맵에서 클래스 C에 대한 위치(i, j)에서점수가 되고, ycij가 인간의 포즈 추정과 유사한 정규화되지 않은 가우시안으로 증강 된 ""지상 진실""히트맵이 되도록 하십시오.또한 RetinaNet에서초점 손실의 사용으로, 검출 손실 Ldet는 다음과 같은: 여기서 N은 이미지의 개체 수, α=2 및 β=4입니다.히트맵에서 입력 이미지로 위치를 다시 매핑하면 일부 정밀도가 손실될 수 있으며, 이는 접지 진실로 작은 경계 상자의 IoU에 큰 영향을 줄 수 있습니다.따라서 위치 오프셋은 입력 해상도로 다시 매핑하기 전에 코너 위치를 약간 조정할 것으로 예상됩니다. 여기서 확인은 오프셋, xK 및 yk는 코너 k에대한 x 및 y 좌표입니다.빠른 R-CNN에서와 같이 매끄러운 L1 손실은 지상 진실 코너 위치에서 사용됩니다. ​3. 코너 그룹화 (포함) 네트워크는 동일한 개체에 속한 모서리에 대해 유사한 포함을 예측하도록 학습됩니다.Newell ECCV'16에있는 동료 포함에서 영감을 얻은 CornerNet은 왼쪽 상단 모서리와 오른쪽 하단 모서리가 동일한 경계 상자에 속하는 경우 포함 된 사이의 거리가 작아야한다는 것을 감지 한 각 모서리에 대한 포함 벡터를 예측합니다.포함물의 실제 값은 중요하지 않습니다. 포함 된 사이의 거리만 모서리를 그룹화하는 데 사용됩니다.etk는 오른쪽 하단 모서리에 대한 객체 K와 ebk의 왼쪽 상단 모서리에 대한 포함될 수 있습니다.""끌어오기"" 손실은 네트워크를 학습하여 모서리를 그룹화하는 데 사용되며 모서리를 분리하는 데 ""푸시"" 손실이 사용됩니다. 여기서 ek는 etk와 ebk의평균입니다. Δ=1.오프셋 손실과 마찬가지로 손실은 지면 모서리 위치에만 적용됩니다. 예측을 지상 진실로 대체하여 오류 분석지상 진실 히트맵만 으로AP는 38.5%에서 74.0%로 향상됩니다.예측된 오프셋을 기본-진실 오프셋으로 대체하면 AP는 13.1%에서 87.1%로 추가 증가합니다.이는 감지 및 그룹화 코너 모두에서 여전히 개선의 여지가 있지만 주요 병목 현상이 모서리를 감지하고 있음을 시사합니다.​3. 코너 풀링 종종 경계 상자 코너의 위치를 결정하는 로컬 증거가 없습니다.위와 같이 모서리의 존재에 대한 로컬 시각적 증거가 없는 경우가 많습니다.코너 풀링은 명시적 사전 지식을 인코딩하여 모서리를 더 잘 지역화하는 것이 좋습니다. 코너 풀링아래 방정식으로 공식화할 수 있습니다. 다음은 다음과 같습니다. 코너 풀링의 예왼쪽 위 모서리 풀링 레이어를 매우 효율적으로 구현할 수 있습니다.피처 맵은 수평 최대 풀링을 위해 오른쪽에서 왼쪽으로 스캔되고 수직 최대 풀링을 위해 아래에서 위쪽으로 스캔됩니다.그런 다음 최대 풀이 된 두 개의 피쳐 맵이 추가됩니다. MS COCO 유효성 검사에서 코너 풀링.코너 풀링의 경우 AP 2.0%, AP50 2.1%, AP75 2.2% 등 상당한 개선이 있었습니다.​4. 최첨단 검출기와의 비교 코너넷 과 MS COCO 테스트 개발에 다른 사람.다단계 평가를 통해 CornerNet은 42.1%의 AP를 달성하여 YOLOv2, DSOD, GRF-DSOD, SSD, DSSD,RefineDet와 같은 기존 1단계 방법 중에서 최첨단 기술을 능가합니다.코너넷은 캐스케이드 R-CNN과 같은 2단계 방법으로 경쟁력을 갖출 수 있으며, 마스크 R-CNN, 더 빠른 R-CNN, 레티나넷, 커플넷을 능가합니다.참조[2018 ECCV] [코너넷]​코너넷: 오브젝트를 페어링키포인트로 감지개체 감지2014: [OverFeat][R-CNN][ 2015] [빠른 R-CNN][빠른 R-CNN][MR-CNN & S-CNN][DEEPID-Net]2016: [OHEM] [크래프트] [R-FCN][ION] [ 멀티패스넷][Hikvision][GBD-Net/ GBD & GBD-v2] [SSD][ YOLOv1 ] [YOLOv1]2017: [NoC] [G-RMI] [TDM] [DSSD] [YOLOv2 / YOLO9000] [FPN][망막넷][DCN / DCNv1] [ 라이트 헤드R-CNN] [ 라이트 헤드 R-CNN][DSOD] [커플넷]2018: [YOLOv3][캐스케이드 R-CNN][메그데트][스테어넷][RefineDet][CornerNet]2019: [DCNv2] [이미지넷 사전 교육][GRF-DSOD & GRF-SSD] "
"Sep 25, 2019 [Direct SLAM + object detection][semantic SLAM] 오늘 논문 너무 안 읽음. 이러면 안돼! ",https://blog.naver.com/deanoh90/221659479501,20190926,"From 11:00pm to 11:20am논문 1.- Hachiuma, Ryo, et al. ""DetectFusion: Detecting and Segmenting Both Known and Unknown Dynamic Objects in Real-time SLAM."" arXiv preprint arXiv:1907.09127 (2019).https://arxiv.org/abs/1907.09127 DetectFusion: Detecting and Segmenting Both Known and Unknown Dynamic Objects in Real-time SLAMWe present DetectFusion, an RGB-D SLAM system that runs in real-time and canrobustly handle semantically known and unknown objects that can movedynamically in the scene. Our system detects, segments and assigns semanticclass labels to known objects in the scene, while tracking and reconstructing...arxiv.org - 특징 : 내용이 굉장히 재밌어 보임. 아래의 유튜브보고 전체 그림 파악하고 논문 정독https://www.youtube.com/watch?v=Ys3FXEP3A_4 - 기존의 문제 :1) 기존의 SLAM들은 static scene으로 가정 -> 즉, 움직이는 물체가 있으면 이를 outlier로 생각해서 tracking과 mapping 과정에서 제외- 해결하고자 하는 문제 :1) 정적인 scene이 아닌 known and unknown objects가 dynamic movement2) Instance segementation at full frame rate- 문제 해결을 위한 방법 :1) Tracking and mapping : ElasticFusion[1] 과 MaskFusion[2] 으로 영감을 받음. 즉, dense surfel maps 유지2) To the best of our knowledge, all previous work uses Mask R-CNN (or one of its predecessors) for the detection and segmentation of rigid and non-rigid objects. Mask R-CNN is a two-stage instance segmentation which is impressively accurate (40mAP on the MS COCO dataset) and also reasonably fast (5Hz). However, the single-stage detector YOLOv3 is much faster (30Hz), while still very accurate in comparison (30mAP). For each detected instance, YOLOv3 only returns a 2D bounding box and not a per-pixel mask. 즉, 기존 방법들은 R-CNN (5 Hz)을 사용했지만 저자는 YOLOv3 (30 Hz)를 사용해서 즉각적인 segmentation이 가능해짐- 결론 :1) It robustly tracks the camera pose in a highly dynamic environment and continuously estimates semantics and object geometry.2) 더 좋은 object detection 방법을 활용할 계획 (알고있는 물체를 항상 잘 찾는 것은 아니라고 함)3) BB (Bounding Box)의 intersection이 항상 robust 하지 않음.  From 11:35 pm to 11:55 pm논문 2.- MaskFusion: Real-Time Recognition, Tracking and Reconstruction of Multiple Moving Objectshttps://ieeexplore.ieee.org/abstract/document/8613746 MaskFusion: Real-Time Recognition, Tracking and Reconstruction of Multiple Moving Objects - IEEE Conference PublicationFile failed to load: https://ieeexplore.ieee.org/xploreAssets/MathJax-274/extensions/MathMenu.js Conferences > 2018 IEEE International Sympo... MaskFusion: Real-Time Recognition, Tracking and Reconstruction of Multiple Moving Objects Publisher: IEEE 3 Author(s) Martin Runz ; Maud Buffier ; Lourdes A...ieeexplore.ieee.org - 특징 : semantic SLAM, 이 논문은 특히 introduction에 있는 관련 논문 전체를 보자 (이번 주 주말까지)http://visual.cs.ucl.ac.uk/pubs/maskfusion/ MaskFusion: Real-Time Recognition, Tracking and Reconstruction of Multiple Moving Objects | UCL Visual ComputingWe present MaskFusion, a real-time, object-aware,              semantic and dynamic RGB-D SLAM system that goes beyond              traditional systems which output a purely geometric map of a static              scene. MaskFusion recognizes, segments and assigns semantic              class labe...visual.cs.ucl.ac.uk - Good to know :For instance, we show that often, being able to detect and segment people allows us to be aware of their presence, ignore those pixels and focus instead on the objects that they are manipulating.즉, 제거 -> 남은 것들 detection 가능하다고~?- 기존의 문제 :1) output a purely geometric map of a static scene.이 논문은 내일 오전 수업 끝나고 다시 정리하며, 특히 introduction에 있는 관련 논문 전체를 보자 (이번 주 주말까지)---------------------------------------19.09.25 11:55 pm 끝- 해결하고자 하는 문제 :1) - 문제 해결을 위한 방법 :+) MaskRCNN1) - 결론 :1)   공부Q)​  Ref[1] Whelan, Thomas, et al. ""ElasticFusion: Dense SLAM without a pose graph."" Robotics: Science and Systems, 2015.[2] Runz, Martin, Maud Buffier, and Lourdes Agapito. ""Maskfusion: Real-time recognition, tracking and reconstruction of multiple moving objects."" 2018 IEEE International Symposium on Mixed and Augmented Reality (ISMAR). IEEE, 2018. "
object detection model zoo (2) ,https://blog.naver.com/artistically22/222937210651,20221124,"jupyter notebook에 들어갔다면 먼저1. Image Collection.ipynb파일을 연다. !pip install opencv-python 으로 opencv 라이브러리를 설치한다. # Import opencvimport cv2 # Import uuidimport uuid# Import Operating Systemimport os# Import timeimport time 이후 필요한 라이브러리들을 import 해 준다.준비된 이미지들이 있다면 2. define images to collect 부터 4. capture images까지는 패스한다. !pip install --upgrade pyqt5 lxml 이미지에서 구역을 설정하고 label 하는 툴을 제공하는 라이브러리를 깔아준다. LABELIMG_PATH = os.path.join('Tensorflow', 'labelimg')if not os.path.exists(LABELIMG_PATH):    !mkdir {LABELIMG_PATH}    !git clone https://github.com/tzutalin/labelImg {LABELIMG_PATH} label하는 툴이 필요한 파일들은 위 깃허브 링크에서 가져와 복붙 해 준다.나 같은 경우에 위치는 c:\TFOD\TFODC\Tensorflow안에 깔아주었다.#가상공간은  c:\TFOD\TFODC에 깔려있다. !cd {LABELIMG_PATH} && python labelImg.py 이 명령을 입력함으로써 이미지에서 필요한 부분을 드래그 해 label 해주면 xml파일을 만들어주는 툴을 열 수 있다.이미지들이 들어 있는 폴더를 열고, w키를 눌러 드래그 툴을 활성화 시켜 준다.이후 만든 xml파일들과 이미지들을 함께c:\TFOD\TFODC\Tensorflow\workspace\images안에 넣고 train, test폴더를 각각 만들어 이미지들을 배치해 주었다.이걸로 ai공부자료를 만들었다.​출처: https://github.com/nicknochnack/TFODCourse GitHub - nicknochnack/TFODCourseContribute to nicknochnack/TFODCourse development by creating an account on GitHub.github.com ​​ "
[ML] list of deep learning object detection ,https://blog.naver.com/whalsgh0520/221631343034,20190828," Paper list from 2014 to 2019 ​하이라이트된 글씨는 먼저 공부하기 좋은, 중요도 높은 알고리즘들입니다.   방대한 양의 계보를 잘 정리해주신 글이 있어  하나씩 차근히 공부해보려 공유합니다.​https://github.com/hoya012/deep_learning_object_detection hoya012/deep_learning_object_detectionA paper list of object detection using deep learning. - hoya012/deep_learning_object_detectiongithub.com ​ "
[DL with Matlab #11] YOLO v2를 통한 Object Detection ,https://blog.naver.com/sonyi/222407124902,20210623,"YOLO까지 지원되는지는 몰랐는데 최근에 지원이 시작된 모양이다.정확히는 어떤 원리인지는 아직은 아래 그림을 봐도 정확히는 모르겠다. YOLO v2 network을 구성하는 방법은 아래와 같이 하면 된다. layerGraph = yolov2Layers(imageSize,numClasses,anchorBoxes,net,featureLayer,...                          ""ReorgLayerSource"",reorgLayer) net에는 pre-trained network을 집어넣을 수 있다. featureLayer에는 보통 RELU를 집어넣는다. ReorgLayerSource는 YOLO 레이어가 서로 다른 depth로 feature를 고려할 수 있게 해준다는 의미라는데 좀더 공부가 필요한 듯 싶다.​한번 예제를 살펴보자.앞서 고양이 구분하는 예제에서 했던 전처리 과정들을 쭉 아래에 다시 열거한다. rng(0)load petGroundTruth.matimds = imageDatastore(petGT.imageFilename);bxds = boxLabelDatastore(petGT(:,2:end));data = combine(imds,bxds);scaledData = transform(data,@scaleGT);function data = scaleGT(data)      targetSize = [224 224];    % data{1} is the image    scale = targetSize./size(data{1},[1 2]);    data{1} = imresize(data{1},targetSize);    % data{2} is the bounding box    data{2} = bboxresize(data{2},scale);end bounding boxes를 구하기 위해서는 anchor box를 구하는게 중요하다고 한다 (흠 뭔 소리인지?)이를 위한 함수는 아래와 같다. 두 번째 인자인 numBxs는 파라미터로서 몇 개의 박스가 있으면 좋겠냐는 거다. 보통은 class의 숫자와 같게 놓고 시작한다.  numClass = width(petGT)-1anchorBoxes = estimateAnchorBoxes(scaledData, numClass) 이제 pre-trained network을 선택하고 입력 image의 크기를 선택한다.  net = resnet18numClasses = width(petGT)-1imageSize = [224 224 3] 그리고 아래와 같이 layer를 만든다. feature extraction layer는 ""res5b_relu""이고 reorganization layer로는 ""res3a_relu""이다.  lgraph = yolov2Layers(imageSize, numClasses, anchorBoxes, net, ...                      ""res5b_relu"", ""ReorgLayerSource"", ""res3a_relu"") 자 이제 학습을 해야 하는데 아래와 같이 한다. detector = trainYOLOv2ObjectDetector(trainingData,layerGraph,options) 실제 이 detector를 쓰는 방법은 아래와 같다. box와 label이 생성되고 score 값이 생성된다. 이를 화면에 포함시켜서 보여줄 수 있다.  [dbox,dscore,dlabel] = detect(detector,dogim)detectedDogs = insertObjectAnnotation(dogim, ""rectangle"", dbox, ...                                      cellstr(dlabel))imshow(detectedDogs) ​ "
Keras Tutorial 중 Object Detection(MobileNet & SSD) 테스트 중 느낀 점 - 2 ,https://blog.naver.com/wujuchoi/221478748075,20190302,"1. 대상   - Object Detection Test 주1​2. 느낀점   - 사이트에서 제공하는 image는 테스트가 잘되나,   - google에서 Download한 image들은 잘 되지 않는다.    - MobileNetSSD 모델을 사용하는 데, train한 Dataset 대상이 궁금해진다.   - 결론은 train을 어떻게 하느냐에 따라 성패가 달린것 같다.     ( 다다익선? : 인프라 성능, 대량의 Dataset 들 ) ​주1) https://www.pyimagesearch.com/2017/09/11/object-detection-with-deep-learning-and-opencv/ "
Raspberry Pi4와 Tensorflow lite를 이용한 Ojbect Detection 예제 ,https://blog.naver.com/seodaewoo/222042009711,20200726,"Tensorflow lite는 Android, iOS, Raspberry Pi 등을 위한 tensorflow의 경량 버전으로, 이전 글에서 사용한 tensorflow보다 빠르기를 기대하면서 설치해 보았다.​출처 : https://www.tensorflow.org/lite/examples TensorFlow Lite 예시 | 머신러닝 모바일 앱Android, iOS, Raspberry Pi용 샘플 ML 앱 휴대기기에서 모델을 학습, 테스트, 배포하기 위한 자세한 안내가 포함된 엔드 투 엔드 예시를 확인하세요.www.tensorflow.org 위 사이트의 객체감지 - Raspberry Pi에서 시도해 보기 링크를 클릭하면 아래 사이트로 연결된다.https://github.com/tensorflow/examples/blob/master/lite/examples/object_detection/raspberry_pi/README.md tensorflow/examplesTensorFlow examples. Contribute to tensorflow/examples development by creating an account on GitHub.github.com Tensorflow Lite의 설치먼저 이전 글에서 사용된 것처럼 virtualenv 환경으로 진입한다. cd Desktop/tensorflowRaspvirtualenv envsource env/bin/activate 아래 사이트에 소개된 대로 Raspberry Pi용 Tensorflow Lite를 설치한다.https://www.tensorflow.org/lite/guide/python Python quickstart  |  TensorFlow LiteTensorFlow 학습 For Mobile & IoT 가이드 Python quickstart 목차 Install just the TensorFlow Lite interpreter Run an inference using tflite_runtime Learn more Using TensorFlow Lite with Python is great for embedded devices based on Linux,such as Raspberry Pi and Coral devices with Edge TPU ,among many othe...www.tensorflow.org pip3 install https://dl.google.com/coral/python/tflite_runtime-2.1.0.post1-cp37-cp37m-linux_armv7l.whl 예제 파일 다운로드 git clone https://github.com/tensorflow/examples --depth 1 예제 폴더로 이동 cd examples/lite/examples/object_detection/raspberry_pi ​예제 실행해 보기 python3 detect_picamera.py --model /tmp/detect.tflite --labels /tmp/coco_labels.txt 동작하는 것이 그냥 tensorflow를 사용하는 것보다 체감상 훨씬 빠르다.​시간날 때 소스분석을 해봐야겠다. "
[Boost Camp AI Tech] Object Detection p-stage Retrospect  ,https://blog.naver.com/gkswns3708/222539569793,20211017,https://gkswns3708.notion.site/Object-Detection-Retrospect-5556c2d87ac2456e811c98d766d30a04 Object Detection Retrospect팀 회고글gkswns3708.notion.site ​언제나 부족함을 느낍니다... 
Object detection papers ,https://blog.naver.com/brucks1217/222248991010,20210219,LiDAR point cloud for object detectionhttps://github.com/Yvanali/Deep-Learning-for-LiDAR-Point-Clouds/blob/master/md/Detection.md Yvanali/Deep-Learning-for-LiDAR-Point-CloudsDeep Learning for LiDAR Point Clouds in Autonomous Driving: A Review - Yvanali/Deep-Learning-for-LiDAR-Point-Cloudsgithub.com from 2017~ 2020​Camera for object detectionhttps://github.com/hoya012/deep_learning_object_detection hoya012/deep_learning_object_detectionA paper list of object detection using deep learning. - hoya012/deep_learning_object_detectiongithub.com from 2014~2020 
Object Detection on Thermal Images  ,https://blog.naver.com/brucks1217/222249140157,20210219,https://medium.com/@joehoeller/object-detection-on-thermal-images-f9526237686a Object Detection on Thermal ImagesRobust Object Classification of Occluded Objects in Forward Looking Infrared Cameras using Ultralytics Yolov3 and Dark Chocolatemedium.com https://github.com/joehoeller/Object-Detection-on-Thermal-Images joehoeller/Object-Detection-on-Thermal-ImagesRobust Object Classification of Occluded Objects in Forward Looking Infrared (FLIR) Cameras - joehoeller/Object-Detection-on-Thermal-Imagesgithub.com ​​https://github.com/enesozi/object-detection enesozi/object-detectionObject detection on thermal images(FLIR dataset). Contribute to enesozi/object-detection development by creating an account on GitHub.github.com ​https://github.com/tdchaitanya/MMTOD tdchaitanya/MMTODMulti-modal Thermal Object Detector. Contribute to tdchaitanya/MMTOD development by creating an account on GitHub.github.com ​ 
object detection 관련 깃허브 ,https://blog.naver.com/owl6615/221551957454,20190601,​​https://github.com/hoya012/deep_learning_object_detection hoya012/deep_learning_object_detectionA paper list of object detection using deep learning. - hoya012/deep_learning_object_detectiongithub.com ​https://github.com/amusi/awesome-object-detection amusi/awesome-object-detectionAwesome Object Detection based on handong1587 github: https://handong1587.github.io/deep_learning/2015/10/09/object-detection.html - amusi/awesome-object-detectiongithub.com ​​​​ 
[Object Detection] 성능평가 - IOU ,https://blog.naver.com/seohee653/222235460823,20210207,"#IOU #Intersection Over Union​Object Detection에서 성능평가를 위해 mAP를 주로 사용합니다. mAP를 알기 위해서는 먼저 IOU를 알아야합니다.​IOU (Intersection Over Union) 는 모델이 예측한 bounding box와 실제 bounding box가 얼마나 겹치는지를 나타내는 지표입니다.​ ​​ IoU값이 높을수록 실제와 비슷하게 예측했다고 볼 수 있다.​이렇게 얻은 IOU는 후에 예측한 결과값이 정답인지 아닌지를 구분하는 threshold로 쓰입니다.Dataset마다 이 값이 다른데 아래와 같이 각각 다른 값을 적용하는 것을 알 수 있습니다.​PASCAL VOC: 0.5ImageNet: min(0.5, wh/(w+10)(h+10))MS COCO: 0.5, 0.55, 0.6, .., 0.95​​참고 사이트1. http://ronny.rest/tutorials/module/localization_001/iou/2. https://www.kakaobrain.com/blog/48​ "
"[Detection,Segmentation] Mask R-CNN ",https://blog.naver.com/kona419/223066858494,20230406,"논문 : https://arxiv.org/abs/1703.06870 Mask R-CNNWe present a conceptually simple, flexible, and general framework for object instance segmentation. Our approach efficiently detects objects in an image while simultaneously generating a high-quality segmentation mask for each instance. The method, called Mask R-CNN, extends Faster R-CNN by adding a...arxiv.org Mask R-CNN은 Object Detection + Semantic Segmentation = Instance Segmentation.​AbstractMask R-CNN은 이미지에서 객체를 효율적으로 탐지한다. 동시에 각 instance에 대해 고품질 segmentation mask를 생성함.Faster R-CNN + 객체 mask를 예측하는 branch를 병렬로 추가하여 bounding box를 인식함. ​Introductionobject detection : 객체를 분류하고 각각의 bounding box를 이용해 localize함.semantic segmentation : 객체 인스턴스 구별 없이 고정된 카테고리로 pixel을 분류함. Faster R-CNN의 문제 : 네트워크의 입력과 출력사이의 pixel-to-pixel 정렬을 위해 설계된 것이 아님.==> 해결책 : RoIAlign - 정확한 공간적 위치를 충실히 보존함.​Mask R-CNNstage 1 : RPNstage 2 : class와 box offset을 병렬적으로 예측 Lcls = Classification LossLbox = bounding-box lossLmask = average binary cross-entropy losssoftmax를 픽셀 단위로 사용하고 multinomial cross-entropy loss를 사용하는 것이 다른 방식들과의 차이점이다.​ - Mask Representationmask : input 객체의 공간 레이아웃을 인코딩함.그러나 class, box 정보들은 FC layer 때문에 고정된 벡터로 변환되기 때문에 공간적 정보 손실이 발생함.mask는 conv 연산으로 공간적 정보 손실을 최소화 가능.Fully Conv. network를 사용해서 각 RoI에서 mxm 크기의 mask 예측 가능. mask 정보를 mxm 형태로 보존하기 위해서 RoI feature가 요구됨.​ - RoIAlign각 RoI에서 뽑아낸 작은 feature map을 input에 적절하게 정렬함.RoI의 경계나 bin의 quantization(양자화)를 피함. bilinear interpolation 연산을 사용해 각 RoI의 4개의 냄플링 포인트에서 input feature의 정확한 value를 계산.​ - Network ArchitectureResNet50, ResNeXt, FPN을 백본으로 사용함.FPN을 백본으로 사용했을 때 정확도와 속도 모두 좋음.Faster R-CNN의 Head에 Mask prediction branch를 추가함.​Experiments 백본으로 ResNeXt+FPN을 사용한 것이 정확도가 가장 높음 FCIS(위)와 Mask R-CNN(아래)를 비교한 것이다. 자세히 보면 FCIS의 segmentation이 조악하고 Mask R-CNN은 훨씬 깔끔하게 segmentation된 것을 알 수 있다. "
Object Detection - HOG 알고리즘 실습 편 (2)   ,https://blog.naver.com/dongju0531hb/222444041181,20210724,"#시각지능 #객체검출 #AI #비전인식 #인공지능 #HOG​0. Intro...저번 시간에 HOG 알고리즘에 대한 개념을 살펴보았다.핵심은이미지를 cell 구역으로 나누고,각 cell에 대해 oriented gradient histogram을 구한 뒤,block으로 cell을 묶어서 L1, L2 regularization을 한 후,이미지 각각의 cell에 대해 feature vector를 얻고,feature vector를 가지고 SVM으로 학습을 하면,영상에서 cell의 feature를 가지고 특정 object를 탐지하고 그 위치를 알 수 있게 된다.​오늘은 OpenCV 라이브러리에 있는 HOGDescriptor를 가지고 보행자 검출을 수행해보자!HOGDescriptor 클래스를 이용하면 특정 객체의 HOG 특징을 굉장히 쉽게 구할 수 있다.즉 보행자 검출을 위한 용도로 미리 계산된 hog 기술자 정보를 제공한다.​  1. HOGDescriptor 클래스 생성​먼저 HOGDescriptor 객체를 생성하자.기본 생성자를 이용하여, 객체를 생성하면 된다.이때 기본 생성자는, hyperparmeter 종류검색 윈도우cell 크기block 크기히스토그램 bin 갯수값64x128 pixels8x8 pixels16x16 pixels (2 x 2 cells)9 이렇게 한 이미지를 위한 HOG descriptor 하나에는 3780개의 float 실수로 구성된 feature로 표현되는 것이다.  ​1) 우선 hog 클래스 객체를 선언 HOGDescriptor hog;  2) 미리 계산된 보행자 검출을 위한 HOG descriptor 정보를 반환하는 멤버 함수 이용​64x128 크기의 윈도우에서 똑바로 서 있는 사람을 검출하는 용도로 이미 훈련된 분류기(classifier) 계수(coefficeint)를 return한다. 아래는 함수 원형 static std::vector<float> HOGDescriptor::getDefaultPeopleDetector(); 이 함수는 정적 member 함수이기 때문에, 소스 코드 작성 시에 클래스 이름과 함께 사용해야 한다. 즉 class단독으로 쓰이는 함수가 아님.. A.sum()이런게 아니라는 뜻..​​​  2. 이미 훈련된 분류기 계수를 SVM 분류기에 등록하기​HOGDescriptor 클래스를 이용하여 원하는 객체를 검출하려면 먼저 검출할 객체에 대해 훈련된,SVM 계수를 함수에 등록해야 한다.​전달인자인 svmdetector에 이미 훈련된 분류기 계수를 전달한다.  아래는 setSVMDetector( )함수 원형 virtual void HOGDescriptor::setSVMDetector(InputArray svmdetector); hog.setSVMDetector( HOGDescriptor::getDefaultPeopleDetector() ); ​​​  3. 분류기를 이용하여 보행자(서 있는 사람) 객체 탐지하기​1) frame 받아오기​우선 동영상을 받아오자. Mat frame;while (true) {   cap >> frame;        // 동영상 받아오기   if ( frame.empty())      break;            //frame 잘 받아왔는지 확인  2) HOG 기술자 분류기로부터 탐지된 결과값 가져오기​위에서 세팅된 SVM 분류기는 hog라는 이름의 HOGDescriptor 변수에 member로 존재하게 된다.그럼 detectMultiScale( ) 멤버 함수를 사용하게 되면 multiScale로 물체를 detect하게 된다.함수의 원형을 살펴보자 virtual void HOGDescriptor::detectMultiScale(InputArray img,                                             std::vector<Rect>& foundLocations,                                             std::vector<Double>& foundWeights,                                             double hitThreshold=0,                                             Size winStride=Size(),                                             Size padding = Size(),                                             double scale = 1.05,                                             double finalThreshold = 2.0,                                             bool useMeanshiftGrouping = false) const; ​ img입력영상, CV_8UC1 (흑백 영상) 또는 CV_8UC3(컬러 영상)foundLocations(출력) 검출된 사각형 영역 정보를 <Rect> 형태의 벡터 foundLocations에 저장한다.foundWeights(출력) 검출된 사각형 영역에 대한 신뢰도hitThresholdfeature Vector와 SVM 분류 hyperplane까지의 거리에 대한 임계값windStridecell 윈도우 이동 크기. 기본값은 셀 크기와 같게 설정된다. 값에 따라 block의 종류가 달라지겠다. padding패딩크기scale검색 윈도우 크기 확대 비율, 처음의 window 128x64 검출 실패시 해당 비율로 검색 window를 점차 크게 만든다.finalThreshold검출 결정을 위한 임계값. 기본은 2.0 임계값이 높을수록 정확한 형태로 잡혀야 검출이 된다.useMeanshiftGrouping겹쳐진 검색 window를 합치는 방법 지정 flag 기본값 false ​​   vector<Rect> detected;  hog.detectMultiScale(frame, detected);  // frame에서 보행자 탐지한 영역의 직사각형 정보는 detected에 저장된다.  for( Rect r : detected ){    Scalar c = Scalar(rand()%256, rand()%256, rand()%256); // 탐지된 직사각형을 랜덤한 색상으로 테두리 칠하기 위해서    rectange(frame, r, c, 3);  // frame에 r 직사각형을 c 색깔로 3 두께로 칠하자  }    imshow(""frame"", frame);    if (waitKey(10) == 27 ) // ESC를 누르면 while문 탈출      break;  }destoryAllWindows(); // 창 닫기return 0; ​​​​  4. 코드 #include ""opencv2/opencv.hpp""#include <iostream>using namespace cv;using namespace std;int main() {	VideoCapture cap(""vtest.avi"");	if (!cap.isOpened()) {		cerr << ""Video open failed!"" << endl;		return -1;	}	// 검출된 동영상 저장을 위한	int w = cvRound(cap.get(CAP_PROP_FRAME_WIDTH));	int h = cvRound(cap.get(CAP_PROP_FRAME_HEIGHT));	double fps = cap.get(CAP_PROP_FPS);	int fourcc = VideoWriter::fourcc('D', 'I', 'V', 'X');	int delay = cvRound(1000 / fps);	VideoWriter outputVideo(""pedestrian.avi"", fourcc, 15, Size(w,h));	if (!outputVideo.isOpened()) {		cout << ""File open failed!"" << endl;		return -1;	}	// hog 알고리즘	HOGDescriptor hog;	hog.setSVMDetector(HOGDescriptor::getDefaultPeopleDetector());		Mat frame;	while (true) {		cap >> frame;		outputVideo << frame;		if (frame.empty())			break;		vector<Rect> detected;		hog.detectMultiScale(frame, detected);				for (Rect r : detected) {			Scalar c = Scalar(rand() % 256, rand() % 256, rand() % 256);			rectangle(frame, r, c, 3);		}		imshow(""frame"", frame);		if (waitKey(10) == 27)			break;	}	return 0;}  5. 결과   모르는 함수 내용은 질문으로 남겨주세요openCV 4로 배우는 컴퓨터 비전과 머신 러닝​ "
Imbalance Problems in Object Detection : 객체 검출 분야의 불균형 문제 A Review(1) ,https://blog.naver.com/phj8498/222346515902,20210511,"#object detection#객체검출불균형#imbalancedata​해당 논문은 객체 검출 영역에서 다양한 불균형 문제들을 검토하고 식별하는 논문입니다. 34페이지 짜리의 굉장히 긴 논문이며, object detection 부터 classification을 공부하시고 연구하시는 분들에게 굉장히 도움이 될거 같아서 정리를 블로그에 공유하게 되었습니다.해당 논문에서는 object detection의 기본적인 용어 및 개념부터 과거부터 최신논문의 연구동향을 파악하기 좋고 실제 학습하고 실험하시는 분들이 겪을 여러 문제들에 대해서 참고가 될 내용이 많아 최대한 자세히 작성하였으니 도움이 되길 바랍니다. 그럼 바로 논문 리뷰 시작합니다. 해당 논문은 part1~part3까지 포스팅될 예정입니다.2020/11/03 - [지식 저장소] - Imbalance Problems in Object Detection : 객체 검출 분야의 불균형 문제 A Review(2)2020/11/03 - [지식 저장소] - Imbalance Problems in Object Detection : 객체 검출 분야의 불균형 문제 A Review(3)​요약논문에서는 객체 검출에 대한 포괄적인 검토를 할것임을 얘기한다. 문제를 분석하기 위해 문제를 기반으로 분류하여 설명하며, 이 분류에 따라 각 문제에 대해 심층적으로 살펴보며 다양한 솔루션에 대해 통합적이고 중요한 관점을 제시한다. 또한, 기존 불균형 문제와 이전에 논의되지 않은 불균형 문제에 대해 미해결된 것을 파악한다고 한다.1 소개객체 검출은 주어진 이미지에서 물체의 카테고리(클래스)와 위치(BBOX)를 동시에 진행하는 것이다. 이 것은 Computer vision 분야(자율주행, 의학적 견해,Robotics에서 일어나는 문제) 에서 관제하는 응용프래그램들에게서 기본적인 문제들이 발생한다.Object detection(이하 'OD')이 머신러닝에서 사용된 이후로, OD의 1세대는 Hand-craft Feature 와 선형, max-margin분류기에 의존하는 형태 였다. 1세대의 가장 성공적이고 대표적인 방법은 Deformable Parts Model(DPM)이었다.DeformablePart Model 참고 자료닫기 Deformable Parts의 구조와 Inference 방법inst.eecs.berkeley.edu/~cs280/sp15/lectures/11.pdfDPM은 HOG feature pyramid를 이용하여 객체 전체의 기반이 되는 Root filter와 객체의 부분별 Part filter를 이용하여 SVM을 통해 훈련하며, Linear 매개변수(Filter score, spring cost) 를 이용하여 score를 산출한다. 부분별 filter를 통해 부분별 예측에 성공하면 각 부분들을 합쳐 하나의 object를 예측한 것처럼 만드는 방법이다. 그 이후, 가장 영향력있는 것은 Krizhevsky를 시작으로, Computer vision 분야에 딥러닝이 적용되기 시작했으며, OD 또한 마찬가지로 연구되어 왔다. 현재 세대의 OD 방법은 모두 딥러닝을 기반으로 제안되고 있다. 이 변화는 상당한 성능향상을 가져왔다. (PASCAL VOC 데이터 셋을 기반으로 DPM이 mAP 0.34 이었지만 현재 대부분 mAP 0.80을 웃돈다.)최근 5년간, OD의 주 영향력을 딥러닝이 가져오고 있지만서도, 불균형 문제는 OD의 여러 수준에서 상당한 관심이 쏠리고 있다. Input 에 대한 불균형 문제는 해당 속성의 대한 분포가 성능에 영향을 미친다. 이 것이 해결되지 않은 경우, 최종 detection 성능에 안좋은 영향을 미친다. OD에서 가장 일반적인 불균형 문제는 전경과 배경의 불균형으로 positive의 수와 negative의 수가 극도로 불균형하다. 이미지 한장을 주었을때, positive 객체는 몇가지 없지만, negative에 속하는 객체가 무수히 많기 때문이다.해당 논문에서는, 딥러닝 기반의 OD 논문을 검토하고 8가지의 불균형 문제를 식별한다. 크게 4가지의 불균형 문제로 분류한다. Class imbalance, Scale imbalance, Spatial imbalance, Object imbalance. Table 1 : Imbalance  problems reviewed in this paper. We state that an imbalance problem with respect to an input property occurs when the distribution regarding that property affects the performance. The first column shows the major imbalance categories. For each Imbalance problem given in the middle column, the last column shows the associated input property concerning the definition of the imbalance problemClass imbalace (클래스 불균형)서로 다른 클래스에 속하는 데이터의 양에 불균형이 있을 때 발생전경과 배경의 불균형클래스(positive)간의 불균형Scale imbalance (크기 불균형)Input의 크기 불균형이 존재할 때 발생객체들의 크기 불균형Feature 들의 크기 불균형Spatial imbalance (공간 불균형)중심 위치, IOU 같은 Box regression 과정에서 일어나는 문제Regression Loss 불균형IOU 분포 불균형 (한 객체에 대해 너무 넓게 Box들이 분포)객체 위치 불균형Object imabalance (객체 불균형)최소화 해야할 loss function이 너무 많을 때 발생1.1 Scope and Aim불균형 문제는 머신러닝, Computer vision, 패턴 인식에서 넓은 범위를 가지지만 논문에서는 OD에 대해서만 제한적으로 이야기한다. State-of-the-art(이하 'SOTA') 등록되어 있는 논문들을 가지고, 논의 한다.물체 감지의 포괄적인 배경을 말하는 것이 논문의 목표는 아니지만 일련의 배경지식이 필요하다고 한다. 이는 SOTA 의 OD section 에 대한 것이다. 아래는 논문에서 말하는 목표이다.불균형 문제를 식별 및 정의하고 해결책을 위한 분류체계 제안기존 연구를 통합시키려는 동시에 비판적으로 논문을 검토(주요 접근 방식, 특정 솔루션의 심층 분석, 비교 및 요약)Open issue 및 문제에 대해 제시 및 논의객체감지 이외의 영역은 다루지 않음Github 주소를 통해 문제기반 지속적인 업데이트(https://github.com/kemaloksuz/ObjectDetectionImbalance)1.2 Comparison with Previous Reviews해당 논문에서는 인기 있는 데이터셋에서의 연구를 다룰 것이다. 불균형 문제의 분류에 초점을 맞추어 집중적으로 다룰것이며, OD 분야에서 딥러닝 이전의 방법을 제시하거나 딥러닝 이후의 방법을 제시하는 논문 일지라도, imbalance에 대해 중요한 부분이 있다면 해당 논문들도 다룬다고 한다. 또한, 머신러닝 분야에서의 불균형문제도 다루는데, 이러한 것은 전경 클래스 불균형 문제에 제한하여 다룬다고 한다.이러한 방법들로 다양한 논문을 참조하지만, 초점을 OD와 imbalance에 두고 참조하겠다라는 것이다.1.3 A Guide to Reading This Review해당 논문을 읽는 방법(?)과 논문의 Section들에 대해 설명한다. 하지만 필자는 전부 읽어볼 생각이기 때문에 해당 부분은 패쓰2 Background, Definitions And NotationSOTA의 OD분야를 간략하게 제공하고 표기법 용어를 제시하고 넘어간다고 한다.2.1 State of the Art in Object Detection해당 부분에서는 Object detection 의 Keyword 내용을 확인하며 넘어가도록 하겠다.Object detection의 두가지 주요 접근방식상향식(top-down), 하향식(bottom-up)하향식 모델이 비교적 인기있는 상황, 상향식은 최근에 제안됨.하향식은 Object를 감지할때 탐지 파이프라인 초기에 평가를 하는 방식(i.e. anchors, regions-of-interests/proposals)상향식은 나중에 처리 파이프라인에서 keypoint, parts 같은 하위 개체를 그룹화하여 Object를 탐지하향식 모델one-stage모델과 two-stage모델Two-stage 모델R-CNN, Faster R-CNN 계열의 모델sliding window를 통해 proposal 메커니즘 사용하여 영역 검출(ROI)ROI의 카테고리를 분류한후 NMS를 통해 post-processingOne-stage 모델SSD, YOLO, RetinaNet 계열의 모델카테고리 분류와 Box 검출을 동시에 진행상향식 모델객체의 중요한 key-points (모서리, 중심점 등) 먼저 예측key-points 를 이용하여 전체 객체 인스턴스를 형성 및 그룹화 Fig. 1 : (a) The common training pipeline of a generic detection network. The pipeline has 3 phases (i.e. feature extraction, detection and BB matching, labeling and sampling) represented by different background colors. (b) Illustration of an example imbalance problem from each category for object detection through the training pipeline. Background colors specify at which phase an imbalance problem occurs.2.2 Frequently Used Terms and Notation해당 부분에서는 SOTA 에서 사용되는 수식이 정리된 표와 Keyword들이 어떤것이 있는지 살펴보고 넘어가도록 한다. Table 2 : Frequently used notations in the paperFeature Extraction Network/Backbone : 객체 검출시 Input image를 받는 NetworkClassification Network/Classifier : backbone에서 추출한 feature에서 분류 결과 까지 포함되며, confidence score를 표시Regression Network/Regressor : backbone feature를 기반으로 box coordinate를 생성하는 부분이며, x,y축에 대한 값을 추출Detection Network/Detector : classifier 와 regressor 두개를 포함하여 detection 결과로 변환Region Proposal Network (RPN) : 2-stage 모델에서 사용되며 backbone을 통해 생성된 proposal을 가지고 confidence score와 box coordinates를 포함Bounding Box : [x1,y1,x2,y2]가 일반 적이며 detection된 box의 정보를 나타냄Anchor : 2-stage 모델에서는 RPN에 1-stage 모델에서는 감지 부분에 사전에 정의된 Box 셋Region of Interest (ROI)/Proposal : RPN 에서 사용하는 proposal 메커니즘으로 생성된 박스 셋Input Bounding Box : detection network나 RPN에서 훈련시 사용하는 Anchor나 ROI 샘플Ground Truth : class, label, box등 사용자의 전처리된 데이터 셋Detection : (box정보, 클래스별 confidence score) 형식의 Output 데이터 Intersection Over Union (IOU) : ground truth의 box와 detection output box와 겹침 정도 GIOU, CIOU, DIOU 등 여러 함수가 있다.Under-represented Class : 훈련시 데이터셋 또는 미니배치에 샘플이 적은 클래스 (클래스 불균형)Over-represented Class : 훈련시 데이터셋 또는 미니배치에 샘플이 많은 클래스 (클래스 불균형)Backbone Features : 백본에 적용하는동안 포함되는 feature setRegression Objective Input : 몇몇 방식에서는 log 도메인에서 Box를 직접 예측하게 변환하여 예측하는데, 이 때 명확성을 위해 모든 방법에 대한 regression loss 입력을 log에 표기함3 A Taxonomy of the Imbalance Problems and their Solutions in Object Detection Fig. 2: Problem based categorization of the methods used for imbalance problems. Note that a work may appear at multiple locations if it addresses multiple imbalance problems - e.g. Libra R-CNNOD 과정에서 일어나는 다양한 (Figure1 참고) imbalance문제들을 한번에 분류된 정보를 확인하고 세세한 내용은 다음장 부터 설명한다. (Figure2 확대해서 참고) 논문에는 위 그림에 대한 내용들을 Class imbalance 부터 설명하지만 그림을 보면 분류에 대한 내용이 상세히 다시 적혀 있기 때문에 그림을 확인하며 바로 다음 장으로 넘어가도록 하겠다. Fig. 3 : Number of papers per imbalance problem category through years.Imbalance 카테고리의 논문은 매년 증가중이다. (Figure3 참고)4 Imbalance 1 : Class Imbalance클래스 불균형은 데이터셋안에서 너무 많은 데이터를 가진 클래스에서 관측된다. 이것은 두가지로 나눌 수 있는데 위에서 말했던 배경-전경 불균형과 전경-전경 불균형이다. Fig. 4: Illustration of the class imbalance problems. The numbers of RetinaNet anchors on MS-COCO are plotted for foreground-background, and foreground classes. The values are normalized with the total number of images in the dataset. The figures depict severe imbalance towards some classes.Figure4는 RetinaNet의 MS-COCO 데이터셋 기반의 Anchors 셋을 나타낸 그래프이다. IOU threshold는 IOU>0.5 는 전경(Foreground), IOU<0.4 는 배경(Background)으로 분류했을때, 두가지 타입의 Class불균형이 모두 나타났다. 이는 서로 다른 솔루션이 적용되어 해결되거나, 일부 솔루션은 두가지를 모두 해결한다.4.1 Foreground-Background Class ImbalanceDefinition 과량의 Background 클래스와 소량의 Foreground 클래스는 훈련중에 문제가 발생하며 이는 Background에 대한 labeling작업은 하지 않기에 클래스당 example수에 의존하지 않는다.Solutions (i) hard sampling methods, (ii) soft sampling method, (iii) sampling-free method, and (iv) generative methods 네가지를 제시한다.4.1.1 Hard Sampling MethodsHard sampling method는 OD에서 불균형 해결을 위해 자주 쓰이는 방법이다. 이 것은 cross-entropy의 계수를 0,1로 이진화 시키는 것이다. 쉽게 말하면 positive와 negative선택을 쉽게 만들며, 선택되지 못한 example을 모두 무시한다.random sampling은 직관적인 Hard sampling method이다. 단순함에도 R-CNN계열의 모델에서 사용되며, RPN학습을 위해 positive Box가 요구되는 값보다 적으면 미니 배치에 무작위로 샘플링된 negative 로 채워진다. 물론 좋지는 않음.Hard-example mining methods 는 loss가 큰 예제에서 더 많이 훈련하면 성능이 향상된다라는 가설에 의존한다. 이 는 negative의 하위집합을 사용하여 초기 모델을 학습한 다음, 실패한 classifier를 훈련시켜 새로운 classifier를 얻는다. 이 과정을 반복하여 여러 classifier를 얻게 된다. SSD는 이를 채택하여 사용하며 가장 높은 loss를 갖는 negative를 선택하여 훈련한다. 하지만 훈련속도의 저하와 추가 메모리가 필요한 점이 있고, 이를 해결하기 위해 IOU를 기반으로 다시 샘플링 하는 방법을 채택하였다.Limit the search space 는 2-stage 모델에서 anchor 중 가장 가능성이 있는 ROI를 찾은 다음 가장 높은 score를 가진 상위 N개의 ROI를 선택하는 것을 목표로 한다. 이러한 방법으로 positive sample은 모두 훈련에 사용되며, negative sample은 균형에 맞게 훈련에 사용될 수 있도록 유도한다. (즉, 너무 많은 negative sample을 거르는 방법)4.1.2 Soft Sampling MethodsSoft Sampling Method는 Hard Sampling Method처럼 sample을 삭제하지 않고, 상대적 중요도에 따라 sample의 가중치를 조정하여 모든 sample이 훈련에 사용될 수 있도록 한다. 상대적으로 anchor수를 적게 가져가는 YOLO 모델은 배경클래스의 loss의 가중치를 0.5로 설정하여 사용한다. Focal Loss 는 hard example에 가중치를 동적으로 할당하는 함수 중 선구적인 예이다. γ = 0이면 일반 cross entropy 로 동작하지만, γ = 2이면 hard와 easy사이 trade-off가 된다. Gradient Harmonizing Mechanism(GHM) 은 easy example에 대해 gradient를 억제한다. 즉, small gradient와 large gradient는 매우 많은데에 비해 midium gradient는 제한되어 있으므로, 유사한 gradient샘플에 대하여 그 수가 많을 경우 패널티를 주는 계수 기반 접근 방식이다. PrIme Sample Attention(PISA) 는 서로 다른 기준에 따라 positive와 negative 에 가중치를 할당한다. IOU가 높은 positive가 선택되는 동안, negative는 더 큰 foreground score로 승격한다 (?) 즉, IOU와 classification score를 기반으로 각 클래스의 순위를 매겨 정규화된 순위를 얻는다. 이를 통해 순위별 가중치를 할당하여 준다.  Table 3 : A toy example depicting the selection methods of common hard and soft sampling methods. One positive and two negative examples are to be chosen from six bounding boxes (drawn at top-right). The properties are the basis for the sampling methods.4.1.3 Sampling-Free MethodsResidual Objectness는 위에서 언급된 hand-crafted sampling heuristics를 피하기 위해 등장한 방법으로 파라미터의 수를 줄였으며, 점수를 예측하기 위해 branch를 추가시켰는데, 이는 전경-배경 불균형을 해결하는 동안, classification branch는 positive 클래스만 처리한다. classification score와 Residual Objectness score가 곱하여 출력한다. 이 것을 통해 cascaded pipeline이 성능을 향상 시킨다는 것을 알 수 있다.4.1.4 Generative Methods Table 4 : Comparison of major generative methods addressing class imbalancesampling-based, sampling-free와 다르게 generative method는 불균형을 직접적인 데이터 생성을 통해 해결한다. GAN을 사용하여 데이터를 생성하며, feature map을 생성하여 더 어려운 예제를 생성하며 네트워크와 End-to-End로 훈련 파이프라인에 통합된다.4.2 Foreground-Foreground Class ImbalanceDefinition 과량과 소량의 Foreground 클래스간의 불균형문제가 일어나는 경우이다. 이 문제는 두가지의 카테고리로 나누며 (i) dataset-level, (ii) batch-level 로 나누어 확인한다.4.2.1 Foreground-Foreground Imbalance Owing to the Dataset Fig. 5 : Some statistics of common datasets (training sets). For readability, the y axes are in logarithmic scale. (a) The total number of examples from each class. (b) The number of images vs. the number of examples. (c) The number of images vs. the number of classes.Definition 객체는 본디 자연에서 다른 빈도로 존재한다. 그러므로 데이터셋의 객체간의 불균형은 자연스러운 것이다. 그러나 데이터 셋에서 클래스 간의 차이는 충분히 있고, 이는 데이터가 많은 클래스로 overfitting되는 것이 불가피할 수 있다.Solutions 훈련과정에서 얻을 수 없는 새로운 이미지 또는 box를 생성하여 foreground-foreground 문제를 해결할 수 있다. 또 다른 방법으로는 finetuning long-tail distribution for object detection 이 있다. 이 방법은 시각적 유사성을 기반으로 clustering을 한다. (i) the accuracy of the prediction, (ii) the number of examples 두가지 를 바탕으로 클래스간의 유사성을 측정하고, 불균형을 처리하기 위해 그룹화를 하여 학습합니다. 즉, Input BBox에 대해 최종 detection을 결정하는 것은 SVM이 됩니다.4.2.2 Foreground-Foreground Imbalance Owing to the BatchDefinition 객체의 Class 분포가 고르지 않아 학습과정에서 bias가 생길 수 있다. 즉, 무작위로 sampling하여 batch에 삽입하는 경우, 해당 batch에 하나의 클래스가 너무 많은 경우, 모델이 해당 클래스에 편향되어 학습될 수 있다.Solutions Online Foreground Balanced(OFB)를 통해 불균형을 완화한다. 말하자면, 배치 수준에서 positive가 적은 클래스를 같이 유입시키면서 불균형을 해소 한다. 방법 자체는 이론적으로 합당하지만 실제로 성능향상을 가져오진 않았다고 한다. 해당 논문에서도 Open Issue로 논의할 것을 제안한다.4.3 Comparative Summary예전에는 균형잡힌 훈련을 위해 Hard sampling을 사용하였지만 요즘은 모든 샘플을 사용할 수 있는 Soft sampling이나 Sampling-free를 사용한다. 이를 통해 균형 잡힌 훈련으로 나아가고 있고, 과거의 논문들을 통해 우리는 어려운 example을 투입하는 것 보다, 작은 loss값을 발생 시킬 수 있는 높은 IOU를 가진 positive가 AP향상에 더 도움이 된다는 것을 알 수 있다.4.3 Open Issue해당 글에서는 Open Issue에 대해 다루지 않으며, 논문에 제시된 Issue만 간단하게 언급하고 넘어간다.4.4.1 Sampling More Useful Examples유용한 예를 식별하기 위한 많은 기준이 필요하다.4.4.2 Foreground-Foreground Class Imbalance ProblemForeground-Background 문제에 비해 해결되지 않은 부분중 하나 이다.4.4.3 Foreground-Foreground Imbalance Owing to the Batch균형잡힌 데이터를 배치안에 넣는 법에 대해 필요하다.4.4.4 Ranking-Based Loss Functions클래스 별로 최적의 임계값이 다를 수 있다는 것을 인지하지 않고,전체의 AP loss를 사용한다. 클래스 별로 신뢰도 점수를 정렬하는 것이 더 나은 성능을 얻을 수도 있다.​​출처: https://keyog.tistory.com/40?category=879582 [인간지능이 인공지능을 공부하는 장소] "
Image Detection 방법론: RCNN ,https://blog.naver.com/ygw5455/223096236608,20230508,"RCNN​논문 제목 : Rich feature hierarchies for accurate object detection and semantic segmentation​딥러닝 image detection의 시초가 됐던 논문이다. 딥러닝 기법을 이용해서 image detection을 효과적으로 풀었다는 것에 큰 의의가 있다. 결론부터 이야기하면 이 방법론은 매우 간단하다. feature extractor 인 cnn을 이용하였다. 이것을 semantic segmentation과 비교해 보자. semantic segmentation은 트레이닝 데이터가 직관적으로 그려진다. 픽셀별로 라벨링이 되어있는 label 데이터를 생각해볼 수 있다. 따라서, 입력 이미지가 주어지면 그것을 픽셀별로 분류하면 되는데, image detection 문제는 물체를 직접 찾아서 네모를 쳐주어야 한다. 그래서 이것은 end to end로 학습하기가 더 어렵다.​그래서 이 연구에서는 이미지 안에서 bounding box라고 하는 네모를 많이 뽑아낸다. 기존에 있는 딥러닝과 상관없는 방법론을 통해 네모를 뽑는다. 네모를 내가 원하는 사이즈로 resize할 수 있다. 그렇게 하면 cnn에 집어넣을 수 있다. 여기서는 svm을 통해 분류하였다. 예를 들어 2000개를 뽑았으면 2000개를 다 cnn에 넣는 것이다. 여기서 cnn은 미리 학습되어있는 것을 쓴다.  카테고리와 무관하게 bounding box 후보들을 생성해낸다. 여기서는 selective search 라는 방법을 쓴다. ​테스트할때는 2000개의 rp를 뽑고 resize를 하고 cnn에 집어넣은 다음 feature 뽑고 svm한다. 여기서 오래 걸리는 이유는 이미지는 한장이지만 2000개의 rp가 cnn에 들어가야 하기 때문에 cnn이 2000번 돌아야한다. 여기서 bottleneck이 생긴다. 그리고 training이 어렵다. rp를 통해 2000개를 뽑는다. 뽑힌 네모가 실제 네모와 얼마나 겹치는지 확인한다. IoU가 0.5보다 크면 positive data로 보고 0.3보다 작으면 negative data로 본다. 네모를 뽑을 때 학습데이터 안에 들어가 있는 네모를 전혀 고려할 수 없고 다만 얼마나 겹치는 지만 알 수 있다. 이렇게 뽑히는 네모가 어떻게 움직여 줘야지 실제 bounding box와 비슷해지는지 찾는 것을 bounding box regression이라고 한다. 이 연구에서는 4가지 파라미터를 추가적으로 학습했는데 중심점 (x,y)과 종횡비 (width, height)이다. 이 논문의 실험 결과 자체는 그렇게 좋지는 않았다.  참고자료​https://www.edwith.org/deeplearningchoi/lecture/15568?isDesc=false [LECTURE] Image Detection 방법론: RCNN, SPPnet, FastRCNN, FasterRCNN : edwith학습목표 바로 앞 강의에서 Localization과 함께 언급됐던 Image Detection 방법론들을 알아보도록 하겠습니다. Detection 방법론은 두 파트로 나누어 이번 ... - 커넥트재단www.edwith.org https://arxiv.org/pdf/1311.2524.pdf "
Imbalance Problems in Object Detection : 객체 검출 분야의 불균형 문제 A Review(3) ,https://blog.naver.com/phj8498/222358876295,20210520,"Imbalance Problems in Object Detection : 객체 검출 분야의 불균형 문제 A Review(3) (tistory.com)​해당 논문은 part1~part3까지 포스팅될 예정입니다.2020/11/02 - [지식 저장소] - Imbalance Problems in Object Detection : 객체 검출 분야의 불균형 문제 A Review(1)2020/11/03 - [지식 저장소] - Imbalance Problems in Object Detection : 객체 검출 분야의 불균형 문제 A Review(2)​Imbalance 4 : Objective Imbalance Fig. 16: (a) Randomly sampled 32 positive RoIs using the pRoI Generator [66]. (b) Average classification and regression losses of these RoIs at the initialization of the object detector for MS COCO dataset [90] with 80 classes. We use cross entropy for the classification task assuming that initially each class has the same confidence score, and smooth L1 loss for regression task. Note that right after initialization, the classification loss has more effect on the total loss.Definition Objective Imbalance는 훈련 과정 loss function이 최소화되는 것과 관련이 있다. (i)Task에 따라 기울기의 Norm 규칙들이 하나의 작업에 지배 될 수 있다. (ii)Task에 따라 Loss function 의 범위가 다를 수 있고, 이 것은 optimization을 방해한다. (iii)Task의 어려운 정도가 다를 수 있고, 학습되는 속도에 영향을 미치므로 training process를 방해한다.(여기서 Task란 classification과 regression작업과 같은 훈련과정의 작업에 대한것을 말한다.)Figure 16을 보면 Classification loss가 전체 기울기를 지배하는 경우를 확인할 수 있다.Solutions 가장 일반적인 해결법은 Task Weighting이다. 가중치 factor로 작용할 수 있는 hyperparameter를 추가하는 방법이다. hyperparameter는 validation set에 의해 선택된다. 말하자면, 2-stage 모델의 경우 Task의 수를 늘리면 가중치 factor와 search space가 증가한다. Multi-task 특성 때문에 서로 다른 loss function 간의 범위 불일치가 발생한다. AP loss는 smooth L1이 같이 사용되고, GIoU는 cross-entropy와 같이 사용되는 것 처럼 loss function이 같이 사용되기 때문이다.서로 다른 범위의 균형을 맞추는 것은 어려운 일이기 때문에, 범위를 비교할 수 있도록 만드는 것이 더 나은 전략이다.classification과 regression을 결합하는 것중 두드러지는 접근 방식중 하나는 **Classification-Aware Regression Loss(CARL)**이다. 이 는 classification과 regression이 상관 관계가 있다고 가정한다. loss term을 결합하기 위해, regression loss는 classification의 confidence score에 의해 결정된 계수로 scale이 조정된다. 위 식에서 c'_i는 p_i를 기반으로 하는 classification task의 추정치이다. 이러한 방법은 regression branch에도 classification의 기울기 신호를 제공해서 고품질 example에도 localization을 향상시킨다.CARL방법은 regression과 classification의 상관관계를 사용할 수 있다는 것에 기여했다.최근에는 cross-entropy가 사용될 때, 각각의 epoch 마다 개별의 loss component 기여율이 다를 수있다는 것을 통해 cross-entropy에 발생하는 loss를 동적으로 weighted 해야한다는 것을 보여준다.Objective Imbalance를 방지하기위해 loss의 총 크기를 고려하여 classification component에 가중치를 부여하는 Guided Loss가 제안 되었다. 이 방법은 regression loss가 foreground 예제를 통해서만 구성되고 foreground의 클래스 수에 의해서만 정규화 되기 때문에, classification loss를 통해 정규화 할 수 있도록 한 방법이다.​7.1 Comparative Summary현재, linear task weighting을 제외하고, objective imbalance를 완화할 수 있는 적합한 방법이 없으며, 이에 따라 task 가중치를 설정하지 않으면 훈련이 분산되어 버린다. 많은 논문에서 regression과 classification에 동일한 가중치를 사용하지만, 적절한 선형 가중치가 성능을 약간 향상시킬 수 있다는 것을 보여준다.​7.2 Open Issues Fig. 18: An example suggesting the necessity of considering different imbalance problems together in a unified manner. Blue, green and red colors indicate ground-truth, positive example (prediction) and negative examples, respectively. The larger example (i.e. the one with higher IoU with the blue box) in (a) is shifted right. In (b), its IoU is significantly decreased and it eventually becomes a negative example in (c). This example shows the interplay between class imbalance, scale imbalance, spatial imbalance and objective imbalance by a change in BB positions.현재 일반적인 접근 방식은 전체 loss function을 얻기위해 작은 task들의 loss function을 선형적으로 결합하는 방식이지만, box가 이미지 위에서 이동할때, regression및 classification loss 모두 영향을 받으며 그에 따른 종속성을 알 수 있다. 이는 (i)loss value와 그것의 기울기 (ii)작업 속도를 선형 가중치 결합 방식이 objective imbalance를 해결하고 있지 못함을 알 수있다.​8 Imbalance Problem In Other Domains해당 section은 object detection 분야에 적용될 수 있는 관련 domain들의 불균형 문제를 다룬다. 본 글에서는 이 부분 중 많은 관심이 있는 image classification에 대해서만 정리하며, 나머지는 논문을 통해 확인하기를 바란다.​8.1 Image Classification이미지 분류에서는 Class imbalance에 대해 광범위한 연구가 진행되고 있다.일반적으로 사용되는 Resampling the dataset은 dataset의 균형을 맞추기 위해 oversampling과 undersampling을 포함하여 resampling하는 방법을 사용한다. oversampling은 under-represented classes에서 더 많은 샘플을 추가하고, undersampling은 over-represented classes의 일부 데이터를 무시하여서 클래스간의 균형을 맞춘다.만약, under-represented classes를 단순히 복제하여 불균형을 해소하는 oversampling을 사용한다면 overfitting이 일어날 가능성이 높다. 따라서, 데이터를 일부 무시하는 방법이지만, non-deep-learning 방식에서는 undersampling이 선호되었다.그러나, deep-learning에서는 oversampling에서 overfitting의 어려움을 겪지 않으며, undersampling에 비해 나은 성능을 보여준다고 한다. 단순히 복제하는 것이 아닌 interpolation of neighboring samples를 통해 새로운 sample을 생성하는 방법과 동일한 example과 class가 동일한 순서로 나타나게끔 제한하여, 모든 class에서 최대한 균일한 mini-batch를 sampling하는 방법이 연구되었다.다른 방법으로는 transfer learning이 제안되었다. train set크기가 증가할 때, 모델이 어떻게 진화되는지 확인하기 위한 연구를 하였는데, meta-learner를 생성하여 점진적으로 example의 수를 늘려주었다. 그 결과 meta-model은 적은 example로 훈련된 모델에서 더 많은 example로 훈련된 모델로 변환 될 수 있다는 것을 알아 내었고, 이것을 under-represented class를 개척하는데 쓸 수 있다는 것을 의미한다.Weighting the loss function 방법은 inverse class frequency를 사용하여 under-represented class에 더 많은 가중치를 부여하는 방법이다. 또한 더 'useful'한 example에 중요성을 더 준다.Data redundancy관점에서 해결하는 방법도 제안되었다. 데이터의 중복을 해결하는 것은 쓸모없는 데이터를 무시하여 더 빠른 수렴을 얻을 수 있다. 기울기의 크기를 보고 중복성을 결정하여 훈련과정에 서용하는 방법이다.dataset을 보강하는 또 다른 방법은 weak supervision 방법이다. label이 없는 example을 통합하기 위해 사용되는데 Instagram 이미지를 hashtag로 label을 하여 보강한다던지, classification 을 통해 데이터에 label을 추가하여 train sample에 추가한다.GAN을 통해 데이터를 확장하는 방법 또한 활발한 연구가 이루어지고 있다.위 방법들은 모두 Foreground-Foreground class imbalance의 대한 현재 solution이며, 이 것들은 모두 object detection 분야에서는 완전히 사용될 수 없다. 이러한 접근 방식들에 대해 연구가 필요하고, 발전해 나가야 한다.​8.2 Metric LearningMetric learning은 Sampling 방법, Generate 방법, 새로운 loss function를 통해 완화하는 방법을 소개한다. 이는 Foreground-background imbalance에 사용될 수 있다고 한다.위 방법들은 데이터자체에서 직접 학습한 Metric이 더 나은 결과를 가져올 수 있다고 한다. self-pace learning 또한 불균형을 해결하기 위한 중요한 개념이며, label지정이 online으로 진행되는 것 또한 균형을 맞추는 것에 효과적이라고 한다.​8.3 Multi-Task LearningMulti-Task Learning에는 여러 task를 동시에 학습하는 것이며, 일반적으로 task의 목표를 평가하여 균형을 맞추는 방식을 사용한다.위 방법은 object detection분야에서 상당한 이점을 가질 수 있지만, 이러한 방법은 주목받지 못하고 있다고 한다.​9 Open Issue for All Imbalance Problems해당 section에서는 모든 불균형 문제에 대해 미해결된 open issue를 소개한다. 본 글에서는 짧게 알아보고 넘어간다.​9.1 A Unified Approach to Addressing Imbalance불균형 문제간의 상호 의존성을 고려해서 모든 불균형을 해결하는 통합 접근방식을 찾아야 한다.​9.2 Measuring and Identifying Imbalance불균형을 정량화하거나 측정하는 방법, 불균형이 있을 때 식별하는 방법이 필요하다.What is a balanced distribution for a property that is critical for a task? (task에서 중요한 property 의 균형 분포가 무엇인가?)What is the desired distribution for the properties that are critical for a task? (task에서 중요한 property 의 원하는 분포가 무엇인가?)How can we quantify how imbalanced a distribution is? (분포의 불균형을 어떻게 정량화 할 것인가?)​9.3 Labeling a Bounding Box as Positive or Negative Fig. 19: An illustration on ambiguities resulting from labeling examples. The blue boxes denote the ground truth. The green boxes are the estimated positive boxes with IoU > 0.5. The input image is from the MS COCO.현재는 IoU 기반 임계 값을 사용하여 positive와 negative로 labeling하지만, 이에 대한 합의가 없으며 labeling을 통한 어려운 example인지 여부를 결정하는데 중요하므로 합의가 필요하다.Figure 19를 보면 positive로 분류된 두개의 box를 확인할 수 있는데, 그림으로 봐도 꽤나 큰 차이가 보인다.​9.4 Imbalance in Bottom-Up Object DetectorsBottom-up detector는 1-stage, 2-stage detector와 완전히 접근방식이 다르며, 이런 Bottom-up detector에 대한 불균형 문제를 분석하고 해결하기 위한 연구가 필요하다.​Conclusion이 논문에서는 object detection의 불균형 문제에 대한 철저한 검토를 하였다. 일관된 관점을 위해 분류와 해결방법을 적었고, 비판적인 관점을 유지하며 solution을 제공했다고 한다.여러가지 불균형 문제는 수 없이 존재하고, 이를 식별하고 해결해 나가는 과정이 해당 논문의 저자들의 연구방향이라고 한다. 커뮤니티를 통해 발전 시키고자하는 의지를 표현하며 논문은 마무리된다.​논문에 대한 개인 의견논문은 여러가지로 Imbalance를 분류하여 상세히 알아보았고, 그에 대한 해결과정부터 문제제기까지 굉장히 좋은 내용들이 많았다. 하지만 논문의 큰 맹점은 약간의 억지스러운 Imbalance 분류, 또 그것의 해결과정에서 등장하는 Dataset이 PASCAL VOC나 MS COCO뿐만으로 대부분 설명되기 때문에, 실제로 해당 해결방법들이 Deep learning분야에 정말 해결책인가를 알기는 쉽지는 않았다. 물론 이론적인 설명을 덧붙여서 어느정도 이해가 되긴하지만, 현재 Deep learning 분야가 0.1%~1% 정도의 성능향상만 있어도 순위나 좋은 모델이라고 평가받기 때문에 Dataset에 상당히 의존되는 경향이 있다. (정말 실세계 어디서든 잘할수 있는 모델을 만드는 것은 매우 어렵겠지만) 하지만 논문이 Object detection 분야를 다각도로 보고 있고, 최대한 객관적으로 보려고 하기 때문에 좋은 부분이 더 많은 논문임에 틀림없다. 가장 기억에 남는 멘트는 기준자체가 없이 연구되는 이 학문에 어떠한 기준을 마련해야 한다는 것이 굉장히 기억에 남는다. 어쨌든, 약간의 비판적인 사고를 가지고 보면 상당히 얻을 것이 많고, 내용이 전부 이해되지 않더라도 처음 입문하는 사람에게는 도움되는 논문임에 틀림없다. ​​출처: https://keyog.tistory.com/42?category=879582 [인간지능이 인공지능을 공부하는 장소] "
[ML] Object detection mechanism based on deep learning algorithm using  embedded IoT devices  ,https://blog.naver.com/horajjan/221496472135,20190325,"Object detection mechanism based on deep learning algorithm using embedded IoT devices for smart home appliances control in CoT   Object detection mechanism based on deep learning algorithm using embedded IoT devices for smart home appliances control in CoTWe use cookies to personalise content and ads, to provide social media features and to analyse our traffic. We also share information about your use of our site with our social media, advertising and analytics partners in accordance with our Privacy Statement . You can manage your preferences in Man...link.springer.com "
[Object Detection] SPP-Net ,https://blog.naver.com/hk428428/222642166861,20220208,"오늘은 객체 탐지 알고리즘인 R-CNN 에서 조금 더 발전한 SPP-Net 에 대해 간략하게 정리해볼 것이다. ​​SPP-Net 등장 배경 ​ [R-CNN]​기존의 R-CNN 은 이미지 한장당 2000개의 Selective Search를 통해 2000개의 이미지 영역 (Region proposal) 을 만들고 warp 을 진행한다.​​때문에,  이미지에서 원하지 않는  왜곡이 생기는 경우가 있었다. 이렇게 왜곡이 되어버림.. ​​이렇게 추출한 영역을 CNN에서 계산을 하면 매우 느리다는 단점이 있다.​Time-Consuming(region proposal 2000개 추출하는데 2초나 걸림...)​​​SPP-Net 등장 !   ​​R-CNN 에서 warp으로 인한 왜곡은 특히나 우리가 탐지하고자 하는 객체의 Scale 이 다양할 때 객체를 인식하는 정확도를 떨어뜨릴 수 있다. ​그래서 06년에 발표된 SPM 을 고려해서 SPP net 이라는 새로운 CNN 의 아키텍처가 제안이 되었다! ​(Spatial Pyramid Matching  에 대한 분할 방법이 따로 있음)​뭐 어쨌든,​SPP- Net 의 장점은입력 이미지를 먼저 CNN 작업을 진행하고 5번째 conv layer에 도달한 feature map 을 기반으로 region proposal 방식을 적용해서 후보가 되는 바운딩 박스를 선별하게 된다.​계산량이 줄어들게 되는데,​RCNN 2000 번 => SPP-Net 1번 ​즉, CNN Operation 절감 효과가 나타나기 때문에​굉장히 시간을 빠르게 단축할 수 있다! ​ ​​ ​이것만 봐도 확실히 R-CNN 이 느린 것을 알 수 있다.!​​​그리고, SPP-Net은 warping 으로 인한 왜곡을 없애주기 위해서  spatial pyramid pooling 을 시행한다.​   SPP-net 에서 바로 저부분이 피라미드 풀링에 해당한다.​ ​1. 먼저, Conv layer 5 까지 거친 feature map 에 대해서  selective search를 통해 region proposal 방식을 적용한다.​그런다음, 후보가 되는 바운딩 박스를 선별해 준다.candidate bounding box  = RoI (region of interests)​​2. RoI 영역에서 Pyramid Pooling 알고리즘을 적용한다.​(알고리즘 과정은 아래 블로그 참조해주세요! )​​​결과 ​ => R-CNN 보다 SPP-Net bb(bounding box) 가 mAP 가 가장 높은 것을 확인이 가능하다. ​​요약.1. R-CNN 에서 CNN 연산을 2000번 < SPP-Net 에서 CNN 연산 1번​=> Train, Test 시간이 매우 단축 된다!​2. Spatial Pyramid Pooling 을 통해 R-CNN 에서 사용된 warp 작업을 안함. => 이미지의 왜곡 현상을 피할 수 있음!  ​​​Reference  https://89douner.tistory.com/89 6. SPP Net안녕하세요~ 이번글에서는 RCNN의 단점을 극복하고자 나온 SPP-Net object detection 모델에 대해서 알아보도록 할게요~ 1) Too CNN operation RCNN은 selective search를 통해 대략 2000개의 candidate bounding..89douner.tistory.com https://nepersica.tistory.com/4 [Object Detection] SPP-Net, Fast R-CNN, Faster R-CNN안녕하세요. 지난번 리뷰했던 R-CNN에 이어서 후에 발표된 R-CNN 계열 논문들을 간단하게 리뷰하고자 합니다. 순서는 타이틀과 같이 SPP-Net, Fast R-CNN, Faster R-CNN 순으로 진행하겠습니다. 1. SPP-Net - R-CN..nepersica.tistory.com https://ieeexplore.ieee.org/document/8901325 A Review of Object Detection TechniquesObject detection is widely used in the field of computer vision and crucial for variety of applications, e.g., self-driving car. During the development of half a century, object detection methods have been continuously developed, and generated numerous approaches which obtained promising achievement...ieeexplore.ieee.org ​ "
[Object detection] 3. Fast R-CNN ,https://blog.naver.com/ollehw/221824476611,20200225,"Fast R-CNN은 R-CNN의 단점인 시간이 오래걸린다는 것을 보완한 방법입니다.​R-CNN에서는 2000개의 RoI에 대해서 CNN모델을 각각 훈련시켰습니다.이는 매우 비효율적인 과정이죠.​   R-CNN 과 Fast R-CNN의 차이점은 크게 2가지로 볼 수 있습니다.​첫 번째로, RoI Pooling layer 입니다.R-CNN에서는 각 RoI를 CNN의 Input으로 들어가기 이전에, 별도의 전처리과정을 거치기 때문에, CNN 모델에 포함되지 않습니다.​RoI Pooling Layer를 사용하면, CNN에 별도의 전처리과정 없이 RoI 이미지가 그대로 입력되고, CNN Model 내에서 사이즈가 학습과정에서 바뀌게 됩니다.​   Roi Pooling layer를 요악한 그림입니다.원본 이미지에서 RoI에 해당하는 부분을 특정 개수의 분할 (여기서는 4개)로 나눕니다.그리고, 이 나눈 분할에 대해서 Max Pooling을 시키는 것이지요.​두 번째로, Loss function을 R-CNN과는 다르게 SVM Model과 Bounding Box Regressor를 합쳤습니다.   Loss Function을 합침으로써, 보다 계산적인 측면에서 효율성을 달성할 수 있었습니다.​하지만 Fast R-CNN에는 아직 시간적인 문제가 남아있습니다.바로, RoI를 선정하는 데 있어서, Selective Search를 하는데, 이 과정이 별도의 과징이기 때문에 시간적 비효율이 남아있습니다.​이를 해결한 것이 다음에 배울 Faster R-CNN입니다.​ "
[KUSMO] 5/22 Object Tracking Using RealSense in ROS Melodic ,https://blog.naver.com/yoouungg/223109195274,20230523,"목표 : 영상 인식을 통한 접속부 인식​ Intel RealSense D457 사용​Intel RealSense Viwer 설치https://changun516.tistory.com/199 Intel RealSense Viwer 및 SDK Ubuntu 20.04 설치Intel RealSense Viwer 및 SDK Ubuntu 20.04 설치 1. Intel에서 제공하는 설치 가이드는 아래의 사이트와 같다. https://github.com/IntelRealSense/librealsense/blob/master/doc/distribution_linux.md GitHub - IntelRealSense/librealsense: Intel® RealSense™ SDK Intel® RealSense™ SDK. Contribute to IntelRealSense/librealsense developme...changun516.tistory.com ​​Intel RealSense SDK 설치https://doongdoongeee.tistory.com/99 Ubuntu에서 ROS를 통해 Intel Realsense D435i 사용하기가장 먼저 Intel realsense sdk를 설치한다. www.intelrealsense.com/sdk-2/ Intel RealSense SDK 2.0 – Intel RealSense Depth and Tracking cameras Free Cross-platform SDK for depth cameras (lidar, stereo, coded light). Windows, Linux and other. 10+ wrappers including ROS, Python, C/C++, C#, unity. Try! www.intelre...doongdoongeee.tistory.com ​ ﻿$ export ROS_VER=melodic $ sudo apt install ros-melodic-realsense2-camera﻿ Intel realsense sdk-2 설치​ ﻿$ mkdir -p ~/d457_ws/src $ cd ~/d457_ws/src $ git clone https://github.com/IntelRealSense/realsense-ros.git﻿ RealSense (이 글의 경우 D457)사용을 위한 workspace 생성 후 Intel RealSense package 다운로드​ ﻿$ cd d457_ws $ catkin_init_workspace $ cd .. $ catkin_make clean $ catkin_make -DCATKIN_ENABLE_TESTING=False -DCMAKE_BUILD_TYPE=Release $ catkin_make install﻿ catkin_make 설치​ ﻿$ echo ""source ~/d457_ws/devel/setup.bash"">> ~/.bashrc $ source ~/.bashrc﻿ setup.bash 파일 source​ ﻿$ roslaunch realsense2_camera rs_camera.launch Camera node 실행아래와 같이 카메라를 찾지 못함 (USB C-C로 연결) ​​센서 상단부의 구멍에 핀을 꽂아 넣어 뚜껑을 연 후 M -> U로 전환  GMSL의 M, USB의 U로 예상됨​ ﻿$ cd d457_ws $ roslaunch realsense2_camera rs_camera.launch﻿ Camera node 실행아래와 같이 정상 동작함을 확인 ​ ﻿$ roslaunch realsense2_camera demo_pointcloud.launch Pointcloud demo node 실행 ​Node Graph ​ ﻿$ cd d457_ws/src $ git clone https://github.com/dovanhuong/object_detection_2d $ catkin_make﻿ OpenCV Object Tracking Package을 d457 workspace에 clone​ ﻿$ sudo apt install ros-melodic-rgbd-launch rgbd launch file 설치​ ﻿$ cd d457_ws $ roslaunch realsense2_camera rs_rgbd.launch﻿ d457 workspace로 이동 후 rgbd launch file 실행​ ﻿$ rosrun opencv_object_tracking object_filter 새로운 terminal 창을 연 후 opencv object tracking package 실행  HSV parameter를 조정하여 객체 인식 성공HSV : Hue (색조. 0~180) Saturation (채도. 0~255) Value(0~255)Hue : 사람에게 우선적으로 보이는 색깔. Tint(순색에 흰색이 섞인 색). 첫번째 사진에선 순색이 Hue에 해당한다 하고 논문에선 Tint에 해당한다 함Saturation : Hue와 어울리는 흰색의 양. Shade(Tint와 반대되는 색으로, 순색에 검은 색이 섞인 색)Value : 밝기/강도. Tone(순색에 회색이 섞인 색) ​ 출처 : https://subscription.packtpub.com/book/data/9781789537147/1/ch01lvl1sec09/object-detection-using-color-in-hsv위 색상표에서 x축이 H, y축이 S, S = 255, V=255에 해당​ ﻿OpenCV Error: One of arguments' values is out of range (Bad trackbar maximal value) in icvCreateTrackbar, file /build/opencv-L2vuMj/opencv-3.2.0+dfsg/modules/highgui/src/window_gtk.cpp, line 1454 terminate called after throwing an instance of 'cv::Exception' what(): /build/opencv-L2vuMj/opencv-3.2.0+dfsg/modules/highgui/src/window_gtk.cpp:1454: error: (-211) Bad trackbar maximal value in function icvCreateTrackbar Aborted (core dumped)﻿ Object Tracking을 계속 돌릴 시 위와 같은 Error창이 뜨며 꺼짐​​- 그림자를 물체로 인식하는 문제 발생 -> 해결 방안 모색 필요End Effector에 조명을 달아 그림자 제거SP 커플러에 R,G,B 중 한 색깔을 입혀 인식 Object Tracking 외의 방법 사용 (Aruco Marker 등)​​​- 향후 계획X, Y축 리니어 엑추에이터의 24V에서 시간 당 이동거리 파악 후 현재 자신의 위치에서 접속부까지의 거리 계산 후 필요 거리만큼을 이동하게 해 접속​​​​  참고자료 :https://github.com/dovanhuong/object_detection_2d GitHub - dovanhuong/object_detection_2d: This source code to take object detection from 2D and point out the position in 3D coordinate of object.This source code to take object detection from 2D and point out the position in 3D coordinate of object. - GitHub - dovanhuong/object_detection_2d: This source code to take object detection from 2D...github.com https://www.youtube.com/watch?v=2N6FJhoSYeo https://www.youtube.com/watch?v=VHO4G2h8O4g https://www.youtube.com/watch?v=qMPjaLAXaaw https://www.youtube.com/watch?v=YjotU6UdgZI https://mzu.edu.in/wp-content/uploads/2020/05/Interactive-Color-Image-Segmentation-using-HSV-Color-Space.pdf​https://subscription.packtpub.com/book/data/9781789537147/1/ch01lvl1sec09/object-detection-using-color-in-hsv Object detection using color in HSV | Python Image Processing CookbookIn this recipe, you will learn how to detect objects using colors in the HSV color space using OpenCV-Python. You need to specify a range of color values by measubscription.packtpub.com https://m.blog.naver.com/PostView.naver?isHttpsRedirect=true&blogId=phominator&logNo=220773016858 [포토샵 보정기능] Hue/Saturation : 색조와 채도 조절하기  Hue/Saturation은 이미지 전체 혹은 특정 부위의 컬러를 간단히 변화시키고자 할 때 사용하는 기...m.blog.naver.com https://illustmei.tistory.com/441 컬러 및 색상에 대한 디자인 용어 - HUE, TINT, SHADE, NEUTRAL색상을 보편적으로 사람들이 어떻게 느끼는지, 디자인에 어떤 영향을 일으키는가에 대한 많은 색상 연구가 세상에는 많습니다. 색상 이론은 기본적으로 이미 우리 생활 깊숙이 자리를 잡고 있습니다. 예를 들어 식당에는 난색 계통이 좋고 여름용 마케팅 이미지에는 한색계열의 컬러를 베리에이션 합니다. 아이들을 위한 컬러는 비비드한 원색 계통을 선택하는 브랜드가 많고 럭셔리 브랜드에서는 색을 자제하고 오가닉한 컬러를 선택하는 경우도 많습니다. 이미 컬러의 궁합은 보편적으로 기준이 많이 잡혀있는 상태이기 때문에 명도와 채도를 조절하면서 여러 색상...illustmei.tistory.com ​ "
[Ubuntu] Single Shot Detection(SSD) for ROS ,https://blog.naver.com/tinz6461/222059492958,20200813,"작년부터 object detection에 대해 계속 공부를 해 본 결과 나에게 필요한알고리즘은 SSD로 결론을 지었다​작년 말에 Faster-RCNN을Stereo로 구현한 알고리즘도 써보고올해 초에 YOLO V3와 V4를 써봤는데Faster-RCNN은 정확도는 높은 편이라100m 원거리 물체도 정확히 구분하지만속도가 매우 느린 단점이 있었고​YOLO 속도는 거의 실시간 급인데50m 이내 물체의 정확도가 높은 편이고 또한, 개발자가 제공하는 다크넷을 통해원하는 디텍션 구현이 쉽게 가능하다 보니 학술적으로 크게 기여도를 찾기 어려워서내가 하려는 연구에 적합하지 않았다 그래서 Faster-RCNN의 정확도와YOLO의 속도를 모두 갖추면서딥러닝 알고리즘에 대한 수정이 가능하여학술적으로도 충분히 보완 및 활용 가능한SSD를 타겟 알고리즘으로 선택했다(물론 YOLO V4의 정확도도 높아졌다)​Object detection에 대한 코드는구글 깃허브를 검색하다 보면 수없이 많은데내 경우엔 언어는 텐서플로우로 되어 있고ROS에서 활용가능한 코드를 찾아본 결과아래 코드를 찾아 볼 수 있었다​https://github.com/osrf/tensorflow_object_detector osrf/tensorflow_object_detectorTensorflow Object Detector. Contribute to osrf/tensorflow_object_detector development by creating an account on GitHub.github.com 빌드업은 워크스페이트 내 src 폴더에다운받은 뒤 catkin-make 해주면 된다(src 내 vision_msgs도 함께 빌드업)​내가 설치한 텐서 플로우 버전과호환이 되지 않아서 에러가 났는데에러 메시지에 나온대로 detect_ros.py 내용을변경해주면 동일한 에러 메세지가 사라진다 ﻿tf.GraphDef → tf.compat.v1.GraphDeftf.Session → tf.compat.v1.Sessiontf.gfile.GFile → tf.io.gfile.GFiletf.ConfigProto → tf.compat.v1.ConfigProto﻿ 아래와 같이 label_map_util.py 파일의코드를 수정하라고 하면 동일하게 변경해준다  ﻿ tf.gfile.GFile → tf.io.gfile.GFile 또는 아래와 같은 에러가 뜨는 경우텐서플로우를 설치해주면 된다(이런 초보적인 실수 땜에 시간을 낭비하다니ㅋ) ​이후에도 아래 에러가 나오면$ gedit ~/.bashrc 를 실행한 다음아래 내용을 추가한 다음 저장해주면 된다  ﻿export ROS_PACKAGE_PATH=/home/parkjiil/catkin_sensor_ws/:$ROS_PACKAGE_PATH ​현재는 ssd_mobilenet_v1_coco인데만약 사용하려는 디테션 모델을 변경하려면 아래 'TensorFlow 1 Model Zoo'에 들어가원하는 모델을 다운받아 압축을 풀은 뒤에data/models 폴더에 파일을 넣고​detect_ros.py 파일을 열어 라인 33에 있는 모델 이름을 다운받은 모델명으로 변경하고 추가로 라벨링 리스트가 기록되어 있는 pbtxt 파일도 변경해준다(참고로 btxt 파일은  pb 파일을읽을 수 있는 텍스트 파일이다) labels 폴더에 있는 3개 중 선택 혹은 추가 가능하다이 밖에도 detect_ros.py수정을 통해 학습된 weight 값인 checkpoint 경로와 class의 갯수도원하는 값으로 변경 가능하다​https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf1_detection_zoo.md tensorflow/modelsModels and examples built with TensorFlow. Contribute to tensorflow/models development by creating an account on GitHub.github.com 만약 모델을 바꾸는 경우 위 파일 내에 모델명을 함께 변경해야 된다마지막으로 $ roscore 실행 후에 SSD 실행$ roslaunch tensorflow_object_detector usb_cam_detector.launch ​  위 영상은 SSD mobilenet에서SSD inception으로 변경한 결과인데생각보다 나쁘지 않고 괜찮다​  ※ Jabra camera SSD 적용​SSD 디텍션 영상을 기존에 포스팅한 자브라 카메라(3840x1080 해상도)의 경우화면에 다 보이지 않고 짤리는 문제가 생기는데​이를 해결하기 위해서는 아래와 같이$ roslaunch tensorflow_object_detector usb_cam_detector.launch 살행 후에 다시 $ rviz를 입력하여rviz 창이 뜨면 아래 [add]를 클릭하여[by topic]을 선택한 뒤 [/image_view] 내[/ouptput] - [Image]를 선택한 다음​해당 창을 위로 옮기면 설정한 해상도에맞게 SSD 디텍션 결과를 확인할 수 있다! 지금은 오브젝트가 없지만 실제로 앞에 서면 잘 검출된다ㅋ  ※ 2020.12.3 업데이트 내용​노트북을 밀어야 되는 일이 발생해위에서 포시팅하는 방법으로 재설치하는데동일한 방법으로 했음에도 불구하고 util이 에러가 나오며 실행되지 않는다​detect_ros.py"", line 27, in <module> from object_detection.utils import label_map_utilImportError: No module named utils​도대체 같은 방법으로 했는데 무엇이 문제인지 --;며칠동안 구글링을 해보며 아래와 같이이거저것 해보았는데 전부 실패 python path 문제란 얘기도 있어서경로를 추가했지만 이것도 실패 ImportError: No module named utils이란뜻이 util이란 폴더가 없는걸 의미하기 때문에폴더를 확인해 보니 아니나 다를까 예상대로object_detection 폴더에 utils 폴더가 없다​그래서 깃허브 주소에 들어가 원래 소스코드를 다시 다운로드 받은 뒤 아래와 같이 utils 폴더를 넣어주었다  그럼에 불구하고 mscoco_label_map.pbtxt파일이 없다는 에러가 나서 찾아보니 data 폴더 내 labels 폴더가 없다 --;; 위와 같은 방법으로 붙여넣기 해주고 런치 파일을 실행해 주니 다시 잘 실행된다​왜 소소코드를 복사할 때 위 2개의 폴더가누락되는지 알수는 없지만(아마 권한 때문인 듯)그래서 해결방법을 찾았으니 다행이다(속초 출장와서 사용해야 하는 코드인데데이터 취득 전에 해결해서 정말 다행임ㅋ) "
[컴퓨터비전] Face Detection (Haar Cascade 이론 by Viola and Jones) ,https://blog.naver.com/justarose/222873906363,20220913,"​컴퓨터 비전은 현재 공부하고 있는 분야는 아닌데, Face Detection 혁신의 시초가 되는 논문을 읽게되어 재미있게 본 김에 정리해본다. ​논문: ""Rapid Object Detection using a Boosted Cascade of Simple Features"" (저자: Paul Viola, Machael Jones) ​이 논문은 2001년에 나왔지만, 당시 얼굴인식(Face detection)의 혁신을 불러일으킨 모델이고 (방법론이나 퍼포먼스측면에서) 현재까지도 opencv 패키지 라이브러리를 통해 많은 사람들이 이용하고 있는 모델이다. ​Viola and Jones 의 Haar Cascade 모델로 불리운다.  Haar feature​​Haar feature는 위와 같이 생긴 여러모양의 사각 틀이라고 생각하면 된다. 각 틀을 사진의 작은 부분 부분을 왼->오 로 훑으면서 또 위->아래로 훑으면서 지나가면서 어두운 부분의 픽셀 밝기 값과 흰 부분의 픽셀 밝기 값의 차이를 계산해 나간다. ​​ ​​이 틀이 하고자 하는 일은 밝기 차이를 통해 얼굴의 edge를 감지(detection)하는 것이다. 예를들어 위와 같이 아래위로 나눠진 사각틀은 눈썹이나 눈동자가 있는 곳을 감지할 수 있을 것이고, 세로로 세개로 나눠진 사각틀은 입술을 감지할 수 있을 것이다. ​​  ​​위와 같이 사진 곳곳을 이동하면서 흰구역의 픽셀 값의 평균과 검정 구역의 픽셀 값을 평균값의 차이를 계산해 나간다. ​하지만 이렇게 틀을 이동시키면서 그 안에 들어있는 픽셀값들을 다 더해서 평균 하는건 엄청난 계산을 필요로 한다. ​​ Integral Image​​계산을 조금이라도 줄이고자 도입한게 Integral Image 개념이다. 위의 사진에서 보는 것과 같이 Integral Image는 Original Image에서 왼 - >오 그리고 다시 아래로 내려가면서  ""자기자신의 값+자기위에있는 픽셀들의 값+ 자기왼쪽에있는 픽셀들의값""을누적해나가면서 계산하여 픽셀값을 재계산한 이미지다. ​​ ​​​그렇게 되면 해당구역안에 픽셀 값들의 평균을 구하려면 위와 같이 4개 픽셀의 값만 알면 바로 계산을 할 수 있게 되어 계산시간이 줄어들게 된다. ​​​ Adaboost ​​하지만 이 일은 여전히 엄청난 계산(computation)을 필요로 한다.  각 사각틀의 사이즈도 작게도 해보고 크게도 해보고 하기 때문에 여러개고 픽셀갯수도 여러개기 때문에 feature 값이 180,000개가 나왔다고 한다. ​​그래서 도입한게 머신러닝 모델의 AdaBoost다. AdaBoost를 통해 180,000개의 feature들 중에서 facial feature와 관련이 없어보이는 많은 feature는 버리고 6,000개의 feature만을 선택했다. (feature selection technique) ​AdaBoost의 원리를 다루면 너무 길어져서 이부분은 나중에 따로 정리해봐야겠다. ​​​ Attentional Cascade​ ​​Attentional Cascade도 역시 Computation 시간을 줄이고자 적용된 구조이다. feature 갯수가 6,000개로 줄어들긴 했지만 여전히 6,000개의 feature를 전체 사진에 대해 돌면서 계산하는 것은 많은 시간을 요구한다. ​​따라서 24 X 24 사이즈의 윈도우를 하나의 구역이라고 정하고, 단계(stage)별로 feature들을 나눠서 각단계에서  facial feature가 감지됐는지 보고 감지되지 않았으면 과감하게 그 구역은 버리고 다음 윈도우로 이동하는 방식이다. ​만약 facial feature를 발견했으면 다음 stage로 넘어가 다른 feature로도 계산을 한다. 모든 stage를 통과하여 facial feature를 발견하면 그 구역은 face detection(얼굴감지, 얼굴인식)된 구역이 된다. ​​벌써 20년도 더 된 모델이지만, 당시 이런게 없었던 시절에 어떻게 이런 생각을 해낼 수 있었을까? 과학자들은 정말 대단하다.. 리스펙 😮​​​#공부로그 #컴퓨터비전 #Haarcascade #ViolaJones #얼굴인식 #facedetection #opencv #머신러닝​​​ "
Imbalance Problems in Object Detection : 객체 검출 분야의 불균형 문제 A Review(2) ,https://blog.naver.com/phj8498/222348167467,20210512,"Imbalance Problems in Object Detection : 객체 검출 분야의 불균형 문제 A Review(2) (tistory.com)​해당 논문은 part1~part3까지 포스팅될 예정입니다.2020/11/02 - [지식 저장소] - Imbalance Problems in Object Detection : 객체 검출 분야의 불균형 문제 A Review(1)2020/11/03 - [지식 저장소] - Imbalance Problems in Object Detection : 객체 검출 분야의 불균형 문제 A Review(3)5 Imbalance 2 : Scale Imbalance이번 장에서는 두가지의 불균형을 다룬다. Object /Box-Level Scale 불균형 문제와 Feature 불균형의 문제를 다룬다. Fig. 7 : Imbalance in scales of the objects in common datasets : the distributions of BB width(a), height(b), and area(c). Values are with respect to the normalized image(i.e. relative to the image). For readabilith, the y axes are in log-scaleFig. 8 : An illustration and comparison of the solutions for scale imbalance. ""Predict"" refers to the prediction performed by a detection network. The layered boxes correspond to convolutional layers. (a) No scale balancing method is employed. (b) Prediction is performed from backbone features at different levels (i.e. scales) (e.g. Feature Pyramid Networks). (d) The input image is scaled first and then processed. Each I corresponds to an image pyramidal feature (e.g. Image Pyramids) (e) Image and feature pyramids are combined. Rather than applying backbones, light networks are used in order to extract features from smaller images.5.1 Object/Box-Level Scale ImbalanceDefinition Scale 불균형은 객체의 사이즈 또는 input BBox에 의해 나타난다. Figure 7은 COCO 데이터셋의 scale불균형을 나타내는 그래프이다. 감지를 위해 특수 설계된 Backbone일 지라도, 이 것만으로 BBox의 scale의 다양성을 처리하기에는 충분하지 않다.Solutions Figure 8 을 기준으로 (a)는 backbone의 predict를 그대로 사용하므로 BBox scale을 무시한다. 이를 해결하기 위해 (b)~(e) 까지 제안되어 왔다. (b)부터 backbone feature의 계층별 predict를 사용, feature pyramid 방법(FPN), image pyramid 방법, 마지막으로, image pyramid와 feature pyramid의 융합이다.5.1.1 Methods Predicting from the Feature Hierarchy of Backbone Features이 방법(b)은 backbone network의 결과에 의존적이다. 다양한 level의 정보를 사용하면서, 작은 객체의 정보를 앞단에서 가져오며, 작은 객체에 대해 강해진다. 이는 1-stage 모델중 SSD에 사용되며, 각각의 다른 feature map을 통해 결과를 얻는다. 2-stage 모델에서는 영역의 scale을 얻은 후 해당 영역마다의 분류 결과를 얻는다. Multi scale CNN(MSCNN) 에서 사용되었다.5.1.2 Methods Based on Feature Pyramids Fig. 9 : Feature-Level imbalance is illustrated on the FPN architecture방법(b)는 feature 계층의 low level과 high level의 feature를 통합하지 않고 서로 다른 level을 독립적으로 사용한다. 그러나 정보의 abstractness(추상)은 layer별로 전부 다르므로, 단일 레이어에서 직접 예측한 것은 신뢰할 수 없다.이를 해결하고자 **Feature Pyramid Network (FPN)**이 제안 되었고, 서로다른 scale을 결합시켜 prediction을 추출한다. FPN은 균형있게 결합하기 위해 (b)방법인 low level feature 에서 high level feature로 top-down 형태로 진행한 뒤 feature들을 결합한다. (결합을 위해 1x1 conv를 사용)이러한 방법은 중간,소형 크기의 객체를 검출하는데에 성능을 향상 시켰다. 이 방법은 인기를 얻고, shadow detection, instance segmentation, panoptic segmentation 분야에서도 사용되었다.그러나, FPN은 Backbone Network에서 얻은 feature들이 직접적으로 연결되어 feature 불균형 문제가 있다.5.1.3 Methods Based on Image Pyramids방법(d)는 image자체를 multi-scale로 주어 image pyramid를 들어 사용한다. 이 방법은 딥러닝이전의 영상처리 방법에서도 사용되던 방법이다. 이 방법은 딥러닝에서는 많아지는 계산량과 메모리 비용으로 인해 잘 사용되진 않았다.그러나 최근에, 이미지 피라미드를 딥러닝에서 메모리 비용을 완화시키는 방법이 제안되었다. 그 방법은 다양한 크기의 이미지를 사용하여 훈련하지만 Input Box에 대해서만 다중 scale을 사용하여 GPU 메모리 비용을 완화 했다고한다. 또 Image pyramid를 설명하며, Scale-specific 방법(b) 은 데이터의 중요한 부분이 손실되는 경우가 많고, Multi scale방법은 데이터의 다양성이 보존되어 scale 불균형이 증가한다고 적혀있다.5.1.4 Methods Combining Image and Feature PyramidsImage pyramid 방법(d)는 일반적으로 feature pyramid 방법(c)보다 메모리와 시간 비용이 더 비효율적이다. 그러나 Image pyramid방법(d)이 성능적인 면에서 더 나아보인다. 이러한 이유로 두가지의 장점들을 가져와 결합시킨 방법이 제안되었다. Efficient Featurized Image Pyramids 는 5개의 image scale을 사용하고 그 중 4개는 light-weight network로 원본 image는 backbone 모델로 input된다.light-weight 모델은 4개의 연속적인 conv layer로 설계되어있다. 이 network에서 추출된 feature들은 크기에 따라 적절한 level에서 backbone 모델의 feature와 결합되어 attention modules 이후에 최종 output을 만들기 전에 forward fusion module을 통해 high-level과 융합된다.다음은 GAN을 이용한 방법이다. 2-stage모델에서 super-resolution 기법을 이용하여 small object에 대한 feature를 고해상도로 변경하여 prediction을 추출하는 방법이다. 이 역시 객체간의 scale imbalance를 해결하는 방법으로 소개된다.마지막으로는 Scale Aware Trident Networks 를 이용한 접근법이다. 이 방법은 Feature pyramid와 image pyramid에 기반을 두는데, downsample 과정에서 생성되는 feature map을 사용하지 않고 dilate(확장)된 convolution만을 사용한다. dilate(확장) convolution의 확장 rate 1~3까지 주어 각 scale-specific feature map에 전달한다. (FPN방식 이용) 각 branch가 특정한 scale에 특화되도록 Box의 크기에 따라 적절하게 제공하여, 객체의 크기별로 receptive field를 주는 방식이다.5.2 Feature-level Imbalance Fig.10 : High-level diagrams of the methods designed for feature-level imbalance. (a) Path Aggregation Network. FPN is augmented by an additional bottom-up pathway to facilitate a shortcut of the low-level features to the final pyramidal feature maps. Red arrows represent the shortcuts. (b) Libra FPN. FPN pyramidal features are integrated and refined to learn a residual feature map. We illustrate the process originating from P2 feature map. Remaining feature maps (P3-P5) follow the same operations. (c) Scale Transferrable Detection Network. Pyramidal features are learned via pooling, identity mapping and scale transfer layers depending on the layer size. (d) Parallel FPN. Feature maps with difference scales are followed by spatial pyramid pooling. These feature maps are fed into the multi-scale context aggregation (MSCA) module. We show the input and outputs of MSCA module for P3 by red arrows. (e) Deep Feature Pyramid Reconfiguration. A set of residual features are learned via global attention and local reconfiguration modules. We illustrate the process originating from P2 feature map. Remaining feature maps (P3-P5) follow the same operations. (f) Zoom Out-And-In Network. A zoomin phase based on deconvolution (shown with red arrows) is adopted before stacking the layers of zoom-out and zoom-in phases. The weighting between them is determined by map attention decision module. (g) Multi-Level FPN. Backbone features from two different levels are fed into Thinned U-Shape Module (TUM) recursively to generate a sequence of pyramidal features, which are finally combined into one by acale-wise feature aggregation module. (h) NAS-FPN. The layers between backbone features and pyramidal features are learned via Neural Architecture Search.Definition Backbone의 완성은 low-level과 high-level의 feature들이 균형있게되어 일관적인 prediction을 추출하는 것이다.Backbone에는 low-level 계층부터 high-level계층까지 여러 layer가 있고, 그 layer들의 feature들이 끼치는 영향은 전부 다르다는 것이다.Solutions FPN의 불균형을 해결하기 위해 top-down방식을 개선한 방법, 완전히 새로운 구조까지 여러 방법들이 제안된다. 해당 논문에서는 feature-level의 불균형을 완화시키기 위해 새로운 구조를 사용하는 것과 기존의 방법(피라미드나 백본feature이용)으로 두가지로 나누어 말한다.5.2.1 Methods Using Pyramidal Features as a BasisFPN을 그대로 이용하는 방법(a,b)중 PANet은 FPN에 두가지에 기여하였는데 그 내용은 다음과 같다.Bottom-up path augmentation 를 이용하여 짧은 단계를 가진 low-level의 featurer가 high-level에 도달할 수 있도록 pyramid기능을 확장한다. 이러한 방법은 초기 layer에 shortcut way가 생성되기 때문에 edge, instance parts 등 localization을 위한 정보가 증가한다고 한다.FPN과 PANet구조의 연결들이 모든 level, ROI pooling 을 혼합하여 만들어진 고정된 크기의 feature grid를 detector network에 전달할 수 있게 되었다. 이것을 Adaptive Feature Pooling 이라 한다.그러나 PANet은 여전히 순차적인 경로를 통해 feature를 추출하므로, Libra FPN은 FPN계층의 모든 feature를 한번에 사용하여 Residual feature layer를 생성해 그것을 통해 학습하는 것을 목표로 한다. Residual feature layer는 두 단계로 처리한다.Integrate : 서로 다른 layer들의 모든 feature map의 크기를 rescaling 및 평균화를 하여 하나의 단일 맵으로 축소한다.Refine : 통한된 단일 맵은 conv layer 또는 non-local neural network를 통해 정제된다.5.2.2 Methods Using Backbone Features as a Basis이 방법(c~h)은 backbone feature에서 구조를 구축하고 다양한 'feature 통합 메커니즘'을 사용한다.Scale-Transferrable Detection Network(STDN) 는 DenseNet 블록을 이용하여 backbone의 마지막 feature를 이용하여 pyramid를 생성한다. 하지만 해당 방법은 DenseNet 블록을 이용하여 통합하기 때문에 다른 backbone network를 사용하기 어렵다. 또, DenseNet 의 마지막 블록에서 low-level과 high-level feature의 균형을 조정하는 방법이 없으므로 좋은 방안이라고 보기 힘들다.비슷한 방법으로 Parallel FPN 이 있다. 이 방법은 backbone의 마지막 layer만 사용하고, spatial pyramid pooling(SPP)을 활용하여 multiscale feature를 생성한다.backbone의 마지막 layer만 사용하는 위 두 방법과는 달리 Deep Feature Pyramid Reconfiguration 은 여러 level의 layer를 결합하여 하나의 tensor처럼 사용한다. 그 다음 이 tensor를 통해 residual features를 얻는다. 이 residual features을 backbone이 학습하기 위해 두가지의 모듈이 사용된다.Global Attention Module : 서로 다른 feature map의 상호의존성을 학습하는 것을 목표로 한다.Local Configuration Module : global attention 이후의 feature를 backbone feature에 residual features가 더해져 output이 출력될 수 있도록 한다.Zoom Out-and-In Network 또한 low-level과 high-level feature를 결합하는 방법이다. 이 방법에서는 deconvolution을 통해 feature map을 확대하는데, 확대된 feature map과, backbone에서 convolution을 통해 축소된 feature map을 각각 layer의 크기에 맞게 stacking한다. 이러한 방식 때문에 이름이 Zoom out Zoom in 인듯 하다. 하지만 이 방법 또한 Inception v2 기반으로 구축되어 다른 backbone에 적용하기 어렵다는 단점이 있다.Multi-Level FPN 은 가장 low-level의 feature와 가장 high-level feature를 stacking 하여 재귀를 통해 pyramid 형태로 출력을 하며, 이 다양한 pyramidal feature들은 scale-wise 방식으로 하나의 feature pyramid를 생성한다. 이 방법은 thinned U-shape modules (TUM)이라는 방법이 적용되었는데, 이를 적용할때 마다, FPN의 불균형과 유사한 문제가 발생한다고 한다.마지막 Neural Architecture Search FPN(NAS-FPN) 은 searching(검색)을 이용하여 backbone이 주는 feature중에 최고의 pyramidal feature의 구조를 찾는 것을 목표로 한다. 하지만 아직은 다른 definitions of search spaces가 NAS보다 좋은 성능을 보여주고 있어서, NAS로 FPN을 설계하려면 연구가 더 필요하다고 한다.5.3 Comparative SummaryFeature pyramid, Image pyramid 또 그로 인해 발생하는 문제에 대한 해결 등 scale imbalance를 줄이면서 성능이 향상되어 왔다. scale imbalance 에서 시간과 성능을 trade-off 하는 방법 중 일반적인 방법으로 여러개의 lighter network를 이용한 multi scale의 이미지를 사용하는 것 이었다.5.4 Open Issue해당 글에서는 Open Issue에 대해 다루지 않으며, 논문에 제시된 Issue만 간단하게 언급하고 넘어간다.5.4.1 Characteristics of Different Layers of Feature HierarchiesFeature 계층의 서로 다른 layer가 가지는 특성을 분석하고 심도 깊은 솔루션을 제시해야 한다.5.4.2 Image Pyramids in Deep Object Detectors메모리 비용으로 인해 deep learning에서 이미지 피라미드를 이용하는 것이 어렵기 때문에 이를 해결하기 위한 솔루션이 필요하다.6 Imbalance 3 : Spatial ImabalanceDefinition 크기, 모양, 위치, 이미지, box, IOU 들은 공간 속성이다. 이러한 속성의 불균형은 훈련과 generalization의 성능에 영향을 끼친다. 예를 들어, 위치의 약간의 변화만으로도 localization loss가 급격하게 변경되어 적절한 loss function이 선택되지 못할 수 있다.6.1 Imbalance in Regression Loss Fig. 11: An illustration of imbalance in regression loss. Blue denotes the ground truth BB. There are three prediction boxes, marked with green, red and yellow colors. In the table on the right, L1 and L2 columns show the sum of L1 and L2 errors between the box corners of the associated prediction box and the ground-truth (blue) box. Note that the contribution of the yellow box to the L2 loss is more dominating than its effect on total L1 error. Also, the contribution of the green box is less for the L2 errorDefinition 공간 불균형은 Regression loss와의 관련이 깊다. Figure 11 을 보면 Box들 이 그려졌을때, IOU에 대한 Loss를 계산하기 위해 L1과 L2를 사용하는 example을 보여주는데, L1이 L2보다 loss 변동폭이 적고 좀 더 balace한 오류를 보여준다.Solutions Object detection 분야에서 regression loss는 2가지의 주요 주제로 진화해 왔는데, 첫번째는, L-norm-based(like L1,L2)함수와 IoU-based함수이다. 그에 대한 내용은 아래의 표와 같다. Table 5 : A list of widely used loss functions for the BB regression task해당 함수들의 수식과 자세한 내용은 논문을 직접 참조 하는 것이 좋겠다. 본 글에서는 생략.6.2 IoU Distribution Imbalance Fig. 12: The IoU distribution of the positive anchors for a converged RetinaNet with ResNet-50 on MS COCO before regression (a), and (b) the density of how the IoU values are affected by regression (IoUB: before regression, IoUA: after regression). For clarity, the density is plotted in log-scale. (c) A concise summary of how regression changes IoUs of anchors as a function of their starting IoUs (IoUB). Notice that while it is better not to regress the boxes in larger IoUs, regressor causes false positives more in lower IoUsDefinition IoU 분포 불균형은 box에 기울어진 IoU분포가 있을때 나타난다. (a)그림은 RetinaNet에서 anchor의 분포가 낮은 쪽으로 치우쳐져 있는 것을 확인할 수 있다. (b)그림은 anchor가 regression에게 받는 영향을 나타낸다. (c)그림은 (b)그림의 파란선과 빨간선을 기준으로 anchor의 비율을 나타낸다. 파란선을 기준으로는 0.5로 갈 수록 anchor의 수가 줄어드는 모습을, 빨간선을 기준으로는 False positive가 0.5-0.6 에서 5%밖에 차이안나는 것을 확인할 수 있다. 이것들로 하여금, input box의 IoU결과에 대한 regressor의 효과를 확인해야한다라고 한다.Solutions Cascade R-CNN은 최초로 IoU imbalance를 해결하였는데, 단일 IoU threshold에 단일 detector가 최적일 수 있고, regressor가 임계값에 overfitting시킬 수 있다는 것을 주장했다.그들은 IoU 임계값을 0.5~0.7을 사용하여 세개의 detector를 훈련했다. detector는 box를 새로 샘플링하지 않고, 이전 단계의 box를 사용함으로써, IoU 분포를 거의 균등한 상태로 만드는 것에 성공했다.Faster R-CNN 은 무작위로 생성된 positive input box를 사용하여 IoU 분포의 균형을 맞추었다.이외에 jittering을 주는 방법, pRoI generater 등 IoU분포가 균일할 때 최상의 성능을 얻을 수 있다는 것을 확인했다.6.3 Object Location Imbalance Fig. 13 : Distribution of the centers of the objects in the common datasets over the normalized image.Definition 현재 물체 감지기는 조밀하게 샘플링된 anchor를 사용하기 때문에 이미지 전체의 object의 분포가 중요하다. 대부분의 방법에서 각 부분들이 동알한 중요도를 가지기 때문이다. 그러나 현실의 이미지는 균일한 분포를 가지지 못하는 것이 일반적이다.위 그림을 보면 dataset들의 객체 위치가 대부분 중앙에 몰려있음을 알 수 있다.Solutions anchor는 이미지에 따라 다르기 때문에 fully convolutional classifier와 다른 방식을 사용한다. anchor 크기에 따라 균형있게 만들기 위해 변형가능한 convolution을 이용하여 anchor-guided feature adaption를 제안한다. 즉, anchor방식이 아닌 free anchor방법은 IoU>0.5는 postive로 보고 ground truth와 매칭되게 anchor를 학습한다. 이런 방식으로 모인 anchor집합중 ground truth와 가장 적함한 anchor를 선정하여 학습된다.6.4 Comparative Summary공간 불균형은 object detector 분야에서 상당한 개선을 해왔다. 일반적으로, Regression loss 및 IoU 분포 불균형을 해결하면, regressor branch에서 개선되는 반면 anchor의 bias를 제거하면 classification또한 성능이 향상된다는 것을 알 수 있었다.6.5 Open Issues해당 글에서는 Open Issue에 대해 다루지 않으며, 논문에 제시된 Issue만 간단하게 언급하고 넘어간다.6.5.1 A Regression Loss with Many Aspects최근 연구는 서로 다른 관점과 측면으로 regression loss들을 제안해왔다. 이러한 것들의 장점들을 결합할 수 있는 단일 regression loss function이 필요하다.6.5.2 Analyzing the Loss Functionsoutliers와 inliers가 regression loss에 미치는 영향을 분석하려면 loss function과 input에 대한 기울기를 분석하는 것이 유용하다. 다양한 연구들이 이루어지고 있고, AP loss의 경우는 전체에 따라 값이 달라지므로 단일 입력에대한 Loss를 알기 어렵다는 것이 있다. 그러므로, Loss function이 example들에게 패널티를 주는 방식을 알기위한 적절한 분석방법이 필요하다.6.5.3 Designing Better Anchors최적의 Anchor를 찾는 것은 관심받고 있지 못하다. 자연에 있는 물체의 위치와 크기 불균형을 해결해야 한다.6.5.4 Relative Spatial Distribution Imbalanceoutput Box와 ground truth의 IoU 분포는 불균형이고, 이는 성능에 영향을 미친다.6.5.5 Imbalance in Overlapping BBsBounding box의 동적인 특성 때문에 sampling하는 동안 input 이미지에 대해 over sampling이나 under sampling이 될 수 있고, 그로 인한 불균형의 영향을 조사되지 않고 있다.6.5.6 Analysis of the Orientation Imbalance물체의 orientation 분포의 불균형의 영향을 조사해야한다.​​출처: https://keyog.tistory.com/41?category=879582 [인간지능이 인공지능을 공부하는 장소] "
Jetson Xavier NX and YOLOv5 Object Detection ,https://blog.naver.com/kwy1052aa/222757486915,20220603,"Jetson Xavier NX & YOLOv5 & Intel Realsense Jetson Xavier & Intel RealsenseYOLOv5 설치 sudo apt-get install gitsudo git clone https://github.com/ultralytics/yolov5.git 필요한 패키지 버전 확인sudo apt install python3-pippip3 install -U PyYAML==5.3.1pip3 install tqdmpip3 install cythonpip3 install -U numpy==1.19.4sudo apt install build-essential libssl-dev libffi-dev python3-devpip3 install cycler==0.10pip3 install kiwisolver==1.3.1pip3 install pyparsing==2.4.7pip3 install python-dateutil==2.8.2sudo apt install libfreetype6-devpip3 install matplotlibsudo apt install gfortransudo apt install libopenblas-devsudo apt install liblapack-devpip3 install scipy==1.4.1 #*****sudo apt install libjpeg-devpip3 install pillow==8.3.2 https://forums.developer.nvidia.com/t/pytorch-for-jetson-version-1-11-now-available/72048 PyTorch for Jetson - version 1.11 now availableBelow are pre-built PyTorch pip wheel installers for Python on Jetson Nano, Jetson TX1/TX2, Jetson Xavier NX/AGX, and Jetson AGX Orin with JetPack 4.2 and newer. Download one of the PyTorch binaries from below for your version of JetPack, and see the installation instructions to run on your Jetson. ...forums.developer.nvidia.com wget https://nvidia.box.com/shared/static/p57jwntv436lfrd78inwl7iml6p13fzh.whl -O torch-1.9.0-cp36-cp36m-linux_aarch64.whlpip3 install typing-extensions==3.10.0.2pip3 install torch-1.9.0-cp36-cp36m-linux_aarch64.whlsudo apt-get install libjpeg-dev zlib1g-dev libpython3-dev libavcodec-dev libavformat-dev libswscale-devsudo git clone --branch v0.9.0 https://github.com/pytorch/vision torchvisioncd torchvisionexport BUILD_VERSION=0.9.0python3 setup.py install --user #*****pip3 install --no-deps seaborn==0.11.0cd ..python3 detect.py --source data/images --weights yolov5s.pt --img 640 YOLO5설치 완료2. RealSense SDK 설치 git clone https://github.com/jetsonhacks/installRealSenseSDK.gitcd installRealSenseSDK/./buildLibrealsense.sh==================The library is installed in /usr/local/libThe header files are in /usr/local/includeThe demos and tools are located in /usr/local/bin===================sudo gedit ~/.bashrcexport PYTHONPATH=$PYTHONPATH:/usr/local/lib/python3.6/pyrealsense2source ~/.bashrcsudo apt install libcanberra-gtk* 3. Sample 코드 실행 샘플 코드 실행설치과정에서 중간중간 인내심을 필요로 하는 구간도 있지만 정상 설치 후 예제 코드까지 실행해봤다. 이제 샘플코드를 조금씩 가공하면서 가지고 놀아봐야겠다. "
파이썬 ImportError: cannot import name 'trainer' from 'object_detection' 에러 ,https://blog.naver.com/alaldi2006/222149433385,20201120,ImportError: cannot import name 'trainer' from 'object_detection'​ object_detection/train.py 에 가서from object_detection import trainer => from object_detection.legacy import trainer 로 변경 
NH AI Store Object Detection 성능(CoCo 대비) ,https://blog.naver.com/mikangel/222624176559,20220117,(왼쪽) CoCo Pretrained  vs NH AI Store (오른쪽)​  ​Pretrained CoCo Data 와 BBox 정확도 및 Realtime을 위한 Low Latency 성능 비교 
"[ML] Object Detection (Faster R-CNN / YOLO / YOLO9000 /SSD, 동영상) ",https://blog.naver.com/horajjan/221437523696,20190109,"1. Faster R-CNN : Towards Real-Time Object Detection with Region Proposal Networks 2. You only look once: Unified, real-time object detection 3. YOLO9000: Better, Faster, Stronger 4. SSD: Single Shot MultiBox Detector "
Object Detection - Haar filter를 이용한 얼굴 인식 개념 (1) ,https://blog.naver.com/dongju0531hb/222441758496,20210723,"#시각지능 #객체검출 #AI #비전인식 #인공지능​0. Intro...저번시간에는 단순히 찾고자하는 타겟 template 이미지를 전체 이미지에서 유사한 위치 x,y를 찾아내는 것을 했다. (template matching)이번 시간은 조금 더 심화된,타겟(찾아야 하는) template 이미지 없어도, 전체 이미지만 있으면 거기서 얼굴을 찾아낼 수 있는 알고리즘을 공부해보자.이 알고리즘은 2001년 비올라와 존스가 발표한 boosting 기반의 cascade classifier 알고리즘을 기반으로 만들어졌다.아직 뭔말인지 몰라도 된다... 차차 알아보자​haar filter를 가장 설명 잘한 사이트를 찾았다. 영어라서... 다시 번역하고 정리해서 올린다.모든 자료는 아래 사이트에서 가져왔고, 몇 가지 추가적으로 이해한 부분과 haar 개념을 추가했다.​ Face Detection with Haar CascadeExploring a bit older algorithm which proves to be challenging even in the Deep Learning timestowardsdatascience.com ​​  ​1. Haar-like filter란?이미지에서 얼굴을 찾아내기 위해서 사용되었던 필터가 있는데 (마치 template처럼, 근데 훨어씬 단순한 형태)Haar-like filter를 이용하면 쉽고 빠르게 얼굴 feature를 찾아낼 수 있다고 한다.​1) 우선 Haar는 또 무슨 의미 일까?헝가리 수학자 Alfred Haar에 의해 1909년에 발표된 개념인데,haar를 자세히 이해할려면 Haar wavelet function을 이해해야하고, 또 wavelet function 개념과, MRA(multi resoultion analysis) 개념도 알아야 한다... 대학원 영상처리 과목을 들으면 이해가 되는데, 겁나 어려움... 나도 이해하는데 2주가 걸렸다(아직 잘모름ㅋ)​어쨌든 blueprint를 설명해보자면, 가장 기본 개념은 푸리에 변환처럼 wavelet이라는 함수를 이용하면,어떤 신호든 이미지를 wavelet 함수들의 합으로 나타낼 수 있다. (마치 조화함수로 푸리에 급수로 변환하는 것처럼)근데 이때 wavelet 함수의 종류 중 하나인 haar 함수가 있다. haar 함수를 2차로 했을 때 아래와 같이 나타낼 수 있다.아래는 detail를 나타내는 함수이다.  즉 이런 haar 함수는 n차로 resolution(해상도)를 증가하면 모든 함수를 haar 함수들 가지고 표현할 수 있다.그럼 이 haar 함수들은 basis(기저) 고유한 성질(나만 가질 수 있어)을 가지고 있기에, 어떤 특징을 설명할 수 있는 능력이 있다.이런 haar 함수를 signal processing이 아닌 image processing 분야에서 생각해보면 그 이미지의 특징 feature가 되는 것이다.​​위의 haar 함수에서는 음수가 가능했는데, 이미지 분야에서는 음수가 불가능하기 때문에 아래처럼 haar feature라고 부르고 0,1로 단순히 표현된다. 출처 : https://miro.medium.com/max/1400/1*fQBZTdPk_YzaR7If7Sjzxg.png 2) 자 haar filter(0,1 값으로 이루어진)와 얼굴이 있는 이미지를 가지고 뭘 할까?아래 그림을 보자자 8*8 이미지가 있다. 자 왼쪽 pixel 값이 0~1의 실수 값으로 있다.(이는 0~255를 정규화한 것)​자 중앙에 6*6 사각형에서 왼쪽 흰색 테두리에는 값을 다 더하고, 오른쪽 검은색 테두리에는 값은 다 빼자.그럼 0.51-0.53 = -0.02가 된다. 출처 : https://miro.medium.com/max/2000/1*O-jJazmNKnYDygHkc35nrw.png이렇게 만약에 왼쪽 이미지에서 흰색 테두리안의 pixel 값이 다 1이고, 검은색 테두리의 안의 pixel 값들은 다 0이라고 해보자. 그러면 (1+1+...+1)/18, (0+0+...+)/18이 되어 1-0 = 1이 된다.이렇게 1에 가까울 수록 haar filter에 feature(특징)을 잘 갖고 있다고 볼 수 있다. 즉 edge 모서리라는 특징을 잘 갖게 되는 것이다.​여기서 목적은 haar feature에서 어두운 영역에 놓여있는 image 모든 pixel의 합과haar feature에서 밝은 영역에 놓여있는 모든 pixel의 합을 찾아내는 것이다.그리고 그 차를 찾아내는 것. 그럼 haar 값이 1에 더 가까우면 haar feature를 갖고 있고, 1에서 멀면 갖고 있지 않다.​그리고 haar filter를 움직이면서 전체 이미지에서 특정 부위에서 haar feature를 갖고 있는지 찾아낼 수 있다. 이미지 출처 : https://miro.medium.com/max/952/1*BpHwuCr9q9eldVKzUcFkwA.gif​자 그럼 전체 이미지에서 아래 a와 b처럼 수평 또는 수직 방향의 edge를 찾고 싶을 수도 있고, c와 e같은 feature도 찾고 싶고, d처럼 대각선 edge도 찾고 싶을 수 있다.​ 출처 : https://miro.medium.com/max/1400/1*fQBZTdPk_YzaR7If7Sjzxg.png그러면 이 각각의 feature filter를 위에서 처럼 순회를 하면 3(필터 3개)*N*N*F*F 많은 수학적 계산이 필요하다.당시에는 그런 고성능이 힘들었다.​​​이를 해결하기 위해   3) original image의 pixel 값을 사용하는 것이 아니라, integral image라는 개념을 사용하였다.​integral image는 original image에서 각 픽셀의 x,y라고 할 때 x,y보다 왼쪽과 위쪽 방향으로 위치한 픽셀 값을 다 더한 값이다.그러면 당연히 오른쪽 아래쪽으로 갈수록 pixel 값은 커지겠지? 출처 : https://towardsdatascience.com/face-detection-with-haar-cascade-727f68dafd08​그럼 이렇게 integral image를 구하게 되면, haar 값을 계산하는 데 사용되는데이 때, haar 영역에 놓여있는 픽셀 값을 일일히 모두 더하지 않아도, 몇 개의 특정 부분만 더하면 그 합을 알 수 있다.integral image에서 아래 이미지처럼 초록색 4개 위치의 값을 더하면 위에서 구했던 0.51, 0.53 값을 그대로 구할 수 있다.  위처럼 original image를 딱 한번의 integral image로 만들고기존의 for문으로 하나하나 전부 안 더하고 특정위치에서 값만 더하는 연산으로, 수십가지의 feature에 적용하면 계산량을 훨씬 줄일 수 있다. (integral image가 멱살 잡고 캐리)​​아래의 제일 왼쪽 이미지에서는 딱봐도 수직방향으로 edge가 있다. 실제로 haar 값은 0.74-0.24 =0.5로 훨씬 1에 가까워 해당 feature(수직 모서리)가 존재하는 것을 알 수 있다.  여기까지는 haar cascade 연구에 사용된 fearue와 이미지의 표현이다.그럼 자 이제 드는 의문은,그래 좋은 feature만 갖고 있으면, 얼굴을 빠르고 잘 찾을 수 있겠다.​​​  4) 근데 얼굴에 특징을 잘 찾아내는 좋은 feature는 뭐라고?????????    얼굴은 feature를 어떻게 정했을까?​얼굴 feature를 잘 나타내는 것은 인중, 미간, 눈과 하관, 코, 이마, 다크써클 부위가 있다. 이렇게 사람의 얼굴은 굴곡이 다 비슷하기 때문에, 그림자가 지는 부위도 비슷하다. ​원래 얼굴과 관련된 haar feature set은 대략 180000개를 가지고 있다. 여기서 더 좋은 feature를 어떻게 찾아내라는 말이야....근데 비올라 존스는 180000개에서 6000개로 무려 줄였다. 이때 addaboost라는 boosting 기법을 사용하여 180000개의 feature 각각을 이미지에게 적용하여 weak learner를 생성하였다. (즉 기계학습 모델)이들 중 일부는 다른 것보다 이미지를 더 잘 분리하고, 일부는 그렇지 않았다. 이러한 weak learner는 최소한의 이미지만 잘못 분류하도록 설계되었다.addaboost 알고리즘은 아래 참조. 에이다부스트 - 위키백과, 우리 모두의 백과사전2020 도쿄 올림픽 에디터톤 이 7월 23일 오후 8시부터 개최됩니다. 10개 언어 에이다부스트 위키백과, 우리 모두의 백과사전. 기계 학습 과 데이터 마이닝 문제 [보이기] 지도 학습 ( 통계적 분류  • 회귀 분석 ) [보이기] 클러스터 분석 [보이기] 차원축소 [보이기] 그래프 모형 [보이기] 이상 발견 [보이기] 인공 신경망 [보이기] 강화 학습 [보이기] 이론 [보이기] 관련 문서 [보이기] v t e 에이다부스트 ( 영어 : AdaBoost : adaptive boosting의 줄임말, 아다부스트는 잘못된 발음)는 Yo...ko.wikipedia.org ​자 여기서 만약에 6000개 전부 feature 계산하다가 1번째 특징에서도 얼굴이 아니라고 판정이 났는데 계속 계산하면 5999번은 계산낭비 한 것이다. 그러면 이런 경우에는 5999번 계산이 이루어지지 않도록 해야겠지?​​​  5) 이것이 바로 Cascading 구조 이다.1단계를 통과하면 2단계에서 haar-like filter 다섯개를 사용하여 얼굴이 아닌지를 검사하고, 얼굴이 아니라고 판단하면 이미지에서 filter를 살짝 옮겨서 다른 부분에 얼굴이 있는지 검사하고, 전부 검사를 통과하면 거기는 얼굴인 것이다. 아래는 지금 original image 표현되어 있긴 한데, 실제로는 integral image을 사용한다. stage1의 haar-like filter를 통과하면 stage2로 가고, stage1에서 실패하면 아래 이미지에서 파란색 테두리가 옮겨지는 것을 볼 수 있다.​​  2. Conclusion비올라-존스 연구에서는 영상을 24x24 크기로 정규화한 후(조절), haar-like filter 집합으로부터 haar 값을 구하고 feature를 찾아낸다.만약 모든 feature가 해당 부분에 있으면 얼굴이다!!!!!!!!!없으면 window를 옮겨서 이미지의 다른 부분에서 얼굴 feature를 찾아낸다. (계속 반복)​6000개 feature에 대해 총 38단계가 있고, 처음 5단계의 feature 수는 1(stage 1), 10(stage2), 25(stage3), 25(stage4), 50(stage5)이다.​ 초기 단계얼굴 특징이 없는 대부분의 window를 제거하여 위음성 (실제로 얼굴이 아닌데, 얼굴이라고 탐지하는) 비율을 줄이고,후기 단계위양성(실제로 얼굴인데, 얼굴이 아니라고 탐지)을 줄이는 데 집중하여 얼굴을 찾아낸다! ​​ https://miro.medium.com/max/551/1*CMDXEnq9_XkBaT_3CZEh1Q.gif​​​이렇게 비올라-존스 알고리즘은 동시대 알고리즘보다 15배 빨랐다.​다음은 구현에 대해서 알아보자! 이해가 되지 않는 부분은 댓글로 남겨주세요!! ​ "
[object detection] 성능지표 ,https://blog.naver.com/lsj952005/222603716976,20211230,"<TP / TN / FP / FN> •True Positive(TP) : 실제 Positive 인 정답을 Positive 라고 예측 (정답) (올바르게 잘 예측한 것)•True Negative(TN) : 실제 Negative 인 정답을 Negative 라고 예측 (정답) (거절해야할것을 잘 거절한것 (아닌걸 아닌거라고 한 것))•False Positive(FP) : 실제 Negative 인 정답을 Positive 라고 예측 (오답) (예측한것 중에 잘못 예측한 값)•False Negative(FN) : 실제 Positive 인 정답을 Negative 라고 예측 (오답) (잘못 예측한 값)​  <Precision & Recall > Precision : 예측한 값들 중 얼마나 정확한지​ Recall : 실제 True 중에 얼마나 잘 예측했는지​  <F1 Score> - 조화평균은 A 높이에서 B까지 선을 내리고, B에서 A로 선을 내리면 h라는 높이가 나오게 되는데, 여기서 h는 조화평균의 절반값입니다.​  만약 다음과 같이 Precision 값이 recall 값에 비해 훨씬 큰 경우 작은 값인 recall 값 근처에 조화 평균 값이 계산되고, 마찬가지로 Recall 값이 precision 값에 비해 훨씬 클경우 조화평균은 precision 값 근처로 계산 결과가 나오게 됩니다.이 처럼 조화 평균을 사용했을 때, 작은 값위주로 평균을 내기 때문에 f1 score는 inbalance 한 데이터에서 사용하기 적합니다.(큰 값에 패널티 주는 개념으로 이해)F1 score는 Precision과 recall 값을 이용해 다음과 같이 계산됩니다.​​출처 : https://eunsukimme.github.io/ml/2019/10/21/Accuracy-Recall-Precision-F1-score/ "
Exploring RGB+Depth Fusion for Real-Time Object Detection ,https://blog.naver.com/deepstock/222058781718,20200812,Keywords : Object detection; Single-shot; Neural Networks; Sensor fusion; RGB; Depth; RGBD​  파악한 내용 최신 경향 최신 논문 최신 성능 
Object Detection(객체 탐지) - 오브젝트디텍션(젝슨나노) ,https://blog.naver.com/85honesty/222734672523,20220516,객체 인식 해보기 외투를 인식 시켜봤습니다.​​ 그런데 말입니다~​외투를 인식하려고 하였으나 사람으로 인식하는 오류가 발생했습니다.옷으로 인식하기 보다는 의자에 있어서 그랬는지 사람으로 인식하는 결과가 도출되었습니다.​​  [Reference]​  dailyman데일리남매일매일 좋은 추억 한두개씩 함께 만들어요😀 선한영향력 긍정적인변화 즐거운하루 같이 가치있게 만들어 가요😄www.youtube.com  구독과 공감 좋아요는 콘텐츠를 만드는 데 큰 힘이 됩니다. 
[ Object Detection] Training후의 결과 파일들  ,https://blog.naver.com/sgkim21/221918866823,20200420, Reference : https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/exporting_models.md tensorflow/modelsModels and examples built with TensorFlow. Contribute to tensorflow/models development by creating an account on GitHub.github.com ​ 
[TensorFlow] TensorFlow Lite를 사용하여 Object Detection IOS APP 빌드해보기 ,https://blog.naver.com/kmo6901/222304041211,20210409,"Tensor Flow Lite 이용하여 IOS APP 빌드하기 ​https://velog.io/@u_jinju/TensorFlow-TensorFlow-Lite를-사용하여-Object-Detection-IOS-APP-빌드해보기 [TensorFlow] TensorFlow Lite를 사용하여 Object Detection IOS APP 빌드해보기나는 TensorFlow의 공식 홈페이지에서 라이브러리를 이용하여 한번 실행해보았다. TensorFlow Lite Object Detection iOS Example Application 필수사항 xcode 설치, 유효한 apple developer ID 1. 설치방법 우선 가장 먼저 github의 주소를 터미널에서 설치한다. 나는 파이썬 가상환경에...velog.io 해당 하는 홈페이지는 내가 작성하는 velog 이다. *참고바람 * "
[object detection] R2CNN ,https://blog.naver.com/lsj952005/222603708622,20211229,"​ R2CNN은 Faster R-CNN의 아키텍처를 기반으로 이루워져있습니다.가장 큰 차이점은 축과 나란한 box 좌표와 기울어진 box 좌표를 둘 다 output으로 얻어내 사용하는 것과 더 많은 텍스트 유형을 검출하기 위해 서로다른 크기의 ROI Pooling을 진행합니다.​<R2CNN 진행 과정>(1) CNN feature maps에서 RPN을 거처 roi를 얻어냄(2) feature map에 얻어낸 roi를 투영 시킨 뒤 fully connected layer의 입력으로 넣기위해 proposal 마다 (7x7, 3x11, 11x3) 여러 크기의 ROIPooling을 수행(3) fully connected layer 거처 (4) text/non-text score(객체가 있는지 없는지에 대한 score), 축과 나란한 방향을 가지는 box(axis-aligned box)의 중심좌표와 높이 너비, 그리고 임의의 방향을 가지는 box(inclined box)의 왼쪽 상단 좌표와 오른쪽 상단 좌표(시계방향) 그리고 높이에 대한 예측을 수행(5) NMS를 통해 최종 bounding box를 얻어 냄​ ​​​​​​​https://github.com/factorLee/R2CNN_Faster-RCNN_Tensorflow GitHub - factorLee/R2CNN_Faster-RCNN_Tensorflow: Rotational region detection based on Faster-RCNN.Rotational region detection based on Faster-RCNN. Contribute to factorLee/R2CNN_Faster-RCNN_Tensorflow development by creating an account on GitHub.github.com ​ "
[영상처리] Saliency Object Detection의 개념/종류/역사  ,https://blog.naver.com/dydgus_55/221879658117,20200329,Saliency object detection에 대해 공부하고 정리한 내용을 공유합니다.    ​ 
[AI 논문 번역]/Object Detection/ST_Swin-L ,https://blog.naver.com/dnlee119/222552362425,20211029,* 해석간 틀린 해석 및 생략된 부분이 동시에 존재합니다! 참고 하시고 사용하시기 바랍니다!* 혹여나 오역이나 오타 등이나 기타 사항 질문은 댓글에 달아주시면 최대한 빠르게 답변드리겠습니다! 첨부파일영어원문.pdf파일 다운로드 첨부파일Soft Teacher.pdf파일 다운로드 ​ ​ 
머신 러닝 Object Detection Sigmoid vs Softmax 차이 ,https://blog.naver.com/rkttndk/222310742757,20210415,"객체 탐지를 할 때, pipeline.config 에 보면score_converter라는 옵션이 있습니다.​여기에 SIGMOID라고 쓸 수도 있고, SOFTMAX라고 쓸 수도 있는데요.​제가 그 동안 알아봤던 바에 의하면sigmoid 는 이진 분류 할때 쓰고, softmax는 다중 분류할 때 쓴다고 머릿속에 남아있습니다.​오늘 제가 다시 찾아본 이유는다중 분류를 하다가, 이진 분류로 넘어왔는데확률이 계속 99%, 98% 에 육박하는 것입니다.labelmap에 없고 train에 없는 사진을, test에 넣었는데 말이죠..제가 바라는 건 확률이 50%이하여서 unknown으로 분류할 수 있는 건데 말이죠..​뭐가 문제일까.. 다중분류(label 11개)일때는 확률이 이렇게 까지 나오지 않았는데, 0.3도 있고 그랬는데..오늘 불현듯 pipeline.config를 보니 이진분류면 sigmoid를 써야한다는 게 스쳐가더군요..​그래서 sigmoid로 바꿨습니다.​그리고 어떤 차이가 있는지 다시 찾아봤습니다.​1. multi label classification 문제일 때, 즉 정답이 하나 이상일 때, 정답은 상호배타적이지 않다.  그러면 raw output 각각에 sigmoid 함수를 써야한다. sigmoid는, 모든 클래스, 아님 그 중에 몇개, 아니면 그중 아무것도 ㅡ 에 대해 가장 높은 확률을 반환해준다.예를들어, x-ray 가슴 이미지에대해서 질병을 분류하고자 할때,이미지는 폐렴, 폐기종, 암, 또는 아무것도 발견이 안 될수도 있다.헐.. 이거였다.나는 한 이미지에 대해 어떤 defect이 있는지, 아니면 아무것도 발견을 못하는 (unknown)이런 걸 원했는데,이거였다. multi label classification problem 이었구나.. sigmoid를 써야하는구나.무조건 이중 분류일때 sigmoid 인줄 알았는데, 다중 분류여도 multi label 이라면 sigmoid를 써야하겠구나!​2. multi-class classifcation 문제일 때, 그러면 여기엔 ""하나의 정답""만 있는 것이다. 결과는 상호 배타적이다. 그러면 softmax 함수를 써야한다. softmax는 전체 출력 클래스의 확률 합을 내놓는다. 합은 1이다. 특정 클래스의 확률을 높이고 싶다면, 다른 클래스의 확률을 줄여야만 한다.예를들어, 손으로 쓴 숫자의 MNIST DATA 집합으로부터 이미지를 분류하고 싶다면, 한 숫자에 대한 한 사진은 오직 하나의 IDENTITY만 가진다. 사진은 동시에 7이나 8이 될 수 없다.앗,나는 하나의 사진에 여러가지 종류의 DEFECT이 있는 것이었는데, SOFTMAX로 하니 안되었던 거구나.오늘도 새로운 걸 하나 배웠다..​​​​참고:https://stats.stackexchange.com/questions/233658/softmax-vs-sigmoid-function-in-logistic-classifier Softmax vs Sigmoid function in Logistic classifier?What decides the choice of function ( Softmax vs Sigmoid ) in a Logistic classifier ? Suppose there are 4 output classes . Each of the above function gives the probabilities of each class being thestats.stackexchange.com ​ "
FLIR_Detection 환경 구성 ,https://blog.naver.com/fainstec_sales/222893470362,20221006," 1.영상 취득​학습을 하고자 하는 대상체의 영상을 취득합니다.확장자는 PNG, JPG로 이미지를 저장 합니다.  ​2. Data Annotation​Data Annotation은 Detection과 같은 지도 학습 전처리의 필수 단계입니다.딥러닝 네트워크는 Annotation 설정이 되어 있는 데이터에서 반복되는 패턴을 인식하는 방법을 학습니다.충분한 학습이 되어야 새 데이터가 제공될 때, 패턴을 인식하게 됩니다.​다운로드 링크 : https://tzutalin.github.io/labelImg/​ Labelmg를 실행 합니다.Open Dir를 클릭하여 이미지 경로를 Open 합니다.​ Change Save Dir를 클릭합니다.Data Annotation 결과 파일 (XML)이 저장될 경로를 설정 합니다.      ※이미지 경로 하위경로로 만드는 걸 권장합니다. 키보드의 “W“ 버튼을 클릭 합니다.Drag 기능을 이용하여 Object에 Bounding Box를 그립니다.Label 명을 기입하고 OK 버튼을 클릭 합니다.File 을 눌러 Save를 실행 합니다.※View 목록에 “Auto Saving” 을 체크 할 경우, 자동 저장이 됩니다.File List의 이미지를 선택해 가며 모든 이미지에 Annotation을 진행 합니다. ​Save 경로에, XML 파일이 정상적으로 저장되어 있는지 확인 합니다.​  3. Augmentation​Detection 학습을 위해 데이터 셋의 숫자를 증가 시키는 작업이 필요합니다.(이미지 수가 충분한 경우, 해당 단계는 Skip하여도 됩니다.)Augmentation은 취득된 이미지를 변형시켜 새로운 이미지를 생성 합니다.Pip를 설치 합니다.Sudo apt-get install python3-pip​ ​다음 명령어를 실행 합니다.sudo -H pip3 install --upgrade --ignore-installed pip setuptoolsImgaug를 설치 합니다.pip3 install imgaug ​ 하기 명령어를 실행 합니다cd Desktopgit clone https://github.com/FLIR/IIS_Object_Detection.git​ ​IIS_Object_Detection 폴더의 image_aug_w_bounding_boxes.py 를 실행합니다.“RGB”를 “L”로 변경 합니다. (Mono 이미지 학습 시)다른이름으로 저장을 진행 합니다. ex ) image_aug_w_bounding_boxes_Mono.py​ IIS_Object_Detection 폴더에 학습에 필요한 이미지를 옮깁니다.수정된 py를 이미지 경로로 복사 합니다.터미널을 실행  합니다.​ 하기 명령어를 실행 합니다.python3 image_aug_w_bounding_boxes_Mono.py --input_image_dir='./image/’ --input_bbox_dir='./Annotate/'​ 정상적으로 동작이 완료 되었는지 확인 합니다. ​  4. Docker 실행 하기​하기 명령어를 실행  합니다. docker run --gpus all --rm -it --name caffe-env-1 -e DISPLAY=${DISPLAY} --net=host --privileged --shm-size=2g --ulimit memlock=-1 --ulimit stack=67108864 -v /dev:/dev -v ~/Desktop:/home/dockerasigiuk/caffe-ssd_devel:latest해당 terminal은 켜둔 상태로 유지 합니다.​  5. Docker 학습 환경 구성하기​새로운 terminal을 실행합니다.하기 명령어를 실행 합니다cd Desktop/IIS_Object_Detectionnano project.config​ ​​DATA_DIR에 Augmentation 경로를 설정 합니다. CLASSNUM은 학습하고자 하는 Detection Class 개수를 의미 합니다.*항상 학습 개수 +1 로 설정해야 합니다. (BackGround)CLASSES 는 학습하는 Detection Class의 이름을 지정 합니다.다수인 경우 “camera,lens” 형태로 ‘,’로 구분합니다.PROJECT_NAME은 출력물이 저장될 폴더명을 지정합니다. (학습 시 자동 생성 됩니다)​  6. Project.config 수정 하기​하기 명령어를 실행 합니다.Nano project.config수정할 부분을 수정한 뒤 CTRL + O , Enter, CTRL + X 를 진행 합니다.​​  7. 학습 하기​Docker가 실행중인 terminal에서 경로를 이동 합니다. cd IIS_Object_Detection학습 실행 명령을 실행 합니다. ./run.sh​  8. Project.config 수정하기​Docker가 실행중인 Terminal에서 목록을 확인 합니다.ls IIS_Object_Detection 폴더로 이동 합니다.cd IIS_Object_Detection학습을 시작 합니다../run.sh​ ​ "
"""FCOS"", One shot Anchor - Free Object Detection ",https://blog.naver.com/tomatian/221876480557,20200327,"FCOS, One Shot Anchor-Free Object DetectionFCOS, an Anchor free object detection models our perform SoTA RetinaNet on average precision. Here’s how it works.medium.com 여기있는 medium의 내용을 해석 요약한 것  ​​​ Anchor free의 장점1. Decrease the number of hyperparameters to set  · anchor를 사용하게 되면 각 feature map에 hyperparameter를 조정해가면서 anchor의 scale과 ratio를 지정해 줘야한다.· anchor를 사용하지 않으면 결국 hyperparameter를 manually하게 조정해 줘야하는 귀찮음을 덜 수 있고, 계산량도 줄어주는 것이다.​​​2. Decrease imbalances between positive and negative samples​· SSD와 같은 모델들은 엄청 많은 anchor를 만들어 낸다. 그럼에도 불구하고 positive로 판별되는 box의 비율은 매우 적다. · 그래서 결국 positive와 negative의 엄청난 imbalance가 생긴다. (IoU 계산에 따라)· 그래서 Focal Loss는 imabalanced sample의 weighted loss를 줄였으나 그래도 해결되지 않음· FCOS는 처음부터 negative와 positive의 balance를 맞춰서 결국 recall rate도 적절하게 만들었다. (하지만 어떻게 negative, positive비율을 맞추는 지는 잘 모르겠다.)​​​​​ FCOS의 구조 [ entire architecture ]figure 1 ·  FCOS는 feature map을 만들기 위해서 FPN을 사용한다. · 그리고 거기에 head를 모든 feature map에 붙여서 classification과 bbox regression을 진행한다.· 추가적으로 centerness라는 과정을 수행한다.· FCOS = FPN (+head) + centerness​​FPN이 궁금하다면 아래 링크로! Feature Pyramid Networks for Object DetectionFeature Pyramid Networks for Object Detection※Github - kerasFPN의 아키텍쳐만 궁금하신 분은...blog.naver.com ​​​ FCOS의 구조 [ Label mapping ]왼쪽 : FCOS가 사용하는 (l,t,r,b)  /  오른쪽 : ambiguous annotation example (테니스 채 , 사람 gt에 걸침) ​· FCOS는 center를 찾아내는 건데. 그걸 point라고 한다.· 만약 point가 ground truth (gt)에 걸쳐 있으면 무조건 positive라고 친다. (IoU를 사용하는 기존의 anchor방법과 대비된다. 그리고 이러한 방식으로 positive의 비율이 늘어나는 거 같음)· 하지만 gt가 여러개 걸친 곳에 point가 자리잡을 일도 많아지기 마련이다. 하지만 이러한 문제점은 FPN을 사용하면 줄어든다.· 이렇게 여러 gt에 걸친 point를 ambiguous annotation이라고 한다.· FCOS는 bbox regression에서 뽑아내는 정보다 edge간의 range와 point(center)정보를 사용한다. -> (l, t, r, b)​​​​​​​ FCOS의 구조 [ centerness ] · FCOS는 gt에 걸친 point는 모두 positive로 판단하기 때문에 low quality의 box를 포함하기 마련이다.·  이걸 방지하고자 하는 것이 centerness다.·  centerness = gt의 center에서 point (prediction box의 center)까지의 거리·  그리고 feature map이후에 branch로 더해진다.  (figure 1을 보면 나와있음)· 0 <= centerness <= 1·  training 중에 centerness는 BCELoss(binary cross entropy loss)를 계산한다. 그리고 모델을 예측하는 데 사용한다.·  centerness는 classification score (figrue1 에서 마지막 classification box에서 나온 결과)에 곱해진다.· 이러한 과정을 통해서 low quality box 뽑아내고 마지막으로 NMS가 이러한 박스들을 제거한다.​​​​​​​​​  이상 동산이었습니다.공부 열심히 하시고 최상에서 만납시다 :)​​​ "
"Object Detection을 위한 Image Annotation, A to Z (2) ",https://blog.naver.com/panzer05/222258371258,20210227,"이미지 자료 모으기1. 구글 사이트 혹은 자신이 찍은 사진을 한 폴더에 몰아 넣습니다. ​2. https://blog.naver.com/panzer05/222253265430 요기를 참조 해서, labelimg tool을 설치합니다.  Bounding Box 작업시 Tool이미지 분류 라든가, 객체인식 등의 훈련을 시킬때, Training Data로 이미지에 Bounding Box작업을 해...blog.naver.com ​3. Anaconda terminal에서 labelimg tool을 실행 시킵니다.ㅣ  ​4. 실행시키면 사진이 있는 폴더를 엽니다. (붉은색 네모박스) 5. 사진이 열리면 Bounding Box를 만듭니다.  좌측에 보면 'Create RectBox' 라는 것이 보이는데, 그것을 클릭하면, Box를 그릴 수 있고, 완료되면, 꼭 Save버튼을 눌러 저장 해야 됩니다. 그리고, Next Image를 누르면 다음 이미지로 넘어가서 이것을 다 끝내면 됩니다.​6. 예제로 비행기 10장 정도 작업해 보면 사진이 있는 폴더에 사진 이름과 같은 파일명의 xml 파일이 생깁니다.  7. VScode가 설치 되어 있다면, 아래와 같이 마우스 오른쪽 키를 누르면 이 폴더에서 작업 할 수 있도록 VS code가 실행 됩니다.  8. VS code가 열리면, test.py를 생성하던, test.ipynb(jupyter file)을 생성 합니다.  9. xml file을 열어 봅니다.  ​이제 부터 이것을 파싱 해야 됩니다. ​- panzer - "
Learning Transferable Human-Object Interaction Detector with Natural Language Supervision(CVPR 2022) ,https://blog.naver.com/kingjykim/223111302060,20230525,"Paper : https://openaccess.thecvf.com/content/CVPR2022/html/Wang_Learning_Transferable_Human-Object_Interaction_Detector_With_Natural_Language_Supervision_CVPR_2022_paper.htmlGit : https://github.com/scwangdyd/promting_hoiAuthor : Suchen Wang, Yueqi Duan, Henghui Ding, Yap-Peng Tan, Kim-Hui Yap, Junsong Yuan​​1. Introduction​  HOI(Human-Object Interaction) 감지는 human-centric visual 분석 작업에서 인간의 의도와 행동에 대한 더 깊은 이해할 수 있도록 하는 중요한 역할을 맡고 있다. 그것은 interaction하는 인간과 객체를 현지화한 다음 그들의 interaction을 인식하는 것을 목표로 한다. interaction은 ""자전거 타기""와 같이 인간의 행동과 체를 한꺼번에 취급할 수 있다. 조합 특성을 고려할 때, 특히 동작 및 객체 범주 공간이 커질 경우(예: SWIG-HOI의 400개 동작 및 1000개 객체) 가능한 모든 HOI를 포함하는 데이터 컬렉션을 만드는 것은 비현실적이다. 따라서 저는 학습할 때 보이지 않는 수많은 잠재적 interaction으로 쉽게 확장될 수 있는 transferable HOI detector를 연구한다.  최근 연구에서는 HOI detectors를 통한 보이지 않는 interaction의 일반화 능력을 향상시키기 위해 compositional learning을 사용했다. 그들의 핵심 아이디어는 interaction을 동작과 객체로 분해하고 데이터 확대를 수행하여 동작과 객체의 새로운 조합을 생성하는 것이다. 그러나 보이지 않는 interaction의 목록을 사용할 수 있도록  interaction의 특정 샘플에 따라 생성될 수 있다는 기본적인 가정을 한다. 그러나 주어진 사전 없이 생성된 interaction의 유효성을 자동으로 결정하는 방법은 여전히 미해결 문제이다. 이런 의미에서 기존 방법은 미리 결정된 경우에만 적합하지만 보이지 않는 다른 interaction으로 이전할 수 없다. Figure 1.   본 논문에서, 보이지 않는 interaction에 대해 사전에 가정하지 않고 transferable HOI detector를 훈련시키는 것을 목표로 한다. Figure 1은 기존 솔루션과의 주요 차이점을 보여준다. 특히, 대부분의 이전 연구에서는 고정된 크기의 가중치를 가진 classifier를 학습하기 위해 이산 레이블을 사용한다. 미리 결정된 설정은 사전 정의된 목록에서 새로운 HOI를 처리할 수 없기 때문에 일반화 가능성과 유효성을 제한한다. 최근 CLIP의 성공에 자극을 받아, 저자는 독립적인 원핫 HOI 레이블을 visual-and-text 공동 모델링을 통해 language supervision으로 변환한다. 이러한 방식으로 HOI 탐지를 visual-to-text matching 문제로 재구성하고 보이지 않는 interaction에 대한 인식을 가능하게 할 수 있다.  구체적으로, 하나의 시각적 인코더와 하나의 텍스트 인코더로 구성된 새로운 1단계 HOI detector를 제안한다. visual encoder의 경우, (1) 추가 [HOI] 토큰을 설계하여 새로운 HOI Vision Transformer를 제시하고 (2) 이미지의 unique HOI detections를 찾기 위해 sequence parser module을 제시한다. 저자는 [HOI] 토큰과 관련하여 최종 레이어의 출력을 interaction의 표현으로 취한다. 그런 다음 bounding box 회귀 및 interaction 인식을 위해 각각 두 개의 헤드에 입력한다. 저자는 regressor를 사용하여 interaction하는 인간과 객체의 bounding boxes를 예측하고 예측에 대한 신뢰 점수를 추정한다. 또한 텍스트 인코더에 내장된 가장 가까운 interaction labels을 찾기 위해 시각적 특징을 공동 visual-and-text feature 공간에 투영한다.  interaction category는 일반적으로 행동 및 객체를 한 쌍으로 정의된다. 저자는 그것들을 별개의 레이블로 취급하는 대신, 행동과 객체 이름을 사용하여 natural language supervision을 구축하고 그것들을 공동 visual-and-text 공간으로 인코딩하여 의미론적 상관관계를 탐구하는 것을 목표로 한다. 최근 연구에 따르면 클래스 명을 둘러싼 context words가 인식 정확도에 상당한 영향을 미칠 수 있다. 저자의 경우, 그것은 더 복잡해지고 사례마다 다른 문장 형식을 요구할 수 있다. 예를 들어, ""타는"" 행동과 ""자전거""라는 객체를 통해 ""자전거를 타는 사람의 사진""과 같은 문장을 만들 수 있다. 그러나 ""낚시""라는 행동와 ""낚시대""이라는 객체를 고려할 때, ""낚시대를 낚는 사람의 사진""이라는 문장은 의미가 없다. 텍스트 생성을 용이하게 하도록 학습 가능한 토큰을 사용하여 수동으로 결정된 단어를 대체한 뒤 문장을 구성하는 자동화된 방법을 제안한다. 이것은 학습 과정에서 보이는 interaction과 보이지 않는 interaction 모두에 대해 더 일반화된 결과를 가져온다.  저자의 기여는 다음과 같다.(1) HOI 탐지를 visual-to-text matching로 재구성하고 보이지 않는 interaction을 탐지할 수 있도록 한다. (2) 추가 HOI 토큰과 HOI sequence parser를 설계하여 Vision Transformer를 사용한 새로운 1단계 HOI detector를 제안하여 인간과 객체를 공동으로 감지하고 interaction을 인식한다. (3) HICO-DET와 SWIG-HOI에 대한 실험을 통해 제안된 방법이 특히 보이지 않는 상호 작용에 대한 HOI detection이 최첨단 결과를 달성할 수 있는지 검증한다.​​2. Related Work​Generic HOI detection 표준 HOI detection는 주로 알려진 interactions에 초점을 맞춘다. 기존의 방법은 크게 1단계 방법과 2단계 방법의 두 그룹으로 나눌 수 있다. 2단계 방법은 일반적으로 offline object detector를 적용하여 먼저 사람과 객체를 감지한 다음 감지된 상자를 분류를 위해 interaction model에 전달한다. object detector를 사용한 end-to-end training이 필요하지 않은 두 번째 단계는 일반적으로 탐지된 bounding boxes 사이의 관계(예: multisteams 또는 graphs)를 분석하기 해 더 정교한 아키텍처를 사용한다. 또한 interaction 인식을 지원하는 정보인 자세, 공간 분포 등을 고려하는 것에 유리하다. 2단계 방법과 비교하여, 1단계 방법은 하나의 모델을 사용하여 bounding boxes를 공동으로 탐지하고 interactions을 인식하는 것을 목표로 한다. 초기의 1단계 검출기는 종종 병렬 구조를 적용하여 bounding boxes 후보를 동시에 생성하고 interactions하는 지점 또는 쌍을 예측한 다음 최종 HOI 예측을 형성하는 matching 단계를 따른다. 최근 연구는 HOI 탐지를 세트 예측 문제로 공식화하고 다양한 Transformer 기반 detectors를 제안했다.​Novel HOI detection 상호 작용의 큰 조합 category 공간을 고려할 때, 가능한 모든 category를 포함하는 데이터 수집을 구축하는 것은 어렵다. 최근 몇 가지 연구는 novel HOIs 또는 zero-shot HOIs를 처리하는 방법을 연구했다. 대상 개체를 알고 있는지 여부에 따라 새로운 interactions은 대략 두 가지 유형으로 나눌 수 있다. 첫 번째 유형은 알려진 동작과 알려진 객체 간의 새로운 조합이다. 이와 대조적으로, 새로운 물체와의 보이지 않는 interactions을 감지하는 것은 더 어렵다. 일부 연구는 zero-shot 인식을 지원하기 위해 의미 단어 임베딩을 사용할 것을 제안했다. 그러나 저자의 방법과는 달리, 그들은 여전히 사전 정의된 목록에서 보이지 않는 다른 interactions는 처리하기 어려운 사전 정의된 분류기에 의존한다.​Natural language supervision Vision-language 사전 훈련은 최근 이미지 및 비디오 이해를 위한 유망한 접근 방식으로 부상했다. 이산 레이블을 사용하는 기존 방식과는 달리 시각적 및 텍스트 특징 정렬을 기반으로 인식을 수행하는 새로운 패러다임을 제공한다. 다양한 다운스트림 작업에 대해 zero-shot transfer을 사용하는 것에 자연스럽게 적합하다. 최근 연구에서는 visual question answering(VQA), zero-shot object detection 및 image captioning 등을 해결하기 위해 사전 훈련된 모델의 전송 가능한 지식을 사용하는 법도 탐구했다. 그들의 연구에 자극을 받아, 저자는 자연어 감독을 기반으로 전송 가능한 HOI detector를 학습하는 방법을 탐구하는 것을 목표로 한다.​​3. Methodology​  저자는 human-object interaction(HOI) detection 문제를 해결하고 보이지 않는 interactions을 처리할 수 있는 transferable detector를 개발하는 것을 목표로 한다. 공식적으로, interactions을 tuple {(bp, bo, a, o)}로 정의한다. 여기서 bp, bo ∈ R4는 인간과 객체의 bounding box를 나타내고, a ∈ A = {1, ... , A}는 인간의 행동을 나타내며, o ∈ O = {1, ... , C}는 객체 category를 나타낸다. interactions의 조합 특성 때문에, 특히 A와 O가 큰 공간(예: SWiG-HOI에서 약 400개의 동작과 1000개의 객체)을 포함하는 가능한 모든 동작과 객체의 조합을 포함하는 data collection을 구성하는 것은 비현실적이다. 이전 연구와 유사하게, 저자는 일반화된 제로샷 interactions 감지 설정에 초점을 맞춘다. 구체적으로, A와 O가 모두 알려져 있지만 학습 중에 이들 사이의 부분적인 조합만 관찰될 수 있다고 가정한다. 추론에서, 보이는 것과 보이지 않는 human-object interaction을 모두 감지하고자 한다.​3.1. Preliminary: Visual-and-Text Modeling  인식 작업에 대한 전통적인 접근 방식은 이미지 입력을 고정된 이산 레이블 세트에 매핑하는 visual classifier를 학습하는 것이다. 이러한 처리에서 보이지 않는 category는 원래 레이블 세트에서 벗어났기 때문에 처리하기가 본질적으로 어렵다. HOI detection에 대한 이 문제를 해결하기 위해 기존 연구는 종종 compositional learning에 의존한다. 핵심 아이디어는 HOI를 별도의 동작 및 객체 구성 요소로 분해하는 것이다. classifier level 분해의 경우, a와 o가 독립적이라고 가정하고 p(a, o)를 p(a) · p(o)로 근사하며, 여기서 p(·)는 신뢰 점수를 나타낸다. 그러나 이 방법은 동작-객체 상관관계를 모델링하지 못한다. 게다가, 인간이 여러 객체와 다수의  interactions을 수행할 때 오류가 발생하기 쉽다. 공동 분포를 더 잘 모델링하기 위해, 최근 연구는 데이터 생성을 통해 누락된 interactions을 채울 것을 제안한다. 그러나 이러한 방법은 훈련 중에 특정 샘플이 생성될 수 있도록 보이지 않는 interactions의 category를 미리 결정해야 한다. 이로 인해 사전 정의된 목록에서 벗어난 보이지 않는 다른 interactions을 처리하는 것이 여전히 어렵다.  이 연구에서, 저자는 텍스트 설명을 기반으로 interactions을 인식하는 것을 탐구한다. CLIP에 기반으로 이미지 및 관련 텍스트 설명의 HOI가 잘 정렬될 수 있도록 joint visual-and-text feature space을 학습하는 것을 목표로 한다. 이는 다음과 같이 두 가지의 장점이 있다. 첫째, interactions을 별개의 레이블로 보는 것과는 달리 interactions 간의 의미론적 유사성을 활용하여 보이지 않는 interactions 인식 문제를 제거할 수 있다. 둘째, 고정 레이블 세트가 있는 classifier를 학습하는 것에 비해, 이 방법은 joint feature space에서 가장 가까운 검색을 수행하여 보이지 않는 시각적 개념을 기존 텍스트 설명과 연결할 수 있기 때문에 zero-shot 작업을 수행하는 데 더 유연하다. 이 아이디어가 HOI detection 작업에 효과가 있는지 검토하기 위해 CLIP으로 시작한다. CLIP은 대규모 이미지 텍스트 쌍에서 좋은 visual-and-text 참조를 학습하고 다양한 zero-shot 인식 작업에서 큰 성공을 거두었다. Table 1. Preliminary studies on HICO-DET dataset.  CLIP은 원래 이미지 수준 인식을 위해 설계되었으며, interaction하는 인간과 객체의 bounding boxes를 감지하고 instance-level interaction 인식을 수행하는 것을 목표로 한다. 예비 연구로서 CLIP를 기존 객체 감지기와 결합하여 간단한 기준선을 구현한다. 특히, 사전 훈련된 Faster RCNN을 사용하여 bounding boxes를 생성한다. 그런 다음 모든 인간을 객체 상자와 쌍으로 구성하고 CLIP 시각 인코더의 입력으로 결합 영역을 잘라낸다. interactions에 대한 기존 택스트 설을 구성하기 위해, 문장을 ""a photo of a person [ACT] [OBJ]""으로 생성하는데, 여기서 [ACT]와 [OBJ]는 각각 동작 및 객체 범주 이름(예: riding horse, lassoing cow 등)으로 대체할 수 있다. Table 1은 표준 mAP 평가 메트릭을 사용하여 HICO-DET 데이터 세트의 최첨단 방법과 성능을 비교한다. 저자는 이러한 간단한 기준선이 대상 데이터 세트에 대한 조정 없이도 보이지 않는 interaction에서 유망한 결과를 얻을 수 있음을 관찰한다. 이 결과에 자극을 받아 CLIP와 같은 시각 및 언어 모델링 방식으로 HOI 탐지 문제를 재구성하고 보이지 않는 interaction으로 전달 가능한 1단계 HOI detector를 학습할 수 있는지 여부를 추가로 탐구한다.​ Figure 2. The architecture of our proposed one-stage transferable HOI detector.3.2. The Proposed Method  그림 2는 제안된 HOI detector의 전반적인 아키텍처를 보여준다. 주로 HOI 시각적 인코더와 텍스트 인코더로 구성된다. 시각적 인코더는 RGB 이미지를 입력하여 예측 집합 {(h, c, bp, bo)}을 출력한다. 여기서 h ∈ RD는 interactions의 특징 표현을, bp ∈ R4는 interactions하는 인간과 객체의 bounding boxes를, c ∈ [0, 1]은 경계 상자 예측에 대한 신뢰 점수를 나타낸다. 텍스트 인코더의 경우, 그것은 interactions의 기존 텍스트(예: riding horse, lassoing cow)를 입력으로 받아 의미론적 특징 집합 {s}로 인코딩하며, 여기서 s ∈ RD는 h와 동일한 특징 공간을 공유한다. 그런 다음 유사성 h를 기반으로 interactions 인식을 수행한다. 다음에서는 HOI detector를 잘 학습하기 위해 visual 인코더와 text 인코더의 세부 정보뿐만 아니라 손실 함수도 제공한다.​3.2.1 HOI Visual Encoder  연구에서, CLIP가 많은 image-text 쌍에서 학습함으로써 transferable visual-and-text reference를 얻었다는 것을 발견했다. 저자의 방법은 CLIP을 외부 지식 기반으로 취급하고 HOI detection를 위해 지식을 증류하는 것을 목표로 한다. CLIP은 원래 이미지 수준 인식을 위해 설계된 반면 bounding boxes를 감지하고 instance-level interaction 인식을 수행하기 위해 설계되었기 때문에 간단한 작업이 아니다. 이러한 격차를 개선하기 위해 새로운 ViT 기반 시각 인코더를 제안한다.​ViT-based Visual Encoder visual encoder는 고정 해상도(예: 224 x 224)의 이미지 I ∈ RHxWx3로 둔다. 이미지를 크기가 P × P(예: 16 × 16)인 작은 patch로 나누고 시퀀스 [x01 ; x02 ; ... ; x0N ]로 투영한다. 여기서 xℓi ∈ RD는 i번째 이미지 패치의 embedding을 나타내고 ℓ ∈ {0, 1, ... , L}은 Transformer 레이어의 인덱스를 나타낸다. 여기서는 설명의 용이성을 위해 x0i가 이미 위치 embedding을 포함하고 있다고 생각한다. ViT에서 학습 가능한 embedding z00 ∈ RD([CLS] 토큰이라고도 함)는 패치 embedding 순서 앞에 추가된다. 마지막 레이어의 출력 zL0은 이미지를 표현하는 역할을 한다. [CLS] 토큰은 softmax attention mechanism을 사용하여 일련의 패치로부터 필요한 정보를 집계한다. 입력을 X0 = [z00 ; z01 ; z02 ; ... ; z0N ] ∈ R (N+1)×D로 표시한다. ViT는 각 계층 ℓ = 1, ... , L에서 다음 단계를 수행한다. 여기서 MHA는 Multi-head Attention 모듈을, LN은 LayerNorm을, MLP는 two-layer 퍼셉트론을 나타낸다. MHA가 입력을 정규화하기 위해 내부에 LN 레이어를 가지고 있다고 가정하며, 간결한 설명을 위해 위의 방정식에서 생략한다.​New Tokens for HOI Detection image-level 인식과는 달리 instance-level HOI detection를 수행하는 것을 목표로 한다. 이러한 방식으로 [CLS]와 유사하게 일련의 새로운 HOI 토큰 [HOI]1, [HOI]2, ... , [HOI]M을 도입한다. 이미지의 다양한 interactions과 관련하여 유용한 정보를 집계하기 위해 [CLS]와 같은 역할을 할 수 있을 것으로 예상된다. hℓi ∈ RD 일 때, H0 = [h01; h02; ... ; h0M]을 [HOI] 토큰의 초기 상태라고 가정한다. 이미지 패치에서 정보를 집계하기 위해 다음 단계를 대신 수행한다.   여기서 Hℓ-1을 query로, X[1:]ℓ-1을 MHA의 key와 value로 다룬다. 모든 입력이 먼저 정규화될 LN 계층을 통과할 것이라고 가정한다. 간결한 설명을 위해, 위의 MHA 방정식에서 LN을 생략한다. [CLS] 토큰 zℓ-10을 마스크하고 image patch 임베딩 X[1:]ℓ-1 = [xℓ-11; xℓ-12 ; ... ; xℓ-1N]만 공급한다는 점에 주목할 필요가 있다. 주요 이유는 [HOI] 토큰이 [CLS]의 기능을 직접 복사하는 경향이 있으며, 실제 의미 있는 이미지 패치를 식별하지 못하기 때문이다.​ Figure 3. Illustration of the HOI sequence parsing.HOI Sequence Parser Figure 2에서 보는 것처럼, 하나의 이미지에서 인간과 객체 사이에 여러 가지 interactions이 있을 수 있다. 이 경우 [HOI] 토큰이 이를 차별화하고 다양한 interactions에 대응할 수 있을 것으로 기대한다. 그러나 초기 상태 [h01; h02; ... ; h0M]만으로 그렇게 달성하기는 어렵다는 것을 알게 되었다. 이것은 주로 [HOI] 토큰이 병렬로 처리되기 때문인 것으로 추정된다. 결과적으로, 컴퓨터는 어떤 interaction이 인간과 객체에 의해 감지되었는지 알지 못한다. 이 문제를 완화하기 위해, 토큰을 시퀀스 방식으로 구문 분석하는 모듈을 도입한다.  각 계층 ℓ에서, MHA로부터 출력을 얻는다. 즉, [hℓ1; hℓ2; ... ; hℓM]. parser module의 주요 목표는 토큰이 자신을 차별화하고 서로 다른 interactions을 찾을 수 있도록 하는 것이다. 이에 영감을 얻어 시퀀스의 이전 버전을 기반으로 각 [HOI] 토큰을 구문 분석할 것을 제안한다. 구체적으로, 저자는 [CLS]와 그 이전 버전인 [HOI]1에서 [HOI]i-1에 따라 [HOI]i, hℓi의 기능을 업데이트하는 매핑 함수 F(·|·)를 배우는 것을 목표로 한다. 이것을 다음과 같이 표현할 수 있다. 직관적으로 첫 번째 토큰 [HOI]1이 [CLS] 토큰(예: zℓ 0)을 기반으로 하나의 interaction(예: 가장 중요한 interaction)을 찾을 수 있을 것으로 예상한다. 그런 다음, 두 번째 토큰 [HOI]2는 [HOI]1이 다른 토큰을 탐지하고 찾는 것을 목표로 하는 것을 기반으로 구축할 수 있으며, 다른 토큰에 대해서도 마찬가지이다. parser는 MHA 모듈로 인스턴스화되며 mask를 사용하여 관심 영역을 제한하여 각 토큰에 대해 이전 영역만 고려할 수 있다. 이 모듈은 Eq 2의 단계 이후에 각 레이어에 추가된다. Figure 3은 원래 병렬 처리와 비교하여 parser가 작동하는 방식의 예를 보여준다.​Projection Head and Bounding Box Regressor final layer의 출력은 상호 작용, 즉 [hL1; hL2; ... ; hLM]으로 표현한다. 그런 다음, 그것들을 두 개의 다른 head networks에 공급한다. 첫 번째는 linear projection, Fproj(h) : RD → RD다. 특징을 시각 및 텍스트 공동 공간에 매핑한한다. CLIP와 유사하게, interaction 인식을 위해 텍스트 인코더의 특징들의 유사성을 계산한다. 두 번째는 bounding box regressor Fbbox(h) : RD → R9이다. 신뢰 점수와 상호 작용하는 인간과 객체의 bounding box [c, bp, bo]를 예측한다. 여기서 bt ∈ R4, t ∈ {p, s}는 DETR로 좌표를 나타내는 정규화된 bounding box를 나타낸다. 객체 범주는 공동 시각 및 텍스트 공간에서도 인식되기 때문에 이것이 클래스에 구애받지 않는 regressor라는 것에 주목할 필요가 있다. 또한, bounding box 예측이 실제 interaction을 포착하는지 여부를 추정하기 위해 신뢰 점수 c ∈ [0,1]을 사용한다. 저자의 방법에서, 고정된 크기의 M 토큰을 도입하는 반면, 이미지의 interaction 수는 경우에 따라 다를 수 있다. 이미지에 m < M 상호 작용이 있다고 가정하면, 나머지 M - m 토큰은 예측이 필터링될 수 있도록 낮은 점수를 가질 것으로 예상된다.​3.2.2 Text Encoding  텍스트 인코더의 주요 목표는 interaction의 기존 텍스트 설명을 특징 공간에 매핑한 다음 위 HOI visual encoder의 출력과 비교하는 것이다. 기존 택스트는 토큰화되고 일련의 단어 임베딩으로 인코딩된다. 학습 가능한 [EOS] 토큰은 끝에 추가되고 마지막 계층의 출력은 전체 문장의 표현으로 처리된다. 최근의 연구는 클래스 이름을 둘러싼 contexts words의 선택이 인식 정확도에 상당한 영향을 미칠 수 있다는 것을 보여주었다. 본 연구에서는 사전 훈련된 CLIP 텍스트 인코더를 사용하고 훈련 중에 동결한다. 주요 초점은 human interactions을 위한 기존 텍스트 설명을 구축하는 가장 적합한 방법을 탐구하는 것이다.   interaction category는 일반적으로 (ride, horse), (lasso, cow) 등과 같은 actions-objects 쌍으로 정의된다. CLIP에 따르면, 한 가지 단순한 접근법은 사전 정의된 형식을 사용하여 빈칸을 채우는 것이다. 예를 들어, ""a photo of a person [ACT] [OBJ]"". 여기서 [ACT]와 [OBJ]는 각각 동작과 객체의 실제 범주 이름으로 대체될 수 있다. 그러나 모든 interactions에 적합한 보편적인 형식을 찾기가 어렵다는 것을 관찰한다. 예를 들어, 어떤 행동 ""낚시""와 물체 ""낚시대""를 고려할 때, ""낚시대를 낚는 사람의 사진""이라는 문장은 의미가 없다. ""Learning to prompt for vision-language models""에서 영감을 받아 학습 가능한 context 토큰을 사용하여 문장을 구성한다. Figure 2는 텍스트 입력을 구성하는 방법을 보여준다. 구체적으로, ""a photo of a person""과 같이 수동으로 정의된 단어를 대체하기 위해 문장의 시작 부분에 몇 가지 [PREFIX] 토큰을 도입한다. 또한 몇 가지 학습 가능한 [CONJUN] 토큰을 사용하여 동작과 객체의 범주 이름을 연결하는 방법을 자동으로 결정한다. Table 4에서는 텍스트 설명을 작성하는 다른 방법을 조사한다. 저자는 그러한 자동 방법이 사전 정의된 형식과 비교하여 HOI detection에 대해 주목할 만한 개선을 얻을 수 있음을 관찰한다.​3.2.3 Training  이 섹션에서는 HOI detector를 훈련하는 데 사용되는 손실 함수를 제시한다. 손실 함수는 크게 (1) visual-and-text 정렬에 대한 손실과 (2) box regression에 대한 손실의 두 부분으로 나눌 수 있다.  먼저 고유한 탐지를 장려하기 위해 이미지당 예측과 실제 사이의 이분 매칭(DETR)을 수행한다. 이러한 방식으로 각 실측 정보는 하나의 [HOI] 토큰에만 연결될 수 있다. 이미지당 예측 수는 {(hi, cˆi, bˆip, bˆio}Mi=1이고 주석이 달린 대상은 {(si, bip, bio)} Ki=1이라고 가정한다. 이분 매칭은 연관성인 ϕ = [ϕ1, ϕ2, ... , ϕM]을 찾는 것을 목표로 한다. 여기서 ϕi ∈ {0, 1, 2, ... , K}는 [HOI]i에 대한 관련 target의 지수를 나타내며, ϕi = 0은 어떠한 target도 할당되지 않았음을 의미한다. Lm(i, ϕi)을 i번째 토큰과 ϕi번째 주석 대상 사이의 matching cost라고 할 때 다음과 같이 계산할 수 있다. 여기서 Lb는 DETR에서 사용된 바와 같이 ℓ1 손실과 일반화된 IoU 손실을 모두 포함하여 경계 상자 회귀 손실을 나타낸다. 마지막 항목 Lh는 interaction 인식에 대한 손실이다. CLIP를 따라 visual-to-text및 text-to-visual 교차 엔트로피 손실의 합으로 계산한다. 이미지 수준 인식과는 달리 이미지당 인스턴스 수준의 손실을 계산해야 한다. Figure 2와 같이, 하나의 텍스트 라벨은 복수의 [HOI] 토큰에 대응할 수 있다. 이를 처리하기 위해 text-to-visual classification loss을 다음과 같이 다시 작성한다. 여기서 τ는 temperature parameter다. 주요 아이디어는 동일한 텍스트 레이블을 가진 다른 [HOI] 토큰을 생략하는 것이다. 각 [HOI] 토큰은 할당할 텍스트 레이블이 하나만 있음을 보장할 수 있으므로 visual-to-text는 다음과 같이 작성할 수 있다. 역전파 전에, 먼저 목적 함수 minϕ∑Mi=1 Lm(i, ϕi) - cˆi를 풀어서 최상의 이분 매칭 ϕ*를 찾는다. M이 보통 작다는 점을 고려할 때, 이분 매칭은 많은 계산이 필요하지 않을 것이다. 일단 ϕ∗를 찾으면 신뢰 점수에 대한 레이블 ci를 결정할 수 있다. 여기서 [HOI]i가 어떤 대상과도 일치하지 않는 경우 ci = 0이고, 그렇지 않은 경우 ci = 1이. 그 후 일치 결과 ϕ*에 기초하여 다음 손실을 최소화하여 모델 매개 변수를 업데이트한다. 여기서 Lc는 표준 교차 엔트로피 손실이다. CLIP의 사전 훈련된 가중치를 사용하여 Eq.2에서 MHA를 초기화한다. 대상 세트에서 fine-tuning하는 동안의 knowledge forgetting을 제거하기 위해 CLIP의 모든 매개 변수(parameters in Eq.1)를 동결하고 새로 추가된 모듈만 업데이트했다.​4. Experiments-생략-​ "
"OpenCV, Object Detection - HOG, 사람 검출 ",https://blog.naver.com/handuelly/221833630061,20200302," # HOG HOG(Histograms of Oriented Gradients)는 분류를 더 쉽게하기 위해서, 동일한 객체가 약간의 변화(사람 객체가 걷는 모습의 경우)가 있더라도 가능한 하나의 객체로 일반화한다.SVM을 적용한 HOG는 영상 내에서 '사람 검출'을 위해 가장 많이 쓰이는 알고리즘이고, 성능 또한 우수하다.​HOG Feautr 계산 프로세스는 다음과 같다.영상에서 Gradient를 계산하고(1), 이를 이용해 Local Histogram을 생성하고(2), 히스토그램을 이어 붙여서 1차원 벡터를 생성하는데(3), 이 것이 HOG Feature다.(1) Gradient를 계산하기 위해 먼저 영상의 Edge를 계산한다. 다양한 커털을 이용해서 계산이 가능하고, x&y축 방향으로 계산하면 두 값을 이용하여 Orientation(기울기의 방향으로 이해하면 될 듯)을 계산한다.(2) Orientation들을 Histogram의 int 형태로 사용하여 히스토그램을 만든다.(3) 이 히스토그램을 1차원으로 이어 붙여 벡터를 만들면 HOG Feature가 된다.​   출처 : https://darkpgmr.tistory.com/116   # Template Matching vs Histogram Matching 템플릿 매칭은 원래 영상의 기하학적 정보를 그대로 유지하며 매칭하지만, 대상의 형태나 위치에 조금만 변형이 있어도 매칭이 잘 되지 않는 문제점이 있다.반면에 히스토그램 매칭은 대상의 형태가 변해도 매칭을 할 수 있지만, 대상의 기하학적 정보를 잃어버리고, 단치 분포(구성비) 정보만을 가지기 때문에 엉뚱한 객체와 매칭되는 문제가 발생하기도 한다.​HOG는 Template Matching과 Histogram Mathcing의 중간 단계라고 생각하면 된다.블럭 단위로 기하학적 정보를 기억하되, 각 블록 내부에서는 히스토그램을 사용하기 때문에 Local의 변화는 잘 검출하는 특성을 갖는다.또한, Edge 방향 정보를 이용한다는 측면에서 Template Matching 방법으로도 볼 수 있다. Edge 정보를 기반으로 하기 때문에 영상의 밝기 변화, 조명 변화 등에 덜 민감한 특성을 그대로 갖는다.이러한 특성들 때문에 HOG는 객체의 윤곽선 정보를 사용하는 사람 검출, 자동차 검출, 내부 패턴이 복잡하지 않고 고유의 독특한 윤곽선 정보를 갖는 물체 식별에 적합하다.   # 사람 검출 이제 코드를 작성해서 영상 내에 사람을 검출해보자.   테스트 할 원본 이미지   # 3~4번 라인 : HOG 검출기 객체를 생성해서 사람을 검출하기 위해 set 한다.# 7번 라인 : 이미지를 불러온다.# 8번 라인 : 원본 이미지에서 사람으로 판단 되는 부분의 좌표를 저장한다.   # 10~11번 라인 : 해당 영역에 사각형을 그린다.# 13번 라인 : 결과 이미지를 저장한다.   여러 이미지에서도 시도해봤지만, 성능이 매우 우수한 것은 아니라고 생각한다.추후에 개선된 알고리즘인 YOLO를 다루겠다.  # 참고 링크 영상 feature 비교 (SIFT, HOG, Haar, Ferns, LBP, MCT)이번 글에서는 영상인식에 사용되는 대표적인 몇몇 영상 feature들을 비교 정리해 볼까 합니다. 여기서 다룰 영상 feature들은 SIFT, HOG, Haar, Ferns, LBP, MCT 입니다. 주로 HOG, Haar, LBP 등 성격이 다른 여..darkpgmr.tistory.com HOG Feature / Descriptor일반적으로 보행자 검출이나 사람의 형태에 대한 검출에 많이 사용되는 HOG Feature Histogram of Oriented Gradients 의 줄임말로 image의 지역적 gradient를 해당영상의 특징으로 사용하는 방법이다. cell 이니..jangjy.tistory.com 사람인식 HOG,  Python , OpenCVhttps://chrisjmccormick.wordpress.com/2013/05/09/hog-person-detector-tutorial/ 번역 HOG PERSON DETECTOR TUTORIAL May 9, 2013 · by Chris McCormick · in Tutorials. · 가장 인기있고 성공적인 ""사람 탐..hamait.tistory.com [Image Processing] HOG Algorithm참고자료 1 : http://sijoo.tistory.com/75 참고자료 2 : http://jangjy.tistory.com/163 참고자료 3 : http://web.mit.edu/vondrick/ihog/ (HOG Demo page) 참고자료 4 : https://en.wikipedia.org/wiki/Histogra..eehoeskrap.tistory.com ​ "
tensorflow custom object detection (android) ,https://blog.naver.com/kyz173/221787576009,20200128,"https://www.skcript.com/svr/realtime-object-and-face-detection-in-android-using-tensorflow-object-detection-api/ Realtime Object and Face Detection in Android using Tensorflow Object Detection API | SkcriptComputer Science has seen many advancements as the years go by. One such advancement is AI and in AI, Image Recognition is making waves. In keeping up with this tech, our AI team worked on a small image recognition project and find out what it is right here.www.skcript.com ​ "
Ubuntu내 Pytorch Object detection training 구현 ,https://blog.naver.com/causeimyourpilot/222621039518,20220113,2022년 1월 13일https://github.com/nicknochnack/YOLO-Drowsiness-Detection/blob/main/Drowsiness%20Detection%20Tutorial.ipynb YOLO-Drowsiness-Detection/Drowsiness Detection Tutorial.ipynb at main · nicknochnack/YOLO-Drowsiness-DetectionA lightweight walkthrough accompanying the video on Drowsiness Detection using Ultralytics YOLOv5 - YOLO-Drowsiness-Detection/Drowsiness Detection Tutorial.ipynb at main · nicknochnack/YOLO-Drowsin...github.com Ubuntu version 18.04PC GPU GT 1030 ddr4Cuda 6.1 지원 -> Nvdia 공식사이트 미지원으로 11.6버전 설치Cuda Ubuntu 설치 : 문제 발생​ 
[ML / 영상처리 / Object Detection & 이상행동_2] ,https://blog.naver.com/myj0406/222625837412,20220119,"* 행동 인식 / 탐지논문 : alpha pose, open pose, dance pose, slowfast -> 뼈기반​3. OpenPose - 단일 이미지에서 신체, 얼굴, 손 및 발의 keypoints(in total 135 keypoints)을 공동으로 detect하는 mulit-person system, MCU Studio dataset이 필요함.  Openpose의 runtime은 사람의 수와 관계없이 일정하다. 실시간 다중 2D pose 추정은 이미지나 비디오에서 사람을 이해하는 핵심 요소이다. PAF(Part Affinity Fields)라고 하는 비모수 표현을 사용해 이미지 개인과 신체 부위를 연관시키는 법을 학습한다. 이 상향식 시스템은 사람 수에 관계 없이 높은 정확도와 실시간 성능을 제공한다.  ​  [참고 링크]논문 : https://arxiv.org/pdf/1812.08008.pdfgithub : https://github.com/CMU-Perceptual-Computing-Lab/openpose GitHub - CMU-Perceptual-Computing-Lab/openpose: OpenPose: Real-time multi-person keypoint detection library for body, face, hands, and foot estimationOpenPose: Real-time multi-person keypoint detection library for body, face, hands, and foot estimation - GitHub - CMU-Perceptual-Computing-Lab/openpose: OpenPose: Real-time multi-person keypoint de...github.com https://cmu-perceptual-computing-lab.github.io/openpose/web/html/doc/index.html OpenPose: Main PageOpenPose Documentation Build Type Linux MacOS Windows Build Status OpenPose has represented the first real-time multi-person system to jointly detect human body, hand, facial, and foot keypoints (in total 135 keypoints) on single images . It is authored by Ginés Hidalgo , Zhe Cao , Tomas Simon , Shi...cmu-perceptual-computing-lab.github.io ​ "
"YOLO: Unified, Real-Time Object Detection  ",https://blog.naver.com/dongju0531hb/223006210500,20230205,"#시각지능 #객체검출 #AI #비전인식 #인공지능 #YOLO #CV #컴퓨터비전 #논문리뷰​정리한 YOLO 알고리즘​ You Only Look Once: Unified, Real-Time Object DetectionYOLO(You Only Look Once)는 이미지 내의 bounding box와 class probability를 single regresion 문제로 간주하여, 이미지를 1번 보는 것으로 object 종류 위치를 추측.www.notion.so ​ "
[YOLOv4] google colab으로 yolo4 사용해 Object Detection 물체 감지 실행하기 ,https://blog.naver.com/ska097777/222692449907,20220405,"google colab에서 yolo4을 이용해 좌측의 그림에서 객체를 탐지해 다음 결과를 얻었습니다.  YOLO(You Only Look Once)란 객체 탐지 모델 입니다. 객체 탐지란 이미지에서 관심 객체를 배경과 구분해 식별하는 자동화 기법입니다. YOLO는 기존의 다른 객체탐지 모델들보다 높은 정확도를 추구하는것이 아닌, 근접한 정확도를 가지면서 더 빠른 속도를 추구합니다. 다음의 모델이 훈련되어 있습니다.person, bicycle, car, motorbike, aeroplane, bus, train, truck, boat, traffic light, fire hydrant, stop sign, parking meter, bench, bird, cat, dog, horse, sheep, cow, elephant, bear, zebra, giraffe, backpack, umbrella, handbag, tie, suitcase, frisbee, skis, snowboard, sports ball, kite, baseball bat, baseball glove, skateboard, surfboard, tennis racket, bottle, wine glass, cup, fork, knife, spoon, bowl, banana, apple, sandwich, orange, broccoli, carrot, hot dog, pizza, donut, cake, chair, sofa, potted plant, bed, dining table, toilet, tvmonitor, laptop, mouse, remote, keyboard, cell phone, microwave, oven, toaster, sink, refrigerator, book, clock, vase, scissors, teddy bear, hair drier, toothbrush,​1. 설정gpu를 사용하므로 런타임-런타임 유형 변경에서 하드웨어 가속기를 gpu로 바꿔줍니다.  깃허브에서 darknet을 다운받습니다. !git clone https://github.com/AlexeyAB/darknet 다운받은 darknet 디렉토리의 Makefile 내용을 수정 후 실행합니다. %cd darknet!sed -i 's/OPENCV=0/OPENCV=1/' Makefile!sed -i 's/GPU=0/GPU=1/' Makefile!sed -i 's/CUDNN=0/CUDNN=1/' Makefile!sed -i 's/CUDNN_HALF=0/CUDNN_HALF=1/' Makefile!make 깃허브에서 yolov4 가중치를 다운받습니다. !wget https://github.com/AlexeyAB/darknet/releases/download/darknet_yolo_v3_optimal/yolov4.weights 사진을 띄워주는 imshow 함수, 사진을 업로드하는 upload 함수, 사진을 다운로드하는 download 함수를 선언합니다. # define helper functionsdef imShow(path):  import cv2  import matplotlib.pyplot as plt  %matplotlib inline  image = cv2.imread(path)  height, width = image.shape[:2]  resized_image = cv2.resize(image,(3*width, 3*height), interpolation = cv2.INTER_CUBIC)  fig = plt.gcf()  fig.set_size_inches(18, 10)  plt.axis(""off"")  plt.imshow(cv2.cvtColor(resized_image, cv2.COLOR_BGR2RGB))  plt.show()# use this to upload filesdef upload():  from google.colab import files  uploaded = files.upload()   for name, data in uploaded.items():    with open(name, 'wb') as f:      f.write(data)      print ('saved file', name)# use this to download a file  def download(path):  from google.colab import files  files.download(path) 2. 다크넷 폴더내 사진다크넷 디렉토리 내의 예제(/darknet/data/person.jpg)로 물체 감지를 실행해 결과를 보여줍니다. !./darknet detector test cfg/coco.data cfg/yolov4.cfg yolov4.weights data/person.jpgimShow('predictions.jpg') 3. 사진 업로드이번에는 사진을 직접 업로드해서 물체감지를 실행해보겠습니다.상위 디렉토리로 이동해 사진을 업로드하고 다시 darknet 디렉토리로 돌아오겠습니다. %cd ..upload()%cd darknet 업로드된 사진(/content/img1.jpg)에 물체 감지를 실행해 결과를 보여줍니다. !./darknet detector test cfg/coco.data cfg/yolov4.cfg yolov4.weights ../img1.jpgimShow('predictions.jpg') 4. 비디오 업로드비디오에서도 물체감지를 실행하겠습니다. 비디오를 업로드 합니다. upload() 업로드된 비디오(/content/vid.mp4)에 물체 감지를 실행합니다. !./darknet detector demo cfg/coco.data cfg/yolov4.cfg yolov4.weights -dont_show vid.mp4 -i 0 -out_filename results.avi 결과 영상을 다운받아 확인하겠습니다. download('results.avi') 트럭 버스 자동차 사람 각각에 맞게 잘 인식됨을 확인할 수 있습니다. 참고https://github.com/AlexeyAB AlexeyAB - OverviewAlexeyAB has 122 repositories available. Follow their code on GitHub.github.com 사진, 영상https://www.pexels.com/ko-kr/video/2103099/ "
yolo:real-time object detection ,https://blog.naver.com/jihoon8390/222419171317,20210703,"사람은 이미지를 보면 어디에 무엇이 있는지를 한번에 파악할 수 있다.그렇다면 ai에게 이처럼 이미지의 객체 검출을 할 수 있게 하기 위해 있는 것이 yolo다.yolo는 이미지 전체에 대해서 하나의 신경망이 한 번의 계산만으로 bounding bo와 클래스 확률을 예측한다.- bounding box란 객체의 위치를 알려주기 위해 객체의 둘레를 감싼 직사각형 박스를 말함- 클래스 확률이란 bounding box로 둘러싸인 객체가 어떤 클래스에 해당하는지에 관한 확률을 의미분류란 하나의 이미지를 보고 그것이 개인지 고양이인지 판단하는 것을 뜻한다. 하지만 객체 검출은 하나의 이미지 내에서 객체들이 어디에 위치해 있는지 판단하는 것이다.​yolo는 단일 신경망 구조이기 때문에 구성이 단순하며 빠르고 주변 정보까지 학습하며 이미지 전체를 처리하기 때문에 background error가 적다.- background error : 배경에 아무 물체가 없는데 물체가 있다고 판단하는 애러​yolo는 입력 이미지를 s*s그리드로 나눈다. 만약 어떤 객체의 중심이 특정 그리드 셀 안에 위치한다면, 그 그리드 셀이 해당 객체를 검출해야 한다.각각의 그리드 셀은 B개의 bounding box와 그에 대한 confidence score를 예측해 bounding box가 객체를 포함한다는 것을 얼마나 믿을만한지. 그리고 얼마나 정확한지를 나타낸다.confidence score란 로 정의 하는데 여기서 IOU는 객체의 실제 bounding box와 예측 bounding box의 합집합 면적 대비 교집합 면적의 비율을 뜻함만약 그리드 셀에 아무 객체가 없다면 Pr=0이고, 그러므로 confidence score도 0이다. 그리드 셀에 어떤 객체가 확실히 있다고 예측했을 때 Pr=1일 때 가장 이상적이라고 한다. 그러니 confidence score와 IOU가 같은 값이 가장 이상적인 점수이다.​각각의 bounding box는 x,y,w,h,confidence 라는 5개의 예측치로 구성되어 있다.(x,y) 좌표 쌍은 bounding box 중심의 그리드 셀 내에 상대 위치를 뜻함. 절대 위치가 아닌 그리드 셀 내의 상대 위치이므로 0~1 사이의 값을 갖는다. 만일 bounding box의 중심인 (x,y)가 정확히 그리드 셀 중앙에 위치한다면 좌표(x,y)의 값은 (0.5,0.5)이다.(w,h) 쌍은 bounding box의 상대 너비와 상대 높이를 뜻함. 이때 (w,h)는 이미지 전체의 너비와 높이를 1이라고 했을 때 bounding box의 너비와 높이가 면인지를 상대적인 값으로 나타냄. 그러므로 (w,h)의 좌표 값도 0~1 사이의 값을 갖는다.마지막으로 confidence는 위에 confidence score와 동일함​그리고 각각의 그리드 셀은 conditional class probabilities(C)를 예측한다. C는  과 같이 계산할 수 있다.이는 그리드 셀 안에 객체가 있다는 조건 하에 그 객체가 어떤 클래스 인지에 대한 조건부 확률이다. 그리드 셀에 몇 개의 bounding box가 있는지와는 무관하게 하나의 그리드 셀에는 오직 하나의 클래스에 대한 확률 값만 구한다.하나의 그리드 셀은 B개의 bounding box를 예측하는데, 클래스 확률은 B의 개수와는 무관하게 하나의 그리드 셀에서는 하나의 클래스만 예측한다.​테스트 단계에서는 C와 개별 bounding box의 confidence score를 곱해주는데, 이를 각 bounding box에 대한 class-specific confidence score(CSCS)라고 함 CSCS = C * confidence score이 값은 bounding box에 특정 클래스 객체가 나타날 확률과 예측된 bounding box가 그 클래스 객체에 얼마나 잘 들어맞는지를 나타냄  훈련 - yolo를 연구진들이 훈련시킨 방법우선 1000개의 클래스를 갖는 ImageNet 데이터 셋으로 yolo의 컨볼루션 계층을 사전훈련을 위해서 24개의 컨볼루션 계층 중 첫 20개의 컨볼루션 계층만 사용했고, 이어서 전결합 계층을 연결했습니다. 이 모델을 약 1주간 훈련시켰고 이렇게 사전 훈련된 모델은 ImageNet 2012 검증 데이터 셋에서 88%의 정확도를 보여주었습니다. YOLO 연구진은 이 모든 훈련과 추론을 위해 Darknet 프레임워크를 사용했습니다.​Darknet 프레임워크는 yolo개발자가 독자적으로 개발한 신경망 프레임워크이다. 신경망들을 학습하거나 실행할 수 있는 프레임워크로 yolo도 Darknet에서 학습된 모델 중 하나이다.​ImageNet은 분류를 위한 데이터 셋입니다. 따라서 사전 훈련된 분류 모델을 객체 검출 모델로 바꾸어야 합니다. 연구진은 사전 훈련된 20개의 컨볼루션 계층 뒤에 4개의 컨볼루션 계층 및 2개의 전결합 계층을 추가하여  성능을 향상시켰습니다. 4개의 컨볼루션 계층 및 2개의 전결합 계층을 추가할 때, 이 계층의 가중치는 임의로 초기화했습니다. 또한, 객체 검출을 위해서는 이미지 정보의 해상도가 높아야 합니다. 따라서 입력 이미지의 해상도를 224 x 224에서 448 x 448로 증가시켰습니다.  앵커 박스앵커 박스란 미리 정의된 형태를 가진 경계박스 수를 앵커박스 라고 함앵커 박스는 k - 평균 알고리즘에 의한 데이터로부터 생성되며, 데이터 세트의 객체 크기와 형태에 대한 사전 정보를 확보함 각각의 앵커는 각기 다른 크기와 형태의 객체를 탐지하도록 설계되어 있음  사진의 경우엔 한 장소에 3개의 앵커가 있는데, 이 중 붉은색 앵커 박스는 가운데 있는 사람을 탐지함. 이 알고리즘은 앵커 박스와 유사한 크기의 개체를 탐지한다는 뜻인데, 최종 예측은 앵커의 위치나 크기와는 차이가 있음. 이미지의 피쳐맵에서 확보한 최적화된 오프셋이 앵커 위치나 크기에 추가됨 그림을 통해 YOLO 알고리즘의 아키텍쳐를 설명할 수 있는데, 탐지 레이어는 많은 회귀 및 분류 최적화 도구를 포함하고 있으며 레이어의 개수는 앵커의 개수에 따라 결정됨 "
[ Jetson nano ] Real-time object detection ,https://blog.naver.com/sgkim21/221906145899,20200413,"https://news.developer.nvidia.com/realtime-object-detection-in-10-lines-of-python-on-jetson-nano/ Real-Time Object Detection in 10 Lines of Python on Jetson Nano – NVIDIA Developer News CenterReal-Time Object Detection in 10 Lines of Python on Jetson Nano January 31, 2020 Comments Share To help you get up-and-running with deep learning and inference on NVIDIA’s Jetson platform , today we are releasing a new video series named Hello AI World to help you get started.  In the first episode ...news.developer.nvidia.com ​ "
[Object Detection] YOLOv3 windows환경에서 사용하기 (CPU ver) ,https://blog.naver.com/rudals2901/222429311833,20210712,"인터넷에는 죄다 GPU 버전이고,,,, 꽤나 애먹었기때문에 하는 정리,,,,​실행환경0. Visual Studio 2019 (2015 이상으로)-> 설치할때 꼭 C++ 개발환경 꼭 깔기 (혹여 못깔았어도 Installer 켜서 VC 수정하면 됨)-> SDK 설치 (참고: https://stackoverflow.com/questions/34028259/cmake-windows-10-sdk ) cmake Windows 10 SDKtried to compile VTK using cmake on Windows 10 with Visual Studio compiler ... however, cmake says ""Could not find an appropriate version of the Windows 10 SDK installed on this machine"". Okay. So Istackoverflow.com C++ 개발환경SDK1. opencv설치  (2.4 이상) (- 3.4.9 설치)빌드해서 설치해야 함 (pip install 안됨)    1) cmake 설치 https://cmake.org/download/ Download | CMakecmake.org             windows 버전으로 설치 후 압축 풀기     2) opcncv 폴더 다운로드 https://opencv.org/releases/      Sources를 눌러서 zip 파일 다운로드      원하는 곳에 압축을 풀고 (C 바로 아래 추천 ) bulid 폴더를 만들어준다     3) Cmake 실행 후 아래와 같이 옵션 지정/폴더를 지정해주고 Configure            --> 옵션:  Visual Studio 16 2019, x64 사용     4) 빨간게 가득 떠도 마지막에 Configuring Done가 떠있으면 된것    5) 옵션 추가        이정도만 추가 했던 것 같다(world는 다들 꼭 추가하는듯)    6) 다시 configue하고 generate 하면  bulid 폴더에 파일이 잔뜩 생김 -> OpenCV.sln 파일 생성돼야 함    7) Visual Studio 2019로 OpenCV.sln 파일 열기         켰는데 VS창에 아무것도 안나온다면 ,, 솔루션 탐색기 켜졌는지 확인  ​      -> ALL_BUILD 우클릭 -> 빌드 클릭       -> 완료 되면 Release를 Debug로 바꾸고 다시 빌드      ->Release로 바꾸고 INSTULL 우클릭 -> 빌드       -> 완료 되면 다시 Debug로 빌드        설치 완료 ^_^​      컴퓨터내 환경변수를 수정하지 않고 Darknet 사용환경만 수정할 것임   2. Darknet 설정     git clone https://github.com/AlexeyAB/darknet.git    darknet -> build -> darknet 의 darknet_no_gpu 사용할거임 vcxproj 파일을 메모장에서 열어 tool 검색 후 숫자 변경VS 2019 -> 14.2ver 이기때문에 맨 위는 14.2로 나머지는 142로 변경     VS 버전확인 하는법 - VS켜고 Alt+Enter  ​  Visual Studio에서 darknet_no_gpu.sln 실행     Alt+Enter 로 속성 열기 ""모든 구성"" 클릭     1) C/C++ -> 일반 -> 추카포함 디렉터리에서 설치한 opencv 경로 지정 (사진 참고)      2) 링커 -> 일반 -> 추가 라이브러리 디렉터리에서 lib파일이 있는 경로 지정       3) 링커 -> 입력 -> 추가 종속성에서 자신이 가진 버전의 lib 추가 (내가 가진 opencv가 3.4.9여서 349임)      4) C:\opencv-3.4.9\build\install\x64\vc16\bin 의 dll파일을 복사하여 darknet -> x64폴더에 붙여넣기      5) 빌드        이와 같은 창이 나온다면 성공 이와같은 exe가 생김​  실행해보기https://pjreddie.com/media/files/yolov3.weights -> weight 파일 다운받아 weights 폴더 만들고 저장​cmd켜고 .exe 있는 폴더 가서 다음 실행 ​darknet_no_gpu.exe detector test data/coco.data cfg/yolov3.cfg weights/yolov3.weights dog.jpg ​​  참고 사이트 opcncv 설치 https://blog.naver.com/hyunmonn/221864512384 05. [3.4.9] OpenCV를 CMake로 빌드해보자.https://blog.naver.com/hyunmonn/221787038253 ※ 위의 게시글에서 한 번 자세하게 다뤘기 때문에 빠르게 ...blog.naver.com https://m.blog.naver.com/PostView.naver?isHttpsRedirect=true&blogId=shapin94&logNo=220935031079 초보 관점으로 풀어 낸 OpenCV / CMake 설치이 포스트는 전혀 전문적이지 않은 1인의 입장에서 서술되었습니다. 설치의 모든 과정을 설명하기보다 제 ...m.blog.naver.com http://wanochoi.com/?p=5260 How to build OpenCV on Windows – Wanho ChoiHow to build OpenCV on Windows Published by wano on 2020-03-09 윈도우즈 환경에서 OpenCV 개발 환경을 구성할 때 다음의 두 가지 방법이 있다. 이미 빌드(build)가 되어 있는 바이너리(binary) 파일을 찾아서 설치 소스코드(source code)를 받아서 직접 빌드(build) 1번 방법의 경우 설치가 간편하지만 빌드시 이미 고정되어 있는 여러 가지 옵션들을 변경할 수 없다는 점, 최신 업데이트가 반영되지 않을 수 있다는 점, 그리고 마지막으로 확장 모듈인 OpenCV c...wanochoi.com darknet on windows (CPU)​https://analyticsindiamag.com/guide-to-darknet-installation-on-windows-10-cpu-version/ Guide To Darknet Installation On Windows 10 – CPU VersionDarknet is a framework for real-time object detection. Darknet is an open-source neural network framework and is written in C and CUDA. A major reason it is used widely is that it is highly accurate and very fast. The reason for Darknet to be fast is because it is written in C and CUDA. Darknet…analyticsindiamag.com https://bluemoon-1st.tistory.com/5 darknet windows 설치안녕하세요!! 이번에는 제가 darknet을 통한 yolo 알고리즘으로 Deep Learning을 공부해보려고 하기 때문에 오늘은 간단한 설치 방법에 대해서 포스팅 해보겠습니다!! (darknet 소스코드를 다운 받아서 빌드 하는..bluemoon-1st.tistory.com https://kd1658.tistory.com/23 [Darknet yolo]yolo를 이용한 물체감지(Object Detection) 튜토리얼 윈도우 환경에서 YOLO CNN을 이용한 물체감지(Object Detection) 튜토리얼 darknet과 yolo 관련자료 링크 -> https://pjreddie.com/darknet/yolo/ 알파고를 필두로 인공지능이 핫해지면서 딥러닝이다 4차산업..kd1658.tistory.com ​ "
[딥러닝] OpenCV DNN Faster RCNN Object Detection Video Inference ,https://blog.naver.com/wldlwn12/222950800503,20221209,"video_input_path = '/content/data/Jonh_Wick_small.mp4'cap = cv2.VideoCapture(video_input_path)frame_cnt = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))print('총 Frame 갯수:', frame_cnt) 총 Frame 갯수: 58 video_input_path = '/content/data/Jonh_Wick_small.mp4'video_output_path = './data/John_Wick_small_cv01.mp4'cap = cv2.VideoCapture(video_input_path)codec = cv2.VideoWriter_fourcc(*'XVID')vid_size = (round(cap.get(cv2.CAP_PROP_FRAME_WIDTH)),round(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))) vid_fps = cap.get(cv2.CAP_PROP_FPS )    vid_writer = cv2.VideoWriter(video_output_path, codec, vid_fps, vid_size) frame_cnt = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))print('총 Frame 갯수:', frame_cnt) 총 Frame 갯수: 58 # bounding box의 테두리와 caption 글자색 지정green_color=(0, 255, 0)red_color=(0, 0, 255)while True:    hasFrame, img_frame = cap.read()    if not hasFrame:        print('더 이상 처리할 frame이 없습니다.')        break    rows = img_frame.shape[0]    cols = img_frame.shape[1]    # 원본 이미지 배열 BGR을 RGB로 변환하여 배열 입력    cv_net.setInput(cv2.dnn.blobFromImage(img_frame,  swapRB=True, crop=False))        start= time.time()    # Object Detection 수행하여 결과를 cv_out으로 반환     cv_out = cv_net.forward()    frame_index = 0    # detected 된 object들을 iteration 하면서 정보 추출    for detection in cv_out[0,0,:,:]:        score = float(detection[2])        class_id = int(detection[1])        # detected된 object들의 score가 0.5 이상만 추출        if score > 0.5:            # detected된 object들은 scale된 기준으로 예측되었으므로 다시 원본 이미지 비율로 계산            left = detection[3] * cols            top = detection[4] * rows            right = detection[5] * cols            bottom = detection[6] * rows            # labels_to_names_0딕셔너리로 class_id값을 클래스명으로 변경.            caption = ""{}: {:.4f}"".format(labels_to_names_0[class_id], score)            #print(class_id, caption)            #cv2.rectangle()은 인자로 들어온 draw_img에 사각형을 그림. 위치 인자는 반드시 정수형.            cv2.rectangle(img_frame, (int(left), int(top)), (int(right), int(bottom)), color=green_color, thickness=2)            cv2.putText(img_frame, caption, (int(left), int(top - 5)), cv2.FONT_HERSHEY_SIMPLEX, 0.5, red_color, 1)    print('Detection 수행 시간:', round(time.time()-start, 2),'초')    vid_writer.write(img_frame)# end of while loopvid_writer.release()cap.release()    Detection 수행 시간: 3.55 초Detection 수행 시간: 4.23 초Detection 수행 시간: 3.83 초Detection 수행 시간: 3.5 초Detection 수행 시간: 3.5 초Detection 수행 시간: 3.44 초Detection 수행 시간: 3.45 초Detection 수행 시간: 3.42 초Detection 수행 시간: 3.45 초Detection 수행 시간: 3.47 초Detection 수행 시간: 3.43 초Detection 수행 시간: 3.43 초Detection 수행 시간: 3.51 초Detection 수행 시간: 3.47 초Detection 수행 시간: 3.46 초Detection 수행 시간: 3.51 초Detection 수행 시간: 3.49 초Detection 수행 시간: 3.43 초Detection 수행 시간: 3.44 초Detection 수행 시간: 3.43 초Detection 수행 시간: 3.49 초Detection 수행 시간: 3.44 초Detection 수행 시간: 3.48 초Detection 수행 시간: 3.46 초Detection 수행 시간: 3.47 초Detection 수행 시간: 3.44 초Detection 수행 시간: 3.45 초Detection 수행 시간: 3.47 초Detection 수행 시간: 3.46 초Detection 수행 시간: 3.49 초Detection 수행 시간: 3.45 초Detection 수행 시간: 3.44 초Detection 수행 시간: 3.46 초Detection 수행 시간: 3.44 초Detection 수행 시간: 3.47 초Detection 수행 시간: 3.45 초Detection 수행 시간: 3.38 초Detection 수행 시간: 3.46 초Detection 수행 시간: 3.43 초Detection 수행 시간: 3.47 초Detection 수행 시간: 3.45 초Detection 수행 시간: 3.5 초Detection 수행 시간: 4.37 초Detection 수행 시간: 3.86 초Detection 수행 시간: 3.46 초Detection 수행 시간: 3.42 초Detection 수행 시간: 3.46 초Detection 수행 시간: 3.49 초Detection 수행 시간: 3.4 초Detection 수행 시간: 3.46 초Detection 수행 시간: 3.48 초Detection 수행 시간: 3.46 초Detection 수행 시간: 3.45 초Detection 수행 시간: 3.42 초Detection 수행 시간: 3.47 초Detection 수행 시간: 3.46 초Detection 수행 시간: 3.47 초더 이상 처리할 frame이 없습니다.  2초도 안되는 영상인데 코드 돌리는데 5분넘게 걸린거같다왜 사람들이 코드 돌려놓고 어디 갔다오는지 이제야 알겠다.내용이 상당히 어렵긴한데 꽤나 재밌는듯?ㅋㅋㅎ 무지성복붙 드가자  ​ "
ModuleNotFoundError: No module named 'object_detection' ,https://blog.naver.com/hyuckang15/221706790668,20191113,how to install tensorflow object detection​https://musma.github.io/2019/02/15/tensorflow-on-windows.html Windows에서 Tensorflow Object Detection API 설치하기!!Windows에서 Tensorflow Object Detection API 설치하기!musma.github.io ​ 
"객체검출 (Object Detection), 속성부여, YOLO ",https://blog.naver.com/datahunt_official/222851731849,20220820,"​ AI 테크의 ‘논문리뷰’는 주목할 만한 AI 관련 논문을 선정, 리뷰하는 시리즈물입니다.리뷰되는 내용은 데이터헌트가 실제 활용하는 기술과 무관할 수 있습니다.​객체검출은 이미지 내 객체의 위치를 특정(Localization)하고 그 속성(Classification)을 부여하는 작업을 말합니다. 데이터헌트에서 진행되는 다양한 프로젝트는 객체의 일반적인 속성을 분류하는 것 뿐만 아니라 이미지 내의 위치까지 제공할 필요가 있고, 상황에 따라서 그 속성이 ‘동물의 종류’와 같은 한 개의 속성이 아니라, ‘동물의 종류’, ‘색상’, ‘자세’ 등 다양한 속성을 부여해야 하는 경우도 있습니다.하나의 객체에 다양한 속성을 부여하기도 합니다. 예를 들어 사진에 부여되는 객체의 속성은 ‘강아지’ 인 것 뿐만 아니라 동물의 위치가 ‘중앙’, 동물의 자세는 ‘앉은자세’, 색상은 ‘흰색’ 처럼 다양한 속성을 부여하고 비교합니다.구체적인 데이터 라벨링으로 위의 사진처럼 앉은자세의 강아지 인형과의 공통적인 속성을 캐치하고 부여합니다. 다양한 객체 검출 레이블링 작업은 요즘 큰 관심을 받는 자율주행 뿐 아니라, 패션, 건설업, 의료 등 다양한 분야에서 인공지능을 도입하기 위해 필요한 과정입니다.​​​  YOLO를 AI Tech의 첫 포스팅으로 정한 이유​데이터헌트가 가지는 가장 큰 장점 중 하나는 데이터 가공 작업이 진행될 때 작업자들의 작업 능률 향상을 위해 AI 모델을 활용하고 있다는 것입니다.그렇기 때문에 작업자들의 능률향상을 지원하는 AI 모델에 대해 많은 고민이 필요한데, 빠르게 레이블을 전달하더라도 정확하지 않으면 차후에 추가적인 수정이 필요해지고, 최대한 정확한 레이블이더라도 전달에 너무 오랜 시간이 걸린다면 작업 효율에 큰 도움이 되지 못합니다.따라서 많은 객체 검출 모델 중에서 속도와 정확도의 균형이 잘 잡힌 모델을 고민해야했고, 그 결과 YOLO를 눈여겨보게 되었습니다.​ ​YOLO가 다른 모델에 비해 속도와 정확도 모두 좋은 결과를 냈을까요? 현재 YOLO는 v1 부터 v5 모델까지 많은 개선이 이루어졌고, 현재도 개선 중 입니다. 이번에는 그 시작이 되는 YOLO v1에서부터 이후의 시리즈까지 관통하는 핵심 개념을 다른 모델과의 차이점, 장단점과 함께 리뷰하겠습니다.​​​  One-stage Detector​Two-stage Detector로 분류되는 R-CNN 계열의 모든 모델은 이미지의 많은 영역에 걸쳐 후보 영역군을 만들고 각 영역에 대해 Objectness와 객체 영역, 그리고 클래스를 예측하는 작업을 수행하게 됩니다.장점은 후보 영역군을 만드는 일부터 이후 과정이 많은 연산량을 필요로 하고 직렬적으로 수행되기 때문에 보다 정확합니다. 하지만, 그만큼 느리게 동작하는 단점도 있습니다.YOLO에서는 위의 속도의 문제를 개선하기 위해 One-stage Detector의 구조를 제안했습니다. Backbone을 통해 나온 feature map이 곧 원본 이미지를 격자(grid)로 잘라 놓은 역할을 하기 때문에, 후보 영역군을 생성할 필요도 없이 각 격자에서 물체가 유무를 포함한 나머지 일들을 수행할 수 있습니다.​ ​이것은 네트워크 전체에 걸친 Convolution 연산이 Spatial-invariant 하기 때문에 가능한 일이라고 생각할 수 있습니다.다시 말해, 연산을 해도 이미지 내 픽셀의 상대적 위치가 변하지 않는다는 특징으로 인해 내 양 옆자리에 지수, 재윤이라면 사무실을 옮기고 몇번의 이사를 가도 양옆에 지수와 재윤이가 앉아 있는것은 변하지 않습니다.​​​  Predict at Once​위의 그림에서 알 수 있듯, YOLO에서는 마지막 layer를 통해 한 번에 모든 정보를 추론하게 되는데 그 내용물은 아래의 그림과 같습니다.​ ​각 격자에서 서로 다른 크기와 비율을 가진 영역을 사전에 정의 (이것을 Anchor라고 명칭), 그 Anchor의 개수만큼 영역의 위치와 크기(x, y, w, h) 그리고 objectness를 추론합니다.그리고 그와 별개로 해당 Anchor가 포함하는 영역의 객체의 클래스를 분류할 수 있게 설계합니다. 이러한 구조를 통해 YOLO는 원하는 정보(영역 위치, Objectness, 클래스)를 한 번에 얻어내도록 구성되었습니다.Two-stage 구조에서는 각 영역 후보마다 이 과정을 각자 따로 계산하는 반면, YOLO에서는 추론해야 할 결과의 수가 영역의 수에서 격자의 수로 변하게 됩니다. 그리고 보통 영역의 수는 1k 단위로 설계되는 반면 격자의 수는 10, 100개의 단위로 만들어지기 때문에 연산 속도를 크게 줄일 수 있습니다.​​​  YOLO 모델의 장 단점​여기까지 YOLO 모델을 간단히 살펴봤습니다. 언급했듯 위의 내용은 앞으로의 시리즈에서도 큰 변화 없이 공통되는 내용이므로 앞서 정리한 YOLO 모델의 특징, 장단점을 짚어봐야 할 필요가 있습니다.​  장점​Two-stage에 비해 빠르다.배경에 대한 positive error가 낮다.  단점​작은 물체에 대한 검출 성능이 낮다.겹쳐있는 물체에 대한 성능은 다소 떨어집니다. ​YOLO는 결론적으로 정확도를 조금 낮추는 대신 빠른 추론을 얻어낸 모델입니다. 하지만 소폭의 정확도 감소를 충분히 상쇄하고도 남을 빠른 속도를 얻어냅니다. 이미 다양한 실서비스에서 활용되고 있으며 이후 YOLO 시리즈에서는 위의 단점을 개선하기 위한 여러 구조를 제안하여 발표되었으니, 이후 포스트에서 더 다뤄보겠습니다. "
[object detection] IoU & NMS ,https://blog.naver.com/lsj952005/222603694898,20211226,<IoU(Intersection over Union)> https://blog.kakaocdn.net/dn/bdqz5d/btqNMhGPjvM/u16H7ABNO6o4da0FgfIlN1/img.png- IoU는 예측한 bounding box와 실제 물체의 bounding box가 겹치는 비율을 의미하며 성능평가에 사용됩니다.(교집합 / 합집합)​​​<NMS(Non Maximum Suppression)> - NMS는 동일한 객체에 대해서 IoU가 높은 순서대로 정렬하여 Socre가 가장 높은 bounding box를 기준으로 threshold를 설정하여 중복된 bounding box 하나로 합치는 방법입니다. 
TensorFlow C++ API to run a object detection model ,https://blog.naver.com/slumdunk/221422681138,20181219,"TensorFlow C++ API to run a object detection model – zong fan – MediumIn recently day, I am working on deploying deep learning model trained in TensorFlow framework on device. It’s quite easy to write custom…medium.com ​https://medium.com/@fanzongshaoxing/tensorflow-c-api-to-run-a-object-detection-model-4d5928893b02 "
Tflite object detection inference ,https://blog.naver.com/qpfjf56/222591848282,20211210,"모델을 inference하기 위해서는 정확한 타입의 input이 들어가야한다.​import tensorflow as tfinterpreter = tf.lite.Interpreter(model_path=""model.tflite"")interpreter.allocate_tensors()​input_details = interpreter.get_input_details()output_details = interpreter.get_output_details()​print(""input_details :: ""+str(input_details))print(""output_details :: ""+str(output_details))​Output :: input_details :: [{'name': 'serving_default_images:0', 'index': 0, 'shape': array([ 1, 320, 320, 3], dtype=int32), 'shape_signature': array([ 1, 320, 320, 3], dtype=int32), 'dtype': <class 'numpy.uint8'>, 'quantization': (0.0078125, 127), 'quantization_parameters': {'scales': array([0.0078125], dtype=float32), 'zero_points': array([127], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}] output_details :: [{'name': 'StatefulPartitionedCall:1', 'index': 600, 'shape': array([ 1, 25], dtype=int32), 'shape_signature': array([ 1, 25], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}, {'name': 'StatefulPartitionedCall:3', 'index': 598, 'shape': array([ 1, 25, 4], dtype=int32), 'shape_signature': array([ 1, 25, 4], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}, {'name': 'StatefulPartitionedCall:0', 'index': 601, 'shape': array([1], dtype=int32), 'shape_signature': array([1], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}, {'name': 'StatefulPartitionedCall:2', 'index': 599, 'shape': array([ 1, 25], dtype=int32), 'shape_signature': array([ 1, 25], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]​우리가 수행하려는것은 이미지파일 하나를 넣고 inference를 하는것인데 input 이미지의 포멧을 맞춰주어야한다. input_details에서 shape 필드가 바로 그것이다. [1, 320, 320, 3], dtype=int32 로 이미지를 재포맷한뒤 모델을 실행하면 결과가 잘 나오게된다.​import cv2img = cv2.imread('input.jpg')img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)img_rgb = cv2.resize(img_rgb, (320, 320), cv2.INTER_AREA)img_rgb = img_rgb.reshape([1, 320, 320, 3])interpreter.set_tensor(input_details[0]['index'], img_rgb)interpreter.invoke()de_boxes = interpreter.get_tensor(output_details[0]['index'])[0]det_classes = interpreter.get_tensor(output_details[1]['index'])[0]det_scores = interpreter.get_tensor(output_details[2]['index'])[0]num_det = interpreter.get_tensor(output_details[3]['index'])[0]​print(str(de_boxes)+""\n""+str(det_classes)+""\n""+str(det_scores)+""\n""+str(num_det))​Output :: [0.09765625 0.05859375 0.0234375 0.01953125 0.01953125 0.01171875 0.01171875 0.01171875 0.0078125 0.0078125 0.0078125 0.0078125 0.0078125 0.0078125 0.0078125 0.0078125 0.0078125 0.0078125 0.00390625 0.00390625 0.00390625 0.00390625 0.00390625 0.00390625 0.00390625] [[ 0.46860185 0.4065891 0.49953797 0.49585187] [ 0.45536545 0.40145117 0.4856417 0.5052549 ] [ 0.45526388 0.39154527 0.511587 0.50382733] [ 0.43902603 0.38289565 0.49173614 0.505489 ] [ 0.83771944 0.3766753 0.9732156 0.49319112] [ 0.8388392 0.39217323 0.9204619 0.4874003 ] [ 0.83694255 0.10245222 0.9534582 0.23794848] [ 0.8955041 0.63559276 0.9615103 0.7028194 ] [ 0.42581406 0.36669207 0.5064459 0.4809075 ] [ 0.4543099 0.42902175 0.49813107 0.48683122] [ 0.446229 0.43375212 0.52514106 0.56946796] [ 0.4971841 0.59357303 0.57781595 0.7322467 ] [ 0.86978483 0.62604994 0.93722963 0.7009285 ] [ 0.88816875 0.56697947 0.94597834 0.6985652 ] [ 0.935252 0.54932964 0.9943214 0.6753621 ] [ 0.9867666 0.09677655 1.0019538 0.20992953] [ 0.43682566 0.36580628 0.52232134 0.5482226 ] [ 0.38318878 0.22545949 0.79975086 0.82448864] [-0.01010776 0.9354235 0.04375112 1.028052 ] [-0.01961397 0.9661307 0.04824397 1.0524291 ] [ 0.13004187 0.65161246 0.17011234 0.73710805] [ 0.29461017 0.34742913 0.3578308 0.42071065] [ 0.32574064 0.383465 0.37670034 0.44382173] [ 0.34074363 0.4201305 0.3845648 0.4731635 ] [ 0.4643939 0.40743214 0.4883554 0.4404353 ]] 25.0 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]​​ "
tensorflow object detection 안드로이드 연동 ,https://blog.naver.com/kkang9901/221809269124,20200214,"우선 모델은 tensorflow에서 지원하는 모델을 사용했다. 이번 기회에 안드로이드 스튜디오를 처음 설치해봐서, 필자도 안드로이드 스튜디오를 잘 모른다는 점 양해부탁드립니다...​하드웨어로서는​mac os 가장 최신버전을 사용했고, 휴대폰은 갤럭시 S8+입니다.​먼저 안드로이드 스튜디오를 설치했다.​https://developer.android.com/studio?hl=ko Download Android Studio and SDK tools  |  Android Developers<!-- hide description -->developer.android.com 그리고​터미널에서 아래 명령어를 실행하자. 적당한 폴더를 하나 만들어 git에서 다운로드 받자. git clone https://github.com/tensorflow/tensorflow ​​이제 안드로이드 스튜디오를 열어, tensorflow/examples/android 폴더를 열어보자. 처음 프로젝트를 열어보면 Gradle,  SDK같은걸 업데이트 하라고 하는데, 어쩔수 있나, 따라서 설치해보자.​그 이후 가장 먼저 할 일은 build.gradle 파일의 nativeBuildSystem 변수 값을 'none'로 바꾸자.​ def nativeBuildSystem = 'none' 또, 해당 데모 앱에서는 Class ~ , Style ~ , Speech ~, DetectorActivity 총 4가지 기능이 들어있는데, 우리가 쓸 것은 Detector 이므로, 나머지 부분은 전부 주석처리해서 비활성화 하자. 써보고 싶으면 이 과정은 생략해도 좋다.  해당 파일은 AndroidMainfest.xml 이다. 아래 내용을 붙여넣기 해도 된다. ( 전부 붙여넣기 하면 안된다. 시작하는 줄을 대조해서 맞추길 바란다. ) <application android:allowBackup=""true""    android:debuggable=""true""    android:label=""@string/app_name""    android:icon=""@drawable/ic_launcher""    android:theme=""@style/MaterialTheme"">    <activity android:name=""org.tensorflow.demo.DetectorActivity""        android:screenOrientation=""portrait""        android:label=""@string/activity_name_detection"">        <intent-filter>            <action android:name=""android.intent.action.MAIN"" />            <category android:name=""android.intent.category.LAUNCHER"" />            <category android:name=""android.intent.category.LEANBACK_LAUNCHER"" />        </intent-filter>    </activity>    <!--    <activity android:name=""org.tensorflow.demo.StylizeActivity""        android:screenOrientation=""portrait""        android:label=""@string/activity_name_stylize"">        <intent-filter>            <action android:name=""android.intent.action.MAIN"" />            <category android:name=""android.intent.category.LAUNCHER"" />            <category android:name=""android.intent.category.LEANBACK_LAUNCHER"" />        </intent-filter>    </activity>    <activity android:name=""org.tensorflow.demo.ClassifierActivity""              android:screenOrientation=""portrait""              android:label=""@string/activity_name_classification"">        <intent-filter>            <action android:name=""android.intent.action.MAIN"" />            <category android:name=""android.intent.category.LAUNCHER"" />            <category android:name=""android.intent.category.LEANBACK_LAUNCHER"" />        </intent-filter>    </activity>    <activity android:name=""org.tensorflow.demo.SpeechActivity""        android:screenOrientation=""portrait""        android:label=""@string/activity_name_speech"">        <intent-filter>            <action android:name=""android.intent.action.MAIN"" />            <category android:name=""android.intent.category.LAUNCHER"" />            <category android:name=""android.intent.category.LEANBACK_LAUNCHER"" />        </intent-filter>    </activity>    --></application> 이제 안드로이드 스튜디오를 Run할건데, 휴대폰과 맥북을 연결하자. 이때 아래 사진처럼 휴대폰과 맥북을 연결하면, 자동적으로 타겟이 잡힌다.  맨위 중간 부분을 보면 휴대폰이 연결된걸 볼 수 있다. 이게 안된다면 아래 경로를 따라 휴대폰을 조금 설정해보자.  휴대폰 설정화면을 들어가보자. 개발자 옵션이 있다면 아래 과정은 생략하자. 개발자 옵션이 없다면 바로 아래 사진을 보자.  휴대전화 정보를 누르면 소프트웨어 정보 탭이 뜬다. 들어가자.  들어오면 빌드번호 탭이 나오는데, 아무런 반응이 없더라도 신나게 누르자. 그러면 개발자옵션이 활성화 된다.  개발자옵션을 들어오고, 쭉 내려오면 USB 디버깅이 꺼져있을 것이다. 활성 시키고 다시 연결하면 맥북이 휴대폰을 인식한다. 그러고 나서 해당 휴대폰이 연결된걸 확인하자.  휴대폰 이름 바로 오른쪽 초록색 삼각형을 누르자. 아니면 run을 해도 좋다. 정상적으로 실행이 되었다면, 휴대폰에 TF Detect가 설치가 되었다. 한번 켜서 인식시켜보자.​     책상이 좀 더럽긴하지만. 잘 인식한다.. ​​​다음 내용에서는 이전 내용에서 사용자가 직접 생성한 모델을 TF Detect에 심어서, 잘 인식하는지 테스트 해보려고 한다. "
[Object Detection] YOLOv3 커스텀 데이터 사용하기 ,https://blog.naver.com/rudals2901/222425780667,20210709,"YOLOv3 custom data 학습하기 ​실험 환경 세팅 (CUDA와 cudnn안맞추면 make 안됨 호환되는 버전 잘 보고 깔것) Ubuntu 18.04CUDA 10.1cudnn 7.6.5opcnCV 3.4.2 (빌드하여 설치, pip로 깔면 실행 안됨) - https://sunkyoo.github.io/opencv4cvml/OpenCV4Linux.html Ubuntu 18.04에서 OpenCV 4.0.0 설치하기예제 소스 코드는 아래 링크를 참고하세요sunkyoo.github.io -> 링크 참고하여 해당 부분만 이걸로 대체​wget -O opencv-3.4.2.zip https://github.com/opencv/opencv/archive/3.4.2.zipwget -O opencv_contrib-3.4.2.zip https://github.com/opencv/opencv_contrib/archive/3.4.2.zipcmake \\ -D CMAKE_BUILD_TYPE=Release \\ -D CMAKE_INSTALL_PREFIX=/usr/local \\ -D BUILD_WITH_DEBUG_INFO=OFF \\ -D BUILD_EXAMPLES=ON \\ -D BUILD_opencv_python3=ON \\ -D INSTALL_PYTHON_EXAMPLES=ON \\ -D OPENCV_ENABLE_NONFREE=ON \\ -D OPENCV_EXTRA_MODULES_PATH=../opencv_contrib-3.4.2/modules \\ -D OPENCV_GENERATE_PKGCONFIG=ON \\ -D WITH_TBB=ON \\ ../opencv-3.4.2/ ​  Darknet git clone $ git clone https://github.com/pjreddie/darknet.git﻿ -> GPU, openCV 사용시 makefile 켜서 GPU =1 , OPENCV=1 으로 수정​이후 cmd 창에서  $ make 오류 없이 구동된다면 성공​  Custom Data 준비​1. input data 만들기 yolo 학습에 필요한 데이터1) image, annotation.txt2) custom.data3) custom.names4) custom_yolov3.cfg5) train.txt, valid.txt ​1) image, annotation.txt- yolo 에서의 포맷은 클래스 번호와 전체 영상 크기에 대한 center x, center y, w, h 비율 값으로 구성됨- 사진 한장당 하나의 annotation file이 존재 (txt 확장자)예) 클래스번호 centered x centered y w h46 0.446 0.557 0.465 0.209​여러장인경우0 0.559 0.336 0.047 0.1941 0.362 0.25 0.04 0.0942 0.286 0.221 0.021 0.092​만약 custom data의 annotation이 yolo에 맞는 annotation이 아니라면 변환하는 과정이 필요​Custom data annotation이  COCO or VOC or UDACITY or KITTI 2D 형식이라면 -> https://github.com/ssaru/convert2Yolo 사용하여 변환​그냥 라벨과 bbox 좌표만 있다면 위 깃허브의 계산식을 이용하여 새로운 annotation file 생성-> 아래 함수를 수정하여 자신의 데이터에 맞게 사용하면 됨 ​image_w, image_h -> detecting 할 물체가 포함된 이미지의 width, weight ​return이 이미지 비율에 맞게 변환된 centered x, centered y, w, h 이기만 하면 된다  def convert (image_w, image_h, xmin, ymin, w, h):    xmax = xmin + w    ymax = ymin + h        dw = 1. / image_w    dh = 1. / image_h    # (xmin + xmax / 2)    x = (xmin + xmax) / 2.0    # (ymin + ymax / 2)    y = (ymin + ymax) / 2.0    x = x * dw    w = w * dw    y = y * dh    h = h * dh    return (round(x, 3), round(y, 3), round(w, 3), round(h, 3))image_w = contents['Image_Width']image_h = contents['Image_Height']x,y,w,h = convert (image_w, image_h, xmin, ymin, w, h)print (x,y,w,h) image파일과 annotation file은 같은 폴더안에 넣고 파일명은 같되 확장자만 다르게 저장한다​2) custom.data클래스 개수train data경로valid data경로 names file 경로backup 경로 (학습 중 weights 파일이 저장되는 경로) ​3) custom.names클래스 명  4) custom_yolov3.cfg darknet/cfg의 yolov3.cfg를 복사하여 custom_yolov3.cfg 파일 생성기본 yolov3.cfg을 열면 다음과 같다 여기서 batch, width, height 등을 수정해도 되고 (out of memory 난다면 여기를 줄이면 됨)max_batches = 보통 (class 수 * 2000) + 200  으로 설정steps = max_batches 사이즈(200을 더하지 않은..)의 80% 와 90%를 설정​이외 여러가지 하이퍼 파라미터 설정을 본인 판단하에 수정하면 된다 ​각 하이퍼 파라미터 의미는 아래 링크를 참고  https://eehoeskrap.tistory.com/370  [Object Detection] darknet custom 학습하기darknet 학습을 위해 이전에 처리해야할 과정들은 다음 포스팅을 참고 [Object Detection] darknet custom 학습 준비하기 https://eehoeskrap.tistory.com/367 [Object Detection] Darknet 학습 준비하기 환경 Ubu..eehoeskrap.tistory.com 또,  yolo를 검색하여 총 3부분을 수정해줘야 한다  fillters - (class 개수 + 5) *3 classes - class 개수이게 3개가 있다 모두 수정 해야 함​예) class가 1개인 경우  [convolutional]size=1stride=1pad=1filters=18activation=linear[yolo]mask = 6,7,8anchors = 10,13,  16,30,  33,23,  30,61,  62,45,  59,119,  116,90,  156,198,  373,326classes= 1num=9jitter=.3ignore_thresh = .7truth_thresh = 1random=1 5) train.txt, valid.txttrain, valid image와 annotation.txt가 있는 폴더 경로  train image경로 - valid 도 동일하게 설정train, valid txt 만드는 코드-> train image를 8:2로 쪼개서 valid txt만듦  import os## train.txt /valid.txt  만들기 ## train-image 내에서 쪼개서 사용path = 'dataset/train_image/'file_list = os.listdir(path)f = open('train.txt', 'w')for train in file_list[:int(len(file_list)*0.8)]:    f.write(path + train + '\n')f.close()fv = open('valid.txt', 'w')for val in file_list[int(len(file_list)*0.8):]:    fv.write(path + val + '\n')fv.close() ​이렇게 만든 파일들을 custom 폴더에 담아 놓는다 (폴더명 아거 아니어도 됨) yolov3-tiny 모델도 학습에 사용하려면 위와 같은 과정으로 수정하면 됩니다​​  학습하기 ./darknet detector train .data파일위치 .cfg파일위치 weight파일위치 ./darknet detector train custom/custom.data custom/custom_yolov3.cfg darknet53.conv.74 | tee backup/train.log log를 보기 위해 tee backup/train.log 줄 추가 backup 폴더에 log파일이 생성됨​tiny 학습을 원한다면? -> yolov3 tiny weight 파일을 다운받고 yolov3 tiny cfg를 수정하여 이 두가지를 학습에 사용해야 함 weight파일과 cfg의 짝을 맞춰서 학습하는것이 중요학습시 epoch 1000이하까지는 100번에 한번씩 weight파일이 저장되지만 이를 수정하고 싶다면 darknet/examples 폴더 안에 있는 detector.c 를 수정138번째줄의 if문을 수정하면 됨 c언어로 작성된 코드 (확장자가 c인것)는 수정 후 make를 다시 해주어야 다음 실행시 수정한 내용을 적용 할 수 있다 ​다른 pretrained 모델을 사용하고싶다면 해당 weight파일과 cfg파일을 준비하고 다음 코드를 실행  ./darknet partial custom/yolov3-tiny.cfg custom/yolov3-tiny.weights yolov3-tiny.conv.15 15 위 코드는 yolov3 tiny 모델을 사용할때의 예시이며 tiny모델을 중간에서 가져와 아래부분은 나의 데이터로만 학습하겠다는의미이다 conv.?? 숫자는 어떻게 정하는가?-> 터미널에 summry되는 model 구조에서 yolo 윗부분 conv 번호를 하나 따오면 된다내 데이터가 충분히 많다면 위쪽 레이어에서 따와도 되지만 웬만하면 아래단 레이어에서 따오는걸 추천한다   테스트하기./darknet detector test .data파일위치 .cfg파일위치 weight파일위치 test 이미지  ./darknet detector test custom/custom.data custom/custom_yolov3.cfg custom_yolov3_100.weights test.jpg   이외 여러가지 함수 해석참고 사이트: https://dhhwang89.tistory.com/118?category=733930  [Object Detection / YOLO DARKNET] object detection code review :: image structure - [6][Object Detection / YOLO DARKNET] object detection code review :: read_data_cfg -[1] [Object Detection / YOLO DARKNET] object detection code review :: read_data_cfg -[2] [Object Detection / YOLO DAR..dhhwang89.tistory.com ​****./src/detector.c def test_detector printf(""%s: Predicted in %lf milli-seconds.\n"", input, ((double)get_time_point() - time) / 1000);​-> data/nomask.jpeg: Predicted in 435.908000 milli-seconds. 해당 라인 출력부​​****./src/image.c def draw_detections_v3 printf(""%s: %.0f%%"", names[best_class], selected_detections[i].det.prob[best_class] * 100);-> word 100%실행시 -ext_output 추가하면 좌표 출력됨 // ext_output = 실행 옵션​c언어에서 문자열 비교할때 -> strcmp(,)사용- strcmp()함수는 비교하는 대상이 같을 때 0을 리턴하는 함수if (!strcmp(names[best_class],""without_mask"")){ 추가하여 mask가 없을때만 좌표 출력하게 함​**사각형 그리는 함수 draw_box_width (3채널) - draw_boxdraw_box_width_bw (1채널) - draw_box_bw​**글씨 출력하는 함수 draw_weighted_label - get pixel, set pixel​*get pixel - image 구조체에서 특정 위치의 pixel 값을 반환해주는 함수입니다.1. 파라미터로 받은 x,y,c의 값이 유효한 값인지 검사합니다.2. image 구조체 m의 pixel들에서 원하는 위치의 pixel값을 반환합니다.​*set pixel - image 구조체의 pixel 데이터의 값을 바꿔주는 함수입니다.1. 파라미터로 받은 x,y,z의 값이 유효한지 유효성검사를 합니다.2. image 구조체 m의 특정 위치의 pixel 데이터값을 파라미터로 받은 val값으로 변경합니다.​C언어 파일을 수정한다면 꼭 make 하고 사용하기 "
[Object Detection] ,https://blog.naver.com/cogito21n/222310181581,20210414,"1-Stage, 2-Stage DetectorsObjects Detection​ "
Object Detection 모델의 Training 유의사항 ,https://blog.naver.com/doctor_song/222702833953,20220417,"· img 개수 memory 사용량 대부분 좌우​· <Keras> fit_generator()를 이용한 학습  - ex> img : 1000장, Batch_size : 10(10장의 img)를 array로 변환 후 학습 이후 100회 반복(Steps_Per_Epoch)  - Python Generator 이용( yield로 값을 순차적으로 반환 → memory 절약)  - Batch_size가 memory 사용량의 많은 부분 좌우    - Batch_size↑ ~ 무조건 수행속도↑- x    - 현재 GPU 서버는 CPU 4core, GPU P100. Batch Size는 4~8이 적합 "
[딥러닝] openCV Faster RCNN Object Detection ,https://blog.naver.com/wldlwn12/222950781440,20221209,"​​ import timedef get_detected_img(cv_net, img_array, score_threshold, use_copied_array=True, is_print=True):        rows = img_array.shape[0]    cols = img_array.shape[1]        draw_img = None    if use_copied_array:        draw_img = img_array.copy()    else:        draw_img = img_array        cv_net.setInput(cv2.dnn.blobFromImage(img_array, swapRB=True, crop=False))        start = time.time()    cv_out = cv_net.forward()        green_color=(0, 255, 0)    red_color=(0, 0, 255)    # detected 된 object들을 iteration 하면서 정보 추출    for detection in cv_out[0,0,:,:]:        score = float(detection[2])        class_id = int(detection[1])        # detected된 object들의 score가 함수 인자로 들어온 score_threshold 이상만 추출        if score > score_threshold:            # detected된 object들은 scale된 기준으로 예측되었으므로 다시 원본 이미지 비율로 계산            left = detection[3] * cols            top = detection[4] * rows            right = detection[5] * cols            bottom = detection[6] * rows            # labels_to_names 딕셔너리로 class_id값을 클래스명으로 변경. opencv에서는 class_id + 1로 매핑해야함.            caption = ""{}: {:.4f}"".format(labels_to_names_0[class_id], score)            print(caption)            #cv2.rectangle()은 인자로 들어온 draw_img에 사각형을 그림. 위치 인자는 반드시 정수형.            cv2.rectangle(draw_img, (int(left), int(top)), (int(right), int(bottom)), color=green_color, thickness=2)            cv2.putText(draw_img, caption, (int(left), int(top - 5)), cv2.FONT_HERSHEY_SIMPLEX, 0.4, red_color, 1)    if is_print:        print('Detection 수행시간:',round(time.time() - start, 2),""초"")    return draw_img img = cv2.imread('./data/IMG_2153.JPG')print('image shape:', img.shape)# tensorflow inference 모델 로딩cv_net = cv2.dnn.readNetFromTensorflow('./pretrained/faster_rcnn_resnet50_coco_2018_01_28/frozen_inference_graph.pb',                                      './pretrained/config_graph.pbtxt')# Object Detetion 수행 후 시각화 draw_img = get_detected_img(cv_net, img, score_threshold=0.5, use_copied_array=True, is_print=True)img_rgb = cv2.cvtColor(draw_img, cv2.COLOR_BGR2RGB)plt.figure(figsize=(12, 12))plt.imshow(img_rgb) 비전 독학중난 사람범주에 들어가지 못했나보다인식이 안됐다. "
[object detection] 2-Stage Detector vs 1-Stage Detector ,https://blog.naver.com/lsj952005/222603160930,20211223,"​​ 출처: https://airsbigdata.tistory.com/211<2-Stage Detector>- 비교적 느리지만 정확도가 높다.- Ex) R-CNN 계열 (R-CNN, Fast R-CNN, Faster R-CNN)...​​​ 출처: https://airsbigdata.tistory.com/211<1-Stage Detector>- 비교적 빠르지만 정확도가 낮다- Ex) YOLO, SPPNet, SSD...​​  ​​ 출처: https://airsbigdata.tistory.com/211(위 2-Stage detector / 아래 1-Stage detector) "
Object detection state of the art 2022 ,https://blog.naver.com/nostresss12/222797222072,20220702, https://link.medium.com/GJch0dd2jrb Object Detection State of the Art 2022Object detection has been a hot topic ever since the boom of Deep Learning techniques. This article compares SOTA detectors like YOLOv4…link.medium.com 
파이썬을 이용한 드론 Object Detection / 컨트롤 ,https://blog.naver.com/hiddenent/222171888359,20201213,"RGB 색 모형 빛의 삼원색을 이용하여 색을 표현하는 방식이다. 빨강(RED), 초록(GREEN), 파랑(BLUE) 세 종류의 광원(光源)을 이용하여 색을 혼합하며 색을 섞을수록 밝아지기 때문에 '가산 혼합'이라고 한다.​HSV 색 모형 색상(Hue), 채도(Saturation), 명도(Value)의 좌표를 써서 특정한 색을 지정한다​ ​드론 조종 http://naver.me/FWmlCc5s 네이버 MYBOX사진, 자료를 안전하게 보관하고 손쉽게 정리, 공유하세요naver.me 설치 경로C:\telloedu\Tello-Python-master  ​ "
[ Object Detection ] Training tool Install - DIGITS ,https://blog.naver.com/sgkim21/221931496579,20200427,"Download source for DIGITS # example location - can be customized   DIGITS_ROOT=~/digits   git clone https://github.com/NVIDIA/DIGITS.git $DIGITS_ROOT​Python Packages can be installed​   sudo pip install -r $DIGITS_ROOT/requirements.txt  ==> if you get error, then  at first install  build-essential      $sudo apt-get install build-essential​​[Optional] Enable support for plug-insDIGITS needs to be installed to enable loading data and visualization plug-ins:sudo pip install -e $DIGITS_ROOT​​Starting the server $ cd digits     $ ./digits-devserver​​Starts a server at ​ http://localhost:5000/​.$ ./digits-devserver --help usage: __main__.py [-h] [-p PORT] [-d] [--version] DIGITS development server optional arguments: -h, --help show this help message and exit -p PORT, --port PORT Port to run app on (default 5000) -d, --debug Run the application in debug mode (reloads when the source changes and gives more detailed error messages) --version Print the version number and exit​​Reference​https://github.com/NVIDIA/DIGITS/blob/digits-6.0/docs/BuildDigits.md NVIDIA/DIGITSDeep Learning GPU Training System. Contribute to NVIDIA/DIGITS development by creating an account on GitHub.github.com ​ "
object detection  ,https://blog.naver.com/lobiz/222668870851,20220310,"- https://github.com/ultralytics/yolov5 GitHub - ultralytics/yolov5: YOLOv5 🚀 in PyTorch > ONNX > CoreML > TFLiteYOLOv5 🚀 in PyTorch > ONNX > CoreML > TFLite. Contribute to ultralytics/yolov5 development by creating an account on GitHub.github.com git clone https://github.com/ultralytics/yolov5 # clone cd yolov5 pip install -r requirements.txt # install import torch  # Modelmodel = torch.hub.load('ultralytics/yolov5', 'yolov5s')  # or yolov5m, yolov5l, yolov5x, custom# Imagesimg = 'https://ultralytics.com/images/zidane.jpg'  # or file, Path, PIL, OpenCV, numpy, listimg = 'data/images/bus.jpg'  # or file, Path, PIL, OpenCV, numpy, list# Inferenceresults = model(img)# Resultsresults.print()  # or .show(), .save(), .crop(), .pandas(), etc.results.save('output') ​ "
Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks 세부설명 ,https://blog.naver.com/grow_bigger/222783635177,20220622,"[EdgeBoxes]Edge Boxes (엣지 박스) (donghwa-kim.github.io)​[translation invariance]translation invariance 설명 및 정리 :: 프라이데이 (tistory.com)위치가 변해도 결과에 변동이 없는 것을 말한다. 해당 물체가 이동한다고 다른 객체라고 인식하지 않는 것인데,Max pooling, softmax등을 통해 위치에 영향을 받지 않도록 할 수 있다.​​ "
Dominating an Online Game with Object Detection Using OpenCV - Template Matching. ,https://blog.naver.com/wnxodnr/222998407756,20230129,https://youtu.be/vXqKniVe6P8 ​ 
Object Detection 논문 흐름 ,https://blog.naver.com/qkrdnjsrl0628/222891402344,20221004,https://github.com/kalelpark/AI_PAPER GitHub - kalelpark/AI_PAPER: AI_REVIEWAI_REVIEW. Contribute to kalelpark/AI_PAPER development by creating an account on GitHub.github.com * 참고용 ​ 
"mediapipe object detection, 우분투 리눅스 사용해서 돌려보는데 많은 걸림돌들 ",https://blog.naver.com/silver9030/222964648580,20221224,"일단은 conda active myproject명령어로 가상환경을 만들고 그안에 들어와있음을 가정하고 그 이후부터 설명... sudo apt-get updatesudo apt-get upgradegit clone https://github.com/google/mediapipe.gitsudo apt install g++ unzip zipsudo apt-get install openjdk-11-jdkwget https://github.com/bazelbuild/bazel/releases/download/3.7.2/bazel-3.7.2-installer-linux-x86_64.shchmod +x bazel-3.7.2-installer-linux-x86_64.sh./bazel-3.7.2-installer-linux-x86_64.sh --user# 한줄씩 따라칠것~! 폴더위치는 그냥 홈 기준으로함# 제 기준으로는 home/rcoona 였던듯... #이부분부터는 잠시 opencv 설치부분...!pkg-config --modversion opencvsudo apt-get install build-essential cmakesudo apt-get install libjpeg-dev libtiff5-dev libpng-devsudo apt-get install ffmpeg libavcodec-dev libavformat-dev libswscale-dev libxvidcore-dev libx264-dev libxine2-devsudo apt-get install libv4l-dev v4l-utilssudo apt-get install libgstreamer1.0-dev libgstreamer-plugins-base1.0-devsudo apt-get install libgtk-3-devsudo apt-get install mesa-utils libgl1-mesa-dri libgtkgl2.0-dev libgtkglext1-devsudo apt-get install libatlas-base-dev gfortran libeigen3-devsudo apt-get install python3-dev python3-numpypip3 install numpymkdir opencvcd opencvwget -O opencv.zip https://github.com/opencv/opencv/archive/4.4.0.zipunzip opencv.zipwget -O opencv_contrib.zip https://github.com/opencv/opencv_contrib/archive/4.4.0.zipunzip opencv_contrib.zipls -d */ #이명령어친후 opencv-4.4.0/ 과 opencv_contrib-4.4.0/ 이 2개가 잘있는지확인cd opencv-4.4.0mkdir buildcd build cmake -D CMAKE_BUILD_TYPE=RELEASE -D CMAKE_INSTALL_PREFIX=/usr/local -D WITH_TBB=OFF -D WITH_IPP=OFF -D WITH_1394=OFF -D BUILD_WITH_DEBUG_INFO=OFF -D BUILD_DOCS=OFF -D INSTALL_C_EXAMPLES=ON -D INSTALL_PYTHON_EXAMPLES=ON -D BUILD_EXAMPLES=OFF -D BUILD_PACKAGE=OFF -D BUILD_TESTS=OFF -D BUILD_PERF_TESTS=OFF -D WITH_QT=OFF -D WITH_GTK=ON -D WITH_OPENGL=ON -D BUILD_opencv_python3=ON -D OPENCV_EXTRA_MODULES_PATH=../../opencv_contrib_4.4.0/modules -D WITH_V4L=ON -D WITH_FFMPEG=ON -D WITH_XINE=ON -D OPENCV_ENABLE_NONFREE=ON -D BUILD_NEW_PYTHON_SUPPORT=ON -D OEPNCV_SKIP_PYTHON_LOADER=ON -D PENCV_GENERATE_PKGCONFIG=ON -D PYTHON3_INCLUDE_DIR=/usr/include/python3.8 -D PYTHON3_NUMPY_INCLUDE_DIRS=/usr/lib/python3/dist-packages/numpy/core/include/ -D PYTHON3_PACKAGES_PATH=/usr/lib/python3/dist-packages -D PYTHON3_LIBRARY=/usr/lib/x86_64-linux-gnu/libpython3.8.so ../ #삼성노트북 웹캠이 우분투에서 잘 인식하지 못하는거같어 안드로이드캠 다시설치..#이전부분 dev47 관련 홈페이지에서 linux관련 설치글 앞부분은 따라하면됨(쉬움)cd ~/.cd /tmp/wget -O droidcam_latest.zip https://files.dev47apps.net/linux/droidcam_1.8.2.zipunzip droidcam_latest.zip -d droidcamcd droidcam && sudo ./install-clientsudo apt install linux-headers-'uname -r' gcc make #이부분 빼먹지말고 잘칠것.. 안그럼 자꾸 device 인식못하는... # 다시 mediapipe 폴더로이동...cd ~/.cd mediapipetime make -j$(nproc)sudo make installcat /etc/ld.so.conf.d/*sudo ldconfigg++ -o facedetect /usr/local/share/opencv4/samples/cpp/facedetect.cpp $(pkg-config opencv4 --libs --cflags)./facedetect --cascade=""/usr/local/share/opencv4/haarcascades/haarcascade_frontalface_alt.xml"" --nested-cascade=""/usr/local/share/opencv4/haarcascades/haarcascade_eye_tree_eyeglasses.xml"" --scale=1.3nano ~/.bashrc# 편집창에들어가면 맨아래 빈공간에export PATH=""$PATH:$HOME/bin""추가하고 ctrl+x 눌러서 저장하고나가기~!sudo apt install ffmpegcd ~/.cd mediapipenano WORKSPACE# 편집창의 수많은 내용중에...# NAME = ""linux_opencv"" 문구를 찾고 그 아래 두번째줄# path = ""/usr/local"" 이부분을 잘 수정해주면된다bazel build -c opt --define MEDIAPIPE_DISABLE_GPU=1 mediapipe/examples/desktop/object_detection:object_detection_cpu ​ "
Open Images 2019 - Object Detection ,https://blog.naver.com/tomatian/221880218293,20200330,"​  ​이 대회 1등 한 팀의 논문이 나왔다.중국 팀에서 1등이 나왔다.​RSNA 챌린지도 그렇고 중국이 점점 딥러닝을 꽉 잡고 있는 추세로 변하는 듯 하다.​​  ​ 1st Place Solutions for OpenImage2019 -- Object Detection and Instance SegmentationThis article introduces the solutions of the two champion teams, `MMfruit'for the detection track and `MMfruitSeg' for the segmentation track, inOpenImage Challenge 2019. It is commonly known that for an object detector, theshared feature at the end of the backbone is not appropriate for bothcla...arxiv.org 전체 플로우는 위 링크 논문에서 확인 가능​​​ Revisiting the Sibling Head in Object DetectorThe ``shared head for classification and localization'' (sibling head),firstly denominated in Fast RCNN~\cite{girshick2015fast}, has been leading thefashion of the object detection community in the past five years. This paperprovides the observation that the spatial misalignment between the two o...arxiv.org 그 코어 모델인 TSD에 대한 설명은 지금 링크에서 확인 가능​​​​ "
Object Detection(객체 탐지) - 오브젝트딕텍션(젝슨나노) ,https://blog.naver.com/85honesty/222734728904,20220516,필터가 의미하는 것이 어떤 건지 정확하게 파악하기는 저로써는 어렵네요어떤 의미가 숨겨져 있을까요? 마지막은 Thumbs up  [Reference]​  dailyman데일리남매일매일 좋은 추억 한두개씩 함께 만들어요😀 선한영향력 긍정적인변화 즐거운하루 같이 가치있게 만들어 가요😄www.youtube.com  구독과 공감 좋아요는 콘텐츠를 만드는 데 큰 힘이 됩니다. 
tensorflow object detection pip list(model zoo) ,https://blog.naver.com/artistically22/222928365345,20221114,첨부파일piplist.txt파일 다운로드 버전 안맞는거 일일이 수정하다가 ok가 뜬 pip list 
CNN-Based Object Detection and Distance Prediction for Autonomous Driving Using Stereo Images ,https://blog.naver.com/sjg918/222998790379,20230130,"​​http://www.ijat.net/ International Journal of Automotive TechnologyARTICLE MECHANICAL PROPERTIES AND OPTIMIZATION ANALYSIS ON BATTERY BOX WITH HONEYCOMB SANDWICH COMPOSITE STRUCTURE A honeycomb sandwich battery box composed of high-strength steel outer layer, sandwich aluminum alloy honeycomb and inner layer is proposed. Firstly, the expressions of platform stress,...www.ijat.net ​​https://github.com/sjg918/kitti-2d-stereo GitHub - sjg918/kitti-2d-stereo: cnn-based object detection and distance prediction for autonomous driving using stereo imagescnn-based object detection and distance prediction for autonomous driving using stereo images - GitHub - sjg918/kitti-2d-stereo: cnn-based object detection and distance prediction for autonomous dr...github.com ​ "
Object Detection(객체 탐지) - 오브젝트디텍션(젝슨나노) ,https://blog.naver.com/85honesty/222734677564,20220516,인식된 부분만 분홍색상과 회색생삭으로 처리 된 걸 볼 수 있습니다.  [Reference]​  dailyman데일리남매일매일 좋은 추억 한두개씩 함께 만들어요😀 선한영향력 긍정적인변화 즐거운하루 같이 가치있게 만들어 가요😄www.youtube.com  구독과 공감 좋아요는 콘텐츠를 만드는 데 큰 힘이 됩니다. 
YOLO(Object Detection) 관련 링크 모음집 ,https://blog.naver.com/friendtoworld/222461130183,20210807,"​YOLO논문https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Redmon_You_Only_Look_CVPR_2016_paper.pdf​초보자 보기 좋은 영상(흐름이랑 간단한 설명 같이 있어서 좋음_Tensor Flow 변환본 코드 간단한 사용법도 같이 있음!)https://www.youtube.com/watch?v=4eIBisqx9_g YOLO관련 CVPR(2016) 발표 내용 유튜브 Andrew Ng교수님의 개념적 설명 유튜브https://www.youtube.com/watch?v=9s_FpMpdYW8 참고할 만한 TED 영상 https://www.youtube.com/watch?v=XS2UWYuh5u0​YOLO페이지https://pjreddie.com/darknet/yolo/->1저자 페이지: https://pjreddie.com/​​​블로거분들이 한국어로 요약하신 내용https://mickael-k.tistory.com/27 Darknet YOLO(You Only Look Once) 공부했다. Yolov3start() { YOLO 란? You Only Live Once가  아닌, You Only Look Once의 약어로 Joseph Redmon이 워싱턴 대학교에서 여러 친구들과 함께 2015년에 yolov1을 처음 논문과 함께 발표 했습니다. 당시만 해도 Object..mickael-k.tistory.com https://yeomko.tistory.com/19https://herbwood.tistory.com/13https://bkshin.tistory.com/entry/%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0-YOLOYou-Only-Look-Once "
[14] Natural Language Guided Visual Relationship Detection ,https://blog.naver.com/jgyy4775/222569756599,20211116,"논문 링크 : https://openaccess.thecvf.com/content_CVPRW_2019/papers/MULA/Liao_Natural_Language_Guided_Visual_Relationship_Detection_CVPRW_2019_paper.pdf​​<Introduction>이 논문은 위치 정보와 자연어를 이용하여 장면 그래프를 생성하는 방법에 관한 논문입니다. 이에 대해 제안하는 방법과 기대 효과들은 다음과 같습니다.▶ Subject물체와 object물체의 단어 임베딩을 통한 관계예측(Glove 이용)    - Image captioning등으로 미리 학습된 임베딩값 사용    - 자주 등장하는 관계를 통해 자주 등장하지 않는 관계 예측 가능    - EX) 자주 등장(person-ride-horse) /  자주 등장하지 않는(person-ride-elephant)    - => horse와 elephant는 모두 animal 범주에 속하므로 비슷한 임베딩 값을 가짐      그러므로, horse를 통해 elephant도 ride라는 관계를 가질 수 있음을 예측      가능​▶ 이러한 정보들을 임베딩하기 위해 Bi-directional  recurrent neural network(BiRNN)을 사용​​​​<Model> 전체 구조도1. Object detection CNN을 거쳐 각 탐지된 영역들의 feature추출​2. Object Pair GenerationSubject와 object feature: 초기 예측된 class 분포spatial information: 두 물체의 bbox의 중심 좌표, 넓이, 높이 이용​3. Object Pair Feature Initializationobject, subject word vector: 2에서 예측된 분포 중 가장 높은 class의 word vector사용relative spatial feature:  2의 spatial information을 이용한 feature BRNN을 사용함으로써 <person-ride-horse>와 <horse-ride-person>이 다름을 학습하며 아래 그림처럼 주어와 목적어 물체가 같아도 다른 관계를 가질 수 있음을 학습 - 기존 BRNN - Our BRNN주어와 목적어 물체의 정보 이용 최종 output y ​4. Natural language guided relationship recognition Subject-predicate-object사이의 RNN을 이용한 정보 공유​5. Joint recognition  ​​<Result>Dataset- Visual Genome- VRD​Metrics- Predicate detection: relation만 예측- Phrase detection: object class와 relation예측- Relationship detection: 물체의 영역, class, relation 모두 예측​ ​​​<결론>- 이미지에서 시각 관계를 탐지하기 위한 자연어 지식 기반 방법을 제시- 이 자연어 지식과 spatial 정보를 기반으로 탐지된 물체 사이의 관계를 예측하기 위해 BRNN 모델을 설계했= > 특히, 롱테일 문제(데이터 불균형 문제)를 처리하는 데 있어 자주 등장하는 관계 인스턴스에서 자주 등장하지 않는 관계를 추론 가능하게 함. "
Deformable DETR: Deformable Transformers for End-to-End Object Detection ,https://blog.naver.com/sonkwan821/222875434092,20220915,"https://www.youtube.com/watch?v=5s_NyhipNis ​https://www.youtube.com/watch?v=q1wSykClIMk ​https://deep-learning-study.tistory.com/825 [논문 읽기] Deformable DETR(2020), Deformable Transformers for End-to-End Object DetectionDeformable DETR: Deformable Transformers for End-to-End Object Detection  PDF, Object Detection, Xizhou Zhu, Weije Su, Lewei Lu, Xiaogang Wang, Jifeng Dai, arXiv 2020 Summary  DETR의 문제점을 개..deep-learning-study.tistory.com ​ "
object detection 최적화 방법  ,https://blog.naver.com/102wnsdh/222893857057,20221007,개발용 
DETR: End-to-End Objecct Detection with Transformers ,https://blog.naver.com/jihee9711/222767629144,20220610,"기존 object detection 기법​(1) Anchor anchor란 가로, 세로, 비율(ratio)이 정해진 바운딩박스를 의미 --> 검은색 점을 기준(중심)으로 여러개의 anchor box가 생성됨☞ grid 안의 검은색 점을 기준으로 사람과 자동차를 모두 detect함(하지만 anchor는 기존에 미리 정해놓은 비율로써만 anchor box 생성이 가능하다는 단점이 있움!)​(2) NMS Non-Maximum SuppressionNMS는 하나의 객체에 중복된 prediction을 제거하는 작업을 의미  AbstractDETR(DEtection TRansformer) : 기존의 NMS, anchor등과 같은 요소들을 제거하고 pipeline을 간소화시킴=> Transformer + 이분매칭 손실함수(bipartite matching) 제안 ※ 이미지에서 추가적으로 positional encoding을 해주면 트랜스포머의 encoder부분으로 들어가게 됨 그리고 그 query정보들이 decoder부분으로 들어가서 수행하면 각각 object에 대한 class와 객체가 어디 존재하는지에 대한 bounding box의 위치들을 알려줄 수 있게 해줌 ​ no object※ 객체가 없을 시 ☞ DETR 구조1) CNN (Backbone)- CNN을 통과하여 feature map을 추출함 - 1 x 1 convolution을 적용하여 d x H x W형태로 바꿈- transformer로 들어가기 위해서는 2차원이어야 하므로 d x HW의 2차원으로 구조를 change2) Transformer(encoder-decoder)3) FFN(Feed Forward Network, 최종 detection 예측 반환함)​▶ 이분매칭(bipartite matching) 기존방식은 후처리DETR는 이분매칭을 통해 set(집합) prediction problem을 직접적으로 해결함※ 기존의 detection에서는 region proposal같은 것을 여러개를 잡아두고 중복된 것을 제거하는 간접적인 방식으로 탐지를 했음​즉 set of box predictions(물체가 있음직한 곳을 잡음)가 어떤 특정 갯수로 나오면 이분매칭 수행= 인스턴스 중복을 피하기 위해 ​EX)출력 개수를 6으로 고정 시 (이미지 안에 객체가 많이 존재한다면 출력 개수를 크게 잡는 것이 Good) (좌) 예측결과 , (우) 실제 값C 0의 경우 물체가 없음 C 1은 새(bird)가 (180, 180, 150, 240)위치에 존재함...                             x         y      w       hC 5의 경우 prediction이 잘못됨  (점점 이분매치의 loss값을 줄여나가게 함)​= 한 마디로 bounding box가 ground truth의 어떤 객체를 검출하고 있는지 1대 1로 매칭을 해주는 과정을 의미​▶ Transformer​ Transformer (attention is all you need, 2017)                                                                                                        DETR 구조 (object quries = n개, 2020) ※ 트랜스포머는 sequential한 데이터 간의 연관성을 병렬적으로 수행하여 자연어처리 및 음성 처리에도 활용되고 있음​→ DETR의 경우, 이미지 즉 다차원 형태의 행렬을 sequential하게 바꾸어 트랜스포머로 수행하면 픽셀 간의 연관성 파악 가능할 것이라고 논문에서는 생각함​     ▶ 기존 Transformer와 다른점?1) positional encoding 위가 다름2) parallel(병렬) 방식으로 output을 출력함​EX) ※Encoder※ positional encoding을 거쳐 input data(d x HW)는 attention mechanism을 거침 --> image 내에서 픽셀들 간의 유사성, 연관성 정보들을 추출--> attention score에 반영​결과적으로 어떤 grid가 object인지, background인지 학습하게 됨!​EX) Encoder visualization attention score※Decoder※  1) N개의 bounding box에 대해 N개의 object query생성함 (분홍색 부분)2) 이 object query를 입력받아 멀티 헤드 셀프 어텐션을 거쳐서 가공된 N개의 unit을 출력함 (보라색 부분)3) 일부는 query, 일부는 인코더의 key, value에서 가져와서 multi-head attention을 수행함4) 최종 unit들은 FFN을 거쳐 class와 box위치들을 출력함​결과적으로 디코더는 각 인스턴스의 클래스와 경계선을 추출하는 역할을 함!​EX) Decoder visualization 참고) https://wikidocs.net/145910  ​ "
[10강] Detection and Segmentation ,https://blog.naver.com/kdesaran/222937946571,20221125,"지난 시간에는 Recurrent Neural Networks를 소개했었다. 오늘은 Detection과 Segmentation등 Compuver Vision task들을 소개할 예정이다. Segmentation, Localization, Detection 등 다양한 Computer Vision Tasks와 이 문제들을 CNN으로 어떻게 접근해 볼 수 있을지 고민할 예정이다. 주제SementicSegmentationSementic Segmentation 문제에서는 입력은 이미지이고 출력으로 이미지의 모든 픽셀에 카테고리를 정한다. 밑에 왼쪽 예제를 보면 입력은 고양이이고, 출력은 모든 픽셀에 대해서 그 픽셀이 ""고양이, 잔디, 하늘, 나무, 배경""인지를 결정하는 것이다. Sementic Segmentation 에서도 Classification에서 처럼 카테고리가 있다. 하지만 다른 점은 Classification 처럼 이미지 전체에 카테고리 하나가 아니라 모든 픽셀에 카테고리가 매겨지는 형태다. 또 하나 유의해야 할 점은, semantic segmentation은 개별 객체를 구분하지 않는다. 오른쪽 이미지를 보면 소 두마리가 있는데, Semantic Segmentation의 경우에는 픽셀의 카테고리만 구분하기 때문에 소 두 마리를 구별할 수 없고 ""cow""라고 레이블링된 픽셀 덩어리만 얻을 수 있다. 이는 Sementatic Segmentation의 단점이라고도 할 수 있고, 나중에 배울 Instance Segmentation이 문제를 해결할 수 있다.​ Semantic Segmentation 문제에 접근해볼 수 있는 방법 중 하나는 Classification을 통한 접근이다. Semantic Segmentation을 위해서 Sliding Window를 적용하는 방법이 있는데, 입력 이미지를 아주 작은 단위로 쪼갠다. 밑에 예제를 보면 소의 머리 주변에서 영역 3개를 추출했다. 그 다음에 우리는 이 작은 영역만을 가지고 Classification 문제를 푼다고 생각해 보는 것이다. 즉, 해당 영역이 어떤 카테고리에 속하는지를 정하는 것이다. 이는 이미지 한장을 분류하기 위해서 만든 모델을 이용해서 이미지의 작은 영역을 분류하게 해볼 수 있을 것 같다. 이 방법이 어느 정도 동작할 수도 있지만, 그렇게 좋은 방법이 아니다. 왜냐하면 모든 픽셀에 대해서 작은 영역으로 쪼개고, 이 모든 영역을 forward/backward pass 하는 일은 상당히 비효율적으로 비용이 엄청나게 크기 때문이다. 또한, 서로 다른 영역이라도 인접해 있으면 어느정도는 겹쳐있기 때문에 특징들을 공유할 수도 있다. 그래서 이렇게 개별적으로 접근하는 방법은 나쁜 방법이라고 생각할 수 있다. 하지만, Semantic Segmentation을 하고자 할 때 가장 먼저 생각해 볼 수 있는 방법일 것이다. sliding window 방법또 다른 방법 (개선된 방법)으로는 Fully Convolutional Network가 있다. 이는 이미지 영역을 나누고 독집적으로 분류하는 방법은 아니다. FC-Layer가 없고 Convolution Layer 구성된 네트워크이다. Conv Layer들을 쌓아올면 이미지의 공간정보를 손실하지 않고 학습 할 수 있다. 이 네트워크의 출력 Tensor는C x H x W로, C는 카테고리의 수이다. 그리고 이 출력 Tensor는 입력 이미지의 모든 픽셀 값에 대해 Classification Scores를 매긴 값이다. 이 네트워크를 학습시키려면 우선 모든 픽셀의 Classification loss를 계산하고 평균 값을 취한 다음 기존처럼 back propagation을 수행하면 된다.​ fully convolutional 방법Training data는 어떻게 만들까?입력 이미지에 모든 픽셀에 대해서 레이블링을 하는 작업을 거쳐야한다. 객체의 외관선만 그려주면 안을 채워넣는 식으로 툴을 만들어서 사용하는 경우도 있는데, 일반적으로는 Train data를 만들거나 수집하는것은 비용이 상당이 클 것이다. 손실 함수는 어떻게 결정될까? fully convolutional network 문제에서는 모든 픽셀을 Classification하고 출력의 모든 픽셀에 Cross entropy를 적용하는 것이다. 그리고 이 모델을 학습시킬 때, 모든 픽셀의 카테고리를 알고 있다는 가정이 있어야 된다. image classification이랑 비슷한 경우라고 생각하면 된다. Semantic Segmentation에서도 클래스의 수가 고정된다고 생각하면 된다.어떤 식으로 동작할까? Max pooling, Stride Convolution 등으로 특정 맵을 Downsample한다. Image Classification에서는 이렇게 downsampling한 다음에 FC-Layer가 있었다. 하지만 여기에서는 Spatial Resolution을 다시 키운다. 출력이 다시 입력 이미지의 해상도와 같아지도록 하는 것이다 (upsampling).우리는 Convolutional Networks에서의 Downsampling에 대해서는 이미 본 적이 있다. 이미지의 Spatial Size를 줄이기 위한 Stried Conv 라던가 다양한 Pooling layer에 대해 다뤄본 적이 있다. 하지만 upsampling은 처음 접하는 개념일 것이다. Upsampling 전량 중 하나는 unpooling이다. Downsample에서의 pooling에는 average/max pooling 등이 있었다. upsampling 방법 중에는 nearest neighbor unpooling이 있다. 밑에 예제의 왼쪽을 보면 nearest neighbor unpooling 예제가 있다. 입력은 2x2 그리드이고 출력은 4x4 그리드이다. 2x2 stride nearest neighbor unpooling은 해당하는 receptive field로 값을 그냥 복사한다. 오른쪽에서 보면 bed of nails unpooling이란 방법도 있다. bed of nails upsampling이라고도 하는데, 이 방법은 unpooling region에만 값을 복사하고 다른 곳에는 모두 0을 채워넣는 방법이다. 이 경우 하나의 요소를 제외하고 모두 0으로 만든다. 이 방법이 bed of nails라고 불리는 이유는 zero region은 평평하고 non-zero region은 바늘처럼 뾰족하게 값이 튀기 때문이다.​ upsampling - unpoolingUpsampling 하는 방법중에는 Max unpooling이란 방법도 있다. 각 unpooling과 pooling을 연관짓는 방법이다. Upsampling의bed of nailsupsampling 과 유사한데, downsampling시에 Max pooling에 사용했던 요소들을 잘 기억하고 있다가 bed of nails upsampling처럼 같은 자리에 값을 넣어주는 것이 아니라 이전 Maxpooling에서 선택된 위치에 맞게 넣어주고,다른 곳에는 모두 0을 채워넣어 준다. 정리하자면,Low Resolution 특징 맵을 High Resolution 특징 맵으로 만들어 주는 것인데, 이 때 Low Resolution 의 값들을 Maxpooling에서 선택된 위치로 넣어주는 것이다. Semantic segmentation 에서는 모든 픽셀들의 클래스를 모두 잘 분류해야 한다. 이 때, 예측한 Segmentation 결과에서 객체들간의 디테일한 경계가 명확할수록 좋다. 하지만 Maxpooling을 하게되면 특징맵의 비균진성이 발생합니다. 즉 공간정보를 잃게 된다. Maxpooling 후의 특징 맵만 봐서는 이 값들이 Receptive field 중 어디에서 왔는지 알 수가 없다. 그래서 Unpool 시에 기존 Maxpool 에서 뽑아온 자리로 값을 넣어주면 공간 정보를 조금은 더 디테일하게 다룰 수 있다. 결국, Max pooling 에서 읽어버린 공간정보를 조금은 더 잘 유지하도록 도와주는 것이다.​ upsampling - max unpooling4번째 upsampling 방법은 Transpose Convolution 이다. 지금까지 살펴봤던 Unpooling 방법은 정리해보면, Bed of nails, Nearest Neighbor, Max unpooling 까지 다뤘다. 이 방법들은 ""고정된 함수"" 이고 별도로 학습을 시키지는 않는다. Strided convolution을 다시 생각해보면 어떤 식으로 Downsampling을 해야할지를 네트워크가 학습할 수 있었다 (convolution layer with stride). Upsampling에서 이와 유사한 방법이 바로 Transpose convolution이다. Transpose convolution은 특징 맵을 Upsampling 할 때 어떤 방식으로 할 지를 학습할 수 있다. 이는 특수한 방식의 Convolution의 일종이다. 일반적인 3 x 3 (stride = 1, padding =1) Convolution Filter가 동작하는 방식을 다시한번 살펴보자. 아래 예제를 보면, 입력은 4x4 이고, 출력도 4x4이다. 3x3 필터가 있고 이미지와 내적을 수행한다. 시작은 우선 이미지의 좌 상단 구석부터 시작한다. 내적의 결과는 출력(4x4)의 좌 상단 코너의 값이 되는 형식이다. Strided convolution을 살펴보자. 다른점은 입력이 4x4 이고 출력은 2x2이다. 계산 방식은 기존과 유사합니다. 3x3 필터가 있고 이미지의 좌상단 구석에서부터 내적을 계산한다. 하지만 Strided convolution은 한 픽셀씩 이동하면서 계산하지 않는다. 출력에서 한 픽셀 씩 움직이려면 입력에서는 두 픽셀 씩 움직이는 방식이다. ""Stride = 2"" 는 입력/출력에서 움직이는 거리 사이의 비율이라고 해석할 수도 있다. 즉, 따라서 Stride = 2인 Strided convolution은 ""학습 가능한 방법"" 으로 2배 downsampling 하는 것을 의미한다.​ Transpose convolution은 반대의 경우로, 입력이 2x2 이고 출력이 4x4 이다. Transposeconvolution을 위한 연산은 조금 다르게 생겼는데, 여기에서는 내적을 수행하지 않는다. 우선 입력 특징맵에서 값을 하나 선택한다 (빨간색). 밑에 예제에서 보듯이 좌상단에서 뽑은 이 하나의 값은 스칼라 값일 것이다. 그 다음 이 스칼라 값을 필터(3 * 3)와 곱한다. 그리고 출력의 3x3 영역에 그 값을 넣는다. Transpose convolution에서는 필터와 입력의 내적을 계산하는 것이 아니라, 입력 값이 필터에 곱해지는 가중치의 역할을 해 출력 값은 필터 * 입력(가중치) 이다. 그리고 Upsampling 시에는 입력에서 한 칸씩 움직이는 동안 출력에서는 두 칸찍 움직인다. 아래 예제에서 오른쪽 이미지 처럼 출력에서 Transpose convolution 간에 Receptive Field가 겹치는 현상이 발생할 수 있다. 이렇게 겹치는 경우에는 간단하게 두 값을 더해주면 된다. 이 과정을 반복해서 끝마치면 학습 가능한 upsampling을 수행한 것이다. Spatial size를 키워주기 위해서 학습된 필터 가중치를 이용한 것이다. 이 방법은 문헌에 따라서 부르는 이름이 다양하다. Deconvolution라는 이름이 붙기도 하는데 신호처리의 관점에서 deconvolution은 Convolution 연산의 역 연산을 의미하지만 실제로 transpose convolution은 그런 연산은 아님으로 그 이름으로 부르는 것은 그닥 좋지 않다고 한다. 딥러닝 관련 논문에서 deconvolution이라는 용어는 주의해서 이해해야 한다. 그리고 간혹 upconvolution이라고 부르기도 하고, fractionally strided convolution 이라고도 한다. fractionally strided convolution이라고 이름이 붙은 이유는 stride를 input/output간의 크기의 비율로 생각하면 밑에 예제는 input : output = 1 : 2 이기 때문에 stride 1/2 convolution 이라고 할 수 있다. Backwards strided convolution이라고도 부른다. 왜냐하면 transpose conv의 forward pass를 수학적으로 계산해보면 일반 Convolution의 backward pass와 수식이 동일하기 때문이다.​ classification+localization이미지가 어떤 카테고리에 속하는지 뿐만 아니라 실제 객체가 어디에 있는지를 알고 싶을 수도 있다. 이미지를 ""Cat""에 분류하는 것 뿐만 아니라 이미지 내에 Cat이 어디에 있는지 네모박스를 그리는 것이다. classification plus localization 문제는 object detection 문제와는 구별된다. localization 문제에서는 이미지 내에서 내가 관심있는 객체가 오직 하나 뿐이라고 가정한다. 더 많은 객체를 찾고 싶을수도 있겠지만, 기본적으로 이미지 내에 객체 하나만 찾아서 레이블을 매기고 위치를 찾아낸다. 이 Task를 바로 classification plus localization라고 한다. 이 문제를 풀 때도 기존의 image classification에서 사용하던 기법들을 사용할 수 있다.​ Architectures의 기본 구조는 다음과 같다. 네트워크는 이미지를 입력으로 받는다. 밑에 예제에서는 AlexNet를 사용했다. 마지막에 FC-Layer는 ""Class score""로 연결되서 카테고리를 결정한다. FC-Layer가 하나 더 있는데, 이는 4개의 원소를 가진 vector와 연결되어있다.이 4개의 출력 값은 width/height/x/y로 bounding box의 위치를 나타낸다. 이런 식으로 네트워크는 두 가지 출력값을 반환한다. 하나는 Class Score, 다른 하나는 입력 영상 내의 객체 위치의 bounding box의 좌표이다. 이 문제는 fully supervised setting 을 가정한다. 따라서 학습 이미지에는 카테고리 레이블과 해당 객체의 bounding box GT를 동시에 가지고 있어야 한다. 이 네트워크를 학습시킬 때는 loss가 두 개 존재하는데, 우선 Class scores를 예측하기 위한 Softmax loss가 있다. 그리고 Ground Truth Bbox와 예측한 Bbox 사이의 차이를 측정하는 Loss도 있다. L2 Loss로 BBox Loss를 가장 쉽게 디자인할 수 있다고 한다. 1이나 smooth L1을 사용해도 상관없다. 이 Loss들은 모두 예측한 Bbox와 GT Bbox 좌표 간의 차이에 대한 regression loss이다.​ Bbox와 같이 이미지 내의 어떤 위치를 예측한다는 아이디어는 classification plus localization 문제 이외에도 아주 다양한 문제에도 적용해 볼 수 있는데, 그 중 하나가 human pose estimation이다. human pose estimation 문제 에서는 사람 이미지가 입력으로 들어간다. 출력은 이 사람의 각 관절의 위치로, 이 네트워크는 사람의 포즈를 예측한다. 즉, 이 사람의 팔다리가 어디에 있는지를 예측하는 것이다. 일반적으로 대부분은 사람들의 관절의 수는 같다. 모든 사람들이 그런 것은 아니겠지만 이 네트워크의 가정은 그렇다. 이런 문제를 풀기 위해서는 가령 일부 Data sets은 14개의 관절의 위치로 (사람의 발, 무릎, 엉덩이와 같이) 사람의 포즈를 정의한다. 이 네트워크의 입력은 사람 이미지이고, 네트워크의 출력은 각 관절에 해당하는 14개의 좌표 값이다. 예측된 14개의 점에 대해서 regression loss를 계산하고 backprop으로 학습을 시킨다. 가장 심플하게 L2 loss를 사용하기도 하고 또는 다양한 regression losses를 적용할 수 있다.* ""Regression Loss"" 는 cross entropy나 softmax가 아닌 Losses를 의미하는데, L2, L1, smooth L1 loss 등이 있다.​ object detectionObject Detection 문제에서도 고정된 카테고리가 존재한다. 예를 들어, 고양이, 개, 물고기 등 고정된 카테고리 갯수만 생각하는 형식이다. Object Detection의 task는 입력 이미지가 주어지면, 이미지에 나타나는 객체들의 Bbox와 해당하는 카테고리를 예측한다. classification plus localization와는 조금 다른데, 예측해야 하는 Bbox의 수가 입력 이미지에 따라 달라진다.​ Object Detection이 Localization과는 다르게 객체의 수가 이미지마다 다르다. Object Detection 문제를 풀 때 예전부터 사람들이 많이 시도했던 방법은 sliding window이다. 앞서 Semantic segmentation에서 작은 영역으로 쪼갰던 아이디어와 비슷한 방법을 사용한다. Sliding window를 이용하려면 입력 이미지로부터 다양한 영역을 나눠서 처리해야되는데, 예를 들면, 이미지의 왼쪽 밑에서 작은 영역을 추출해서 그 작은 영역만 CNN의 입력으로 넣는다. 그러면 CNN이 작은 영역에 대해서 Classification을 수행하는 것이다. 밑에 예시를 보면 이 영역에는 개는 있고, 고양이는 없고, 배경도 아니다. 이때 가장 큰 문제는 어떻게 영역을 추출할지이다. 이미지에 Objects가 몇 개가 존재할지, 어디에 존재할지를 알 수가 없다. 그리고 크기가 어떨지도 알 수 없다. 그래서 이런식의 sliding window를 하려면 너무나 많은 경우의 수가 존재하게 되고, 작은 영역 하나, 하나마다 거대한 CNN을 통과시키려면 이 때의 계산량은 어마어마해 다룰 수가 없을 것이다. 그래서 Object Detection 문제를 풀려고 brute force sliding window를 하는 일은 없다.​ Sliding window 대신에Region Proposals 이라는 방법이 있다. RegionProposalNetwork은 전통적인 신호처리 기법을 사용한다. Region Proposal Network는 Object가 있을법한 가령, 1000개의 Bbox를 제공해 준다. 이미지 내에서 객체가 있을법한 후보 Region Proposas을 찾아내는 다양한 방법(edges, etc)이 있겠지만, Region Proposal Network는 이미지 내에 뭉텅진(blobby) 곳들을 찾아내고, 이 지역들은 객체가 있을지도 모르는 후보 영역들이 된다. 이 알고리즘은 비교적 빠르게 동작한다. Region Proposal을 만들어낼 수 있는 방법에는 Selective Search가 있다. Selective Search은 밑에 슬라이드에 적힌 1000개가 아니라 2000개의 Region Proposal을 만들어 낸다. 우리가 CPU로 2초간 Selective Search를 돌리면 객체가 있을만한 2000개의 Region Proposal을 만들어난다. 우리는 이 Region Proposal Network를 이용해서 이미지 내의 모든 위치와 스케일을 전부 고려하는거 대신에, 우선 Region Proposal Networks를 적용하고 객체가 있을법한 Region Proposal 을 얻어낸다. 그리고 이 Region Proposals을 CNN의 입력으로 함으로써 계산량을 줄일 수 있다. 모든 위치와 스케일을 전부 고려하는 (brute force) 방법보다 낫다.​ 위에서 설명한 region proposal 아이디어는 몇 년 전에 나온 R-CNN이라는 논문에서 소개된다. 먼저, 이미지가 주어지면 Region Proposal을 얻기 위해 Region Proposal Network를 수행한다. Region Proposal은 Region of Interest (ROI) 라고도 한다. Selective Search를 통해 2000개의 ROI를 얻어낸다. 하지만 여기에서 각 ROI의 사이즈가 각양각색이라는 점이 문제가 될 수 있다. 그렇기때문에 추출된 ROI로 CNN Classification을 수행하려면 FC-Layer 등으로 사용해 같은 입력 사이즈로 맞춰줘야 한다. 즉, Region proposals을 추출하면 CNN의 입력으로 사용하기 위해서는 동일한 고정된 크기로 변형시키는 과정을 먼저 거쳐야 한다. R-CNN의 경우에는 ROI들의 최종 Classification에 SVM을 사용했다. RCNN은 BBox의 카테고리도 예측하지만, BBox를 보정해 줄 수 있는 offset 값 4개도 예측한다. 이를 Multi-task loss로 두고 한 번에 학습하는 방식이다. 이 Task는 Fully Supervised입니다. 따라서 학습데이터에는 이미지 내의 모든 객체에 대한 BBox가 있어야 한다.​ R-CNN Frsmework에는 많은 문제점들이 있다. R-CNN은 여전히 계산비용이 높다. R-CNN은 2000개의 Region proposals이 있고 각각이 독립적으로 CNN입력으로 들어갑니다. 이때학습이 되지 않은 이 Region Proposal은앞으로 문제가 될 소지가 많다. 또한, R-CNN은 학습과정 자체가 상당히 오래걸린다. 이미지당 2000개의 ROIs를 forward/backwrad pass를 수행한다. Fast R-CNN은 문제들을 상당부분 해결했다.​ ​Fast R-CNN도 R-CNN과 시작은 같다. 하지만 Fast R-CNN에서는 각 ROI마다 각각 CNN을 수행하지 않고, 전체 이미지에 CNN을 수행한다. 그 결과 전체 이미지에 대한 고해상도 Feature Map을 얻을 수 있다. Fast R-CNN에는 여전히 Selective Search같은 방법으로 Region proposals을 계산한다. 이미지에서 ROI를 가져오는 형식이 아니라 CNN Feature map에 ROI를 Projection 시키고 전체 이미지가 아닌 Feature map에서 가져온다. 그러므로 CNN의 Feature를 여러 ROIs가 서로 공유할 수 있다. 그 다음 FC-Layer가 있는데, FC-Layer는 고정된 크기의 입력을 받기 떄문에CNN Feature Map에서 가져온 ROI는 FC-Layer의 입력에 알맞게 크기를 조정해 줘야한다. 학습이 가능하도록 미분가능한 방법을 사용해야하는데, 이 방법이 바로 ROI pooling layer이다. Feature Map에서 나온 ROI 크기를 조정하고 나면 FC-Layer의 입력으로 넣어서 Classification Score와 Linear Regression Offset을 계산한다. Fast R-CNN을 학습할 때는 두 Loss를 합쳐 Multi-task Loss로 Backprop를 진행한다. Train time 에는 fast R-CNN이 10배 가량 더 빠르다. Faster R-CNN은 Feature map을 서로 공유하기 때문이다. Test time에는 fast R-CNN은 정말 빠르다. 이렇게 Fast R-CNN은 정말 빠르기 때문에 Region Proposal을 계산하는 시간이 대부분이게 된다. 2000개의 Region Proposal을 Selective Search로 계산하는데 2초 가량 걸린다. Region Proposals을 계산한 이후 CNN 을 거치는 과정은 모든 Region Proposals이 공유하기 때문에 1초도 안걸린다. 따라서 fast R-CNN은 Region Proposal을 계산하는 구간이 병목이다. 이 문제를 faster R-CNN이 해결해준다. Faster R-CNN은 네트워크가 region proposal 을 직접 만들 수 있다. Faster R-CNN은 별도의 Region proposal network가 존재해 RPN은 네트워크가 Feature Map을 가지고 Region proposals을 계산하도록 한다. RPN을 거쳐 Region Proposal을 예측 하고나면 나머지 동작은 fast R-CNN과 동일하다. Conv Feature map에서 Region proposals을 뜯어내고 이들을 나머지 네트워크에 통과한 다음 multl-task loss를 이용해서 여러가지 Losses를 한 번에 계산한다. Faster R-CNN은 4개의 Losses를 한 번에 학습한다.​ ​RPN에는 2가지 Losses가 있다. 한가지는 이곳에 객체가 있는지 없는지를 예측한다. 그리고 나머지 Loss는 예측한 BBox에 관한 것이다. Faster R-CNN의 최종단 에서도 두 개의 Losses가 존재한다. 하나는 Region Proposals의 Classification을 결정하고 남은 하나는 BBox Regression으로 앞서 만든 Region Proposal을 보정해 주는 역할을 한다.​ 지금까지 살펴본 내용은 R-CNN 패밀리에 관한 내용이었다. R은 "" Region"" 을 뜻한다. R-CNN 계열 네트워크들은 후보 ROIs 마다 독립적으로 연산을 수행한다. R-CNN 계열의 네트워크들을 region-based method 라고 한다. object detection에는 다른 방법도 존재한다. 그 중 하나가 YOLO로 You Only Look Once라는 뜻이다. 다른 하나는 SSD 인데, Single Shot Detection라는 뜻이다. 이들의 주요 아이디어는 각 Task를 따로 계산하지 말고 하나의 regression 문제로 풀어보자는 것이다.입력 이미지가 있으면 이미지를 큼지막하게 나눈다. 예를들어, 밑에 예제를 보면 7x7 grid로 나눈다. 각 Grid Cell 내부에는 Base BBox가 존재한다. 밑에 예제의 경우에는 Base BBox가 세 가지 있다 (길쭉한놈 넓죽한놈 정사각형). 실제로는 세 개 이상 사용한다. 이제 grid cell에 대해서 BBoxes가 있고 이를 기반으로 예측을 수행할 것이다. 우선 BBox의 offset을 예측할 수 있다. 실제 위치가 되려면 base BBox를 얼마만큼 옮겨야 하는지를 뜻한다. 그리고 각 BBox에 대해서 Classification scores를 계산한다. 이 BBox 안에 이 카테고리에 속한 객체가 존재할 가능성을 의미한다. 네트워크에 입력 이미지가 들어오면 7 x 7Grid마다 (5B + C) 개의 tensor를 가진다. 여기에서 B는 base BBox의 offset (4개)과 confidence score(1개)로 구성된다. 그리고 C는 C개의 카테고리에 대한 Classification score이다. nstance SegmentationInstance segmentation는 지금까지 배운 것들의 종합 선물 세트 느낌이다. 입력 이미지가 주어지면 객체별로 객체의 위치를 알아내야 한다는 점에서 Object Detection 문제와 유사하다. 하지만 객체별로 BBox를 예측하는 것이 아니라 객체 별 Segmentation Mask를 예측해야 한다 (이미지에서 각 객체에 해당하는 픽셀을 예측해야 하는 문제). Instance Segmentation은 Semantic Segmentation과 Object Detection을 더한 것이다. Object Detection 문제 처럼 객체별로 여러 객체를 찾고 각각을 구분해 줘야 한다. 가령 이미지 내에 두 마리의 개가 있으면 Instance segmentation은 이 두 마리를 구별해야 하고, 그리고 각 픽셀이 어떤 객체에 속하는지를 전부 다 결정해 줘야 한다. Instance Segmentation 문제를 푸는 아주 다양한 방법이 있지만 오늘 소개할 방법은 Mask R-CNN이라는 모델이다.​ Mask R-CNN은 faster R-CNN과 유사하다. Mask R-CNN은 여러 스테이지를 거친다. 처음 입력 이미지가 CNN과 RPN을 거치는데, 여기까지는 Faster R-CNN과 유사하다. 그리고 Fast/Faster R-CNN에서 했던 것 처럼 특징 맵에서 RPN의 ROI만큼을 뜯어(project)낸다. 그 다음 단계는 Faster R-CNN에서 처럼 Classification/ BBox Regression을 하는 것이 아니라 각 BBox마다 Segmentation mask를 예측하도록 한다. RPN으로 뽑은 ROI 영역 내에서 각각 semantic segmentation을 수행한다. Feature Map으로부터 ROI Pooling(Align)을 수행하면 두 갈래로 나뉜다. 첫 번째 갈래에는 각 Region proposal이 어떤 카테고리에 속하는지 계산하고, Region Proposal의 좌표를 보정해주는 BBox Regression도 예측한다. 두 번째 갈래는 각 픽셀마다 객체인지 아닌지를 분류한다.​ Mask R-CNN은 성능이 아주 뛰어나면서도 Faster R-CNN Frsmework 기반으로 비교적 쉽게 구현할 수 있다.​ ​ "
[1] Video Visual Relation Detection ,https://blog.naver.com/jgyy4775/222543308810,20211021,"논문 링크:https://dl.acm.org/doi/pdf/10.1145/3123266.3123380?casa_token=Y_82TDmkH_EAAAAA:8GSiLS7kYaPj9ncAsizbVBRbxDI4CoJAcWeHbF46dJI8kfrtCPu9v3gFABKnM7x1bmKDt-Si56Td5Q Video Visual Relation Detection | Proceedings of the 25th ACM international conference on Multimediaresearch-article Video Visual Relation Detection Share on Authors: Xindi Shang , Tongwei Ren , Jingfan Guo , Hanwang Zhang , Tat-Seng Chua Authors Info & Claims MM '17: Proceedings of the 25th ACM international conference on Multimedia October 2017 Pages 1300–1308 https://doi.org/10.1145/3123266.3...dl.acm.org Github: https://github.com/xdshang/VidVRD-helper/tree/9ea80afcfa431aa74f4c29073617dad5d03a8b0f GitHub - xdshang/VidVRD-helper at 9ea80afcfa431aa74f4c29073617dad5d03a8b0fTo keep updates with VRU Grand Challenge, please use https://github.com/NExTplusplus/VidVRD-helper - GitHub - xdshang/VidVRD-helper at 9ea80afcfa431aa74f4c29073617dad5d03a8b0fgithub.com 이 논문은 ""비디오 관계 탐지""를 가장 처음 제안한 논문입니다.​<Introduction>이 논문은 ""Video Visual Relationship Detection"" task를 처음 제안한 논문이며 이 task에 가장 처음 등장한 데이터 집합인 VidVRD를 제안한 논문입니다. ​- Video Visual Relationship Detection이란?VidVRD는 입력으로 주어지는 비디오 안에 등장하는 모든 물체들과 그들 간의 관계들을 탐지하여 <Subject(주어) - Relationship(관계) - Object(목적어)>의 형태로 나타내는 작업입니다. 이 작업은 비디오에 등장하는 모든 물체 트랙들을 탐지하고 시공간의 변화에 따라 변화하는 관계를 잘 탐지할 수 있는 방법을 요구합니다. 다시 말해, 물체 trajectory 탐지, 관계 탐지를 동시에 수행하는 높은 수준의 도전 과제라고 할 수 있습니다. 위 그림은 이미지에서의 관계 탐지와 비디오에서의 관계 탐지를 비교한 그림입니다. -이미지에서는 등장하는 물체를 찾기 위해 물체 탐지(Object Detection)를 수행하지만 비디오에서는 변화하는 물체의 위치를 추적해야 하기 때문에 물체 추적(Object Tracking)을 수행합니다.-이미지에서는 정적인 관계만을 다루지만 비디오에서는 다양한 동적인 관계를 다룹니다. 또한 동일한 주어와 목적어 사이에 시간이 변화함에 따라 관계가 변화하는 모습 또한 확인할 수 있습니다.​<Dataset>본 논문에서는 데이터 집합 VidVRD와 이 task를 위한 새로운 평가지표를 제안하였습니다.- VidVRD Dataset총 1000개의 비디오로 이루어져 있으며 training set : test set = 800 : 200입니다.물체 카테고리는 airplane, antelope, ball, bicycle, cat, cattle, dog, elephant, motorcycle, person, sheep, skateboard, snake를 포함하여 35개가 존재합니다.관계 카테고리는  ride와 같은 타동사, faster와 같은 비교 서술자, above와 같은 공간 관계 서술자, fly와 같은 자동사 등이 있습니다. 아래 표는 데이터 집합의 통계표입니다. -Metrics1) Relationship tagging         => Relation triple 예측         => Precision 사용2) Relationship detection         => Relation triple, object trajectory예측         => Recall 사용​​<Model> 전체 구조도(Baseline Model)1) Object Tracklet Proposal먼저 입력 비디오를 30프레임씩 일정한 크기의 비디오(segment)로 분할합니다. 이때 각 segment는 이웃 비디오와 15프레임씩 겹치게 분할합니다. 이는 등장하는 관계들 중 30프레임 이상 지속되는 관계들을 탐지하기 위함입니다.=> per-frame detection: Faster R-CNN(ResNet 101)=> object trajectory generation: MOSSE tracker(설명: https://neverabandon.tistory.com/2)=> trajectory refinement: NMS​2) Relationship Prediction 가능한 모든 물체 쌍들에 대해서 아래 feature들을 추출합니다.=> 각 tracklet의 물체 클래스 분포도=> 각 tracklet의 HoG, HoF, MBH=> relativity feature 위 3가지 feature들을 모두 concatenation 해줍니다. 최종 feature로 주어 물체 분류, 관계 분류, 목적어 분류를 모두 수행하며 학습합니다. 이때 사용하는 loss 함수는 아래와 같습니다. ​3) Greedy Relational Associationsegment들 사이의 <s - p - o>관계를 하나로 통합해주는 과정입니다. short-term 관계들을 이어붙여  long-term관계가 될 수 있도록합니다. 모든 pair들에 대해 수행하고 두 <s - p - o>의 클래스가 같고 주어와 목적어의 트랙의 vIoU가 0.5이상이면 같은 관계로 판단합니다.​​<Result>다른 모델들과의 성능 비교표입니다.표에서 1~4번모델은 기존의 이미지 관계탐지 모델을 동영상 관계 탐지 모델로 변형한 것입니다.본 논문에서 제안하는 Baseline Model이 가장 높은 성능을 보이고 있습니다. ​<결론>- VidVRD라는 새로운 vision task제안- VidVRD라는 새로운 데이터 집합 제안 - 제안하는 task를 해결하기 위해 Object tracklet proposal - relation prediction - greedy relational association의 3단계로 이루어진 모델 제안 "
Object Detection by YOLO v3 실습 ,https://blog.naver.com/ha00116/221622742319,20190820,"https://github.com/YunYang1994/tensorflow-yolov3 YunYang1994/tensorflow-yolov3🔥 pure tensorflow Implement of YOLOv3 with support to train your own dataset  - YunYang1994/tensorflow-yolov3github.com  1. Quick Start1) requirement 설치   오류 : 요구하는 tensorflow-gpu 버전을 찾을 수 없음 해결 : requirments.txt 파일 수정 numpy==1.17.0Pillow==5.3.0scipy==1.1.0tensorflow-gpu==1.14.0wget==3.2seaborn==0.9.0 2) checkpoint- yolov3_coco.tar.gz 다운로드 및 압축 해제(https://github.com/YunYang1994/tensorflow-yolov3/releases)- convert_weight.py 실행   오류 : ImportError 해결 : CUDA, cuDNN 설치 및 tensorflow-gpu, tf-nightly install(https://towardsdatascience.com/installing-tensorflow-with-cuda-cudnn-and-gpu-support-on-windows-10-60693e46e781) pip install tf-nightly   오류 : ModuleNotFoundError 해결 : numpy upgrade pip install numpy --upgrade   오류 : ModuleNotFoundError 해결 : opencv-python install pip install opencv-python   오류 : ModuleNotFoundError 해결 : easydict install pip install easydict - freeze_graph.py 실행​3) .pb 파일 가져오기- image_demo.py 실행   image_demo.py 실행 결과 - video_demo.py 실행   video_demo.py 실행 결과(1)   video_demo.py 실행 결과(2)  2. Train VOC dataset1) VOC PASCAL trainval and test data 다운로드http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tarhttp://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tarhttp://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtest_06-Nov-2007.tar​2) scripts/voc_annotation.py 실행- 결과   3) core/config.py 수정 __C.YOLO.CLASSES                = ""./data/classes/vos.names"" 4) train from COCO weights- convert_weight.py 실행   - train.py 실행   오류 : ModuleNotFoundError 해결 : tqdm install conda install -c conda-forge tqdmconda install -c conda-forge/label/gcc7 tqdmconda install -c conda-forge/label/cf201901 tqdm   - 결과    3. test and evaluate1) core/config.py 수정 __C.TEST.WEIGHT_FILE            = ""./checkpoint/yolov3_test_loss=43.2264.ckpt-1"" 2) evaluate.py 실행- 결과   3) main.py   오류 : ValueError 해결 : evaluate.py 수정 self.trainable: True ​ "
Object detection using deep learning (yolov3) ,https://blog.naver.com/savagegard_n/221742698585,20191219,"딥러닝기반 물체인식 수업의 결과를 공유합니다.yolov3을 이용하여 새로운 15개의 물체를 학습하고 인식했습니다.하나의 동영상이 4개로 분할되어 있습니다. 왼쪽 위에 부터 시계방향으로,원본, 기존 웨이트, 기존+맞춤 웨이트, 맞춤 웨이트를 가지고 물체 인식하는 영상입니다.얕게나마 딥러닝을 해보니, 저랑은 맞지 않다는 것을 느낄 수 있었고, 학습을 위해서 데이터셋을 만드는 데에 많은 시간과 노력이 소요되고, 또 결과를 예측하는 것이 초보자인 저에게는 쉽지 않아서 더 별로라고 생각하게 되었습니다. ㅎㅎ    첨부파일딥러닝발표자료_출력용1.pdf파일 다운로드 발표자료를 PDF로 변환한 파일을 올립니다. 거의 정보는 없지만, 혹시나 싶어 올립니다. "
[15] Improving visual relationship detection using linguistic and spatial cues ,https://blog.naver.com/jgyy4775/222569783323,20211116,"논문 링크 : https://onlinelibrary.wiley.com/doi/full/10.4218/etrij.2019-0093 Improving visual relationship detection using linguistic and spatial cuesDetecting visual relationships in an image is important in an image understanding task. It enables higher image understanding tasks, that is, predicting the next scene and understanding what occurs i...onlinelibrary.wiley.com ​<Introduction>이전 논문과 비슷하게 관계 탐지에 있어 자연어를 사용하고자 한 논문입니다. ▶ ‘관계’를 language, visual, spatial cue로 연관지어 생각     - language: monkey와 banana는 ‘wear’보다 ‘eat’이 더 적합함     - visual: 이미지 feature     - spatial: banana는 monkey의 손과 입에 더 가까이 있을 수 있음     => 세 가지 정보가 서로 다른 네트워크를 거치도록 설계▶ 보다 정교한 언어 공간벡터를 얻기 위해 노력​​<Model> 전체 구조도1. Object Detection- Faster R-CNN사용​2. 다양한 모듈들▶ Spatial Module주어 물체와 목적어 물체의 위치 정보 ▶Langauge Module물체 예측 클래스의 단어벡터 사용L loss와 K loss:  비슷한 종류의 단어들은 vector space에서 비슷한 위치에 놓이게 됨​▶Visual ModuleVGG-16을 이용하여 추출 3. Relationship Detection ​spatial module의 결과와 language+visual module의 결과를 합하여 최종 관계 클래스 결정​​​<Result>Dataset- Visual Genome- VRD​Metrics- Predicate detection: relation만 예측- Phrase detection: object class와 relation예측- Relationship detection: 물체의 영역, class, relation 모두 예측 ​<결론>- language, visual, spatial 정보를 모두 사용하는 관계 탐지 모델 제안- 비슷한 종류의 물체는 비슷한 vector space에 위치 시키기 위한 language module 제안​ "
Obeject Detection(객체 탐지) - 수업관련모음 ,https://blog.naver.com/85honesty/222760559035,20220605,"수업관련모음  [Reference]https://github.com/facebookresearch/Detectron2 GitHub - facebookresearch/detectron2: Detectron2 is a platform for object detection, segmentation and other visual recognition tasks.Detectron2 is a platform for object detection, segmentation and other visual recognition tasks. - GitHub - facebookresearch/detectron2: Detectron2 is a platform for object detection, segmentation a...github.com https://github.com/pytorch pytorchpytorch has 64 repositories available. Follow their code on GitHub.github.com https://github.com/facebookresearch/detectron2/blob/main/README.md detectron2/README.md at main · facebookresearch/detectron2Detectron2 is a platform for object detection, segmentation and other visual recognition tasks. - detectron2/README.md at main · facebookresearch/detectron2github.com https://ai.facebook.com/tools/detectron2/ Detectron2Detectron2 is FAIR's next-generation platform for object detection and segmentation.ai.facebook.com https://github.com/tensorflow/hub/blob/master/examples/colab/tf2_object_detection.ipynb hub/tf2_object_detection.ipynb at master · tensorflow/hubA library for transfer learning by reusing parts of TensorFlow models. - hub/tf2_object_detection.ipynb at master · tensorflow/hubgithub.com https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2.md models/tf2.md at master · tensorflow/modelsModels and examples built with TensorFlow. Contribute to tensorflow/models development by creating an account on GitHub.github.com https://github.com/tensorflow/models/blob/master/research/object_detection/colab_tutorials/eager_few_shot_od_training_tf2_colab.ipynb models/eager_few_shot_od_training_tf2_colab.ipynb at master · tensorflow/modelsModels and examples built with TensorFlow. Contribute to tensorflow/models development by creating an account on GitHub.github.com https://github.com/tensorflow/models/blob/master/research/object_detection/colab_tutorials/eager_few_shot_od_training_tf2_colab.ipynb models/eager_few_shot_od_training_tf2_colab.ipynb at master · tensorflow/modelsModels and examples built with TensorFlow. Contribute to tensorflow/models development by creating an account on GitHub.github.com https://tfhub.dev/tensorflow/collections/object_detection/1 TensorFlow Hubmenu search Send feedbacktfhub.dev https://colab.research.google.com/github/tensorflow/hub/blob/master/examples/colab/tf2_object_detection.ipynb#scrollTo=V1UUX8SUUiMO Object Detection Inference on TF 2 and TF HubRun, share, and edit Python notebookscolab.research.google.com https://stackoverflow.com/questions/69151313/demo-needs-opencv-for-webcam-images-opencv-is-installed-and-set-opencv4-1 Demo needs OpenCV for webcam images.(opencv is installed and set opencv4=1)I am trying to do object detection from a video file by using https://github.com/pjreddie/darknet. I've installed libopencv-dev for opencv. I've set opencv4=1 in Makefile. And run this code. ./dark...stackoverflow.com https://stackoverflow.com/ Stack Overflow - Where Developers Learn, Share, & Build CareersStack Overflow | The World’s Largest Online Community for Developersstackoverflow.com  dailyman데일리남매일매일 좋은 추억 한두개씩 함께 만들어요😀 선한영향력 긍정적인변화 즐거운하루 같이 가치있게 만들어 가요😄www.youtube.com  구독과 공감 좋아요는 콘텐츠를 만드는 데 큰 힘이 됩니다. "
[16] Deep Variation-structured Reinforcement Learning  for Visual Relationship Detection ,https://blog.naver.com/jgyy4775/222570867502,20211117,"논문 링크 : https://openaccess.thecvf.com/content_cvpr_2017/papers/Liang_Deep_Variation-Structured_Reinforcement_CVPR_2017_paper.pdf​Github : https://github.com/nexusapoorvacus/DeepVariationStructuredRL GitHub - nexusapoorvacus/DeepVariationStructuredRL: A PyTorch implementation of the ""Deep Variation-structured Reinforcement Learning for Visual Relationship and Attribute Detection"" paper by Liang et. al.A PyTorch implementation of the ""Deep Variation-structured Reinforcement Learning for Visual Relationship and Attribute Detection"" paper by Liang et. al. - GitHub - nexusapoorvacus/DeepVa...github.com ​​<Introduction>이 논문은 강화학습을 이용하여 scene graph 생성하는 모델인  ""Deep variation-structured Reinforcement Learning(VRL)""를 제안한 논문입니다. 또한, 관계만 예측하던 다른 연구들과는 다르게 물체의 속성도 함께 예측합니다. ▶강화학습 알고리즘 중 하나인 Deep Q-Learning을 사용하여 장면 그래프를 생성합니다.   => 주어 물체의 속성 예측(color,  shape,  pose)   => 목적어 물체 예측 (people, places, parts of objects.)   => 주어 물체와 목적어 물체의 관계 예측 (spatial,compositional, action)▶효과적인 state vector추출법 제안​​​<Model> 전체 구조도1) Object Detection- Faster R-CNN 이용- S: object instance-각 object의 confidence score-> 초기 정보들을 바탕으로 relation과 attribute 분류​2) Directed Semantic Action Graph 데이터 집합으로 부터 가능한 물체의 클래스, 집합, 관계들을 미리 사전에 저장하여 그래프로 생성한 것 ​3) Variation-structured RL-Variation-structured action space. - Q-Learning 사용앞서 추출된 attribute, relation, object를 이용해 Q-learning ​​<Result>Dataset- Visual Genome- VRD​Metrics- Predicate detection: relation만 예측- Phrase detection: object class와 relation예측- Attribute detection VRD 데이터 집합을 이용한 실험 결과VG 데이터 집합을 이용한 실험 결과​​<결론>- 강화 학습을 이용한 장면 그래프 생성 방법 제안- 물체, 관계 뿐만 아니라 속성까지 함께 탐지하는 모델 제안​​​ "
Object Detection in Image ,https://blog.naver.com/rlawoals93/221536366944,20190513,사전에 훈련된 모델을 가지고 object를 감지다음 할일은 내가 직접 모델 훈련시키기​ Object Detection in Image원본 출처:www.notion.so     ​ 
NXT Youtube 영상_Object detection with on-camera AI  ,https://blog.naver.com/idsimagingkorea/221988462191,20200603,"#Smartcamera #AI #NXT #IDS Korea 이 비디오에서는 AI 기반 이미지 프로세싱에 대한 정의와 함께 객체 분류, 탐지와 같은 딥러닝 작업을 다룹니다. 또한 산업용 카메라에 대한 딥 러닝의 작동 방식과 새로운 기술로 어떤 어플리케이션을 보다 편리하게 해결할 수 있는지 설명하고 있습니다. https://www.youtube.com/watch?v=QtmbixnVIik ​ "
1. DERT:End-to-End Object Detection with Transformers ,https://blog.naver.com/codingzoa/222835587416,20220731,Temp1-1대응으로 더 쉽게 병렬적으로 처리가 가능하다?Transformer에 대한걸 찾아봐야겠다. 
[퍼옴] 3D Object Detection from LiDAR Data with Deep Learning ,https://blog.naver.com/hyungjungkim/222423500303,20210707,기억을 위해 기록...​ 3D Object Detection from LiDAR Data with Deep LearningAuthor: Alex Naslismartlabai.medium.com ​ 
[라즈베리 파이 Object Detecion #1] 파이썬 이미지 크롤링 하기 ,https://blog.naver.com/gjrqkqkqk/223049506830,20230319,"4학년이 되어서 캡스톤 디자인으로 캔 위성을 만들게 되었다.주제는 캔 위성에 달린 카메라를 이용해 위험 요소를 탐지해서 재난 예방에 활용하는 것.이를 위해서 Object Detection 알고리즘을 사용하기로 했다. 필자도 관심은 가지고 있었으나 처음 써보는 알고리즘인데 나중에 다시 쓸 때 활용도 할 겸 블로그에 과정을 게시할 생각이다.인터넷을 찾으면 자료가 나와있긴 한데 제대로 정리가 안 돼있고 특히 한국어로 친절하게 설명되어 있는 글은 거의 없다.필자는 라즈베리 파이의 라즈비안 환경에서 작업했고 tensorflow lite를 사용했다. (tensorflow lite를 이용하면 좀 더 높은 프레임이 나온다!)다른 작업환경을 이용할 사람들은 어떤 과정을 거쳐 Object detection이 수행되는지 정도만 확인하고 다른 자료를 찾아봐야 한다.​Object detection을 수행하기 위해서 아래와 같은 세 단계를 거칠 것이다.​필요한 이미지 크롤링 하기 (데이터 수집)이미지를 라벨링하고 학습 모델을 만들기 (데이터 전처리)모델을 학습시키고 테스트하기 (데이터 학습, 테스트)​1, 2 단계는 windows 환경에서 진행해도 되고 3단계는 라즈비안 환경에서 진행할 것이다.여기서 라벨링이란 이미지 데이터의 객체(물건)에 이름을 지정해 주는 작업이다. 이와 같이 이미지의 객체에 이름표(라벨)을 달아주는 작업이라고 생각하면 된다.​하여튼 이번 포스팅에서는 첫 번째 이미지 크롤링 하기부터 수행하겠다!이미지 크롤링을 하는 데에는 다양한 방법이 있다. 이번에는 그중 파이썬의 selenium이라는 라이브러리를 활용해 보겠다.파이썬은 깔려있다고 가정하겠다.  step.1 라이브러리 설치​먼저 cmd를 켜고 pip install selenium을 입력한다.​step.2 사용할 브라우저 드라이버 다운로드​사용할 인터넷 브라우저를 선택해야 한다. 필자는 크롬을 선택하겠다.구글에 chromedriver를 치고 처음 나오는 링크에 들어간다.  그 후 크롬 브라우저 오른쪽 위의 메뉴 > 도움말 > chrome 정보를 클릭하여 본인의 크롬 버전을 확인한다. 버전이 111.~~~이다.내 크롬 버전에 맞는 버전을 다운로드한다.  그럼 아래와 같은 파일이 다운로드되는데 이를 자신이 원하는 경로에 옮겨준다. 그 후 같은 경로에 imagecrawling.py 파일을 만들어준다. 이렇게 드라이버와 .py 파일이 같은 경로에 있어야 함step.3 코드 입력, 실행 from selenium import webdriverfrom selenium.webdriver.common.keys import Keysfrom selenium.webdriver.common.by import Byimport timeimport urllib.request# 검색어, 스크롤 대기 시간, 클릭 대기시간 설정# 대기시간은 인터넷이 느리거나 컴퓨터가 느릴수록 길게 설정해줄 것# 대기시간이 짧을수록 검색 속도는 빨라짐, BUT 에러가능성 높음 # AMOUNT는 검색량, 클수록 많이 검색한다name=""로켓""SCROLL_PAUSE_TIME = 1.5CLICK_PAUSE_TIME= 2AMOUNT=3driver = webdriver.Chrome()driver.get(""https://www.google.co.kr/imghp?hl=ko&ogbl"")elem = driver.find_element(By.NAME, ""q"")elem.send_keys(name)elem.send_keys(Keys.RETURN)# Get scroll heightNOWAMOUNT=0last_height = driver.execute_script(""return document.body.scrollHeight"")while NOWAMOUNT<AMOUNT:    # Scroll down to bottom    driver.execute_script(""window.scrollTo(0, document.body.scrollHeight);"")    # Wait to load page    time.sleep(SCROLL_PAUSE_TIME)    # Calculate new scroll height and compare with last scroll height    new_height = driver.execute_script(""return document.body.scrollHeight"")    if new_height == last_height:        try:            driver.find_element(By.CSS_SELECTOR,"".mye4qd"").click()            NOWAMOUNT+=1        except:            break    last_height = new_height#검색한 이미지를 리스트로 만들기images=driver.find_elements(By.CSS_SELECTOR,"".rg_i.Q4LuWd"")#이미지를 돌아가면서 클릭해 다운로드하기count=1for image in images:    try:        image.click()        time.sleep(CLICK_PAUSE_TIME)        imgUrl=driver.find_element(By.CSS_SELECTOR,'.n3VNCb').get_attribute(""src"")        opener=urllib.request.build_opener()        opener.addheaders=[('User-Agent','Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1941.0 Safari/537.36')]        urllib.request.install_opener(opener)        urllib.request.urlretrieve(imgUrl,name+str(count)+"".jpg"")        count+=1    except:        passdriver.close() imagecrawling.py 파일에 위를 복붙한다. 11번째 줄부터 원하는 검색어, 대기시간, 검색량을 설정할 수 있다. 그리고 실행을 하면,  위와 같이 파이썬이 알아서 원하는 이미지를 크롤링 한다!​크롤링 된 이미지는 인공지능 학습을 위한 데이터로 사용될 것이다.이 글을 읽고 따라 할 때 주의할 점은, 지금으로부터 시간이 지나있다면 위 코드가 작동하지 않을 확률이 매우 높다는 것이다.위 프로그램은 구글의 html 코드로부터 이미지를 얻어오는데 html 코드의 상세한 설정값은 구글의 보안 환경 변화나 업데이트에 따라서 항상 변할 수 있기 때문이다. 웹에서 데이터를 크롤링 하거나 스크레이핑할 때는 한 프로그램을 만들어놓고 오랫동안 우려먹을 수 없다는 말이다. 또 라이브러리의 버전이 업데이트되거나 크롬 드라이버의 버전이 다르거나 등등의 이유로 사용해야 하는 어떤 함수가 사라지거나 변할 수도 있다.그러니 이미지 크롤링이 잘 작동하도록 하고 싶다면 이 글의 내용의 출처인 아래 유튜브 영상을 보고 어떤 식으로 프로그램이 작동하는지 파악하고 추가적인 정보는 검색해서 찾아보는 것이 좋다.​추가) 한국어로 검색할 경우 초반 한~두 페이지 정도만 정확한 검색 결과를 제공하고 그 후로는 검색어와 상관없는 이미지가 많이 나온다. 그러니 만약 로켓 이미지를 크롤링 하고 싶다면 '로켓','로켓 사진','로켓 이미지','Rocket','Rocket Picture' 등으로 검색어를 바꿔가며 AMOUNT는 3 정도로 설정해 크롤링을 반복하는 것을 추천한다.​https://www.youtube.com/watch?v=1b7pXC1-IbE&t=0s 다음 시간에는 이미지 라벨링과 모델을 만드는 법을 배워보겠다! "
병변 검출 AI 경진대회 (Object Detection) ,https://blog.naver.com/yesjerry/222790549412,20220627,"https://dacon.io/en/competitions/official/235855/codeshare/3829 Barcelona팀, Private 3위, Private 점수: 0.82661, 앙상블병변 검출 AI 경진대회dacon.io ​ "
[object detection] R-CNN 정리 ,https://blog.naver.com/lsj952005/222603698101,20211228,​​ ​ 
[논문 훑어보기] PseCo: Pseudo Labeling and Consistency Training for Semi-Supervised Object Detection ,https://blog.naver.com/ckgn316/222939861612,20221128,​논문 원본) Papers with Code - PseCo: Pseudo Labeling and Consistency Training for Semi-Supervised Object Detection#3 best model for Semi-Supervised Object Detection on COCO 100% labeled data (mAP metric)paperswithcode.com https://paperswithcode.com/paper/pseco-pseudo-labeling-and-consistency​​https://youtu.be/C-NVH6StFQw ​ 
Object detection using Yolov3  with Customdataset  ,https://blog.naver.com/ehdgml9997/221768913877,20200112,"1. Crawling datahttps://github.com/hardikvasa/google-images-download hardikvasa/google-images-downloadPython Script to download hundreds of images from 'Google Images'. It is a ready-to-run code! - hardikvasa/google-images-downloadgithub.com my custom dataset : radioactivity sign , door knob, industry valve​2. Rename the dataset​https://software.naver.com/software/summary.nhn?softwareId=GWS_000169 파일 이름을 한꺼번에 바꿔주는 프로그램software.naver.com 3. Resize the dataset to 450x450​https://www.iloveimg.com/ko iLoveIMG | 쉽고 빠른 온라인 무료 이미지 편집 툴신속하게 파일 수정이 가능한 무료 이미지 편집 툴 iLoveIMG. 잘라내기, 크기 조정, 압축, 변환 등의 작업을 진행해 보세요!www.iloveimg.com 4. txt labeling & create cfg, data, names, train,test txt files​5. modify YOLOv3 algorithm with the number of custom data​6. train the algrorithm with GPU server.​7. test "
[딥러닝 이전의 영상인식][논문 리뷰] Histogram of Oriented Gradients for Human detection ,https://blog.naver.com/koreadeep/222588894193,20211207,"​이번 포스팅에서는SVM 기반 Machine Learning으로 Pedestrian Detection(보행자 인식)이라는 Application을 구현한 논문인 “Histogram of Oriented Gradients for Human Detection”(HOG)을 리뷰해 보도록 하겠습니다.​논문은 https://lear.inrialpes.fr/people/triggs/pubs/Dalal-cvpr05.pdf에서 보실 수 있습니다.​ Pedestrian Detection먼저 Pedestrian Detection 은 아래 그림과 같이 영상에서 걸어가거나 서있는 모습의 사람을찾아내어 Bound Box로 표시하는 것을 말합니다.​ HOG 논문은 컴퓨터 비전(Computer vision) Object detection의 시초가 되는 논문으로Histogram of oriented gradient(이하 HOG)라는 통계적 기법에 의거한 Feature를 제안한 것과이 Feature들을 머신러닝과 접목하여 괄목할 만한 성과를 이뤄냈다는 점이학계에 큰 긍정적인 영향을 끼쳤습니다. ​ Gradient 계산영상에서 Gradient 즉, 기울기를 구하기 위해서는 Edge를 먼저 계산해야 합니다.기울기는 미분을 통해 구할 수 있는데, 우리가 알고 있는 미분의 정의식은 아래와 같습니다. ​ 2차원 자료구조인 영상에서는 x 방향, y 방향으로 나누어 각각 미분식을 적용해 주면 됩니다. 적용하는 방법은 미분 Filter를 이용하는 것입니다.​ 미분 filter 위 그림 왼쪽의 Filter mask를 오른쪽 그림의 7에 각각 덮어봅시다(Masking)그리고 3x1, 1x3 벡터를 각각 내적 연산을 합니다.즉, x 방향 증분 dx 값은-1*6 + 0*7 + 1*7 = 1y 방향 증분 dy 값은-1*3 + 0*7 + 1*8 = 5입니다. Magnitue , Orientation 계산두 번째 단계로 gradient 값들을 이용하여 새로운 Factor로 representation 합니다.그 값들은 Magnitude와 Orientation입니다.  Magnitude는 각 방향에 대한 증분의 Euclidian distance로 정의합니다.위의 예시에 따르면 dx = 1, dy = 5이므로 Magnitude 값 s는 약 5.1 이 되는 것입니다.​Orientation은 각 증분이 이루는 각을 arctan라는 삼각함수를 이용하여 계산합니다.마찬가지로 arctan(5/1) 계산을 통해 두 증분이 이루는 각 θ는 약  78.7도라는 것을 유추할 수 있습니다. Cells, Blocks 정의다음 단계는 데이터 구조 설계입니다.HOG 논문에서는 96 x160 크기의 이미지를 사용했습니다.이 논문에서 정의한 데이터 구조로는 Cell이라는 개념과 Block이라는 개념이 있으며,Cell 크기는 8 x 8로 정의했습니다.따라서 한 이미지 안에는 12x20 = 240 개의 cell 이 존재하게 되어있습니다.​한편,Block의 크기는 16 x 16으로 정의했습니다.이렇게 4배 큰 데이터 구조를 별도로 정의한 이유는아래 그림의 Block1 , Block 2가 50% 겹쳐져 (overlap) 있는 것처럼 Overlap을 적용하기 위함입니다.이처럼 Block은 50% overlap 구조로 정의를 하여총 11 x 19 = 209개가 정의되었습니다. Cell과 Block​ Spatial / Orientation Binning and Voting항목구성요소Block2x2 CellCell8x8 pixelPixelmagnitude, orientation 각 Block은 2x2개의 cell을 포함하고 있고,각 cell은 8x8 크기로 이루어져 있으며,각 픽셀들은 저마다 magnitude 와 orientation 값을 갖고 있습니다.이제 0 ~ 180도의 각도 구간을 9등분 하여 각 픽셀들이 어느 구간에 속하는지투표(voting)를 하여 histogram을 생성합니다. ​예를 들어, 어떤 포인트의 orientation 값이 85도였다면,Median(중앙값)이 70인 구간, Median이 90인 구간과 차이를 반영하여선형적으로 voting이 될 수 있게끔 하였습니다. magnitude 값은 이 ratio에 곱하여 histogram에 반영하였습니다.​​ 모든 cell에 대해 크기가 9인 벡터 histogram 을 생성한 후,네 개의 cell을, 50% overlap 형태로 묶어 209개 block으로 재구조화합니다.​​ 결론적으로 한 block이 크기 36인 vector로 표현되는 것이고,전체 이미지는 크기 7524인 벡터로 representation( 209 blocks x 4 cells x 9 bins) 되는 것입니다. ​ Visualize위 그림은 각 image patch들 위에 gradient 정보를 visualize 해본 것입니다.사람의 shape가 반영되어 gradient 들이 그려져 있는 것을 볼 수 있습니다.어깨 부분에는 horizontal 한 정보들이 많고,다리 부분에는 vertical 한 정보들이 많이 보이는 것을 확인할 수 있습니다. Training with SVM model본 논문에서는 총 2426개의 데이터를 training에 사용했습니다. Binary classification그중 1208개는 사람이 있는 positive data, 1218개는 사람이 없는 negative data였습니다.Binary classification(이진 분류)이죠.​ 위에서 설명드렸듯, 이미지가 입력으로 들어오면학습된 모델은 이 이미지를 크기 7524인 벡터로 representaton 하게 됩니다.​이 벡터를 입력으로 하여 Linear SVM 학습을 진행했습니다. Result성능 측정은  MIT와 INRIA data set에 대해 다양한 detector 들과 비교하였습니다.​ HOG 기반 detector는 wavelet, PCA-SIFT 및 Shape context보다 월등한 성능을 나타냈습니다.특히  MIT 테스트 세트에서 굉장히 높은 성능을 보였으며, INRIA에서도 타 detector에 비해 FPPW(오검출율)을 최소화하는 것을 확인하였습니다.​ Conclusion이상으로 SVM 기반 영상인식 논문인 HOG를 리뷰해 보았습니다.​영상의 Gradient 정보를 추출하여 통계적으로 분석하여 의미 있는 Feature를 만들어냈다는 점과 이 Feature를 Machine Learning 모델인 SVM 을 통해 학습하여 의미 있는 결과를 도출해낸 것이  이 논문의 기여라고 볼 수 있습니다.​물론 이 논문의 접근 방식은 딥러닝으로 패러다임이 크게 바뀐 지금과는 많은 차이가 있습니다.하지만 이 논문에서 복잡한 과정을 거쳐 data representation을 하는 과정을컴퓨터에 맡기게 되는 것이 바로 '딥러닝'의 시작이라고 생각할 수 있습니다.​​​​다음 포스팅에서는 기존의 접근 방법과의 차이에 주목해 보면서,'LeNet' 논문을 리뷰해 보도록 하겠습니다감사합니다:) ​​ ​  한국딥러닝(주)는 인공지능, 머신러닝, 딥러닝 솔루션을 개발하여 세상이 해결할 수 없는 문제를 해결하고 제안하고 있습니다. 끊임없이 발생되는 새로운 빅데이터와 응용 기술의 홍수 속에서 최신 인공지능 기술 동향을 예측하고 빠르게 반영하는 한국딥러닝을 만나보세요.​ ​ "
predicting coordinate by using regression for object detection 72 ,https://blog.naver.com/wonnho71/222542075548,20211019,​ 
[DMQA Open Seminar] Scene Text Detection and Recognition 참고 및 정리 ,https://blog.naver.com/kovtjw/222656762696,20220224,"​https://www.youtube.com/watch?v=_4CFxre4b1s ​1. Introduction 1) what is Scene tet detection and recognition- 일상적인 이미지에서 글자가 있는 영역을 탐지하고, 이를 컴퓨터 문자로 변환하는 것- 이미지 번역, 차량 번호판/이정표/명함 인식, 이미지 검색 등에 실생활에서 다양하게 활용 됨  2) what is different from traditional OCR- 고전적인 OCR은 종이 문서 등에 인쇄된 문자를 읽어냈다. > OCR보다 복잡도가 높아 정교한 모델이 필요하다. ​ 3) Challenge of Scene text detection and recognition- 3가지 주요 어려움 비스듬 하거나 회전된 단어 탐지서로 겹쳐져 있는 문자 판별곡선형으로 나열한 단어​- detection format 예시 RECT : 일반적인 object detection의 형태, 왼쪽 상변에 x,y좌표와 가로 세로를 출력한다.(X,Y,W,H)RBOX : 직사각형에 각도가 더해진 형태 (X,Y,W,H, 각도)QUAD : 포괄적 사각형 형태, 꼭지점이 어디에 찍히든 사각형의 조건만 맞추면, 형태의 박스로 탐지할 수 있다.POLY : 여러 쪽지점을 갖고 있는 다각형 형태​ 3) scene text detection vs recognition vs End-to-end text recognition이미지에서 글자가 위치한 '영역'을 탐지하는 Detection영역별로 잘라 영역 내 글자를 읽어내는 RecognitionDetection과 Recognition을 한꺼번에 수행하는 End-to-end scene text recognition(text spotting이라고도 함)​ ​2. Scene text detection​1) Basic of scene text detection- 글자가 위치한 bounding box의 좌표를 최대한 정확히 맞추는 것이 목표이기 때문에 회귀 문제로 접근 한다.- 글자 영역을 Region proposal 또는 Region of Interest(ROI)라고 한다. 픽셀의 index를 좌표라고 부른다.  - 연산 과정 : Input Image -> cnn (특징 추출) -> 압축 - > decoder 통과 -> 단어 영역 생성 ​2) EAST : An Efficient  and Accurate Scene Text Detector- Text detection 모델들이 3~5차례 Convolution 블록을 거치게 한 것과 달리 하나의 Convolution 블록으로 줄여 연산 시간을 대폭 단축한다.- 이미지 분할을 위해 고안된 Fully Convolutional Network 알고리즘을 활용하여, 단어가 포함된 Rotated rectangle 또는 Quadrilateral box를 예측한다.- EAST 모델의 구조​3) Structure of EAST model Input image가 convolution을 통해서 채널이 들어나고 피쳐맵의 사이즈가 줄어드는 Encode과정을  거쳐서 위 그림의 1/32 feature map을 추출한다. 그 이후 feature map이 deconvolution 과정을 거치게 되는데, 사이즈가 다시 커지면워 원본의 Image 사이즈로 돌아가는 Decoder 과정을 진행한다. Decoder 과정이 끝난 후 처음의 1/4 사이즈 정도 되는 score map이 추출이 된다 . output(score map)은 우리가 원하는 5가지 정보(x1, y1, w, h, 각도)인 '회전된 다각형의 박스' 의 좌표 등을 구하기 용이하게 해준다. ​4) Fully Convolutional Network(FCN)  - FCN은 기존 semantic Segmentation을 위해서 제안이 된 모델이었다. - 하나하나의 픽셀을 어떤 클래스에 속하는지 분류를 해야 한다. > ex) 이 픽셀이 고양이 인지, 강아지 인지 - 모든 픽셀을 분류 모델에 투입해 구분 할 수도 있지만 매우 비효율적이다.​- FCN은 기존 CNN에서 마지막 Fully Connected layer를 거치기 전 위치 정보가 보존된 feature map을 활용해 Segmentation map으로 복원 하는 모델이다.  특징을 압축할 때 과정과 정반대로 복원 생성해 내야 하기 때문에  Unpooling, Transposed convolution 등을 사용한다. ​5) Output of EAST model- score map을 바탕으로 5개 정보가 출력된다. 이미지 사이즈 내에 있는 픽셀 하나하나마다 값을 매겨준다. 값을 이용해서 최종적으로 취합을 하고 좌표를 구한다.  - (b) : 각 단어 영역을 어느 정도 추정한 다음에 각 픽셀이 단어 영역 내에 있을 확률을 0~1 사이로 부여하게 된다.값이 높을수록 이 영역은 단어가 있을 확률이 높다 이것만으로 박스를 만들 수는 없다. 그렇기 때문에 (b)에 해당되는 4가지 정보(d, 단어 box 추정 후 각 픽셀과 box 4개 변 사이의 거리 정보) 를 각 4개 채널에 담게해준다. 단어 박스를 추정한 다음에 각 채널하나마다 값이 부여가 되고,  (c)의 거리가 높다는 것은 네개의 변과 모두 거리가 멀다는 것이기 때문에4가지 값이 모두 높게 나타나면 단어 영역의 중심일 확률이 높게된다. 이 중심 정보를 가져와서 (e)를 통해 얼마나 기울어져 있는지 확인을 하고 각도를 저장하게 된다.​총 5개의 정보를 저장하게 된다. 저장된 정보가 임계치를 넘으면, 이 부분을 우리가 박스로 확실하게 추정할 수 있다고 보고 레이블을 예측하게 된다. 실제 레이블과 예측 레이블 사이에 차이가 있을 텐데 그 부분 LOSS를 개선해서 모델 학습이 이루어지게 된다. 실제 레이블을 바탕으로도 아까 우리가 사용했던 5개의 정보를 추출할 수 있지만 두 가지를 비교해서 로스를 개선하게 된다.​ ​Ls = 예측과 실제의 score map 차이 뒤에 항 Lg = b에 있는 loss 값이 5개의 정보에 대한 손실을 더한 것이 두 가지의 로스를 합쳐서 최종적인 로스를 개선하게 된다. ​6) Experiment result ​3. Scene text recognition​1) Basic of Scene text recognition- 각 단어 영역이 어떤 문자인지 찾는 Classification 문제- 단어 영역에 해당하는 이미지로부터 특징 추출 후 sequential하게 만들어 각 글자의 조합(단어)을 찾아가는 방식 ​2) Why RNN?- 이미지마다 글자의 크기, 배치가 다르기 때문에 글자 단위로 정확하게 나누는 것은 어렵다.- feature map의 부분적 정보만을 이용해 글자를 예측하려면 앞뒤의 다른 정보를 종합적으로 고려해야 한다.ex) m의 일부는 i 일 수도 l일 수도 있지만 시퀀스 내 다른 정보를 고려해 m으로 판단 가능하다.- 최근에는 Attention model을 활용한 text recognition 모델도 주목받고 있다. - 시퀀셜 데이터로 나누고, 처리도 시퀀스를 반영할 수 있는 RNN계열의 모델을 활용하게 된다. ​3) Structure of CRNN modelstep 1. Convolution 연산을 통해 특징 추출 step 2. 추출된 feature map을 열 벡터 단위로 나눠 시퀀스 형태로 변환step 3. 길이가 긴 시퀀스 입력을 gradient vanishing 문제 없이 처리 할 수 있는 BI -LSTM 모델을 이용해, 각 벡터에 대한 글자 예측값 출력step 4. CTC 알고리즘을 통해 중복 문자와 공백 등 제거한 후 최종 ​ 4) Connectionist Tmeporal Classification(CTC)- CTC 알고리즘은 기존 음성 인식 모델에서 사용했었다. sequence 분류 모델에 맨 마지막에 위치하고, 손실과 gradient 계산을 조정하면서 일종의 sequence 예측 값을 합춰준다.- Q1. 'hhe--lll-lo'는 'hellllo'인가? 'hhello'인가? 'hello인가?- Q2. 'hello'라면 몇 번째 h, l을 사용 할 것인가?- 어떤 출력값을 선택/제거 할지에 대한 정보가 없는 상황에서 정답 레이블을 만드는 모든 경우의 수에 대해 분류 확률을 최대화 하도록 모델을 학습한다.​ 3. End-to-end Scene Text Spotting with a Unified Network​1) FOTS : Fast Oriented Text Spotting with a Unified Network- EAST와 CRNN 모델을 하나로 합친 모델, 다만 각 모델을 단순히 이어 붙인 것이 아닌 한 번의 특징 추출로 detection과 recognition을 수행함으로써, 연산 시간을 크게 줄임​2) Structure of FOTS model- ROI Rotate 블록이 두 작업(EAST, CRNN)의 연결 고리로, Text detection 에서 출력된 단어 영역을 수평으로 회전 시키고 같은 높이로 변환하여 Text recognition에 입력시켜 준다. ​3) Shared Convolutions- Detection과 recognition을 End-to-end 방식으로 수행 할 경우 각 작업에 쓰이는 정보를 교차로 활용할 수 있어서 따로 할 때보다 성능이 올라간다. ​ "
"자율주행/ADAS의 핵심, OD(Obeject Detection, 객체인식)를 알아보자! (스트라드비젼-SVNet) ",https://blog.naver.com/stradvisionary/222246882964,20210217,"안녕하세요, 스트라드비젼입니다!저희는 ADAS(첨단운전자보조시스템) & 자율주행 기술을 구현하기 위한 딥러닝 기반 사물 인식 소프트웨어를 개발하고 있습니다.​ OBJECT DETECTION 객체인식오늘은 자율주행의 핵심인 OD(Object Detection)을 소개해드립니다.Object Detection은""사물 인식"" 또는 ""객체 인식""이라고도 불리며,말그대로 카메라를 통해 들어온 영상 및 이미지 데이터에서사물을 인식해내는 기술을 의미합니다.​객체인식 기술이 자율주행에 필요한 이유는,주행 시 보게 되는 사물들을 정확히 인지해야 하기 때문입니다.예를들어, 주행 중에는 차량 주변에 있는다양한 차량,보행자, 동물을 인식해야 사고없는 안전한 운전이 가능합니다.또한, 신호등, 차선, 교통표지판 등의 인식이 가능해야도로 법규에 따라 운전을 할 수 있겠죠? 객체인식은 동물 인식, 차선/차량 인식,주행가능공간 인식, 주차가능구역 인식, 보행자/신호등/표지판 인식 등​주행환경에서 객체를 실시간으로 인식하는 기술입니다.​이를 위해 카메라, 라이더(Lidar), 레이더(Radar) 등의다양한 센서가 활용되지만그 중에서도 스트라드비젼은 카메라 센서를 중심으로 운전자의 눈과 같은 역할을 할 수 있는딥러닝 기반의 인식 소프트웨어를 개발하고 있습니다.​스트라드비젼 SVNet의 객체인식 기능을 더 잘 이해할 수 있는유튜브 영상을 같이 보실까요?​https://youtu.be/5W04LRTD0G8 이 영상에서는,- 비오는 날 - 밤 도로 환경- 눈 내린 도로 환경- 시내도로등의 다양한 주행 조건 하에서 객체인식을 해내는 SVNet의 기능을 확인할 수 있습니다.​ 비오는 날의 주행 영상을 보시면차량 - 붉은색 박스신호등 - 연두색 박스교통 표지판 - 노란색 박스차선 - 보라색 선또한, 차량 밑의 숫자를 통해 자차와 각 물체간의 계산된 거리값을 알 수 있답니다.​안전한 자율주행을 위한인식 소프트웨어를 개발하는스트라드비젼의 더 많은 영상을 보고 싶으시면유튜브 채널을 방문해주세요 :)​국문채널 ▼https://www.youtube.com/channel/UCnohRJDaRVNDn2j-Kjw4hHw 스트라드비젼 공식 유튜브채널스트라드비젼은 2014년에 설립된 딥러닝 기반 자율주행용 소프트웨어를 개발하는 국내 스타트업입니다. 서울, 포항, 산호세, 도쿄, 뮌헨 등 글로벌 사업망을 운영중이며, 실제 양산에 성공하여 전 세계 약 9백만에 차량에 스트라드비젼의 소프트웨어 SVNet이 탑재 될 예정입니다. 2020 Autonomous Vehicle Technology ACES Award 자율주행 소프트웨어 부문 수상 ISO 9001:2015 인증 획득 ISO 27001:2013 인증 획득www.youtube.com 영문채널 ▼https://www.youtube.com/channel/UCNlK8eQSlotdRfGi344UeXg StradVisionFounded in 2014, StradVision is an automotive industry pioneer in AI-based vision processing technology for Advanced Driver Assistance Systems (ADAS). The company is accelerating the advent of fully autonomous vehicles by making ADAS features available at a fraction of the market cost compared with ...www.youtube.com ​ "
Cs231n_L11_Detection and Segmentation ,https://blog.naver.com/asdjklfgh97/222186168289,20201226,"11강에서는 Segmentation ,localization, detection에 대해 다룬다.  여태 배운 내용은 Image classification에 대해 배웠다. CNN에서는 그 외에도 Semantic segmentation ,classification + localization , object detection, instance segmentaion 등이 있다. ​ pixel 단위로 classification 하기 때문에 붙어 있는 두마리 소를 한 마라로 인식하는 문제가 있었다. ​ 이렇게 부분을 잘라 옆으로 sliding 하며 , 중심 픽셀의 라벨을 예측 하는 방법이 제시 되었으나, 매우 비효율적이라고 한다.  여러 문제들을 통해 나온 방법이다. Downsasmpling과 upsampling을 해서 high-res 상대로 복구한다. spatial resolution이 계속 증가하는 방식이라고 한다. 또, 다운샘플링 이후 processing이 일어나므로 효율적이라고 한다.  upsampling은 Transpose convoulution으로 진행한다. 기존에는 이렇게 down sampling  됬었다.  Transpose convolution을 진행하면, 1개의 픽셀에다 벡터값을 곱해서 여러 값을 산출한다. 그리고 stride를 움직이는데 이런 과정에 image의 크기가 커진다. 필터를 거치며 사이즈가 커지는 것을 눈으로 볼 수 있다. az+bx와 같이 겹치는 부분에서는 그냥 덧셈 연산만 한다고 한다.  다음은 Classification + Localization이다.  객체의 bounding box를 localization하고, 객체를 classification 해야하므로 Multitask Loss를 학습한다. 두 로스의 균형을 맞추는 작업이 필요하다고 한다.   다음은 object detection이다.  앞에서도 나왔듯, 매번 crop과 그걸 sliding 하는 방식은 비효율적이라고 했었다.  그래서 쓰는 것이 Region proposal이다. object가 있는 부분을 먼저 proposal을 받는다고 한다.  R-cnn은 fixed wat region propoal network를 사용 , conv로 process한다.  문제점은, Region proposal 이 너무 오래걸린다는 것.  그래서 나온 것이 Fast r-cnn 으로, conv로 나온 feature map을 얻어 이후 region proposal을 진행.  그런데 이것 역시 region proposal의 점유 시간이 너무 높은 것을 알 수 있다.  그 이후 나온 것이 Faster r-cnn이다. 처음 conv 에서 proposal을 학습 시킨다. 그래서 region proposal에서 발생하는 비효율적인 문제가 해결된다.  Dectection without proposal, region proposal을 쓰지 않는 방법이라고 한다. Faster r cnn이 더 높은 정확도를 가지긴 하지만, 이 방식은 각 region에 대해 독립적으로 처리하지 않기 때문에 훨 빠르다고 한다.   마지막으로 instance segmentaion이다.  object detection 과 segmentaion을 동시에 수행한다.  rol을 먼저 찾고 segmentation을 해결하는 방식이라고 하는데 , 눈에 띄는 결과가 있었다고 한다.  "
[Coursera] Detection Algorithm (1) ,https://blog.naver.com/thdakfwn/221998369979,20200612,"Dection Algorithm​Image classificationImage classification with localization (1 object)Detection (multiple objects)​  ​  Landmark detection ​NN outputs (x,y) coordinate of important points in image (landmark)​will need a labeled training set where someone had to go through and annotate all the landmarks​Object Detection  ​once you've trained ConvNet, can use it in Sliding Windows Detection  disadvantage : computational cost- cropping out so many different square regions in the image and running each of them independently through a ConvNet​- using big stride, big step size, may hurt performance- using small stride, result in high computational cost  and not able to localize the object accurately​Convolutional Impementation of Sliding Windows  ​​  ​instead of forcing you to run 4 propogation on 4 subset of input image independently,combines all into one form of computation and share a lot of computation ​max pool 2 : running NN with a stride of 2 on the original image​  ​make all the predictions at the same time by one foward pass through big ConvNet ​​ "
Fast and Robust Object Tracking Using Tracking Failure Detection in Kernelized Correlation Filter ,https://blog.naver.com/yeajin522/222349502585,20210513,"#object tracking #tracker #deeplearning #KCF​오랜만에 논문 리뷰,,,​Abstract  본 논문에서는, 처리 속도는 유지하면서 tracking accuracy 를 높이기 위해 (1) tracking failure 감지(2) re-tracking : search window 를 이용(3) motion vector 분석등 세 가지 기능 모듈을 통합하는 imporved kernelized correlation filter(KCF)-based tracking 방법을 제안함​real-time streaming condition 에서 제안 방법은 target movement가 매우 클 때 기존 MDNet 보다 더 나은 결과를 보임 ..근데 MDNet 오래된 논문인데 아직도 비교당하네 짱인걸​​​Introduction  ​CF-based tracker의 장점1. Fourier transform domain 으로 spatial correlation estimation process 를 대체하여 계산 효율성이 높다2. 모든 frame 에서 traget object 의 location을 update 해준다 ​본 논문의 알고리즘은 세가지 스텝을 거쳐준다~!step 1. neighboring correlation value의 peak 및 average를 분석하여 tracking failure를 감지한다step 2. tracking failure 가 감지될 때 tracking 오류 처리 알고리즘을 사용하여 re-tracking 해준다step 3. tracking failure가 감지 될 때 preferred search window를 선택하기 위한 motion vector 를 계산해준다 (어떤 윈도우부터 처리할건지 계산하는듯)​​​Related Work  Correlation Filter(CF) 의 기본 메커니즘: RoI에서는 값이 크지만, 다른 region 에서는 값이 작은 맵을 생성해준다따라서 tracking 은, 가장 값이 높은 것을 대상으로 수행해준다.​그리고 CF 기반의 장점은, GPU를 사용하지 않는 환경이라도 처리 속도가 높다는 것임​​​The Proposed Method  제안하는 방법은 세 가지 파트로 나뉜다.(1) Tracking failure detection algorithm(2) re-tracking algorithm(3) KCF 방법에 기반한 motion vector analysis algorithm​이 방법은 모든 frame 에서 수행되며 아래와 같은 알고리즘으로 수행 된다. ​ ​​​Tracking Failure Prediction by Analysing Correlation Values-> Correlation value 분석을 통한 Tracking Failure 예측   Responce map 의 각 셀에 사전 지정 된 search window를 cyclic shift(수직 및 수평 이동) 하여 target 과 reference 사이의 correlation value를 계산한다. ​​ Fig. 2. (a) 성공적은 tracking sequence 에서 발생하는 일반적인 형태의 correlation 분포 , (b) 어려운 장면이 있는 sequence를 tracking 하여 얻은 correlation distributionSequence 에 어려운 피쳐(occlusion, appearnce distortion, light change 등등) 가 포함되지 않는다면 Fig 2a와 같이 단일 peak 값이 나타나고, tracking 할 때 이 peak 값을 따라가면 된다.그러나 어려운 scene은 peak 값이 상대적으로 감소하여 fig 2b와 같이 고유한 peak 값이 거의 없다. 이러한 경우 실제 target 의 위치에 peak가 생기지 않으므로 peak 값과 이웃 peak 값을 분석하여 tracking 실패를 예측한다. ​​​(p,q) 좌표 주변의 5x5 neighboring region 의 average correlation value는 다음과 같은 식으로 나타난다. ​ ​Ct(i,j) (i = 0,1,2, n-1 / j= 0,1,2, m-1) :  mxn 사이즈의 i번째 프레임 response map의 correlation value 이다.  (p-,q-) : response map 의 peak coordinateCt(p-,q-): peak corelation values Nt(p-,q-): average correlation valuesσ: over recent fames ​average value of Ct(p-,q-) and Nt(p-,q-) over recent σ frames can be dereved as Equation (2), (3) ​ ​​σ frame 동안의 Tracking failure 는 P_aver 와 N_aver 에 의해 분석될 수 있다. ​ Fig. 3. Neighbor: N_aver, Peak: Ct(p-,q-)  (a) successful tracking case (b) failure case​​Fig 3a 는 Ct(p-,q-)가 상대적으로 높고, N_aver와 충분히 구별 되는 successful tracking case 이다.반면에 Fig3b는 Ct(p-,q-)가 상대적으로 크게 떨어지고, 심한 blur 등 때문에 N_aver 와의 차이가 구별되지 않는 구간이 있는 failure tracking case 이다. 또한, peak가 잘못 된 위치에서 발생할 경우 tracking process가 완전히 failure한 경우가 된다. ​​Tracking failure Case를 정하기 위해, 실험을 통한 조건을 찾았음 T1과 T2 는 threshold 이며 σ 는 peak 의 standard deviation (표준 편차) 이고 α ,β 는 실험을 통해 찾은 상수임​ ​데이터 셋을 통해  α ,β 가 각각 0.85, -2.4 일 때 가장 좋은 값임을 찾았고 이를 통해 T1 과 T2 의 값을 0.6으로 설정하였음 ​​​​​Re-Tracking a Disappeared Target Using Multiple Search Windows -> Search Window 를 사용하여 사라진 target Re-tracking​  한 장면에서 object 가 완전히 사라지면 계속 tracking 하는 것은  불가능 하지만 , neighborhood 어딘가에 있다고 가정한다. ​​​ Fig. 5. (a) target 이 사라짐에 따라 tracking failure가 감지됨 (b) tracking failure 처리 과정에 Multiple search windows를 배치​​Target 이 사라진 위치 주변에 여러 search window를 배치한다. (Re-tracking 프로세스 중에는 target appearance를 업데이트 하지 않음)​예를 들면, Occlusion에 의한 tracking failure 는 Fig5a 와 같이 예측되며, 5b 와 같이 여러 search windows 를 배치해준다. 각 seach window 에서 responce map과 peak value 가 생성되고 이러한 search windows 에서 target 이 포착 되면 다시 tracking (re-tracking) 을 시작해준다. 정리하자면-> tracking 실패-> search windows 배치-> 각 search windows 에서 Response map 과 peak value 생성-> target 포착 되면, re-tracking​또한, 이러한 search windows 에 계산 우선 순위를 부여하여 computation load를 줄였다. (다음 절에서 설명)multiple search windows 에서  Ct(p-,q-)/P_aver >0.6 조건을 만족하는 대상을 re-tracking 해준다. ​​​Prioritization of Search Windows Based on Motion Vectors-> Motion Vectors 기반 search window 우선 순위 지정  ​target 은 보통 선형적인 움직임을 띄므로 target 의 최근 움직임을 분석하여 다음 frame 에서 예측할 수 있다. ​Eq(6) 과 같이 최근 3 frames 에서 target 의 motion vector를 평균화하여 movements 를 분석한다.  vxi 와 vyi는 i 번째 frame 에서 target의 수평 및 수직 coordinates changes 를 나타내고,n 은 현재 frame number 를 나타낸다. ​ Fig 6. (a) motion vector 9개 영역  (b) 각 motion vector와 연결 된 search window ​Fig 6a와 같이 motion vector의 값을 9개의 영역으로 구분한다. motion vector의 x와 y 의 절대값이 2보다 작으면, 영역 5에 속한다 (유의한 이동이 없다고 봄)그 외의 경우, 각 motion vector의 방향을 기준으로 각 영역에 속하게 된다. 그리고 각 motion vector 영역은 Fig6b 와 같이 각 search window와 연결 된다 ​Tracking failure 감지 -> target의 현재 motion vector 영역과 연관 된 search window 에 response map 가 생성되고, 우선 순위 search window에 target이 다시 포착 되면, 다른 search window 는 건너뜀 (계산 load를 줄임) ​​​​​Experimental Results​  CAU_Ped1, CAU_Ped2, CAU_Ped3 and CAU_Ped4 CCTV를 이용하여 일상 거리 장면의 테스트 비디오를 획득함  보행자, 차량 및 나무에 의한 occlusion이 있기 때문에 제안 된 방법을 평가하기 적합하다고 판단한듯 그리고 CAU 인걸 보아 중앙대에서 직접 모은 데이터셋인거같음 ​​ Visual Tracker Benchmark dataset 이미지를 MPEG-4 형식의 동영상 클립으로 변환시키고 각 영상 타임스탬프 간격은 0.03초로 두었음변환 된 영화 클립은 초당 33.3 프레임으로 재생​​이미지 기반 데이터셋 (영화) GT에는 그림 7과 같이 각target의 x,y,w,h 가 나타나있음 ​​ ​Fig 8은 스트리밍 조건에서 일정 기간 동안 GT가 제공 됨 ​​Tracking Performance of Deep Feature Based Tracker and the Proposed Method Based Tracker  Fig 10. 제안 방법, MDNet 기반 tracker, GOTURN 기반 trackier 의 정확도 평가 (a) significant target movement or challenging scenes exist , (b) no significant target movements and challenging scenes exist ​(a)는 타겟의 이동이 크거나, 가려짐 등이 많은 데이터 셋, (b)는 그렇지 않은 데이터 셋이다. 제안 방법은 (a) 와 같은 어플리케이션에서 잘 동작하는 것을 확인할 수 있음  반면 , 딥러닝 기반 추적기는 (b) ㅜ 와 같이 continuously blurred 되어있고, slightly  moving target을 성공적으로 추적하였다. 하지만 tracking failure가 저하된 정확도로 감지되기 떄문에 re-tracking process에 많은 시간을 소비한다. ​​Re-Tracking Steps on Tracking Failures​  ​ Fig. 12. Target re-capturing process (a) Tracking failure detected (b) Tracking failure handling (c) Target re-captured ​Fig.12. 는 Tracking failure 가 발생 했을 때, 제안 방법이 대상을 다시 포착하는 방법​12a 는 여러 sequence 에서 Tracking failure를 감지하는 것을 보여준다.12b 는 Tracking failure가 감지되고 multiple search windows 가 배치되는 모습이다. 9개의 인접한 search windows 중 빨간 색 search window가 preferred search window이다. target 이 preferred search window에 포착 된 경우 다른 search window 프로세스를 건너뛴다.​데이터 셋 BlurOwl 및 Coke 에서는 target의 불규칙한 이동으로 인해 preferred search window의 이점이 적은 것으로 보임. 나머지 데이터셋에서는 computation loads 가 줄어들 수 있도록 target이 선형 이동을 함.target 이 preferred search window나 다른 search window 에 포착 되면 해당 위치에서 re-tracking 이 시작됨​​​​Conclusions   본 논문에서는, GPU 를 사용하지 않고도 성공적이고 빠르게 tracking을 할 수 있도록 하였으며 KCF 기반으로 tracking failure 감지, re-tracking algorithm 을 제안하였음 re-tracking process 에서 추가 계산이 필요하지만 search windows 에 우선순위를 부여하여 계산을 최소화했음occlusion 및 blur 등의 문제점을 해결하고 그러한 어플리케이션에서 좋은 성능을 보였움​ "
"CascadeClassifier를 이용한 Object Detection (OpenCV, Python) ",https://blog.naver.com/engcang/221406096568,20181126,​​자세한 설명과 코드는 깃헙에 올려두었다.https://github.com/engcang/Opencv_tutorial_Matlab_and_python/tree/master/HSV_detect_within_CascadeClassifier​​​결과물 부터 투척   ​​ 먼저 학습은 MATLAB 툴박스를 이용해서 진행했다. 결과물인 .xml파일을 파이썬에서 불러와서 사용학습 : here - MATLAB검출 코드 :  here  
AI object detection ,https://blog.naver.com/arnoldkang44/221604671445,20190803,https://nitr0.tistory.com/266 [Python]웹캠을 이용한 실시간 사물 감지 프로그램 만들기 (텐서플로우 API)안녕하세요! Nitro입니다. 오늘은 텐서플로우(TensorFlow) API에서 지원하는 사물 감지(Object Detection)를 웹캠까지 연결하여 실시간으로 확인해봅시다.  사실 모든 소스코드는 GitHub에 올라가 있습니다. 누..nitr0.tistory.com ​​https://github.com/ukayzm/opencv ukayzm/opencvPractice OpenCV and TensorFlow. Contribute to ukayzm/opencv development by creating an account on GitHub.github.com ​ 
Detection and Segmentation 기본 정리 ,https://blog.naver.com/kimnanhee97/222026032725,20200709,"이전에도 이 강의를 들었는데 흥미로 들어보는 것과 공부를 위해 들어보는 것은 차이가 있는 듯 싶다. 내 머리속 지우개를 방지하기 위해서 이렇게 기록으로 남긴다.stanford university에서 justin johnson 님(https://web.eecs.umich.edu/~justincj/)이 강의하신 Detection and Segmentation 강의를 정리해보았다. 글 아래에 가면 동영상 정보(https://youtu.be/nDPWywWRIRo)를 올려놓았다.​​ 1. Sementic Segmentation2. Classification + Localization3. Detection4. Instance Segmentation​표시 : 딥러닝 방법론, 이론 /  모델  /  중요한 부분 /  더 알아볼 부분 / 소제목 / 내용  1. Sementic segmentation​- 같은 카테코리의 Object를 나누지 않는다.​- Idea 1: Sliding window를 이용해서 각 픽셀에 대해 라벨을 붙인다.                 - Very ineffcient!               - Patch 마다 계산하면 겹치는 부분이 있다.                              - Patch란 이미지를 보는 부분, 아주 작은 Crops                              - 실제로는 계산을 공유할 수 있다.​- Idea 2: Fully Convolutional                - 이미지에서 개별 패치를 분리하는 대신 독립적으로 생각               - 패딩이 없는 3x3 컨볼루션 레이어를 주로 사용하여 공간 크기를 유지하여 이미지 전달               - 모든 픽셀 각각의 라벨을 Classification loss를 써서 한번에 계산할 수 있다.                              - 픽셀 당 Cross entropy loss를 넣어서 우리가 알고 있는 정답인 모든 픽셀의 카테코리와 비교                - down sampling을 통해 feature를 추출하고 다시 upsampling해서 Segmentation map를 구한다.​ - upsampling idea 1: Unpooling for reception field                               - max unpooling을 할 때 max pooling의 위치를 기억해서 그 위치에 할당한다. 나머지는 0할당                              - 정확한 Segmentation map을 예측하기 위해 중요한 역할을 하므로 이러한 아이디어들을 알고 있는 것이 중요하다.                              - max pooling중 feature map을 구하면서 어느 공간에서 왔는지 공간 정보를 손실할 수 있다.                               그렇기 때문에 공간 정보를 다시 살리기 위해 max unpooling을 하는 것을 고려해보는 것이 중요하다. ​               - upsampling idea 2: Transpose Convolution                               - 3x3 transpose convolution, 1 padding, 2 stride: 사이즈 2배로 늘리기                              - output을 이동하면서 각 convolution 값을 copy 해주고 겹치는 부분은 sum 해준다.                              - deconvolution이라고 하는 사람도 있지만, Transpose Convolution이 맞는 용어다.                                             - 신호처리에서 Convolution의 inverse 연산이라고 칭한다.                              - upconvolution/ fractional convolution이라고도 한다.   2. Classification + Localization​- Single Object로 bounding box를 통해 위치를 표시한다.​- Fully Connected layer를 통해 만약 4096 size의 vector가 있다면 하나는 class score(cat:0.9/dog:0.05/car:0.01/...)를 계산하기 위해 4096 to 1000하나는 box coordinates(x,y,w,h)를 계산하기 위해 4096 to 4 를 계산한다. ​- 2개의 정답을 가지고 있는 fully supervised learning이다.두 가지 손실함수는 다음과 같다.(1) class를 알기 위한 Softmax loss function(2) bounding box 위치를 알기 위한 L2 loss function               - 경계를 조금 더 smooth 하게 만들기 위해 L1 loss를 사용하기도 한다.               - regression loss가 있는 아이디어는 같다.- 서로 다른 두 손실 함수를 정의하고 있기 때문에 (두 스칼라의 손실 차이를 구해) 더 중요한 것에 가중치를 주어 GD를 실행한다.               - 하이퍼 파라미터 설정을 할 때 일반적인 전략은 각각 서로 다른 성능 지표를 갖는 것이다.​- Human Pose Estimation에 적용할 수 있다.                - 4096 to 14 categorial를 적용하여 각 관절 위치(x,y)를 구한다.               - 각 관절에 대해 regression loss(L2, L1, Smooth L1)를 적용한다.                              - L2를 사용할 수 도 있다.                              - regression loss는 카테고리 분류를 할 때 cross entropy loss/ softmax loss/ svm margin type loss                              output이 어떤 일정한 값을 되게 하려면 L1/L2 loss를 적용한다.​  3. Object detection​- fix 카테고리를 사용하나, 특정 원하는 카테고리만 표시할 수 있다.​- Multiple Obeject를 추정하므로 object detection과 localization은 다른 개념이다. - 얼마나 많은 수의 Object가 나올지 미리 예측하지 않는다. ​- Idea 1: Sliding window  - Sementic segmentation에 적용한 아이디어와 비슷하다.  - Crop된 이미지마다 Class를 예측한다. (ex. dog no/ cat no/ background yes)​- Idea 2: Region Proposals (computer vision technology)  - 약 2000개 정도의 bounding box로 물체가 있을 만한 곳을 찾아준다.                    * 물체가 있을 만한 곳은 어떻게 찾는가?                        Segmentation을 사용하여서 물체라고 생각드는 Segment에 대해서 Window를 찾는다. Region proposal 방법 예시  - 정확하지는 않지만 recall이 있다. 모든 bounding box가 object이지는 않지만, bounding box 중에 object가 있다. - blobby image regions를 찾는다. - 이 각각의 영역에 대해서 convoltional network를 적용하면,                   모든 가능한 crops or regions을 찾는 것(슬라이딩 윈도우 방식)보다 더욱 효율적이게 된다.                 - 굉장히 느리지만 꽤 정확한 Bounding box를 찾을 수 있다.​ - R-CNN 아이디어.  - Regions of Interest(RoI) 2000개 제시 - 카테고리 라벨, offset or correction(box - 4 positions) - R-CNN은 log loss(softmax classifier) fine-tune  + hinge loss(linear svm) + least square(bounding box) - 각 ConvNet 실행 후 Classify regions with svm(hinge loss: class score) - 찾은 각 regions에 대해 regression loss 적용(Bbox reg) - multi tasking loss​- Fast R-CNN    - 한 번에 convolution layer 를 거친다. - 그 후에 이 feature map에서 RoI 추정. - 기존은 원 이미지에서 RoI를 사용했다면 Fast R-CNN은 feature map에서 RoI 추정 및 Crop - 그 후 RoI Pooling layer가 있다. - 그 후 Fully-connected layer를 통해 마지막으로 - Classifier(Linear + softmax) - Bounding box regressions(Linear)을 거친다.  - 그 후 Multi-task loss(Log loss + Smooth L1 loss)을 통해 두 Task를 공동으로 학습한다. - Faster R-CNN은 R-CNN 보다 학습 시 10배 더 빠르다.  - Regions proposals을 넣느냐 마느냐의 시간을 측정해볼 수 있다. - 포함하였을 때 병목 현상(bottle neck)이 발생한다. - Faster R-CNN의 등장 배경​- Faster R-CNN  - Regions proposal 부분에 이 역할을 하는 네트워크를 넣는다. - Region Proposla Network(RPN) - RPN으로 인해 Multi tasking loss가 총 4개 이다. - (1) RPN classify object / not object (2) RPN regress box coordinates (3) Final classification score ( object classes) (4) Final box coordinates - 네트워크 구조에 대해서 더 자세히 이해하기 - Test-Time speed가 Fast R-CNN에 비해 10배 빨라졌다.                - 그러나 Yolo보다는 느리다. ​- Proposals 없이 Detection 하는 방법도 있다. - YOLO(you only look once) - SSD(single shot detection)  - 큰 grid(7x7 base bounding boxes)를 구한 후에 각 grid에 대해 3개의 base bounding box로 시작 - 각 bounding box의 offset 측정(True locations of objects)과 Classification score 측정 - output: 7x7x(5xB+C)​                - Yolo의 기본 아이디어 (예를 들어 3개의 클래스(사람, 자동차, 오토바이)를 구분하고자 할때)                     - Grid를 나누고, 각 Grid에 대해 객체가 있는지 없는지 찾는다.  - 2개 앵커 박스를 기준으로 기록한다.    - Pc: 물체가 있는지 없는지                                  bx: 0,0으로 부터 가로 길이                                  by: 0,0으로 부터 세로 길이                                  bh: 박스 전체 세로 길이                                  bw: 박스 전체 가로 길이                                  C1: person인가/아닌가                                  C2: 자동차인가/아닌가                                  C3: 오토바이인가/아닌가                   - non-max suppression을 실시한다.                              - (1) 앵커박스 2개로 Bounded box 그리기                                 (2) 낮은 가능성 예측 제거                                 (3) 각 클래스에 대해 non-max suppression을 실시하여 최종 prediction을 얻기​​- Object Detection + Captioning = Dense Captioning  - Faster R-CNN 처럼 보인다. - Region processing - svm이나 softmax로 classifier를 하는 부분을 RNN을 이용한 Captioning 처리한다.​  4. Instance Segmentation​- 각 object 즉, instance별로 Detection 한다.​- Mask R-CNN   - faster R-CNN과 비슷하다.​ - Regions of bounding box 부분, 즉 RoI(Region of Interest) 부분을 찾는다.                                - 기존의 RoI 방식은 픽셀의 중심을 지나 Object를 잡게 된다면, 정수이기 때문에 반올림을 사용하여 픽셀을 잡았다.                                    그러나 Mask R-CNN에서는 RoI Align 방식을 사용하여 정확히 픽셀의 퍼센트를 잡고                                    Bilinear Interporation(Computer vision을 공부하면 알 수 있다.)을 실시하여 구한다. https://cdm98.tistory.com/33 - ​Classification Scores와 Box Coordinates(4)를 매기고, Convolutional layer를 통해 클래스 각각의 Mask를 예측한다. - Input Region proposal에서 object인지 배경인지에 따라 Mask를 예측한다.​ - 앞서 언급했던 Multi-task loss를 잘 적용하면 Mask R-CNN으로 Pose estimate도 가능하다.   - Microsoft dataset COCO dataset으로 훈련 가능하다. - 약 200,000개 training sets/ 80개 카테고리의 instance label이 지정되어 있다. - pose를 위한 Joint도 지정되어 있다.​​  Reference:​https://youtu.be/nDPWywWRIRo Keywords: #Semantic_segmentation, #fully_convolutional_networks, #unpooling, #transpose_convolution, #localization, #multitask_losses, #pose_estimation, #object_detection, #sliding_window, #region_proposals, #R_CNN, #Fast_R_CNN, #Faster_R_CNN, #YOLO, #SSD, #DenseCap, #instance_segmentation, #Mask_R_CNN ​Slides: http://cs231n.stanford.edu/slides/201...​Convolutional Neural Networks for Visual Recognition ​Instructors: Fei-Fei Li: http://vision.stanford.edu/feifeili/ Justin Johnson: http://cs.stanford.edu/people/jcjohns/ Serena Yeung: http://ai.stanford.edu/~syyeung/ ​Computer Vision has become ubiquitous in our society, with applications in search, image understanding, apps, mapping, medicine, drones, and self-driving cars. Core to many of these applications are visual recognition tasks such as image classification, localization and detection. Recent developments in neural network (aka “deep learning”) approaches have greatly advanced the performance of these state-of-the-art visual recognition systems. This lecture collection is a deep dive into details of the deep learning architectures with a focus on learning end-to-end models for these tasks, particularly image classification. From this lecture collection, students will learn to implement, train and debug their own neural networks and gain a detailed understanding of cutting-edge research in computer vision. ​Website: http://cs231n.stanford.edu/ ​For additional learning opportunities please visit:http://online.stanford.edu/ "
"[Object Detection] RCNN, Fast RCNN, Faster RCNN ",https://blog.naver.com/cogito21n/222310203839,20210414,RCNN  Fast RCNN  Faster RCNN ​ 
웨이모 - 3D Detection을 롱테일로 확장하기  ,https://blog.naver.com/economic_moat/222923366979,20221108,"최근 진행된 2022년 유럽컴퓨터비전학술대회(ECCV 2022)에서 선보인 웨이모 발표영상 Waymo at ECCV 2022 | Scaling 3D Detection to the Long Tail 의 스크립트입니다. Drago Anguelov: Hi, I'm Drago Anguelov. Thank you for having me. I lead the research team at Waymo. In this talk, I will give you an overview of our recent work on scaling 3D detection to the long tail. Overall, we have eight papers accepted at this ECCV conference on different topics, and I invite you to check them out. ​안녕하세요, 드라고 앵글로브입니다. 저는 웨이모에서 연구팀을 이끌고 있습니다. 이 강연에서는 3D detection을 롱테일로 확장하기 위한 최근 작업에 대해 간략히 설명하겠습니다. 전체적으로 이번 ECCV 컨퍼런스에서 서로 다른 주제에 대해 8편의 논문을 접수하였으니 확인해보시기 바랍니다.​ Waymo is Alphabet's autonomous driving technology company. We're building the world's most experienced driver and focusing on two categories-- moving people, which we do through Waymo One, our ride-hailing service, and moving commercial goods, which we do with our commercial arm called Waymo Via. That features these class A trucks and also local delivery.​웨이모는 알파벳의 자율주행 기술 회사입니다. 우리는 세계에서 가장 경험이 많은 차량을 만들고 있습니다. 두 가지 카테고리에 초점을 맞추고 있습니다. 웨이모 원(Waymo One)을 통해 사람들을 이송하는 승차 공유 서비스와 웨이모 비아(Waymo Via)라는 상업 물품을 배송하는 서비스입니다. 클래스 A 트럭과 지역 배송이 특징입니다.​  Our goal is to have a fully autonomous rider-only operations. Rider-only means no one in the driver's seat at all. So you can hail a car through your app, and you would get a fully empty vehicle that can take you anywhere you like. Currently, we have rider-only operations in Phoenix East Valley, San Francisco, and Downtown Phoenix. And we just recently shared that Los Angeles will be Waymo's next ride-hailing city. ​우리의 목표는 완전한 자율 주행 차량 운영을 하는 것입니다. Rider-only은 운전석에 아무도 타지 않는 완전자율주행을 의미합니다. 여러분은 앱을 통해 차를 불러올 수 있고, 여러분이 원하는 곳 어디든 데려다 줄 수 있는 완전한 무인 차량을 탈 수 있습니다. 현재 피닉스 이스트밸리, 샌프란시스코, 피닉스 다운타운에서 Rider-only 운영을 하고 있습니다. 그리고 우리는 최근에 로스앤젤레스가 Waymo의 다음 번 무인승차공유 서비스 장소가 될 것이라는 것을 공유했었습니다. Since October 2020, we have been offering Waymo One, the world's first fully autonomous service in Phoenix East Valley with 100% of rides without human drivers. And this service there has been operating ever since.​2020년 10월부터 피닉스 이스트밸리에서 세계 최초로 운전자 없이 탈 수 있는 완전 자율 주행 서비스인 웨이모원을 제공하고 있습니다. 그리고 이 서비스는 그 이후로 계속 운영되어 왔습니다. 동영상In March of 2022, we had our first rider-only ride in San Francisco, which I will show you. Even on the first ride, it's quite eventful. There is this double-loading truck. In the middle, there is-- actually, this double-parked truck. And then we continue. So one thing I will show you-- this is our visualization on top. This shows the LiDAR points. Our sensor suite has camera LiDAR radar, and you can see that the points in the vehicles are in very high resolution because we have a wonderful radar in our fifth generation. Here, we just gave room to a cyclist to cross in front of us. We will try to turn left here. And we want to make our way without necessarily having all the oncoming cars go through, which we succeeded. Maybe a few more interesting scenarios. So here, as we turn left, you will see there is a parking vehicle in front of us, so we comfortably let them park and proceed around them. So while I love this visualization, I think it's very cool and shows off our LiDAR a little bit.​2022년 3월, 샌프란시스코에서 처음으로 완전자율주행 차량을 주행했습니다. 보여드리겠습니다. [동영상] 심지어 첫 번째 주행에서도, 꽤 다사다난합니다. 이중 적재 트럭이 있습니다. 가운데에 이중 주차된 트럭이 있습니다. 그리고 우리는 계속 주행합니다. 한 가지만 보여드리죠. 이것이 맨 위에 있는 우리의 시각화입니다. LiDAR 포인트가 표시됩니다. 저희 센서 스위트(세트)에는 카메라, LiDAR, 레이더가 있는데, 5세대 훌륭한 레이더가 있기 때문에 차량 내 지점들이 매우 고해상도로 표시되어 있습니다. 여기, 영상에서 우리는 자전거 타는 사람이 우리 앞을 건널 수 있는 공간을 제공했습니다. 여기서 좌회전하도록 하겠습니다. 그리고 우리는 마주 오는 모든 차들을 통과시키지 않고, 우리의 길을 주행하고 싶습니다. 성공했습니다. 몇 가지 흥미로운 시나리오가 더 있을 수 있습니다. 그래서 여기서 왼쪽으로 돌면 앞에 차량이 주차하고 있는 것을 볼 수 있습니다. 그래서 우리는 앞에 있는 차량이 주차하게 하고, 그 차량 옆을 지나 잘 주행합니다. 저는 이 시각화를 좋아하는데, 매우 멋지고 우리의 LiDAR을 자랑하는 영상이라고 생각합니다. 동영상I have been on the vehicle myself a few times in the city, driving fully autonomously. And it's quite an experience, and so I wanted to show you what it looks like from inside the car. So it's a bit of a different thrill seeing the steering wheel and what it is like to be in there. It's very exciting-- maybe a different level of excitement, as much as I like our other viewer. And hopefully, it gets boring after a while, but we do this a lot, and we have been doing rider-only operations in San Francisco in a very regular cadence in reasonable volume ever since. ​저는 시내에서 완전 자율 주행 차량을 직접 타본 적이 몇 번 있습니다. 이것은 꽤 상당한 경험입니다. 저는 차 안에서 어떤 모습인지 보여드리고 싶었습니다. 운전대를 보는 것과 그 안에 있는 것을 보는 것은 약간 다른 스릴입니다. 매우 흥미롭습니다. 그리고 바라건대, 시간이 지나면 지루해지겠지만, 우리는 완전 자율주행을 많이 합니다. 그리고 우리는 그 이후로도 샌프란시스코에서 rider-only 운영을 상당히 규칙적으로 수행하고 있습니다. So I showed you scenes of San Francisco, but the Waymo Driver needs to handle diverse operating domains. So San Francisco is dense urban, but we also want to do suburban areas and freeways, and, of course, we want to do them in all kinds of weather, and so that's one challenge of our task. ​그래서 샌프란시스코의 장면을 보여드렸지만, 웨이모 드라이버는 다양한 지역에서 운전을 할줄 알아야 합니다. 샌프란시스코는 밀집된 도시이지만 교외 지역과 고속도로도 잘 주행하고 싶습니다. 물론, 모든 종류의 날씨에도 잘 주행하고 싶습니다. 그래서 그것이 우리의 과제 중 하나입니다. 4:28 동영상Another is that we want to handle very interactive environments. So one of the aspects that make autonomous driving challenging is just the sheer amount of people and vehicles that are in the streets, and they can exhibit a variety of different behaviors. So what you see here, actually, is us driving fully autonomously after a baseball game in San Francisco. And so you can see people leaving, getting into their vehicles, crossing the street. And here is the Waymo Driver making a very good and safe progress through this crowded scene. So it can see people getting out of the parking garages, cutting in, turning around. Here, we keep going. And another thing I would point out is you can see just how much objects and people there are around us can be a very crowded complex environment, and we are dealing with all of this quite well.​다른 하나는 매우 상호작용적인 환경을 다루고 싶다는 것입니다. 따라서 자율주행을 어렵게 만드는 요소 중 하나는 거리에 있는 사람과 차량의 수가 너무 많다는 것입니다. 그리고 그들은 다양한 행동을 보일 수 있습니다. [동영상] 여기 보시는 것은, 샌프란시스코에서 야구 경기가 끝난 후 거리를 주행하는 완전 자율주행차량의 주행모습입니다. 사람들이 떠나고, 차에 타고, 길을 건너는 것을 볼 수 있습니다. 그리고 여기 웨이모 드라이버가 붐비는 이 거리에서도 매우 훌륭하고 안전한 주행을 하고 있습니다. 사람들이 주차장에서 나와 끼어들어 돌아서는 것을 볼 수 있습니다. 자, 계속 갑니다. 그리고 또 하나 짚고 싶은 것은 우리 주변에 얼마나 많은 물건과 사람들이 있는지 알 수 있다는 것입니다. 우리는 이 모든 것을 꽤 잘 다루고 있습니다. 5:24 동영상 So one more challenge that pertains to the topic of this talk is long tail. So you can get a lot of very different things happen to you if you drive millions of miles. This is just one of them. What you see here is-- so there's a pickup truck. At 64 miles an hour, a barbecue falls out and goes straight in the path that we're driving. ​그래서 이 강연의 주제와 관련된 또 다른 도전은 롱테일입니다. 수백만 마일을 달리면 아주 다양한 일들이 일어날 수 있습니다. 이것그 중 한 사례입니다. [동영상] 여기 픽업트럭이 있네요. 시속 64마일의 속도로, 바베큐 머신이 우리가 운전하고 있는 길로 떨어집니다. 5:47 동영상Here's another one, where you can have house on wheels moving around. This is actually reasonably common but still interesting. You need to understand these distinctions.​여기 또 다른 영상이 있습니다. 바퀴 달린 집을 돌아다닐 수 있습니다. 이것은 사실 꽤 흔하지만 여전히 흥미롭습니다. 여러분은 이러한 차이를 이해할 필요가 있습니다. 1. 확장 가능한 모델 아키텍처So the rest of the talk will be on how do we handle better this long tail and more efficiently in our perception systems. And a good talk and scalability starts with scalable neural net architectures. I think they're the foundation on which the rest of the capabilities are built, and we want to pick such architectures that have good scaling properties.​그래서 나머지 이야기는 어떻게 하면 이 롱테일을 더 잘 다룰 수 있고 우리의 인식 체계에서 더 효율적으로 다룰 수 있는지에 대한 것입니다. 그리고 좋은 확장성은 확장 가능한 신경망 아키텍처에서 시작됩니다. 저는 확장 가능한 신경망 아키텍처들이 나머지 기능들이 구축되는 토대라고 생각하며, 우리는 확장 가능한 아키텍처들을 선택하기를 원합니다.​ So I'll give you a little bit-- I'll take you through a history of this. So as you know, those of you in this workshop should be well aware LiDAR-based detection has seen significant improvements in the past few years. Some of the most popular techniques would extract features from the 3D point cloud, render them in the top-down view, and then apply typical object detection model architectures on top. So this is one method that's quite popular called PointPillars. ​자, 조금 더 말씀드리죠. 이 역사를 살펴보도록 하겠습니다. 아시다시피, 이 워크숍에 참석하신 분들은 지난 몇 년 동안 LiDAR 기반 탐지가 크게 개선되었다는 사실을 잘 알고 계실 것입니다. 가장 인기 있는 기술 중 일부는 3D 포인트 클라우드에서 특징을 추출하여 탑다운 뷰에 렌더링한 다음 일반적인 object detection 모델 아키텍처를 위에 적용합니다. 이것은 Point Pillars라고 불리는 매우 인기 있는 방법입니다. Point Cloud 데이터로부터 격자 단위의 Feature Map을 생성해 이를 해석하는 인공지능 모델* PointPillars : object detection등의 task에 적용할 수 있는 형태로 point cloud를 encoding하는 방법. 기본적으로 PointNet(포인트클라우드를 raw data 그대로 사용하는 방식)을 활용하여 point cloud를 pillar의 형태로 표현하는 방식. PointPillar는 3D point cloud 정보를 받아 우선 feature encoder network에 넣어서 sparse pseudo-image(슈도 이미지, 가상이미지)로 변환. 이 때 input point cloud는 x, y, z, r (reflectance) 이렇게 4차원임을 가정. 이 pseudo-image는 2D conv backbone에 들어가서 high-level representation으로 변환되며, 최종적으로 detection head를 통과하며 3D bounding box를 예측  And this is a great method, quite simple and high performant. And it has one interesting property, though, a drawback, which is that because we form this dense top-down image, it scales quadratically with range. And as our LiDARs have been improving themselves dramatically over the last few years, the range has become 200, 300 meters. At such range, it starts becoming prohibitive creating and maintaining this dense top-down bird's eye view with pillars.  ​그리고 이것은 꽤 간단하고 고성능인 훌륭한 방법입니다. 하지만 한가지 흥미로운 특성이 있습니다. 단점은 우리가 이 dense 탑다운 이미지를 형성하기 때문에 범위에 따라 사분면적으로 확장된다는 것입니다. 그리고 우리의 LiDAR가 지난 몇 년 동안 극적으로 개선되면서, 그 범위는 200, 300미터가 되었습니다. 이러한 범위 이후부터는 필러로 이 밀집된 하향식 버드아이뷰(Top-Down)를 만들고 유지하는 것이 안되기 시작합니다. And so maybe a couple of years ago, we set ourselves the challenge, can we have a high-performance model that actually does not depend on the range? ​그래서 몇 년 전에, 우리는 우리 스스로에게 도전장을 던졌습니다. range에 의존하지 않는 고성능 모델을 가질 수 있을까요?​요약 : Object detection에 PointPillars라는 방법을 사용했는데, range가 늘어날수록 range가 200-300m 이상되니까 잘 안됨. 이러한 range에 의존하지 않는 고성능 모델을 찾을 수 있을 것인가?  https://arxiv.org/pdf/2106.13365v1.pdfAnd I would like to, of course, say we succeeded. Last year, in CVPR, represented a model called Range Sparse Net. ​물론 저는 우리가 성공했다고 말하고 싶습니다. 작년에는 CVPR에서 Range Sparse Net이라는 모델을 선보였습니다. And I'll just give you a short overview of this method. This architecture does a lot of the processing in the native range image representation, which is the dense representation containing the LiDAR scans themselves. So here, in a y-axis, you have the LiDAR beams of the sensor. On the x-axis, you have the azimuth angle. So as the sensor is spinning, it keeps capturing and drawing this range image. So we process the range images themselves and extract features.​이 방법에 대한 간략한 개요를 말씀드리겠습니다. 이 아키텍처는 LiDAR 스캔을 포함하는 dense 표현인 native range image representation에서 많은 처리를 수행합니다. 여기 Y축에는 센서의 LiDAR 빔이 있습니다. X 축에서는 방위각(azimuth angle)을 나타냅니다. (x축) 센서가 회전하면서 이 range image를 계속 캡처하고 그립니다. 그래서 우리는 range image 자체를 처리하고 특징을 추출하며, ​ And we also segment the foreground objects of interest in these range images. This selects maybe 10% of the points, or fewer, in many cases, of what could be objects of interest.​또한 이러한 range image에서 관심 있는 전경 객체(foreground objects)를 분할합니다. 이렇게 하면 관심 대상이 될 수 있는 포인트의 10% 또는 그 이하가 선택​될 수 있습니다. And then we project these points, just the foreground points, in 3D in bird's-eye view. Now, this is a sparse point cloud, and it's a relatively small one. It does not contain all the points initially captured by the LiDAR. And so now, we maintain the sparse representation. So to do that, we do sparse processing.​그리고 우리는 이 포인트들을, 전경 포인트들을, 버드아이뷰에서 3D로 투영합니다. 자, 이것은 희박한(sparse) 포인트 클라우드이고, 상대적으로 작은 하나의 결과물입니다. 여기에는 처음에 LiDAR에 의해 캡처된 모든 지점(dense)이 포함되어 있지는 않습니다. 그래서 이제 우리는 희박한 표현을 유지합니다. 그러기 위해서, 우리는 희소 처리(sparse processing)를 합니다. ​And we do this with sparse convolution operations. There's multiple layers of sparse convolution.그리고 우리는 이것을 Sparse 컨볼루션 연산으로 합니다. Sparse 컨볼루션의 멀티 레이어가 있습니다. That produces upgraded features in bird's-eye view, where things generalize better, then apply a box regression. Here, there is a reasonably standard one, and you get 3D boxes. And so you will see that none of the steps of this method explicitly depend on the range, which is great, which yields a very performant method.​이렇게 하면 버드아이뷰에서 기능이 업그레이드되어 일반화가 더 잘 되고 box 회귀 분석을 적용할 수 있습니다. 여기, 상당히 표준적인 것이 있고, 여러분은 3D 박스를 얻을 수 있습니다. 그래서 이 방법의 어떤 단계도 분명히 범위(range)에 의존하지 않는다는 것을 알게 될 것입니다. 이것은 매우 훌륭한 방법입니다. 이것은 매우 효율적인 방법을 만들어냅니다.​*box regression : 바운딩 박스 위치를 교정해주는 선형 회귀를 학습시켜주는 것.  왼쪽 : 차량 / 오른쪽 : 보행자And this is, in red, on the left, you have a vehicle performance. On the right, you have pedestrian performance. On x-axis, we have latency, so left is better, much faster. On the y-axis, you have quality in bounding boxes. And you can see that our methods are dominated at least the contemporaries from about a year or so ago in both quality and especially latency. So that provides very efficient methods for very long ranges and short ranges. ​그리고 RSN는 빨간색으로 표시되어 있습니다. 왼쪽 그래프는 차량에 대한 성능입니다. 오른쪽 그래프는 보행자에 대한 성능을 보여줍니다. x축에서는 지연 시간(latency)이 있기 때문에 x축 상에서 왼쪽에 위치한 게 더 낫고 훨씬 빠릅니다. Y 축에서는 바운딩박스의 품질을 확인할 수 있습니다. 보시다시피 우리의 방법은 품질과 특히 레이턴시 면에서 적어도 1년 전쯤의 동시대의 방식들보다 훨씬 좋은 성능을 보이고 있습니다. 따라서 매우 긴 범위와 짧은 범위에 대해 매우 효율적인 방법을 제공합니다.​요약 : x축 상으로는 왼쪽, y축 상으로는 위쪽에 있는 게 좋은 것. 빨간색으로 표시된 웨이모의 RSN 방식이 다른 보라색, 파란색, 초록색, 노란색 모델들(약 1년전 나온 모델들) 보다 우수한 것을 볼 수 있음. 차량 detection과 보행자 detection 모두 우수함. RSN은 짧은 범위뿐 아니라 긴범위에 대해서도 효율적인 방법임.  So this is great, right? Well, we set ourselves the next question on that point, which is, can we have a model with better scaling properties? ​정말 대단하죠? 우리는 그 점에 대해 다음 질문을 합니다. 즉, 더 나은 스케일링 특성을 가진 모델을 가질 수 있을까요? So the model was good, but maybe we could do even better. And so we were inspired particularly by the great success of transformers, which have been revolutionizing quite a lot of fields in computer vision and language processing. And so they have great scaling properties. They have great capacity for learning, ultimately, general patterns, and then why not 3D modeling patterns? Now, so we wanted to do this, but we took constraints. ​앞선 모델도 좋았지만, 어쩌면 더 잘 할 수 있을지도 모릅니다. 그래서 우리는 특히 컴퓨터 비전과 언어 처리 분야에서 꽤 많은 분야에 혁명을 일으키고 있는 트랜스포머의 위대한 성공에 영감을 받았습니다. 그래서 트랜스포머는 훌륭한 스케일링 특성을 가지고 있습니다. 궁극적으로는 일반적인 패턴을 학습할 수 있는 뛰어난 능력을 가지고 있습니다. 그렇다면 3D 모델링 패턴은 어떨까요? 그래서 우리는 이것을 하고 싶었지만, 제약이 있었습니다. So one of them is preserve this insight sparsity that we had in RSN. We did not want the model that grows quadratically with range.​그 중 하나는 우리가 RSN에서 가졌던 통찰력의 희소성(sparsity)을 보존하는 것입니다. 우리는 range와 함께 사분면적으로 성장하는 모델을 원하지 않았습니다. (dense 원하지 않음. sparsity 유지하기를 원함)  And, furthermore, based on our experience with RSN, we wanted to address some limitations of the 3-by-3 sparse convolutions. Specifically, they are unable to pass information among point islands that are not closely connected in 3D space, just the receptive field is reasonably small. And also, they're incompatible with hardware accelerators, such as tensor processing units. So we wanted to mitigate this. ​또한 RSN에 대한 경험을 바탕으로 3x3 희소 컨볼루션의 몇 가지 한계를 해결하고자 했습니다. 특히, 그들은 3D 공간에서 밀접하게 연결되지 않은 점섬들 사이에서 정보를 전달할 수 없으며, 수용 필드가 상당히 작습니다. 또한 텐서 처리 장치와 같은 하드웨어 엑셀레이터와도 호환되지 않습니다. 그래서 우리는 이것을 완화시키고 싶었습니다. And in this ECCV, we have a paper that achieves this, called ""Sparse Window Transformer for 3D Object Detection in Point Clouds."" In this work, we take some inspiration from the Swin transformer, which is a predecessor work on images that proposed to partition them into windows and merge the context information from these images in a hierarchical manner using transformers. Our method makes several key adaptations of Swin transformers to enable efficient sparse feature process.​그리고 이 ECCV에는 이를 달성하는 ""포인트 클라우드에서 3D 객체 감지를 위한 희소 윈도우 트랜스포머""라는 논문이 있습니다. 이 작업에서 우리는 Swin 트랜스포머에서 영감을 얻는데, 이는 트랜스포머를 사용하여 window로 분할하고, 이러한 이미지의 컨텍스트 정보를 계층적으로 병합할 것을 제안한 이미지에 대한 전처리 작업입니다. 우리의 방법은 효율적인 희소 특징(sparse features) 프로세스를 가능하게 하기 위해 Swin 트랜스포머에서 몇 가지 주요 각색들을 만듭니다.​요약 : Swin 트랜스포머에서 몇가지 각색해서 SWformer 만듦.​ 첨부파일﻿SWFormer.pdf파일 다운로드 So this is the overall architecture of our model. It takes a sequence of point clouds as input and does the standard PointPillar-style voxelization, which produces a set of sparse features, where we have LiDAR points. ​이것이 우리 모델의 전반적인 아키텍처입니다. 포인트 클라우드의 시퀀스를 입력으로 사용하고 일련의 희소 특징을 생성하는 표준 PointPillar 스타일 복셀화를 수행합니다. 여기(제일 왼쪽에 검정색 부분) 보시면 LiDAR 포인트들이 있습니다. Then these sparse features are processed by several layers of SWFormer blocks, one block per hierarchical scale. These multiscale features then are fused with transformer layers in sparse exampling operations. And we have several heads at several resolution detecting objects of different sizes. ​그런 다음 이러한 희박한 특징은 hierarchical(계층적) scale당 하나의 블록으로 구성된 SWFormer(sparse window 트랜스포머) 블록의 여러 레이어에 의해 처리됩니다. 그런 다음 이러한 멀티스케일 특징은 희소 샘플 작업에서 트랜스포머 레이어와 융합됩니다. 그리고 우리는 다른 크기의 물체를 감지하는 여러 해상도의 여러 개의 헤드를 가지고 있습니다. multi-head self-attention (MSA) layer norm (LN)multilayer perceptron (MLP)So let's dig into the block. This is the sparse window transformer block. It's the basic building block of our backbone. It's similar to Swin transformers but contains several innovations aimed at efficiently processing sparse inputs. So we process pictures in 10-by-10 sparse windows and shift the window partitions between layers. Note that 10-by-10 window is a lot more than the 3-by-3 convolutional filters, which allows us to capture more context. Although each window has the same spatial size, such as a 10-by-10 voxel grid, the number of nonempty voxel in each window can vary significantly. And so our first interpretation is that the bucketing-based window partition for these windows. So we group the features of these windows into buckets with different effective sequence lengths, and that's for efficiency on accelerated hardware. Compared to Swin transformer, we minimized the use of window shift operations as well. So these window shift operations are expensive in a sparse world in tensor processing units as well. And so we do only one shift operation in each block, and like swin transformer, which we can just have. ​자, 이제 블록에 대해 알아보죠. 이것은 sparse window 트랜스포머 블록입니다. 그것은 우리 아키텍처의 기본 구성 요소입니다. Swin 트랜스포머와 유사하지만 희소 입력을 효율적으로 처리하기 위한 몇 가지 혁신이 포함되어 있습니다. 그래서 우리는 10x10의 sparse window에서 사진을 처리하고 레이어들 사이에서 윈도우 파티션을 이동합니다. 10x10 창은 3x3 컨볼루션 필터보다 훨씬 더 많으므로 더 많은 컨텍스트를 캡처할 수 있습니다. 각 윈도우의 공간 크기는 10x10 복셀 그리드처럼 동일하지만 각 윈도우의 비어 있지 않은 복셀 수는 크게 다를 수 있습니다. 첫 번째 해석은 이러한 윈도우를 위한 *버킷 기반 윈도우 파티션입니다. 따라서 이러한 창의 특징을 서로 다른 유효 시퀀스 길이를 가진 버킷으로 그룹화합니다. 이는 가속 하드웨어의 효율성을 위한 것입니다. 우리는 Swin 트랜스포머와 비교하여 윈도우 시프트 작업도 최소화했습니다. 따라서 이러한 윈도우 시프트 작업은 텐서 처리 장치의 sparse 세계에서도 비용이 많이 듭니다. 그래서 우리는 각 블록에서 한 번의 시프트 작업만 합니다. Swin 트랜스포머와 같이, 우리는 그냥 할 수 있습니다.​요약 : 기존 RSN에서는 3X3 Convolution 필터 사용했는데, sparse window 트랜스포머에서는 10X10 Convolution 필터 사용. 더 많은 컨텍스트 캡처 가능. 그리고 하드웨어 엑셀레이터의 효율성을 위해 서로 다른 유효 시퀀스 길이를 가진 버킷으로 그룹화함(버킷 기반 윈도우 파티션). 그리고 윈도우 시프트는 비용 절감을 위해 각 블록에서 한번만 수행  https://selectfrom.dev/apache-spark-partitioning-bucketing-3fd350816911* 파티션과 버킷 : 버킷은 데이터를 더 관리하기 쉽거나 동일한 부분으로 분해함. 파티션을 사용해서 먼저 나눠줌. 파티션 분할 및 버킷은 부작용을 최소화하면서 이점을 극대화하기 위해 사용됨. 셔플링의 오버헤드, 직렬화의 필요성 및 네트워크 트래픽을 줄일 수 있음. 이는 결국 성능, 클러스터 활용률 및 비용 효율성을 개선함.​ We can also observe that anchor-based detectors-- their performance is closely related to the average difference between where the anchor is in the ground truth. And so this can be addressed one way effectively with two-stage methods but at the cost of more compute. And another approach and methods like CenterNet, which strive to define anchors in the center of the ground-truth boxes, which enforces distributions closer to zero mean and smaller variance. However, in the sparse setting, the centers of objects may not have any voxels, and this phenomenon particularly affects larger objects, such as buses or trucks, where the object center can be quite far from any sparse voxels. And so to mitigate this issue or to improve the quality, based on this insight, we propose a voxel diffusion model. ​우리는 또한 anchor-based detectors를 관찰할 수 있습니다 -- detectors의 성능은 ground truth에서 앵커가 있는 곳의 평균 차이와 밀접한 관련이 있습니다. 따라서 이 문제를 2단계 방법으로 효과적으로 해결할 수 있지만 더 많은 컴퓨팅 비용이 듭니다. 그리고 CenterNet과 같은 또 다른 접근 방식과 방법은 ground-truth boxes의 중심에 앵커를 정의하려고 노력하며, 이는 평균이 0이고 분산이 더 작도록 분포를 강제합니다. 그러나 sparse 설정에서 객체의 중심에는 복셀이 없을 수 있으며, 이 현상은 특히 객체 중심이 sparse 복셀에서 상당히 멀리 있을 수 있는 버스나 트럭과 같은 더 큰 객체에 영향을 미칩니다. 그리고 이러한 통찰력을 바탕으로 이 문제를 완화하거나 품질을 개선하기 위해 복셀 확산 모델(voxel diffusion model)을 제안합니다.​요약 : 기존 detection 방법들(anchor-based detectors, CenterNet)에 문제점이 존재. 버스나 트럭같은 큰 객체 detection 문제를 완화하기 위해 해드에서 복셀확산모델 사용을 제안.  https://www.edwith.org/ai218/lecture/410017?isDesc=false* 물체 감지 알고리즘에서 각각의 격자 셀이 오직 하나의 물체만 감지 할 수밖에 없다는 단점을 해결하기 위해 앵커 박스를 사용. 바운딩 박스를 직접적으로 예측 하는 대신 미리 크기가 정해진 앵커 박스를 여러 개 만들어 사용* anchor-based detector : 미리 세팅해놓은 수 많은 anchor(point)에서 category를 예측하고 coordinates를 조정하는 방식  anchor-based method (앵커가 제공하는 사전 지식이 가로 세로 비율을 제한하고 불규칙한 감지 모양을 방지)CenterNetSo it's a different head design. 그래서 우리 모델은 다른 헤드 디자인을 가집니다. Voxel Diffusion. After foreground segmentation, each voxel receives a segmentation score s ∈ [0, 1]. All voxels with scores greater than a threshold γ = 0.05 are scattered to a dense BEV grid, and then we apply a k × k max pooling on the dense BEV grid to expand valid voxel features to their neighboring locations where k is set to 5 in this example. (Left) before diffusion, there are only two foreground voxels with segmentation scores {0.5, 0.9} greater than γ; (Right) after voxel diffusion, 47 voxels become valid. Best viewed in color​복셀 확산. 전경 분할 후 각 복셀은 분할 점수 ∈ [0, 1]을 받음. 임계값 θ = 0.05보다 큰 점수를 가진 모든 복셀은 밀집 BEV 그리드로 산재된 다음 밀집 BEV 그리드에서 k × k max pooling을 적용하여 유효한 복셀 기능을 이 예에서 k가 5로 설정된 인접 위치로 확장함. (왼쪽) 확산 전에 분할 점수가 {0.5, 0.9}인 전경 복셀은 두 개뿐. λ보다 큼; (오른쪽) 복셀 확산 후 47 복셀이 유효하게 됨. 컬러로 가장 잘 보임.And at the core of this design is that, first, we segment voxels in bird's-eye view towards the end to be foreground to background. The background voxels are dropped. And for foreground, we apply a max [INAUDIBLE] operation shown in this picture, with a specific diffusion size relevant to the size of objects we expect to detect. And so after diffusing to nearby voxels, we apply one sparse window transformer layer and apply a standard box head, and that improves the results, especially for large objects.​이 디자인의 핵심은 첫째, 복셀을 전경에서 배경의 끝을 향해 버드 아이 뷰에서 분할(segment)하는 것입니다. 배경 복셀이 삭제됩니다. 그리고 전경에 대해, 우리는 우리가 탐지할 것으로 예상되는 물체의 크기와 관련된 특정 확산 크기를 가진 이 그림에 표시된 최대 연산을 적용합니다. 그래서 가까운 복셀로 확산(diffusing)된 후, 우리는 one sparse window transformer layer를 적용하고 스탠다드 박스 헤드를 적용하는데, 이는 특히 큰 물체에 대한 결과를 향상시킵니다.​요약 : Voxel Diffusion 방법 활용해서 큰 물체 detect 결과 향상시킴 3D average precision (APH) / L1 (easy) and L2 (hard)So how well does this do? Well, it's actually a state of the art for LiDAR detectors in the Waymo Open Data set. This is the result on the test set. It outperforms, well, all the methods, single or two-stage methods, that are often a lot more computationally expensive than a sparse window transformer. And our method also achieves quite strong results, not only on the 3D detection, but on the LiDAR semantic segmentation task as well. ​그럼 이게 얼마나 잘 될까요? 이것은 Waymo Open Data 세트의 LiDAR detectors를 위한 최첨단 기술입니다. 테스트 세트의 결과입니다. 단일 또는 2단계 방법인 모든 방법보다 성능이 우수 (73.36) 합니다. 종종 희소 윈도우 트랜스포머보다 계산 비용이 훨씬 더 많이 듭니다. 그리고 우리의 방법은 3D detection뿐만 아니라 LiDAR 의미 분할(semantic segmentation) 작업에서도 상당히 강력한 결과를 달성합니다.​ MVF (Multi-View Fusion)RSN (recent range sparse net)And here are some results on large vehicles since we talked about large objects. You can see that, on them, the performance increase is particularly strong. So while, on average, maybe you get around 1 MEP, on large objects, you can get up to 6 or 7. And we also compare to our RSN, which is, well, what we intuitively would want. And so, yeah, these gains are due to our carefully thought-out transformer design and also innovations such as the voxel diffusion model. ​그리고 여기 대형 차량에 대한 몇 가지 결과가 있습니다. 우리가 대형 물체에 대해 이야기한 이후로 말이죠. 성능 향상이 특히 강하다는 것을 알 수 있습니다. 따라서 평균적으로 1 정도의 차이(47.5-46.4=1)를 얻을 수 있지만, 큰 물체의 경우 최대 6~7(51.5-45.2=63, 60.1-53.1=7)까지 더 나은 성능을 얻을 수 있습니다. 우리는 RSN과 비교합니다. 이것은 우리가 직관적으로 원하는 것입니다. 그리고, 네, 이러한 이득은 신중하게 고안된 트랜스포머 설계와 복셀 확산 모델과 같은 혁신 덕분입니다.​요약 : SWFormer가 RSN보다 대형 차량 감지 성능이 6~7 AP(average precision)나 더 높음.  https://arxiv.org/pdf/2210.09267.pdfNow, we don't only do LiDAR architectures. So we have another paper at this conference on our camera-radar fusion architecture. And this is just the shout out, the poster. Please go check out the work at the conference. If you're interested, ultimately, camera and radar have very complementary strengths, but their fusion is an underexplored domain. It's very natural to fuse these two sensors. And we show how to do this with a transformer-based sensor fusion setup. Please check out our poster to see that type of work. ​이제 우리는 LiDAR 아키텍처만 하는 것이 아닙니다. 그래서 우리는 이 컨퍼런스에 우리의 카메라-레이더 융합 아키텍처에 대한 또 다른 논문을 가지고 있습니다. 그리고 이것은 단지 포스터입니다. 컨퍼런스에 가서 작업을 확인해 보세요. 만약 여러분이 관심이 있다면, 궁극적으로 카메라와 레이더는 매우 상호 보완적인 강점을 가지고 있지만, 카메라와 레이더의 융합은 충분히 탐구되지 않은 영역입니다. 이 두 센서를 융합하는 것은 매우 자연스러운 일입니다. 그리고 우리는 트랜스포머 기반 센서 융합 셋업으로 이것을 하는 방법을 보여줍니다. 이 방법을 보시려면 저희 포스터를 봐주세요.​요약 : 트랜스포머를 통해 카메라, 레이더를 융합하는 아키텍처에 대한 논문도 있다. 2. 보기 드문 예제를 사용하여 능동적인 학습을 합니다.Now, I will move to a different topic, which is active learning, where there are rare examples. ​이제 다른 주제인 능동 학습(active learning)으로 넘어가겠습니다. 여기에서는 드문 예시가 있습니다.​*능동학습(active learning) : 초기 라벨링된 일부 데이터를 이용해 모델 학습한 이후 추가적인 데이터 선별, 활용하는 기계학습 모델​ 17:50 비디오The story here starts with our auto labeling work from a year ago. So this work was driven by the idea that, after we've collected the examples in the real world, on board, we have, one, a lot more compute, and two, we have the benefit of seeing how objects look not just in the past but in the future. And so over long periods of time, we can see how an object looks from all angles, typically, and that allows us to estimate really accurately its shape. Also, if we are able to detect these objects and track them in time, we can refine the trajectory over time for every object to be really, really accurate, as you can see in this video. So even in the very crowded scene, this is our vehicle. We maintain a very rich set of boxes very consistently with very little noise. And that is, of course, great. It provides auto labels. So we can, on board, create through the system a lot of bounding boxes that actually can then be even used to train models. So you can label sequences completely automatically. And we show that, on average, this box quality is as good or quite almost as good as human labelers.​이 이야기는 1년 전의 오토 라벨링 작업으로 시작됩니다. 그래서 이 작업은 실제 세계의 예를 수집한 후에, 우리는 하나, 훨씬 더 많은 컴퓨팅을 가지게 되고, 둘, 우리는 과거뿐만 아니라 미래에 물체가 어떻게 보이는지를 볼 수 있다는 아이디어에 의해 추진되었습니다. 그래서 오랜 시간 동안, 우리는 물체가 어떻게 보이는지 볼 수 있습니다. 일반적으로, 그것은 우리가 그것의 모양을 정확하게 추정할 수 있게 해줍니다. 또한, 만약 우리가 이러한 물체들을 감지하고 시간 내에 추적할 수 있다면, 우리는 모든 물체들이 정말로 정확하도록 시간이 지남에 따라 궤적을 정교하게 만들 수 있습니다. [동영상] 이 비디오에서 볼 수 있듯이 말이죠. 그래서 매우 붐비는 장면에서도, 이것은 우리의 차량입니다. 우리는 매우 풍부한 박스 세트를 매우 적은 노이즈로 일관되게 유지합니다. 그리고 그것은, 물론, 훌륭합니다. 오토 라벨을 제공합니다. 그래서 우리는  차량 내부에서 많은 바운딩 상자를 만들 수 있습니다. 실제로 모델을 훈련시키는 데 사용될 수 있습니다. 따라서 자동으로 시퀀스에 레이블을 지정할 수 있습니다. 그리고 우리는 평균적으로 이 박스 성능이 인간 라벨만큼이나 좋거나 거의 비슷하다는 것을 보여줍니다.​요약 : 웨이모의 오토라벨링 이야기.​ 100%의 라벨링된 데이터 vs 13%의 라벨링된 데이터 + 온보드 오토라벨링 시스템으로 트레이닝한 87%So that was great. Of course, let's look what happens for the long tail. So at the top, you have the performance here of a model that is trained with all the Waymo Open Data set in the standard way. So, of course, it gets certain performance on all vehicles and on the large vehicles. Here on the bottom is a system where we use 13% of label data. We train the onboard auto labeling system. We complete the other 87% of sequences using that system. And so now, we train a model using, well, 100% label sequences, but labeled in a hybrid way. And so what we see there is actually the quality on average matches, or potentially even exceeds if you squint a little bit, the quality from the supervised boxes. But for a rare category, at least in the open data set-- large vehicles are reasonably rare-- we have significant regression. So the auto labeling system is not able to, with very few examples, produce good enough bounding boxes, such that the model derived from the labels is good on them. And so we come up short here, and we want to fill in this gap. ​훌륭했습니다. 물론, 롱테일에서는 어떻게 되는지 봅시다. 상단(fully-supervised 부분)에는 표준 방식으로 설정된 모든 Waymo Open Data로 훈련된 모델의 성능이 있습니다. 물론 모든 차량과 대형 차량에서 일정한 성능을 발휘합니다. 여기 아래(semi-supervised 부분)에는 13%의 라벨 데이터를 사용하는 시스템이 있습니다. 우리는 차량탑재 오토라벨링 시스템을 교육합니다. 우리는 그 시스템을 사용하여 시퀀스의 다른 87%를 완성합니다. 그래서 이제 우리는 100% 라벨 시퀀스를 사용하여 모델을 훈련하지만 하이브리드 방식으로 라벨링된 것입니다. 이 그래프에서 우리가 볼 수 있는 것은 평균적으로 일치하는 품질입니다. (fully-supervised랑 semi-supervised의 파란색 부분을 보면 거의 비슷함). 여러분이 눈을 가늘게 뜨고 보면, supervised 보다도 semi-supervised가 더 성능이 잘 나온 걸 보실 수도 있습니다. 하지만 희귀한 범주(rare)의 경우, 적어도 개방형 데이터 집합에서는 대형 차량이 상당히 희귀합니다. 우리는 상당한 회귀를 가지고 있습니다. 그래서 자동 라벨링 시스템은 아주 적은 예시로 충분한 바운딩 박스를 만들 수 없습니다. 라벨에서 파생된 모델이 그것들에 적합하도록 말이죠. 그래서 우리는 부족했고, 이 공백을 메우고 싶습니다.​요약 : 100%의 라벨링된 데이터 vs 13%의 라벨링된 데이터 + 온보드 오토라벨링 시스템으로 트레이닝한 87% 비교했을 때 all vehicles은 비슷. 아주 약간 후자가 높음. 그런데 large vehicle은 후자 성능이 떨어짐. 이 부족한 부분을 채우고 싶음.  Now, a natural way, one natural way to fill this gap is to look at active learning. So you have the auto labeler as an option always to label a sequence. But we have the option, given the detector, which also, of course, have access to the feature of the detector, we can have some criterion here that picks out certain examples and say, OK, these are rare examples. They are worth labeling by humans. And so we can then send these few examples that we identified as interesting to humans-- not too many, because it's effort intensive-- and then we create these scenarios, which have mostly auto labeled boxes, but then they have a few for the interesting cases. They're labeled by humans. And now we can train with this combination of boxes and hopefully fill in the gap. ​이 공백을 메우는 자연스러운 방법 중 하나는 active learning을 보는 것입니다. 따라서 오토 레이블은 항상 시퀀스에 레이블을 지정하는 옵션으로 사용할 수 있습니다. 하지만 우리는 선택권이 있습니다. 물론, detector의 feature에 접근할 수 있는 것을 고려하면, 우리는 여기서 특정한 예들을 골라내고, ""좋아, 이것들은 희귀한 예들이야.""라고 말할 수 있습니다. 그것들은 인간에 의해 라벨링 될 가치가 있습니다. 그래서 우리는 우리가 흥미로웠던 몇 가지 예를 인간에게 보낼 수 있습니다. 너무 많지는 않습니다. 왜냐하면 그것은 노력을 많이 하기 때문입니다. 그리고 우리는 이러한 시나리오를 만듭니다. 대부분 오토 라벨이 붙은 박스이지만, 흥미로운 경우를 위한 몇 가지 시나리오를 가지고 있습니다. 인간에 의해 라벨이 붙여집니다. 이제 우리는 이 박스의 조합(오토라벨+인간라벨)으로 훈련할 수 있습니다. 그리고 바라건대 그 공백을 메울 수 있습니다.​요약 : 대다수는 오토라벨링. 예외적으로 희귀한 예시들은 사람에 의해 라벨링 Now, the main question is, what is a suitable rareness criterion? We have a paper in this conference called ""Rare Example Mining"" that addresses this question.​자, 가장 중요한 질문은, 희귀함에 대한 적절한 기준이 무엇인가 하는 것입니다. 이 컨퍼런스에 이 문제를 다루는 ""희귀한 예제 추출""이라는 논문이 이 문제를 다루고 있습니다.​요약 : 그렇다면 희귀하다는 것의 기준은? 무엇은 희귀하니까 사람에게 가고, 무엇은 안 희귀하니까 오토라벨링으로 하는지 그 기준을 어떻게 정할것인가 conventional long tail classfication VS long tail continuumAnd before I go into the solution, I'll talk a little bit about what is really rare. So defining even what is rare is a bit challenging. So first, rare is defined relative to a given population. For example, cyclists in urban environments are reasonably common, but on highways, they're very rare. And so the concept of rare is relative, not absolute. And second, defining rare in a continuous setting is tricky. So unlike conventional work in long-tail classification, where there is well-defined boundaries between long-tail classes, we're working with a continuum in feature space. So the class vehicle can have all kinds of different types of vehicle, shapes, and sizes. Here, things are a lot more homogeneous, typically. And also, unlike conventional works that treat long-tail classification as a domain adaptation problem-- and domain adaptation is usually the task of matching training and testing distributions better-- we are mostly looking at the problem of increasing data support we are mining. So we want to have a definition of rareness that mostly helps us fill in this distribution without relying as much on ontology, which is the new challenge.​해결책에 들어가기 전에, 정말 희귀한 것이란 무엇인지에 대해 조금 말씀드리겠습니다. 그래서 희귀한 것을 정의하는 것은 조금 어렵습니다. 먼저, 희소성은 주어진 모집단에 비례하여 정의됩니다. 예를 들어, 도시 환경에서 자전거를 타는 사람들은 꽤 흔하지만, 고속도로에서는 매우 희귀합니다. 그래서 희귀의 개념은 절대적인 것이 아니라 상대적인 것입니다. 둘째, 연속적인 환경에서 희귀성을 정의하는 것은 까다롭습니다. 그래서 롱테일 분류의 경계가 잘 정해져있는 롱테일 분류의 전통적인 연구와는 달리, 우리는 feaure space에서 연속체(continuum)를 가지고 작업하고 있습니다. 그래서 클래스 차량은 모든 종류의 다른 종류의 차량, 모양, 크기를 가질 수 있습니다. 여기서, 사물은 일반적으로 훨씬 더 동질적입니다. 또한, 롱테일 분류를 도메인 적응 문제로 취급하는 기존 작업과 달리 -- 도메인 적응은 일반적으로 훈련과 분포를 더 잘 매칭하는 작업입니다 -- 우리는 대부분 추출 중인 데이터 지원을 늘리는 문제를 보고 있습니다. 그래서 우리는 존재론에 의존하지 않고 이 분포를 채우는 데 도움이 되는 희귀성의 정의를 얻고자 합니다. 이것이 새로운 도전과제입니다.​요약 : 먼저, 모집단에 비례해서 희귀함이 정의됨. 도시환경에서 자전거 타는 사람은 흔하지만, 고속도로에서는 희귀함. 이처럼 희귀하다는 건 상대적임그리고, conventional long tail classfication (기존의 경계가 잘 나눠져있는 경우) VS long tail continuum (잘 나눠져있지 않고, 연속적인 환경에서 정의해야함. 모든 종류의 형태 크기 모양을 가질 수 있음) So here in this slide, I will show you a little bit of intuition about what rare examples mean and how they differ from hard examples. So let's have y-axis easy to hard, and on the x-axis, we have common to rare. So when you look at this, you will see that hard examples are objectively ambiguous, even for humans. For example, occluded vehicles can be very hard. You barely see some of the parts of it, or you see very few points. It's hard to make a bounding box. But occluded objects are actually very common. They are not necessarily rare. Training on more hard examples like that does not help improve the model. We've already seen a lot. And rare examples are infrequent in the training set. Addition of rare data, like this one, can provide better coverage in the data distribution. We want these examples. And so how do we get these examples? I will show you next two different definitions of finding rare.​그래서 여기 이 슬라이드에서, 저는 여러분에게 희귀한 예들이 무엇을 의미하는지 그리고 그것들이 어려운 예들과 어떻게 다른지에 대한 약간의 직관을 보여줄 것입니다. 그래서 y축은 쉬운지, 어려운지, x축은 흔한지, 안흔한지를 나타냅니다. 그래서 여러분이 이 도표를 볼 때, 여러분은 어려운 예들이 객관적으로 모호하다는 것을 알게 될 것입니다, 심지어 인간에게도요. 예를 들어, 일부가 가려진 차량(회색)은 매우 어려울 수 있습니다. 일부분을 거의 볼 수 없거나, 아주 적은 부분을 볼 수 있습니다. 바운딩 박스를 만드는 것은 어렵습니다. 하지만 일부가 가려진 물체는 실제로 매우 흔합니다. 어려운 것이 반드시 드문 것은 아닙니다. 이와 같은 어려운 예에 대한 교육은 모델을 개선하는 데 도움이 되지 않습니다.(회색 부분 데이터는 별로 도움 안됨) 우리는 이미 많이 봤습니다. 그리고 훈련 세트에서는 드문 예​가 있습니다. 이와 같은 희귀 데이터를 추가하면 데이터 분포에서 더 나은 적용 범위를 제공할 수 있습니다. 우리는 이 예들을 원합니다. 그러면 어떻게 이런 예들(노란색, 파란색)을 얻을 수 있을까요? 다음 두 가지 정의를 보여드리겠습니다. So one of them, we call model-centric rareness. The key here is to look at all the set of uncertain examples, and so they can be all in all these three quadrants. And so we take the uncertain examples, which we can mine by, for example, an ensemble of detectors, and seeing that, wherever they disagree, you have uncertainty. But you want to remove the hard objects. And hard, if you have a reasonable definition of hard-- in our case, we can say, well, too few LiDAR points, or object is maybe too far away-- then you'll get your definition of rare. Now, this model-centric rareness works whenever you have some domain knowledge or understanding of what might be hard. But often, that knowledge is limited, right? So maybe there are some other examples that are common that we cannot enumerate here in a natural way.​그 중 하나는 ①모델 중심적 희귀성입니다. 여기서 핵심은 모든 불확실한 예들을 살펴보는 것입니다. 그래서 그것들은 이 세 사분면에 모두 있을 수 있습니다. 그래서 우리는 불확실한 예를 들어,detectors의 앙상블을 통해 추출할 수 있습니다. 그리고 detector들이 동의하지 않는 곳이라면 어디든 불확실성을 가지고 있다는 것을 봅니다. 하지만 당신은 어려운 물체(hard)를 제거하고 싶어합니다. 그리고 만약 여러분이 어려움에 대한 합리적인 정의를 가지고 있다면-- 우리의 경우, 우리는, 너무 적은 LiDAR 점, 혹은 물체가 너무 멀리 떨어져 있다고 말할 수 있습니다-- 그러면 여러분은 희귀에 대한 정의를 얻을 수 있을 것입니다. 자, 이 모델 중심성은 여러분이 도메인 지식이 별로 없거나 무언가에 대한 이해가 어려울 때마다 작동합니다. 하지만 종종, 그 지식은 제한적입니다, 그렇죠? 그래서 어쩌면 우리가 여기서 자연스러운 방법으로 열거할 수 없는 몇 가지 흔한 예들이 있을지도 모릅니다.​요약 : 먼저 모델 중심적 희귀성으로 설명. uncertain(파란 점선 바운더리에 해당하는 세개)에서 hard(노란점, 하늘색점)을 빼면 rare가 남음(빨간 점)하지만 이렇게 하면 도메인지식이 없을때마다 작동 or 무언가 이해가 어려울 때마다 작동하지만, 그 지식이 제한적일 수 있음.  So we have an alternative definition of rareness, which is called data-centric rareness. And this does not make any special assumption on what might be hard, right? And we instead obtain a direct estimate of object rareness by doing an estimation of the probability density of samples in the embedding space of a pretrained detector model. So you have a detector. You have an object that you may think you want to check is rare or not. You have a prototypical bounding box. You can pull the features, region-pull the features, in this bounding box to obtain an embedding. And then we can-- for all such embeddings of interest, we can train a normalizing flows model to estimate the probability density. This is an invertible model. And here, I'm showing you an intuition of the distribution that such a model can give you. So what's closer to red is an area of higher density, and what is in blue is an area of lower density. And so lower density is what you want to mine because you haven't seen anything with those type of features much before, and so you want to label them too. ​그래서 우리는 희귀성에 대한 대안적인 정의를 가지고 있는데, 이것은 ②데이터 중심 희귀성이라고 불립니다. 그리고 이것은 무엇이 어려울 수 있는지에 대한 특별한 가정을 하지 않습니다, 그렇죠? 그리고 대신 사전 훈련된 검출기 모델의 내장 공간에서 샘플의 확률 밀도를 추정하여 객체 희귀성의 직접적인 추정치를 얻습니다. detector가 있고, 희귀하거나 그렇지 않은 것으로 확인하려는 개체가 있습니다. 프로토타입 바운딩 상자가 있습니다. 이 바운딩 상자에서 피쳐를 끌어다 지역별로 끌어다 놓으면 임베딩을 얻을 수 있습니다. 그런 다음, 이러한 모든 관심 임베딩에 대해, 우리는 확률 밀도를 추정하기 위해 정규화 흐름 모델을 훈련시킬 수 있습니다. 이것은 가역 모형입니다. 그리고 여기 오른쪽 그래프에서는 이 모델이 여러분에게 줄 수 있는 분포에 대한 직관을 보여드리고 있습니다. 빨간색에 더 가까운 것은 밀도가 높은 영역이고 파란색은 밀도가 낮은 영역입니다. 그래서 더 낮은 밀도로 추출하는 것은 여러분이 원하는 것입니다. 왜냐하면 여러분은 이런 종류의 특징을 가진 어떤 것도 본 적이 없기 때문입니다. 그래서 여러분은 그것들에 라벨을 붙이기를 원합니다.​요약 : 무엇이 어려울 수 있는지에 대한 가정 X. 사전 훈련된 검출기 모델의 내장 공간에서 샘플(희귀하거나 그렇지 않은 것으로 확인하려는 개체들)의 확률 밀도를 추정하여 객체 희귀성의 직접적인 추정치를 얻습니다. 밀도가 높은 부분은 빨간색, 밀도가 낮은 부분은 파란색. 파란색이 우리가 원하는 것. 그래서 여기에는 사람이 라벨링함.  And let's look at what the results look like. So the baseline is on top. We label 10% of data with humans, and this is the baseline. This isn't a set of results we get. And here, I will focus on an example where we might need an additional 3% by using an ensemble of detectors. So we'll look for areas where the detectors are uncertain, and we label them. So you can see that this is a very standard technique, right? And you can see that, yes, with this method, we get some improvement in the regular objects. But here, for large objects, which is our proxy rare subcategory, we actually see a decline. And below, you can see our two respective model-centric definitions, model-centric and data-centric REM, especially both of them improve even more both the regular and the long-tail cases. So that value helps validate that our definition is helpful. And then, of course, on the bottom, we add auto labeling, which is where we started. If you add the auto labeling system and data-centric REM, you get yet stronger results because auto labeling is generally helpful for training models. And if we really, really want to maximize performance on the rare class, then having both model and data centric is even better, right? ​그리고 결과가 어떻게 보이는지 살펴보죠. 그래서 기준선이 도표의 맨 위에 있습니다. 우리는 10%의 데이터를 사람이 분류합니다. 이것이 기준입니다. 이것은 우리가 얻는 일련의 결과들이 아닙니다. 그리고 도표에서 셋째 줄에서는 detectors의 앙상블을 사용하여 3%가 추가로 필요할 수 있는 예에 초점을 맞출 것입니다. 그래서 우리는 detectos가 불확실한 부분을 찾고, 그것들을 분류할 것입니다. 이것이 매우 표준적인 기술이라는 것을 알 수 있습니다. 그렇죠? 보시다시피, 이 방법으로 우리는 일반적인 물체에서 약간의 개선을 얻을 수 있습니다. 하지만 여기서, 우리가 rare 한 경우로 설정한 큰 물체의 경우, 우리는 실제로 감소를 볼 수 있습니다. 아래에서는 모델 중심 REM과 데이터 중심 REM이라는 두 가지 각각의 정의를 볼 수 있습니다. 특히 두 가지 모두 일반 사례와 롱테일 사례를 훨씬 더 개선합니다. 그 가치는 우리의 정의가 도움이 된다는 것을 입증하는 데 도움이 됩니다. 그리고 나서, 물론, 하단에, 우리는 오토 라벨링을 붙입니다. 이것이 우리가 시작했던 곳입니다. 자동 레이블링 시스템과 데이터 중심 REM을 추가하면 자동 레이블링이 일반적으로 교육 모델에 도움이 되기 때문에 더욱 강력한 결과를 얻을 수 있습니다. 그리고 우리가 정말로 정말로 희귀 클래스에서 성능을 극대화하고 싶다면 모델과 데이터 중심 모두를 갖는 것이 훨씬 더 낫습니다. 그렇죠?​요약 : human label 10%가 기준. 세번째 줄은 detector를 3% 사용하는 것.불확실한 부분에서 컴퓨터가 자동으로 라벨링함. 성능이 올라가지만 large(희귀 클래스)에서는 성능이 감소함. 이후 Model-centric REM과 Data-centric REM 모두 성능이 올라감. Data-centric REM과 오토라벨링(제일 처음 얘기했던)을 합치면 성능 더 높아짐. 그리고 Model-centric REM과 Data-centric REM과 오토라벨링 다 합치면(제일 마지막) 성능도 높고, 무엇보다 large(희귀 클래스)에 대한 성능이 제일 높음.  ​ And so when you come back here to the initial case, you can see that, with this system, we mostly did manage to close the gap in rare objects and improved by relative 30% among vehicles with very limited amount of rare example mining and labeling by humans, which is great. ​그래서 여기 초기 사례로 돌아가보면, 이 시스템을 통해 우리는 100% 오토 라벨링 한 것과, 13% 오토라벨링한 것 (87%에서는 희귀한 것, 안희귀한 것으로 나눠서 희귀한 부분은 사람이 라벨링하는 식으로 함)에서 발생한 희귀 물체에 대한 격차를 좁히고(초록색), 희귀 차량에 대한 퍼포먼스가  30.97% 개선되었다는 것을 알 수 있습니다. 이것은 훌륭한 일입니다. Waymo — Research — LESS: Label-Efficient Semantic Segmentation for LiDAR Point CloudsOne more shout out for a poster for you guys to check out at this conference. I showed you how to mine rare examples for object detection, but we also have work on efficient labeling for 3D semantic segmentation, which is a typically very effort-intensive task. This will be an ECCV oral talk, so I invite you to check it out. Just in summary, we co-designed an efficient labeling process and a learning algorithm that can take very sparse scribbles in data or labels in the LiDAR point clouds. And so with such a method, we can match the quality of SemanticKITTI by using just 0.1% of all LiDAR points being labeled. So, yeah, please check it out. ​이 컨퍼런스에서 확인할 포스터를 한 번 더 보여드리겠습니다. 객체 감지를 위한 드문 예제를 추출하는 방법을 보여드렸지만, 일반적으로 매우 노력 집약적인 작업인 3D 의미 분할을 위한 효율적인 레이블링 작업도 있습니다. 이번 강연은 ECCVolutional Talk가 될 예정이오니, 꼭 확인해보시기 바랍니다. 요약하자면, 우리는 LiDAR 포인트 클라우드의 데이터 또는 레이블에 매우 희박한 scribbles를 취할 수 있는 효율적인 레이블 지정 프로세스와 학습 알고리즘을 공동 설계했습니다. 그래서 그런 방법으로 우리는  SemanticKITTI의 품질과 일치할 수 있습니다. 라벨링된 모든 LiDAR 포인트의 0.1%만 사용합니다. 그러니, 네, 확인해 보시길 바랍니다. ​요약 : 지금까지는 object detection을 위한 희귀 케이스 추출 방법을 설명했지만,  3D semantic segmentation에 대한 효율적인 라벨링 작업도 논문으로 냈음. 3. 데이터 증강 개선And I will go on to the next topic, which is data augmentation. ​그리고 저는 다음 주제인 데이터 증강으로 넘어갈게요.​*데이터 증강(data augmentation) : 실질적으로 새로운 데이터를 수집하지 않고도 교육 모형에 사용하는 데이터의 다양성을 늘릴 수 있는 전략 So we just talked about mining and labeling new rare examples, but we can also focus on making the most of the examples we have, right? So we can try to apply augmentations, especially to the existing rare examples. And so augmentations is a well known technique, data augmentation to improve neural networks. And it's been an area of work by our team for a while. So here, I've illustrated a set of potential augmentation. You can apply to an object. You can flip it or scale or drop laser points and so on. So there's quite a number of things you can do. And the set of augmentations has a set of parameters associated with it-- how often you apply the operation, how much of it you apply. And so these parameters actually affect the final performance. It's a reasonably large search space, about 30 parameters or so. And search in that parameter space for the best policy is, at worst case, exponential in the number of these parameters.​그래서 우리는 방금 새로운 희귀한 사례들을 발굴하고 분류하는 것에 대해 이야기했습니다. 하지만 또한 우리가 가지고 있는 사례들을 최대한 활용하는 데 집중할 수 있습니다. 그렇죠? 그래서 우리는 증강을, 특히 기존의 희귀한 예에 적용하려고 시도할 수 있습니다. 그래서 증강은 잘 알려진 기술입니다. 신경망을 개선하기 위한 데이터 증강입니다. 그리고 그것은 한동안 우리 팀의 작업 영역이었습니다. 그래서 여기에 잠재적으로 사용할 수 있는 데이터 확장에 대해 그림으로 표현해놓았습니다. 개체에 적용할 수 있습니다. 뒤집기, 스케일링 또는 레이저 포인트 드롭 등을 할 수 있습니다.  할 수 있는 일은 꽤 많습니다. 그리고 증강에는 이와 관련된 변수들이 있습니다. 얼마나 자주  operation을 적용하는지, 얼마나 많이 operation을 적용하는지 말입니다. 그리고 이러한 변수들은 실제로 최종 성과에 영향을 미칩니다. 30개 정도의 상당히 큰 검색 공간입니다. 그리고 이 매개 변수 공간에서 최상의 정책을 검색하면 최악의 경우 이러한 매개 변수의 수가 기하급수적으로 증가합니다.  And so we have two works to address this over the last couple of years, and they're both handling the search with a fairly sophisticated method called progressive population-based augmentation, which involves a population of models trained with different augmentation settings. So you have, say, this model with some augmentation that does reasonably well. Here's another model with some other augmentation that does also reasonably well. So then every once in a while, we resample the population of models and try to splice in different augmentations from different successful models. ​그래서 우리는 지난 몇 년간 이 문제를 해결하기 위한 두 가지 작업을 하고 있습니다. 그리고 그들은 둘 다 서로 다른 확대 설정으로 훈련된 모델들의 모집단을 포함하는 progressive population-based augmentation라고 불리는 꽤 정교한 방법으로 검색을 처리하고 있습니다. 예를 들어, 이 모델에는 상당히 잘 되는 확장 기능이 있습니다. 여기 또 다른 모델이 있습니다. 이 모델도 상당히 잘 됩니다. 그래서 때때로 우리는 모델의 모집단을 다시 표본으로 추출하고 다른 성공적인 모델로부터 다른 증강을 분리하려고 합니다. And this works reasonably well, but I think-- here is an example. So what does it mean, reasonably well? So as you-- it can push the performance for maybe the black curve to the blue curve in the supervised case. So you're only using sequences where you have done bounding boxes. Here is on the x-axis shows what percent of sequences that is. This is quality. And orange adds-- so let's say, here, at 30%, using 30% label and 70% unlabel sequences, we can still eke some benefit at that operating point. So we can push generally black to orange, which is significant gains. So data augmentation helps. However, such methods like PPBA require complex infrastructure. So it's not clear that a small population of models evolved over many steps can even find the optimal parameter settings. And so we set ourselves the question, can we do better and simpler? ​그리고 이것은 꽤 잘 작동하지만, 제 생각엔-- 여기 한 예가 있습니다. 그렇다면 합리적으로 잘했다는 것은 무엇을 의미할까요? 그래서 감독된 케이스에서 검은색 곡선의 성능을 파란색 곡선으로 끌어올릴 수 있습니다. 따라서 바운딩 박스를 수행한 시퀀스만 사용합니다. x축은 시퀀스의 백분율을 나타냅니다. Y축이 퀄리티입니다. 그리고 오렌지가 더해진다고 합시다. 여기 오렌지색 점이 30%의 라벨과 70%의 라벨되지 않은 시퀀스를 사용한다고 가정해 봅시다. 우리는 여전히 그 운영 지점에서 약간의 이익을 얻을 수 있습니다. 그래서 우리는 (vehicle도 그렇고 pedestrain에서도 그렇고) 일반적으로 검은색에서 주황색으로 바꿀 수 있는데, 이것은 상당한 이득입니다. 데이터 확대가 도움이 됩니다. 그러나 PPBA와 같은 방법에는 복잡한 인프라가 필요합니다. 따라서 여러 단계에 걸쳐 진화한 소수의 모델 모집단이 최적의 매개 변수 설정을 찾을 수 있다는 것은 분명하지 않습니다. 그래서 우리는 우리 스스로에게 질문을 던졌습니다. 더 좋고 간단하게 할 수 있을까요? And so this is a working submission, but we're presenting it anyway. The idea here is, can we define a simple search space with just two hyperparameters as opposed to close to 30 in the general case? And then we do a simple grid search in that space to explore it better. ​그래서 이것은 작업 중인 내용입니다. 어쨌든 우리는 그것을 제시합니다. 여기서 아이디어는, 일반적인 경우에서 30에 가까운 것이 아니라, 두 개의 초 매개 변수만으로 간단한 검색 공간을 정의할 수 있는가 하는 것입니다. 그리고 우리는 그 공간에서 더 나은 탐사를 더 잘 하기 위해 간단한 격자 검색을 합니다. And this idea is inspired by the RandAugment work by Google Brain from CVPR 2020. And the high-level idea here is we want the factorize the set of searches. And so we do-- for each augmentation operation, we do a separate small-scale proxy search for two parameters. So there's the probability of applying the operation, and then maybe some parameter that corresponds to magnitude-- for example, what's the maximum rotation angle? How big can it be? So we search and find good value for these. You can do it with just small search, just for that operation, compared to the baseline and operations. ​이 아이디어는 CVPR 2020의 Google Brain의 LandAugment 작업에서 영감을 받았습니다. 여기서 중요한 것은 검색 집합을 인수분해해야 한다는 것입니다. 그래서 우리는 각각의 증강 작업에 대해 두 개의 매개 변수에 대해 별도의 소규모 프록시 검색을 수행합니다. 그래서 이 작업을 적용할 확률(p)도 있고, 그 다음 크기(m)에 해당하는 매개 변수도 있을 수 있습니다. 예를 들어, 최대 회전 각도는 얼마입니까? 얼마나 클까요? 그래서 우리는 이것들에 대한 좋은 value를 찾아냅니다. 이 작업은 기준선 및 작업에 비해 해당 작업에 대해서만 소규모 검색으로 수행할 수 있습니다.​ And then we align. So we want to normalize and center these parameters. So for each operation, the optimal parameter should map to roughly the same values. ​그리고 정렬합니다. 그래서 우리는 이 매개 변수들을 정규화하고 중심화하기를 원합니다. 따라서 각 작업에 대해 최적의 변수가 거의 동일한 값에 매핑되어야 합니다. And then in this aligned and normalized and centered space, we just look 2D grid search and two parameters, p and m. So more p and m or less p and m. And the search-- so given a set of values for p and m, this is the full algorithm. It's very short pseudocode. You basically apply augmentations with some magnitude, and that is it.​그리고 이 정렬되고 정규화된 중심 공간에서는 두 가지 매개 변수인 p와 m으로 2D 그리드 검색을 합니다. 그래서 더 많은 p와 m 혹은 더 적은 p와 m입니다. 그리고 검색은 -- 그래서 p와 m에 대한 값들의 집합이 주어지면, 이것(초록색 코드 창)은 완전한 알고리즘입니다. 아주 짧은 의사 코드입니다. 기본적으로 어느 정도 크기의 증강을 적용하면 됩니다.​ And how well does this do? Well, it turns out it does a lot better than PPBA, so almost 5 points better. For this example, it's U-shaped PointPillar, and we push the quality dramatically on the Waymo Open Data set validation. And there is some other interesting insights.​이게 얼마나 잘 될까요? 음, PPBA보다 훨씬 더 나은 것으로 밝혀졌습니다. 그래서 거의 5점이 더 낫습니다. 이 예에서는 U자형 PointPillar로 Waymo Open Data 세트 검증에서 품질을 크게 높입니다. 그리고 또 다른 흥미로운 통찰이 있습니다.​ So we experimented with taking the U-shaped PointPillar model and scaling it to at least twice the capacity. And then we take the initial baseline augmentation operations that are usually operations that are popular for U-shaped PointPillar models, and we apply them on the two models. And what happens is-- well, that's the blue boxes. So, yes, you improve by having a larger model, but the gain is limited, and even some metrics can get worse. Now, applying LidarAgument custom search for each architecture makes a dramatic difference. And what's particularly interesting is the large model. It pushes the quality, where the quality gap is even higher. And so the insight is we actually want to optimize the augmentation strategy when changing model architecture or capacity. This is intuitive. The more the larger the capacity is, probably the more augmentations you want to apply. But this only proves that intuition. ​그래서 우리는 U자형 PointPillar 모델을 가지고 두 배이상의 용량(capacity)으로 확장해 보았습니다. 그런 다음 U자형 PointPillar 모델에 일반적으로 인기 있는 초기 기준 확대 작업을 가져와서 두 모델(Upillars, Upillars-L, 처음 껀 빨간색 사각형 표시된 점수 보면 됨)에 적용합니다. 그리고 무슨 일이 일어났냐면, 결과가 파란 박스입니다. 더 큰 모델을 사용함으로써 개선되지만, 그 이득은 제한적이며 일부 메트릭스도 더 나빠질 수 있습니다. (빨간색 점수보다 파란색 박스 점수가 더 낮음) 이제 각 아키텍처에 대해 LidarAgument를 사용자 지정 검색을 적용하면 극적인 차이가 발생합니다. 특히 흥미로운 것은 대형 모델(Upillar-L)입니다. 그것은 품질을 좋게 만들어내고, 품질 차이는 더 커집니다. 따라서 모델 아키텍처 또는 용량을 변경할 때 확장 전략을 최적화해야 합니다. 이것은 직관적입니다. 용량(capacity)이 클수록 더 많은 증강을 적용할 수 있습니다. 하지만 이것은 그 직관을 증명할 뿐입니다. And one interesting thing that happens is you take a fairly simple model, which is PointPillars, and you push it to 71% on the Waymo Data set. If you look here, the vanilla PointPillar is at just 55, and we got it to 71, which is very competitive now to some of the state of the art. So PVRCNN, dramatically-- well, more sophisticated method, and now we're almost matched it with this simple method. So that's not to be underestimated. The second thing I would point out is-- so SWFormer, we set the new state of the art on the Waymo Open Data set in our LiDAR detection methods. Applying augmentation pushes the state of the art even higher, so now we are 74.8%. ​한 가지 흥미로운 점은 상당히 간단한 모델인 포인트 필라(PointPillars)를 Waymo Data 세트에서 71%까지 끌어올린다는 것입니다. 여기 보시면 가장 기본인 포인트 필러가 55에 불과합니다. 그리고 71에 도달했습니다. 이것은 현재 최첨단 기술들과 매우 경쟁적입니다. PVRCNN, 극적으로 -- 음, 더 정교한 방법이죠. 그리고 이제 우리는 거의 이 간단한 방법과 일치합니다. 그래서 그것은 과소평가되지 않습니다.  제가 두 번째로 지적하고 싶은 것은 SWFormer는 LiDAR 탐지 방법에 있는 Waymo Open Data 세트에 새로운 최첨단 기술을 설정했다는 것입니다. 증강을 적용하면 최첨단 기술이 훨씬 더 높아지므로 이제 우리는 74.8%입니다. ​요약 : 제일 기본인 PointPillars(P.Pillars)가 처음에 55에 불과. 그런데 웨이모의 데이터 세트인 PVRCNN을 통해 71에 도달. 그리고 오늘 소개한 방식으로 SWFormer로 73.4를 만들고, 그리고 거기에 LidarAugment(LA) 더하면 74.8로 제일 성능 높아짐. ​즉, 데이터 증강을 이용해서 트레이닝 시키니까 detection 성능이 높아졌다.  4. 오픈 세트 인식과 예측Last topic I will talk about is open set perception and prediction. 마지막으로 말씀드릴 주제는 오픈 세트 인식과 예측입니다. Waymo — Research — Motion Inspired Unsupervised Perception and Prediction in Autonomous DrivingThis is from a paper also in this conference. And the motivation here is the following.​이것은 이 컨퍼런스에 있는 논문에서도 나온 것입니다. 여기서 동기는 다음과 같습니다. 첨부파일aml.pdf파일 다운로드 So we talked so far about 3D object detection models that are bootstrapped with expensive human labels and defined to handle a preset number of object types. So this paradigm is quite successful by now in ordinary examples. ​그래서 지금까지 고가의 인간 라벨링으로 부트스트랩되고 미리 설정된 수의 객체 유형을 처리하도록 정의된 3D 객체 감지 모델에 대해 이야기했습니다. 그래서 이 패러다임은 지금까지 일반적인 사례에서 꽤 성공적이었습니다. And also, with the techniques I just presented in this talk, you can get it quite successful on handling even long-tail examples from these classes.​그리고 또한, 제가 방금 이 강연에서 소개한 기술들로, 여러분은 이 수업에서 롱테일 예시를 다루는 것을 꽤 성공적으로 얻을 수 있습니다.​ However, beyond this pretty fine closed set, there are many more object types. So we could keep extending our ontologies and naming them and mining for each type separately. But the natural question is, can we do a more scalable approach?​하지만, 이 꽤 괜찮은 클로즈 세트 외에도, 더 많은 객체 유형이 있습니다. 그래서 우리는 온톨로지를 계속 확장하고 각 유형에 대해 개별적으로 이름을 지정하고 추출할 수 있었습니다. 하지만 자연스러운 질문은, 우리가 좀 더 확장 가능한 접근 방식을 할 수 있을까 하는 것입니다.​요약 : 이외에도 굉장히 많은 객체들이 존재함. 물론 계속 레이블 달아주고, 추가해줄 수 있겠지만, with no annotation하는 방식으로! 좀더 확장가능하게 접근할 수 있을까?  And so, of course, we do want to deal with dinosaurs running around and with taxiing airplanes on the road too. After all, if they can be on the road, we want to do something reasonable. But we don't want to name them and mine specifically this object. So we have started leveraging motion estimation, which is a semantics-agnostic task, as a source of supervision for objects like this. And I believe this is an exciting direction, and I will present to you briefly the specific details of our work at this conference and the problem formulation for this task. ​그래서 물론, 우리는 뛰어다니는 공룡들과 길에서 활주하는 비행기들을 다루기를 원합니다. 결국, 그런 것들이 도로 위에 존재한다면, 우리는 합리적인 무언가를 하고 싶습니다. 하지만 우리는 그들의 이름을 붙이고 싶지는 않습니다. 그리고 특별히 이 물체를 추출하고 싶지는 않습니다. 그래서 우리는 의미론에 구애받지 않는 작업인 움직임 추정을 이와 같은 사물에 대한 학습 원천으로 활용하기 시작했습니다. 그리고 저는 이것이 흥미로운 방향이라고 믿으며, 저는 이 회의에서 우리의 작업에 대한 구체적인 세부사항과 이 과제를 위한 문제 공식에 대해 간략하게 발표할 것입니다. So our specific method takes raw LiDAR sequences as input. And to obtain unsupervised auto labels, we designed two key components of simple estimation and then automate the labeling. Without human labels, we can estimate the 3D scene flow we relied upon. Then in the blue box, we go through a series of clustering, tracking, shape registration to obtain object labels and 3D bounding boxes for these tracklets, which essentially creates automatic training data on which we can apply-- I mean, ultimately train 3D object detectors and also trajectory predictors for these objects. This is just one method. I think this is still relatively early in this task. A lot of headroom is possible. I invite the community to consider it more seriously. It's a fascinating task.​그래서 우리의 특정 방법은 raw LiDAR 시퀀스를 입력으로 사용합니다. 그리고 비지도 오토 레이블을 얻기 위해 간단한 추정의 두 가지 핵심 구성 요소를 설계한 다음 레이블을 자동화했습니다. 인간 라벨이 없어도 (이전에는 의존했던) 3D 장면 흐름을 추정할 수 있습니다. 그런 다음 파란색 상자에서는 이러한 트랙렛(5-6프레임 사이의 짧은 트랙)에 대한 객체 레이블과 3D 바운딩 상자를 얻기 위해 일련의 클러스터링, 추적, 형상 등록을 거칩니다. 기본적으로 적용할 수 있는 자동 훈련 데이터를 생성합니다. 즉, 궁극적으로 3D object detector와 이러한 객체에 대한 trajectory predictors를 훈련합니다. 이것은 단지 하나의 방법입니다. 저는 이것이 아직 이 과제에서 비교적 이른 시기라고 생각합니다. 많은 헤드룸이 가능합니다. 저는 커뮤니티가 그것을 더 진지하게 고려하기를 바랍니다. 아주 흥미로운 작업입니다.​ 37:25 동영상 Here is some results. So this is a PointPillar network trained with our system to detect any moving objects, and this is done without any human-labeled bounding boxes whatsoever. And so you can see that it tracks a lot of objects quite well-- pedestrians, vehicles, and quite stably. So this is detector. So it doesn't track them. I mean, it's a frame-by-frame detection. That's why the colors change in this case. ​여기 몇 가지 결과가 있습니다. 그래서 이것은 움직이는 물체를 감지하도록 우리의 시스템으로 훈련된 포인트 필러 네트워크입니다. 그리고 이것은 사람이 라벨링한 바운딩 박스 없이 이루어집니다. 그래서 많은 물체들을 잘 추적하는 것을 볼 수 있습니다. 보행자, 차량, 그리고 꽤 안정적으로 말이죠. 이것은 detector입니다. 그래서 그들을 추적하지 않습니다. 제 말은, 그것은 프레임별 탐지를 합니다. 보시면 색깔이 계속 변하는 이유입니다. And I think one other thing that's interesting is that, because we're not constrained to predefined classes, we can capture objects that typically are not necessarily labeled coherently in the ground truth. For example, well, we label pedestrians often in the ground truth, but that misses the object associated with them, the stroller. Our automatic label method directly infers and correctly infers that this is roughly the same object moving coherently and bounding boxes. ​그리고 또 다른 흥미로운 점은, 우리가 미리 정의된 클래스에 얽매이지 않기 때문에, 우리는 일반적으로 실제 groundtruth에 일관성 있게 라벨이 붙어있지 않은 물체들을 포착할 수 있다는 것입니다. 예를 들어, 우리는 종종 groundtruth에서 보행자에게 라벨을 붙입니다. 하지만 그것은 보행자와 관련된 물체인 유모차를 놓칩니다. 우리의 오토 레이블 방법은 이것이 대개 일관되게 움직이는 동일한 물체라고 바로 추론하고, 정확하게 추론하고, 바운딩박스로 표시​합니다. 3D 감지 / AP(average precision)는 높은 게 좋음AML (Auto Meta Labeling)And in terms of quantitative metrics in the open-set setting, when we assume that human labels are only available for some categories or not others-- so in this case, we can set up a test case where we have only vehicle labels available,but we don't have pedestrian labels, or vice versa. We found that unsupervised auto labels provide very helpful supervision. So we get quite reasonable results for the object we've never seen compared to the case of-- well, this is the supervised case. Obviously, we're not going to detect this object. ​그리고 오픈 세트 환경에서 정량적 측정 기준의 관점에서, 사람이 하는 라벨은 어떤 범주에 대해서만 사용할 수 있고 다른 범주에 대해서는 사용할 수 없다고 가정할 때 -- 이 경우, 우리는 차량 라벨만 사용할 수 있지만, 보행자 라벨은 사용할 수 없는 테스트 케이스를 설정할 수 있습니다. 또는 그 반대의 경우도 마찬가지입니다. 우리는 비지도 자동차 라벨이 매우 유용한 감독을 제공한다는 것을 발견했습니다. 그래서 우리는 우리가 본 적이 없는 물체에 대해 꽤 합리적인 결과를 얻습니다.(초록색 박스) 빨간색 박스로 표시된 건 지도학습된 방법을 사용한 사례입니다. 분명히, 우리는 이 물체를 감지하지 못할 것입니다.​요약 : 사람이 라벨링 해준 경우(supervised) vs Supervised + AML(auto meta labeling) 즉, 마지막 두 행에서 인간 라벨링을 사용할 수 없을때, 알 수 없는 범주를 채우기 위해 unspervised 오토라벨링을 하는 경우.  경로 예측 / minADE(Minimum Average Displacement Error), minFDE(Minimum Final Displacement Error) 낮은게 좋음And similar things happen for prediction. So we have trying to predict the behavior of objects we've never seen. Using our auto labels in addition to the ground truth for one object, that's much better for the other object than just in the purely supervised case. Again, I think it's still early. I think, given the rich structure, multiple sensors, and temporal consistency that we can enforce, we can go quite far here. And I encourage people to explore this task further. ​그리고 비슷한 일들이 예측에 대해서도 일어납니다. 그래서 우리는 우리가 본 적이 없는 물체의 행동을 예측하려고 노력합니다. 하나의 물체에 대한 실측 자료 외에 우리의 자동차 라벨을 사용하는 것은 순수하게 지도학습되는 경우보다 훨씬 더 좋습니다. (초록색 점수가 빨간색 점수보다 낮아서 좋은 것)다시 말하지만, 저는 아직 이르다고 생각합니다. 저는 우리가 시행할 수 있는 풍부한 구조, 다중 센서, 시간적 일관성을 고려할 때, 우리는 여기서 꽤 멀리 갈 수 있다고 생각합니다. 그리고 저는 사람들이 이 과제를 더 탐구하도록 격려합니다. On this, I will conclude my talk. Thank you so much. And you can see a lot more of our work at either this conference or at this website, waymo.com/research, where we have posted a lot of the papers that we have managed to publish so far. Thanks again, and have a great day.​이것으로 저의 이야기를 마치겠습니다.  감사합니다. 그리고 이 컨퍼런스에서나 waymo.com/research/에서 더 많은 우리의 작업을 볼 수 있습니다. 이 곳에 지금까지 우리가 발표해 온 많은 논문들을 게시했습니다. 다시 한 번 감사드리며, 좋은 하루 되시길 바랍니다.​아래는 작년 웨이모 발표 영상입니다.ㄴㄴㄴㄴㄴㄴㄴㄴㄴㄴㄴㄴㄴㄴㄴㄴㄴ Scale AI’s TransformX Presentation with Waymo Head of Research Drago AnguelovWaymo 연구 책임자인 Drago Angelov와 함께하는 Scale AI의 TransformX 프레젠테이션 Nika : Nex...blog.naver.com ​#waymo #웨이모 #자율주행 #구글자율주행 #구글웨이모  "
맥세이프 아이폰 보조배터리 무선 고속충전 지원하고 가벼운 그립뱅크 그립톡 GB-WL3000 ,https://blog.naver.com/sweetk2ss/223110460153,20230524," ​배터리 기술이 발전하면서 전기차의 최대 주행거리는 점점 길어지는 반면 스마트폰의 최대 사용시간은 예전과 비교했을 때 체감되는 수준으로 차이가 나지 않는다. 스마트폰 특성상 탑재할 수 있는 배터리 크기가 한정되어 있고 배터리 전성비는 좋아졌지만 디스플레이 크기가 커지고 해상도와 주사율이 높아지면서 배터리 사용량도 증가하였기 때문이다. 때문에 여전히 보조배터리는 필수로 챙겨 다녀야 하는 기기 중 하나이며, 특히 아이폰의 경우 라이트닝 케이블을 구비해놓지 않은 매장들이 많기 때문에 필요성이 더욱더 크게 느껴진다.​ ​아이폰 보조배터리를 구입하는 사람들 대부분 10,000mAh 제품에 관심을 많이 가질 텐데, 지금까지 다양한 제품을 사용해 본 필자의 경험으로는 모바일 게임을 하루 종일 플레이하는 등 정말 하드하게 스마트폰을 사용하는 사람이 아니라면 굳이 불편한 휴대성을 감수하면서까지 10,000mAh 제품까지 사용할 필요는 없다고 생각된다. 일반적인 사용 패턴으로는 한 번 완충할 수 있는 수준이면 충분하기 때문이다.​ ​맥세이프를 지원하는 3,000mAh 제품 중에는 가방에 휴대하지 않고 스마트폰에 부착해서 들고 다녀도 불편함이 없는 콤팩트한 제품이 상당히 많은데, 필자가 여러분들에게 추천하고 싶은 가벼운 맥세이프 아이폰 보조배터리는 그립톡 기능도 사용할 수 있고 고속 무선충전도 지원하는 그립뱅크 그립톡 GB-WL3000이라는 제품이다.​ ​이 제품의 구성품을 살펴보면 그립뱅크 본체와 USB-C to USB-C 충전 케이블, 사용 설명서, 이지톡, 마운트 2개, 부착가이드, 이지톡 사용설명서가 있다. 이러한 제품 중에는 제공되는 케이블의 길이가 터무니없이 짧아 별도로 케이블을 구입해서 사용해야 하는 제품도 적지 않은데, 이 제품은 케이블 길이가 넉넉해서 불편함이 없다. 그리고 이지톡 부착을 위한 가이드를 제공하는 등 구성품만 보더라도 작은 부분까지 신경 쓴 제품이라는 것을 쉽게 알 수 있다.​ ​참고로 현재 힉스코리아 네이버 스마트 스토어에서 제품 구입 시 그립뱅크만을 위해 직접 제작한 마그네틱 클리어 케이스와 이지톡 NEW 컬러(LEMON, APPLE MINT, SKY BLUE 중 랜덤 1개)를 선착순 증정하는 프로모션이 진행되고 있다. 그리고 리뷰를 작성한 사람 중 총 10명을 추첨하여 네이버 포인트 10,000원을 지급하는 프로모션도 진행 중이다. 선착순으로 증정되는 만큼 아이폰 보조배터리와 그립톡의 필요성을 느끼고 있는 사람이라면 지금 바로 구입하는 것을 추천한다.​ ​무선 고속충전 지원하고 가벼운 맥세이프 아이폰 보조배터리 그립뱅크 그립톡 GB-WL3000 본체의 사이즈는 65 x 81 x 12mm(마운트 미포함)이며, 무게는 101g(±5g, 이지톡 미포함)이다. 재질은 폴리카보네이트와 ABS. PVC가 사용되었다. 컬러는 블랙 컬러와 화이트 컬러 두 가지 컬러가 있으며, 콤팩트한 사이즈라 아이폰에 부착한 상태에서도 렌즈가 가려지는 현상이 발생하지 않는다. 그리고 두께 또한 12mm로 상당히 얇아 손에 폰을 쥐었을 때 그립감이 불편하지 않다. 상단부 중앙에는 그립톡 연결부가 위치하고 있고 좌측 하단에는 배터리 상태를 확인할 수 있는 3개의 LED가 위치하고 있다.​ ​USB-C 포트가 위치한 부분에는 내부에 발생하는 열이 잘 빠져나가도록 작은 홀들이 있으며, 전원 버튼은 측면에 위치하고 있다.​ ​맥세이프 아이폰 보조배터리 제품 중에서는 자성이 약해 마그네틱 케이스 사용 시 조금만 흔들어도 분리되는 제품이 적지 않은데, 무선 고속충전 지원하는 그립뱅크 그립톡 GB-WL3000은 4200G 자성으로 강력하게 부착된다. 참고로 4200G에서 G는 자속밀도를 나타내는 단위인 가우스를 의미하며, 값이 클수록 자성이 세다.​ ​이 제품은 아이폰뿐만 아니라 무선 충전을 지원하는 스마트폰이라면 위에 올려놓기만 해도 충전이 가능하다. 고속 무선 충전을 지원하는 아이폰 시리즈는 10W 고속 무선 충전이 가능하고 안드로이드 스마트폰 또한 10W 고속 무선 충전이 가능하다. 그리고 아이폰 8/X/11처럼 고속 무선 충전을 지원하지 않는 제품들은 7.5W 일반 무선 충전이 가능하고 에어팟, 갤럭시 버즈는 5W 일반 무선 충전이 가능하다. 참고로 갤럭시 워치나 애플 워치의 경우 자체 무선 충전 규격을 갖고 있어 무선 충전이 불가능하다.​무선 충전 패드에 금속성의 물질을 자동으로 감지하여 충전을 중단하는 FOD(Foreign Object Detection) 기능이 탑재되어 있고 실리콘 써멀 패드와 알루미늄 방열필름을 탑재하여 발열로 인해 무선 충전이 중단되는 것을 막아준다. 그리고 보조배터리의 경우 주로 야외에서 사용하기에 본체의 내구성도 상당히 중요한데, 이 제품은 단단한 PC와 강력한 네오디뮴 자석이 사용되어 내구성이 굉장히 뛰어나다.​ ​보조배터리 없이 그립톡만 사용하고 싶은 사람들은 구성품에 있는 마운트를 스마트폰 본체 혹은 스마트폰 케이스에 부착하면 된다. 제공되는 가이드를 사용하면 보다 쉽게 부착이 가능하다. 마운트 테이프는 끈적임이 남지 않는 3M 투명 테이프가 적용되어 있어 추후 제거도 쉽게 할 수 있다.​ ​그립톡(이지톡)을 끼우고 난 후 위 사진에 동그라미로 표시된 부분은 12시 방향으로 향하도록 세팅하는 것이 좋다. 기본적으로 이 제품의 그립톡(이지톡)은 단단하게 고정되긴 하나 혹시라도 빠지는 것을 방지하기 위해서는 12시 방향으로 향하게 하는 것이 좋다. 참고로 이지톡 사이즈는 45 x 45mm이며, 무게는 7g이다. 재질은 ABS, 실리콘, TPU가 사용되었다.​  ​맥세이프 아이폰 보조배터리는 사용하고 싶지만 그립톡을 포기할 수 없어 유선으로 충전하는 분들이 적지 않을 듯한데, 이제는 그립톡과 맥세이프 기능을 모두 사용할 수 있고 무선 고속충전까지 지원하고 가벼운 그립뱅크 그립톡 GB-WL3000을 통해 보다 편안한 스마트폰 라이프를 누리길 바란다.​앞서 이야기했듯 현재 힉스코리아 네이버 스마트 스토어에서 제품 구입 시 그립뱅크만을 위해 직접 제작한 마그네틱 클리어 케이스와 이지톡 NEW 컬러(LEMON, APPLE MINT, SKY BLUE 중 랜덤 1개)를 선착순 증정하는 프로모션, 리뷰를 작성한 사람 중 총 10명을 추첨하여 네이버 포인트 10,000원을 지급하는 프로모션이 진행 중이며, 제품 특징 및 프로모션에 대한 보다 자세한 내용은 아래의 링크에서 확인할 수 있다.​ 힉스코리아 그립뱅크 고속 무선충전 맥세이프 보조배터리 아이폰 그립톡 GB-WL3000 : 힉스코리아 공식몰맥세이프, 맥세이프보조배터리, 아이폰보조배터리, 무선보조배터리, 고속충전보조배터리, 미니보조배터리, 휴대용보조배터리, 가벼운보조배터리, 핸드폰악세사리, 핸드폰손잡이, 탈부착그립톡, 맥세이프그립톡, 힉스코리아, 아이폰, 갤럭시smartstore.naver.com  "
EDTER: Edge Detection with Transformer ,https://blog.naver.com/taeeon_study/222759184488,20220604,"이번에 영상처리 프로젝트에서 edge detection을 러닝으로 접근해보라는 과제가 있었다. 단순히 SVM이나 SLP. MLP로 접근하여 직접 구현해보니 성능이 영... 내가 구현을 못한 거일 수도 있고 학습 데이터가 적어서 실제로 일반화 능력이 부족했을 수도 있다... 물론 나의 코딩 실력은 아직 한참 멀었지만 ㅎ어쨌거나 edge detection을 러닝으로 접근한 러닝 네트워크를 찾아보다가 이 분야도 마찬가지로 트렌디하게 transformer를 사용하는 네트워크를 하나 발견했다. 흥미로워서  이것을 이번에 한 번 간단하게 리뷰해보도록 하겠다. transformer는 'attention is all you need'라는 논문을 한 번 참고하면 좋을 것 같다. 본래 이건 예전부터 여기 올리기로 마음 먹었었는데 뭔가 미루고 미루다보니..본론으로 돌아와서 간단한 네트워크 리뷰만 조금 해보겠다.HED 같은 CNN 기반 edge detection 접근은 많았다. Deep contour 같은 네트워크들도 이의 예시가 될 수 있다. 다만 최근에 주로 자연어 처리에서 사용됐던 transformer 구조가 비전 분야에도 많이 쓰이기 시작하면서 ViT, Swin transformer 등 유명한 네트워크들이 많이 나왔다. transformer 구조는 long-range dependency를 확보하는데 상당히 좋은 역할을 한다고 알려져있다. ​-       Long-range dependency?​본래 연속적인 데이터는 문맥적인 의존성을 가지게 된다. 연속 데이터에서 각기 다른 시간 차이를 두고 서로 관련성을 가지게 되는데, 관련된 요소가 멀리 떨어져 있을 때 장기 의존성이 존재한다고 한다. 그렇기에 이를 보완하기 위해 RNN, LSTM, GRU 같은 unit들을 넘어 attention이라는 구조가 나오고 attention만을 활용한 transformer 구조가 나오게 된 것이다.​​​본격적으로 EDTER에 대해 설명하도록 한다. 논문에서 이 네트워크는 2단계로 동작한다고 소개한다. 1단계에서는 크게 이미지를 패치로 나누어서 (논문에서는 coarse-grained patch라고 표현함.)전역 transformer 인코더가 long-range인 전역 문맥을 따온다. 2단계에서는 1단계와 달리 작게 이미지를 패치로 나누어서 (논문에서는 fine-grained patch라고 표현함.) 지역적 transformer 인코더라 local한 문맥을 따오게 된다. 그 후에 양방향 Multi-level 통합 디코더를 통과하여 높은 resolution 특징을 따온다. 마지막으로 전역 특징과 지역 특징이 Fusion 모듈을 통해 합쳐지고 결정 head로 넘어가게 된다. 아래는 이를 나타낸 그림이다. overall architecture이제 각각의 stage를 알아보도록 한다.​Stage 1​Stage 1에서는 전체 이미지의 전역적인 문맥과 정확한 정보를 얻는 것이 중요하다. 앞서 언급했듯이 16x16 크기로 coarse-grained patch들로 input 이미지를 나누고 임베딩한다. 그 후에 global transformer에 넣어서 attention을 계산한다. 이 특징들은 BiMLA Decoder에 들어가게 되는데 이 구조는 Stage 2에서도 쓰이게 된다.​BiMLA Decoder​BiMLA Decoder는 Top-down 형식과 Botton-up 형식을 둘 다 사용하여 두 개를 통합하는 역할을 한다. 아래는 이에 대한 그림을 나타낸다.​ BiMLA structure구체적으로 transformer에서 나온 block들을 4개의 그룹으로 나누어 각 그룹의 마지막 block으로부터 임베딩 특징을 뽑아낸다. 이것들이 BiMLA의 input이 되는 것이다. 그 후에 이것들을 3차원으로 reshape 한 후에 1x1 conv 층을 거친다. 이제 이 이미지들이 top-down path와 botton-up path로 들어가게 된다. 이는 그림으로부터 잘 이해 가능하다. Top-down path에서 나온 특징은 t, botton-up path에서 나온 특징은 b이고 t와 b는 벡터라고 이해하면 된다. 또 이 네트워크에서 특이한 점은 deconvolution을 사용했다는 점이다. 이를 통해서 upsampling하고 이 모든 것들은 concatenation하면 1 tensor가 나오게 되고 이것들이 기존에 배웠던 convolution 연산층을 거치게 되면 최종 output인 f_g가 나오게 되는 것이다. 이것은 pixel-level 전역 특징이다.​Stage 2​Stage 2에서는 non-overlapping sliding window를 활용하여 이미지를 sampling 한다. 이것을 통해 {X1, X2, X3, X4}의 seqeunce가 나오게 되고 이것들을 8x8 patch 크기로 나누어 input으로 넣는다. input을 임베딩하고 attention을 계산한다. 그 후에 BiMLA로 들어가게 되는데 Stage 1과 거의 동일하게 그룹으로 나누어 마지막 것들이 BiMLA로 들어가게 된다. 여기서는 transformer의 output이 12개로 이루어져있기에 4개의 그룹으로 나누면 각 3, 6, 9, 12번 째 feature들이 input으로 들어가게 되는 것이다. 마지막으로 Stage 2에서 BiMLA를 쓸 때는 3X3 conv를 1x1 conv로 변경하여 padding 연산에 의해 거짓 에지 검출을 막는다.​각 Stage에 대한 간략한 설명이 끝났고 마지막으로 특징들을 Fusion 해야한다. 이 네트워크에서는 FFM이라는 모듈이 이를 담당하고 있다.​FFM (Feature Fusion Module)​FFM이 global context를 가지고 있고 local context를 맞춘다고 생각하면 편하다. 이 모듈은 spatial transformer block과 3x3 conv layer가 구성되어 있는데 transformer block은 조정을 위한 것이고 3x3 conv는 스무딩을 위한 것이다. 아래는 FFM을 확대한 그림이다. FFM structure이제 이 모듈을 거치고 local decision head를 거치면 E_r을 추출하게 되는 것이다. E_g는 Stage 1에서 단순히 global feature만 가지고 head로 예측한 것이다. 위 전체 architecture 그림을 참고하면 이해하기 쉽다.​training 방식​이 네트워크 훈련 방식은 Stage1을 훈련한 뒤에 Stage1의 파라미터를 고정시키고 Stage2를 훈련하는 방식이다. Loss function은 HED 네트워크와 거의 동일하다고 보면 된다. image-to-image 학습에서 모든 픽셀들이 edge인가 아닌가를 학습하기 위한 손실함수이다. 일종의 cross entropy 함수이다. 다만 non-edge 부분과 edge 부분 비율이 실제 이미지에서 상당히 많이 차이나므로 이를 맞추어주는 작업을 앞 term에서 수행하게 된다. 아래는 이에 대한 loss function을 나타낸 것이다.​ loss functionStage 1의 훈련은 아래와 같다. E_g를 head를 통해 추출하였고 이를 위의 loss function에 넣은 것과 deconv layer를 통해 추출했던 각 특징 벡터 t, b는 모두 8개이다. HED에서 했던 것처럼 이것들을 side output이라고 생각하고 각각을 또 loss function을 넣어 성능 평가를 하면 아래와 같은 식이 만들어진다. Stage 1 train여기서 람다는  balancing weight이다.이 과정을 통해서 Stage 1을 학습시키고 global feature 학습 파라미터는 다 학습된 파라미터로 고정시킨다. 이 상태로 stage 2를 학습하게 된다. Stage1과 마찬가지로 E_r과 ground-truth 사이의 loss function과 BiMLA에서 추출했던 8개의 side output에 대한 loss function을 모두 더해서 학습시킨다.  Stage2 trainside ouput에 대한 개념은 HED 논문을 읽어보는 것을 추천한다.이로써 EDTER의 학습단계까지 간단하게 알아보았다. 아래는 논문에서 나온 성능이다. performanceTransformer를 활용하여 local feature 뿐 아니라 global feature까지 학습하여 edge를 검출하게 되면 detail하게 검출할 수 있음을 볼 수 있다. 이처럼 edge-detection 뿐만 아니라 비전 분야의 다양한 task에서도 global 문맥을 파악한다면 좋은 결과를 얻을 수 있다. 그렇기에 transformer 기반 접근이 갈수록 트렌드가 되고 있고 대표적으로 object detection에서는 swin transformer같은 네트워크가,  semantic segmentation에서는 앞에서 간단히 언급했었던 ViT 같은 네트워크들이 좋은 성능을 내고 있다. 이들 모두 transformer 기반 backbone이다. ​아래 논문 원본과 HED, ViT 논문들의 링크를 가지고 왔다. 참고하면 좋을 것 같다. ​논문 출처: [2203.08566] EDTER: Edge Detection with Transformer (arxiv.org)​참고하면 좋은 것들: ICCV 2015 Open Access Repository (thecvf.com) (HED)​[2010.11929] An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale (arxiv.org) (ViT)​​ "
Anomaly Detection Survey (+ Machine Vision & Image Segmentation) ,https://blog.naver.com/a_tte/222564298619,20211110,"Anomaly Detection 연구분야 용어 정리 (awesome-anomaly-detection github repository 참고)Train Dataset에 (1) 비정상 sample 포함되어 있는가? 그리고 각 sample에 label이 존재하는가, (2) 비정상 sample과 정상 sample의 성격은 어떻게 다른가, (3) 정상 sample class는 단일 class인가 multi-class인가? 등에 따라 다른 용어를 사용한다.  (1) Train 시 비정상 sample 사용여부 및 label 존재 유무에 따른 분류Supervised Anomaly DetectionTrain dataset에 정상과 비정상 sample이 모두 있고, label이 모두 존재하는 경우특징- supervised learning 방식이기 때문에 supervised anomaly detection이라 부른다.- 정확도가 높음. 따라서 높은 정확도를 요구하는 경우 주로 사용되며, 비정상 sample을 다양하게 보유할 수록 더 높은 성능을 달성.문제- class-imabalance 문제를 자주 겪는다.- 따라서 Data augmentation, loss function 재설계, batch sampling 등 다양한 연구가 수행되고 있다.요약장점: 양/불 분류 정확도가 높음단점: 비정상 sample 취득에 시간과 비용이 많이 들며, class-imbalance 문제 해결이 필요하다.​cf. 제조업의 경우 수백만 장의 정상 sample이 취득되는 동안 단 1-2장의 비정상 sample이 취득되는 상황이 종종 발생한다. 만일 supervised learning 방식으로 학습하기 위해 각 class 당 최소 100장의 이미지가 필요하다고 가정해보자. 그렇다면 sample 1억 장을 모아야 100장 가량의 비정상 sample 확보가 가능하다. 이런 상황에서는 데이터셋 확보에 굉장히 오랜 시간이 걸린다.​Semi-supervised (One-class) Anomaly Detection비정상 sample 확보에 관한 문제 해결 위해 등장. class-imabalance 문제가 심한 경우 정상 sample만 이용해서 모델을 학습하기도 함.특징- 정상 sample들을 둘러싸는 discriminative boundary(결정경계)를 설정하고, 이 boundary를 최대한 좁혀서 boundary 밖에 있는 samples을 모두 비정상으로 간주- Classification을 사용하는 대표적 방법론- Deep Learning based one-class classification 방법론: Deep SVDD가 대표적 Deep SVDD Architecture요약장점: 비교적 활발히 연구되고 있으며, 정상 sample만 있어도 학습이 가능함단점: supervised anomaly detection 방법론과 비교했을 때 상대적으로 양/불 판정 정확도가 떨어짐​cf. 정상 Sample에 대한 label 확보 과정이 필요함.​Unsupervised Anomaly Detection대부분 데이터가 정상 sample이라는 가정 하에 label 없이 학습을 시키는 방법론특징- 주어진 데이터에 대해 PCA를 이용해 차원을 축소하고 복원하는 과정을 통해 비정상 sample 검출- NN 기반으로는 Autoencoder 기반 방법론이 주로 사용됨cf. 입력을 code 혹은 latent variable로 압축하는 encoding과 이를 다시 원본에 가깝게 복원해내는 decoding 과정으로 진행됨. 데이터의 중요한 정보들만 압축적으로 배울 수 있다는 점에서 PCA와 유사하게 동작 함.- Autoencoder를 이용해 Unsupervised Anomaly detection을 적용하여 defect를 segment 하는 대표적 논문들에서는, unsupervised dataset이 존재하지 않았음. 따라서 실험의 편의를 위해 학습에 정상 sample만 사용하는 semi-supervised learning 방식을 이용하였음- 하지만 Autoencoder를 이용한 방법론은 기본적으로 unsupervised learning 방법론임. Autoencoder 기반 Unsupervised Anomaly Detection문제- Autoencoder의 code size(= latent variable의 dimension) 같은 hyper-parameter에 따라 전반적인 복원 성능이 좌우됨- 이에 따라서 양/불 판정 정확도가 supervised anomaly detection에 비해 다소 불안정하다는 단점 존재- Autoencoder의 input과 output의 차이를 어떻게 정의할 것인지(= 어떤 방식으로 difference map을 계산할 지) 어느 loss fuction을 사용해 autoencoder를 학습시킬지 등 여러 요인에 따라 성능이 크게 좌우됨- 즉, 성능에 영향을 주는 요인이 많다는 약점이 존재.요약장점: labeling 과정이 불필요단점: 양/불 판정 정확도가 높지 않고 hyper-parameter에 매우 민감함  ​(2) 비정상 sample 정의에 따른 분류엄밀하게 분류하지는 않으나, 주로 Novelty Detection과 Outlier Detection으로 구분됨. 종종 두 방법론을 합쳐서 Anomaly Detection이라고 통칭하기도 함 Anomaly Detection 용어 정리Novelty Detection같은 class 내에서 이전에 없던 새로운 데이터가 등장하는 경우, 이를 novel sample, unseen sample이라 부른다. 이러한 sample을 찾아내는 방법론을 novelty detection이라 한다.​Outlier Detection마찬가지로 새로운 데이터가 등장했을 때, class에 규정되어 있지 않은 데이터가 등장하는 경우, 이를 outlier sample, abnormal sample이라 부르며 이를 찾아내는 방법론을 outlier detection이라 부른다.​요약Novelty detection은 지금까지 등장하지 않았지만 충분히 등장할 수 있는 sample을 찾아내는 연구임. 즉, 데이터가 오염되어 있지 않은 상황을 가정하는 연구와 관련됨.Outlier detection은 등장할 가능성이 거의 없는, 데이터에 오염이 발생했을 가능성을 상정해 sample을 찾아내는 연구임.  ​(3) 정상 sample의 class 개수에 따른 분류정상 sample이 단순히 '양품' one-class가 아닌, multi-class로 분류되는 경우이다.이때에는 정상 sample 대신 In-distribution sample이라는 용어를 사용한다.('강아지'가 아니라 골든 리트리버, 푸들, 말티즈, 도베르만 4가지로 분류된다고 가정해보자)​특징- In-distribution dataset으로 network를 학습시킨 뒤 test 단계에서 비정상 sample을 찾는다. 이를 Out-of-distribution detection이라 부름- cifar 10으로 classifier를 학습시키고, SVHN, LSUN 등 out-of-distribution 데이터셋으로 cifar 10 (in-distribution dataset)을 얼마나 잘 걸러내는지 살피는 방식이 그 예.- 만일 '불독'과 같이 novel smaple이 관찰되었을 때 이를 걸러낸 뒤 classifier가 기존의 class 대신 추가 class를 구분하도록 학습하는 Incremental learning 방법론과도 응용 가능  ​Anomaly Detection 최근 연구 동향- 제조업 이미지 데이터 외관 검사 (외관 상 발생하는 결함 탐지)- 시계열 데이터 기반 고장 예측 (장비 고장 등 비정상 sample 감지)​​ReferenceRetrieved from: https://hoya012.github.io/blog/anomaly-detection-overview-1/  Smart Factory & Machine Vision​제조 환경에서 Image based anomaly detection 연구의 의의- Smart Manufactirung / Smart Factory 구축 의의개념한국의 대표적 스마트팩토리는 포스코 광양제철소가 있음(반갑..). 센서와 카메라를 이용해 데이터를 수집하고, 이를 통해 불량이나 기계 문제를 파악함. 특정 시점에서 불량품이 다음 공정으로 넘어가지 않도록 판단하여 전체 공정을 제어.특징- 제품 기획부터 판매까지 전 과정이 ICT 기술과 융합되어 있음. - 최소 시간 최소 비용으로 생산 가능토록 함- 기존 공장의 '자동화'에서 '디지털화'로 진일보 (ICT를 활용하여 기존 제조업의 전 과정을 디지털화)- 혁신과 지속가능한 성장 가능성에 대한 관심이 고조됨에 따라 발전​Machine Vision- 제조 공정에서 카메라, 광학계, 이미지를 처리하고 분석하는 소프트웨어 등으로 구성된 시스템을 통해 사람이 눈으로 보고 판단하는 작업을 빠르고 정밀하게 대신해주는 솔루션- 컴퓨터가 마치 사람이 사물을 인지하고 판단하는 것처럼 도와주는 기술- 전통적 비전 검사에서는 결함의 크기, 유형, 위치 등 수많은 변화에 일일이 대처할 수 없었음. 이러한 검사 조건을 프로그래밍하여 반영.- 사전 불량검수 가능​Machine Vision에서의 딥러닝- 머신러닝은 사람이 데이터 분석에 참여- 딥러닝은 여러 층을 가진 네트워크를 사용해 학습을 수행하는 방식. 컴퓨터가 자동으로 대규모 데이터에서 중요한 패턴 및 규칙을 학습하고 이를 토대로 의사결정 및 예측 수행.- Object Detection, Segmentation, Classification이 주로 사용됨.​Generative Model 알고리즘을 사용하여 학습 성능을 높일 수도 있음- 불량이 적을 경우 자체 개발한 학습용 이미지 자동 augmentation 기능 및 generative model 사용.- 다양한 조건의 조명에서도 불량 유형을 검출할 수 있도록, 여러 조도에서의 같은 제품 이미지를 한 세트로 구성해 한 번에 학습하도록 할 수 있음. - 이미지 간의 상관관계를 분석하여 학습하고 불량을 검출​★ 대용량 이미지의 경우, 이미지를 작은 크기로 만들어 학습할 경우, 불량 부위에 대한 정보가 왜곡될 수 있음. 정확도 하락 유발. 따라서 전체 이미지를 일정 크기로 분할하거나 ROI만 분리하여 학습 및 검사하는 방법론 도입 가능.동일 산업군 내에서 유사한 제품을 검사할 경우, Transfer Learning 가능.​scratch, crack과 같은 defect/fault detection- 타이어, 철강, 금속, 식품, 의료 바이오, 2차전지(젤리롤 tab 뜯김 등), 반도체 웨이퍼​ReferenceRetreived from: https://www.epnc.co.kr/news/articleView.html?idxno=114928​  ​Image Segmentation- Image Segmentation은 의미론적 분할(Semantic Segmentation)과 인스턴스 분할(Instance Segmentation)으로 나눌 수 있음. - Semantic Segmentation은 이미지 내 각 pixel이 주어진 클래스 중 어떤 클래스에 속하는지 분류함(강아지. 3마리가 있어도 그냥 강아지). - Instance Segmentation은 주어진 클래스에 속하는 객체를 찾고, 해당 객체에 해당하는 픽셀을 표시함(강아지1,2,3).​대표적 모델(1) Fully Convolutional Network(FCN)- 2015년 CVPR에서 발표.- Semantic Segmentation 이용- 최종 feature map을 32배, 16배, 8배 크기로 키움 (upsampling)- 키운 feature map을 합성곱 연산하여 진행해 pixel 별로 분류함 - FCN-8s의 경우, input image와 가까운 feature map을 예측해 사용- 입력 이미지에 가까운 feature map을 사용해 예측에 활용할 수록 뛰어난 성능을 보임​(2) Mask-RCNN- 2017년 ICCV에서 발표됨- Instance Segmentation 수행- Step 1: 객체가 있을 만한 영역 탐지 (Bounding Box Regression)  Step 2: 탐지 영역 내 어떠한 범주가 있을지 예측(Classification)  Step 3: 탐지 영역 내 픽셀이 Step 2에서 예측한 객체인지 아닌지 segmentation (FCN)- 손실함수: 기존 Faster R-CNN의 loss function에 segmentation 관련 손실 함수 추가함. Multitask(classification, regression, segmentation) 손실함수임. - Bounding Box를 탐지한 후(Object Detection 이후) Segmentation을 진행했기 때문에 성능이 뛰어남- 레이블이 존재하지 않는 경우 알고리즘 사용 불가능.​ "
"[ML 다이어리] yolov4-tiny를 활용한 사람 감지 모듈 학습 | person detection, yolov4-tiny-tflite with custom dataset  ",https://blog.naver.com/cheeryun/222291537097,20210329,"연구실 과제 일환으로 person detection module을 학습했다. 이전에는 사전 학습된 object detection  모듈을 그냥 가져와서 person 클래스일 때만 활용하게끔 레이블 옵션만 수정해서 사용했는데, 사람이 카메라와 근접 거리에 있을 때는 성능이 좋지 못했다. ​모델 재학습의 필요성을 느껴서 자료를 찾아 다녔고 다음과 같은 방식으로 문제를 해결했다:1. AVA dataset을 통해 사람(person) 카테고리만 있는 custom dataset 구축 2. 경량화 모델인 yolov4-tiny 로 학습 3. Tensorflow lite 모델로 변환 ​​1. AVA dataset을 통해 사람(person) 카테고리만 있는 custom dataset 구축 AVA dataset은 여러 과업(task)을 위해 활용되지만 나의 경우에는 bbox 레이블이 주어진 사람 이미지만을 얻는데 활용했다. 자세한 사용 방법은 [ref1]을 참고하자.​해당 DB는 YouTube 영상에서 추출한 사람 이미지들로 구성되는데, 드라마 영상의 경우 클로즈업된 장면이 많기 때문에 사람이 카메라와 근접해 있을 때의 상황에 대해서도 감지 성능을 높힐 수 있다. ​2. 사용 방법 (2) 경량화 모델인 yolov4-tiny 로 학습 (3) Tensorflow lite 모델로 변환 내용은 [ref2]에 수록되어 있다. 해당 레포지토리는 바로 사용할 수 있는 사전 학습 모델만 포함한다. 개인적으로 다시 학습하는 방법 또한 소개했다. ​ ​​  Reference [1] AVA-Dataset-Processing-for-Person-Detection, github / dataset 구축하는 방법 [2] yolov4-tiny-tflite-for-person-detection, github / 사전 학습된 모델  "
[Coursera] Detection Algorithm (2) ,https://blog.naver.com/thdakfwn/221999209043,20200613,Output accurate bounding boxes of any aspect ratio - YOLO Algorithm​  ​takes the midpoint of the image and assigns the object to the grid cell containing the midpoint pretending the central grid cell has no objects​use back-propogagtion to train NN to map from any input x to output volume yusing much finer grid reduces chance that they are multiple objects assigned to the same grid cell​  ​there is multiple way of specifying the bounding boxes​Intersection Over Union (IoU)​use both for evalutating object detection algorithm and to add another component to the algorithm to work better​  ​* 0.5 is just a human chosen convention​Non-max suppression​make sure algorithm detect each object only once  multiple box can think that they found the car​  ​if you try to detect 3 multiple objects than the output vector will have 3 multiple componentsindependently carry out non-max suppression on each of the output classes​Anchor Boxes​want grid cell detect multiple object​predefine two different shapes called anchor boxes​  ​  ​​what if you have two anchor boxes but three object in the same grid cell?or what if you have two object in same grid cell but both of them has same anchor box shape?that's one case this algorithm doesn't handle well​YOLO Algorithm​  ​  ​Region Proposal : R-CNN​tries to pick few regions that make sense to run ConvNet classifier - run segmentation algorithm​  R-CNN: Propose regions (clustering). Classify proposed regions one at a time.Output label + bounding box​Fast R-CNN: Propose regions (clustering). Use convolution implementation of sliding windowsto classify all the proposed regions​Faster R-CNN: Use convolutional network to propose regions 
pytorch mask r-cnn detection prolem ,https://blog.naver.com/dschae9/221868631686,20200323,"PyTorch and TensorFlow object detection 실행할 때, object of type <class 'numpy.float64'> cannot be safely interpreted as an integer 오류가 발생한다. 이 오류의 원인은 numpy 1.18.1과 pycocotools가 안 맞아서 생기는 오류이다. 해결책은 numpy 버전을 1.17.4로 재설치 한다.​conda install numpy=1.17.4​Traceback (most recent call last):File ""C:\Users\Administrator\.conda\envs\tf\lib\site-packages\numpy\core\function_base.py"", line 117, in linspacenum = operator.index(num)TypeError: 'numpy.float64' object cannot be interpreted as an integer​https://github.com/cocodataset/cocoapi/pull/354 Fix the use of linspace for numpy 1.18 by ppwwyyxx · Pull Request #354 · cocodataset/cocoapiThe original code cannot run under latest numpy 1.18:╰─$ np.linspace(.5, 0.95, np.round((0.95 - .5) / .05) + 1, endpoint=True)                                          ---------------------------...github.com ​(base) C:\Users\Administrator>conda activate tf​(tf) C:\Users\Administrator>pythonPython 3.7.6 (default, Jan 8 2020, 20:23:39) [MSC v.1916 64 bit (AMD64)] :: Anaconda, Inc. on win32Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.>>> import numpy>>> numpy.__version__'1.18.1'>>> exitUse exit() or Ctrl-Z plus Return to exit>>> exit()​(tf) C:\Users\Administrator>conda install numpy=1.17.4Collecting package metadata (current_repodata.json): doneSolving environment: failed with initial frozen solve. Retrying with flexible solve.Collecting package metadata (repodata.json): doneSolving environment: done​​==> WARNING: A newer version of conda exists. <==current version: 4.7.12latest version: 4.8.3​Please update conda by running​$ conda update -n base -c defaults conda​​​## Package Plan ##​environment location: C:\Users\Administrator\.conda\envs\tf​added / updated specs:- numpy=1.17.4​​The following packages will be DOWNGRADED:​numpy 1.18.1-py37h93ca92e_0 --> 1.17.4-py37h4320e6b_0numpy-base 1.18.1-py37hc3f5095_1 --> 1.17.4-py37hc3f5095_0​​Proceed ([y]/n)? y​Preparing transaction: doneVerifying transaction: doneExecuting transaction: done​(tf) C:\Users\Administrator> "
진보된 detection technique- YOLOX ,https://blog.naver.com/cobslab/222820945508,20220720,"오늘 소개해 드릴 논문은 ‘YOLOX’입니다.콥스랩(COBS LAB)에서는 주요 논문 및 최신 논문을 지속적으로 소개해드리고 있습니다.해당 내용은 유튜브 ‘딥러닝 논문읽기 모임' 중 ‘YOLOX’ 영상 스크립트를 편집한 내용으로, 영상으로도 확인하실 수 있습니다. (영상 링크: https://youtu.be/N2rLSzEqqI8) ​YOLOX 논문은 Object Detection에서 자주 사용되는 YOLO 모델에 Anchor free 메서드를 적용해서 성능을 향상하는 방법을 소개한 논문입니다. 본격적인 모델 소개 앞서 Object Detection이라는 건 무엇인지, 그리고 YOLO 시리즈의 관해서 간략하게만 되짚어보겠습니다. Object Detection은 이미지에 있는 객체를 찾고 그 해당 영역을 bounding box를 하는 테스크를 의미를 합니다. 최근에 트랜스포머 계열의 모델들이 Object Detection 태스크에 들어오기 전에 대부분의 Object Detection 모델들은 CNN 기반이었습니다.  YOLO 역시 대표적인 CNN 기반의 네트워크입니다. 위 그림에서 오른쪽 숫자들은 YOLO 시리즈들의 퍼블리시 연도 그리고 citation 수를 나타내고 있습니다. 해당 논문의 citation수가 굉장히 높은 것을 확인하실 수 있습니다. detection관련 논문에서 가장 유명한 논문이라고 해도 과언이 아닌 것 같습니다. YOLO의 작동방식에 대해 설명드리겠습니다. YOLO는 시리즈별로 작동방식이 조금은 상이하지만 V1을 제외한 모든 버전에서는 공통적으로 사용되는 Anchor라는 특징이 있습니다. Anchor는 object detection을 할 때 다양한 비율과 다양한 크기의 오브젝트를 잘 잡아내기 위해서 도입이 되었습니다. Anchor를 간단히 말하면 bounding box를 만들어내기 위해 기반이 되는 설정값입니다. Anchor를 사용하는 방식은 위 그림들이 나타내고 있습니다. 첫 번째 단계로는 일반적인 네트워크와 같이 backbone을 통해서 feature extraction을 진행합니다. 위 그림에서는 13 * 13인데 바로 맨 마지막 backbone 아웃풋의 feature map의 shape이라고 생각을 해 주시면 될 것 같습니다. 피쳐 맵 각각의 한 칸 들을 YOLO에서는 Cell이라는 명칭으로 부릅니다. 따라서 본 예제에서 개수는 Cell이 169개라고 볼 수 있습니다. 그리고 각각의 Cell에는 depth를 포함하고 있습니다. 만약에 아웃풋 디멘션이 13 * 13 * 10이라면 Cell 하나의 shape은 1 * 1 * 10입니다. 각 Cell에서 Anchor를 만들어 내고 그 Anchor를 기반으로 다시 bounding box를 만들어내는 과정을 거치게 됩니다. Cell 각각에서 Anchor를 만들어 내고 bounding box를 다 만들어 내다 보니까 전부 다 visualization 하면, 위 그림과 같이 엄청나게 많은 bounding box가 생깁니다. 이 bounding box 후보군들 중에서 ground truth(GT)와 겹치는 accuracy가 높은 bounding box만 남기고, 그 bounding box 중에서도 또 좋은 것만 골라서, 결국에는 이렇게 원하는 레이블을 찾는 오브젝트의 영역을 디텍션 하는 방식이 됩니다. 결국 Anchor라는 것이 bounding box를 만드는 기반이 되고, 이것으로 bounding box를 만들어서 GT에 근접한 것들을 남기는 과정이라고 생각하시면 될 것 같습니다.  Anchor에 대해서 조금 더 자세히 알아보도록 하겠습니다. Anchor는 bounding box를 만드는 기반이 되는 값들입니다. 그래서 Anchor는 다양한 비율과 다양한 크기의 오브젝트를 좀 더 잘 찾을 수 있도록 하는 목적으로 만들어졌기 때문에, 이런 Anchor 또한 다양한 사이즈와 다양한 Aspect ratio를 가지고 있습니다. 위 그림에서 보시면, 하나의 사이즈가 세 개의 Aspect ratio를 가져서 총 아홉 개 Anchor를 가지고 있습니다. 일반적으로 9개 Anchor를 많이 사용하기에, 여기서도 예시로써 9개 Anchor를 들었습니다. backbone 후에 feature map이 위와 같다고 하면, 9개 Cell만을 보면 9개 Cell 중, 한 Cell에서는 9개 Anchor를 기반으로 bounding box를 만들어내게 됩니다. 따라서, 총 Anchor의 개수는 13 * 13이니까 169 * 9개의 Anchor가 만들어지게 됩니다. bounding box 또한 똑같은 개수로 만들어진다고 생각을 하시면 될 것 같습니다. 그림과 같이 만들어진 bounding box는 초록색 부분이 GT 부분이고 파란색 부분이 bounding box부분입니다. 그리고 GT와 이 bounding box와 얼마나 겹치는지 측정을 합니다. 얼마나 많이 겹치는지 를 나타내는 지표를 IoU 라고 합니다. IoU는 두 박스가 있을 때, 두 박스 전체영역 / 겹치는 영역으로 하나의 밸류를 만들어내서 밸류가 일정 이상이면 이것이 GT와 어느정도 근접한 bounding box를 예측했다 라고 생각을 해서 Positive 샘플로 분류를 하게 되고, Positive 샘플들을 모아서 학습을 진행을 하게 됩니다. 여기까지 진행하면 마지막에 후보군 bounding box들 중에서 조금 더 accuracy가 높은 것들만 추려내면 깔끔하게 bounding box가 남는 과정이 진행됩니다. 이런 방식으로 Anchor를 통해서 bounding box를 만들어 내고, 학습을 진행하는 만큼 Anchor base 메서드는 recall은 조금 높지만 계산량이 많고 Negative 샘플이 많기 때문에, class imbalance problem이 가중되는 단점이 있습니다. YOLOX에서는 Anchor를 사용하지 않는 Anchor free 방법을 사용합니다. 간략하게 말해서, Anchor free 메서드는 feature extraction 과정까지는 같지만, Anchor free라는 이름과 같이, 각 Cell마다 Anchor를 사용하지 않고, 바로 bounding box나 class를 classification 하는 과정을 거칩니다. 위 그림에서 그라운드 박스가 파란색 Cell이라고 하면, GT bounding box에 속하는 Cell들은 Positive 샘플로 할당하고, 속하지 않는 것은 Negative 샘플로 할당을 하게 됩니다. 이 논문에서는 이와 같은 Anchor free 방법에 추가적인 여러 방법들을 사용을 해서 성능 향상을 이끌어 냈습니다.  다음으로 YOLOX의 퍼포먼스 부분입니다. 위 그림은 약간 덩치가 큰 모델들에 Anchor free 방법과 다양한 방법들을 사용을 해서 latency 대비 average precision을 제시한 그래프입니다. 초록색으로 표시된 것들이 원래 모델이고, 빨간색이 Anchor free 방법을 적용한 것입니다. 원래 모델보다는 Anchor free를 적용한 것이 average precision이 높게 나타났습니다.  오른쪽 그림은 조금 작은 모델들에 Anchor free 방법을 적용한 그래프이고, 파라미터 대비 average precision을 나타내고 있는데, YOLOV4의 Tiny나 NanoDet 등에 적용해서, 원래 모델의 성능보다는 더 높은 성능을 보여주고 있습니다. YOLOX의 네트워크 부분을 살펴보겠습니다. 전체적인 네트워크 구조를 나타내 보았습니다. YOLOX는 backbone 네트워크로는 YOLOV3의 spp를 탑재한 backbone 네트워크를 사용을 하였습니다. 그 이후에는 PAFPN이라는 피라미드 네트워크로 연결하고 그다음에 prediction을 진행하는 구조로 되어 있습니다. backbone 네트워크부터 다시 살펴보자면, 앞에서 언급했던 그래프에서는 YOLO V5를 개량한 모델의 그래프를 제시를 하였는데, 여기서는 YOLOV3을 사용을 하고 있습니다. 논문에서는 그 이유를 YOLOV4나 V5는 Anchor base 메서드에 과적합 돼 있어서 Anchor free 메서드를 적용하기 위해서는 V3가 더 적합했기 때문이라고 밝히고 있습니다.그다음으로는 피쳐 피라미드 네트워크 줄여서 PAFPN이라고 하는 네트워크입니다. FPN는 backbone에서 여러 가지 스케일에 feature map을 뽑아내고 여러 스케일의 feature map에서 prediction을 진행을 해서 멀티 스케일을 고려하는 네트워크입니다. 멀티스케일 feature를 뽑아내고 탑다운까지 진행을 하면서 바로 헤드로 넘어가는데 PAFPN에서는 하나가 더 붙었습니다. 그래서 바텀업 라인도 생기고 최종적으로 위 그림과 같이 세 개에서 각각 프레딕션 헤드가 있어서 프레딕션을 진행하도록 되어 있습니다.Anchor free의 단점으로 멀티 스케일을 고려하지 못해서 작은 오브젝트 등을 잘 찾기가 힘든 부분이 있는데, 저자는 이런 약점들을 FPN를 통해서 멀티스케일을 고려해서 극복하고자 했습니다.마지막으로 그림의 우측 부분은 프레딕션을 나타내고 있습니다. FPN의 3개의 아웃풋에서 각각 진행이 되고, 프레딕션은 classification, regression, 그리고 Objectness에 대해서 진행을 합니다. 참고로, Objectness 스코어는 프레딕션이 각각의 Cell에서 이루어지기 때문에, 그 Cell이 백그라운드를 나타내는지 아니면 오브젝트를 어떤 오브젝트 라도 포함하고 있는지, 오브젝트가 있는지 없는지를 0에서 1 사이의 밸류로 나타내는 스코어를 뜻합니다.저자는 전체적으로 이러한 네트워크 위에 크게 4가지 방법을 가지고 성능 향상을 이끌어냅니다. 이렇게 헤드가 여러 개인 것을 뜻하는 Decoupled head와 데이터 증강인 Data augmentation 그리고 Anchor free Multi positive라는 방법을 적용했습니다. YOLOX 포스팅에서는 이렇게 4가지 항목을 중심으로 설명을 드리겠습니다. YOLOX에 적용한 기법 중 첫 번째로 Decoupled head가 있습니다. Decoupled head라는 것은 이름처럼 하나의 헤드가 아닌 여러 개 헤드로 나뉜 것을 의미합니다. YOLOV3부터 V5까지는 하나의 헤드만 사용을 했습니다. 즉, regression 파트, classification 파트, 그리고 Objectness파트까지 전부 합쳐서 하나의 벡터로 만들어서 학습을 하고 prediction을 하는 방법을 사용을 했습니다.  위 그래프를 보시면, Decoupled head와 YOLO head의 epoch당 AP값으로 표현된 학습 속도를 보실 수 있습니다. Decoupled head가 아무래도 조금 더 converging 속도가 빠른 것을 확인하실 수 있습니다. YOLOX에 적용한 기법 두 번째는 end-to-end로 가기 위해서는 Decoupled head가 필수적이라는 측면입니다. 위 그림의 아래 테이블은 기존의 YOLO와 Decoupled head를 사용했을 때와 기존의 YOLO 방식과 end-to-end YOLO 방식을 비교한 테이블입니다. 결과적으로는 end-to-end의 Decoupled head가 조금 더 AP값이 잘 나오는 것을 확인할 수 있습니다.  다음으로는 augmentation 방법입니다. 이 논문에서는 총 4가지의 Data augmentation 방법을 적용을 했습니다. random horizon flip과 원본 영상의 hsl을 변경하여 증강시키는 클러스터라는 방법, 원본 이미지 외에 3개의 추가적인 사진을 섞는 Mosaic이라는 방법, 마지막으로는 그 이미지랑 레이블의 다른 것을 조금씩 섞는 Mixup이라는 방법을 사용했습니다. 오른쪽 그림에서 비행기라는 레이블에 나비라는 레이블을 조금씩 섞어서 임시로 만들고, 레이블도 만든 것을 확인하실 수 있습니다. 놀라운 점 중 한 가지는, 이 논문의 저자는 강력한 Data augmentation을 써서, pre-training 된 weight를 써도 성능 향상이 별로 안 되었다고 합니다. 그래서 논문에서는 스크래치부터 학습을 했다고 언급하고 있습니다.  다음으로 Anchor free 기법입니다. 중간 이미지가 그 feature map을 거친 backbone 네트워크를 거친 최종 feature map을 나타냅니다. 좌측이 Anchor free 그리고 우측이 Anchor base 방법을 나타내고 있습니다. Anchor base 방법부터 말씀을 드리자면, 위 그림 좌측의 검은 점이 있는 Cell에서 프레딕션을 진행한다고 가정을 했을 때, Anchor base 방법은 regression을 진행을 하고 regression에서 나오는 아웃풋 값을 바탕으로 Anchor의 중심점과 가로와 세로의 길이를 연산을 해서, 결과적으로는 bounding box에 가로 세로 길이와 중심점을 같은 것을 학습합니다. 여기서 빨간색 박스와 파란색 박스는 각각 Anchor가 만들어낸 bounding box를 의미합니다. 여기서는 아무래도 파란색 박스가 초록색 GT박스와 IoU가 높기 때문에, 이 파란색 bounding box가 Positive 샘플로 분류가 되고 빨간색 박스는 학습에 참여하지 않게 됩니다. 그리고 Anchor free 방법에서는 그라운드 트루스 bounding box 안에 있는 Cell들이 전부 Positive 샘플이 됩니다. 그래서 그중에서도 이렇게 검은 점에서 프레딕션을 진행한다고 했을 때, Anchor free 방법에서는 해당 Cell에서부터 GT의 각 모서리까지 길이를 학습을 하게 됩니다. 그래서 이를 바탕으로 bounding box를 만들어 내고 디텍션 테스크를 수행을 합니다.  다음으로, 위 표는 Anchor free 방법과 Anchor base 방법을 사용했을 때, 결과적으로 장단점을 나타낸 표입니다. 첫 번째로는 Hand-crafted parameter입니다. Anchor는 데이터셋에 따라서 그 사용자가 직접 디자인을 해야 합니다. 데이터셋에서 가로로 긴 것이 많으면, 아무래도 Aspect ratio가 가로가 좀 더 커지는 단점이 있고, 반면에 Anchor free 같은 경우에는 불편함은 없습니다. 두 번째로 Computational cost 같은 경우에는 Anchor는 하나의 Cell에서 9개의 Anchor를 만들어 내고 anchor free는 하나의 프레딕션만 진행을 한다고 했을 때, 단순 계산만으로 9배의 차이가 나는 계산량을 생각해볼 수 있습니다. 그리고 세 번째로는 Generalization인데, Anchor는 데이터셋에 따라서 각자가 디자인하고, 그래서 데이터셋에 조금 디펜던시가 있어서 Generalization 측면에서는 Anchor free가 더 높은 측면이 있습니다. Class imbalanced problem 같은 경우에는 Anchor base는 Cell마다 9개 Anchor를 만들어 내고, 거기에서 나오는 Negative 샘플도 많기 때문에 아무래도 Anchor free 방법이 이런 문제에서는 좀 더 자유롭습니다. accuracy 같은 경우에는 사실이 모델마다 적용된 기법들이 다 달라서 anchor base랑 Anchor free와 따로 정확히 비교를 하긴 힘들겠지만 본 논문에서는 기존의 YOLO V3보다는 Anchor free가 좀 더 우세하다고 볼 수 있습니다. 마지막으로 recall에서 Anchor base는 각 Cell에서 9개를 예측을 하고, Anchor free는 1개만 예측을 하기 때문에, 좀 더 Anchor base가 recall이 높은 것을 확인을 하실 수 있습니다.  적용한 기법들을 소개하는 것 중에, 마지막으로는 Multi positive 내용이 있습니다. 사실 기본적인 Anchor free의 방법만으로는 Anchor base방식의 accuracy를 따라잡을 수가 없습니다. 그래서 본 논문에서는 accuracy를 향상하기 위해서 다른 방법을 적용을 했습니다. 첫 번째 방법은 CenterYOLOness입니다. 이 방법은 좌측에 사진과 같이 예측하고자 하는 Cell이 있다고 하면, 이 Cell이 오브젝트 레이블이 불분명 한 문제 때문에 디텍션 ratio이 떨어졌다고 해보겠습니다. 각 Cell 중에 원래 Positive가 있는데, 오브젝트에 중심에 해당하는 Cell들만 Positive로 할당하는 CenterYOLOness라는 방법을 사용을 해서 정확도를 더 향상했습니다.YOLOX의 저자는 가운데에 있는 Cell 말고 옆에 주위에 있는 Cell들도 더 충분히 좋은 prediction을 할 수 있다고 보았고, 그런 관점에서 Multi positive라는 것을 제안을 해서 센터 말고 주위에 있는 Cell들도 다 사용을 해서 prediction을 하자라고 제안을 했습니다.결론적으로는 저자는 오브젝트 중심에 있으면서도 Loss가 낮은 k개의 샘플을 Positive 샘플로 적용을 해서 프레딕션을 진행하였고 디텍션 퍼포먼스의 향상을 이끌어냈습니다. 마지막으로는 Loss에 관련된 사항들을 정리를 해 보았습니다. Objectness와 classification과 regression에 대해서 프레딕션을 진행을 한다고 이 글 초반에 말씀을 드렸습니다. Objectness와 classification에 대해서는 BCE Loss를 사용하였습니다. regression에 관해서는 IoU Loss를 사용합니다. 여기서는 초록색이 GT 레이블이고 그리고 민트색이 프레딕션 한 bounding box입니다. IoU를 그대로 Loss에 사용을 하게 되면 좌측과 우측을 그 예측한 정도가 다르게 됩니다. 왼쪽 그림에서 IoU는 둘 다 0입니다. 왜냐하면 겹치는 값이 아무것도 없기 때문입니다. 이런 면들이 IoU를 그대로 Loss에 사용하기에는 좀 부적합하다고 생각을 해서 GIoU가 나왔습니다.오른쪽 그림에서 그라운드 트루스 박스와 프레딕션 한 bounding box를 모두 포괄하는 C라는 박스를 만들어냅니다. C박스는 이 두 박스가 근접하면 근접할수록 더 작아지게 됩니다. 오른쪽 그림에서 C박스 영역에서 A와 B의 합집합을 뺀 영역을 확인하실 수 있습니다. 그래서 좌측과 우측의 IoU는 모두 0이지만 GIoU는 0이고 오른쪽 GIoU는 0.7이 됩니다. 다음으로, Experiment 중 ablation study를 살펴보겠습니다. 위 표를 보시면 YOLO V3 base모델에 비해서, 앞에서 소개한 방법들을 적용하는 것이, 성능 향상에 도움이 되었다는 것이 결론입니다. 추가적으로 Anchor free 방법을 사용했을 때는 strong augmentation까지는 Anchor free방법이 적용이 되어있지 않은 것으로 보이는데, Anchor free를 적용하고 조금 더 속도가 빨라져서 계산량 부분에서 이득이 있었다는 것을 확인할 수 있었습니다. 마지막에 NMS free라는 것은 최종적으로 나온 bounding box 중에 accuracy가 높은 bounding box만 추려내는 post processing 파트입니다. 표를 보시면, Anchor free 네트워크라 해도, post processing을 적용을 하였을 때가 조금 더 AP가 향상되고, 없앤다면 낮아지는 결과를 확인을 하실 수가 있습니다. 다른 backbone을 적용했을 때의 결과입니다. 이것들은 각각 그 YOLO V5를 base 라인으로 해서, YOLOX에서 제안한 방법들을 적용하였고, 각각 AP이 조금씩 올라가는 것을 확인하실 수 있습니다. 그에 따라서 파라미터도 조금씩 상승을 하고 latency도 조금씩 상승을 하는 trade off가 있지만, AP값은 조금 더 향상된 결과를 보실 수가 있습니다. ​ 작은 모델들에 대해서 살펴보겠습니다. 각각 AP는 올랐고 파라미터수는 좀 늘었지만 AP값은 조금 더 향상된 결과를 확인할 수 있습니다.  Data augmentation 방식인데, 저자들은 그 모델 사이즈에 따라서 Data augmentation 차등적으로 적용을 해야 된다고 생각을 했습니다. 그래서 실험을 해봤을 때도 작은 모델 같은 경우에는 Mixup 같은 것을 사용했을 때 AP가 더 떨어졌고, 큰 모델 같은 경우는 Mixup을 사용해서 많이 올랐다는 것을 알 수 있습니다. 최종적으로, YOLO V5에 이 논문에서 제시한 방법을 적용한 것이 AP면에서는 가장 좋다는 것을 알 수 있습니다. ​마지막으로 정리해보면, 이 논문의 contribution은 Anchor free 방법을 YOLO 시리즈에 적용했다는 점, 그리고 label assigning strategy를 썼다는 점을 들 수 있습니다. 또한, 진보된 디텍션 테크닉과 모델 사이즈에 관계없이 효과적인 방법들을 적용했다는 것이고, 적용한 방법들이 오리지널 모델의 accuracy를 더 향상시켰다는 점 입니다. 지금까지 YOLOX에 대한 포스팅이었습니다. 감사합니다. ​​​ "
Object Tracking이란 ,https://blog.naver.com/byunsohyun/222698599959,20220412,"Object Tracking을 공부하다보면 MOT라는 단어를 흔하게 볼 수 있다.MOT란 Multi-Object Tracking이다.3DMOT란 3D Multi-object Tracking이다.​전체적인 절차는 다음과 같다.(1) Object Detection(2) Tracking (Association)​1. Object Detection: object detection은 유명한 네트워크를 사용해보면 된다.Yolo, Faster R-CNN, SSD 등 다양한 네트워크들이 많으니 그냥 사용하면 된다.근데, Point Cloud 데이터를 이용하는 경우 PointNet 과 같은 유명한 다른 네트워크들이 있다.최종적으로 검출(detect)하고자 하는 객체(object)의 좌표(x, y)값이 bounding box 형태로 출력된다.​2. Tracking (Association)현재 frame에 대해서 detection 한 뒤,현 frame에 대한 정보 - 이전 frame에 대한 정보 간 similar detection과의 matching process 수행한다.​여기까지가 기본적인 개념이다.그럼 좀 더 구체적인 개념에 대해서 파악해보자.​frame별로 객체를 검출만 하는 것이 아니라 매 frame마다 검출되는 객체를 추적하기 위해서는 다양한 알고리즘이 제안되고 있는 것 같다.여러 방법론 중에서도 ""SORT""에 대해서 정리한다.​*SORT란?​SORT = simple online and real-time tracking여기서 online인지, offline 에 대한 차이점도 알고 가는것이 중요하다.- On-line은 현재 frame, 이전 frame 간의 유사도만 분석- Off-line은 전체 frame의 유사도 분석다시 본론으로 돌아가면,SORT란 detector + kalman filter + Hungarian Method 세개의 연속된 일련의 과정이라고 보면 된다.(+) 빠름, 간단, 높은 정확도(ㅡ) occlusion에는 좋지 않은 성능​*DeepSort란?​DeepSORT = SORT + feature based on DL(Deep learning)kalman filter를 기본으로 딥러닝 피처(Re-ID)를 추가로 반영해 Hungarian Method 수행SORT에서 물체가 겹쳐 서로 갑자기 반대로 가면 Tracking ID 반대로 바뀌거나 새로운 ID 부여 (ID Switching 문제) 발생한다.DeepSORT는 딥러닝 feature 통해 이런 경우도 상당 부분 보정을 진행Kalman gain이 높다고 하더라도, image feature가 서로 유사하면 아이디 유지해주고 새로운 아이디 부여를 하지 않게한다.(진짜 굳 아이디어인 것 같다.)​​ "
Opencv Haar cascade : object detector ,https://blog.naver.com/codingteacher/222978936778,20230109,"https://medium.com/@vipulgote4/guide-to-make-custom-haar-cascade-xml-file-for-object-detection-with-opencv-6932e22c3f0e Guide to make Custom Haar Cascade XML file for Object detection with OpenCVIn this blog, we are training any custom Haar Cascade XML file for object detection on an image /real-timemedium.com https://kr.mathworks.com/help/vision/ug/train-a-cascade-object-detector.html Get Started with Cascade Object Detector - MATLAB & Simulink - MathWorks 한국Documentation 더 보기 Videos Answers 평가판 제품 업데이트 Get Started with Cascade Object Detector Why Train a Detector? The vision.CascadeObjectDetector System object comes with several pretrained classifiers for detecting frontal faces, profile faces, noses, eyes, and the upper body. However, these classifier...kr.mathworks.com https://github.com/EshginGuluzade/stop_sign_detection GitHub - EshginGuluzade/stop_sign_detectionContribute to EshginGuluzade/stop_sign_detection development by creating an account on GitHub.github.com https://github.com/D-KG5/opencv GitHub - D-KG5/opencv: Code utilizing the opencv libraryCode utilizing the opencv library. Contribute to D-KG5/opencv development by creating an account on GitHub.github.com https://github.com/cfizette/road-sign-cascades GitHub - cfizette/road-sign-cascades: Collection of HAAR and LBP cascades designed to recognize various street signsCollection of HAAR and LBP cascades designed to recognize various street signs - GitHub - cfizette/road-sign-cascades: Collection of HAAR and LBP cascades designed to recognize various street signsgithub.com https://colab.research.google.com/drive/1Ln-Vty7OaQaaxMREEaGYbDnRekclJKts?usp=sharing pedestrianDetection.ipynbColaboratory notebookcolab.research.google.com Opencv 공식 haar xml들https://github.com/opencv/opencv/tree/4.x/data/haarcascades opencv/data/haarcascades at 4.x · opencv/opencvOpen Source Computer Vision Library. Contribute to opencv/opencv development by creating an account on GitHub.github.com ​ "
"SOD(Salient Object Detection) 중요한 사물찾기(배경 분리) U^2 Net, BASNet ",https://blog.naver.com/chaechunjae/222004675868,20200618,"NathanUA/U-2-NetThe code for our newly accepted paper in Pattern Recognition 2020: ""U^2-Net: Going Deeper with Nested U-Structure for Salient Object Detection."" - NathanUA/U-2-Netgithub.com 오늘도 열일하는 BMW i4   보면 알겠지만 graph axis 까지 한번에 날려버림. 그냥 잘 된다.뭐 이렇게 강력하냐. "
[ object detection | 객체인식 ] coco annotation format ,https://blog.naver.com/tomatian/222058418900,20200812,https://www.immersivelimit.com/tutorials/create-coco-annotations-from-scratch Create COCO Annotations From Scratch — Immersive LimitThis tutorial will teach you how to create a simple COCO-like dataset from scratch. It gives example code and example JSON annotations.www.immersivelimit.com ​​  이상 동산이었습니다.정상에서 만나요! 
CS231n Lecture 11 강의노트 -Detection and Segmentation (핵심 정리) ,https://blog.naver.com/wpxkxmfpdls/221878233486,20200328,"cs231n의 강의 내용은 그렇게 어렵지 않습니다(보는게 귀찮을 뿐).그래서 되도록이면 직접 강의를 시청하시는 것을 추천드립니다만,시간이 없으신 분들을 위해 핵심만 요약합니다.https://youtu.be/nDPWywWRIRo  ​Image segmentation은 요즘 되게 핫한 분야입니다. 특히 medical imagine에서 많이 쓰이고 있죠.​  Image Classification 말고도 CNN을 이용한 task를 정리하면 4가지 정도가 있습니다. 또한 그 hybrid로 여러 모델을 결합해 다양한 problem을 만들 수 있습니다. 제가 관심있는 분야는 Sematic Segmentation이므로 이쪽과 관련되서 자세히 알아보고 나머지도 간단히 알아보겠습니다.​  Image segmentation은 image 단위로 labeling을 하는 것이 아니라, 모든 pixel에 대해 independently labeling하여  pixel 단위로 classification하는 것을 의미합니다. 근데, 문제는 붙은 2개의 cow를 한개로 친다는 것입니다. 이 문제를 해결한 것이 마지막 문제인 instance segmentation입니다.​  물론 crop을 만들고 옆으로 계속 sliding하면서 center pixel의 label을 예측할 수도 있겠지만, 굉장히 비쌉니다. ICML에서 컨셉을 제시했다고 말할 수 있겠죠.​  그래서 CONV를 이용해서 high resolution image를 계속 processing하여 segmentation하는 방법이 나왔으나, 이렇게 high resolution image를 sequence로 계속 처리하는 것 또한 비쌉니다. 그 이유는 CS231n의 CNN 강의에서 알아보시죠! param도 많아지고 등등.. 여러 문제가 있습니다.​  그래서 나온것이 Downsampling하고 Upsampling하여 high resolution image로 복구하는 것입니다. FC를 통해 transitioning을 하는 것과는 달리 spatial resolution을 계속 증가시키는 방식입니다. 그래서 우리의 prediction이 ground truth에 근접하도록 학습하는 방법이죠. 또한 input image size와 output image size가 같다고 합니다. downsampling하여 lower spatial resolution에서 processing이 일어나므로 computationally effiient하다고 말할 수 있습니다. 근데 U-NET같은 paper를 보면 pooling하는 순간 위치 정보에서 loss가 발생하는 바람에 pooling을 일체 안 하는 것으로 알고 있습니다.​  upsampling하는 concept method로는 MaxUnpooling이 있습니다. 같은 position에 max value가 오도록 constraint를 걸어 image의 size를 키워줍니다. 근데 사실 말이 안 됩니다. 거의 모든 정보가 사라지기 때문이죠.​  그래서 나온 것이 Transpose Convolution입니다. 저희가 Convolution연산을 할때 stride를 넣어주면 down smapling되는 현상이 발생합니다.​  반대로 1개의 pixel에다 vector(kernel)를 곱해 여러 개의 값을 뽑아내고 stride를 주고 움직여서 image size를 키우는 방식입니다. overlap되는 부분에서는 평균을 취하지 않고 더해주기만 합니다. Deconvolution 등등 다양한 이름으로 불리고 있지만 교수님은 Transpose Convolution이라고 부르는 것을 좋아한다고 합니다.​  시각화가 굉장히 잘 되어 있는데, 이렇게 input에서 output으로 mapping되며 size가 증가하게 됩니다.​  matrix operation에서 해석을 해보자면 x가 우리가 흔히 말하는 filter혹은 kernel입니다. 우리가 stride 1을 줄때 transpose convolution과 일반 convolution은 비슷한 현상이 발생합니다(padding rule만 달라지는 것입니다.). 근데, stride를 키워주면 좀 다릅니다.​  stride를 키워주면 완전 다른 현상이 발생합니다. convolution은 적은 차원에다가 정보를 압축하여 쑤셔박으려고 하지만, transpose convoltion은 오히려 input image인 [a b] tensor를 kernel을 곱해 확 펼쳐주는 역할을 합니다. 이렇게 되면 kernel을 학습할 수 있는 형태가 되며 size를 확장시켜주는 역할을 할 수 있게 되므로 segmentation 작업에 적합한 형태가 됩니다.​  다음은 classification과 Localization인데, 여기서는 object의 bounding box를 찾는 작업과 그 bounding box 안에 있는 object의 종류를 맞추는 작업 즉, multitask를 하게 됩니다. 그래서 2개의 Loss를 사용하는 Multitask Loss를 학습해야 하는데, Loss를 hyperparameter를 끼고 weighted sum을 합니다. 따라서, 한쪽이 Loss를 dominate하지 않도록 하이퍼파라미터를 조절하는 것이 중요한데, tricky하다고 하네요. In practice에서는 학습하다가 network를 중단하고, 두 개의 task에 대해 각각 fine tuning하는 방식으로 이용한다고도 합니다.​  Human Pose Estimation도 마찬가지 입니다. Loss로 regression Loss를 쓰는 것 말고는 classification과 큰 차이는 없습니다.​  Object Detection은 Localization과 달리 여러 개의 object에 대해 annotation을 하는 것입니다. 그래서 regression Problem으로 생각하면 조금 tricky하며, 다른 패러다임을 사용합니다.​  매번 다른 crop을 만들고, sliding하고 하는 방식의 brute force는 굉장히 비쌉니다. 그래서 object가 있는 곳을 먼저 proposal을 받는 알고리즘을 사용합니다.​  이런 방식을 Region Proposals라고 하며 fixed way입니다. closed edge과 같은 방법을 사용하여 region poposal network가 blobby region을 찾도록 하는 방법입니다. 예를들어 selective search가 있고 recall은 높은 편에 속합니다. ​  R-CNN은 앞에서 말한 방식대로 fixed way regin proposal network을 사용하여 candidate를 뽑고 CONV로 처리를 합니다.​  하지만, region proposal에 굉장한 시간을 쏟게 되며, candidate가 많으면 그만큼 학습량도 증가합니다. 따라서 이 방법은 적합하지 않습니다.​  그래서 Fast R-CNN이 나왔는데, 이것은 CONV로 high resolution feature map을 얻은 뒤 fixed way selective search로 crop들을 땁니다. 그리고 나머지는 이전과 비슷한데 여전히 문제가 있습니다.​  fixed way region proposal algorithm을 사용하기 때문에 Fast R-CNN의 Test time을 보시면, region proposal이 점유하는 시간이 2sec입니다.​  그래서 나온것이 Faster R-CNN인데, CNN으로 feature map을 딸때 proposal을 같이 학습하게 됩니다. 이렇게 되면 region proposal또한 learnable part가 되어 빠르게 forwarding할 수 있겠죠?​  예상대로 시간이 매우 단축되는 것을 확인할 수 있습니다.​  반대로 region proposal을 활용하지 않는 방법도 있습니다. YOLO(You Only Look Once)와 SSD(Single-Shot MultiBox Detector)인데, forwarding한방이면 object dectection을 수행할 수 있습니다. potential regions를 독립적으로 처리하는 방식 대신에, regression problem으로 다루게 됩니다. 일단 7x7 grid로 이미지를 나눈 뒤 object가 어느 grid에 있는지 offset을 학습합니다. 또한, 해당 offset에서 가로로 길쭉하거나, 세로로 길쭉하거나, 정사각형을 사용하여 다양한 Bounding Box를 만들게 됩니다. 이를 학습하게 되면 7X7(각 grid) X 5(bounding box feature) X B(서로 다른 종류의 bounding box 개수) X C(score)을 output으로 하는 Network를 학습하게 되는 것입니다.​faster R-CNN style의 region-based method가 single shot보다 더 높은 정확도를 가집니다. 하지만, 이러한 방식은 각 region에 대해 독립적으로 processing하지 않아도 되기 때문에 훨씬 빠릅니다.​  Object Detection과 Segmentation을 동시에 수행하기 위해  Mask R-CNN도 나왔는데, 먼저 RoI을 찾은 뒤 거기서 sub problem으로 segmentation을 해결하는 형태입니다.​끗끗 "
[cs231n] 11. Detection and Segmentation ,https://blog.naver.com/mmmy2513/222306502361,20210411,"이번 강의에서는 Image Classification을 넘어 다른 Computer Vision Task에 대하여 알아보았다!대표적으로 아래 네가지 task를 생각할 수 있다. ​1. Semantic Segmentation Semantic Segmentation은 픽셀 단위로 물체를 분류하는 작업이다. 이 때, 각 개체 별 구분을 두지 않고 class에 대한 정보만을 나타내기 때문에 오른쪽 아래 그림에서 볼 수 있듯 두 마리의 소가 구분되지 않는 것을 볼 수 있다. Semantic Segmentation의 첫 접근 방식은 Sliding Window 방식이다. Window간 feature를 공유하지 않아 매우 비효율적이다! 그래서 제안된 다른 방안이 Fully Convolutional 방법이다. 이 때 마지막 feature map의 C는 카테고리의 수이다. Input size를 유지하며 Convolution하기 때문에 여전히 expensive하다! 연산량을 줄이고자, 중간 feature map의 크기를 작게하여 연산 후, 다시 늘려 원래 size를 복원하는 방법이 사용된다. 일반적으로 Symmetric한 구조를 가진다. feature map크기를 줄이는 일은 Pooling 또는 strided convolution을 통하여 줄일 수 있지만, 이를 다시 키우는 작업은 이전에 소개된 적이 없다. 이를 위해 다양한 방법들이 소개된다. 가장 단순한 방법으로 위의 두 가지 방법이 있다. Nearest Neighbor는 같은 값을 복제하는 것이고, Bed of Nails는 고정된 위치에 Input 값을 넣고 나머지 값은 0으로 채우는 작업이다. 단순하지만 성능은 그닥! 보다 나은 방법으로 Max Unpooling 방법이 소개된다. Feature map을 줄이는 단계에서 Maxpooling할 때 뽑아낸 element의 위치를 저장하였다가, Unpooling할 때, 해당 위치에 값을 대입하는 것이다! 다른 방법으로 Transpose Convolution방법이 있다. filter와 feature map의 스칼라 값을 내적하여 채워 넣는 것이다! 이 때, 겹치는 부분은 값을 합하여 대입한다. 같은 이름으로 Upcovolution, Backward strided convolution 등으로 불린다.​2. Classification + Localization Classification + Localization은 말 그대로 이미지에 대해 카테고리의 분류와 해당 오브젝트가 어디에 있는지 알아내는 작업이다. 이 때, Class는 Classfication, 오브젝트 박스의 좌표는 Regression하여 추측하고, 각각 다른 Loss를 동시에 적용하여 Multitask Loss라고 한다. 두 Loss를 더하여 최종값을 내는데, 하이퍼파라미터를 이용하여 두 Loss중 어느 것을 더 Sensitive하게 받아들일지 결정할 수 있겠다. 또한 Network는 주로 ImageNet같이 큰 데이터로 미리 학습된 가중치를 이용하여 전이학습을 진행한다. 이를 이용하여 각 관절에 대해 Classification, Regression을 적용하여 어떤 자세를 취하는지 유추하는 것도 가능하다.​3. Object Detection위에서 단순히 Classification과 Localization을 했던 것과 달리 다수의 Object에 대해서 각각 어떤 카테고리이고 어디에 있는지 추측해내는 문제이다. 가장 초기 시도된 것은 Sliding Window 방법인데, 윈도우를 옮겨가며 어떤 물체가 있는지 판별해내는 방법이다. 하지만 역시 너무 expensive하다는 문제가 있다. 다음 시도는 Selective Search를 이용하여 Object가 있을 곳을 미리 뽑아낸 Region Proposal을 이용하는 방법이다. CPU를 이용하여 1000개 이상의 Region Proposal을 뽑아낸다.  위의 Region Proposal을 이용한 R-CNN이다! 하나의 이미지에 대해서 2000개의 Region Proposal을 추출해내고, 고정된 크기로 Warp한다. 이를 Convolution하여 feature map을 생성하고 각각 어디에 어떤 물체가 있는지 추측해낸다. 하지만 이는 너무너무 오래 걸리고 저장 공간도 굉장히 많이 차지한다는 문제가 있었다. 조금 더 개선된 Fask R-CNN이다. 2000개의 Region Proposal에 대해 모두 Convolution하는게 아니라, 전체 이미지를 한번에 Input으로 넣어 시간을 줄였다. 또한 단순 Warp는 왜곡 등의 문제가 있어서 RoI Pooling 방법을 사용하였다. RoI Pooling은 위와 같이 feature map에 projection된 region proposal을 일정한 grid로 나누어 maxpooling해 일정한 크기로 맞추어 주는 것이다! 최종 예측 결과를 Log loss + smooth L1 loss의 합으로 합쳐 Multi-task Loss를 사용한 것 또한 특징이다. R-CNN과 속도를 비교해보았을 때, 훨씬 빨라진 것을 볼 수 있다. 그러나 오른쪽 그림에서, Region proposal을 생성하는 단계에서 많은 시간이 소요되는 것을 볼 수 있다. 이를 개선한 Faster R-CNN이 등장한다! Region proposal을 Selective Search가 아니라, RPN을 이용하여 Object가 있는지, 없는지에 대해 예측하고 이를 이용하여 마지막 예측에 이용하는 것이다! 이 후 과정은 Fast R-CNN과 같다. 속도가 크게 향상된 것을 볼 수 있다. ㅎ ㅎ​위의 R-CNN계열의 모델들이 Region proposal을 추출하고 다시 클래스와 위치를 예측하는 2-stage 계열이었다면, 한번에 진행되는 1-stage계열 모델에는 YOLO, SSD등이 있다. 이미지를 일정한 grid로 나누어, 물체가 있을만한 Anchor box를 각 grid를 옮겨가며 물체의 위치에 대해 x,y,w,h,confidence와 클래스 별 score를 예측한다. 근래에는 속도가 빠른 이런 1-stage 계열들이 많이 나오는 것 같다!​이 외에도 다른 여러 Detection 모델들이 있고, Captioning을 더하여 Dense Captioning으로 응용되기도 한다! ​4. Instance SegmentationInstance Segmentation은 Semantic Segmentation에서 같은 Class내의 개체까지 구분하는 작업이라고 볼 수 있다! 대표적인 모델로 Mask R-CNN이 있다. 위에서 본 R-CNN 계열의 모델과 크게 다르지 않고, FC layer에 넣지 않고, Convolution을 통해 만들어진 feature map을 바탕으로 각 클래스에 대하여 예측한다. 여기에 더하여 관절 예측까지 할 수 있어, Pose estimation도 가능하다!​끝~ 😃😃😃😃​ "
인도 보행 영상 데이터로 학습시킨 객체 검출(Object Detection) 모델 공개 ,https://blog.naver.com/nia-aidata/222255781695,20210225,https://www.facebook.com/groups/TensorFlowKR/permalink/1072812833059774/ 이재원안녕하세요! 지난번에 이어 Detectron2에서 custom dataset을 활용하여 object detection 알고리즘을 학습시켜보는 간단한 튜토리얼을 공유합니다**[1]**. 이번에도 AIHub에서 제공하는 보행자 데이터셋**[2]**을 이용하였습니다. Detectron2 프레임워크 자체가 인터페이스가 너무 잘 되어 있어서 custom...www.facebook.com ​ 
인도 보행 영상 데이터로 학습시킨 객체 검출(Object Detection) 모델 공개 ,https://blog.naver.com/nia-aidata/222255780239,20210225,"https://www.facebook.com/groups/TensorFlowKR/permalink/1063297144011343?sfns=mo 이재원안녕하세요! 얼마전에 object detection 프레임워크인 Detectron2를 소개해 드렸었습니다. 이번에는 Detectron2를 이용하여 ""AI허브 보행자 공개 데이터셋**[1]""**을 학습시킨 모델과 Colab으로 작성된 Inference 튜토리얼**[2]**을 공유합니다! (데모 비디오**[3])** AIHUB 보행자 데이터셋은...www.facebook.com ​ "
v4 공부하기 #YOLOV4 #Detection #DL #ML ,https://blog.naver.com/qudtngh/222371411828,20210528,"입력 이미지에 필터(filter = kernel = weight)를 합성곱하여 나온 단일 값이 Feature map의 한 조각이 되는 것이다.​이 때, 필터가 이동하는 간격이 정해져있다면 Stride가 다른 것이고 또 이에 따라 피처맵의 크기가 달라질 수 있다. ​입력 이미지의 채널이 3개라면, 각각의 피처맵 3개를 더한 피처맵이 최종 피처맵이 된다. Yolo는 이러한 피처맵을 그리드라고 한다.​GRID 각각의 그리드안의 셀 (하나의 필터가 본 이미지의 계산 값이다.) ​셀 각각에대해서 Yolo는 n개의 디텍터를 가진다. 디텍터의 차원은 (x,y,w,h,conf, class갯수만큼의 conf) ​ 그러면 셀 갯수 ex) 13 x 13 x 디텍터 갯수(5) x (5+클래스 수)  만큼의 채널이 생긴다.​conf는 이 바운딩 박스가 실제 객체를 담고 있을 확률이 얼마인가를 표현한다. (Object-ness Score 라고 한다)(기억할 것은 뭐가 됐든 있는지 없는지만 판단한다는 것이다. 어떤 물체인지는 모른다. 때문에 Class 갯수 만큼의 class conf(probability) 가 필요한 것이다.)​prediction 횟수는 grid size(=cell x cell) x  Detector 갯수​이러한 Prediction 사이에는 반드시 박스 끼리의 Over warp 영역이 있는 경우가 존재할 것이다.​이를 처리하는 방안이 NMS(non-maximum suppression) 이다.​객체 검출에서 풀어나가야 하는 문제를 세분화 하였을 때, 어려운 것은, 어디에 물체가 있는가 (grid를 두고 해당 그리드에서만 있는지 없는 지를 detector 가 판단 하도록함) 뿐 만 아니라,​그 물체는 어떤 모양과 크기를 가졌는가 이다.그래서 학습 시에 미리 Typical Ground Truth의 Anchor의 사이즈를 계산하는데, 데이터 셋 내에 있는 Most Common Object Shape를 미리 아는 것이 유리한 이유이다.​Anchor의 크기와 종류가 정해진다면, 각 종류마다 하나의 디텍터와 매칭하면 된다.​나는 이걸 혼동했는데, Anchor 크기의 도메인은 input 이미지의 도메인과 맞춰야한다.저 위의 Feature 맵에 있는 grid 13 x 13의 한 칸을 Unit으로 하는 Anchor 가 있는 것이 아니라, 애초에 디텍터가 input 이미지에서 해당 디텍터가 가진 단일 사이즈의 Anchor를 가지고 (x,y,_,_,c) + Class prob 를 가지고 grid를 들어가는 것이다. ​It’s important to understand that these anchors are chosen beforehand. They’re constants and they won’t change during training.​Anchor를 Darknet code에서는 biases라고 칭한다(dimension priors) gen Anchor를 하면 k clustering을 하여 Anchor 사이즈를 정한다. 이러한 Centroid가 많아지면 IOU값이 증가한다.하지만 모델의 속도가 감소할 것 이다. 그런데 위 사진의 feature map 으로 어떻게 Inference 하여 BBox를 얻어낼것인가?단지 21,125개의 숫자일 뿐이다.​이제부터 바꿔 보겠다. scaling factor는 predict box가 Anchor 보다 얼마나 작고 큰지를,Position Offset은 grid 의 센터로 부터 predicted box가 얼마나 멀리 있는지를 표현한다.​ 해당 디텍터가 가진 앵커사이즈에  실제 predicted box와의 사이즈 차이를 (그리드 별로) 곱하여 실제 박스 크기를 만들어 내는 것이다.​박스 사이즈가 음수일 수는 없으니 exp을 사용한다.​곱한 32는 image input size 가 416 x 416이고 grid cell size가 13 x 13 이므로 grid의 한 셀이 포함하는 pixel의 갯수가 32이기 때문이다.​그리고 pixel coordinate에서의 bbox 위치 좌표 얻을 때는 다음 연산을 한다. https://machinethink.net/blog/object-detection/ One-stage object detectionObject detection is the computer vision technique for finding objects of interest in an image: This is more advanced than classification , which only tells you what the “main subject” of the image is — whereas object detection can find multiple objects, classify them, and locate where they are in th...machinethink.net Loss Function ​ "
DenseFusion: 6D Object Pose Estimation by Iterative Dense Fusion ,https://blog.naver.com/taeeon_study/222965686536,20221226,"로봇팔의 application 사례는 상당히 많지만 대표적인 것 중 하나는 당연히 물체를 옮기는 것이다. 영상 센서를 통해 획득한 정보를 최대한 활용하여 로봇팔을 움직여 물체를 잡을 수 있도록 해야한다. 물체를 무작정 잡는 것이 아니라 물체가 로봇팔의 위치에서 어떤 위치에 있는지, 어떤 자세를 이루고 있는지를 알아야 어떻게 물체를 잡을지 계산해낼 수 있을 것이다. 영상에서의 pose estimation의 단어를 쓸 때, 많이들 사람의 자세 추정에만 국한된다고 생각한다. 그러나, object detector로부터 인식한 객체의 자세를 추정할 수 있는 것들 등도 다 pose estimation의 영역에 들어가게 된다. paperswithcode에는 6d pose estimation이라는 단어를 치면 이 문제를 해결하기 위한 논문들을 많이 볼 수 있다. https://paperswithcode.com/task/6d-pose-estimation-1 Papers with Code - 6D Pose EstimationImage: [Zeng et al](https://arxiv.org/pdf/1609.09475v3.pdf)paperswithcode.com 내 본 연구는 따로이지만, pose estimation 관련해서 자료들을 찾아주고 논문들도 같이 읽어주면서 서포트 해주는 역할을 해보기로 하였다. 본래 영상과 러닝 그리고 자료 찾는 것들을 좋아하기에 공부 안되는 시간에 하면 좋을 것 같다. 오늘 읽어볼 논문은 6d pose estimation에서 상당히 유명한 densefusion이라는 논문이다. 이 다음에 포스팅할 morefusion이라는 논문도 이 densefusion 논문에서 많이 아이디어를 따오기 때문에, 이것부터 먼저 리뷰해보도록 하겠다.​Abstract이전에는 RGB 영상과 depth 영상에 대한 정보를 따로 추출하거나 후처리 단계에서 사용하곤 했는데, 이들은 성능을 제한시켰다. 이 논문에서는, RGB-D 영상으로부터 알려진 객체 set의 6자유도 pose를 추정하는 generic framework를 제공한다고 한다. 우선, densefusion은 많은 종류로 이루어진 아키텍처인데, 이는 두 가지 데이터를 개별적으로 처리하고 하나의 네트워크를 써서 pixel-wise 한 feature 임베딩을 수행한다고 한다. 또한, end-to-end 방식으로 반복적인 자세 개선 절차들을 통합한다고 한다. 이 두 가지가 main contribution인 것 같고, 본격적으로 살펴보자.​Introduction본래 RGB 영상으로만 자세 추정을 하게 됐을 때, 기존의 영상의 단점인 가림 현상이나, 센서 잡음들, 빛 등에 대해 상당히 큰 영향을 받았으나 서서히 값싼 RGB-D 센서가 나오기 시작하면서 조금씩 개선되고 있다. 그래도, 영상으로부터 자세 추정하는 문제는 아직도 상당히 어려운 문제 중 하나이다. 본래 고전적인 방법들은 영상 데이터로부터 특징들을 추출하고 이에 맞는 대응점들을 grouping 하고, 가설 검증을 하게 된다. 그러나, 특징들에 의존해야 하기에 환경에 상당히 의존하게 되고 이는 문제를 어렵게 만든다. 최근에는 영상 task들이 상당히 러닝을 많이 쓰기에 자세 추정 task에도 PoseCNN 같은 네트워크들이 등장하기 시작했다. pose estimation에서 중요한 부분을 차지하는 pose refinement 방식들은 3D 영상 정보를 활용하기 시작하면서 PoseCNN에서의 ICP (Iterative Closest Point) 방식이나 MCN에서의 multi-view hypothesis verificaiton 같은 방식들이 나오게 된다. 그러나 이 개선 방식들은 한 번에 최적화될 수 없으며 실시간 application에 적합하지가 않다. 비교적 최근, color와 depth 정보를 end-to-end 방식으로 이용하는 방법들이 나오게 되고 이것은 꽤나 효과적이였다. 이 논문에서는, RGB-D 영상을 입력으로 받아 알려진 객체의 6자유도 자세를 추정하는데 end-to-end 방식을 사용하게 된다. 핵심 아이디어는 RGB 값과 pixel 단위의 point cloud들을 임베딩하고 fusion 하는 것이다. 이는 지역적인 appearance 정보와 geometry 정보를 잘 활용하게 만든다. 그리고, 자세 개선 알고리즘을 end-to-end framework로 수행하여 성능을 높였다고 한다. 말이 길었는데, 최종 이 논문의 contribution은 아래 두 가지다.RGB-D 영상 입력으로부터 color와 depth 정보를 결합하는 방법 제공러닝 아키텍처를 가지고 반복적인 자세 개선 절차 통합 ​Related Work과거의 pose estimation 방법들을 간단하게 remind 해보자.Pose from RGB images - 보통 classic 한 방식들은 keypoint 추출 후에 매칭하고 이를 이용해 변환 관계를 파악하는 방식들이다. 조금 더 가서 PnP 같은 문제를 잘 해결하기 시작하면서 이도 활용되었다. CNN이 어떤 곳에서나 쓰였기 때문에 direct로 RGB 영상을 넣고 자세 추정하는 방법들도 꽤나 있었다. 보통 이런 방법들은 orientation 추정에 많이 집중 돼있었다고 한다. 단순한 RGB 영상은 환경 조건이나 깊이의 간접 추정의 어려움 등이 있다.Pose from depth / point cloud - 최근에 기술력이 향상하면서 3D 정보를 그대로 활용하여 detection 하고, 그를 넘어 pose estimation까지 확장되었다. 이 논문의 아키텍처도 비슷하다고 한다. Pose from RGB-D data - 이 경우는 바로 3D feature를 만들어내고 후처리한다. 그런데, 이들은 hard-corded이거나 reconstruction 쪽 task 같은 목적으로 최적화시키는 게 많이 나왔다. PoseCNN 같은 경우에는 depth를 바로 입력으로 fusion 하지만 후처리에 부담이 있었다. 이와 비교해서 이 논문은 3D 데이터를 2D apperance에 fusion해서 좋은 성능을 얻었다고 한다.​이 논문과 가장 관련있는 논문이 PointFusion이라고 한다. geometric 정보와 appearance 정보를 fusion 했다는 것에서 많은 공통점이 있는 것 같다. geometric 정보는 말그대로 기하학적인 정보들을 의미한다.  만일 point cloud 데이터라고 했을 때, 우리는 pixel 단위로 geometry 정보를 얻을 수 있을 것이다. appearance 정보는 color 같은 외관 정보들을 의미하게 된다. 컴퓨터로는 픽셀 값으로 흔히 표현될 것이다.  여기까지 본문을 읽기 위한 것들이고 영상에 대한 지식이 조금 부족하거나 단어의 이해를 못했다고 해도, 본론가서 이해해보면 된다. ​Modelgenerality의 손실 없이, 이 논문에서는 homogeneous transfomation matrix로 6자유도 자세를 나타낸다고 한다. SE(3)라고 흔히 많이 알고 있다. SLAM 혹은 Visual Odometry, 영상의 기하학적 관계 등을 공부해보았다면 SO(3)가 회전 관계, SE(3)가 회전 + 병진까지 나타낸 것을 알고 있을 것이다. 깊은 설명은 넘어간다.color와 depth 영상을 fusion 하는 것은 이번 논문에서 중요한 요소이며 이 문제를 아래와 같이 해결한다.1) 각 data들의 본래 구조를 유지하면서 color와 depth 정보를 다르게 처리하는 아키텍처 2) data들 사이에 내부 mapping을 이용해서 color-depth fusion을 수행하는 pixel-wise 네트워크그 이후에, differentiable (미분 가능한? 변화 가능한?) iterative refinement module로 pose 개선까지 하게 된다. 이 pose 개선 모듈이 main 아키텍처와 함께 training 가능하다고 한다. (여기서, differentiable이라는 개념은 morefusion에서도 나오기 때문에, 여기서 잘 이해할 필요가 있다.) ​1. Architecture Overview전체 시스템 아키텍처를 살펴보자. 이는 아래 그림과 같다. 두 가지의 주요 단계가 있다.1) color 영상을 활용해서 알려진 객체에 대해 의미론적 분할 (semantic segmantation)을 수행한 뒤 마스킹된 depth pixel을 먹이고, 이미지 cropping도 수행하게 된다.  2) segmentation의 결과 처리 및 객체 6자유도 pose 추정을 수행하게 된다. 이는 4가지 요소로 구성되는데 이는 아래와 같다.a) Convolution 네트워크를 이용해서 color 정보 처리하고, crop된 이미지에 각 픽셀을 color 특징 임베딩에 매핑하게 된다. b) PointNet (3D Point cloud 처리 러닝 아키텍처)에 기반한 네트워크를 이용해서 마스킹된 3D point cloud의 각 point들을 geometry 임베딩으로 처리한다.c) pixel 단위로 fusion 하는 네트워크는 두 개의 임베딩을 합치고 confidence 점수에 기반헤서 객체의 6자유도 pose의 추정 값을 출력으로 낸다.d) iterative self-refinement  방법은 반복적으로 추정 결과를 개선하게 된다. 이는 조금 있다 자세히 살펴보자.​아키텍처 설명만 봐도 거의 다 읽은 것 같은 이 느낌은 뭔지 모르겠지만 계속 읽어보자.​2. Semantic Segmentation의미론적 분할 task도 object detection task만큼 워낙 연구많이 되었기에 무슨 아키텍처를 사용했는지는 중요하지 않다. 단지 알려진 class 객체가 N개라면, 영상을 입력 받아 N+1개의 채널을 가진 map을 생성한다 정도만 이해하면 된다.​3. Dense Feature Extraction이 절의 중요한 목표는 어떻게 color와 depth 정보를 시너지 있게 추출할 것이냐이다. 비슷한 format으로 정보를 주긴해도 다른 space 상의 정보이기 때문에 이 논문에서는 color와 geometric feature들을 얻기 위해 따로 처리한다고 한다. - Dense 3D point cloud feature embedding: 핵심은 알려진 카메라 내부 파라미터 행렬을 이용해서 segmented 3D point cloud를 생성한 뒤에 PointNet 같은 아키텍처에 넣어 geometric feature를 생성한다는 것이다. 이 geometric 임베딩 네트워크는 각각의 segmented point들을 d_geo 차원의 특징 공간으로 매핑해서 포인트마다의 특징을 생산한다.​- Dense color image feature embedding:  color 임베딩 네트워크는 픽셀 하나하나의 특징을 추출하여 3D point 특징들과 이미지 특징들 사이에 대응점을 형성시키게 한다. 네트워크는 인코더-디코더 구조이며, H x W x 3의 이미지를 H x W x d_rgb 의 크기로 변환시킨다. 이는 임베딩 픽셀이 d_rgb 차원을 가짐을 볼 수 있다.​4. Pixel-wise Dense Fusion앞에 단점들은 크게 중요하지 않고 방법론들을 살펴보자.- Pixel-wise dense fusion: 지역적인 픽셀 단위의 fusion을 수행하는 것이 주된 아이디어이다. 이는 각각의 합쳐진 특징들에 기반해서 예측들을 만들 수 있게 하기 위함이다. 이 방식으로, 가림과 분할 잡음의 효과를 최소화하고 객체의 보이는 부분에 기반에서 예측을 할 수 있게 된다. 좀 더 구체적으로 보자. dense fusion을 위한 첫 단계는 각 포인트의 geometric 특징을 카메라 내부 파라미터 행렬들을 이용해서 image plane에서의 투영시키고 이에  대응하는 영상 특징의 픽셀과 결합한다. 얻어진 특징 쌍들은 합쳐진 후에 또다른 네트워크로 넘어간다. 이 네트워크는 symmetric reduction 함수를 사용하여 고정 size의 전역 특징 벡터를 만들어낸다. 오직 전역 특징만 쓰는 것은 삼가한다고 한다. 전체 아키텍처 그림에서도 보듯이 per-pixel feature와 global feature가 붙어서 같이 들어간다.​- Per-pixel self-supervised confidence: 이 논문에서는 pose estimation network를 구체적인 근거에 기반해서 가장 좋은 가설을 내놓는 네트워크를 훈련시키려고 했다. 그래서, confidence score를 각각의 예측에 추가하게 된다. (원래는 pose estimation 결과들만임.)​5. 6D Object Pose Estimation러닝이라면 목적함수도 상당히 큰 이슈이다. 이제 그것을 살펴보자. 딱봐도 자세 추정에 대한 loss가 우선 고려돼야 할 것이다. pose estimation에 대한 손실 함수는 ground truth pose와 예측된 pose 사이의 거리이다. 이는 아래 식과 같다.  이 손실 함수는 비대칭 물체에 대한 기본적인 손실 함수이다. 대칭 물체는 모호한 학습 목적을 가질 수 있기에 추정된 model 방향에 point와 가장 가까운 ground truth에 point 사이의 거리를 최소화시키는 목적 함수를 쓴다고 한다.  그리고, 이 함수들을 더한 뒤 픽셀 당 손실에 대한 평균을 최소화 하는 방향으로 학습을 진행하게 된다. 그런데, 앞에서 언급했듯이 confidence 점수가 들어오고 이에 대한 손실함수는 아래와 같다. 결론적으로, 낮은 confidence는 낮은 pose estimation의 결과를 낳지만 높은 패널티를 받을 수 있다. ​6. Iterative Refinement여기서, 뉴럴 네트워크에 기반하여 iterative refinement 모듈을 설계한 것을 설명한다. 이는 아래 그림과 같다. 주된 아이디어는 이전에 예측된 자세를 타깃 객체의 canonical frame의 추정으로 고려하고 입력 point cloud를 추정된 canonical frame으로 변환하는 것이다. 이를 다시 네트워크의 입력으로 넣고 residual pose를 예측하게 된다. 즉, pose residual을 추정하는 네트워크를 훈련하는 것이다. 이는 main 네트워크로부터 초기 pose 추정치를 받아 개선을 수행하기 위함이다. 각 iteration에서, main 네트워크로부터의 영상 특징 임베딩을 재사용하고 새롭게 변환된 point cloud의 geometric 특징과 dense fusion을 수행한다. pose residual estimator는 이들의 global feature를 사용한다고 한다. K 번째 반복 이후에는 아래와 같은 결과를 얻는다. differentiable은 이렇게 계속해서 변하는 자세들을 이용한 iterative refinement 방법을 의미않았나 싶다. ​이상으로 논문 리뷰를 마치겠다. 기본적인 러닝으로 pose estimation은 어떤 것을 고려하는지부터, 색다르게 object detection 처럼 confidence score까지 고려해서 학습하려는 아이디어들은 배울 점이 꽤나 많은 논문이였다.  본 논문은 아래와 같다.Wang, Chen, et al. ""Densefusion: 6d object pose estimation by iterative dense fusion."" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2019. "
[논문] Network virtualization for real-time processing of object detection using deep learning ,https://blog.naver.com/dldudcks1779/222269597562,20210309,"https://link.springer.com/article/10.1007/s11042-020-09603-0 Network virtualization for real-time processing of object detection using deep learningThese days, networked cameras are used in various applications using deep learning. In particular, as the deep learning technology for image processing develops, image-based application services using networked camera are expanding. Object detections are the representative application in the image-b...link.springer.com ​ "
이미지 프로세싱 & 컴퓨터 시각화 22부 - Grid Detection ,https://blog.naver.com/zeus05100/221731468192,20191209,"첨부파일chess_board.png파일 다운로드 첨부파일dot_grid.png파일 다운로드  안녕하세요. Detection시리즈도 거의 막바지에 다다르고 있습니다. 이번시간에는 Grid Detection에 대해서 살펴봅시다. 우리가 소실적 오목 혹은 체스를 둔 적이 있을 것입니다 (물론 모든 사람들이 해본건 아니지만 대부분 말이죠). 그럼 체스판, 바둑판은 어떻게 생겼는지 기억하시나요? (으흠.. 무시하는건 절대 아니구요) 다음과 같은 이미지로 기억하신다면 다행이군요.     그럼 여러분들의 시력을 테스트해보겠습니다. 바둑판과 체스판에서 혹시 페턴같은게 보이시나요? 규칙적인 무언가 말이죠. 그렇습니다. GRID가 보이신다면 바로 정답입니다.  근데 갑자기 쌩뚱맞게 이걸 왜 물어보냐구요? 이번시간에 우리가 다룰 내용과 연관이 있기 때문입니다. 그럼 잠시 주제를 돌려서 다른 이야기를 해보겠습니다.​우리는 태어나서 적어도 한번쯤은 카메라를 가지고 사진을 찍어봤을 것입니다. 물론 요즘에는 대부분 폰으로 찍겠군요. 요지는 이것이 아닙니다. 카메라를 통하여 찍힌 사진은 distortion이 생길 수 있습니다. 이를테면 찌그러지거나 선명치 않거나 흐리게 나오거나 말이죠. 우리가 원래 우리눈으로 보고 있는 원본을 100% 가져온다는건 불가능하죠. 그러나 근접하게 할 순 있습니다. 특히 Object Detection같은 application을 개발할때 말이죠!​제가 예전에 다녔던 Automonous Vehicle과 관련된 회사에서 경험한 것에 근거하여 이야기를 잠깐 풀어나가겠습니다. 회사명은 CV라고 하겠습니다. CV회사에서는 X라 불리는 조그만 장치가 있으며 그 장치는 자동차 앞유리에 부착되어 주행중 전방의 모든 시야를 카메라에 담습니다. 앞차, 신호등, 차선, 횡단보도 등 실시간으로 카메라에 모든것들을 담고 Object Tracking을 하여 기계가 알아서 사물을 판단하게 하는 것이죠. 여기서 제가 힘주어 말하고 싶은 부분은 바로 차선입니다.    ​물론 바둑판과 체스판처럼 차선은 GRID의 형태를 지니고 있진 않지만 상당히 규칙적으로 이어져 있죠? 처음 X기기를 자동차 앞면에 부착하고 사용할때 우리는 Calibration이란 것을 해줘야 한다고 하더군요. 그 이유를 물어보니 실제 주행에 앞서 카메라가 규칙적인 페턴을 인식하여 distortion에 덜 영향을 받음으로서 더 나은 Object Detection을 하기 위함입니다. 아래 그림은 ADAS가 장착된 자동차가 Calibration을 하는 장면입니다. 자동차 앞부분에 규칙적으로 점이 찍혀있는 것이 보이시죠? 저것 역시 Grid Detection의 한 예시가 됩니다.   잡담은 그만하겠습니다 (물론 지금까지 설명한 내용이 도움이 되셨다면 잡담이라 생각 안 될 수도 있겠군요). 그럼 이제 슬슬 코딩을 해보겠습니다. 친절하게도 opencv는 이런 Grid Detection 역시 가능케 해줍니다. 체스판 이미지를 하나 불러오겠습니다.  import cv2import numpy as npimport matplotlib.pyplot as plt%matplotlib inlineimg = cv2.imread(""Image/chess_board.png"")plt.imshow(img)   우리가 이미 위에서 살펴보았던 체스보드 입니다. 그럼 어떻게 GRID를 감지할 수 있을까요? cv2 라이브러리에 있는 findChessBoardCorners 함수를 씁니다. 먼저 중요한 사실은 이 함수는 체스보드처럼 생긴 GRID가 담긴 그림이어야 감지가 가능하다는 것입니다. 저도 모든 이미지를 가지고 테스트 해보진 않았지만 위와 같은 형태의 GRID가  들어있다면 무난히 감지하더군요.​ 함수는 두개의 값을 리턴합니다. found는 boolean값으로 체스보드 페턴을 발견했는지 안했는지에 대한 정보를 알려줍니다. 이 값은 매우 유용하게 사용되며 특히 Object Detection에서도 if statement와 함께 사용될 수 있겠군요. corners는 점의 좌표값입니다. 어디에서 corner가 발견되었는지 알 수 있죠. 그리고 한가지 더! 함수에서 두번째 인자값으로 받는 (7,7)은 무엇인가요? 이건 GRID 크기를 뜻합니다. 잠깐만요! 우리의 체스보드는 8 X 8크기 아닙니까? 그렇습니다. 하지만 맨 가장자리에 존재하는 corner는 감지가 불가능합니다. 불충분한 정보 때문일까요? 체스보드 중간에 있는 corner는 사방을 둘러봐도 GRID가 명백하지만 가장자리는 아니거든요. 그래서 (7,7)로 준 것입니다. GRID를 그려보면 더 이해가 갈 것입니다^^ found, corners = cv2.findChessboardCorners(img, (7,7)) 그럼 우리는 corners를 가지고 GRID를 원본 이미지에 그려봅시다. 방법은 매우 간단합니다. drawChessBoardCorners 함수가 정답입니다. cv2.drawChessboardCorners(img, (7,7), corners, found)plt.imshow(img)   매 ROW마다 다른색으로 corners가 칠해졌습니다. 이에 따라 GRID 역시 감지가 되었군요. 실제로는 저렇게 깨끗한 체스보드가 존재할 일은 드물기 때문에 감지를 100% 정확하게 한다는 것은 불가능하겠군요. 그러나 우리가 지금까지 배운 기법들을 잘 요리해주면서 이미지를 다듬어준다면 정확도는 조금이라도 올릴 수 있겠군요.​또다른 GRID Detection의 유형중 하나인 Circle-based GRID가 있습니다. 체스보드와는 달리 위에서 언급했던 자동차 Calibration을 할때 주로 사용되는 GRID 페턴이죠. 그럼 다른 이미지를 불러와보겠습니다. img = cv2.imread(""Image/dot_grid.png"")plt.imshow(img)   매우 고르지 못한 점이 규칙적으로 찍힌 사진이군요. 그럼 이것 역시 우리가 앞에서 해봤던 것과 비슷한 방법으로 시도해보겠습니다. 과연 페턴을 잘 찾아내는지 말이죠. ​앞서 사용했던것과 다른 함수를 사용합니다. 함수명은 findCirclesGrid 입니다. found, corners = cv2.findCirclesGrid(img, (10,10), cv2.CALIB_CB_SYMMETRIC_GRID) 역시 두개의 값을 반환합니다. 이번에는 두번째 인자값으로 (10,10)을 주었습니다. 그이유는 총 점의 갯수가 가로,세로 10개씩 하여 100개가 있기 때문이죠. 더 작은 값을 준다면 모든 점을 찾지 못하겠군요. 마지막 인자값으로는 디폴트 값을 줍니다. 어떻게 점을 찾을지에 대한 정보가 담겨있는 것입니다. 다른 값을 줄 수도 있지만 우리는 저 디폴트 값으로 진행하겠습니다. 그럼 코드를 돌려본 후 정말 찾았는지 found값을 확인해봅시다. True값이 떴다면 브라보!! 입니다. 그럼 아까와 마찬가지로 원래 이미지에 우리가 찾은 값을 그려봅시다. 똑같은 함수를 사용하여 그립니다. cv2.drawChessboardCorners(img, (10,10), corners, found)plt.imshow(img)   훌륭합니다. 각각의 열에 무지개처럼 다른 색깔이 칠해졌군요. 우리는 완벽하게 GRID를 발견하였습니다. 저렇게 줄이 연결되어 있다는 뜻은 수직선상에 모든 점들이 하나의 페턴을 이루어 균등하게 배분되어 있다는 뜻입니다. 저는 여러분들이 만약에 found값이 False일 경우 corners가 어떤 값을 포함하고 있는지 테스트해보는 것을 추천합니다. 배움이야말로 여러가지 수많은 경우의 수를 통하여 내가 경험을 쌓아나가면서 실력이 향상되는 것이기 때문이죠! :) ​이렇게하여 Grid Detection을 모두 마칩니다. 조금은 긴 섹션이었지만 아주 의미있었군요. 풍부한 읽을거리(?)와 코딩을 통하여 하나도 지루하지 않았기를 바랄뿐입니다 하핫!! 여기까지 따라오셨다면 여러분들은 정말 대단하신 겁니다!! 다음시간에는 Detection시리즈의 대미를 장식할 Contour Detection에 대해서 살펴보겠습니다. 그럼 Bye!! ​  ​ "
엔비디아 주가 폭등 전망 - AI 인공지능 시대 대장주 관련주 ,https://blog.naver.com/resumet/223112150037,20230526,"엔비디아 주가 폭등 전망 - AI 인공지능 시대 대장주 관련주 한때 챗GTP 관련주로 꼽혔던 엔비디아가 실질적인 인공지능 대장주로 꼽히고 있습니다. 2분기 실적 발표에서 어닝 서프라이즈를 발표하고, 어제는 주가가 25% 나 크게 올랐습니다. 그동안 리뷰를 몇번이나 하면서 전망이 좋다고는 봤지만, 이렇게 빠르게 상승할 것이라고는 생각지 못했던걸 반성해봅니다.​어제 기준으로 24.37% 가 오르면서 근 5년간 가장 높은 가격은 379달러로 마감했습니다. 후덜덜하네요. ​ 엔비디아 주가가 상승하는 이유엔비디아의 주가가 이렇게 상승한 이유는 무엇일까요? 바로 인공지능 AI의 훈련, 추론에 필요한 반도체를 만드는 기업이기 때문입니다. 엥 그래픽카드 만다는 회사 아니야? 라고 하실 분들이 많을텐데요.​맞습니다. nVIDIA 는 그래픽카드에 들어가는 핵심 반도체인 GPU 를 만드는 곳입니다. 재미있게도 GPU 는 게임이나 그래픽 작업을 하는데 가장 많이 쓰이지 않고, 각종 지겨운 ""계산"" 이 필요한 곳에 쓰이기 시작합니다. 대표적인 분야가 가상화폐 채굴이었으며, 그 다음이 바로 인공지능의 훈련이었죠. 가상화폐 채굴의 경우 아주 큰 숫자의 인수분해가 필요한 수학적 계산이 아주 많이 필요합니다. 큰 수 하나를 작은 소수 두개의 곱으로 분해하는 인수분해의 경우 방법론이 없기 때문에 하나씩 해보는 수밖에 없고, 이를 위해서 아주 많은 계산을 수행해야 합니다. GPU 는 이런 작업에 아주 강했고, 그래서 엔비디아의 GPU 를 쓴 그래픽 카드들이 아주 비싼 가격에 팔리고는 했죠.​가상화폐 시장이 지금은 죽었고, 기존의 작업증명 방식이 많이 사라졌다고는 해도, 어쨋든 채굴이라는 과정이 진행되기 위해서는 누군가 수학문제를 풀어야 하고, 그러기 위해서는 엔비디아의 성능좋은 GPU 를 가진 사람이 가장 유리하기 때문에 당분간 강세는 계속될 것이라고 봅니다. ​​ 인공지능 시대의 시작그 다음으로 등장한 곳이 바로 인공지능입니다. 원래의 인공지능 AI 분야는 로봇이나 자율주행차였죠. 그래서 엔비디아도 로봇 관련주나 자율주행 관련주로 많이 분류되었습니다. 재미있게도 이 두 분야는 모두 시각적인 AI 가 필요했습니다. 로봇은 앞으로 걷기 위해서 주변의 장애물을 봐야 하는 필요성이 있었고, 자율주행 차량 역시 카메라로 전후방의 차량 정보를 알아내어 위험성을 판단해야 했습니다.​카메라로 얻어진 정보를 분석해서 어떤 차량이 있는지, 어떤 장애물이 있는지를 알아내는 모델을 Object Detection 이라고 합니다. 사진 내에 어떤 목적물이 있는지를 분류해 내는 것이죠. ​이 모델은 수많은 사진을 보면서 사진 안의 어떤 물체가 있는지를 공부합니다. 사진에 동그라미를 치고, 이건 사람이야, 이건 개야, 이건 고양이야 하면서 알려주는 거죠. 그런 사진을 수천만장을 공부한 컴퓨터는 아래처럼 이미지 내에서 사람과 물체를 구분해 낼 수 있게 됩니다. ​문제는 이미지를 분석해서 공부하기 위해서는 엄청나게 많은 수의 계산이 필요하다는 것입니다. 과학자들과 개발자들은 당연히 GPU 를 떠올렸고 인공지능 분야에서 엔비디아는 대체할 수 없는 제품을 공급하는 기업이 되어 버렸습니다.​하드웨어도 뛰어난데 여기에 소프트웨어까지 결합해서 공급하니 따라올 회사가 없었는데요. 바로 쿠다 CUDA 라는 프로그램입니다. GPU를 가속화하고, 쉽게 개발하고 최적화한 이후 배포할 수 있는 프로그램이죠. 실제로 쓰기는 조금 어렵지만, 다른 AI 플랫폼에 비해 정말 좋은 조건인 것은 분명합니다. ​이때까지는 그냥 흐름이 좋은 기업이다라는 생각이었습니다. 주식 역시 그다지 재미가 있지는 않았는데요. 이슈는 챗GPT 로부터 시작되었습니다. 초거대 언어모델인 챗GPT 를 훈련시키기 위해서 가장 필요한 것이 인공지능 반도체였고, 수십억의 사람들로부터 들어오는 질문을 분석해서 적당한 대답을 만들어내기 위해서도 가장 많이 필요한 것이 AI 반도체가 된 것이죠.​아마 1분기의 엔비디아 매출에는 챗GPT 를 위시한 비슷한 결의 서비스들로부터 받은 주문이 대부분을 차지할 것이라고 생각합니다. 주로 데이터센터와 자율주행 자동차 사업부문으로부터 매출이 발생했다고 했는데, 여기서 데이터센터라는 것이 바로 챗GPT 서비스를 하기 위한 서버들이 위치한 곳일테니 제 생각이 크게 틀리지는 않을 것입니다.  주가는 앞서 본 것처럼 하루아침에 25%가 오르면서 시총이 1조달러를 넘은 5번째 기업이 되었다는 뉴스도 있습니다. 새로운 시대의 뉴 골드러시라고 불릴만큼 인공지능 AI 는 빠르게 성장하는 시장이 되었는데, 거기서 가장 주목받는 기업이니만큼 앞으로의 전망도 가장 좋을 것이라고 생각됩니다.​ 관련기업들마이크로소프트 MS 의 빙이 챗GPT 와 코파일럿 기능을 재차 내놓고 있습니다. 심지어 챗GPT 에 역으로 빙AI 기능을 가져다가 붙이면서 구글에 빼앗겼던 10년을 찾아오겠다는 칼을 갈고 있는 상황입니다.​구글도 가만 있지는 않습니다. 바드, 음유시인이라는 이름의 인공지능 서비스를 내놓았죠. 처음에는 실망감을 주는 서비스였습니다. 그러나 차차 개선이 되면서 최신 정보를 담고 있는 바드가 챗GPT 보다 훨씬 좋은 답을 내놓는 경우도 생기고 있습니다.​** 물론 잘못된 답을 내놓기도 합니다.  네! 저는 이태원클라스를 그리고, 안나라수마나라를 연재하고 있는 만화가 최광자입니다. ㅎㅎㅎ​국내에는 아숙업 AskUp 이라는 이름으로 서비스되는 인공지능 모델도 있습니다. 실제 대규모 모델을 훈련시킨 것인지 아니면 ChatGPT4 의 API 를 이용만 하고 있는 것인지는 잘 모르겠습니다만, 어쨋든 인공지능 서비스가 대중화하는 데 큰 역할을 했죠.​이렇듯 초거대 글로벌 기업들이 모두 이 시장을 탐내고 있습니다. 우리가 모르는 다양한 기업들이 관련이 되어 있을 것이고 이는 엄청난 경제적 효과를 만들어오게 될 것입니다. 이미 ChatGPT 라는 주제 하나만 해도 유튜브나 책, 강의와 강연 등등이 활성화되고 있고, 인공지능 그림을 그려서 팔아먹는 모델, 이를 위한 명령어인 프롬프트를 만들어 파는 엔지니어링 시장까지 크게 성장하고 있는 상황입니다.​개인적으로 메타버스 같은 허황된 이야기보다 지금의 인공지능이 더 큰 시장이라고 생각합니다. 근간이 되는 기술이 확실하기 때문이죠. 그 기술의 중심에 엔비디아가 있어 더 주목받는게 아닌가 합니다.​  관련ETF세상에 GPU 를 만드는 회사가 nVIDIA 만 있는건 아닙니다. 우리가 익히 알고 있는 AMD 역시 CPU 와 GPU 를 만든 회사입니다. 주목도가 엔비디아처럼 큰 것은 아니지만 일종의 FOMO  를 느낀다면 한번쯤 고려해볼만 한 종목입니다. ​너무 올라서 들어가기 두렵다면 엔비디아를 포함한 ETF 들을 찾아보는 것도 좋습니다. 국내에도 다양하게 나와 있습니다.​ACE글로벌반도체TOP4솔라액티브ACE글로벌메타버스테크액티브KODEX미국반도체MVTIGER미국필라델피아반도체나스닥KODEX미국FANG플러스​비중은 높은 순대로 정렬되어 있습니다. 최근 수익률은 비등비등한데 ACE글로벌반도체TOP4솔라액티브가 20% 가 넘는 비중으로 보유하고 있으면서 30%가 넘는 수익률을 보이고 있네요. ASML, TSMC, 삼성전자 등 세계 탑티어 반도체 관련주 4곳이 80%가 넘는 비중으로 포함되어 있습니다. 이들 모두 엔비디아 관련주라고 할 수 있겠죠?  ​해외ETF 도 당연히 있습니다. SOXX ETF 는 10% 수준으로 엔비디아를 가지고 있는 종목입니다. 이외 브로드컴, AMD, TI, 퀄컴, 인텔, 램리서치 등의 기업들이 다양하게 포함되어 있습니다. (삼성은 없음..) ​엔비디아의 폭등 덕분에 SOXX 역시 주가가 크게 올랐습니다.  ​SOXX 의 3배 레버리지 ETF 버전인 SOXL 역시 큰 폭으로 주가가 올랐습니다. 어제 하루에만 20%가 올랐네요.  ​​이처럼 다양한 형태의 ETF 가 있으니 목적과 취향에 맞는 종목을 잘 선택해서 투자하시면 되겠습니다. 현재의 열풍에 탑승하는 안전한 방법이기도 하구요. 장기적으로 계속해서 발전할 분야이니만큼 꾸준히 조금씩 분산투자를 시작하는 좋은 계기가 될 수 있다고도 생각합니다.​ 같이 읽어보면 좋은 글미국주식 인공지능 AI 반도체 관련주 - 엔비디아 주가 전망 (엔디비아 아님)미국주식 인공지능 AI 반도체 관련주 - 엔비디아 주가 전망 (엔디비아 아님) 인공지능이라는 것이 무엇일...blog.naver.com 인공지능 서비스와 엔비디아 관련주에 대해 알아보시려면 읽어보세요!​끝. "
OpenCV - Haar Cascade 방식 Face Detection(1) ,https://blog.naver.com/haneengineer/222078740879,20200903,"Object Detection - Face Detection​Haar CascadeOpenCV 튜토리얼 중 Haar Cascade방식은 Paul Viola와 Michael Jones가 2001년에 발표 한 ""Rapid Object Detection using a Boosted Cascade of Simple Features"" 논문에서 제안한 효과적인 객체 감지 방법입니다. ​검출할 물체가 있는 이미지(Positive)와 없는 이미지(Negative)을 최대한 많이 활용해서 다단계 함수를 훈련시키는 Machine Learning 방법입니다. 빠르고 심플하다는 장점이 있습니다.  가장 먼저 알고리즘에서 분류자(Classifier)를 훈련시키기 위해 엄청난 양의 훈련용 Positive, Negative 이미지가 필요합니다. 그 이후 이미지 안에서 특징들(Features)를 추출해야 합니다. 이를 위해, 아래 그림에서의 Haar 특징이 사용됩니다.​ 왼 : convolutional kernel 오 : HaarCascade ​이는 마치 컨볼루션 커널(Convolutional Kernel)과 같습니다. 각 특징은 하얀색 사각형에서의 픽셀값의 합을 검정색 사각형 영역의 픽셀값의 합에서 뺀 값입니다. ( 세 개의 사각형으로 구성된 Haar 특징은 중앙에 있는 검은색 사각 영역 내부의 픽셀 합에서 바깥에 있는 두 개의 흰색 사각 영역 내부의  픽셀 합을 뺀 것입니다. 네개의 사각형도 유사합니다) ​모든 가능한 크기의 커널을 가지고 이미지 전체를 스캔하여 Haar 특징을 계산합니다.이 방식처럼 입력 이미지와 컨볼 루션 커널이 주어지면 커널을 모서리에 배치하고 커널을 이동시키는 컨볼루션 곱셈을 수행합니다.(Haar-Feature는 CNN의 커널과 비슷하지만 CNN에서 커널의 값은 훈련에 의해 결정되고 Haar-Feature는 수동으로 결정됩니다.)  Haar 특징을 구하기 위해서는 검은색 사각형과 흰색 사각형 아래에 있는 픽셀의 합을 구해야 합니다. 픽셀의 합을 구하는것을 빠르게 하기 위해서 Integral image를 사용합니다. 큰 이미지라도 빠르게 지정한 영역의 픽셀의 합을 구할 수 있습니다. ​ OpenCV 강좌 - Haar Cascades에 대해 알아보자.얼굴 인식에 사용하는 Haar Cascades에 대해 간단히 알아보았습니다. 아직 공부중이라 정확하지 않은 내용이 있을 수 있습니다 ;; 최초 포스팅 2019. 6. 28 Haar Cascade는 머신 러닝기반의 오브젝트 검출 알고리..webnautes.tistory.com 이 블로그에서 자세히 설명하고 있습니다.  24 x 24를 기본 윈도우로 사용하고 1 PX 씩 이동하여 이미지 전체에 걸쳐 위의 특징을 계산합니다.24×24 크기의 이미지만 해도 160000개 이상의 features 결과값이 존재합니다. 이러한 결과를 다 사용하기에는 시간 소모도 많고 비효율적입니다. 이를 위해서 Adaboost를 사용합니다.​이후 각  특징에 대해 얼굴이 포함된 이미지와 얼굴이 없는 이미지를 분류하기 위한 최적의 임계값(threshold)을  찾습니다. 흰색 부분과 검정색 부분의 차이가 어느정도의 값 이상이면 얼굴과 관련된 영역이라고 여기는 것입니다. ​처음에는 동일한 가중치가 부여되지만 분류를 한 뒤 잘못 분류된 이미지의 가중치를 증가시킵니다. 이후 동일한 프로세스를 수행하면서 필요한 정확도, 오류율에 도달하거나 필요한 features수만큼 프로세스가 계속됩니다.얼굴 이미지와 얼굴이 아닌 이미지를 가장 잘 분류하는 features을 선택합니다.​최종 Classifier은 이러한 약한 Classifier의 가중치 합입니다. 약한 Classifier은 이미지를 단독으로 분류할 수 없지만 다른 Classifier과 함께 강력한 힘을 발휘할 수 있습니다. 이 논문에서는 200 개의 features이 95 %의 정확도를 제공한다고 쓰여있습니다.​이러한 Adaboost를 통해 160,000개의 특징은 6000개의 특징으로 줄어들게 됩니다.   이러한 방식으로 줄어든 6000개의 특징도 전부 비교하기에는 너무 시간이 많이 걸립니다. ​그래서 Haar Cascade는 얼굴이 아닌 영역을 제외하여 시간을 줄입니다. 이미지의 대부분의 공간은 얼굴이 없는 영역입니다. 만약 얼굴 영역이 아니라면, 버리고  다시 수행하지 않습니다. 대신 얼굴이 있는 곳의 영역에 초점을 맞춥니다. 이 방식은 Cascade Classifier라고 합니다. ​윈도우가 이미지 위를 이동할 때마다 6000개의 특징을 모두 적용하지 않고 여러 단계의 그룹으로 묶어 사용하는 겁니다. 첫번째 단계의 특징에서 얼굴 영역이 아니라는 판정이 나면 바로 다음 위치로 윈도우를 이동하며첫번째 단계의 특징에서 얼굴 영역이라는 판정이 내려지면 현재 윈도우가 위치한 곳에 다음 단계의 특징을 적용합니다. ​​이 HaarCascade를 이용해서 Face Detection을 해볼 수 있습니다. 이런 식으로 사람의 얼굴과 눈을 Detect할 수 있습니다.소스코드는 내일 포스팅하겠습니다.​감사합니다.  ​​​​​​​​참고한 자료 :  Face Detection using Haar Cascades — OpenCV-Python Tutorials 1 documentationOpenCV-Python Tutorials latest OpenCV-Python Tutorials Introduction to OpenCV Gui Features in OpenCV Core Operations Image Processing in OpenCV Feature Detection and Description Video Analysis Camera Calibration and 3D Reconstruction Machine Learning Computational Photography Object Detection Face D...opencv-python-tutroals.readthedocs.io Haar cascade Face Identification – mc.aimc.ai OpenCV 강좌 - Haar Cascades에 대해 알아보자.얼굴 인식에 사용하는 Haar Cascades에 대해 간단히 알아보았습니다. 아직 공부중이라 정확하지 않은 내용이 있을 수 있습니다 ;; 최초 포스팅 2019. 6. 28 Haar Cascade는 머신 러닝기반의 오브젝트 검출 알고리..webnautes.tistory.com ​ "
이미지 프로세싱 & 컴퓨터 시각화 28부 - Face Detection (1부) ,https://blog.naver.com/zeus05100/221877166103,20200328," 여러분들 안녕하세요. 아주 오랜만에 이렇게 다시 인사를 드립니다. 여러가지 사정으로 인해 블로그 글 업뎃이 지연된점 죄송해요 ㅠㅠ 이번시간에는 좀 쓸모있는? 내용에 대해서 다뤄볼까 합니다. 우리의 안면을 컴퓨터가 사진속에서 얼마나 잘 인식할 수 있는지에 대한 Face Detection이란 주제를 가지고 왔습니다. 그럼 Face Detection은 무엇이며 왜 사용되어야 할까요?​무의식속에서 우리는 우리들의 모습을 추억에 담기 위해 종종 셀카를 찍기도 하고 남을 찍어주기도 하죠. 그럴때 우리의 카메라(폰에 내장된)는 신기하게도 사람의 얼굴을 인식하며 얼굴 주변에 정사각형의 네모가 만들어 지는 것을 적어도 한번쯤은 보셨을 꺼에요. 뿐만 아니라 인스타나 페북에서 다른 사람들을 테그할때 사진에서 얼굴부분만 정사각형 네모가 생기며 그곳을 클릭하면 사람의 이름이 나타나며 테그를 할 수 있는 기능이 생기죠? 뿐만 아니라 아이폰10같은 경우에는 얼굴인식을 통하여 잠금헤제를 할 수 있죠? 더이상 페턴이나 지문인식에만 의존했던 시대는 사라지고 있는 것같습니다. 이것들 외에도 너무나도 많은 예시가 떠오르지만 이렇게 두가지에서 마무리 지을꺼에요. 그럼 우리는 이제 대략 Face Detection은 무엇이며 어디에 사용되고 있으며 왜 쓰이는지에 대해 감을 잡았지요? ​그럼 또다른 질문이 생깁니다..​Face Detection은 어떻게 하나요? 엄청 복잡하나요?​복잡하다... 뭐 주관적인 답변이지만 그리 복잡하진 않습니다! 이미 여러분들은 그보다 훨씬 어려운 내용들을 다뤘으며 따라서 Face Detection은 별거 아닐꺼에요 (저는 그렇게 믿고 싶습니다)​그럼 Face Detection은 어떤 알고리즘을 통하여 돌아가나요? 바로 Haar Cascades라는 기법이 사용됩니다. 그리고 Haar Cascades는 Viloa-Jones Object Detection기법의 중요한 요소중 하나입니다. 수많은 Face Detection 어플리케이션에서 종종 사용되며 얼굴인식에 중추적인 역할을 합니다. 이걸 사용하여 우리는 하나의 사진속에서 사람의 얼굴이 진짜 존재하는지, 그렇다면 그 위치는 어디인지(x, y축)를 알 수 있습니다. 그러나 안타깝게도 이건 Face Recognition이 아니라 단순 Detection이기 때문에 누구의 얼굴인지를 알 수는 없습니다. 예를 들자면 어떤 사진속에 BTS맴버들이 폼잡고 있는 사진이라면 우리는 그 사진을 보고 1초도 안되는 찰나에 BTS라는 것을 단숨에 알 수 있지만 컴퓨터는 BTS인지, 블랙핑크인지, 알 수 없는 것이죠 ㅎㅎ. 사람의 얼굴인지를 판별하기 위해서는 무엇이 필요할까요? 두말하면 잔소리군요! 둥그런 얼굴에 두눈, 하나의 코, 하나의 입, 두개의 귀 눈 윗부분에 위치해 있는 눈썹? 것들이 있어야 우리는 얼굴이라 할 수 있지요. 지금 다루진 않을 꺼지만 Face Recognition을 하기 위해선 딥러닝과 더 많은 데이터셋이 있어야 합니다. 나중에 기회가 되면 특집으로 다뤄보겠습니다^^​그럼 Viloa-Jones Object Detection에 대해서 간단히 살펴볼께요. 눈치채셨을지는 모르겠지만 Viloa와 Jones라는 두 사람이 머리를 맞대고 고안해낸 기법입니다. 어떻게 사물을 감지할 수 있으며 더 나아가 고유한 사물의 특징을 근거로 특정 이미지에 라벨을 부여할 수 있는 방법은 무엇이 있을까에 대해 심히 머리를 쥐어짜냈다고 합니다. ​첫번째로 Edge Feature가 있습니다. 하얀색과 검정색을 기준으로 우리는 모서리를 구분지을 수 있습니다. 가로든 세로든 말이죠.   ​두번째로 Line Feature가 있습니다. 마찬가지로 검정색 선을 기준으로 양쪽에 하얀색 경계선이 그어져 있습니다. 역시 가로든 세로든 말이죠.  마지막으로 Four-Sided Feature가 있습니다. 체스판처럼 생겼네요.  위에 언급한 세가지의 특징을 가지고 사진속에서 모서리와 선들을 구분짓습니다. 쉽게 생각해보면 다음과 같지요. 우리는 지금까지 다양한 기법들을 사용해오면서 이미지는 0부터 255까지의 숫자들이 들어있는 Matrix형태인 것을 아시죠? 숫자가 작아지고 커질수록 밝기에 현저한 변화도 생기는 것을 알 고 있구요. 그렇다면 다음의 경우를 생각해볼께요. 어느 그림의 일부분이라 상상해 봅니다.  왼쪽의 숫자는 픽셀값입니다. 보시다시피 두번째와 세번째 픽셀의 차이가 (183-14) = 169나 차이가 납니다. 이 말인 즉슨 컴퓨터는 저 부분을 선으로 인식할 것이라는 거군요 (위에서 살펴보았던 첫번째 Feature와 사뭇 흡사하지 않나요?) 그러나 실제 모든 사진들은 저렇게 규칙적으로 직선의 형태를 띄지는 않을 껍니다. 곡선도 존재할 것이고 둥그런 부분도 있을 꺼구요. 따라서 정확도가 완벽하게 100%를 만들어내는것은 거의 불가능에 가깝겠습니다. ​좀 다른 예시를 들어볼께요. 아래의 두 이미지를 살펴볼께요. 우리의 컴퓨터가 모서리라고 100% 확신하려면 밝은 부분들의 모든 픽셀의 평균값(A)과 어두운 부분들의 모든 픽셀의 평균값(B)을 계산한 후 B에서 A를 뺍니다. 만약 그 수가 1이거나 1이랑 가깝다면 모서리라 판단합니다. 왼쪽 이미지라면 1에서 0을 빼니 1이므로 완벽한 모서리가 되겠군요. 오른쪽 이미지는 실제 우리의 사진일 것입니다. 한번 계산을 해볼께요. ​0.5를 기준으로 크다면 어두운 부분(1+0.8+0.8+0.8+0.8+0.8+0.8+0.7) = 6.5, 0.5보다 작다면 밝은 부분(0+0.3+0.1+0.2+0.1+0.1+0.2+0.2) = 1.2​다음 저들의 평균을 구합니다. ​6.5 / 8 = 0.8131.2 / 8 = 0.15​그럼 이제 저 둘을 빼면, 0.813 - 0.15 => 0.663이 나옵니다. 그럼 저 값을 가지고 모서리라 판단해야 할까요? 정답은 없습니다. 보편적으로 0.5를 마지노선으로 잡고 그 이상이면 모서리라 하고 그렇지 않다면 모서리라 판별하지 않게 하지만 우리는 알고리즘을 구현할때 파라미터 값으로 지정할 수 있습니다. 0.7이라는 값을 넣는다면 우리의 0.663은 0.7보다 작기 때문에 모서리라 생각하지 않겠군요.  지금까지 이해가 잘 되셨나요? 그런데 여기에는 한가지 중요한 문제점이 있습니다...  우리의 이미지 크기를 (800 * 640)이라 합시다. 그럼 분명 수많은 픽셀값들이 존재할 것이며 컴퓨터는 저것들을 모두 계산해야 할까요? 컴퓨터가 너무 고통이 심하겠어요!! 그러나 괜찮습니다. Viloa-Jones Object Detection 기법은 Cascade 분류기를 사용함으로서 시간을 단축시킵니다. 이미지를 전체 한바퀴 순회하며 사람의 얼굴을 찾아내는 것이 아니라 Cascade를 뚝하니 사진에 던져놓고 얼굴이 어디있는지 찾는 방법입니다. 그럼 이런 생각이 들 수가 있습니다.​""사람의 얼굴은 정면을 바라볼 수도 있고, 측면을 바라볼 수도 있고 뒷면을 볼 수도 있습니다. 그럼 우리의 기법은 어디를 바라보던 정확히 사람의 얼굴을 찾아낼 수 있을까요?""​안타깝게도 우리의 Haar Cascades만으로는 정면을 바라보는 얼굴이어야만 합니다. 그이유는 Cascade분류기를 사용하기 때문인것도 하나의 이유죠 ㅠㅠ 그럼 Haar Cascade를 실행시키기 위해 어떤 과정을 거쳐야 할까요?​1. 정면으로 바라보는 얼굴이 담겨있는 이미지를 연다2. 사진을 흑백화 시킨다3. Haar Cascade를 돌려 특징들을 찾아낸다. 아래의 이미지처럼 눈썹과 코를 생성한는 Edge와 Line Features를 찾아낸다  4. 저 Features들을 계속 스캔하며 이미지 속에서 저 Features들이 존재하는지 탐색한다. 뿐만 아니라 눈과 입에 해당하는 Features들도 모두 찾아낸다5. 위에서 열거한 Features들이 없다면 사람의 얼굴이라 인식하지 않으며 얼굴은 없다고 우리들에게 알려준다​나중에는 더나아가 얼굴 인식이 아니라 신체부위의 일부분도 탐지할 수 있습니다. 다음과 같이 말이죠  참! 아까 Cascade 분류기를 사용한다 했잖아요? 그럼 얼마나 많은 Training data가 존재하나요? 걱정 안하셔도 됩니다. Opencv에서 이미 어마어마한 분량의 데이터셋이 우리들에게 제공됩니다. 다음시간에 우리가 오늘 배운 이론내용에 근거하여 직접 코딩을 해볼께요. 뿐만 아니라 데이터셋도 함께 첨부해서 올리도록 할께요. 초반에 여러분들에게 약속드렸던 (정말 유용한 기법들) 에 대한 내용이 이제부터 천천히 펼쳐집니다. 저또한 너무 기쁘구요 그럼 다음시간에 재미있는 코딩 같이 해볼까요?  ​ "
[Paper] High-Speed Tracking-by-Detection Without Using Image Information (2017 14th IEEE on AVSS) ,https://blog.naver.com/cho-yani/222120914967,20201020,"원문 : https://ieeexplore.ieee.org/document/8078516 AbstractTracking-by-detection은 multi-object tracking에 대한 일반적인 접근법이다. Object detector의 성능이 저하될수록 tracker의 basis는 훨씬 더 신뢰할(reliable) 수 있게 된다. 일반적으로 더 높은 프레임률과 함께, 이것은 성공적인 tracker의 도전에 변화를 준다. 그러한 변화는 더 sophisticate한 접근방식과 경쟁할 수 있는, computational cost가 극히 적은 훨씬 단순한 tracking 알고리즘의 배치(deployment)를 가능하게 한다. 우리는 그러한 알고리즘을 제시하고 다양한 object detector를 사용하여 그 잠재력(potential)을 철저한 실험으로 보여준다. 제안된 방법은 10만 fps에서도 쉽게 실행될 수 있으며 DETRAC vehicle tracking dataset에서 최첨단 기술을 능가한다. 1. IntroductionObject tracking은 semantic 비디오 해석의 핵심 기술이다. 고전적인 컴퓨터 비전 문제로서, 그것은 교통 분석, 스포츠 또는 포렌식 같은 분석 시스템에 중요한 정보 단서를 제공한다. 또한 보안 감시 영역에서 공통적인 사용 사례인 ANPR(automatic number plate recognition) 또는 안면 인식과 같은 further application에 대한 search space를 줄이는 데 도움이 될 수 있다. General한 시나리오에서 multi-object tracking을 위해서는 동영상에서 관심 object의 개수 추정과 그 path를 모두 필요로 한다.​Tracking-by-detection 분야에서 특히 중요한 점이 있다. 먼저, 각 비디오 프레임에 object detector를 적용한다. 두 번째 단계에서는 tracker를 사용하여 이러한 detection을 track에 연결한다. 특히 온라인에서 적용할 때 tracking-by-detection 시스템에 대한 일반적인 어려움은, 항상 false positive 및 누락 detection을 발생시킬 수 있는 underlying detector의 제한된 성능이었다. 좋은 tracker는 누락된 detection의 ""gap""을 채우고 false positive을 무시함으로써 이러한 결함을 처리할 수 있어야 한다. Multiple object가 교차하고 그 경로가 모호해질 때는 더 많은 문제가 발생한다. 이러한 문제를 해결하기 위한 많은 방법들이 제안되었다: [1, 2]에서는 연속 에너지 함수를 정의하고 정교한 minimization 기법을 사용하여 strong local minima를 검색한다. [6]은 모호하지 않은 프레임에 대한 짧은 tracklet을 추정하여 dynamics-bsaed 유사성에 따라 연결해준다. 다른 접근법에는 globally optimal이며 locally greedy한 방법 및 integer linear 프로그래밍[12] 및 온라인 discriminative appearance learning[3]이 포함된다.​CNN 기반 detection domain[4, 10, 5, 16]과 hand-crafted feature vector[7, 9]를 사용하는 기존의 접근 방식을 포함한 detection domain이 최근 들어 발전함에 따라, tracking 방법에 대한 새로운 가능성이 대두되고 있다. 이전의 접근방식에 비해, 물체에 대한 일시적 탐지 흐름(temporal stream of detection)의 gap은 점점 드물어지고 reported bounding box의 precision은 매우 정확해진다. 일반적으로 높은 프레임률(예: DETRAC dataset[17]은 25fps)과 함께, detection 크기와 위치의 차이도 프레임 간에 유의하게 작아졌다.​이러한 모든 발전은 tracking 작업을 매우 단순화하도록 이끈다. 따라서 본 논문에서는 [8]에 도입된 passive detection filter의 아이디어에 기초하여 매우 단순한 tracking 접근방식을 제시한다. 언급한 detector의 성능 개선 덕분에 훨씬 단순한 tracking 접근법이 성공할 수 있으며, 모든 경우에 보다 정교한 tracking 알고리즘의 오버헤드가 반드시 필요한 것은 아니라는 것을 알 수 있다.​Computational footprint가 매우 낮기 때문에, 제안된 방법은 다른 tracker에게 간단한 baseline 방법으로서 기능할 수 있으며, tracking 알고리즘에서 추가적인 노력의 중요성을 평가할 수 있다. 또한 tracking 벤치마크가 제기하는 특정 challenge(예: 누락된 detection, 프레임률 등)가 이미 달성할 수 있는(achieve) 알고리즘과 일치하는지 여부를 식별할 수 있다. Tracker의 소스 코드를 공개적으로 사용할 수 있다(https://github.com/bochinski/iou-tracker). 2. Method위에서 언급한 바와 같이, high precision detection과 프레임률이 높은 비디오 영상의 사용 둘 다 tracking 작업을 크게 간소화할 수 있다. 우리의 방법은 detector가 추적할 모든 물체에 대해 프레임 당 detection을 생성한다는 가정에 근거한다. 즉, detection에 ""gap""이 없거나 적다는 것이다. 또한, 연속된 프레임에서 물체의 detection들이 high overlap IOU를 가지고 있다고 가정한다(충분히 높은 프레임률을 사용할 때 흔히 있는 일임).​두 가지 요건을 모두 충족하면 tracking은 trival해지며 이미지 정보를 사용하지 않아도 추적이 가능하다. 이전 프레임의 마지막 detection에 detection과 가장 높은 IOU를 연관시켜(특정 threshold σIOU가 충족될 경우에만) track을 본질적으로(essentially) 지속하는 간단한 IOU tracker를 제안한다. 기존 track에 할당되지 않은 모든 detection은 새로운 track을 시작할 것이다. 지정된 detection이 없는 모든 track은 종료된다. 이 원칙은 그림 1에도 설명되어 있다. 그림 1. IOU Tracker의 기본 원리: 높은 프레임률에서 high accuracy detection일 경우, time step 간 spatial overlap으로 detection을 연결하기만 하면 tracking을 수행할 수 있다.tmin보다 길이가 짧은 모든 트랙과 σh보다 높은 점수의 detection이 하나도 없는 track을 걸러냄으로써 성능은 더욱 향상된다. 짧은 트랙은 대개 false positive이고 일반적으로 output에 잡음(clutter)을 더하기 때문에 제거된다. Track에 적어도 하나의 high-scoring detection을 요구하면 track의 완전성(completeness)을 위해 low-scoring detection의 혜택을 받는 동시에 track이 진정한 관심 대상(true object of interest)에 속함을 보장한다.​이 방법에 대한 자세한 설명은 알고리즘 1에 나타나 있다. 여기서 Df는 프레임 f의 detection, dj는 그 프레임의 j번째 detection, Ta는 active track, Tf는 finished track, F는 시퀀스의 프레임 수를 나타낸다. 알고리즘 1.Line 5에서 'unassigned, best-matching detection만' track을 확장하기 위한 후보로 간주한다는 점에 유의하라. 이것이 무조건 detection Df와 track Ta의 최적 연관성으로 이어지지 않지만, 예를 들어 그 프레임에서 모든 IOU의 합계를 최대화하는 Hungarian algorithm을 적용함으로써 해결될 수 있다. 그러나 σIOU는 일반적으로 detector의 non-maxima suppression을 적용한 IOU threshold 범위에서 선택되기 때문에 best match를 선택하는 것이 합리적인 heuristic이다. 따라서 σIOU를 만족시키는 match가 여러 개인 것은 드물다.​이 방법의 overall complexity는 다른 첨단 tracker들에 비해 매우 낮다. 프레임의 시각적 정보가 사용되지 않기 때문에 detection 수준의 간단한 필터링 절차로 볼 수 있다. 즉 tracker를 최첨단 detector와 함께 온라인으로 사용할 경우, detector에 비해 computational cost는 무시할 수 있을 만큼 작다. 따라서 track은 사실상 추가 computational cost 없이 detection으로부터 얻을 수 있다. Tracker를 독립적으로 수행할 경우 다음 실험에서와 같이 10만 fps를 초과하는 프레임률을 쉽게 달성할 수 있다. 또한 속도 덕분에 output을 이미지나 모션 정보를 사용하여 연결할 수 있는 tracklet으로 간주하여 결과(result) 위에 tracking component를 추가할 수 있다는 점도 유의해야 한다. 3. ExperimentsVehicle detection 및 tracking을 대상으로 한 10시간 이상의 비디오 영상(25fps)으로 구성된 DETRAC dataset [17]에서 제안된 tracker의 성능을 검사했다. CompACT[5], R-CNN[10], ACF[7] 및 DPM [9]에 대한 baseline detection은 사용 가능하지만, DPM detection 결과는 일반적으로 너무 부정확하고 따라서 tracker에 적합하지 않기 때문에 DPM detection에 기반하여 보고하지는 않는다. 또한 VGG16 1-3-5 모델의 Evolving Boxes detector(EB) [16]를 사용하여 추가 detection을 계산했다.​평가는 UA-DETRAC 평가 프로토콜을 사용하여 이루어진다. Tracking의 경우, 이는 그 방법이 precision-recall curve를 계산하기 위해 서로 다른 detection score threshold σl로 여러 번 실행된다는 것을 의미한다. 이 curve에 걸쳐 공통 CLEAR MOT metrics[14]가 계산된다. 최종 score는 이 curve 아래 영역에 의해 구성되며 모든 detector threshold σl에 대한 tracker의 성능을 고려한다 ([17] 참조). 이는 σh를 사용한 thresholding에 영향을 미치지 않고 오히려 low-scoring detection의 가용성(availability)에 영향을 미친다는 점에 유의한다. 일반적으로 그리고 [8]에 따라, low-scoring detection의 수가 많을수록 우리의 접근방식에 대한 tracking 성능이 높아질 것이라고 가정할 수 있다.​성능 최적화 없이 Python에서 구현했다.​σIOU, σh 및 tmin에 대한 best parameter는 각 detector에 대한 training dataset에서 grid search로 결정되었다. 검색 범위는 표 2와 같다. 모든 detection score는 σ∈[0.0, 1.0]으로 정규화되었지만 각 detector에 대해 여전히 다르게 분포되어 있다는 점에 유의하라. 따라서 σh에 대해 다른 범위를 선택해야 한다. 표 2. 각 detector에 대한 grid-based parameter 검색 범위. Step size는 σ(IOU)에서는 1, σ(l)와 t(min)에서는 0.1이다.이 범위 내에서 3개 parameter의 모든 조합을 평가하여 detector당 64회 실행했다. 최적의 configuration은 UA-DETRAC challenge의 기본 metric인 PR-MOTA metric에 의해 선택된다. 결과의 시각화는 그림 2에 나타나 있으며, 각 detector와 각각의 configuration에 대한 best results는 표 3에 비교된다. 그림 2. 다양한 detector와 parameter에 대한 PR-MOTA 성능 비교. 각 파란색 점은 측정을 나타내며, 숫자는 t(min)에 대한 best value를 보여준다.표 3. DETRAC-Test dataset의 각 Tracker에 대한 overall, easy 및 medium+hard set에 대한 best results. Easy split은 네 개의 detector 모두에 대한 평균 점수만 사용할 수 있다. Medium과 hard set은 AVSS17 challenge와 함께만 평가할 수 있지만, baseline 결과는 제공되지 않는다. IOU tracker를 제외하고, 결과는 [17]에서 얻었다.지금까지 최고의 결과는 near maximum scoreing detection이 많은 EB detector를 사용하여 달성된다. EB detector는 very low scoring detection과 함께 많은 양의 false positive를 생성하기 때문에 이러한 결과는 평가 지표의 잠재적 결함에서도 이득을 얻는 것으로 보인다. 이는 high recall에서 low precision으로 PR curve를 효과적으로 확장한다. 그러나 우리의 IOU tracker는 이러한 detection의 영향을 받지 않지만 MOTAover-PR curve 아래의 영역이 상당히 커진다. 따라서 PR curve가 precision 축과 recall 축의 교점 사이에 완전히 정의(fully defined)된 경우에만 공정한 비교(fair comparison)가 가능할 것이다.​CompACT는 다른 reference detection보다 PR curve의 AP([17] 참조)가 훨씬 높지만, ACF와 R-CNN을 통해 더 나은 PR-MOTA 값을 달성할 수 있다. 이는 CompACT detection이 일반적으로 R-CNN과 ACF의 detection에 비해 적지만(fewer) 더 정밀하기(precise) 때문이다. 그러나 우리의 detector는 누락된 detection을 예측할 수 없기 때문에 더 많은 detection을 통해 이득을 얻는다. 특히 DETRAC evaluaion script에서는 tracker를 실행하기 전에 σl을 사용하여 detection을 threshold한다. 따라서 일치하는 detection이 없는 경우, 이는 [8]에 따라 σ < σl인 detection 검색을 억제(inhibit)하여 유의적인 개선(significant improvement)이 될 수 있도록 한다.​따라서 하나의 track에 대해 하나의 detection 누락은 ID 전환과 false negative를 모두 발생시켜 전체 성능을 떨어뜨린다. 반면에, false positive는 보통 low-scoring detection으로 구성된 짧은 track을 생산하기 때문에 어느 정도 배제될 수 있다. 그러한 track은 high-scoring detection threshold(σh 및 최소 길이 tmin)를 사용하여 필터링된다.​이 평가에 기초하여, 우리의 tracker는 DETRAC-Test data에서 σIOU = 0.5, σh = 0.7, tmin = 2의 R-CNN detection과 σIOU = 0.5, σh = 0.8 및 tmin = 2의 EB detection으로 시험된다. 6개의 최첨단 tracker와 그 결과를 비교한 것은 표 3에 나와 있다.​우리의 IOU tracker는 mostly tracked(PR-MT)와 mostly-lost(PR-ML) 뿐만 아니라 accuracy(PR-MTA) 및 precision(PR-MOTP)까지의 overall metric에 있어 다른 방법을 상당히 능가한다. 또한 R-CNN detection의 경우 100K fps 이상인데, 이는 0.7-390 fps의 baseline method보다 더 빠르다. EB에 대한 high scoring detection의 양이 많으면 다양한 σl의 더 큰 범위(a larger range of varying σl)에서 처리할 detection의 수가 크게 증가하므로 달성 가능한 fps의 수가 감소한다. 그럼에도 불구하고, EB detection을 하더라도, 다른 tracker에 비해 런타임은 무시할 수 있다.​Training과 overall test data 사이의 성능의 큰 차이는 아마도 그러한 set 사이의 detector의 accuracy 차이와 관련이 있을 것이다. Detector는 training sequence에 대해서도 train되었기 때문에 test sequence보다 비디오에서 더 나은 결과를 생성하는 것이 분명하다. 우리의 tracker는 error를 발견하기 쉬우므로 성능도 떨어진다. 또한, training data에 대해 train된 parameter는 training data에 대한 고품질 detection에 overfitting되어 어려움을 겪을 수 있다.​그림 3은 최종 PR-MOTA와 PR-MOTP 점수에 요약된 σl의 다양한 값에 대한 MOTA와 MOTP 점수를 보여준다. 그림에는 MOTA 값이 약간만 변경되는 동안 tracker가 광범위한 σl 점수를 처리할 수 있다는 것이 나와 있다. 낮은 σl 범위가 아니라 σh 위로 올라간 후에야 성능이 하락하기 시작하는데, track을 필터링(without detections of at least a score of σh)한 덕분이다. MOTP는 이 지표에 대해 σl과 함께 상승하는 경향을 보이며, 보다 정확한(accurate) detection에서 높은 값이 획득된다. 그림 3. 서로 다른 σ(l) threshold에 대한 MOTA 및 MOTP Scores. 점선은 각 방법에 대한 PR-MOTA/PR-MOTP 점수를 나타낸다. MOTA-Test 및 MOTP-Test 값은 EB detection를 사용하여 생성된다.표 3의 easy 시퀀스에 대한 평가는 첨단 기술보다 우수한 결과를 보여주지만, reference methods에 대한 이 수치를 해석할 때 주의해야 한다. 이는 UADETRAC 평가 서버 제출 정책이 test data에 대한 과도한 testing을 금지하기 때문에 결과는 4개 detector에 대한 평균 점수일 뿐이며, 이는 우리에게 적합하지 않다.​실험 결과 DETRAC dataset과 같은 일부 사례에서는 IOU tracker 같은 단순한 tracking 방법이 수십 년의 연구를 바탕으로 한 복잡한(complex) 접근법보다 더 나은 결과를 이끌어낼 수 있다는 것을 알 수 있다. 그러나 이는 보편적으로 유효하지는 않고 사용된 dataset에 따라 달라진다. 보행자 tracking과 같은 다른 tracking 작업에서 크기와 가로 세로 비율은 보통 사람이 걷고 있는 것과 같은 몇 개의 프레임에서만 더 큰 변화를 겪는다. Occlusion이 심해지고 프레임률이 낮아지면 overlap이 계산되어서 correctly matching detection에 대한 성공률을 낮출 수 있기 때문에, 보다 정교한 방법이 필요하다는 것을 알 수 있다.​이러한 고려사항을 염두에 두고 제공된 Faster FR-CNN[13] 및 SDP[19] 탐지를 사용하여 보행자 tracking을 위한 MOT16/MOT17 벤치마크[11]의 IOU tracker의 성능을 평가하였다. 시퀀스의 프레임률은 14 ~ 30 fps이다. 이 방법에 대한 best parameter는 training dataset에 대한 광범위한 grid search에 의해 결정된다. Train 시퀀스가 7개뿐이기 때문에, σh, σl, σIOU, tmin ∈ {1, 2, 3, 4, 5}에 대한 parameter space를 0.1 step으로 complete sweep하는 것이 가능했다. FR-CNN detection을 사용하는 best parameter는 σh = 0.9, σl = 0.0, σIOU = 0.4, tmin = 4로 MOTA 점수 49.96을 달성한다. SDP의 경우, σh = 0.5, σl  = 0.3, σIOU = 0.3, tmin = 5가 MOTA 점수 62.77로 가장 좋았다. Test 시퀀스에 대한 결과는 표 4와 같다. FR-CNN detection이 있는 IOU tracker는 이미 평균을 약간 웃도는 성능을 달성하고 있음을 알 수 있다. 보다 정확한(accurate) SDP detection으로, 특히 MOTA 점수는 상당히 상승할 수 있으며 본 논문 작성 시 64점 중 13점에 tracker를 배치한다. 이는 보행자 tracking, 이동 카메라, 다양한 프레임률 등 보다 까다로운 조건에서도 경쟁적 성과를 달성할 수 있음을 보여준다. 표 4. MOTA 점수 및 MOTA16 test dataset의 전체 평균에 따라 SDP 및 FR-CNN detection에 기반한 IOU tracker 결과를 best performing method와 비교또한 우리의 실험은 vehicle tracking과 고정된 크기의 물체, 정적 카메라, 높은 프레임률에서의 high accuracy detection의 경우, 좋은 tracking을 간단한 수준에서 달성할 수 있다는 것을 보여준다. 이러한 tracking 접근법의 결과를 새로운 tracking 벤치마크 설계에 고려할 것을 권고한다. 4. Conclusions이 논문에서 우리는 단순한 방법으로 성공적인 tracking이 가능하다는 것을 보여주었다. 우리가 제시한 IOU tracker는 complexity와 computational cost의 극히 일부만으로 최첨단 기술을 훨씬 능가한다. 이것은 적어도 현재의 CNN 기반 접근법의 붐 때문이 아니라 최근 object detection 영역의 발달로 인해 가능해졌다. 일반적으로 더 높은 프레임률의 비디오와 결합하여 tracking-by-detection 프레임워크의 multi-object tracker에 대한 요구사항이 대폭 변경되었다. 간단하지만 효과적인 IOU tracker는 이러한 특성을 활용하며 새로운 조건 내에서 tracker 설계를 반영하는 예시 역할을 할 수 있다. Acknowledgements이 연구는 European Community의 FP7과 BMBFVIP+로부터 보조금 협정 번호 607480(LASIE)과 03VP01940(SiGroViD)에 따라 자금을 지원받았다. "
"논문리뷰 | YOLO : Unified, Real-Time Object Detection ",https://blog.naver.com/mqjinwon/221618051097,20190816,"이미 잘 해석해놓은 논문이 있어서 이를 참조하면 좋을 것 같다.http://m.blog.daum.net/sotongman/10 YOLOYOLO ( https://arxiv.org/pdf/1506.02640.pdf ) Abstract YOLO 라는 새로운 object detection 방법을 제시한다. 이전의 detection 작업들은 classifier를 detection 에 맞게 만든 것인m.blog.daum.net ​이건 누군가가 잘 정리해놓은 거라서 가져와봤다http://blog.naver.com/PostView.nhn?blogId=sogangori&logNo=220993971883 YOLO, Object Detection NetworkYou Only Look Once : Unified, Real-Time Object Detection Joseph Redmon - University of ...blog.naver.com ​ "
CS231n | Lecture 11. Detection and Segmentation ,https://blog.naver.com/kimsamuel351/222833844479,20220729,​Semantic Segmentation ​Upsampling  Classification + Localization  Object Detection R-CNN Fast R-CNN Faster R-CNN YOLO / SSD Captioning  Instance SegmentationMask R-CNN 
"[논문 읽기] Progressive Domain Adaptation for Object Detection, CVPRW, 2019 ",https://blog.naver.com/khm159/222479982965,20210822,"https://365mins.tistory.com/4 [매주 논문 리뷰] Progressive Domain Adaptation for Object Detection, CVPRW, 2019논문 세미나를 하지않고 그냥 읽은 논문은 간단히 핵심만 쓰기로 했다. - Unsupervised Domain Adaptation 기법을 Object Detection 에 적용하다. - Intermidiate domain을 생성하여 점진적으로 Adaptation 한다...365mins.tistory.com ​ "
object_detection 의 train.py 에서의 batch_size ,https://blog.naver.com/alzxcvbnm/221456603995,20190201,global batch_size 는 gpu 갯수(num_clones)와 상관없이 변동이 없다.각각의 gpu 에 할당되는 batch_size 는 (batch_size // num_clones) 로 정의된다. 
Object detection_FPS ,https://blog.naver.com/skyfiower_/222037515416,20200721,"2-3. Object Detection 모델 평가지표 (FPS)안녕하세요~ 이번글에서는 Object Detection의 또 다른 성능지표인 FPS에 대해서 알아보도록 할게요. FPS란 Frame Per Second라고 이야기 하는데요. 초당 frame 수라고 번역할 수 있겠네요. 예를들어, 1초당 9개의..89douner.tistory.com ​ "
Bridging the Gap Between Anchor-based and Anchor-free Detection via Adaptive Training Sample Selecti ,https://blog.naver.com/deepstock/222065525873,20200820,"https://arxiv.org/pdf/1912.02424.pdf​AbstractObject detection has been dominated by anchor-based detectors for several years.물체 검출 분야에서 anchor 기반의 검출기가 몇 년동안 대세였다.​Recently, anchor-free detectors have become popular due to the proposal of FPN and Focal Loss.최근에 anchor-free 검출기가 보편화 되었다. 그 이유는 FPN과 Focal Loss가 제안 되었기 때문이다.-> FPN?? :-> Focal Loss?? : AdaBoost 트레이닝 과정처럼, 샘플에 대한 중요도 (weight)이 현재까지 그 샘플의 정확도에 반비려하도록 결정한다.??     그래서 좀더 어려운 샘플에 집중할 수 있도록 한다. 다시 말하면 좀더 어려운 샘플의 gradient를 크게 반영한다.-> 그런데 이 것과 anchor-free 디텍터가 유명하게 된 것과 무슨 관계일까??​In this paper, we first point out that the essential difference between anchor-based and anchor-free detection is  actually how to define positive and negative training samples, which leads to the performance gap between them.이 논문에서는 먼저 anchor-based와 anchor-free 디텍터의 근본적인 차이를 밝힌다. 그 차이는 다름이 아니라 positive와 negative 샘플을 정의하는 방법이다. 이 정의 방법의 차이는 두 디텍터의 성능 차이도 만든다. 성능의 차이라는 것이 어떤 것을 의미하는가? 앵커를 박스로 쓴 것이 안 좋다??​If they adopt the same definition of positive and negative smaples during training, there is no obvious difference in the final performance, no matter regressing from a box or a point.트레이닝 할 때, positive와 negative  샘플의 정이가 같다면, 마지막 성능에서 눈에 띄는 차이는 없다. regression을 box로 하든 point로 하든.​This shows that how to select positive and negative training samples is important for current object detectors.이것으로 미루어 볼 때, 현재 물체 검출기에 서 중요한 것은 positive / negative 트레이닝 샘플을 선택하는 방법이다.​Then, we propose an Adaptive Training Sample Selection (ATSS) to automatically select positive and negative samples according to statistical characteristics of object.따라서 우리는 ATSS를 제안한다. 이는 자동으로 positive와 negative 샘플을 골라낸다. 물체의 통계적 특징에 따라.​It significantly improves the performance of anchor-based and anchor-free detectors and bridges the gap between them.이러한 방법으로 상당한 성능 개선이 anchor-based / ancho-free 모두에서 있었다. 그리고 둘 사이의 성능 갭도 메꿔졌다.​Finally, we discuss the necessity of tiling multiple anchors per location on the image to detect objects.마지막으로 한 위치당 여러 앵커를 타일링하는 것의 필요성을 논의 하였다.그래서??​Extensive experiments conducted on MS COCO supprot our aforementioned analysis and conclusions.MS COCO dataset에서 많은 실험을 수행하였고, 위에서 언급한 분석과 결론이 타당함을 보였다.​With the newly introduced ATSS, we improve state-of-the-art detectors by a large margin to 50.7% AP without introducing any overhead.우리가 제안한 ATSS는 큰 성능 개선을 보였고 (50.7 %) 이는 추가적인 오버헤드 없이 이루어졌다.​The code is available at https://github.com/sfzhang15/ATSS sfzhang15/ATSSBridging the Gap Between Anchor-based and Anchor-free Detection via Adaptive Training Sample Selection, CVPR, Oral, 2020 - sfzhang15/ATSSgithub.com 2. Related WorkCurrent CNN-based object detection consists of - anchor-based and : two-stage and one-stage methods- anchor-free detectors. : keypoint-based and center-based methods​2.1. Anchor-based DetectorTwo-stage method.The emergence of Faster R-CNN [47] establishes the dominant position of two-stage anchor-based detectors.​One-stage method.With the advent of SSD [36], one-stage anchor-based detectors have attracted much attention because of their high computational efficiency.​2.2. Anchor-free DetectorKeypoint-based method.This type of anchor-free method first locates several pre-defined or self-learned keypoints, and then generates bounding boxes to detect objects.CornerNet [26] detects an object bounding box as a pair of keypoints (top-left corner and bottom-right corner) andCornerNet-Lite [27] introduces CornerNet-Saccade and CornerNet-Squeeze to improve its speed.ExtremeNet [71] detects four extreme points (top-most, left-most, bottom-most, right-most) and one center point to generate the object bounding box.CenterNet [11] extends CornerNet as a triplet rather than a pair of keypoints to improve both precision and recall.RepPoints [65] represents objects as a set of sample points and learns to arragne themselves in a manner that bounds the spatial extent of an object and indicates semantically significant local areas.​Center-based method. This kind of anchor-free method regards the center (e.g., the center point or part) of object as foreground to define positives, and then predicts the distances from positives to the four sides of the object bounding box for detection.YOLO [45] divides the image into an S x S grid, and the grid cell that contains the center of an object is responsible for detecting this object.DenseBox [20] uses a filled circle located in the center of the object to define positives and then predicts the four distances from positives to the bound of the object bounding box for location.GA-RPN [59] defines the pixels in the center region of the object as positives to predict the location, width and height of object proposals for Faster R-CNN.FSAF [72] attaches an anchor-free branch with online feature selection to RetinaNet.The newly added branch defines the center region of the object as positives to locate it via predicting four distances to its bounds.FCOS [56] regards all the locations inside the object bounding box as positives with four distances and a novel centerness score to detect objects.CSP [37] only defines the center point of the obbect box as positives to detect pedestrians with fixed aspect ratio.FoveaBox [23] regards the locations in the middle part of object as positives with four distances to perform detection.​ 3. Difference Analysis of Anchor-based and Anchor-free DetectionWithout loss of generality, the representative anchor-based RetinaNet [33] and anchor-free FCOS [56]  are adopted to dissect their differences. RetianNet 과 FCOS를 비교하여 앵커 사용하는 것과 사용하지 않는 것의 차이점을 살펴 보겠다.​In this section, we focus on the last two differences:  the positive/negative smaple definition and   the regression starting status.이 섹션에서는 라스트?? 두 차이점에 집중한다. : positive/negative 샘플의 정의, regression 시작 상태 라스트?? 이전 섹션에서 여러가지 차이점을 설명했는가?​The remaining one difference : the number of anchors tiled per location,   will be discussed in subsequent section.남은 다른 차이저은 한 위치당 타일된 앵커의 개수이다. 이는 다음 섹션에서 논의하자.​Thus, we just tile one square anchor per location for RetinaNet, which is quite similar to FCOS.RetinaNet에 대해 한 위치 당 하나의 스퀘어 앵커를 두었고 이는 FCOS와 유사해 졌다.​In the remaining part, we first introduce the experiment settings, then rule out all the implementation inconsistencies, finally point out the essentail difference between anchor-based and anchor-free detectors.남은 파트는 다음과 같이 구성이 된다. 첫째 실험 세팅을 소개한다. 둘째 모든 구현상 불일치를 제거해 나간다. 마지막으로 앵커 기반과 앵커 무관 디텍터의 근본적인 차이를 알아낸다. 3.1. Experiment Setting  Dataset. All experiments are conducted on the challenging MS COCO [34] dataset that includes 80 object classes. ​ Following the common practice [33, 56],  all 115K images in the trainval35k split is used for training, and  all 5K images in the minival split is used as validation for nalysis study. ​ We also submit our main results to the evaluation server for the final performance on the test-dev split. Training Detail. We use the ImageNet [49] pretrained ResNet-50 [16] with 5-level feature pyramid structure as the backbone. The newly added layers are initialized in the same way as in [33]. For RetinaNet, each layer in the 5-level feature pyramid is associated with one square anchor with 8S scale, where S is the total stride size. During training, we resize the input images to keep their shorter side being 800 and their longer side less or equal to 1,333. The whole network is trained using the Stochastic Gradient Descent (SGD) algorithm for 90K iterations with 0.9 momentum, 0.0001 weight decay and 16 batch size. We set the initial learning rate as 0.01 and decay it by 0.1 at iteration 60K and 80K, respectively. Unless otherwise stated, the aforementioned training details are used in the experiments. Inference Detail. During the inference phase, we resize the input image in the same way as in the training phase, and then forward it through the whole network to output the predicted bounding boxes with a predicted class. After that, we use the preset score 0.05 to filter out plenty of background bounding boxes, and then output the top 1000 detections per feature pyramid. Finally, the Non-Maximum Suppression (NMS) is applied with the IoU threshold 0.6 per class to generate final top 100 confident detections per image. 3.2. Inconsistency Removal GroupNorm [62] in headsGIoU [48] regression loss functionlimiting positive samles in the ground-truth obx [56]the centerness branch [56]trainalbe scalar [56] for each level feature pyramid​therefore they are not the essential differences between anchor-based and anchor-free methods.By now, after removing all the irrelevant differences, we can explore the essential differences between anchor-based and anchor-free detectors in a quite fair way.​​3.3. Essential DifferenceAfter applying those universal improvements, these are only two difference between the anchor-based RetinaNet (#A=1) and the anchor-free FCOS.​One is about the classification sub-task in detection, i.e., the way to define positive and negative samples​Another one is about the regression sub-task, i.e., the regression starting from an anchor box or an anchor point.​​ "
[Object Tracking] SORT 코드 리뷰 ,https://blog.naver.com/ehdrndd/222606847867,20211228,"Sort(SIMPLE ONLINE AND REALTIME TRACKING, 2017, github)-> DeepSort(Simple Online and Realtime Tracking with a Deep Association Metric, 2017, github)​​SORTkeword- 물체 속도예측=>Kalman Filter- 박스와 tracker 연관성 매치=>Hungarian algorithm- box 충돌현상(occlusion) 무시(오버헤드,복잡성 증가)- id switch 문제 해결 불가 sort의 성능. MOTA가 online 방법론 중에 제일 높다 물체의 state를 다음과 같이 표기한다. u, v : 물체 box의 중심 좌표, 픽셀값 (x,y)s, r : 넓이, 가로세로 비율(=w/h)만약 해당 물체의 bounding box가 발견되면, velocity가 kalman filter로 계산이 되지만, 발견되지 않으면, liner velocity model로 state가 업데이트 된다. 자세한건 논문리뷰 참고​먼저 첫번째 프레임 이미지의 object detection 결과가 주어졌다고 하자.(필자는 yolor 사용)각 행들이 box의 정보를 나타내며[x1, y1, x2, y2, confidence(or score)]를 의미한다. 위 detection값이 Sort 클래스의 update 함수의 dets로 들어가게 된다. 먼저 class 변수들을 보면, 3가지 중요한 값이 있는데1. max_age : 관련된 detection이 없어도 과거 몇 프레임까지 detection 데이터를 기억할지(default : 1)(Maximum number of frames to keep alive a track without associated detections)​2. min_hits : 추적하기 전 해당 물체의 최소 몇개의 관련된 detection(box)이 있어야 시작할지.(default : 3)(Minimum number of associated detections before track is initialised)​3. iou_threshold : 연관(associated)되었다고 보는 box의 최소 iou값(default : 0.3)​ 221 : 맨처음엔 trks가 self.trackers의 길이만큼 초기화 해준다.  이후 224 line 과 230 line의 반복문이 실행되야 하지만 첫 프레임의 경우 self.trackers의 내용이 현재 없으니 생략된다.​232 line의 associate_detections_to_trackers 함수를 보자.역할 : 이름 그대로 detection 결과를 tracker와 연관 시키는 함수로써, 향후 얻어진 detection 결과를 현재 tracker가 추적하는 물체와 관련이 있는지 없는지 판단하는 작업을 한다.​matched : 현재 tracker와 관련이 있는 detection 정보unmatched_dets : 현재 tracker와 관련이 없는 detection 정보unmatched_tracker : detection정보로 연관을 못지은 tracker들 처음엔 trackers가 비어있으므로, 161 라인에서 끝난다. matched, unmatched_dets, unmatched_trks총 물체가 7개가 감지가 되었지만, 이전 정보가 없기때문에 matched되는 것이 없고, unmatched에 8개가 전부 있음을 확인 할 수 있다.​다시 돌아와 현재 mathed는 비어있고, unmatched_dets이 line 240에서 객체 수만큼 동작하게 된다. 240 line에서 클래스 인자로 들어가는 dets[i,:]값 예시 KalmanBoxTracker 클래스를 보자. 104 line의 칼만필터를 쓰는데 dim_x는 예측할 state의 차원수 7이고, z는 4차원. 즉, x중 접근 가능한 z는 4차원이라는 뜻. (`u,`v,`s)는 접근 안됨.즉, x의 u,v,s,r로 다음 x를 예측한다는 것.​kf.F : state transition matrix.kf.H : measurement function 이라고 한다..(값의 의미는 잘 모르겠다)106 line에서 kf.R(measurement noise, 4*4)을 찍어보고, s,r에 대해 10을 곱해준다.(u,v보다 s,r에 더 큰 소음이 낀다는 의미인듯?)  kf.P(x의 covariance matrix)에서 아직 모르는(`u,v,s)에 대해서 uncertainty(1000)을 곱해준다. 110 line : 관찰 불가능한 u,v,s에 uncertainty 부여. 111line : 최종 kf.P ​kf.Q(Process noise) 초기값 -> 최종값(line 112) 다음 114 line으로 와서 convert_bbox_to_z 에서bbox(x1,y1,x2,y2)를 z(u,v,s,r)로 바꿔준다. (def convert_bbox_to_z) 이렇게 unmathed된 박스 개수(첫 프레임이기에 detection box 개수)만큼 KalmanBoxTracker 인스턴스가 self.trackers에 추가가 된다. 244 : trackers를 거꾸로 돌면서(이후 pop()때문에) box의 정보를 꺼낸다. x1,y1,x2,y2245 line의 조건문을 보면, 해당 트래커가 업데이트된지 1 미만(즉, 방금 생긴 tracker)이고, hit_streak(연속 hit)이 min_hits(3)보다 크거나, 아직 영상의 초반이라면, 246의 ret에 추가 된다. 249~250 : 만약 해당 tracker의 업데이트 된지가 max_age(1)보다 크다면, 해당 tracker는 pop된다.결과적으로252 line에서 아래를 return한다.(x1,y1,x2,y2,id) 두번째 frame이 들어왔을때를 보자. 먼저 221 line에서 첫번째 프레임 상황에서 self.tracker를 추가 했었기 때문에 trk가 (8,5)로 초기화 되어(221 line),이번엔 225 line이 동작한다. ​ 137 : 예측된 넓이[6]과 현재 넓이[2]를 더했을때 0보다 작으면, 예측된 넓이에 0을 곱한다.(오차 제거)139 : 칼만 필터로 이제 값들을 예측하는데 먼저 아래는 예측하기 전 칼만필터가 가지고 있는 값이다. kf.predict()전 kf값 kf.predict() 후 값. 바뀐거 P(x의 covariance matrix) 여튼 144에 u,v,s,r -> x1,y1,x2,y2 체계로 바꾼후 history에 추가하고, 마지막 history 값을 리턴한다. 이 history들이 결국 trks가 된다. trks가 가지고 있는 값의 의미는 첫번째 frame의 box정보를 이용해 두번째 frame의 box 정보를 예측한 값이다.​이제 실제 두번째 frame 값인 detections(이번엔 9개의 box값)와, 예측값 trks(이전 8개의 box에서 예측한 8개의 box값)로 associtate_detections_to_trackers 함수가 동작한다. 즉,  tracker가 쫓고 있는 box와 실제 detection box간의 연관성을 알아보기 위함인데, 이때 박스 쌍끼리 iou를 계산해서 연관성을 확인한다. (conf와 클래스 정보가 안쓰인다는 점!!) iou matrix를 보면, det box와 trk box들의 연관성을 확인할 수 있는데, 가령 [0,0]인 0.96을 보면, detections의 첫번째 box와 trackers의 box가 iou가 높으므로 연관성이 높다고 이야기 할 수 있다. ​ 166 : iou_matrix 값중에서 iou_threshold보다 높은 값들의 위치를 1로 하는 a 행렬을 만든다. 167 : a.sum(1)은 행의 원소들을 다 더한것, a.sum(0)은 열원소들을 다 더한것인데, max 값이 둘다 1이면, det와 trks간 딱 맞아 떨어졌다는 의미이므로, 그냥 바로 168 line처럼 매치시켜버리면 된다.  하지만, 현재 iou threshold가 넘는 경우가 두가지 이상의 경쟁적 요소가 생기기 때문에 170 line의 linear_assignment로 넘어간다. linear_assignment가 바로 논문에서 말한 헝가리안 알고리즘과 관련이 있다. 여기서 39 line을 거치면, 아래와 같은 결과가 나오는데, x,y의 의미는 행마다 값을 하나씩 뽑아서 다 더했을때 최소값이 나오는 (x,y)를 의미한다(linear_sum_assignment).  관련이슈즉, det의 x번 box와 trks의 y번 box가 가장 연관성있다 라고 보는 것. scipy로 했을때의 x,y코드를 보면, lap 패키지가 없을 경우 scipy.optimize를 사용하기 때문에, 두 리턴값이 같아야 한다.하지만 아래에서 [6,4]랑 [7,4]가 다른것을 볼수 있는데, 이는 cost_matrix의 7,8행이 전부 0이기 때문이다.​ 좌 : lap. 우 : scipy ​다시 associate_detections_to_tracker 함수로 돌아와 detection box는 9개인데 matched 된게 8개 밖에 없으니 하나(6)는 unmatched_detections로 들어간다. tracker도 마찬가지로 unmatched_trackers에 담기는데 tracker는 8개 모두 사용되었으므로 비어있다.(생략)​이제 cost_matrix의 matched_indices를 살펴보며, iou_threshold 미만이면, unmatched_~로 추가한다. 그래서 최종 매치된 det,trk쌍(matches)와 det중 unmatch된것(unmatched_detections)와 tracker중 unmatch된것(unmatched_trackers)를 return한다.​아래 결과를 요약하면, matched_indices의 [7,4]위치의 cost_matrix값은 0이므로, 해당 det,trks가 추가로 제외된 것이다. ​다시 거슬러 Sort 클래스의 update 함수로 올라가면, matched 된 tracker를 update 해주는데, 업데이트 이후 지난 시간을 0으로 다시 초기화 하고, box의값(x1,y1,x2,y2)를 (u,v,s,r)체계로 바꾸어  kalman filter 내부 값들을 업데이트 해준다.(해당 과정은 수식적이라 생략..) 그리고 다시 Sort 클래스의 update 함수로 가서, unmatced_dets에 대해 새로운 tracker를 만들고, tracker에 추가한다. (이 과정에서 이제까지 만들어진 KalmanBoxTracker instance 수에 따라 id가 매겨진다)​ 244 trackers에 대해 상태를 가져와(state kf.x값을 bbox 형태로 가져온것) 245 line의 조건을 만족하면, 살아남고, 246 line처럼 id가 하나 증가되며 ret에 추가가 된다. 만약 조건을 만족하지 못하면, 해당 tracker는 파기(pop)된다.(250 line, trackers를 거꾸로 도는 이유) 최종적으로 ret를 보자 x1,y1,x2,y2, id​여기까지 Sort에 대한 설명 끝! "
[object detecion] mAP(mean average precision) ,https://blog.naver.com/929ok/222914108635,20221029,"mAPmAP는 CNN 모델의 성능을 평가하는데 사용되는 지표입니다. mAP를 이해하기 위해서는 우선 precision, recall, AP(average precision)을 알아야 합니다. 객체 탐지(object detection)의 성능을 평가하기 위해서는 검출율과 정확도를 동시에 고려해야 합니다.​우선 아래의 표에 나와있는 용어를 알아야 합니다. IoU의 결과 값이 0.5 이상이면 옳은 검출(TP,제대로 검출)이라고 판단합니다. 만약 0.5 미만이면 잘못 검출(FP)되었다고 판단합니다. 물론 이 수치는 임의의 값으로 설정할 수 있습니다.IoU에 관한 내용은 추후에 정리하겠습니다.​PrecisionPrecision(정확도)은 검출 결과들 중 옳게 검출한 비율을 의미합니다.예를 들어 알고리즘이 자동차 10개를 검출했는데 그 중 4개를 옳게 검출한 것이라면 Precision은 0.4입니다. ​RecallRecall은 검출율을 의미합니다. 즉 실제 Positive(true)중에서 모델이 Positive(true)라고 예측한 것의 비율입니다.  쉽게 말해 자동차 10개를 검출해야 하는데 실제로 모델이 4개만 옳게 검출했다면 recall은 0.4가 됩니다.​일반적으로 정확도(precision)과 검출율(recall)은 서로 반비례의 관계를 가집니다. 정확도가 높으면 검출율이 낮아지고, 반대로 정확도가 낮아지면 검출율이 올라갑니다. 그렇기 때문의 정확도와 검출율의 변화를 확인해야 합니다. 대표적인 방법으로 precision-recall 그래프가 있습니다.​Precision-recall 곡선Precision-recall 곡선은 객체 탐지 알고리즘의 성능을 평가하는 방법 중 하나로 confidence 레벨에 대한 threshold(임계치) 값의 변화에 따라 precision과 recall 값들이 달라집니다.confidence는 검출한 것에 대해 알고리즘이 얼마나 정확하다고 생각하는지 알려주는 값이지만 confidence이 높다고 해서 무조건 정확한 것은 아닙니다. 그저 알고리즘이 학습한 것에 따른 정확도를 나타내는 것입니다.사용자는 보통 confidence 레벨에 대해 threshold 값을 부여해서 특정값 이상이 되어야 검출된 것으로 인정합니다. 예를 들어 threshold 값이 0.6이라면 confidence 레벨로 0.1, 0.4, 0.5을 갖는 검출은 무시한다는 뜻입니다.따라서 이 confidence 레벨에 대한 threshold 값의 변화에 따라 precision과 recall 값들도 달라질 것입니다.이것을 그래프로 나타낸 것이 Precision-recall곡선(PR곡선)입니다. 15개의 번호판이 존재하는 이미지에서 총 10개의 이미지가 검출되었다고 가정해봅시다. 10개 중 7개가 제대로 검출되었고, 3개는 잘못 검출되었습니다. 이때 Precision은 7/10 = 0.7이 되고,recall은 7/15 = 0.47이 됩니다. 이것은 confidence 레벨이 10%와 같이 아주 낮더라도 검출해낸 것은 모두 인정했을 때의 결과입니다.​ 이번에는 검출된 결과를 confidence 레벨에 따라 높은 순으로 정렬하고, confidence 레벨에 대한 threshold값을 90%로 했다면, 하나만 검출한 것으로 판단할 것입니다.그렇게 되면 Precision은 1/1 = 1, recall = 1/15 = 0.067이 됩니다. threshold값을 검출들의 confidence 레벨에 맞춰 낮춰가면 다음과 같이 precision과 recall값이 계산될 것입니다.  이렇게 계산된 Precision과 Recall값들을 아래의 그래프로 표현한 것이 PR곡선입니다. 즉, PR곡선에서는 Recall 값의 변화에 따른 precision값을 확인할 수 있습니다. ​Average Precision(AP)PR곡선은 알고리즘의 성능을 전반적으로 파악하기에는 좋으나 서로 다른 두 알고리즘의 성능을 정량적으로 비교하기에는 불편합니다. 그래서 나온 개념이 Average precision입니다. Average precision은 객체 알고리즘의 성능을 하나의 값으로 표현한 것으로 PR그래프에서 그래프 선 아래쪽의 면적으로 계산됩니다. Average precision이 높으면 높을수록 그 알고리즘의 성능이 전체적으로 우수하다는 의미입니다.컴퓨터 비전 분야에서 객체인식 알고리즘의 성능은 대부분 AP로 평가합니다.​ 보통 계산 전에 PR그래프를 단조감소 그래프로 바꿔주고 면적을 계산합니다.위의 그래프의 경우 AP = 왼쪽 큰 사각형의 넓이 + 중간 작은 사각형의 넓이 + 오른쪽 작은 사각형의 넓이 = 1 * 0.33 + 0.86 * (0.4-0.33) + 0.77 * (0.46-0.4) = 0.4364가 됩니다.​컴퓨터 비전 분야에서 객체 및 이미지 분류 알고리즘의 성능은 대부분 AP로 평가합니다. 객체 클래스가 여러 개인 경우 각 클래스당 AP를 구한 다음에 평균을 내는데 이것을 바로 mAP(mean average precision)라고 합니다. "
CS231n Lecture 11 | Detection and Segmentation ,https://blog.naver.com/holeman0406/222182005838,20201222,"https://youtu.be/nDPWywWRIRocs231n 11강 강의를 듣고 정리합니다.개인적으로 정리하고 제멋대로 정리하기 때문에 (제가 이해하는대로) 참고하시길!  세분화, 현지화??, 감지11강에서 다룰 내용들이다. 이름 철자만 봐도 뭔가 어려워 보인다.젠장요즘 핫한 분야라고한다? 이미지 내부의 공간 픽셀에 대해~​ Segmentationsegmentation의 종류이다. 뒤에서 자세한걸 다룰테니 이 정도만 알고 가자.​ 1.Semantic Segmentation ·Label each pixel in the image with a category label·Don't differentiate instances, only care about pixels정리하면 사례?를 구분하지 않고 각 이미지 픽셀에 라벨을 한다? 라고 해석했고개인적으로 이해한것은 픽셀에 대한 분류, input image의 각 픽셀에 대한 카테고리 분류를 한다. 정도로 이해했다.  사진에 나온것처럼 하늘,나무,풀,고양이로 분류를 하는데 분류하는 기준이 픽센단위라고 생각을 하면된다. 우측 그림을 보면 소를 인식하는 사진이 있는데 문제점이 여기서 있다 소는 2마리인데 이걸 1마리로 인식을 해버리는 문제가 생긴다. idea_1) : Sliding WindowSliding Window 라는것이 있는데 위 사진처럼(소대가리부분) input image를 작은 픽셀로 잘라내어 각각의 잘린(강의에서는 crop이라고 표현)사진의 중심 픽셀을 확인하여 분류하는 것이다. 사진을 보면(빨강-파랑-보라 순) 중심을 가르키는 부분이 소, 소, 풀 이런식으로 인식을 한다.하지만 이미지의 모든 픽셀에 별도로 픽셀을 잘라 실행을 하기 때문에 계산양이 많아져서 비용이 비싸다! terrible idea라고 언급할 정도로 비효율적이다.​idea_2) : Fully Convolutional input image의 모든 픽셀에 라벨을 지정하고 FC CONV을 이용해서 높은 해상도를 만들어 segmentation하지만 이 또한 연속적으로 high resolution을 처리하기는 계산 비용이 비싸다고메모리 잡아먹는 괴물해서사용하지 않는다고 한다.한계점으로 1.이미지 위치 정보가 바뀐다 2.입력 이미지 크기가 고정된다​하지만 항상 인간은 해결법을 찾아내죠 바로 아래! ·Design network as a bunch of convolutional layers, with downsampling and upsampling inside the network​여기서 잠깐 난 전공자가 아니니깐 down&up sampling에 대해 좀 알아보았다.·Downsampling(인코더)차원을 줄여 적은 메모리로 깊은 CONV을 가능하게 한다.강의에서는 stride&pooling을 사용하고 이 경우 feature값을 잃는다.FCL을 사용하지 않고 FCN을 사용한다.·Upsampling(디코더)downsampling해서 얻은 결과를 차원을 늘려서 input과 같은 차원으로 만든다.주로 stride transpose conv을 사용한다고 한다.하지만 결과가 정확하지 않는다는 단점이 있다고 함.대충 알아보았고~downsampling에서는 Maxpooling이나 stride CONV을 사용하여 downsampling한다고 한다. downsampling하는 이유는 효율적으로 계산이 가능한 네트워크를 깊게 만들 수 있다고 한다.​다음으로 Upsampling에 대해 알아보는데 알아보기 전에 pooling에 대한 개념을 좀 알고가보자·Upsampling Unpoolingaverage pooling의 방법을 언급하는거같은데 맞는지 모르겠다...ㅜ·Nearest Neighbor그림을 보면 input 2x2 grid와 output을 확인 할 수 있다. 우리는 이미 conv을 공부 할 때 maxpooling을 배웠죠? unpooling할 때 저런식으로 평균값?을 설정해 unpooling한다고 한다. unpooling할 때 주변을 같은 값으로 채운다?​·Bed of Nails(평평한 손톱침대?에  뾰족하게 튀어나옴(값이 튀어나온) 작명 센스 ㄹㅇ...)input 2x2 grid의 요소를 제외하고 모두 0으로 만듭니다. 이 방법에는 규칙이 있는데 왼쪽 상단 모서리에 unpooling한다고 한다.Max-Unpooling 는 많은 네트워크에서 대칭?적인 경향이 있다고 한다. ​·Max-Unpooling Max Unpooling하 그림으로는 이해가 되는데 말로 풀어 적기가 어렵다.downsampling 할 때 Max pooling해서 output이 나오죠 그림처럼? 그 다음 위치 정보를 기억합니다. 그리고 앞에서 본 bed of nails와 비슷한 방법으로 output을 출력하는데 이 때 주의할 점은 앞서 위치 정보를 기억한 곳에 값을 입력하고 나머지는 0으로 채웁니다. Max pooling이 발생한 지점에 상응하는 곳에 강제로 max valur값을 넣어 이미지 사이즈를 키워주는 방법이라고 이해했다.​여기서 학생이 질문을 한다 이게 왜 좋은 아이디어이고 왜 중요하냐?(내마음...)semantic segmentation할 때 우리는 예측이 완벽한 픽셀을 원하는데 그러기 위해서는 정확도가 높은 경계값?이 필요합니다. 세분화된 예측이 필요한것이죠. 만약 max pooling을 하면 이질성?이 나타나는데 max pooling 후 local field에서  공간정보를 잃어버리고 등 특징적인 벡터가 어디서 왔는지 모르기 때문에 실제로 벡터를 입력하여 실행을 하면 도움이 되지 않을까? 라는 생각이고 이렇게 시도를 하여 손실된 공간 정보 일부를 보존하기 위해 중요하다고 설명한다. (내 해석이 맞는지 모름..)​하지만 강제로 값을 입력하는 방법이라 대부분의 정보가 많이 사라지고 유실된다.또 다른 방법으로 나온 것이 전치컨볼루션?아니 근데 transpose conv 시작하는데 위에 배운 3가지 아이디어는 실제로 자세히는 안배운다고 하네 그냥 이런것이 있다고 언급만 하네.....·Transpose Convolution 정상적인 상태의 conv3x3, stride 1, pad 1 즉, (4-3+2x1)/1+1 = 4가 되죠 일반적인 conv인 경우 이런식으로 말이죠~ 복습?이제 strided convolution도 비슷하다! stride = 2 / strided convolutionstride = 2 인것을 잘 보고 넘어가자 그러면 그림과 같이 1칸씩 옆으로 움직이는게 아니라 2칸씩 이동으로 output이 2x2가 나오게 됩니다. stride = 2 로 strided convoultion하면 image downsampling을 하게 됩니다. input = 5x5 / kernel=3x3, stride=2, padding=0 / downsampling [ 예시]반대로 transpose convolution은(upsampling!) 하...지금 3시간동안  이부분만 파네 아 현타 야무지네 온다 si..여기서는 내적을 하지 않고 input값의 1개를 지정한다 = sclar, 그리고 filter와 곱한다. 그리고 그 출력 값을 3x3 영역에 입력한다. input과 filter의 내적이 아닌 input이 filter에 곱해지는 가중치 역할이다. 그리고 stirde가 2니깐 저렇게 이동한다. 출력 = 필터 x input(가중치)위 사진 처럼 겹치는 부분은 그냥 더해준다.그리고 다른 이름으로 Deconvolution, 뭐시기 등등이 있다고 한다. 하지만 교수님은 deconvolution은 틀렸다고 한다. 찾아보니 downsampling하는 방식은 비슷하지만 upsampling에서는 완전히 다른 수학적 관계식으로 진행한다고 한다.기존 행렬을 inverse 반전하여 뭐 등등... 자세한건 아래 링크에 있다. What are deconvolutional layers?I recently read Fully Convolutional Networks for Semantic Segmentation by Jonathan Long, Evan Shelhamer, Trevor Darrell. I don't understand what ""deconvolutional layers"" do / how they work. The re...datascience.stackexchange.com 1차원 시각화1차원 시각화를 한것이다(flatten?) 그림만 봐도 이해가 간다. az + bx를 한번 보고 가고 이런 방식으로 input이 output으로 mapping되가며 크기가 증가하게 됩니다. stirde =1 왼쪽은 일반적인 CONV 행렬 곱셈(내적?)으로 1차원 벡터로 표현한 것입니다. 우측 그림은 Transpose CONV 행렬 내적입니다. 커널 사이에 0을 추가합니다.저는 Transpose CONV은 처음 x matrix를 반전(inverse)시켜 곱했다고 생각했습니다. CONV=many to one / Transpose = one to many Up-sampling with Transposed ConvolutionIf you’ve heard about the transposed convolution and got confused what it actually means, this article is written for you.naokishibuya.medium.com stride = 2stide 가 2인 CONV transpose matirx는 근본적으로 다르다고 합니다. normal CONV은 데이터를 압축시키는 반면에  transpose conv은 matrix를 반전시킨 커널을 input a,b 행렬과 곱하여 데이터 사이즈를 키우게 되는 방식이라고 저는 이해했습니다. 이러한 메커니즘이 semantic segmentation 이라고 이해했고   이 모든 계산은 cross entropy loss backpropagation을 사용하여 모든 픽셀에 대해 훈련해볼 수 있다고 합니다! 2.Classification + Localization 목표 물체의 bounding box를 찾고 boxing한 내부의 물체를 분류하는 작업을 합니다.  물체를 구별하고 위치 정보까지 파악을 하고싶은 경우에 사용한다고 생각을 했다. 그래서 위 그림을 보면 input image가 모델에 들어가고(alexnet) FCL가 2개가 나오는데 class scores는 분류 box coordinates는 위치정보 이렇게 2개가 나옵니다. 이런 경우에는 2가지 loss fucntion이 생깁니다. 추가적으로 ground truth category라는걸 언급하는데  그림처럼 초록색 박스가 표폰(정답?)이고 빨강 박스가 예측한 박스인데 두 박스가 겹칠수록 더 좋은 결과와 성능을 가진 모델이라고 판단한다고  이런식으로 말이죠~또 추가적으로  Human Pose Estimation 은 input image에서 사람의 관절을 찾는데 14개의 값에 대해 regression loss 계산하고 역전파로 학습합니다. L2 loss를 사용한다고 하네요. 여기서 학생이 Regression loss가 뭐냐고 물어보는데 마침 나도 모르니 듣고가자Regression Loss란 cross entropy 나 softmax가 아닌 loss를 뜻한다. 출력이 범주형인지 연속형인지 여부에 따라 loss를 선택한다고 한다. 분류를 하고싶으면 cross entropy, softmax, svm loss들을 생각해 볼 수 있지만  output이 연속적인 값이라면 regression loss를 사용한다고 합니다.​ 3.Object Detection 정말 대단하고 컴퓨터 비전의 핵심문제라고 떡밥을 깔고 들어간다 ㅜㅜ중요한 부분이지만 간단히 설명하고 지나간다고 한다. 일반적이고 대대손손 사용된 한가지 접근 방식은 Sliding Window입니다.파랑색 박스가 지나가면서 배경이니? 강아지니? 고양이니? 이런식으로 분류하는 방식입니다. 여기서는 배경이라는 카테고리를 추가했다고 합니다. 개나 고양이가 아니므로 해당 부분은 background이죠.  이런식으로 말이죠 하지만 여기서 문제가 있는데 crop을 어떻게 정하고 object가 몇개인지 모르고  또한 크기 또한 모르는 문제가 생깁니다. 이를 해결하기 위해서  Region Propasals라는 방법인데요. 딥러닝을 사용하지 않는 방법이라고 합니다.전통적인 컴퓨터 비전이고 input image를 입력하면 region proposals는 object가 있을만한 목표 후보를 1000개를 찾아주는 방법으로 전 이해를 했습니다. 후보 이미지와 잠재적으로 발견 될 수 있는 이미지를 모두 찾아주는 방법이고 상대적으로 속도가 빠르다고 합니다. 예로 selective search라는 것이 있고 cpu작동 후 약 2초 후 물체가 발견 될 가능성이 있는 2000개의 후보를 알려줍니다. 노이즈가 있지만 recall이 높다고 한다.   강의를 2번 들으니 이게 1000개인지 2000개인지 헷갈리네요..  앞으로 이 3가지에 대해 알아볼 것이다. 모두 Region Propasals방법을 사용한다.​ R-CNN이다. input image에서 region propasals를 추출하는데 이 때 추출된 이미지 크기가 다를 수 있어서 CONV을 통해 분류를 하려면 동일한 size가 필요하다.고정된 정사각형 모형으로 Warped image regions 처리하고 CONV를 통해 SVM을 사용하여 분류를 합니다. R-CNN은 회귀예측을 한다고한다. 하지만 문제점은 계산량이 너무 많고 느려서 적합하지 않다고 한다. 그래서 나온것이 Fast R-CNN이다. Fast R-CNN은 input image에 처음부터 CONV를 이용하여 고해상도 feature map얻는 다음 여기서 ROI를 구하는 방법이다. feature map에서 나온 ROI를 공유하는 장점이 있다고 합니다. fc-layer를 통해  ROI크기를 조정한다는데 이 과정을 ROI pooling 이라고 합니다. 무튼 이러한 방법으로 R-CNN보다 더 빠르게 처리 가능하다고 한다. R-CNN vs Fast R-CNN비교하면 시간적인 면에서는 확실히 10배이상 Fast R-CNN이 빠른거 확인 할 수 있습니다. 하지만 너무 빠른 속도로 인해 결국 병목현상?이 발생한다고 한다.Fast R-CNN은 ROI를 계산하는 구간이 병목현상이 일어난다고 한다. 그래서 Faster R-CNN으로 해결했다고 한다.ROI를 Region Proposal Network(RPN)라는 것을 이용한다고 한다.먼저 보면 input image에 CNN을 거쳐 high resolution feature map을 구하고 feature map에서 ROI를 구하는 방법에서 따로 RPN 네트워크에서 feature map 데이터를 가지고 ROI를 계산하는 방법입니다. 그 이후의 계산은 Fast R-CNN과 같습니다. RPN:갈아먹는 Object Detection [4] Faster R-CNN (tistory.com) 보이나요 이 빠름이? 병목 현상이 제거되어 Faster R-CNN이 좋은 결과를 확인 할 수 있습니다. 반대로 ROI를 사용하지 않는 방법도 있습니다. YOLO와 SSD입니다.욜로~저희가 아는 그 욜로 아니구요,...YOLO(You Only Look Once) / SSD(Single-Shot MultiBox Detector)이 방법은 forward 한번이면 object detection이 가능하다고 합니다.아이디어는 input image에 격자로 나누고 강의에서는 7x7 grid 각 격자에서 경계상자를 그립니다. 기본적으로 3개이상을 사용한다고 한다. object가 어느 격자에 있는지 학습한다.  output의 크기는 이렇다.7*7*(5*B+C)7*7=grid, 5=box feature B=다른 종류의 bounding boxes, C=scoressingle shot method 라고 하며 Faster R-CNN에 나오는 RPN?과 비슷한 방식이라고 언급을 하는데 faster r-cnn에서는 RPN을 하고 regression해결하고 ROI를 처리하는 반면에 YOLO/SSD는 forward pass 한번으로 끝낸다는 장점?이있다.​여기서 하나 짚고 넘어 갈것은 Faster R-CNN의 ROI는 더 높은 정확도를 제공하지만 single shot 보다는 훨씬 느리다고 합니다. ​ 4.Instance Segmentation 마지막입니다 드디어ㅜsemantic segmentation 과 object detection 2개를 합친?거라고 생각을 했습니다.  image에 2마리의 강아지가 있으면 각각의 강아지를 구별하는 것입니다. Mask R-CNN이라는게 있는데요. input image 가 CNN 과 RPN을 거쳐 ROI를 추출하고 그 이후는 faster R-CNN과 같습니다. 이는 합동으로 훈련할 수 있는 모델이며 잘 작동됩니다.Mask R-CNN은 지금까지 배운걸 모두 통합?이 가능하다는 식으로 강의에서 언급하는것 같다.(내 해석이 맞다면?)  추가적으로 관절을 예측도 추가 할 수 있으며 매우 유용하게 사용이 가능해보인다.  Faster R-CNN을 기반으로 구축되었기 때문에 매우 빠른 계산속도로 처리가 가능하다고 합니다. 거의 실시간! 하지만 GPU에서...​이상입니다. 끝 "
YOLOv8 커스텀 데이터 학습하기 ,https://blog.naver.com/beyondlegend/223050797442,20230321,"​오늘은 커스텀 데이터, 즉 나만의 데이터를 이용해서, YOLOv8 을 학습하는 방법을 알아보도록 하겠습니다===========================================================================================​ ​YOLOv8은 Object Detection과 함께 이미지나 동영상의 Image Segmentation 또한 동일한 API를 이용하여 구현할 수 있습니다. ​즉 YOLOv8 은 사전학습 모델종류와 데이터 위치를 나타내는 yaml 파일만 바꾸어 주면, 동일한 Python API 로 Object Detection 과 Image Segmentation 구현이 가능하다는 것을 알수 있습니다 =========================================================================================== ​커스텀 데이터를 이용해서 YOLO 를 학습하는 프로세스를 알아보면,​먼저 커스텀 데이터로 YOLOv8 모델을 학습하는 경우에는 자료에 나온것처럼 이미지와 정답으로 이루어진 데이터를 준비해야 하는데,  이러한 Custom Data는 Roboflow에서  제공하는 Custom Data를 이용하거나 본인이 직접 labelling 시킨 Custom Data 로 구축해야합니다.​그럼 이제 커스텀 데이터가 준비되었으면, wget 또는 curl 등의 명령어로 Roboflow에서 제공하는 Dateset을 Colab으로 다운로드 한후에,  YAML 파일을 만들어야 하는데, 이러한 YAML 파일은 YOLOv8 으로 Custom Data를 학습하기 위해서는 반드시 필요한 파일입니다. ​이렇게  커스텀 데이터를 준비하고 YAML 파일까지 만들었다면, yolov8을 install 시키고 커스텀 데이터에 맞게 모델을 학습하고 예측하면 우리의 커스텀 데이터에 최적화된 모델을 완성 할수 있습니다. ===========================================================================================​ ​​ ​​ ​YOLOv8 학습하기 위한 커스텀 데이터는 어떤 것을 사용해도 되지만, Object Detection을 위한 이미지 데이터와 정답 데이터가 함께 제공되는 Roboflow (public.roboflow.com) 에서  Aquarium Dataset을 다운받아 사용할 계획이고, ​Raw URL 항목에 나타난 이러한 URL을 드래그해서 복사한후에, Colab에서 wget 명령어 해당 URL 에서 데이터를 가져올 예정입니다===========================================================================================​ ​wget -O 옵션으로 우리가 복사해놓은 URL에서 데이터를 다운받아 Aquarium_Data.zip 으로 저장한다음,  압축 파일을 풀어보면,   이미지와 정답데이터가 있는 test, train, valid 디렉토리 경로, 그리고 클래스 개수와 이름등이 저장되어 있는 data.yaml 파일이 있습니다. ​그런데 이러한  yaml 파일은 실제 커스텀 데이터가 저장되어 있는 train, val 디렉토리 경로는 반드시 본인의 환경에 맞게 변경해줘야 합니다=========================================================================================== ​이제 커스텀 데이터에 맞는 YAML 파일 만들어야 하는데, 그러기 위해서는​먼저 pip install PyYAML 실행후에,  YOLOv8 학습과 검증에 사용되는 train, valid data 가 저장되어 있는 디렉토리 경로와  클래스 개수 그리고 클래스에 대응되는 클래스 이름을 딕셔너리 객체로 설저하고 코드에서처럼 Aquarium_Data.yaml 파일로 저장합니다===========================================================================================​ ​그럼 이젠 데이터와 YAML 파일이 모두 준비되었기 때문에.  pip install ultralytics 를 실행해서 YOLOv8와 필요한 라이브러리를 설치하고  MS COCO Dataset 사전학습된 yolo 나노 모델을 로드합니다=========================================================================================== ​그럼 이젠 train 함수를 이용해서 YOLOv8을 학습할수 있는데, 여기서 중요한 부분은 우리가 앞에서 만들어둔Aquarium_Data.yaml 파일을 반드시 train 함수의 data 파라미터로 지정해야 한다는 것을 알아 두시기 바랍니다===========================================================================================​ ​그럼 마지막으로 predict 함수를 이용해서, 주어진 테스트 디렉토리에 있는 모든 이미지, 즉63개의 이미지에 대해 object detection을 실행해 보면, 이미지에 있는 펭귄과 물고기, 해파리, 바다오리등의 종류와 위치를 정확하게 detection 했다는 것을 알수 있습니다 ​참고로 소스코드는 YOLOv8_Object_Detection_Roboflow_Aquarium_Data.ipynb 이며 제 GitHub 에서 확인 할 수 있습니다. "
논문리뷰 | Tracking People by Detection Using CNN Features  ,https://blog.naver.com/mqjinwon/221661304175,20190927,"논문은 https://www.sciencedirect.com/science/article/pii/S1877050917329113 여기서 다운 받으면 된다. Tracking People by Detection Using CNN FeaturesMultiple people tracking is an important task for surveillance. Recently, tracking by detection methods had emerged as immediate effect of deep learni…www.sciencedirect.com ​Abstract 다수의 사람을 tracking 하는 것은 감시를 위해 매우 중요한 업무이다. 최근에 detection 방법을 이용한 tracking이 object detection안에 딥러닝의 뛰어난 성과의 즉각적인 효과로서 나타나고 있는 추세이다. 이 논문에서는, detection을 위해 ""Fast-RCNN""을 사용하고, 물체들의 연관관계를 보기 위해 두가지 방법을 사용할 것이다. 첫번째 방법은 ""Euclidean distance""를 사용하는 것이고, 두번째는 좀 더 복잡한 ""Siamese neural network""를 사용하는 것이다. 실험에 따르면 ""Euclidean distance""은 물체의 상관관계를 파악하는 방법으로써 좋은 결과를 가져오지만, 이것은 각각의 프레임의 detection 과정의 robustness(강도)에 심히 의존한다.​  1. introduction 다수의 물체를 트레킹하는 것은 어렵고 중요하기 때문에 흥미로운 연구주제이었다. 이것은 감시시스템이나, 비디오의 이해를 위한 목표가 주였다. 최근에 detection을 이용한 tracking이  object detection안에 딥러닝의 뛰어난 성과의 즉각적인 효과로서 나타나고 있는 추세이다. 다수의 사람들이나 행인을 tracking 하는 것또한 이로 인해 많은 이점을 얻었지만, 여전히 모든 문제가 해결된 것이 아니기 때문에, 어려운 업무이다. 이 도전들은 교차뿐아니라 pos. size, shape의 변화들을 포함한다. 다수의 사람을 추적하는 것에 대한 이전 연구들은 CNN의 features를 사용해서 좋은 결과들을 보여줬다.""Siamese CNN""을 사용한 Leal-Taixe, Ferrer and Schindler는 ""MOT Challenge dataset""에 대해서 선형 프로그래밍, ""graident boosting classifer""를 변형했다. Zhu, Porikli and Li는 ""PETS dataset""에서 분류기로써 ""Faster RCNN""과 ""SSVM""을 사용했다. ""Faster RCNN"" 특징들은 이전에서는 object detection에서 가장 유명했다. 이 건 과거 top-rank detector 분류기로 사용되었었다. 만약 우리가 object detection을 이용해 tracking을 한다면, 다양한 물체를 탐지하는데 성공해야하기 때문에 CNN features는 잘 수행되어야한다. 우리의 최근 연구는 감시 카메라들의 의미있는 정보를 찾는 것이다, 얼마나 많은 남자 여자들이 어느 방향에서 어떤 특성으로 들어오는지, 그리고 옷이나 가방 같은 것들을 들고 있는지와 같은. 이 목표를 성취하기 위해서, 우리는 다수의 사람과 그들의 특성들을 따라가야한다. 당분간은, 우리가 연구하는 특징은 성별이다. 우리는 ""Fast-RCNN""이 장면안의 남자와 여자를 인지하도록 학습시켜왔지만, 아직 개발해야할 부분이 많다. 많은 경우에 사람은 어떤 프레임에서는 남자로, 다른 프레임에서는 여자로 탐지될 때가 있다. 우리는 만약 우리가 사람의 궤적 정보를 사용한다면 전체적인 장면에서 사람의 성별을 더 잘 분류할 수 있다고 추측했다. 우리가 ""Faster-RCNN""의 features를 성을 구분하기 위해 사용했기 때문에, 우리는 우리가 이 특징들을 tracking에 재사용해도 되는지 궁금했다. 우리는 연속적인 비디오에서 같은 사람은 연관시켜야했기 때문에, task는 개체 연결 테스크가 된다. 이 논문에서, 우리는 물체 연결을 위한 두가지 방법을 비교한다. 첫번째 방법은 ""Euclidean distance""이고 두번째는 좀 더 복잡한 ""Siamese neural network""이다. 우리는 이미 gender가 알려져 있기 때문에 우리의 데이터셋을 사용한다.   2. related works 2.1 faster RCNN  - 추후 RCNN 정리 예정​ 2.2 Siamese Neural Network    - ""Siamese Neural Network""(SNN)은  Parameter W의 같은 짝을 공유하는 twin 함수 Gw와, 거리 Dw=||Gw(x1)-Gw(x1)||를 생성하는 cost module로 구성되어 있다. SNN의 입력은 이미지(x1, x2)와 label Y이다. label Y는 유사할 경우 0, 유사하지 않을 경우 1의 값을 가진다. loss function L은 Ls = (0.5)(Dw)2 혹은   LD = (0.5){max(0, m - Dw)}2를 만들기 위해 label Y와 Dw를 조합한다.​즉 loss function은 L(W,Y,x1,x2) = (1-Y)(0.5)(Dw)2 + (Y)(0.5){max(0, m - Dw)}2 이다.​매우 잘 정리해주는 강의가 있어 공유한다.https://www.edwith.org/deeplearningai4/lecture/34912/ [LECTURE] 샴 네트워크 (Siamese Network) : edwith학습목표 샴 네트워크를 배운다. 핵심키워드 유사도 (similarity) 샴 네트워크 (Siamese network) - 커넥트재단www.edwith.org ​ 2.3 Multiple Object Tracking (MOT) evaluation metric- 생략  3. method 이 논문에서, 우리는 연속적인 프레임의 각 사람에게 ""Euclidean distance""과 ""SNN"" 두가지 방법을 사용해서 트래킹의 결과를 비교했다. 각 사람은  4096개의 Faster-RCNN feature로 표현된다.  ""Euclidean distance""에 대해서, 우리는 input vector에 대해 단지 두 feature vector의 거리만을 계산하면 된다. 가장 거리가 짧은 한 쌍은 같은 사람으로 고려될 것이다. 이것은 ""Fast-RCNN""으로 생성된 같은 물체의 convolution features가 유사하지 않은 물체의 특징과 비교했을 때 꽤 유사해야한다는 추정때문이다. ""Euclidean distance""과 달리, SNN은 학습 시켜야한다. 우리는 SNN을 학습시키기 위해 23개 데이터를 사용했다. 4096개의 feature 벡터를 input으로써 넣으면서, SNN base network는 각 255의 node로 구성된 fully connected layer 세 층으로 구성되어있다. 우리는 모든 레이어에서 ""ReLu""를 활성화 함수로 사용한다.​이하생략 "
"23년 1학기 세종시캠공II: 6회""드론"" ",https://blog.naver.com/makerhanssem/223107579541,20230521,"5월 20일(토)에 세종시 캠퍼스형 공동교육과정 2 6회차 수업이 세종 ""해밀고등학교""에서 진행이 되었습니다.이번 주의 주제는 ""드론 조종 및 드론 코딩"" 이었습니다. 수업은 제가 진행했습니다.2가지 주제의 실습을 위해 코딩용으로는 텔로를, 조종연습용으로는 Z908 PRO 드론을 준비했습니다.1인 1대씩 실습을 하면 좋겠지만, 2인 1대로 실습을 할 수 있도록 수량을 맞춰서 작년보다는 상황이 좋았습니다.​학생들이 포트폴리오를 만드는데 도움이 되도록 첫번째 수업은 기본적인 드론 이론 수업을 진행했습니다.드론의 이름, 드론 관련 항공역학 및 관련 법칙, 드론의 기본조종법 등을 핵심적으로 학생들에게 설명을 했습니다.​양지고에서 수업을 진행할 때는 각 층마다 넓은 공간이 있었는데, 해밀고에서는 공간확보를 위해서 책상을 한쪽으로 밀고, 가운데 공간을 비웠습니다.조종법을 학생들에게 알려준 후에, Z980PRO 드론을 학생들에게 나눠주었습니다. 페어링의 경우, 전파간섭이 발생할 수 있으므로, 한 팀씩 순차적으로 페어링을 시도하도록 도왔습니다. ​학생들이 먼저 실내에서 기본적인 조종연습을 한 후에 복도에 나가서 날려보도록 했는데, 학생별로 차이가 있었지만, 상당히 재밌게 날리네요. 여분의 배터리를 준비했기 때문에, 필요한 학생들에게는 여분의 배터리를 제공했습니다. ​학생들 중에 야외에서 날려보고 싶어서 야외에서 날린 경우가 있었는데, 한 대는 작은 나무사이에 끼여서 모터 한 개가 고장이 났고, 한 대는 바람에 날려서 분실 했습니다.ㅜ.ㅜ이런 실습에서 생길 수 있는 일이라고 생각하고, 학생들이 즐겁게 실습했다는 OK 입니다.​세번째 시간에는 코딩을 위해 먼저 텔로 드론과 익숙해 지는 시간을 가졌습니다. 스마트폰 어플을 다운 받고, 와이파이로 텔로와 연결해서 날려보도록 했습니다. Z980PRO로 이미 연습을 했기에 학생들이 잘 날리고, 텔로가 좀 더 조종이 잘 되는 것 같습니다.​막간을 활용해서 텔로를 활용해서 인공지능(Object Detection)을 실행할 수 있는 tellome어플로 사람을 인식하고 따라오는 시연을 보여주었습니다.​학생들이 즐겁게 연습을 한 후에, 다시 모여서, ""드론블록스""를 설치하고, 드론코딩에 대해 간단히 설명하고, 예시를 보여주었습니다. 학생들이 잘 따라하고, 열정을 보여줍니다.​드론수업은 드론관리, 배터리 관리 등 번거로운 부분이 있어서 부담스럽긴 하지만, 의미가 있고, 활동적인 즐거운 체험수업이라고 생각합니다. UAM이 곧 상용화되는 시기에 드론에 대해 체험하고, 인사이트를 얻는 것은 무척 중요하다는 생각입니다.​원래는 다음주가 7회차로 마지막 수업인데, 휴무인 관계로 그 다음주에 7회차 수업이 진행됩니다.7회차에는 ""메타버스 개념 이해 및 체험과 3D프린팅 이해 및 체험"" 수업을 진행하려고 합니다.23년도 1학기 캠공도 좋은 마무리가 되길 기도해봅니다.^^ "
"#6: 실시간 사물 인식 앱 만들기 (CoCo-SSD, tensorflow.js, 인공지능, 머신러닝, real time object detection, 텐서플로우) ",https://blog.naver.com/junjun1971/222320395913,20210423,Tensorflow.js 의 CoCo-SSD 모델을 사용해서 실시간 사물 인식을 자바스크립트로 구현하는 실습 영상입니다.​https://youtu.be/eckOXXID7co ​ 
Object Detection 성능평가 ,https://blog.naver.com/dldbfl4550/221618080451,20190816,이론 설명​https://bskyvision.com/465 물체 검출 알고리즘 성능 평가방법 AP(Average Precision)의 이해물체 검출(object detection) 알고리즘의 성능은 precision-recall 곡선과 average precision(AP)로 평가하는 것이 대세다. 이에 대해서 이해하려고 한참을 구글링했지만 초보자가 이해하기에 적당한 문서는 찾기..bskyvision.com 코드https://github.com/Cartucho/mAP Cartucho/mAPmean Average Precision - This code evaluates the performance of your neural net for object recognition. - Cartucho/mAPgithub.com ​ 
[논문 정리] Saliency Detection: A Spectral Residual Approach ,https://blog.naver.com/dydgus_55/221751762119,20191228,"*본 게시글의 논문에 관련 된 모든 사진의 출처  : https://ieeexplore.ieee.org/document/4270292 Saliency Detection: A Spectral Residual Approach - IEEE Conference PublicationFile failed to load: https://ieeexplore.ieee.org/xploreAssets/MathJax-274/extensions/MathMenu.js Conferences > 2007 IEEE Conference on Compu... Saliency Detection: A Spectral Residual Approach Publisher: IEEE Cite This PDF 2 Author(s) Xiaodi Hou ; Liqing Zhang All Authors 1759 Paper Citations 5 Pate...ieeexplore.ieee.org 핵심 요약 :이미지는 비슷한 주파수 스펙트럼을 가지므로, 이 주파수 스펙트럼들을 평균내면 그 일반적인 경향을 알 수 있다. 따라서 이미지 주파수 스펙트럼에서 중요한 것은 이 일반적인 경향과 다른 특징을 보이는 부분이다.1) 공간 도메인의 이미지를 2D 푸리에 변환하여 주파수 영역으로 변환한다.2) 주파수 도메인에서 Prior knowledge part(예측 가능한 주파수 정보, A(f))는 빼준다.즉, Innovation part인 'spectral residual' ( 이 논문에서는 R(f)로 칭함) 만 남긴다.3) 푸리에 역변환를 이용해서 다시 공간 도메인으로 변환해서 Visual Sailent맵을 얻는다.​Introduction객체 인식을 위한 첫 과정은 object detection 이었으며 이것은 인식 이전에 객체를 추출하는 것 이었다. 그렇다면 Saliency map은 어떻게 추출 되는 가? 이 객체를 추출하는 과정은 미리 학습 되어 연관 된 어떤 특정 카테고리의 특징과의 연관을 통해, 즉 훈련을 통해 가능했으며 이것은 보틀넥을 유발했다. 그러나 수 많은 카테고리의 시각 패턴이 생김에 따라 일반적인 Salienct detection system이 필요해 졌다.어떻게 Saliency detection이 휴먼의 시각 시스템에서 역할을 하는가?pre-attentive process1단계 : pre-attentive process : 빠르고 심플하고 병렬 적으로 후보자들을 탐지 (proto object)2단계 : attention process : 느리고, 직렬, 복잡한 과정proto object를 찾기 위해서, 머신 비전이 개발 되었고. 또 Itti and Koch, 그리고 더 최근에는 Walther가 Saliency detection을 제안했다.그러나 이 모델들은 전처리에 쓰이기에는 계산량이 너무 많다.대부분의 모델은 객체의 속성을 요약하는데 주목한다. 그러나 수많은 카테고리의 객체가 공유하는 특성은 존재하지 않을 것이다.따라서 우리는 백그라운드의 속성을 추적한다.Spectral Residual Model (스펙트럼 잔차/잔여 모델)*hypothesis : 가설*suppress : 막다* novelty : 진기함, 새로운효율적인 coding은 우리의 시각 시스템으로 해석될 수 있다.Barlow[1]는 효율적인 코딩에 대한 가설을 제안 했다.이 가설은 sensory input에서 redundancies(여분/중복)를 제거하는 것이다.정보 이론에 따르면 효율적인 코딩은 이미지를 두 파트로 분해한다. 사진 설명을 입력하세요.innovation denote novelty part, 그리고 후자는 redundant 정보이며 이것은 coding system에 의해 suppress 돼야한다. 이 파트는 이미지 통계학적 관점에서는 우리의 환경에서 불변하는 속성과 일치한다.다음으로 우리는 redundant componets를 제거함으로써 innovation part를 근사하는 알고리즘에 대해 소개한다.2.1 Log spectrum representation*참고용 사전 지식푸리에 변환(Fourier transform:을 직관적으로 설명하면 푸리에 변환은 임의의 입력 신호를 다양한 주파수를 갖는 주기함수들의 합으로 분해하여 표현하는 것이다.푸리에 변환(Fourier transform)을 영상처리에 적용하기 위해서는 이미지(영상신호)가 가지고 있는 몇 가지 차이점을 인지해야 한다. 먼저, 이미지는 2차원의(x축 방향의 변화와 y축 방향의 변화가 동시에 포함된) 신호이기 때문에 2차원에서 정의되는 푸리에 변환이 필요하다. 2차원 신호에 대한 푸리에 변환(Fourier transform)은 다음과 같이 정의된다.푸리에 스펙트럼: 푸리에 변환을 통해 얻어지는 복소수의 크기 IF(u,v)I,즉, 해당 주파수 성분이 신호(이미지)에 얼마나 크게 들어 있냐에 대한 것스펙트럼이 시각적으로 잘 안보여서 log 취해서 많이 쓴다. ​출처: <https://darkpgmr.tistory.com/171>저주파일 수록 계수가 크다!scale 불변성이 가장 많이 연구돼는 유명한 특징이다. 이 속성은 1/f 법칙으로도 알려져있다.우리는 f-log scale을 사용하며 이는 아래 Fig1과 같다. ​2.2 From spectral residual to saliency map이미지에서 중복되는 부분을 제거하기 위해서는 여러개의 통계적 유사성에 대해서 알고 있어야 한다.스펙트럼에서 중복되는 부분이 제거된 뾰족한 부분들에서 프로토 객체가 팝업 될 것이다. ​A(f)는 general shape of log spectra (prior information)평균 커브가 local 선형성으로 보이므로, local average filter인 hn(f)를 통해 A(f)를 근사하는 것이 합리적이다.=> n스펙트럼을 평균 스무딩(박스 필터링, 평균 필터링) 함.그리고 L(f)에서 A(f)를 빼면 R(f)가 구해진다. ​R(f)를 역 푸리에 변환 해서 Sailency map을 얻는다. Sailency map의 각 점은 추정오차를 나타내기 위해서 제곱 된다. 더 나은 시각효과를 위해서 시그마 8 인 가우시안 필터로 Sailency map을 스무딩 한다. ​주의 맨 마지막 식은 exp R(f) * exp P(f) 임크기(mag)만 새롭게(residue로) 가져오고 위상은 그대로 가져온 다음에 역 변환 하는 것 인 듯.github에 소스코드 2개 있음.DFT에 대한 설명 있는 사이트https://docs.opencv.org/2.4/modules/core/doc/operations_on_arrays.html#idftDetecting proto-object in a saliency mapSaliency map은 명확한 proto-object이다. 우리는 Saliency map에서Object map(이진맵)인 O(x)를 단순한 Threshold segmentation을 이용해서 구한다. ​경험적으로 Threshold를 E(S(x))*3으로 설정한다. (S(x)의 평균)물론 이문제는 Trade off를 안고 있지만 추후 4.1에서 논의하겠다.O(x)가 만들어지면서 proto-object는 동등한 좌표의 원본 이미지에서 쉽게 추출될 수 있다.3.1 Selection of visual scalesSelection of visual scales = input image의 사이즈 조절비쥬얼 스케일에 따라 큰 집이 객체로 인식 되거나, 문이 객체로 인식 되거나..같은 이미지를 축소/확대 했을 때이미지 사이즈가 크면 : detail한 것에 더욱 집중하게 된다.이미지 사이즈가 작으면 : 더욱 커다란 것에 집중하게 된다.Fig에 나와 있듯, scale에 따라 Saliency map이 다르다.실험에 따르면 64 px of the input image width (or height) 가 좋더라.Experiments and Analysis4명의 실험자들을 사용해서 hand label 시키고 k번째에 의해 된 것 객체 맵을 Ok(x)select regions where objects are presented”라고 지시함그리고 아래와 같이 잘 된 값을 나타내는 지표와, 잘못된 지표를 나타내는 두 값을 계산한다. ​곱한 값이 높다는 것은 유사성이 높다는 것 이므로, 위와 같은 평가 지표를 통해 성능을 평가하는 듯 하다.코드 ​코드 분석 ​결과[원본 이미지 및 결과 이미지] ​ "
[OpenCV⭐] Object Tracking / Background Subtraction(Foreground Detection) ,https://blog.naver.com/hiuejiwon/222395637108,20210612,"객체 추적한번 지정한 객체의 위치를 연속된 영상 프레임에서 지속적으로 찾아내는 것​머신러닝 기법으로도 매 장면에서 원하는 객체를 찾아낼 수 있음그런데 굳이 추적이 필요한 이유는?​1) 객체 검출이나 인식은 많은 데이터를 필요로 하고 속도가 느리므로 상대적으로 빠르고 단순한 방법이 필요​2) 객체 검출이나 인식에 실패했을 때 좋은 대안이 될 수 있음​3) 시간의 흐름에 따라 객체를 식별하는 것이 가능(매 장면을 하나의 이미지로 분석하는 객체 인식 기밥과 달리, 장면과 장면의 흐름에 기반하는 객체 추적 기법은이전 장면의 객체와 다음 장면의 객체가 서로 같고 다르다는 것을 알 수 있다)  배경 제거객체의 움직임을 판단하기 가장 쉬운 방법은 배경만 있는 영상에서 객체가 있는 영상을 빼는 것이다 import numpy as npimport cv2 print(cap.get(3)) # 가로print(cap.get(4)) # 세로print(cap.get(5)) # 프레임레이트 OpenCV에서 제공되는 createBackgroundSubtractorMOG1, MOG2 추상 클래스를 사용함(MOG: Mixture Of Gaussian)​아래에는 배경 추출 코드+매 프레임을 읽어서 동영상 파일로 저장하는 코드가 포함되어있다!!!!(VideoWriter)나는 발표자료에 넣기 위해 창의 영상을 avi로 저장해야 해서 write()코드를 추가하였음복붙해서 실행할 시 주의하시길!!  MOG1 video_file = ""./jedo/sample_data/normal.avi""cap = cv2.VideoCapture(video_file)fps = cap.get(cv2.CAP_PROP_FPS)delay = int(1000/fps)frame_width, frame_height, frame_rate = int(cap.get(3)), int(cap.get(4)), int(cap.get(5))fourcc = cv2.VideoWriter_fourcc(*'DIVX')out = cv2.VideoWriter(""./jedo/sample_data/fgmask_MOG1.avi"",                      fourcc, frame_rate, (frame_width, frame_height), 0)fgbg = cv2.bgsegm.createBackgroundSubtractorMOG()while cap.isOpened():    ret, frame = cap.read()    if not ret:        break    fgmask = fgbg.apply(frame)    cv2.imshow('bgsub',fgmask)    out.write(fgmask)    if (cv2.waitKey(delay) & 0xFF) == ord('q'):          break        cap.release()out.release()cv2.destroyAllWindows()  이 영상의 경우 MOG1 방식이 더 적합한 것 같다다만 MOG1으로 추출한 영상에도 노이즈가 좀 많아서 모폴로지를 적용해보려고 한다  MOG2위 코드와 함수부분만 다르다 video_file = ""./jedo/sample_data/normal.avi""cap = cv2.VideoCapture(video_file)fps = cap.get(cv2.CAP_PROP_FPS)delay = int(1000/fps)frame_width, frame_height, frame_rate = int(cap.get(3)), int(cap.get(4)), int(cap.get(5))fourcc = cv2.VideoWriter_fourcc(*'DIVX')out = cv2.VideoWriter(""./jedo/sample_data/fgmask_MOG2.avi"",                      fourcc, frame_rate, (frame_width, frame_height), 0)fgbg = cv2.createBackgroundSubtractorMOG2()while cap.isOpened():    ret, frame = cap.read()    if not ret:        break    fgmask = fgbg.apply(frame)    cv2.imshow('bgsub',fgmask)    out.write(fgmask)    if (cv2.waitKey(delay) & 0xFF) == ord('q'):          break        cap.release()out.release()cv2.destroyAllWindows() MOG2는 지브코비치 알고리즘을 구현한 것이다각 셀의 적절한 가우시안 분포 값을 선택하므로 빛에 대한 변화가 많은 장면에 적합하다(그림자 여부 등 설정 가능)  노이즈가 많이 심하다  모폴로지 적용 [OpenCV] Morphology(Erosion, Dilatation, Opening & Closing, Gradient, Top Hat & Black Hat)모폴로지 형태학이라는 뜻으로 영상 분야에서는 노이즈 제거, 구멍 메꾸기, 연결되지 않은 경계 이어붙이기...blog.naver.com  References OpenCV: OpenCV TutorialsOpenCV Tutorials Introduction to OpenCV - build and install OpenCV on your computer The Core Functionality (core module) - basic building blocks of the library Image Processing (imgproc module) - image processing functions Application utils (highgui, imgcodecs, videoio modules) - application utils (...docs.opencv.org OpenCV: cv::BackgroundSubtractor Class ReferenceBase class for background/foreground segmentation. : More... #include ""background_segm.hpp"" Inheritance diagram for cv::BackgroundSubtractor: Public Member Functions virtual void  apply ( InputArray image, OutputArray fgmask, double learningRate=-1)=0 Computes a foreground mask. More... virtual void...docs.opencv.org 파이썬으로 만드는 OpenCV 프로젝트“개발자에게 딱 필요한 만큼의 이론과 활용 가능한프로젝트로 배우는 OPENCV 프로그래밍”OPENCV는 영상 처리와 컴퓨터 비전 분야에서 현존하는 가장 영향력 있는 라이브러리이다.이 책은 누구나 쉽게 접근할 수 있는 파이썬 언어를 기반으로 기초적인 영상 출력에서부터영상 합성과 블렌딩, 컨볼루션 연산을 이용한 영상 필터, 객체 인식과 추적 그리고 머신러닝까지 OPENCV의 주요한 부분을 대부분 다룬다. 다소 어렵고 딱딱할 수 있는 배경 이론을 중학교 수학 수준에서 크게 벗어나지 않는 선에서 설명한다. 각 장의 마지막에 수록되어 있는 ...book.naver.com ​ "
[CS231n] Lecture 11. Detection and Segmentation(*블로그 내용 학습 및 퍼옴) ,https://blog.naver.com/boumont/222175037977,20201215,"[CS231n] Lecture 11. Detection and Segmentation(*블로그 내용 학습 및 퍼옴)​​ ​배울 내용이 다 들어가있는 사진이다.​ ​첫 번째는 Semantic Segmentation이다.​입력은 이미지고 출력으로는 이미지의 모든 픽셀에 카테고리를 정한다.​픽셀로 카테고리를 정해서 암소 두마리를 구분할 수가 없다.​이것이 Semantic Segmentation의 단점이다. 이후에 Instance Segmentation이 이를 해결할수 있다.​​ ​입력 이미지를 아주 작은 단위로 쪼갠다.​해당 영역이 어떤 카테고리에 속하는지 정하는 것이다. 비용이 매우 크다.​다 쪼개고 모든 영역을 forward / backward해야 하기 때문이다.​따라서 개별적으로 적용하는 것은 매우 좋지않다.​​ ​Fully convolutional이 더 낫다. 이미지 영역을 나누고 독립적으로 분류하는 방법이 아니다.​출력의 모든 픽셀에 크로스엔트로피를 적용한다.​입력 이미지의 spatial size를 계속 유지시켜야 한다. 그래서 비용이 매우 크다.​​ ​Max Pooling, Stride convolution 등으로 특징맵을 Downsampling 해준다.​Image classification에서는 FC-Layer가 있었지만, 여기서는 Spatial Resolution을 다시 키운다.​그래서 다시 입력 이미지의 해상도와 같아진다.​Upsampling의 전략중 하나는 unpooling이다.​​ ​Nearest neighbor unpooling은 해당 원소를 인풋사이즈만큼 늘려서 붙이는 것이고,​Bed of nails 는 unspooling region에만 값을 복사하고 다른 곳은 0으로 채운다.​Max unpooling도 있다.​​ ​대칭적으로 연관짓는다. Max Pooling시 사용했던 요소들을 잘 기억하고 있어야 한다.​Semantic segmentation 에서는 픽셀의 정보를 잘 알고 있어야하는데,​Max Pooling을 하면 특징맵의 비균진성이 발생한다.​pooling이 어디서 왔는지 모른다. 공간정보를 잃었기 때문이다.​그래서 unpool시 max pooling에서 온 자리 그대로 입력해준다.​​ ​Tanspose Convolution 은 학습가능한 방법이다.​​ ​위처럼 반복하는 것이 일반적인 Conv이다.​​ ​Stride convolution은 두 픽셀씩 움직인다.​출력에서 한 픽셀 씩 움직이려면 입력에서는 두 픽셀 씩 움직여야한다.​ ​Tanspose Convolution는 반대의 경우이다. 입력과 출력의 사이즈가 반대의 경우이다.​ ​여기서는 내적을 하지 않고, 입력 특징맵에서 값을 하나 선택한다.​선택한 스칼라 값과 필터(3x3)을 곱한다.​그리고 출력의 3x3 영역에 그 값을 넣는다.​필터와 입력의 내적을 계산하는 것이 아니라 입력 값이 필터에 곱해지는 가중치의 역할을 한다.​출력 = 필터 X 입력(가중치)​​ ​한 스칼라 움직이면 output은 두 번 움직인다. 겹치는 경우는 값을 더해준다.​​ ​​ ​​ ​Convolutional 필터인 vector x는 3개의 원소를 가지고 있다. 입력 vector는 a,b,c,d 총 4개를 가지고 있다.​앞의 행렬을 transpose 시키고 계산한 결과가 오른쪽이다.​​ ​왼쪽은 stride = 2인 conv를 행렬곱 연산으로 표현한 것이다.​​ ​​Classification + localization 이다.​객체의 카테고리 뿐만 아니라, 어디에 있는지를 알고 싶을 수 있다.​localization은 이미지 내에서 내가 관심있는 객체가 오직 하나 뿐이라고 가정한다.​​ ​FC-Layer가 하나 더 있는데 이는 Width / Height / x / y 로 bounding box의 위치를 나타낸다.​두 개의 출력값을 반환하는데, 하나는 class score, 나머지는 bounding box의 좌표이다.​이 네트워크를 학습할 때는 Loss가 2개 이다.​​ ​Human pose estimation 문제도 있다. 출력이 사람 관절의 위치이다.​ ​14개의 아웃풋을 낸다.​​ ​Object Detection이다.​고정된 카테고리가 있다. 이미지가 주어지면 Bbox로 위치를 예측한다.​Classification + localization와는 다르다.​왜냐하면 예측해야하는 Bbox의 수가 입력이미지에 따라 달라지기 때문이다.​​ ​색다른 패러다임이 필요하다.​​ ​배경도 추가하여 예측한다. 하지만 어떻게 영역을 추출할지 모른다.​ ​R-CNN의 원리이다.​여전히 계산비용이 높다. Region Proposals 각각이 독립적을 CNN입력으로 들어간다.​​ ​Fast R-CNN는 입력 이미지에서 부터 ROI(Regions of interest)를 가져오지 않는다.​​ ​Max pooling과 비슷하다.​​ ​위 만큼 속도차이가 난다.​​ ​Faster R-CNN은 region proposal을 계산하는 과정이 병목이라는 것을 해결한다.​RPN(Resional Proposal Network)에는 두가지 Losses가 있다.​하나는 이곳에 객체가 있는지 없는지를 예측하고, 나머지 Loss는 예측한 Bbox에 관한 것이다.​​ ​YOLO = You Only Look OnceSSD = Single Shot Detector​​ ​Dense captioning​​ ​​ ​Instance segmentation은 모든 것들의 합이다.​입력 이미지가 주어지면 객체 별로 객체의 위치를 알아내야 한다.​객체별 segmentation mask를 예측해야한다. ( 각 픽셀 )​​ ​MASK R-CNN은 faster R-CNN과 유사하다. Classification, regression하지 않고​각 Bbox마다 Segmentation mask를 예측하도록 한다.​​​​https://youtu.be/nDPWywWRIRo ​​https://youtu.be/y1dBz6QPxBc ​​https://leechamin.tistory.com/112?category=830805 [CS231n] 11. Detection and SegmentationSegmentation, Localization, Detection  배울 내용이 다 들어가있는 사진이다.  첫 번째는 Semantic Segmentation이다. 입력은 이미지고 출력으로는 이미지의 모든 픽셀에 카테고리를 정한다.  픽셀로 카테..leechamin.tistory.com ​ "
이미지 프로세싱 & 컴퓨터 시각화 19부 - Corner Detection (1부) ,https://blog.naver.com/zeus05100/221676207092,20191013," 여러분 안녕하세요. 여기까지 잘 따라오고 계시는 여러분들에게 격려의 메세지를 보냅니다. 이제 갈수록 더 재미있는 부분들을 다룰 것이라는 생각을 하니 저도 같이 뿌듯하군요. 이번시간에는 Corner Detection 이라는걸 살펴볼꺼에요. 간단하게 코너라고 할꺼에요. 그럼 코너에 대해서 간단히 정의를 해볼까요? (코너를 모른다고 무시하는거 절대 아닙니다. 돌다리도 두들기고 건넌다는 심정으로 같이 따라오시면 됩니다. 오해는 절대 금물!! )​코너는 우선 최소 두개의 직선이 교차하며 그 끝부분에 해당되는 곳을 코너라고 할 수 있겠군요. 음.. 말로 설명하려니 너무 힘듭니다(헥헥). 그럼 아주 간단한 그림을 살펴봅시다.    위의 그림처럼 빨간 점을 우리는 코너라고 하죠? 코너에 대한 특징을 살펴보면 다음과 같습니다. ​1) 코너쪽으로 갈수록 대부분 주변이 어두워진다. 다시 말해 밝기에 큰 변화가 생긴다. 2) 코너는 하나의 직선으로 절대 이루어질 수 없다.​제가 생각해본 코너의 두가지 정의는 바로 위와 같습니다. 이번에는 좀 더 정교한/실제 이미지를 같이 살펴봅시다.    위에서 두군데 빨간 점을 찾으셨나요? (숨은그림찾기라고 하기에는 너무 쉽나요?ㅋㅋ) 저걸 우리는 코너라고 합니다. ​그럼 Corner Detection은 왜 배워야 할까요? 이런 다시 영어를 사용하였군요. 한글 영어 번갈아 가면서 사용할꺼에요. 본론으로 돌아올께요. 왜 배워야 하는지라... 매우 좋은 질문이군요. 여기에 대한 이유는 차차 알아갈 꺼에요. 지금부터 몰라도 괜찮아요. 첫술에 배부르랴 라는 옛말도 있잖아요. 우선 지금은 왜 배워야 하는지에 대해 아래 간단하게 설명할께요. ​Object Detection을 하기 위해서요!!​컴퓨터가 코너를 인식하고 감지하기 위해서는 우리가 새로운 기법을 배워야 합니다. 아주 대표적인 두가지 방법을 소개할께요. ​1. Harris Corner Detection2. Shi-Tomasi Corner Detection​이번시간에는 두개 모두 살펴보기에는 너무 시간이 빠듯할 것으로 생각하기에 Harris Corner Detection만 살펴볼께요. Harris라는 사람이 처음 생각해 낸 방법이며 코너를 감지하기 위해서 이 사람은 다음과 같은 결론을 지었습니다.​""선을 쭈욱 따라가다보면 사방으로 밝기와 명암이 어마하게 변화하는 부분이 생길 것이며 그 부분을 코너라고 하자!""​사실 맞는 말이지요? 우리가 위에서 살펴본 간단한 정의에 의하면요.  그럼 우리가 맨처음 본 이미지를 다시 가져와볼께요.   먼저 주황색 투명 사각형 안을 살펴봅시다. 사방으로 화살표가 뻗어있죠? 그리고 우리는 저기가 코너라는 것도 알고 있구요. 저것이 코너라면 사방으로 뻗쳐지는 화살표에 해당하는 부분의 밝기에 큰 차이가 있어야 합니다. 그렇지 않다면 코너가 아니겠죠? 아래 사각형처럼 말이지요. ​이제 코너에 대한 개념은 확실히 잡고 왔군요. 이제 코딩을 시작해도 문제가 없겠군요. 위에서도 언급을 하였지만 이번시간에는 Harris Corner Detection을 중심으로 살펴볼꺼에요. ​ import cv2import numpy as npimport matplotlib.pyplot as plt%matplotlib inlineimg = cv2.imread(""Image/flat_chessboard.png"")img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) 체스보드 이미지를 가지고 코너를 잘 감지하는지 살펴볼꺼에요. 우선 이미지를 불러온 후 Color Channel을 RGB로 변경시켜준 후 이미지를 봅시다.    Harris Corner Detection을 하가전에 중요한 작업이 필요합니다. 흑백으로 변경시켜줘야 합니다. 기억나나요? Threshold, Gradient, 등등.. 흑백으로 바꿔주는 작업이 매우 중요했었습니다. 이번에도 예외는 아니군요.  gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)plt.imshow(gray_img, cmap=""gray"")   그럼 이제 Harrid Corner Detection을 한번 적용시켜봅시다. 과연 모든 코너를 찾아낼 수 있을까요?  result = cv2.cornerHarris(src = gray_img, blockSize = 2, ksize =3, k=0.04)result = cv2.dilate(result, None) cornerHarris라는 함수를 사용합니다. Parameter를 간단히 살펴봅시다. 먼저 흑백 이미지를 넣어줍니다. 그다음 blockSize가 있는데 이건 주변 이웃들의 크기를 정해주는 것인데 좀 더 구체적으로 설명하자면 코너 Eigenvalue, Eigenvector값들을 찾아내 줍니다. 이 부분은 선형대수학과 매우 밀접한 관련이 있기에 여기서 설명은 마칠께요. 대부분 디폴트로 2를 넣어줍니다. 그다음 ksize가 있는데 예전에 우리는 Sobel Operator과 연관이 있습니다. 이게 무엇인지 기억이 나지 않는다면 Gradient 부분을 다시 살펴보면 됩니다. 거기에서 사용했던 커널 사이즈 그대로 사용합니다. 디폴트 값은 3이며 경우에 따라 값을 조정하여 원하는 값을 얻을 수 있습니다. 마지막으로 k값이 있는데, Harris Detector Free Parameter라고 합니다. 솔직히 저도 잘 모르는 부분이라 패스할께요 ㅠㅠ. 자세히 알고 싶으시면 다음 링크를 참고하세요[참고로 영어로 설명되어 있기 때문에 네이버에서 검색하시는 것이 더 나을수도.. ]. (디폴트로 0.04를 넣어줍니다)​https://stackoverflow.com/questions/54720646/what-does-ksize-and-k-mean-in-cornerharris What does ksize and k mean in cornerHarris?I was playing around with cornerHarris function in OpenCV. I could not understand what does ksize and k mean in the function. The documentation mentions ksize to be Aperture parameter of Sobel deri...stackoverflow.com 그럼 result를 살펴볼까요? ​아닙니다. result값에는 우리가 원하는 이미지가 들어있지 않습니다. 코너들의 좌표값이 들어있습니다. 그럼 어떻게 할까요? 그렇군요. 원래 이미지에 직접 마킹을 해보면 되겠네요. 하지만 result값은 dilated된 녀석들이 들어있습니다. 따라서 우리는 Dilate과정을 한번 해주면 됩니다. (선택사항입니다.) Dilate가 기억이 안나신다면 Morphological Operator섹션을 다시 살펴보면 됩니다.  result = cv2.dilate(result, None) 다시 강조하지만 Dilate과정은 Harris Corner Detection에서 필수적인 과정이 아닙니다. 단지 원본 이미지에 마킹을 하기 위함입니다.​그럼 마킹을 해볼까요?  img[result > 0.01*result.max()] = [255,0,0] 뭔가 희안한 녀석이 나타나서 당황했나요? 하나하나 살펴봅시다. Array Indexing을 하고 있군요. 우리의 원본 이미지를 훑다가 특정 부분이 일치되면 그 부분을 빨간색으로 정의해준다는 뜻입니다. ​result > 0.01*result.max() 를 풀어서 해석하면 다음과 같습니다.  result 최대 값의 상위 1퍼센트.. 즉 result 값이  최대값의 1퍼센트에 해당된다면 그 부분을 빨간색으로 바꿔준다. 라는 뜻입니다. 또한 img이미지는 이미 RGB칼라 체널을 가지고 있기 때문에 아무런 문제가 없습니다. ​그럼 img를 출력해봅시다.    뷰티풀!! 안에 있는 코너들은 모두 감지를 잘했군요. 하지만 바깥부분의 코너들은 감지를 하지 못했습니다. 그 이유는 Harris Corner Detection은 사방을 모두 살펴보기 때문에 모서리 부분은 사방을 모두 살펴볼 수 없기에 코너라고 인식하지 못한 것입니다. 이부분에 대해서는 더 나은 기법을 통하여 더 나은 Corner Detection을 할 것입니다. ​어땠나요? 코너를 감지함으로서 우리는 추후 어떤 것들을 감지하며 발견할 수 있는지 감이 오시나요? 실생활에서 어떻게 사용될 수 있는지 생각해보면 좋을 것 같군요. 여러분들이 소유하고 있는 이미지에 다양하게 적용시키며 연습해보는 것도 매우 좋은 학습이라 본인은 생각합니다. ​오늘은 여기까지 할께요. 다음 시간에는 Shi-Tomasi Corner Detection에 대해서 살펴보며 Corner Detection을 마무리 지을까 합니다! 이제 추운 겨울이 왔는데 감기 조심이요! ​  ​ "
이미지 프로세싱 & 컴퓨터 시각화 23부 - Contour Detection  ,https://blog.naver.com/zeus05100/221737728483,20191215,"첨부파일contour_exercise.png파일 다운로드  안녕하세요. 아마 이번시간이 Detection시리즈의 마지막일듯 합니다. 아쉽다구요? 전혀 그럴 필요 없겠군요. 더 재미있는걸 해봐야지 언제까지 똑같은것만 해보겠습니까? ㅎㅎㅎ 그럼 Contour Detection에 대해서 한번 살펴봅시다. Contour는 무엇인가요? 꼭 알아야 하는 정의만 설명할꺼에요. Contour 다양한 점들로 연결되어지는 일종의 Curve, 혹은 Line과 같은 것들이며 주로 비슷한 색상, 비슷한 색의 강도로 구분지어 집니다. 이미지를 살펴보게될때 Boundry를 우리는 찾을 수 있죠? 그런 것들을 우리는 Contour라고 할 수 있겠네요. 아래 이미지는 Contour Detection을 함으로서 우리는 다양한 도형들을 발견할 수 있습니다. 심지어 무슨 도형인지도 우리는 Classification을 할 수도 있습니다. 이번시간에 다루지는 않을 꺼에요 :) 아! 결론을 내리자면 Contour Detection은 Shape Analysis, Object Detection에서 종종 사용되어 집니다.    잠깐만요!! 한가지 헷갈립니다. Contour랑 Edge가 혹시 비슷하지 않나요? 그렇군요... 두개가 동의어처럼 들릴순 있겠군요. 그럼 여기서 확실히 짚고 넘어가야죠. 음.. Edge는 여러개의 점들이 이어져 있으며 자기의 이웃들을 살펴본 후 Intensity가 얼마나 큰 Difference가 존재하는지.. 만약 그렇다면 그곳을 Edge로 간주합니다. 반면에 Contour는 점과 점들이 연결되어지는 것은 Edge와는 개념적으로 대동소이하나 무조건 Closed Curve여야만 합니다. 다시말해 둘레처럼 중간에 구멍이 나거나 그러면 안된다는 거에요. 그리고 특히 사물의 둘레, Boundry를 나타낼때 Contour라 지칭합니다. 조금이라도 차이점을 피부로 와닿는다면 다행입니다. 아니어도 괜찮구요^^​그럼 바로 코딩으로 들어가보죠. Opencv에서 Contour Detection에 관한 라이브러리가 이미 존재합니다. Opencv는 그야말로 모든것을 혼자 다 해먹는 독재정권이군요!!! ​참고로 Internal, External Contour가 있는데 이건 이미지를 같이 열어보고 알아보죠. import cv2import numpy as npimport matplotlib.pyplot as plt%matplotlib inlineimg = cv2.imread(""Image/contour_exercise.png"", 0)plt.imshow(img, cmap=""gray"")   이미지를 열때 두번째 인자로 0을 건네주면 나는 Grayscale image로 열겠다는 뜻입니다. 그럼 이미지를 살펴봅시다. 완벽한 흑백 이미지이며 우리는 삼각형, 스마일 페이스, 그리고 누가 먹다 남은 페퍼로니가 뿌려져 있는 피자 이미지를 볼 수 있습니다. (솔직히 피자에 뿌려진게 페퍼로니가 아닐 수도 있는것이 아니냐!!! 이렇게 반박 하지 않았으면 좋겠어요. 우리가 이걸 가지고 다투기에는 우리 인생이 너무 짧거든요!!!) ​이제 Internal과 External Contour에 대해서 알아봅시다. Internal Contour는 스마일 페이스의 눈깔, 너무 기분이 좋아서 입꼬리가 귀에 닿을듯한 저 입꼬리, 피자에 뿌려진 페퍼로니가 좋은 예시가 되겠군요. 다들 공통점이 있습니다. 이미지 안쪽에 존재하죠? 그럼 External Contour는 무엇인지 알 수 있겠죠? 스마일 페이스의 얼굴 둘레, 피자의 둘레, 삼각형 둘레 이렇게 되겠군요. 여기서 모두 ""둘레"" 라는 단어가 사용되었습니다. ​  ​Opencv는 Internal, External Contour를 분리해서 감지할 수 있습니다. 참 재미있군요. 그럼 Contour를 한번 찾아봅시다.  image, contours, hierarchy = cv2.findContours(img, cv2.RETR_CCOMP, cv2.CHAIN_APPROX_SIMPLE) findContours라는 함수를 사용하였습니다. 이건 무려 3가지를 return하는군요.. 첫번째로 원래 이미지, 두번째는 Contour, 세번째는 hierarchy.. 둘째랑 셋쩨를 한번 봅시다. 우선 데이터 타입을 살펴봅니다. type(contours)listlen(contours)22type(hierarchy)numpy.ndarray Contour는 리스트, 그리고 그 크기는 22이군요. Hierarchy는 numpy array타입입니다. 그럼 왜 Contour는 크기가 22일까요? ​findContours로 넘겨지는 파라미터를 살펴봅시다. 우선 우리의 원래 이미지를 넣고 둘째로 RETR_CCOMP라는 해괴망측하게 생긴 녀석이 보입니다. cv2.RETR_를 먼저 치고 그다음 TAB을 누르면 다양한 옵션들이 나옵니다. CCOMP는 Complete이라고 External, Internal Contour를 모두 변환해달라는 의미입니다. 마지막은 cv2.CHAIN_APPROX_SIMPLE입니다. Contour Detection에서 주로 넣어주는 디폴트 값이며 Contour를 그리기위해 필요한 점들에 관한 정보만 변환시켜달라 이런 뜻입니다. ​RETR_CCOMP이기 때문에 우리는 External, Internal Contour를 모두 획득하였으며 따라 그에 대한 갯수는 22개라는 뜻입니다. (실제로 세보면 맞습니다)​그럼.. Hierarchy는 무엇인지요?? 이녀석을 한번 출력해봅시다.  array([[[ 4, -1,  1, -1],        [ 2, -1, -1,  0],        [ 3,  1, -1,  0],        [-1,  2, -1,  0],        [21,  0,  5, -1],        [ 6, -1, -1,  4],        [ 7,  5, -1,  4],        [ 8,  6, -1,  4],        [ 9,  7, -1,  4],        [10,  8, -1,  4],        [11,  9, -1,  4],        [12, 10, -1,  4],        [13, 11, -1,  4],        [14, 12, -1,  4],        [15, 13, -1,  4],        [16, 14, -1,  4],        ............. 뭐가 뭔지 하나도 모르겠습니다. 여기서 제일 중요한 것 하나만 일단 짚고 넘어갈께요. 각 Array의 맨 마지막 index, 즉 -1, 0, 0, 0, -1, 4, 4, 4, ..... 예네들이 Internal 인지 External인지에 대한 정보를 담고 있습니다. 나머지는 천천히 알아보도록 하죠.  ​그럼 Contour를 이미지에 그려보죠. 우리가 지금까지 해왔던과는 조금 다른 방법으로 접근해야 하는데요.. 먼저 Empty Array를 하나 생성한 후 여기에 값을 채워넣은 후 이걸 Display해줘야 합니다. 우리의 이미지와 크기가 똑같은 빈 Array를 하나 생성합니다. 0으로 채워져있기 때문에 칠흑같이 어두운 까망이 이미지가 생성되겠죠? external_contours = np.zeros(img.shape) 그다음 우리는 위에서 반환된 contour과 hierarcy를 가지고 Contour를 그립니다.  for i in range(len(contours)):    if hierarchy[0][i][3] == -1:          cv2.drawContours(external_contours, contours, i, 255, -1) contours갯수만큼 for loop을 돌립니다. 그다음 hierarchy를 인덱싱을 통하여 값을 찾아갑니다. hierarchy array 에서 만약 맨 마지막 index값이 -1이라면 이건 바로 External Contour라는 뜻입니다. 아니면 모두 Internal Contour입니다. 다시말해 index값이 -1이라면 현재 index값에 해당하는 contour를 external_contours에 그려라!! 라는 것이죠. drawContours 함수를 사용하여 까망이 이미지에 하나하나 contour를 그려나갑니다. 그럼 이제 External Contour가 어떻게 그려졌는지 살펴볼까요? plt.imshow(external_contours, cmap='gray')   부라보! 멋집니다.. 모든 테두리들이 정확하게 감지되었습니다. 까만 도화지 위에 External Contours가 잘 그려졌습니다. 삼각형, 얼굴, 피자 둘레 말이죠.. 그럼 이번에는 Internal Contour를 그려봅시다. 방법은 매우 유사합니다. index값이 -1이 아닌것만 찾으면 되죠.  internal_contours = np.zeros(img.shape)for i in range(len(contours)):    if hierarchy[0][i][3] != -1:          cv2.drawContours(internal_contours, contours, i, 255, -1)   결과를 예상하셨나요? 그럼 여기서 우리는 Hierarchy를 좀 더 파헤쳐봅시다. 과연 Hierarchy를 가지고 우리는 페퍼로니랑 스마일 페이스의 눈과 입을 구분지을 수 있을까요? 둘 다 Internal Contour이지만 다른 곳에 속해 있거든요. ​정답은 그렇습니다. 다시한번 Hierarchy를 출력해봅시다. array([[[ 4, -1,  1, -1],        [ 2, -1, -1,  0],        [ 3,  1, -1,  0],        [-1,  2, -1,  0],        [21,  0,  5, -1],        [ 6, -1, -1,  4],        [ 7,  5, -1,  4],        [ 8,  6, -1,  4],        [ 9,  7, -1,  4],        [10,  8, -1,  4],        [11,  9, -1,  4],        ............. -1은 External이기에 재껴둘꺼에요. 그다음 0이 3개가 있죠? 그다음 모조리 4라는 값이 들어있을 꺼구요. 여기서 우리가 생각해볼 수 있는 것은 index값이 0이라는 것은 바로 스마일 페이스에 속해 있는 Internal Contour이고 나머지 4는 모두 피자 페퍼로니입니다. 저 값들이 왜 저렇게 나오는지는 모르겠지만 우리는 저 index를 가지고 어떤 도형에 속해있는지 알 수 있는 것이죠. ​그럼 이제 마지막입니다. External과 Internal Contour를 한번 합쳐볼꺼에요. 물론 Internal과 External의 색을 다르게 줍니다. 그러면 바로 우리의 원래 이미지를 생성할 수 있겠죠? 방법은 매우 간단합니다. If statement를 손봐주면 됩니다. ex_in_contour = np.zeros(img.shape)for i in range(len(contours)):    if hierarchy[0][i][3] == -1:         cv2.drawContours(ex_in_contour, contours, i, 255, -1)    else:        cv2.drawContours(ex_in_contour, contours, i, 123, -1) 지금까지 코드가 에러없이 잘 돌아갔다면... 축하합니다!! 그럼 이제 external_contours에는 모든 Contour가 채워졌겠군요. 결과를 같이 봅시다!     ​축하합니다! 이로서 여러분들은 Contour에 대한 개념을 확실히 잡으셨습니다. 뿐만 아니라 어떻게 찾아내는지도 배우셨구요. 그러나 findContours함수에는 다양한 인자값들이 존재하기 때문에 여러분들이 이것저것 시도해보면서 몸으로 배우시는 것을 권장합니다!! ​이로서 드디어 모든 Detection시리즈를 끝냈습니다... (휴~~~) 배운 내용들 모두 중요한 개념들이며 나중에 더 복잡한 개념을 이해하기 위해 필요한 것들입니다.. 다음 시간부터는 Feature Matching이라는 것을 살펴볼 것입니다. 두개의 이미지(A, B)가 있으며 이미지A가 이미지B에 존재하는지, 존재한다면 얼마나 확신하는지에 대한 내용입니다. 매우 재미있을꺼에요!! 실망 안시키도록 노력할 것입니다 푸하하핫!!!! ​그럼 홧팅입니다!!​​​​ "
Image Detection and Segmentation: From R-CNN to Mask R-CNN ,https://blog.naver.com/angela5730/221828243819,20200228,"​ CNN2012년 ImageNet에서 Alex Krizhevsky, Geoff Hinton 와 Ilya Sutskever 가 우승한 이래로 CNN은 이미지 분류에 있어서 표준 처럼 되어버렸다. 그 이후로도 CNN은 계속 발전하였고, ImageNet Challenge 에서는 사람이 직접 분류한 결과보다 더 뛰어난 수준에 이르렀다. 아래의 결과들은 인상적이지만, 신경망에서 Classification은 아직 사람이 시각적으로 이해하는 과정의 복잡함과 다양함에 비해 꽤 단순하다.​  출처: https://junn.net/archives/2517 ​​​이미지 분류의 목표는 일반적으로 하나의 객체가 그려져 있을 떄 이 객체가 무엇인지 알아내는 것이다. 하지만, 실제 삶 속에서 볼 수 있는 시각적 이미지는 무수히 다른 종류의 객체들이 있고, 또한 겹쳐져 있으며 그 뒤의 배경들과 움직임(동적 혹은 정적)으로 구성되어 있기 때문에 정확히 분류하기가 어렵다. 따라서 정확한 분류를 위해서는 겹쳐져 있는 대상들을 분류(classify)할 뿐만 그 대상의 클래스를 구별(identify)하는 과정이 필요하다.​​  이미지 속에서 분류하고, 그것들의 경계를 확인하는 과정. 출처: Mask R-CNN paper. ​​발전된 CNN 아키텍쳐를 통해 객체를 dection하고 segmention하는 과정을 통해 이미지를 더 명료하게 구분할 수 있다.  이 과정의 근본이 되는   R-CNN(regional CNN), 그 다음 세대인 Fast R-CNN, SPP-Net, Faster R-CNN, 페이스북에서 pixel 수준의 segmentaion을 한 Mask R-CNN을 차례로 살펴보면서 공부해보자. 더 나아가 SSD를 추가로 공부하여 업로드 할 예정이다.​​관련 논문1. R-CNN: https://arxiv.org/abs/1311.25242. SPP-Net: https://arxiv.org/abs/1406.47293. Fast R-CNN: https://arxiv.org/abs/1504.080834. Faster R-CNN: https://arxiv.org/abs/1506.014975. Mask R-CNN: https://arxiv.org/abs/1703.06870​​​​​​ R-CNN(2014)Rich feature hierarchies for accurate object detection and semantic segmentationR-CNN과 같은 객체 검출 알고리즘들은 하나의 이미지에서 주요 객체들을 분류하고 위치를 찾는다. 이 기법은ImageNet과 유사한 PASCAL VOC대회(Visual Object Classes Challenge)에서 선보인 것이다. R-CNN은 CNN이 PASCAL VOC 대회에서 다른 HOG-like feature[1]를 이용한 검출 방식에 비해 객체 탐색에 있어 극적으로 높은 수준의 성능을 이끌어낼 수 있다는 것을 처음으로 보인 알고리즘이다.​[1]HOG-like feature: histogram of oriented gradients 를 이용해 객체를 찾기위해 사용하던 알고리즘, feature를 찾는 방식이다.​​  출처: https://arxiv.org/abs/1311.2524  ​​R-CNN에 대한 이해R-CNN의 목표는 하나의 이미지에서 주요 객체들을 박스(bounding box)로 표현하여 정확히 식별하는 것에 있다. ​- input: 이미지- output: 박스로 영역 표시(bounding box) + 각 객체에 대한 라벨링(class level) ​박스의 위치를 찾아내기 위해선 무수히 많은 박스를 생각한 다음에 그 중에 어떤 것이든 실제 객체에 해당하는 것을 찾게 된다. 즉, 이 논문에서 나와있듯이 Selective Search라고 명명된 알고리즘을 통해 비슷한 텍스쳐(질감)이나, 색, 강도 등이 비슷한 픽셀끼리 연결된 것들을 둘러싸는 다양한 크기의 '창(window)'을 찾는 것이다. ​​​  출처: https://www.koen.me/research/pub/uijlings-ijcv2013-draft.pdf ​​R-CNN은 selective search를 통해 이 상자들, 또는 region proposal이라고 불리는 영역을 생성해낸다.  Selective search[2]는 위의 사진에서 보는 것과 같이 다양한 크기의 창(window)을 만들어내는데, 이 창은 비슷한 질감이나 색, 강도를 갖고 있는 인접한 픽셀들로 구성된다. ​[2] Selective search:  http://www.cs.cornell.edu/courses/cs7670/2014sp/slides/VisionSeminar14.pdf​​​R-CNN에서 Bounding Box개선하기객체에 맞춰서 박스를 정확하게 그려내기 위해서 R-CNN은 더 tight한 box를 만든다. 이를 위해서 linear regression을 이용하는데,  해당 선형회귀식의 입력과 결과물은 다음과 같다.​- input: 객체에 해당하는 이미지의 부분(sub-region)- output: 그 입력 이미지의 부분에 맞는 새로운 bounding box의 좌표들​​그리고 아래와 같이 R-CNN에서 Proposal region이 생성되면, AlexNet을 개량해서 생성한 모델에 이미지를 통과시켜 유효한 영역인지 판별하는 과정을 거친다. ​​R-CNN 정리​  출처: https://seongkyun.github.io/papers/2019/01/06/Object_detection/#disqus_thread ​1. Hypothesize Bounding Boxes (Proposals)- Image로부터 Object가 존재할 적절한 위치에 Bounding Box Proposal (Selective Search)- 2000개의 Proposal이 생성됨.​2. Resampling pixels / features for each boxes- 모든 Proposal을 Crop 후 동일한 크기로 만듦 (2242243)​3. Classifier / Bounding Box Regressor- 위의 영상을 Classifier와 Bounding Box Regressor로 처리​하지만 모든 Proposal에 대해 CNN을 거쳐야 하므로 연산량이 매우 많은 단점이 존재  SPP-Net(2014)Spatial Pyramid Pooling in Deep Convolutional Networks for Visual RecognitionR-CNN은 잘 작동했지만, 몇 가지 이유로 꽤 느렸다. ​1. region proposal을 CNN에서 classification할 때 image를 warp/crop을 하기 때문에 이미지 변형/손실이 일어나서 성능이 저하된다.2. 매번 하나의 이미지에서 나오는 모든 영역들을 각각 CNN에 통과시켜야한다. 약 2000개 region proposal을 뽑고 다 CNN computation을 돌리기 때문에 속도가 저하된다.3. 3개의 다른 모델을 학습시켜야 했다. Image feature를 생성하는 것, Classifier가 class를 예측하는 것, regression model이 bounding box를 찾아낸 것. 이것이 전체적인 pipeline을 학습시키기 어렵게 했다.4. Region Proposal에 쓰는 알고리즘들은 GPU의 빠른 연산에서 이득을 못보는 CPU연산이었고 속도에서 Bottleneck[2]으로 작용함.​[2] Selective Search로 2000개 proposal 뽑을 때 image 크기에 따라 다르지만 2초 정도 걸린다. edgebox는 약 0.2초. 그런데 AlexNet의 Forward 속도는 이보다 빠르다.​  출처: https://www.slideshare.net/simplyinsimple/detection-52781995 ​​1, 2번의 단점을 해결하고자, SPP-Net에서는 input size의 image도 고정된 vector output으로 나오게 했다. 기존의 AlexNet같은 경우에는 227*227 사이즈의 이미지만 input으로 고정해야 했다. CNN에서 Convolution은 input size에 상관이 없으나 Fully Connected(FC) layer에서 input size가 고정되어야 하기 때문이다. 참고로, Convolution은 filter로 conv 계산만 하는 개념이고 FC는 fixed input size에서 fixed output size로 transformation을 하는 개념이다.​위의 AlexNet을 보면 마지막 layer에서 1000개가 나와야하기 때문에 input image가 227*227이어야했다. 결국, FC는 데이터를 transformation하는 개념이었는데 이는 사실 1*1 convolution layer라고 할 수 있다. ​​​  출처: https://www.slideshare.net/simplyinsimple/detection-52781995 ​​SPP-Net 의의SPP layer가 conv layer와  FC layer 사이에 들어감에 따라 어느 사이즈의 이미지든 input으로 받을 수 있게 되었다. Image에서 Selective Search 등으로 Region Proposal한 영역을 CNN Forward computation에서 filter를 적용한 결과인 Feature map을 pooling하고 SPP layer를 통과시킨다. R-CNN은 2000개의 Region에 CNN을 모두 돌렸는데, 이번엔 image 전체에 CNN을 한번만 돌리고 나온 결과물인 feature map으로 detection을 수행한다. 그 결과 R-CNN과 성능은 비슷하면서도 training/test 속도가 몇배로 빠르다.​​  출처: https://www.slideshare.net/simplyinsimple/detection-52781995 ​​​​ Fast R-CNN(2015)빨라진 속도와 심플해진 R-CNN2015년 R-CNN의 제1저자였던 Ross Girshick은  R-CNN와 SPP-Net의 문제점들을 해결하여  Fast R-CNN를 발표하였다. 전체적인 아이디어는 SPP-Net과 비슷한데, 더 나은 방식으로 Back Propogation 알고리즘을 설계해서 기존의 SPP-Net이 Conv layers들을 학습시키지 못한 단점을 보완한 것으로 보인다​​​Insight 1. ROI (Region of Interest) 풀링CNN의 forward pass에 대해 Girshick가 꺠달은 것은, 하나의 이미지에는 많은 수의 proposed regions이 있는데 이 영역들에는 항상 겹쳐진(overlapped) 영역들이 존재하고, 이것들이 계속되는 계산을(약 2000번에 가까운) 유발한다는 것이었다. 그의 생각은 단순했다. - CNN을 한 이미지를 한번만 돌리고, 2000번에 이르는 제안(proposal)들에 대해 나눠서 모델에 넣지 않고, 계산된 값들을 공유하는 방법을 찾을 수 있지 않을까? ​​  출처: Stanford’s CS231N slides by Fei Fei Li, Andrei Karpathy, and Justin Johnson ​​RoIPool은 어떠한 이미지를 통과시키는 full forward pass인데, 각 RoI에 대한 convolution features들이 forward pass를 통해 결과값으로 추출된다. Fast R-CNN이 하는 일이란 정확히 RoIPool(Region of Interest Pooling)이라고 알려진 테크닉으로 이루어 내는 것이다. RoIPool의 핵심은 한 이미지의 subregion에 대한 forward pass값을 서로 공유하는 것이다. 위의 그림을 통해 어떻게 각 region에 대한 CNN feature가 feature map의 동일한 영역으로 부터 선택되어 값을 얻어내는지 확인할 수 있다. (역자 주: 사실 그림만으로는 설명이 빈약하다. https://github.com/deepsense-ai/roi-pooling에서 추가적인 이해에 도움을 얻을 수 있다) 그리고 나서 각 영역의 features들은 풀링을 거친다(주로 max pooling). 결국 우리에게 주어지는 것은 단 하나의 pass일 뿐이다. (2000개와 비교해보자..)​​RoI pooling layer은 SPP layer의 간단 버전이다. 이 레이어는 single level scale만 받는다. 예를 들어, VGG16 모델​을 사용했을때, SPP-Net은3-layer softmax classifer만 fine-tuning이 가능한데, 그 이유는 convoulutional features가 offline으로 작동하기 때문이다. 그래서 Back-propagation이 안된다. 그렇게 되면 첫 l3개의 layers만 초기값으로 고정되고, 마지막 3개의 layer만 업데이트되게 된다. (이게 어떤 의미지..?)따라서 SPP-Net은 R-CNN에서 개발한 ROI-centric하게만 training이 가능하다.​RoI에 대해서 계산하는데 보통 RoI 크기가 원본 image 만함. 그래서 느리고 메모리도 너무 많이든다. 하지만 FRCN에서는 image-centric임. mini-batches들이 hierarchically 샘플되는데, 이미지 그다음 그 이미지의 RoI들에 대해서. 한 image에 대한 RoI들은 computation, memory 공유함.​ RoI pooling layer를 다시 설명하자면 이미지 input size가 다양해도 output vector size를 고정시켜주기 위해 쓴다.​​  ​​​Insight 2. 모든 모델을 하나의 네트워크로 Join Training   Join training! SPP-Net은  Image feature 생성, Classifier가 class를 예측, regression model의 bounding box 서칭에 대한 3개의 다른 모델을 학습시켜야 했던 기존 R-CNN의 문제점을 개선하지 못했다. 이는 전체적인 pipeline을 학습시키기 어렵게 했다. ​이를 해결하기 위해 세개의 모델을 하나의 네트워크로 구성하기로 했다. 이전의 R-CNN은 image feature를 추출(CNN)하고 분류하는 모델(SVM), 그리고 bounding box를 맞추는 것(regressor)를 나누었다. Fast R-CNN은 대신에 하나의 네트워크로 모든 세가지를 계산하였다. 위의 그림을 보면 이를 어떻게 구현했는지 알 수 있다. Fast R-CNN은 SVM classifier를 top layer에 softmax layer를 둠으로써 CNN의 결과를 class로 출력하게 했다. 또한 box regression layer를 softmax layer에 평행하게 둠으로써 bounding box 좌표들을 출력하게 했다. 이러한 방식으로 모든 필요한 결과물을 하나의 네트워크로 구할 수 있는 것이다!​- input: region proposal이 된 이미지들- outputs: 각 region에 있는 객체의 class들과 그에 따른 tighter bounding box들​SPPnet의 input size에 상관없는 장점을 살린 것과, CNN computation을 단 한번으로 detection을 해내서 속도를 빠르게 해냈다는 것이 장점이다. 하지만 여전히 Region Proposal을 필요로 하기 때문에 여기서 bottleneck으로 작용한다.​​​Fast R-CNN 정리   ​​​​ Faster R-CNN(2016)속도를 높인 region proposal 네트워크위의 성과에도 불구하고, 하나의 남은 병목지점(bottleneck)이 Fast R-CNN 과정 중에 있었다. 그것은 region proposer였다. 우리가 봤듯이 객체의 위치를 찾는 과정에의 첫 번째 과정은 바로 ‘다양한, 가능성 있는’ bounding box들 또는 region of interest(ROI)들을 생성하는 과정 이었다. Fast R-CNN에서, 이러한 제안들은 Selective Search라는 꽤 느린 과정을 거치는데, 이 과정이 모든 흐름에 있어서 병목현상을 만들어낸다. ​2015년 중반, Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun로 구성된 Microsoft Research 팀은 region proposal 단계를 거의 지체없이 해낼 수 있는(cost free) 발견했는데, 그들은 그 이름을 (창의적이게도) Faster R-CNN 이라고 하였다.Faster R-CNN의 아이디어는 각 이미지에서 classification의 첫 단계인 CNN의 forward pass 를 통해 얻어진 feature들에 기반하여 영역을 제안하는 것이었다. 즉, CNN결과를 selective search 알고리즘 대신 region proposal에 이용해보는 것이 어떨까? 라는 질문이었다. ​  Faster R-CNN에서 single CNN은 region proposal에 이용되고, classification에도 이용된다.  실제로, 이것이 곧 Faster R-CNN 팀이 이루어 낸 결과였다. 위의 그림을 보면, 어떻게 single CNN이 region proposal과 classification을 동시에 해냈는지 볼 수 있다.(역자주: 방법론은 아래에 다시 나온다.) 이러한 방법으로, 단지 하나의 CNN만 학습시키면 region proposal은 거저 얻을 수 있다! 저자는 다음과 같이 적었다:  우리의 발견은, region-based detector를 이용하여 얻은 convolutional feature map이 Fast R-CNN 처럼 region proposal에 이용될 수 있다는 것이다.(따라서 cost-free region proposal이 가능하다​​따라서 input과 output은 다음과 같다.- Input: 이미지들(region proposal이 필요하지 않다)- Output: 분류(classification) 및 이미지 내부 객체들의 bounding box 좌표​​어떻게 영역이 생성되는가?Faster R-CNN이 어떻게 CNN feature로 부터 region proposal을 해냈는지 볼 차례다. Faster R-CNN은 Fully Convolutional Network을 CNN이 만들어낸 feature들의 맨 위(top layer)에 놓았는데, 이것이 Region Proposal Network 이라는 것이다.​  Region Proposal Network은 특정 크기의 창(window)를 CNN의 feature 위에서 sliding 시킨다. 각각의 윈도우의 위치에서 네트워크는 점수(score) 및 한 앵커지점으로 부터 bounding box 를 출력해낸다. (따라서 k개의 앵커가 있다면, 4k 개의 박스 좌표들이 구해질 것이다.)  ​​​Region Proposal Network은 sliding window를 CNN feature map 을 지나하게 하면서, 각 윈도우에서 k 개의 가능성있는 bouding box와 이 박스들의 기대값(역자주: 일종의 객체 인식률)을 출력해준다. 이 k 개의 박스들은 무엇을 나타내는가?​​​  우리는 사람을 둘러싼 bounding box가 세로가 더 긴 직사각형일 것임을 안다. Anchor 생성을 통해 Region Proposal network을 이용하는 것에 이러한 직관을 이용할 수 있다. ​직관적으로, 우리는 이미지에 있는 객체들이 어떤 일반적인 비율과 크기를 갖을 것이라고 생각할 수 있다. 예를 들어 사람의 형상을 닮은 사각형들을 떠올려 볼 수 있다. 우리는 아마 이것이 매우 매우 얇을 것이라고 생각하지는 않을 것이다. 이러한 방식으로 k 개의 일반적인 비율(common aspect ratio)을 지닌 anchor box 로 이름 붙이고 고안하였다. 각각의 anchor box를 이용하여 우리는 하나의 bounding box 및 score를 이미지의 위치별로 출력해냈다. Anchor box를 생각하면서, 이 Regional Proposal Network의 입력과 출력들을 보자.​- Input: CNN feature map.- Output: 앵커당 하나의 bounding box 와 스코어(이것은 얼마나 이 bounding box 내의 이미지가 실제 객체일지를 나타낸다.)​그리고 나서 우리는 이런 객체일 것 같은 bounding box 을 Fast R-CNN으로 넣어 classification과 tightened bounding box를 구한다.​ Mask R-CNN(2017)픽셀 수준의 segmentation까지 Faster R-CNN을 확장시키다.​차후 업데이트..​​​ Referencehttps://seongkyun.github.io/papers/2019/01/06/Object_detection/https://junn.net/archives/2517https://blog.athelas.com/a-brief-history-of-cnns-in-image-segmentation-from-r-cnn-to-mask-r-cnn-34ea83205de4https://seongkyun.github.io/papers/2019/01/06/Object_detection/#disqus_thread​ "
"편하게 쓰는 고속무선충전기, 데이니즈 DC 배트맨 차량용 고속충전거치대 사용 후기 ",https://blog.naver.com/kmks6739/223068144809,20230407,"​ 데이니즈 DC 배트맨고속충전거치대차량용 고속무선충전기​ ​차량 네비보다는 티맵을 훨씬 선호해서운전할 때마다 핸드폰 거치대를 활용중인데,늘 갖고 싶었던 아이템이 있었다.바로 고속무선충전거치대!그런데 이제 FOD 센서를 갖춘!! ​FOD 센서란, Foreign Object Detection의 약자로디지털 제품에만 반응하는 센서이다.초창기 차량용 무선충전거치대는손만 앞에서 왔다갔다 해도핸드폰인줄 알고 거치대가 불필요하게여닫히는 오작동이 큰 문제였다.​그러던 어느날 FOD 센서를 가진거치대가 등장했고, 오직 핸드폰에만반응해 여닫을 수 있게 되어서주행중 사고 위험성도 크게 줄었다.​​​​ ​내가 사용하게 된 제품은""데이니스 DC 배트맨 차량용 무선충전 거치대""위에서 신나게 설명한 FOD 센서를 갖추고 있고,송풍구 거치, 대쉬보드 거치가 모두 가능하다.​제품에 들어간 배트맨 캐릭터도DC 배트맨 정품 라이센스 제품~박스 뒷면에 정품 표기 로고가 딱!​​​​ ​구성품은 이렇게 들어있다.사진에는 빠졌지만 설명서도 있음.USB선은 패키지에 있어서시거잭에 연결할 충전기만 준비하면바로 간단 설치 가능!​​​​ ​일단 새 제품의 상징.비닐을 샤샤샷 뜯어줬다.처음 뜯어낼 때의 그 쾌감! > ㅁ<​​​​ ​비닐을 뜯으니 날렵한 배트맨 캐릭터가더 멋지게 모습을 드러낸다.​​​​ ​본체를 뒤집으면 요런 모습인데위쪽 둥근 홀에 거치대를 끼워서조립할 수 있다.​충전하면서 생기는 발열이 해결되도록아래쪽에는 통풍구도 뚫려있는 모습.이 때문인지 무선충전거치대를 사용하면서한번도 기기가 너무 뜨겁다거나과열이 된다는 느낌은 못 받았다.​​​​ ​대쉬보드에 부착할 수 있는 부품을꺼내보았다. 옆의 나사를 돌려주니쉽게 길이 조정이 가능하다.나사를 돌리고 뽑고 반대로 돌리면 끝-​​​​ ​대쉬보드가 넓은 차량이라면부착형도 편할 것 같지만내 차는 당최 뭘 올리고 말고 할자리가 없는 관계로 송풍구 거치를 택.​송풍구 거치대는 그냥 집게를 열어서송풍구 날개에 쑥 밀어넣으면 끝~높이는 본인이 보기 편한 위치로 하면 되는데가급적 위쪽으로 끼워주는 편이네비 볼 때 시야가 너무 내려가지 않아서 좋다.​​​​ ​핸드폰 기종에 따라무선충전을 인식하는 위치가 다를 수 있다.거치대 하단에 있는 받침대는조절 가능해서 거치하는 폰의 무선충전위치에 맞출 수 있다.​​​​ ​사실 별거 없지만그래도 설명서 한번 살펴보고설치하러 고고~​송풍구형 설치는 제품 부품 설명 2번을참고한다.​​​​ ​(지금 사진보니 왜이리 더럽..먼지좀 닦을걸.. ㅜㅜ ㅋㅋㅋㅋ)​데이니즈 DC 배트맨 차량용 무선충전 거치대가송풍구에 단단히 거치가 된 모습이다.사실 저 피벗노브와 거치 클립을결합하면서 제일 헤맸는데..​​​​ ​처음에는 아무생각 없이나사가 다 조여진 채로거치 클립을 홀에 끼우려고 하니아무리 힘을 써도 안 들어가는 것..​이게 왜 안돼?! 하면서 잠시 용을 쓰다 깨달았다.사진처럼 나사를 다 풀고둘을 결합한 뒤 다시 나사를 조여주면단단히 고정이 된다.역시 머리가 나쁘면 손발이 고생...  ​​​ ​예전에 쓰던 마그넷 형식 거치대는왼쪽 송풍구에 끼워 썼기 때문에무선충전거치대도 왼쪽에 끼워봤지만오른쪽이 더 편한 듯 하여 다시 이사.​몇 번을 끼웠다 뺏다 해도거치가 간단하기 때문에편하게 움직일 수 있었다.​​​​ ​드디어 최최종 버전.블루투스도 없어서 리시버를 달고냉난방도 손으로 열심히 조절해야 하는참 여러모로 손이 바쁜 차량이지만DC 배트맨 차량용 무선충전 거치대의등장으로 조금 더 현대의 기술력에 가까워진그런 모습이 되었다. ㅋㅋ​다 해놓고 보니 너무 편리하고 멋진 것!대만족이닷~​​​​ ​이전에 마그넷형 거치대를 쓸 때는폰 케이스도 항상 마그넷이 되는그립톡을 찾아서 달아야 했고,거치만 해두면 충전은 안되기 때문에충전선을 따로 꽂아줘야 하는 번거로움이 있었다.​이제는 시동이 켜지면알아서 거치대가 스르르 열리고폰을 올리면 알아서 닫는다!따로 핸드폰에 충전기를 연결하지 않아도주행하는 동안 배터리가 채워진다.​네비 보고 음악 틀고충전 없이 주행하면 폰 배터리가쭉쭉 닳아버리는데, 이젠 그냥 거치대에놓아두기만 하면 충전까지 해결!​  ​​​​  ​시동이 켜질 때 알아서 슥 열리지만,이 외에 폰을 잠시 꺼내야 할 땐본체 뒤쪽의 터치버튼을 손가락으로 툭,건드리면 열거나 닫을 수 있다.​영상으로 무선충전거치대의스무스한 연결 동작을 살펴볼 수 있다.​​​​ ​주변이 어두워지면 더 강렬해지는배트맨의 존재!마치 영화에서 배트맨을 소환할 때 쓰던박쥐 램프같은 느낌도 나고,내 차를 든든하게 지켜줄 것만 같은그런 느낌적인 느낌. ㅋㅋㅋ게다가 램프가 번쩍여주니어두운 밤에도 거치대에 정확히폰을 올려둘 수 있어서 기능적으로도 Good~​포장 패키지도 깔끔하고거치대도 캐릭터가 있어 차량 데코로도손색 없기 때문에 운전하는 친구 선물로도 그만이겠다!​​​​  ​주행 중 얼마나 흔들림없이 잘 붙어있는지영상을 남기고 싶었는데,운전하면서 찍을 순 없으니..마침 옆에 탄 엄마에게 촬영을 부탁했다.​그 결과 미친듯이 흔들리는 앵글이 탄생했지만,거치대는 흔들림이 없었다.(영상 멀미 주의..ㅋㅋㅋ)​200퍼센트 만족한데이니즈 DC 배트맨 차량용 고속충전거치대!나처럼 문명의 혜택을 누리지 못하는구형 차량 유저들에게 진심으로 추천하고프다.​​구매는 스마트스토어 링크에서~ 👇👇 데이니즈 DC 배트맨 차량용 고속 무선충전 거치대 : 데이니즈 공식 스토어데이니즈 DC 배트맨 차량용 고속 무선충전 거치대smartstore.naver.com ​​​  해당 리뷰는 체리플을 통해데이니즈 업체로부터 제공받아 작성되었습니다.체리플 수익의 일부는 소상공인 지원을 위해 사용됩니다.​​​​​​ "
"A01_Autoware Install      < Foxy, lg-svl : Fail > ",https://blog.naver.com/mdc1020/223030488223,20230228,"공부하다 보니,  Autoware 최신 버전: Humble 있음                 하하하  U20 두번 설치 작업  < 5시간 소요 >​Humble: Autoware CARLA                천천히 작업__Humble                             (lg-svl: 서버 죽음)​https://gitlab.com/autowarefoundation/autoware.auto/AutowareAuto>> ade , Foxy 작업,                    # 처음 작업_Fail     10 GB USE​​https://github.com/autowarefoundation/autoware>> humble                                  # 이쪽에서 다시 시작https://autowarefoundation.github.io/autoware-documentation/main/installation/>>  Install:      Docker,   Source-Build>> 'CARLA - Autoware universe Tutorial 1'   >>   Galactic​​Autoware.Auto : ROS 2Autoware.AI      : ROS 1Autoware.Auto (autowarefoundation.gitlab.io)https://answers.ros.org/question/367093/autowareai-vs-autowareauto/​​​•검색: autoware carla CARLA - Autoware universe Tutorial 1/2 살펴보자​>> Script 관련내용정리 https://www.bilibili.com/read/cv19814487?from=articleDetail                Tutorial__Good https://github.com/ZATiTech/autoware.universe.openplanner https://autowarefoundation.github.io/autoware-documentation/main/installation/                   Autoware_Install https://github.com/carla-simulator/carla                                                          Carla ​CARLA-OpenPlanner Tutorial Part 1 to 7                   2년전https://www.youtube.com/watch?v=fL0GJ-Q56js&list=PLVAImlqqGbr5YrMdGBmNIIYQdZwbZF9bx&index=5​​​​​​​​---------------------------------------------------------------------------------------------------- 아래: Foxy 작업: 폐기    <참고>​​Env:  우분투20    FoxyAutoware 설치 Script                   A01_Autoware_Install.ipynb - Colaboratory (google.com)​​​ ​​​•Autoware.Auto Simulation Demo   /   LG SVL Simulator  사용https://husarion.com/tutorials/ros2-tutorials/autoware-auto-sim-demo/​•LG SVL Simulator: 서버 죽음 / 2022년 12월 Alivehttps://velog.io/@spiraline/svl1https://www.svlsimulator.com/https://github.com/lgsvl/simulator ​​​​​​​​​​​​​​​​중간 점검----------------------------------------------------------------------------------------------------------------------- 03.01(수) 2023004_Run object detection demo   작업중​Bashrc ade entersource /opt/ros/foxy/setup.bashsource /opt/AutowareAuto/setup.bashalias cl='clear' # C1udpreplay ~/data/route_small_loop_rw-127.0.0.1.pcap -r -1 # C2: 38.37source /opt/AutowareAuto/setup.bashrviz2 -d /home/${USER}/autowareclass2020/code/src/01_DevelopmentEnvironment/aw_class2020.rviz Screenshot from 2023-03-01 21-16-51.png          TF는 Robot State 실행 후 살아남C3_Not # C3: 38.51source /opt/AutowareAuto/setup.bashros2 run velodyne_nodes velodyne_cloud_node_exe __ns:=/lidar_front __params:=/home/${USER}/autowareclass2020/code/src/01_DevelopmentEnvironment/velodyne_node.param.yaml >> s 추가Package 'velodyne_node' not found theg@ade:~$ # C3: 38.51theg@ade:~$ source /opt/AutowareAuto/setup.bashtheg@ade:~$ ros2 run velodyne_nodes velodyne_cloud_node_exe __ns:=/lidar_front __params:=/home/${USER}/autowareclass2020/code/src/01_DevelopmentEnvironment/velodyne_node.param.yaml[WARN] [1677672898.828853837] [rcl]: Found remap rule '__ns:=/lidar_front'. This syntax is deprecated. Use '--ros-args --remap __ns:=/lidar_front' instead.[WARN] [1677672898.829052950] [rcl]: Found remap rule '__ns:=/lidar_front'. This syntax is deprecated. Use '--ros-args --remap __ns:=/lidar_front' instead.1677672898.837609 [0] velodyne_c: using network interface wlp59s0 (udp/172.30.1.2) selected arbitrarily from: wlp59s0, docker0terminate called after throwing an instance of 'rclcpp::ParameterTypeException'  what():  expected [string] got [not set]theg@ade:~$  Screenshot_from_2023-03-01_22-51-38_velodyne_yaml""Address already in use"" when publishing sensor data under autuware.auto ADE environmenthttps://answers.ros.org/question/374809/address-already-in-use-when-publishing-sensor-data-under-autuwareauto-ade-environment/​좀 더 내용 살펴보고,   실행 안 되도,  우선 그냥 관찰하자.....아직은  내가  전체를  못 본다​최종 object detection demo  # Assy:  여기 있는 CMD 는 무수정   /   현재 3번째 ""velodyne_node""  실행 안되고 있음udpreplay ~/data/route_small_loop_rw-127.0.0.1.pcap -r -1rviz2 -d /home/${USER}/autowareclass2020/code/src/01_DevelopmentEnvironment/aw_class2020.rviz             @@ros2 run velodyne_node velodyne_cloud_node_exe __ns:=/lidar_front __params:=/home/${USER}/autowareclass2020/code/src/01_DevelopmentEnvironment/velodyne_node.param.yamlros2 run robot_state_publisher robot_state_publisher /opt/AutowareAuto/share/lexus_rx_450h_description/urdf/lexus_rx_450h.urdfros2 run point_cloud_filter_transform_nodes  point_cloud_filter_transform_node_exe __ns:=/lidar_front __params:=/opt/AutowareAuto/share/point_cloud_filter_transform_nodes/param/vlp16_sim_lexus_filter_transform.param.yaml __node:=filter_transform_vlp16_front points_filtered:=/perception/points_inros2 run ray_ground_classifier_nodes ray_ground_classifier_cloud_node_exe __ns:=/perception __params:=/opt/AutowareAuto/share/autoware_auto_avp_demo/param/ray_ground_classifier.param.yamlros2 run  euclidean_cluster_nodes euclidean_cluster_exe __ns:=/perception __params:=/opt/AutowareAuto/share/autoware_auto_avp_demo/param/euclidean_cluster.param.yaml 최종 object detection demo              실행 되는거 보고 싶었는데,   쉽지 않네​​​​​​​001. Autoware 실행 필요한 도커 이미지 3개 완성         lgsvl 누락되서, 내가 추가함​​lgsvl 도커 이미지: 처음 실행 잘됨   /   재부팅 후 실행 안됨    >>   연습                                                                                                                             도커 이미지: Pull >> 자동실행​ Screenshot_from_2023-03-01_16-21-56__2021.3__3번째 실행Screenshot from 2023-03-01 16-32-25__lgsvl_ED__3ea_Complete.pngScreenshot from 2023-03-01 18-54-41__ADE_Docker_Img_ED.png​​​​​​​​ Autoware 준비 완료----------------------------------------------------------------------------------------------------------------------- 02.28(화) 2023 우분투20 Foxy 포멧후 재설치 작업 완료                           Foxy 지원: 5월 종료 / 그 전에 연습해 보자https://blog.naver.com/mdc1020/223029127255>> Gparted 사용, 하드 용량 늘리기​ K-20230228-584525__LGSVL Simulator​•유튜브 동영상https://www.youtube.com/watch?v=XTmlhvlmcf8&list=PLL57Sz4fhxLpCXgN0lvCF7aHAlRA5FoFr&index=3​​•Install Git:        도커 이용  < 덩치가 너무 커서 >https://gitlab.com/autowarefoundation/autoware.auto/AutowareAutohttps://autowarefoundation.gitlab.io/autoware.auto/AutowareAuto/installation.html K-20230228-280938__U20_Foxy 도커★​​Foxy 지원   5월에 종료     그 전에  연습 ,   시작.....욕심부리지 말고 하는데까지 하자.노트북 Spec 부족  /  내 능력 부족  >>   다 부족 ^^​​<ED> "
무인 주차장은 차량번호를 어떻게 인식하는 걸까  ,https://blog.naver.com/tech-plus/223072388412,20230414,"(출처:Prollox)요즘 주차장 출입을 관리하는 분들을 보기 어려워졌습니다. 대신 주차장 입구에 자동차 번호판을 인식하거나 정산을 담당하는 기기가 놓여 있죠. 나갈 때도 그래요. 출차하는 차량 번호를 알아보고 요금을 지불해야 차단기가 올라가요. 이런 무인 주차관제 시스템은 주차장뿐만 아니라 여러 곳에 쓰이고 있는데요. 도대체 어떤 원리로 작동하는 걸까요? ​가까이 온 물체가 자동차라는 걸 어떻게 알지?최근 흥미로운 영상을 하나 접했습니다. 사람이 자동차 번호판을 들고 주차장 입구에 섰을 때, 차단기가 열리는지 확인하는 내용이었는데요. 결과적으로 차단기는 열리지 않았습니다. 즉, 주차장 무인 관제 시스템이 주차장에 진입하려는 물체를 인식한다는 거예요. ​주차장 입구 바닥을 보면 어딘가 다른 점을 눈치채신 분들이 있을 겁니다. 커다란 사각형 모양으로 파인 듯한 자국이 있잖아요. 아스팔트나 보도블록에 가려 내부가 잘 보이지 않지만, 안에는 루프 코일이 들어 있어요. 루프 코일은 금속으로 만들어진 코일입니다. 루프 코일에 전류가 흐르면 자기장이 형성되는데요. 금속인 자동차가 그 위를 지나면 자기장에 변화가 생겨요.  (출처:Linkedin / Alistair Gollop)주차 관제 시스템은 자기장의 변화가 감지되면 물체를 자동차로 인식하는 겁니다. 하지만 루프 코일은 설치도 쉽지 않고 유지보수 비용도 만만치 않다고 해요. 땅바닥 안에 코일을 심어야 하기 때문이죠. 그래서 요즘에는 이미지 안에 물체를 구분해내는 기술인 ‘객체 인식(Object Detection)’ 기술을 활용하기도 합니다.​자동차 번호판은 어떻게 읽는 거지?입구에 서 있는 물체가 자동차라는 건 이제 알았습니다. 다음은 번호판을 인식할 차례겠죠. 그래야 언제 어떤 차량이 주차장에 들어왔고 나갔는지 알고, 요금을 정산할 수 있으니까요. ​일반적으로 주차 관제 시스템은 카메라, 무인 정산기, 차단기로 이뤄져 있는데요. 자동차가 주차장에 가까이 다가오면, 카메라가 차량을 촬영해요. 그런 다음 차량에서 번호판을 분리해서 인식하고, 그 안에 쓰인 번호를 읽어요. 번호판으로부터 차량 정보를 확보하면, 주차 관제 서버로 데이터를 전송합니다. 이때 확보한 데이터로 사용 시간에 따른 요금을 자동으로 매기는 거죠.  (출처:Freepik)그럼 주차 관제 시스템은 어떻게 자동차 번호판을 인식하는 걸까요. 주차 관제 시스템은 LPR(License Plate Recognition)이라는 기술을 이용해서 자동차 번호판을 읽어요. LPR은 ANPR(Automatic Number Plate Recognition)이라고 불리기도 해요. 여기선 짧게 LPR이라고 할게요.LPR은 이미지 안에서 텍스트를 구별하는 광학문자판독(OCR) 기술을 활용합니다.​조금 더 자세한 설명이 필요해차량 이미지를 촬영했다고 해서, 컴퓨터가 번호판에 있는 글자와 숫자를 읽을 순 없어요. 그래서 LPR은 다양한 알고리즘을 사용해서 번호판을 구체화하는데요. 먼저 이미지에서 자동차와 구성 요소를 뚜렷하게 만들기 위한 알고리즘이 사용돼요. 쉽게 말해 이미지에서 노이즈를 줄이거나, 여러 필터를 덧씌워 외곽선이 잘 보이게 만드는 거예요. 컴퓨터가 인식하기 쉽도록요. ​그다음 이미지에서 번호판만 인식하도록 합니다. 경계를 찾아서 객체를 분리하는 SVM(Support Vector Machine)과 같은 알고리즘이 쓰인다고 해요. 자동차 번호판은 규격이 정해져 있죠? LPR에 쓰인 알고리즘은 사전에 여러 번호판 데이터를 학습해요. 그래야 이미지에 포함된 번호판을 정확하게 구분할 수 있으니까요.  (출처:AMANO)이제 거의 끝났습니다. 경계선도 구분했고, 번호판도 분리했으니 남은 건 숫자와 글자를 인식하는 겁니다. LPR 소프트웨어 내부엔 번호판에 쓰이는 숫자·한글 데이터가 미리 입력돼 있는데요. 저장된 데이터와 이미지를 비교하면서 텍스트를 추출해요. ​즉 숫자와 한글이 무엇인지 정확히 이해하는 건 아니에요. 이미지와 가장 비슷한 텍스트로 전환하는 것뿐입니다. 예를 들어 ‘3’이라는 이미지가 있으면, 가장 똑같은 숫자 3으로 인식하는 거죠. 요즘에는 합성곱 신경망(CNN)처럼 딥러닝 기반 인공 신경망이 사용된다고 합니다. 합성곱신경망은 이미지의 특징을 분류하는 데 효과적이고, 이를 스스로 학습할 수 있어요.  (출처:국토교통부)번호판 인식, 또 어디에 쓰일까번호판 인식 기술은 여러 영역에서 사용되고 있어요. 주차장에 사용됐다면 차량 번호를 인식한 다음, 관제 서버에 데이터를 전송하고, 차단기를 열겠죠. 그리고 주차장을 나설 때 사용 시간만큼 주차 요금을 부여할 거고요. 아파트 주차장이라면 요금 정산 과정이 없을 거고, 입주민 차량이 아닐 경우 자동으로 출입을 막잖아요. ​또 LPR 기술은 다양한 영역에 사용되고 있는데요. 과속 단속 카메라나, 방범 카메라가 대표적입니다. 차량 번호만 알면 누구의 것인지, 차종은 무엇인지 한번에 파악할 수 있으니까요. 과속 단속 카메라가 기준 이상 속도로 주행하는 차량의 번호판을 알아내면, 차적조회 시스템에 의해 바로 확인되고 과태료를 물게 되죠.  (출처:Mobisoft)​✔ 무인 주차장 차량 번호 인식 원리1. 무인 주차장은 입구에 설치된 루프 센서로 차량을 감지해요.2. 차량 번호 인식에는 LPR 혹은 ANPR이라 불리는 기술이 사용돼요.3. LPR은 광학문자판독(OCR) 기술을 활용해서 번호판을 읽습니다.4. 차량에서 번호판을 정확히 추출하기 위해 여러 알고리즘이 쓰여요.5. 내부에 저장된 데이터와 번호판 내용을 비교해서 한글·숫자를 추출해요.​​​테크플러스 에디터 윤정환tech-plus@naver.com​  "
"[YOLO] You Only Look Once: Unified, Real-Time Object Dectection ",https://blog.naver.com/qhruddl51/222450721812,20210730,"논문 Joseph Redmon 외 3인, You Only Look Once: Unified, Real-Time Object Dectection, 2016 ( https://arxiv.org/pdf/1506.02640.pdf )​​YOLO (You Only Look Once) YOLO 는 One-shot object dectection 방법이다. object dectection을 수행할 때, 하나의 regression 문제로 보고 단일 네트워크로 물체의 종류와 위치를 알 수 있도록 한다. 따라서 end-to-end 방식으로 파라미터 최적화가 가능하기 때문에 좋은 성능을 기대할 수 있다. ​※ 이미지 처리 문제에 관한 전반적인 내용은 아래의 글을 참고https://blog.naver.com/qhruddl51/222430219604 Image Classification & Object Detection & Object TrackingImage Classification 영상 처리에서 이미지를 보고 그게 뭔지 맞추는 문제이다. 즉, output 이 한 가지...blog.naver.com ​​​1. 장점 및 한계점# 장점- FPS(Frames Per Second)가 45로 Two-shot dectection보다 이미지 처리 속도가 굉장히 빠르다.- Region proposal 기반의 방법처럼 Neural network에서 지협적인 조각만 보는 것이 아니라 전체 이미지를 보기 때문에 (즉,  receptive field가 넓기 때문에) 전체적인 상황 정보를 고려해 결과를 도출한다. 따라서 배경을 object로 잘못 인식하는 경우가 적다. - 일반화 성능이 좋다. 실제 사진 이미지를 학습시킨 모델로 그림 이미지를 테스트하면 DPN이나 R-CNN 보다 더 잘 예측한다. 즉, 학습되지 않은 새로운 이미지에 더 잘 대처하는 robust한 모델이다.  출처 : https://docs.google.com/presentation/d/1aeRvtKG21KHdD5lg6Hgyhx5rPq_ZOsGjG5rJ1HP7BbA/pub?start=false&loop=false&delayms=3000&slide=id.p​​# 한계점 각 그리드는 두 개의 bounding box와 하나의 클래스만을 예측하도록 되어있기 때문에 강한 공간적 제약이 생긴다. 따라서 새 떼와 같이 무리지어 나타나는 작은 물체들을 탐지하기 어렵다. 또한 box 좌표의 예측 정확도가 떨어지는 경우가 많은데, 특히 작은 box가 더 그러하다. 그 이유는 첫째로 downsampling layer로 인한 공간적 정보 손실 때문이고, *두 번째로는 IoU에는 작은 box의 작은 error가 큰 box의 작은 error보다 에 더 큰 영향을 미치는 데 Loss를 처리할 때는 box의 크기에 관계없이 동일하게 처리한다는 점 때문이다.  * - 같은 IoU 인데도, loss를 계산할 때는 작은 box가 더 작은 loss로 계산된다.   - box가 작은 쪽이 IoU가 더 작고 box가 큰 쪽이 IoU가 더 큰데도 같은 크기의 loss로 계산한다.   또한 직관적으로 생각해봤을 때에도 (A)가 (C)보다 더 잘 예측한 것이라고 볼 수 있다.​​​​2. Architecture 출처 :https://docs.google.com/presentation/d/1aeRvtKG21KHdD5lg6Hgyhx5rPq_ZOsGjG5rJ1HP7BbA/pub?start=false&loop=false&delayms=3000&slide=id.p1 )  pretrain 된 cnn layer를 통과시킨다.​2 ) cnn layer 4개와 fully connected layer 2개를 통과시킨다.  =>  Output : 7x7x30 tensor    (activation function으로 leaky rectified linear activation을 사용) leaky rectified linear activationleacky relu출처 : https://paperswithcode.com/method/leaky-relu#3 ) 7x7x30 tensor를 이용해서 각 box의 좌표와 각 클래스에 해당할 확률을 구할 수 있다.2)의 Output인 7x7x30 tensor는 7x7개의 그리드에 대한 정보로 30개의 값으로 되어있다. 30개의 값은 ""box의 4개 좌표 + box의 confidence"" 2개 & ""box에 물체가 존재할 때 특정 클래스에 해당할 확률"" 20개이다. (5*2 + 20 = 30)※ 박스의 좌표는 box의 중심점(x,y)이 있는 cell에서의 위치로 나타낸다. 예를 들어 중심점이 어떤 cell의 중앙에 위치할 경우, box의 x, y좌표는 (0.5, 0.5)이다.   box의 confidencebox에 물체가 존재할 때 특정 클래스에 해당할 확률 (조건부확률)출처 : https://docs.google.com/presentation/d/1aeRvtKG21KHdD5lg6Hgyhx5rPq_ZOsGjG5rJ1HP7BbA/pub?start=false&loop=false&delayms=3000&slide=id.p출처 : https://docs.google.com/presentation/d/1aeRvtKG21KHdD5lg6Hgyhx5rPq_ZOsGjG5rJ1HP7BbA/pub?start=false&loop=false&delayms=3000&slide=id.p​​ 특정 클래스 Score출처 : https://docs.google.com/presentation/d/1aeRvtKG21KHdD5lg6Hgyhx5rPq_ZOsGjG5rJ1HP7BbA/pub?start=false&loop=false&delayms=3000&slide=id.p출처 : https://docs.google.com/presentation/d/1aeRvtKG21KHdD5lg6Hgyhx5rPq_ZOsGjG5rJ1HP7BbA/pub?start=false&loop=false&delayms=3000&slide=id.p​4 ) Non-Maximum Suppression으로 7*7*2개의 box에서 중복된 bounding box를 제거한다.98 개 박스에서 하나의 클래스씩 봤을 때, 가장 score(해당 box가 특정 클래스일 확률)가 높은 box를 기준으로 그 box와 IoU가 0.5 이상인 box들의 score를 0으로 만든다. 그 다음으로는 두번째로 가장 점수가 큰 박스를 기준으로 그 box와 IoU가 0.5 이상인 box들의 score를 0으로 만든다. 이렇게 모든 클래스에 대해서 진행하고 나면 대부분의 score 값이 0이 된다. 이제는 하나의 box를 놓고 봤을 때, 가장 score가 큰 class의 점수가 0보다 크면 그 class를 box의 class라고 예측한다. 물론, 가장 score 가 큰 class의 점수가 0 이하이면 그 box는 유효하지 않게 된다.  출처 : https://docs.google.com/presentation/d/1aeRvtKG21KHdD5lg6Hgyhx5rPq_ZOsGjG5rJ1HP7BbA/pub?start=false&loop=false&delayms=3000&slide=id.p​참고 > https://docs.google.com/presentation/d/1aeRvtKG21KHdD5lg6Hgyhx5rPq_ZOsGjG5rJ1HP7BbA/pub?start=false&loop=false&delayms=3000&slide=id.p YOLOYOLO You Only Look Once: Unified, Real-Time Object Detection Joseph Redmon, Santosh Divvala, Ross Girshick, Ali Farhadidocs.google.com ​​​3. Loss function  Loss function​▶ optimization을 위한 전체 loss function을 하나씩 떼어서 살펴보면 다음과 같다. ​​# object 탐지 유무에 대한 error object 탐지 유무에 대한 error이다. box에 대한 confidence score 의 SSE(Sum-squared Error)를 모든 그리드(30개)에 대해 합한다. object 가 있을지 없을지 confidence 값을 내는 데 있어서 loss 신호를 받을 때, object 가 없어서 없다고 예측 하는 box가 많다. 따라서 object가 있다고 예측한 box가 object가 없다고 예측한 box보다 object 탐지 유무에 대한 error에 대한 기여하는 비율이 적다. 물체가 있는데 없다고 한 것에 대한 loss의 경우는 물체가 없을 경우의 loss에 묻혀서 묻히게 되는 것이다. 따라서 object가 있다고 예측하는 경우와 없다고 예측하는 경우의 loss의 비율을 적절히 균형 맞춰주기 위해 object가 없다고 예측한 경우에 λnoobg=j = 5를 곱해준다.​​# classification error classification error를 의미한다. 살아남은 박스가 있는 그리드에서 그 박스의 예측 class score들과 정답 클래스 score의 차이를 제곱한 것을 모든 클래스에 대해 더한 것이다. ​​# localization error localization error이다. 4개 좌표의 SSE(Sum-squared Error)로 classification error와의 크기 균형을 맞춰주기 위해 λcoord = 0.5를 곱해준다. ​​​  Reference ● 논문 Joseph Redmon 외 3인, You Only Look Once: Unified, Real-Time Object Dectection, 2016 ( https://arxiv.org/pdf/1506.02640.pdf )​● https://velog.io/@suminwooo/RPNRegion-Proposal-Network-%EC%A0%95%EB%A6%AC RPN(Region Proposal Network) 정리velog.io ● https://yeomko.tistory.com/19?category=888201 갈아먹는 Object Detection [5] Yolo: You Only Look Once지난 글 갈아먹는 Object Detection [1] R-CNN 갈아먹는 Object Detection [2] Spatial Pyramid Pooling Network 갈아먹는 Object Detection [3] Fast R-CNN 갈아먹는 Object Detection [4] Faster R-CNN 들어..yeomko.tistory.com ​● https://docs.google.com/presentation/d/1aeRvtKG21KHdD5lg6Hgyhx5rPq_ZOsGjG5rJ1HP7BbA/pub?start=false&loop=false&delayms=3000&slide=id.p YOLOYOLO You Only Look Once: Unified, Real-Time Object Detection Joseph Redmon, Santosh Divvala, Ross Girshick, Ali Farhadidocs.google.com ​● https://github.com/aladdinpersson/Machine-Learning-Collection/blob/master/ML/Pytorch/object_detection/YOLO/model.py Machine-Learning-Collection/model.py at master · aladdinpersson/Machine-Learning-CollectionA resource for learning about ML, DL, PyTorch and TensorFlow. Feedback always appreciated :) - Machine-Learning-Collection/model.py at master · aladdinpersson/Machine-Learning-Collectiongithub.com ​ "
[OpenCV] Object Traking (1) - Centroid Tracking ,https://blog.naver.com/cho-yani/222116493599,20201015,"이 포스트는 아래 사이트의 번역 & 정리본임을 밝힌다.https://www.pyimagesearch.com/2018/07/23/simple-object-tracking-with-opencv/  [OpenCV] Object Traking (0)이 포스트는 아래 사이트의 번역 & 정리본임을 밝힌다.https://www.pyimagesearch.com/2018/07/23/simp...blog.naver.com  Centroid Tracking Algorithm- OpenCV library를 사용하는 간단한 object tracking algorithm- 직전 frame에서 tracking된 object의 중심과 새로운 frame의 object의 중심간의 거리를 잰다. 1. Bounding box의 좌표를 받아 중심 계산하기Centroid tracking algorithm은 우리가 모든 frame에 대한 bounding box 좌표를 전달받았다고 가정한다.이때 bounding box가 모든 frame마다 나오기만 한다면 어떤 object detector를 사용하든 관계 없다.Bounding box 좌표를 받았으면 중심 좌표 (x, y)를 계산한다.그리고 algorithm의 맨 첫 단계이므로 각각의 bounding box에 unique ID를 부여해준다. 2. 기존 object와 새로운 bounding box간의 거리 계산이제부터는 영상의 모든 frame에 대해서 1번을 시행하여 중심 좌표를 계산한다.단, unique ID를 부여하지 않고, 이전 frame에서 있던 object인지 또는 새로 나타난 object인지부터 판단한다.그러기 위해서 위의 그림에서 화살표로 나타난 거리를 모두 계산한다. 3. 기존 object의 (x, y) 좌표 업데이트Centroid tracking algorithm은, tracking 중인 object가 frame이 지나감에 따라 움직인다면 2에서 계산한 거리 중 가장 짧은 곳으로 옮겨갔을 것이라 가정한다.따라서, frame이 진행될 때마다 최소 거리를 연결하면 object tracker를 구축할 수 있다.이 때, 아무것도 연결되지 않은 object가 남을 수 있다. 4. 새로운 object 등록tracking 중인 object보다 detection이 더 많을 때는 새로운 object를 등록해야 한다.object를 등록한다는 것은 그냥 traking 중인 object 리스트에 추가한다는 것을 의미한다.새로운 object ID를 부여하고, bounding box의 중심 좌표를 저장해두는 것이다.그 후에는 frame이 지남에 따라 2번부터 반복하면 된다. 5. 지나간 object 삭제object tracking algorithm이라면 응당 사라진 object를 잘 처리해줘야 한다.처리하는 방법은 object traking의 용도에 따라 매우 달라진다.여기서는 단순하게 그냥 다음 frame에서 matching되지 않는 object를 삭제하도록 하겠다. ImplementingPython source code는 위의 원본 글에 가면 다운받을 수 있다. 한계 & 단점object detection이 매 frame마다 실행되어야 함color thresholding이나 Haar cascade 등의 object detector는 괜찮음.하지만 computationally expensive하거나(HOG + linear SVM) resource가 제한되어 있다면(딥러닝 기반) 문제가 됨. frame 간의 중심 좌표가 반드시 가까이 있어야 한다는 가정3에서의 가정이다.만약 object가 서로 겹쳤다면, object ID가 바뀔 수 있다.사실 이러한 overlapping / occluded object problem은 centroid tracking 뿐만 아니라 다른 tracker에서도 문제가 된다.하지만 centroid tracking에서는 이러한 현상이 더 뚜렷해진다는 것이 문제다. 활용영상이 어느 정도 통제되어서 overlapping을 걱정할 필요가 없거나, object detector가 실시간으로 돌아갈 수 있으면 centroid tracking을 사용할 수도 있을 것이다.  [OpenCV] Object Traking (2) - built-in algorithms이 포스트는 아래 사이트의 번역 & 정리본임을 밝힌다.https://www.pyimagesearch.com/2018/07/23/simp...blog.naver.com "
[4] Video Visual Relation Detection via Multi-modal Feature Fusion ,https://blog.naver.com/jgyy4775/222548241721,20211025,"논문 링크: https://dl.acm.org/doi/pdf/10.1145/3343031.3356076?casa_token=LjNHlyyGyH0AAAAA:b_rt89X9KYzbXFbLlNYGEKsS7qzL283c_st__SmuQb9Gy9PhGgHtbHpnkKmNiWCqL8Kjnm2Y_hMV4w Video Visual Relation Detection via Multi-modal Feature Fusion | Proceedings of the 27th ACM International Conference on Multimediaresearch-article Video Visual Relation Detection via Multi-modal Feature Fusion Share on Authors: Xu Sun , Tongwei Ren , Yuan Zi , Gangshan Wu Authors Info & Claims MM '19: Proceedings of the 27th ACM International Conference on Multimedia October 2019 Pages 2657–2661 https://doi.org/10.1145/33430...dl.acm.org 이 논문은 이전 논문에 이어서 비디오 관계 탐지에 관한 논문입니다.  이전 논문들은 VidVRD 데이터 집합을 사용하였지만 본 논문부터는 VidOR이라는 새로운 데이터 집합을 중점적으로 사용합니다.​​<Introduction>비디오에서의 관계 탐지를 위해 다양한 유형의 multi modal feature들을 사용한 논문입니다. 이전 논문들은 비디오내에 등장하는 물체들의 visual feature에 의존하던 것을 개선하여  visual feature이외에 다양한 feature를 사용합니다.추가로, 이 논문에서는 [1] 논문의 object trajectory proposal 단계의 개선 방안 또한 제안합니다.​​<Model> 전체 구조도1) Object trajectory proposal[1] 논문과 마찬가지로 입력 비디오를 일정한 크기로 분할하는 segment방식을 사용합니다. 이 단계에서 [1]의 논문과 다르게 추가된 기법들은 아래와 같습니다.​- Flow-guided feature aggregation    => 주변 frame들을 이용한 feature extract- seq-NMS    => Frame간의 등장 물체들의 갑작스런 변화를 해결- Kernelized correlation filter    => High-speed tracking​2) Predicate recognition주어 물체와 목적어 물체의 위치 정보와 language feature를 동시에 사용합니다. ​ ▶ Spatio-Temporal feature   - relative location feature: 주어 물체와 목적어 물체의 상대 좌표, 크기    - motion feature: 시작 프레임과 끝 프레임 사이에서 주어 물체와 목적어 물체의 상대 위치  ▶ Language context feature- GoogleNews 데이터 집합을 이용한 Word2Vec모델을 이용- 주어 물체와 목적어 물체의 예측 class 이용​최종적으로 위의 두 feature를 각각 서로 다른 분류기를 통과시켜 관계 클래스 분포도를 얻습니다. 얻어진 관계 클래스 분포도는 아래 식을 이용하여 fusion합니다. 이 fusion된 정보를 이용해 최종적으로 관계를 예측합니다. ​<Result>데이터 집합은 VidOR을 사용합니다. 아래는 VidVRD와 VidOR을 비교한 표입니다. 데이터 집합의 수가 확연히 증가하였고 그로인해 instance의 수도 훨씬 많아졌습니다. 뿐만 아니라 탐지하는 관계의 유형에도 차이가 있습니다. VidVRD는 ""jump_left"", ""jump_right"" 처럼 ""jump""라는 하나의 관계도 방향에 따라  뒤에 여러 부사를 붙혀 관계 클래스를 다양화하였으나 VidOR에서는 이를 개선하여 순수하게 관계 클래스를 다양화 하였습니다.  ​아래는 실험 결과입니다. 제안하는 모델이 모든 평가 지표에 대해 우수한 성능을 보이는 것을 알 수 있습니다.​​<결론>- FGFA, seq-NMS, KCF방법을 이용하는 object trajectory detection module 제안- multi-modal feature를 이용한 relation prediction module 제안 "
SMOT: Single-Shot Multi Object Tracking ,https://blog.naver.com/yeajin522/222317246318,20210420,"​Abstract​기존 detection에 의존하던 tracking 은 detection 의 오류에 따라 성능에 영향을 준다. 이에 tracking by re-detection 방법이 제안되었는데 [37,43,17] 본 논문에서도 이를 참조한 방식을 제안함.본 논문에서는, SSD detector 와 결합하여 tracking anchor assignment module을 제안하였다. 이 방식을 통해 frame 당 일정한 runtime으로 tracklets을 생성할 수 있다. 그리고 online trackilet 에 light-weighted 연결을 해준다. MOT17, Music Videos, Hannah 데이터 셋에 대하여 성능을 제시하였다. ​  ​Introduction​​ Fig. 1. 제안 된 SMOT는 두 단계로 구성된다. 첫 번째 단계는 이전 frame 에서 temporal and spatial 상관 관계를 탐색하여 일시적으로 연속적인 tracklets 을 생성한다. 두 번째 단계는 tracklets 의 online-linking 을 수행하여 각 사람에 대한 face track을 생성.​기존의 multi-object tracking algorithms 은​1. frame 당 object detector 를 학습시켜 각 frame 의 object bounding box 를 localize 시킨다.2. tracklet generation으로, detection 결과를 merge 하여 short-term cues 를 기반으로 tracklets set을 생성한다. (short tracks)이러한 short term cues 는 motion, apperance feautres, geometrical affinity 를 포함한다. 3. long-term trackles 을 연결하여 전체 비디오의 tracklets 을 linking 하고 long-term tracking 한다. ​위와 같은 3단계 tracking 은 quality가 좋다. 그러나 다음과 같은 문제점이 있다.​1. object detection 과 tracklet generation의 분리는 motion blurriness, large pose variations, video compression artifacts 와 같은 문제점에 대해 detector 의 능력에 크게 의존하는 tracking quality를 가진다. 이러한 검출기는 대부분 still-image(정지 된 이미지) 를 위해 설계되고, frame 별로 작동된다. temporal context 를 활용하여 이러한 것들을 극복하기 어렵다.2. object bounding box 가 detector 에 의해 누락 된 후에는 temporal context 를 통해 object bounding box를 복구할 수 없다. 3. tracklet generation process 에서 사용되는 low-cost matching techniques (ex: IoU matching) 은 특히 occlusion 의 경우에 대해 잘못 된 matching을 하기 쉬우므로 multi object 가 있는 tracklet을 생성하게 된다. ​​본 논문에서는, 위와 같은 문제들을 해결하기 위해 single-shot multi object trackier(SMOT)를 제안한다. 위와 같은 세가지 단계와 반대로, 두 개의 cascaded stages, 즉 tracklet generation과 tracklet linking stage로 구성된다. ​[1] 에서 제안 된 tracking by re-detection 아이디어에 따라 tracklet generation stage를 설계했다. 이는 detection과 tracking 을 동시에 수행한다. SSD detector 에서 각 frame 의 object 수에 관계 없이 일정한 runtime을 낸다. ​tracklet generation stage: object 를 detecting 하고, 다음 frame 에 detection 결과를 전파하여 같은 object 를 re-detect 하는 것으로 시작한다. 연속된 두 프레임 사이에 significant motion이 존재하지 않는다고 가정하면, 추가 적인 matching 없이 tracklets 을 설정할 수 있다. 본 논문에서는 SSD 검출기로 re-detection 을 하였다. 이는 low-latency requirment requirement of video face understanding 를 기반으로 한다. SSD detector 에서 proposal classification and regression stage는 존재하지 않는다. (?)본 논문에서는 tracking anchor assignment module 을 제안했다. SSD detector 가 sliding window 방식을 사용하는 사실에서 아이디어를 얻었다. ​한 frame 에서 detect 된 bonding box 가 하나 또는 여러 anchor 의 조합으로 approximate 될 수 있으므로 다음 frame 의 re-detection 에 사용 될 수 있다는 사실을 활용한다. tracklet linking 단계를 위해 apperance features 에 따라 새로운 tracklets 을 기존 tracklets 과 연결하는 간단한 online linking 알고리즘을 사용한다. 이는 SMOT tracker 전체 runtime 에서 최소한의 비용으로 잘 작동할 수 있게 해주었다. ​​  Related Works​deep learning-based object detection​딥러닝 검출기는 two-stage detector 와 one stage detector로 나뉜다.two-stage detector 는 먼저 detection proposals 를 생성한 다음 location regression 과 함게 classification 을 수행한다. two-stage detector 의 문제는 탐지할 수 있는 multi instance 가 있을 때 runtime 이 극적으로 증가한다는 것이다.​one stage detector는 feature map 에서 앵커가 직접 생성 된 다음 최종 detection 결과로 regressed 된다. 앵커 기반 regression은 instance number 에 구애되지 않으므로 얼굴 detection과 같은 multi-target detection이 필요한 장면에 잘 적용될 수 있다. ​Multi-object tracking​[1]에서는 tracking 과 detection 의 linking을 entangled way를 이용하여 구축한다. detection 결과를 저아하고, 새 frame 에서 대상의 positions 을 regression 시키기 위한 새 detector 를 제안한다. 이 접근 방식은 대상이 large motion을 가지고 있지 않기 때문에 새로운 regressed position은 동일한 object 에 해당한다는 가정을 가지고 있다. 이는 bounding boxes 쌍이 additional association 없이 자동으로 tracklet을 형성한다는 것을 의미한다. ​ Fig. 2. Frame Ft 가 주어지면 먼저 motion estimator 를 사용하여 identity i 와 관련 된 기존 anchor box At-1_i 를 해당 single shot detector feature map 의 new location 에 정렬한다.  그리고, regressor 와 classifier 를 사용하여 bounding box 예측 Bt_i 및 confidence score Ct_i 를 생성한다. Ct_i는 잠재적으로 occluded 된 track 을 제거하는데 사용되며, Bt_i 는 anchor box At_i를 업데이트 하는데 사용된다. 동시에, 얼굴 detector 는 set of detections 을 제공한다. detecion에 set of active track의 bonding box 와의 IoU가 낮을 경우 new track을 초기화 한다. ​​   Pipeline Overview​SMOT에 대한 입력은 T frames 의 연속 시퀀스인 {Fi}T_t=1 으로 표시되는 video 이며,출력은 input sequence 의 object location을 나타내는 a series of tracklets {Si} 이다. 여기서 하나의 tracklets은 동일한 pathing ID i 를 가진 frame 에서 object bounding boxes Bti 로 표시된다. 그리고 각 bounding box format은 top-left corner (xt_i, yt_i) , height ht_i, width wt_i 이다. ​SMOT는 Fig.1. 과 같이 두가지 구성요소의 cascade 방식으로 작동한다.1. video frame 에서 tracklet 을 생성하는 부분. tracklet Lj는 동일한 object instance의 일련의 bonding box를 여러 연속 프레임에 캡슐화 하는   short-term consecutive bounding box sequence 이다. SMOT 에서 tracklet generator는 normal SSD detector 에서 변환된다.timestep t 에서 tracklet generator 는 현재 frame Ft 를 관찰한다. 또한 이전 frame 에 존재했던 tracklets 을 가져온다. 이전 frame 에서 정보를 전파하여 tracklet 을 확장함으로써 알고있는 objects 를 새 frame 에서 다시 감지한다. 이 작업을 re-detection이라고 한다. re-detection 된 개체는 id 를 상속받아 기존 tracklet 을 확장하는 데 사용된다. re-detection 하지 못한 기존 tracklet 은 종료되고, traklet generator 에서 해제된다. 또한 tracklet generator는 새 frame 에서 normal detection을 수행한다. result bonding boxes는 중복을 피하기 위해 먼저 기존 traklet 과 결합되며, 나머지 독립 bounding box는 new tracklet 을 시작하는데 사용된다.​2. tracklet linkingtracklets을 long-term 으로 연결하는 online 방식으로 작동한다. tracklet link는 새로 형성 된 트랙렛에 대한 appearance embedding feature 를 추출하여 기존 tracklet 과 일치시킨다. 그런 다음 일치하는 tracklet 이 동일한 track ID 로 해당 tracks에 연결된다. Non-match tracklet은 new track ID로 시작한다. ​​​  Tracklet Generator​tracklet generator 는 tracking by redetection 의 개념에 바탕을 두며, detection 과 tracking 을 동시에 수행한다.​Faster-RCNN과 같은 경우 2단계로 object detection을 수행하는데, bounding box regressors 가 대략적인 object의 위치와 모양(박스) 을 교정해준다. 이전 frame 에서 tracked 된 object 를 re-detect 하고 tracklets을 자연스럽게 연장할 수 있다.[1]에서 이 아이디어는 COCO dataset에서 train 된 faster R-CNN에 의해 구현되었다. 이전 frame 에서 tracked 된 object의 bounding boxes는 classification 및 regression을 위한 additional ""tracking proposals"" 으로 다음 frame 으로  propagte 된다. 이는 person tracking 에 효과가 있지만, runtiome이 object 수에 따라 linearly grow 한다는 단점이 있다. multi-object tracking에 문제가 된다. ​* SMOT 에서는 tracklet generator를 설계하였는데, redetection 방식에 의한 tracking을 따르지만, backbone으로 SSD를 사용한다.프레임에 있는 object 수에 관계 없이 일정한 runtime을 가지도록 했다. 그러나 SSD 에는 Faster-RCNN과 같은 proposal classification이나 regression process 가 없기 때문에 explicit proposals 없이 frame 간에 tracked object location을 propagate 하는 것을 가능하게 하는 새로운 tracking anchor assignment module 을 제안한다. ​  Redetection with Tracking Anchors ​Faster-RCNN 과 달리 SSD 는 proposals concept이 없다. 대신, anchors를 사용한다. 따라서, single shot detectors 는 anchor의 이전 locations 과 shapes 를 이용하는 것이다. 그런 다음, network는 image features 를 기반으로 각 proposals 에 대해 prediction 한다.이를 통해 이전 frame 에서 tracked object 로 부터 bounding boxes 를 현재 frame 에서 복원할 수 있다. tracked object location를 proposals 로 삽입하는 대신 미리 정의 된 anchor 집합을 사용하여 tracked object의 대략적인 location을 근사화 할 수 있다. 이 set of anchors 를 ojbect 의 tracking anchors 라고 부른다. ​tracking anchors 의 output을 aggregating 하여 proposals로 생각함으로써 redetection을 추정할 수 있다. ​Ft 를 frame이라고 할 때, tracklet{Lt_j}에 할당 된 여러 object bounding boxes 를 detect 한다.Ft+1 frame의 경우, object 를 다시 detection 하기 위해 다음 단계를 거친다. tracklet {Lt_j} 의 각 bounding box {bt_j} 에 대해 함수 b^t+1_j = H(bt_j) 에 의해 t+1 에서의 location과 shape 을 예측한다.H 의 가장 간단한 형태는 B^t+1_j = Bt_j 를 의미하는 identity function 이 될 수 있다.모든 detector anchor candidates 에서, K anchors {ajk}k_k=1 set 를 추출한다. sclae 및 location spaces 는 모두 Bt_j 에 가깝다. 이 set of anchors 는 Bt_j의 tracking anchors 역할을 할 것이다. 이들은 각각 tracking anchor 할당 함수 A에 의해 결정되는 weight coefficient 계수 wjk=A(ajk, B^t_j) 를 전달한다.그런 다음 다음과 같이 정의 된 Ft+1 frame 의 bounding boxes shape {Bt+1_jk} 와 detection confidence scores {ct+1_jk} 의 detection output 을 aggregating 하여 redetection 을 수행할 수 있다. ​ 여기서 Bt+1_j 는 tracked object j 의 Ft+1 frame 에서 redetected 된 bounding box shape 이며 ct+1_j 는 confidence score 이다. ​tracking specific threshold σactive  으로 결정되며, 이러한 anchors 에 대해 가능한 두가지 결과가 있다.1) ct+1_j >= σactive 인 경우 , 유효한 redetection 이다. bounding box Bt+1_j 는 tracked object 의 새로운 존재로 간주된다.2) ct+1_j < σactive 인 경우, redetection 은 실패 한 것으로, tracklet 은 종료 된 것으로 간주되며, tracklet generator 에서 해제된다. ​  Tracking anchor assignment strategy​SMOT framework 에서, tracking anchor assignment module 을 구축했다.가장 간단한 방법은, K=1 일때, IoU가 가장 높은 B^t+1_j 의  at+1_j  anchor 를 사용하는 것이다. ​Experiments 에서, ""single-assignment""으로 알려진 간단한 전략이 face 나 person tracking 과 같은 작업에 대해 잘 수행된다는 것을 발견했다.K>1 일떄, B^t+1_j 의 top-K IoU anchors를 취하고 그들의 IoU 값을 weight 로 사용할 수 있다.이것을 "" multi-assignment "" 라고 부른다.이는  learned weighting 와 같은 other strategies 를 가져와서 사용할 수도 있다 (future work. 학습된 웨이트 사용하는 것을 말하는듯)​  Is the prediction by the tracking anchors reliable? (tracking anchors 를 신뢰할 수 있는가?)​small set 에 experiments 를 수행.IoU 와 GT bounding boxes 간의 anchors predictions 을 확인한다. ​anchors' predicted bounding boxes 의 대부분이 GT 의 bounding boxes 를 가리키는 것을 확인.정량적으로, Fig. 3. 에서 bonding box 의 초기 이동이 object 를 올바르게 detect 하는 능력에 어떤 영향을 미치는지 보여주었다. ​Fi 에서 bonding box 의 w,h 를 이용하여 영상에 multi-ratios 를 추가해 re-detection을 하였다.Fig. 3. 에서 detector 가 0.33의 상대적인 shift ratio 에서도 object position을 정확하게 위치시킬 수 있다는 것을 확인했다.​즉, 연속된 두 frame 간의 작은 움직임만(small motion) 있는 경우 새 frame 에서 object 를 정확하게 추적할 수 있다. ​tracking anchors 가 track 할 object 의 대략적인 location에 가까울 경우, 새로운 location을 올바르게 찾을 가능성이 높으므로, re-detection을 위해 이러한 tracking anchors' 의 predictios 에 의존할 수 있다. ​ Fig. 3. anchor regression 의 Robustness 실험.  (a-d 는 각각 0.1, 0.25, 0.33, 0.45 의 shift ratio 에 해당) 빨간색은 GT, 노란색은 shift box, 녹색은 redetection box 이다.  The workflow of tracklet generation.​tracklet generator 의 workflow (1) first frame f0:  SSD 를 실행, 모든 object detection results {B0_i} 를 가져온다. 각 bounding box 에 대해 하나의 tracklet ID를 생성한다.(2) Framework F1 에서 track 된 object {b^1_j} 의 location과 shapes 를 예측한다. tracking anchor assignment module 을 통해 trakcing achors {a0_ik} 와 그들의 weight{w0_ik} 를 가져온다. (3) Frame F1 에 detector 를 수행한다. Eq(1) 에 따른 redetection 결과를 가져오고, tracking specific threshold 인 σactive 에 따라 tracklet 을 연장할지 또는 정지할지 결정한다.(4) tracked bounding boxes 에 IoU가 높은 새로운 detection results 를 suppressing 하여 redetection bonding box 에 대해 non-maximal suppression (NMS) 를 수행한다. ​(5) remaining new detection results 를 위해 각각의 detection 결과에 대해 새로운 tracklet 을 만든다.​(6) 나머지 frame 에 대해 2-5 단계를 연속으로 반복한다. ​  Motion Assisted Re-Detection​SMOT tracklet generator 의 또 다른 중요한 모듈은 tracked object의 대략적인 location을 예측하는 함수 H 이다. H의 가장 간단한 형태는 small motion을 가정하는 identity function 이다.video에 large motion 이 포함되어 있으면 이 가정이 깨지고, tracklet generator가 target을 잃기 쉬워진다.따라서, bonding box prediction 에서 motion 을 고려하는 것이 중요하다.본 논문에서는, dense optical flow field 를 이용하여 tracked object 의 함수 H를 만든다.특히, Ft 와 Ft+1 사이의 Optical flow field Dt 와 각 픽셀 위치의 두 요소 변위벡터 (수평 및 수직) ->d(h,w) 를 고려할 때, bounding box , Bt_j (Frame Ft+1에서)  의 새로운 location 을 예측할 수 있다.  여기서 연산자 ⊕(B, ->d) 는 bounding box B의 중심을 벡터 ->b 에 의해 shift 하며 function area(B) 는 bonding boxes 의영역을 제곱 픽셀로 한다. 이러한 형태의 H 를 지원하는 tracklet 생성 process 를 motion-assisted redetection 이라고 한다.​​  Online Linking of the Tracklets​tracklet generator 의 output 은 vidoe 에서 occlusion 및 일시적인 disappearance 로 인해 대개 중단되는 short-tracklets 이므로online tracklet linking module 을 설계하여 tracklet 을 final object trackes 로 그룹화 한다.​링커는 기존 track {Si} 를 가져오며, 여기서 각 track 은 이 track 의 object 에서 embedding model φ 의 output 을평균하여 생성 된 ID embedding φ(Si) 로 표현된다.특정 time step t 에서 track 에는 두 가지 상태가 있을 수 있다.1) 이 track 은 현재 frame Ft 의 acktive tracklet 에 연결되어 있다.2) 이 track 에는 active tracklet 이 없다. Fig.4. 에 Identity linking process 를 보여준다. Ft 에 나타나는 각 new tracklet Ltj appearing 에 대해 상태 2) 의 tarcklet 과 matching시키려 한다. matching 시키기 위해 먼저 Ltj 의 object 에서 appearance embedding 을 추출한 다음, Ltj 와 track 사이의 pariwise match cost 를 계산한다.track 에 new tracklet 이 연결되면 다음  frame 에서 상태  1) 로 이동한다.bipartite matching problem 을 해결함으로써, matching track  을 tracks 에 연결할 수 있다. 기존 track 과 일치하지 않는 new tracklet 의 경우, 각 track 에 대해 new track 을 생성한다. ​ Fig. 4. tracklet-tracking linking 을 위한 기능. feature banck를 online update 한다.  φ_ti 는 t time 에 track i 의 identity embedding 을 의미한다. 각 track 이 하나의 feature 를 갖는 identity embeddings bank 를 저장해둔다. new tracklet 이 있으면, 기존의 banck 에서 identitly에 맞추거나, bandk 에 새로운 tracklet 을 추가한다.  ​​이 tracklet linker 는 시간 경과에 따른 occlusion 및 disappearance 사례를 처리한다.한편, 일반적으로 [5, 30] 과 같은 신경망인 embedding model 을 사용하여 appearance embedding 을 추출해야 하며, 이는runtime 비용을 추가한다. 그러나 두 가지 이유로 인해 이 추가 비용은 최소화 될 수 있다.1) 새로 생성 된 tracklet에 대해서만 linking 이 수행된다.tracklet 은 embedding 을 re-extract 할 필요가 없다.​2) 비디오에서와 같이 track 에 대한 appearance embedding 추출은 각 frame 에서 수행 될 필요가 없다.일반적으로, object 의 외관이 느리게 변화한다 (가정) ​  implementation​ResNet34 를 개인 및 얼굴  추적 작업을 위한 백본 아키텍처로 사용하는 SSD를 사용. confidence threshold 인 σdet 은 0.9, σtk 는 0.4로 사용.redetection 에 사용 되는 NMS threshold는 0.6, IoU threshold 는 0.3 으로 설정. 사용자 추적 작업의 트랙렛 생성기는 탐지 임계값 0.9와 추적 임계값 0.4를 사용하여 경계 상자 품질의 높은 정밀도를 유지합니다.​  Dataset​MOT17 dataset 으로 person tracking 을 하였으며, Hannah, MusicVideo dataset 으로 face tracking 을 진행.​​  result​ ​ "
Research |  Face Detection ,https://blog.naver.com/juntf/222120485354,20201019,"​* OpenCV::Face-Detection- C++/Java/Python 등 버전으로 제공; - 다양한 호출 방식을 제공하면서 특히 C++방식의 사용은 Production환경에서의  얼굴인식의 서비스 제공 가능; - 정확도는 낮은 편이지만 기타 서비스와 조합하는 방식으로 Production환경에서도 충분히 활용 가능; https://docs.opencv.org/3.4/db/d28/tutorial_cascade_classifier.html OpenCV: Cascade ClassifierWe will learn how the Haar cascade object detection works. We will see the basics of face detection and eye detection using the Haar Feature-based Cascade Classifiers We will use the cv::CascadeClassifier class to detect objects in a video stream. Particularly, we will use the functions: cv::Cascade...docs.opencv.org ​​* Object-Detection | Face-Detection - Face-Detect 관련된 학습 데이터를 준비하여 직접 학습하여 사용; - 해당 영역은 Object-Detection을 직접 학습하고 사용하는 형태로 Tensorflow에서 제공하는 API를 활용하면 어렵지 않게 모델 만들고 사용가능; https://github.com/tensorflow/models/tree/master/research/object_detection tensorflow/modelsModels and examples built with TensorFlow. Contribute to tensorflow/models development by creating an account on GitHub.github.com "
"L01 Dev Env    < Autoware.auto Foxy 실행확인 , velodyne_nodes Run_OK > ",https://blog.naver.com/mdc1020/223054060668,20230324,"Docker U20   /   우분투22-로컬​https://www.youtube.com/watch?v=XTmlhvlmcf8&list=PLL57Sz4fhxLpCXgN0lvCF7aHAlRA5FoFr&index=1 https://gitlab.com/ApexAI/autowareclass2020/-/blob/master/lectures/01_DevelopmentEnvironment/devenv.md​CMD Summary:                                   튜토리얼 중 하나,   3D perception stack   참고 # velodyne_nodes: 실행 안 되지만,   흐름 파악udpreplay ~/data/route_small_loop_rw-127.0.0.1.pcap -r -1rviz2 -d /home/${USER}/autowareclass2020/code/src/01_DevelopmentEnvironment/aw_class2020.rvizros2 run velodyne_nodes velodyne_cloud_node_exe __ns:=/lidar_front __params:=/home/${USER}/autowareclass2020/code/src/01_DevelopmentEnvironment/velodyne_node.param.yamlros2 run robot_state_publisher robot_state_publisher /opt/AutowareAuto/share/lexus_rx_450h_description/urdf/lexus_rx_450h.urdf# 주요 Node 3ea Runros2 run point_cloud_filter_transform_nodes  point_cloud_filter_transform_node_exe __ns:=/lidar_front __params:=/opt/AutowareAuto/share/point_cloud_filter_transform_nodes/param/vlp16_sim_lexus_filter_transform.param.yaml __node:=filter_transform_vlp16_front points_filtered:=/perception/points_inros2 run ray_ground_classifier_nodes ray_ground_classifier_cloud_node_exe __ns:=/perception __params:=/opt/AutowareAuto/share/autoware_auto_avp_demo/param/ray_ground_classifier.param.yamlros2 run euclidean_cluster_nodes euclidean_cluster_exe __ns:=/perception __params:=/opt/AutowareAuto/share/autoware_auto_avp_demo/param/euclidean_cluster.param.yaml K-20230324-455663__최종 다 실행Screenshot from 2023-03-24 18-52-30__Assy            실제 작업​​•Autoware.Auto Tutorial                     3D perception stackhttps://autowarefoundation.gitlab.io/autoware.auto/AutowareAuto/perception-stack-howto.htmlhttps://aicurious.io/notes/autoware-course/lecture1​유튜브에 있는 Object Detect 은 위 튜토리얼 따라하면 됨velodyne_nodes  Run_OK Screenshot from 2023-03-24 13-56-11__Example_ST★★.png                         맨 상단: 튜토리얼   /   중하단:  유튜브​Next:•Starting and testing the ★behavior planner / 다음: ★global plannerhttps://autowarefoundation.gitlab.io/autoware.auto/AutowareAuto/behavior-planner-howto.html ros2 topic pub /planning/goal_pose geometry_msgs/msg/PoseStamped '{header: {frame_id: ""map""}, pose: {position: {x: -98.56259155273438, y: 60.99168395996094}, orientation: {z: -0.42844402469653825, w: 0.9035683248663778}}'} --once ​​​​​Autoware.auto 실행 확인  / 튜토리얼 Test----------------------------------------------------------------------------------------------------------------------- 03.24(금) 2023  _03개발 환경은 Husarion: Panther 에서 세팅 끝                     < 여기서는 확인 용도 >https://blog.naver.com/mdc1020/223051592709​001_ade Install:  코드 다운로드...참고용 ade entergit clone https://gitlab.com/autowarefoundation/autoware.auto/AutowareAuto.git 002_ros Install: 작업 없음​003_Autoware.Auto Install # 설치 확인thez@ade:~$ ls /opt/AutowareAuto/COLCON_IGNORE  LICENSE           local_setup.sh            local_setup.zsh  setup.sh   srcinclude        local_setup.bash  _local_setup_util_ps1.py  setup.bash       setup.zshlib            local_setup.ps1   _local_setup_util_sh.py   setup.ps1        sharethez@ade:~$  004_Run object detection demovelodyne_nodes  실행 안 되니깐   >>   튜토리얼:  3D perception stack  참고 # 폴더 구성 참고ade$ ls                          # First>> AutowareAuto  autowareclass2020  data                                    >> .pcap Copy: thez@ade:~$ ls                   # 03.24(금) 2023>> AutowareAuto  autowareclass2020  AutowareDemo  data   'Code Ref'    >> Build ??        'Husarion' thez@ade:~$ source /opt/AutowareAuto/setup.bash                     @@thez@ade:~$ ros2 run velodyne_nodes velodyne_cloud_node_exe __ns:=/lidar_front __params:=/home/${USER}/autowareclass2020/code/src/01_DevelopmentEnvironment/velodyne_node.param.yaml[WARN] [1679620343.008488592] [rcl]: Found remap rule '__ns:=/lidar_front'. This syntax is deprecated. Use '--ros-args --remap __ns:=/lidar_front' instead.[WARN] [1679620343.008960524] [rcl]: Found remap rule '__ns:=/lidar_front'. This syntax is deprecated. Use '--ros-args --remap __ns:=/lidar_front' instead.1679620343.018637 [14] velodyne_c: using network interface wlp59s0 (udp/172.30.1.1) selected arbitrarily from: wlp59s0, br-04246e2a4255, docker0terminate called after throwing an instance of 'rclcpp::ParameterTypeException'  what():  expected [string] got [not set]thez@ade:~$  ​​​3D perception stackhttps://autowarefoundation.gitlab.io/autoware.auto/AutowareAuto/perception-stack-howto.html udpreplay ~/data/route_small_loop_rw.pcap -r -1                  # Hereudpreplay ~/data/route_small_loop_rw-127.0.0.1.pcap -r -1        # Youtube Overview 이 데모는 'perception node' 실행하기 위한 일반적인 프로세스를 설명Autoware.Auto 3D 인식 스택은 '개체 경계 상자를 계산'하고 게시하는 데 필요한 노드 집합으로 구성됩니다. 이를 위한 최소 스택은 다음과 같습니다.1. 'point_cloud_filter_transform_node' : velodyne_node  output 을 common frame 으로 변환    https://autowarefoundation.gitlab.io/autoware.auto/AutowareAuto/point-cloud-filter-transform-nodes.html2. 'ray_ground_classifier_node' : 라이다 점을 분류하여 지면에 속하는지 비지면에 속하는지를 나타냅니다.     https://autowarefoundation.gitlab.io/autoware.auto/AutowareAuto/ray-ground-classifier-design.html3. 'euclidean_cluster_node' : non-ground 지점을 객체 감지로 클러스터링 합니다.     https://autowarefoundation.gitlab.io/autoware.auto/AutowareAuto/euclidean-cluster-design.html스택을 보강하는 데 사용할 수 있는 이 자습서에서 다루지 않는 선택적 노드도 있습니다.1. point_cloud_fusion : 여러 소스의 포인트 클라우드를 단일 메시지로 융합합니다.    이것은 현재 전면 및 후면 LiDAR 데이터를 단일 메시지 스트림으로 융합하는 데 사용됩니다.    이 튜토리얼에서는 전면 LIDAR 데이터만 사용합니다.    https://autowarefoundation.gitlab.io/autoware.auto/AutowareAuto/point-cloud-fusion-nodes.html2. voxel_grid_nodes : 복셀 그리드 표현을 통해 포인트 클라우드 데이터를 다운샘플링하는 데 사용할 수 있습니다.    이 자습서에서는 다운 샘플링을 수행하지 않습니다.    https://autowarefoundation.gitlab.io/autoware.auto/AutowareAuto/voxel-grid-filter-design.html ​Launch Perception Nodes # Prerequisites이 데모에서는 지침이 ADE 환경 내에서 실행된다고 가정합니다. 설치에 대한 자세한 내용은 ADE 설치를 참조하십시오. 데이터를 생성하기 위해 선택한 모드에 따라 .aderc 올바른 환경이 구성되었는지 확인하기 위해 다른 파일을 사용해야 할 수도 있습니다.# Generating Sensor Dataperception stack  실행하려면 차량에 대한 센서 위치를 결정하는 데 사용되는 해당 ""robot state""와 함께 센서 데이터를 생성하고 게시해야 합니다. 이 섹션에서는 이를 달성하는 '세 가지 다른 방법'• Running the simulator• Replaying sensor data• Connecting to the physical sensor Running the Simulator 1. Enter ADE using the LGSVL configuration:ade --rc .aderc-lgsvl start --update --enter2. See Running the SVL Simulator along side Autoware.Auto3. 'Publish the robot state' description:ade enterros2 run robot_state_publisher robot_state_publisher /opt/AutowareAuto/share/lexus_rx_450h_description/urdf/lexus_rx_450h.urdf Replaying Sensor Data 1. Download the PCAP file Dual VLP-16 Hi-Res pcap file.   # 뒤에 숫자 없음2. Enter ADE using the default script with extra flags to properly run RVizade --rc .aderc start --update --enter3. Move the downloaded file into your adehome folder.4. Replay the file using udpreplay:ade enterudpreplay ~/data/route_small_loop_rw.pcap -r -1              @@5. Launch the velodyne_node for the 'front' lidar:ade entersource /opt/AutowareAuto/setup.bashros2 run velodyne_nodes velodyne_cloud_node_exe --ros-args -p model:=vlp16 --remap __ns:=/lidar_front --params-file /opt/AutowareAuto/share/velodyne_nodes/param/vlp16_test.param.yaml>> # Not_Run      'YouTube'ros2 run velodyne_nodes velodyne_cloud_node_exe __ns:=/lidar_front __params:=/home/${USER}/autowareclass2020/code/src/01_DevelopmentEnvironment/velodyne_node.param.yaml>> 1679620343.018637 [14] velodyne_c: using network interface wlp59s0 (udp/172.30.1.1) selected arbitrarily from: wlp59s0, br-04246e2a4255, docker0>> terminate called after throwing an instance of 'rclcpp::ParameterTypeException'>>   what():  expected [string] got [not set]6. Launch the velodyne_node for the 'rear' lidar:ade entersource /opt/AutowareAuto/setup.bashros2 run velodyne_nodes velodyne_cloud_node_exe --ros-args -p model:=vlp16 --remap __ns:=/lidar_rear --params-file /opt/AutowareAuto/share/velodyne_nodes/param/vlp16_test_rear.param.yaml7. Publish the robot state description:ade entersource /opt/AutowareAuto/setup.bashros2 run robot_state_publisher robot_state_publisher /opt/AutowareAuto/share/lexus_rx_450h_description/urdf/lexus_rx_450h_pcap.urdf Connecting to the Physical Sensor # 참고 1. Enter ADE using the default script with extra flags to properly run RViz$ ade --rc .aderc start --update --enter -- --net=host --privileged2. To do this, update the IP address and port arguments in the parameter file for the velodyne_node and then launch the node:$ ade enterade$ source /opt/AutowareAuto/setup.bashade$ ros2 run velodyne_nodes velodyne_cloud_node_exe --ros-args -p model:=vlp16 --remap __ns:=/lidar_front --params-file /opt/AutowareAuto/share/velodyne_nodes/param/vlp16_test.param.yaml ​Launch Visualization __rviz rviz2 can be used to visualize perception data as it is published. To start the visualizer, open a new terminal, then:ade entersource /opt/AutowareAuto/setup.bashrviz2 -d /opt/AutowareAuto/share/autoware_auto_examples/rviz2/autoware_perception_stack.rvizrviz 구성에는 이 자습서의 모든 주제에 대한 디스플레이가 있습니다. 노드가 시작되면 rviz에 표시됩니다. 주제 이름 옆의 확인란을 선택하거나 선택 취소하여 시각화되는 인식 출력을 전환할 수 있습니다. Launch Perception Nodes 1. Launch the 'point_cloud_filter_transform_node' node. This node transforms point clouds from the velodyne_node to a common frame. In a new terminal, do:ade entersource /opt/AutowareAuto/setup.bashros2 run point_cloud_filter_transform_nodes point_cloud_filter_transform_node_exe --ros-args --remap __ns:=/lidar_front --params-file /opt/AutowareAuto/share/point_cloud_filter_transform_nodes/param/vlp16_sim_lexus_filter_transform.param.yaml --remap __node:=filter_transform_vlp16_front  --remap points_in:=/lidar_front/points_xyzi Autoware.Auto transformed points snapshot          PCL2. Launch the 'ray_ground_classifier_node' node. This node classifies point cloud points according to whether they are ground or non-ground. In a new terminal, do:ade entersource /opt/AutowareAuto/setup.bashros2 run ray_ground_classifier_nodes ray_ground_classifier_cloud_node_exe --ros-args --params-file /opt/AutowareAuto/share/ray_ground_classifier_nodes/param/vlp16_lexus_pcap.param.yaml --remap points_in:=/lidar_front/points_filtered Autoware.Auto ray ground filter snapshot       ray3. Launch the 'euclidean_cluster_node' node. This node clusters non-ground points into objects and publishes bounding boxes or convex polygon prisms optionally. For publish bounding boxes, in a new terminal, do:ade entersource /opt/AutowareAuto/setup.bashros2 run euclidean_cluster_nodes euclidean_cluster_node_exe --ros-args --params-file /opt/AutowareAuto/share/euclidean_cluster_nodes/param/vlp16_lexus_cluster.param.yaml --remap points_in:=/points_nonground Autoware.Auto bounding boxes segmentation snapshotAlternatively, publish the convex 'polygon' prism from the euclidean clustering algorithm. In a new terminal, do:ade entersource /opt/AutowareAuto/setup.bashros2 run euclidean_cluster_nodes euclidean_cluster_node_exe --ros-args --params-file /opt/AutowareAuto/share/euclidean_cluster_nodes/param/vlp16_lexus_cluster_as_polygon.param.yaml --remap points_in:=/points_nonground Autoware.Auto convex polygon prisms segmentation snapshot​Convenience Launch Files # 편의 실행 파일: 가볍게 실행 robot_state_publisher이러한 노드를 시작하는 프로세스를 단순화하기 위해 단일 명령을 사용하여 나머지 인식 스택과 함께 를 가져올 수 있는 편리한 시작 파일이 있습니다 . 지각 스택을 불러오려면 다음 실행 파일을 사용하십시오. PCAP Sensor Data NoteSee above steps for replaying the sensor data if you don't have a copy of the PCAP file.1. Replay the file using udpreplay:ade enterudpreplay ~/data/route_small_loop_rw.pcap -r -12. Launch the velodyne_node for the 'front' lidar in another terminal:ade entersource /opt/AutowareAuto/setup.bashros2 run velodyne_nodes velodyne_cloud_node_exe --ros-args -p model:=vlp16 --remap __ns:=/lidar_front --params-file /opt/AutowareAuto/share/velodyne_nodes/param/vlp16_test.param.yaml3. Launch the velodyne_node for the 'rear' lidar in another terminal:ade entersource /opt/AutowareAuto/setup.bashros2 run velodyne_nodes velodyne_cloud_node_exe --ros-args -p model:=vlp16 --remap __ns:=/lidar_rear --params-file /opt/AutowareAuto/share/velodyne_nodes/param/vlp16_test_rear.param.yaml4. Enter ADE and run the launch file   Publish the convex polygon prisms for describe the spatial location of objects.ade entersource /opt/AutowareAuto/setup.bashros2 launch autoware_demos lidar_polygon_prisms_pcap.launch.py>> 'polygon'Or alternativelly, publish the bounding boxes for describe the spatial location of objects.ade entersource /opt/AutowareAuto/setup.bashros2 launch autoware_demos lidar_bounding_boxes_pcap.launch.py>> 'bounding' SVL Simulator:  여기는 아직 제대로 확인 못함 NoteIf the SVL Simulator is already running, skip step 1.1. See Running the SVL Simulator along side Autoware.Auto2. Enter ADE and run the launch file   Publish the convex polygon prisms for describe the spatial location of objects.ade entersource /opt/AutowareAuto/setup.bashros2 launch autoware_demos lidar_polygon_prisms_lgsvl.launch.pyOr alternativelly, publish the bounding boxes for describe the spatial location of objects.ade entersource /opt/AutowareAuto/setup.bashros2 launch autoware_demos lidar_bounding_boxes_lgsvl.launch.py ​​스크린샷 Screenshot from 2023-03-24 08-41-02__도커_6개             실행과정은 Husarion 참고,  맨 처음글Screenshot from 2023-03-24 18-39-23Screenshot from 2023-03-24 18-47-03___PCLScreenshot from 2023-03-24 18-48-17__rayScreenshot from 2023-03-24 18-49-21__euclideanScreenshot from 2023-03-24 18-50-36__polygonScreenshot from 2023-03-24 18-52-30__AssyScreenshot from 2023-03-24 18-57-30__ConvenienceScreenshot from 2023-03-24 19-07-46Screenshot from 2023-03-24 19-08-18​​​​​​​​​여기부터는 테스트 과정​​​​----------------------------------------------------------------------------------------------------------------------- 03.24(금) 2023​•지난 1차 도전https://blog.naver.com/mdc1020/223030488223​•도커 이미지: husarion 사용https://blog.naver.com/mdc1020/223051592709 •Autoware.Auto Simulation Demo < husarion , Foxy , lgsvl simulator local 실행 가능, 웹 없이 >U22 도커 Test https://husarion.com/tutorials/ros2-tutorials/autoware-auto-sim-demo/ https://github...blog.naver.com ​​​​지난번 동일: velodyne_nodes  실행 문제발생----------------------------------------------------------------------------------------------------------------------- 03.24(금) 2023  _02 Screenshot from 2023-03-24 10-13-46__velodyne_nodes_Same_Problem.pngScreenshot from 2023-03-24 10-28-10.png               에러가 칼만 안 됬을때  동일Screenshot from 2023-03-24 10-29-49.pngK-20230324-455663__최종 다 실행​​​​​​​PKG Setting-4ea----------------------------------------------------------------------------------------------------------------------- 03.24(금) 2023  _01Husarion  추가해서 4ea  /   AutowareAuto:  코드 참고  /   autowareclass2020: 빌드 ?? Screenshot from 2023-03-24 09-17-43__PKG_Setting_ED.png​​<ed> "
[OpenCV] Object Traking (2) - built-in algorithms ,https://blog.naver.com/cho-yani/222116559727,20201015,"이 포스트는 아래 사이트의 번역 & 정리본임을 밝힌다.https://www.pyimagesearch.com/2018/07/23/simple-object-tracking-with-opencv/  [OpenCV] Object Traking (1) - Centroid Tracking이 포스트는 아래 사이트의 번역 & 정리본임을 밝힌다.https://www.pyimagesearch.com/2018/07/23/simp...blog.naver.com  object detection을 딱 한 번만 하고 object traker가 모든 frame을 알아서 처리하는 algorithm이OpenCV에는 8가지나 내장되어 있다. OpenCV built-in object trackerBOOSTING TrackerHaar cascades(AdaBoost)에 쓰인 머신러닝 algorithm.나온 지 10년이 넘음.느리고 tracking 잘 안 됨.OpenCV 3.0.0 이상. MIL TrackerBOOSTING보다는 더 정확함.failure reporting은 잘 못 함.OpenCV 3.0.0 이상. KCF TrackerKernelized Correlation FiltersBOOSTING과 MIL보다 빠름.MIL과 KCF처럼, full occlusion을 잘 처리하지는 못함.OpenCV 3.1.0 이상. CSRT TrackerDiscriminative Correlation Filter with Channel and Spatial ReliablityKCF보다는 더 정확하지만 약간 더 느림.OpenCV 3.4.2 이상. MedianFlow Trackerfailure reporting은 괜찮게 함.object가 빠르게 움직이거나 형태가 빠르게 바뀔 때 등은 잘 안 됨.OpenCV 3.0.0 이상. TLD TrackerFalse Positive 경향이 매우 높음.비추.OpenCV 3.0.0 이상. MOSSE Tracker매우 매우 빠름.CSRT나 KCF만큼 정확하지는 않지만 속도는 정말 빠름.OpenCV 3.4.1 이상. GOTURN TrackerOpenCV 내장 object detector 중 유일하게 딥러닝 기반.additional model file이 필요.OpenCV 3.2.0 이상. 추천 trackerCSRT : 정확도가 높아야할 때, FPS 처리는 좀 느려도 될 때KCF : FPS 처리가 빨라야할 때, 정확도는 좀 낮아도 될 때MOSSE : 빠르기만 하면 될 때  [OpenCV] Object Tracking (3) - Tracking multiple objects이 포스트는 아래 사이트의 번역 & 정리본임을 밝힌다.https://www.pyimagesearch.com/2018/07/23/simp...blog.naver.com ​ "
"Image Resolution, FPS, Detection 성능 상관관계 ",https://blog.naver.com/doctor_song/222701015727,20220414,· Image Resolution ↑ → Detection 성능 ↑ →  FPS ↓(∵ array 크기 ↑)​※ FPS(Frame per seconde) - 1초에 몇 frame 처리하느냐(1초에 object detection 할수 있는 image 갯수)↓ 
Object Detetion 성능 평가 지표 ,https://blog.naver.com/km7523/222425597787,20210709,"Object Detection 알고리즘의 성능은 precision-recall 곡선과 average precision(ap)로 보통 평가한다.​Precision :Object-Detector가 검출한 정보들 중에서 Ground-Truth와 일치하는 비율을 의미한다.한국어로 정밀도, 모든 검출 결과 중 옳게 검출한 비율.​Precision = TP / (TP + FP) TP(true positive) : 옳은 검출, FP(fals positive)  : 잘못된 검출​Ground-Truth :실제사진, 모자이크 사진을 AI가 예측한 것과 실제 사진을 비교​Recall :재현율마땅히 검출해내야하는 물체들 중에서 제대로 검출된 것의 비율​Recall = TP / (TP + FN) = TP / all ground truths​FN(False Negative) : 검출되었어야 하는 물체인데 검출되지 않은 것.TN(true Negative) : 검출되지 말아야 할 것이 검출되지 않았음​​​밸런스 데이터 면 Accuracy로 Performance Measure로 채택​비대칭 데이터면 f1-score로 채택​Accuracy정확도.​accuracy  = (TP + TN )  / (TP + FN + FP +TN)​​F1 Score모델의 성능을 측정하는데 있어서 precision과 recall은 유용함.모델이 얼마나 효과적인지는 F1 score를 사용​F1 Score 는 precision과 recall의 조화 평균입니다.​2* ( (Precision * recall ) / (precision + recall) )​F1 Score는 Precision과 recall을 조합하여 하나의 통계치를 뽑아냄.일반적인 평균이 아닌 조화평균을 계산하였는데, 그이유는 precision과 recall이 0에 가까울수록 F1 score도 동일하게 낮은 값을 갖도록 하기 위함조화평균은 먼가 큰값이 있다면 패널티를 주어서 작은 값 위주로 평균을 구하는 원리​​예를 들어 recall =1 , precision = 0.01로 측정된 모델이 있으면​2 *  ( ( 1* 0.01 ) + (1+0.01) ) = 0.019​​​​어느 한값만 가지고 평가하는것은 잘못된것이다.평가할때 필요한것이 precision-recall 곡선 및 AP 이다.​​TP와 FP를 결정해주는 기준은 Iou(Intersection over Union)​Intersection over Union :빨간색 : 실제 라벨 바운더리 박스 데이터초록색 : 예측한 바운더리 박스 데이터IOU =  겹친 부분 / 두개의 바운더리 합 ​A ​B ​IOU = A / B ​0.5 이상이면 제대로 검출 (TP)0.5 미만이면 잘못 검출(FP)​​Precision-Recall 곡선 (PR곡선)​confidence 레벨에 대한 threshold값의 변화에 의한 물체 검출기의 성능을 평가하는 방법.confidence 레벨은 검출한것에 대해 알고리즘이 얼마나 확신이 있는지를 알려주는 값confidence 레벨이 낮으면 그만큼 검출 결과에 대해서 자신이 없는 것.\보통 confidence 레벨에 대해 threshold 값을 부여해서 특정 값 이상이 되어야 검출된것으로 인정. threshold 값이 0.4라면 0.1, 0.2 는 무시됨​예시.15개의 얼굴이 존재하는 어떤 데이터셋에서 한 얼굴 검출(Face Detection) 알고리즘에 의해서 총 10개의 얼굴이 검출되었다고 가정해보자.(confidence 레벨 0부터 100%까지 모두 고려했을때)이때, 각 검출의 confidence 레벨과 TP, FP에 대한 여부는 아래 표에 나타냈다. 10개 중 7개가 제대로 검출되었고, 3개는 잘못 검출되었다. 이 때, Precision = 옳게 검출된 얼굴 개수 / 검출된 얼굴 개수 = 7/10 = 0.7Recall = 옳게 검출된 얼굴 개수 / 실제 얼굴 개수 = 7/15 = 0.47이 된다. 이것은 confidence 레벨이 13%와 같이(Detections-F) 아주 낮더라도 검출해낸 것은 모두 인정했을 때의 결과이다. 이번에는 검출된 결과를 confidence 레벨에 따라 재정렬해보자. confidence 레벨에 대한 threshold 값을 아주 엄격하게 적용해서 95%로 했다면, 하나만 검출한 것으로 판단할 것이고, 이때, Precision = 1/1 = 1Recall = 1/15 = 0.067이 된다. threshold 값을 91%로 했다면, 두 개가 검출된 것으로 판단할 것이고,Precision = 2/2 = 1Recall = 2/15 = 0.13이 된다.threshold 값을 검출들의 confidence 레벨에 맞춰 낮춰가면 다음과 같이 precision과 recall이 계산될 것이다.    이 Precision값들과 Recall값들을 아래와 같이 그래프로 나타내면 그것이 바로 PR 곡선이다.    PR 곡선에서 x축은 recall 값이고, y축은 precision값이다. 즉, PR 곡선에서는 recall 값의 변화에 따른 precision값을 확인할 수 있다.​​AP (average precision) :​ PR 곡선은 성능을 평가하는데 매우 좋은 방법이긴 하지만 단 하나의 숫자로 성능을 평가할 수 있다면 훨씬 더 좋을 것이다. 그래서 나온 것이 AP이다. AP는 precision-recall 그래프에서 그래프 선 아래쪽의 면적으로 계산된다. 그런데 보통 계산 전에 PR 곡선을 살짝 손봐준다. PR 곡선을 단조적으로 감소하는 그래프가 되게 하기 위해서 다음과 같이 바꿔준다.    이렇게 바꾼 다음에 그래프 선 아래의 넓이를 계산함으로 AP를 구한다. 이 경우 AP = 왼쪽 큰 사각형의 넓이 + 오른쪽 작은 사각형의 넓이 = 1*0.33 + 0.88*(0.47-0.33) = 0.4532가 된다. ​​Computer Vision 분야에서 물체 검출 및 이미지 분류 알고리즘의 성능은 대부분 이 AP로 평가한다.​물체 클래스가 여러 개인 경우, 각 클래스당 AP를 구한 다음, 그것의 평균을 구하는 것이 mAP이다.​AP (Average Precision) : Recall Value [0.0, 0.1, ... , 1.0] 값들에 대응하는 Precision 값들의 Average 이다.​mAP (mean Average Precision) : 1개의 Object 당 1개의 AP 값을 구하고, 여러 Object_Detector 에 대해서 mean 값을 구한 것. "
잠깐! 유치원과 어린이집 근처는 금연이라는 사실 알고 계시나요? ,https://blog.naver.com/isaict/222885355780,20220927,"​안녕하세요! 차세대융합기술연구원 AICT 서포터즈 정다현입니다 :)  오늘은 차세대융합기술연구원이 시흥시, (주)팀인터페이스와 공동으로 개발한미소만 피워주세요를 소개 하겠습니다 !​ 출처 : 경기디지털사회혁신센터 Youtube 여러분들 간접흡연 경험은 다들 한 번쯤은 있으시죠?10명 중 9명이 길거리에서 간접흡연을 경험한 적이 있다고 응답했는데요.​간접흡연은 흡연자보다 폐암 발생률이 80% 높고심장병으로 사망할 확률이 30~40% 정도 높다고 합니다!​​  ​​모든 유치원과 어린이집을 대상으로 시설의 경계선에서 ‘10m 이내  구역’은 금연구역으로 지정되었다는 사실을 알고 계셨나요? ​미소만 피워주세요는 어린이집 인근이 금연구역인 것을 인지하지 못한 흡연자의 자발적 행동교정을 유도하기 위해 AI 딥러닝 영상 처리 기술이 활용된 시스템입니다.​​​​여기서 잠깐 ! 딥러닝(Deep learning) 기술이란?​컴퓨터가 인간처럼 판단할 수 있도록데이터를 모으고 분류하는 데 사용하는 기술로,​비슷한 양의 방대한 데이터가 구축 될 수록컴퓨터가 사람처럼 스스로 학습·추론·소통할 수 있는인공지능(AI : Artificial Intelligence)의 기계학습법 중 하나입니다.​ 인공지능(AI)의 범주​​미소만 피워주세요에 적용된 딥러닝 영상처리 기술은 이미지 분야에서 가장 널리 사용되는CNN(합성곱 신경망, Convolutional Neural Network) 기술입니다. ​▲ 분류·범주화(Classification) ▲ 객체 탐지(Object Detection)▲ 세부 의미적 분리(Semantic Segmentation)를 적용해​많은 이미지의 데이터를 학습하고 패턴을 찾아 스스로 판단, 구현할 수 있는 기술입니다!​​ 출처 : 경기디지털사회혁신센터 유튜브 ​어린이집 외부에 설치된 미소만 피워주세요(사이니지 형태)에는평소 미소를 짓는 아이가 등장하지만AI 딥러닝 영상 처리 기술을 통해 흡연자의 흡연 동작을 감지, 인식하게 되면 아이가 얼굴을 찡그리고 기침 소리를 내며 간접흡연의 고통을 표현합니다.​​  ​​단순히 흡연을 단속하고자 하는 목적보다는 어린이집 인근이금연구역이라는 홍보와 더불어 흡연자의 감정을 움직여 자발적인 금연이 가능하도록인식을 개선하자는 취지를 담고 있어 더욱 그 의미가 큽니다!  출처 : 경기디지털사회혁신센터 유튜브 ​우리 아이들의 건강을 지켜줄 프로젝트 ‘미소만 피워주세요’와 시민들이 제안하는 다양한 사회문제를 디지털 기술을 활용해 해결하는 차세대융합기술연구원의 경기디지털사회혁신센터가 가져올긍정적인 변화를 기대해 주세요!   ​  해당 콘텐츠는 차세대융합기술연구원 기자단이 직접 제작한 내용이며,차세대융합기술연구원의 공식 입장이나 발표 자료가 아님을 알려드립니다.​​​ ​ "
CCTV  실시간 영상과 드론영상에서 사람과 사람이 아닌 객체 분석 ,https://blog.naver.com/gisandpython/223086974582,20230427,"CCTV  실시간 영상과 드론영상에서 사람과 사람이 아닌 객체을 분석한뒤 라벨링하고, 라벨링된 결과를  기반으로  사람수을  카운트 한 후, ​밀집도을 분석하는 파이썬 소스을 생성하라고 ChatGPT4에게 물어 봤습니다.​Video analysis for human detection.​Model: GPT-4​​As  a gis developer, generative a simple python source to do based on following conditions.​1. Import a real-time cctv or drone records like mp4.2. Analysis a humans numbers in mp4 file frames.3. Labeling a humans and non humans.4. COUNT an humans and classified a group.5. Segmentation as a density.6. Judge a density of humans  based on the results of Segmentation.7. Send a emails or cellphone messages to warning of density.​Below is a simple Python script using the OpenCV and TensorFlow libraries for video processing and object detection, and smtplib for sending emails. This script assumes you have a pre-trained TensorFlow model that can recognize and classify humans and non-humans.​pythonCopy codeimport cv2import numpy as npimport tensorflow as tffrom email.message import EmailMessageimport smtplib​# Replace these with your own email informationSMTP_SERVER = ""your.smtp.server""SMTP_PORT = 587SENDER_EMAIL = ""your-email@example.com""RECEIVER_EMAIL = ""receiver-email@example.com""PASSWORD = ""your-password""​def analyze_video(video_path, model_path, threshold=0.5):    model = tf.saved_model.load(model_path)    video = cv2.VideoCapture(video_path)    frame_count = 0    humans_count = 0    non_humans_count = 0​    while True:        ret, frame = video.read()        if not ret:            break                frame_count += 1​        input_tensor = tf.convert_to_tensor(frame)        input_tensor = input_tensor[tf.newaxis, ...]        detections = model(input_tensor)        scores = detections[""detection_scores""][0].numpy()        classes = detections[""detection_classes""][0].numpy().astype(np.int32)                for i, score in enumerate(scores):            if score > threshold:                if classes[i] == 1:  # Assuming class 1 is ""human""                    humans_count += 1                else:                    non_humans_count += 1                        video.release()    cv2.destroyAllWindows()        return humans_count, non_humans_count​def send_warning_email(humans_count):    msg = EmailMessage()    msg.set_content(f""Warning: High human density detected - {humans_count} people."")    msg[""Subject""] = ""High Human Density Warning""    msg[""From""] = SENDER_EMAIL    msg[""To""] = RECEIVER_EMAIL​    with smtplib.SMTP(SMTP_SERVER, SMTP_PORT) as server:        server.starttls()        server.login(SENDER_EMAIL, PASSWORD)        server.send_message(msg)​def main():    video_path = ""path/to/your/video.mp4""    model_path = ""path/to/your/tensorflow_model""    threshold = 0.5    density_limit = 50​    humans_count, non_humans_count = analyze_video(video_path, model_path, threshold)    print(f""Humans detected: {humans_count}"")    print(f""Non-humans detected: {non_humans_count}"")​    if humans_count > density_limit:        send_warning_email(humans_count)        print(""Warning email sent due to high human density."")    else:        print(""Human density is within acceptable limits."")​if __name__ == ""__main__"":    main()Please note that you will need to replace placeholders for email server settings, video file path, and TensorFlow model path with your own information. Also, this code assumes that the ""human"" class in the pre-trained model has an ID of 1. Please adjust accordingly if your model has a different class ID for humans. "
[파이썬/머신러닝] nucleus detection and segmentation: Mask-RCNN ,https://blog.naver.com/kyy0810/221426658926,20181225,"이번 포스팅에서는 물체 검출과 분할을 동시에 할 수 있는 Mask-RCNN을 활용해 세포 핵 검출 및 분할 활용 예시를 정리하려고 합니다. ​​1. 간단한 알고리즘 소개​기존 R-CNN부터 Mask-RCNN 까지는 정리된 내용이 많으므로 [1,2,3] 간단히만 정리하려고 합니다. Mask-RCNN이 기존에 다른 점은 R-CNN부터 Faster-RCNN까지는 object detection만 수행하였는데, Mask-RCNN 은 pixcel-level segmentation으로 확장되었다는 점입니다. 기존 RoIPool 기법에서 bilinear interpolation 적용한 RoIAlign 방법을 사용하여 보다 정교한 localization 이 가능하도록 하였습니다. 논문에 따르면 RoIAlign 은 상대적으로 10~50% 정도 정확성을 향상한다고 합니다. ​​2.  데이터 셋 ​Andrew[4]가 제공하는 데이터 셋을 사용하였습니다. 세포 핵 이외에도 다양한 데이터가 존재합니다. 이미지 포맷은 TIFF이며 크기는 2000X2000입니다. 총 130장 정도 되며 일부 세포 핵에 대해서만 annotation이 되어 있습니다. Mask-RCNN 은 mask 각각에 대한 입력을 받습니다. 입력 데이터를 만들어주기 위해 침식-팽창 방법을 사용하여 mask 각각에 대한 입력 이미지를 생성하였습니다. 이에 대한 코드와 설명은 [5]에 있습니다. ​    ​​2. 적용 방법사용한 네트워크는 resnet50이며, 기본적으로 resnet101 와 함께 Mask-RCNN에서 제공하고 있습니다. 이미지 사이즈가 다소 커서 그런지 GPU 메모리 12G에서도 out of error 가 발생하여 돌리는 것이 쉽지 않았습니다.  Mask-RCNN wiki를 참고하여 이미지 사이즈를 절반으로 줄이고 랜덤으로 이미지를 512x512로 crop 함으로써 학습을 시킬 수 있었습니다. 또한, 학습 이미지의 mean pixel 값을 구해 MEAN_PIXEL 값을 변경하였습니다. 아래는 학습 파라미터 중 일부를 발췌한 것입니다. 전체 코드는 깃허브 레퍼지토리 에 업로드하였습니다.  ​ class NucleusConfig(Config):    """"""Configuration for training on the nucleus segmentation dataset.""""""    # Give the configuration a recognizable name    NAME = ""nucleus""    IMAGES_PER_GPU = 1    # Number of classes (including background)    NUM_CLASSES = 1 + 1  # Background + nucleus    # Number of training and validation steps per epoch    STEPS_PER_EPOCH = 70    VALIDATION_STEPS = 30    # Backbone network architecture    # Supported values are: resnet50, resnet101    BACKBONE = ""resnet50""    # Input image resizing    # when image size = 2000px, out of GPU memory ( 12GB)    # reszie half the original image    #IMAGE_MAX_DIM = 1024   # original image 2048    #IMAGE_MIN_DIM = 1024    # Random crops of size 512x512    IMAGE_RESIZE_MODE = ""crop""    IMAGE_MIN_DIM = 512    IMAGE_MAX_DIM = 512    # Length of square anchor side in pixels    RPN_ANCHOR_SCALES = (8, 16, 32, 64, 128)    # Image mean (RGB)    MEAN_PIXEL = np.array([185.7, 136.8, 172.2]) ​3. 결과기존 샘플 코드에서 사용한 대로 backbone (resnet50)과 head는 각각 20epoch 학습하여 총 40epoch를 시켰고, 아쉽게 검출 성능은 좋지 못했습니다. 리소스의 한계로 성능 제한 요소가 많기 때문에 충분히 성능 개선 요소는 많지 않나 생각됩니다. 조금 더 다양한 방법을 시도해서 성능이 훨씬 괜찮아진다면 2편으로 포스팅하도록 하겠습니다. ​    ​​[1] https://junn.in/archives/2517[2] https://blog.lunit.io/2017/06/01/r-cnns-tutorial/[3] https://tensorflow.blog/2017/06/05/from-r-cnn-to-mask-r-cnn/[4] http://www.andrewjanowczyk.com/use-case-1-nuclei-segmentation/[5] https://blog.naver.com/kyy0810/221446424532[6] https://github.com/ai-lab-circle/Mask-RCNN-nucleus-segmentation​​2017-03-02,  지금으로부터 정확히 2년 전에 블로그 포스팅을 시작했습니다. 공부하고 경험한 것 중에 유익하다고 생각한 것을 공유하고 싶었는데 많이 부족하지 않았나 싶습니다. 앞으로 2년간은 더 알차고 지속적으로 포스팅할 것을 스스로 다짐하며 포스팅을 마무리합니다. 부족한 부분에 대해서 언제든 댓글이나 쪽지 남겨주시면 보완하도록 하겠습니다. 감사합니다.​​ "
OpenCV - Object Tracker를 활용한 손흥민 선수 추적 ,https://blog.naver.com/leejs2133/222211168665,20210118,"OpenCV에는 Object Tracker라고 객체를 추적하는 기능이 있다.이를 통해 손흥민 선수를 추적해보고자 한다.영상은 다름 아닌 손흥민 선수의 FIFA Puskas Award 수상 골. (토트넘 vs 번리 70m 드리블 돌파 골)​ # Object Tracking# OpenCV에서는 왼쪽위가 (0, 0) 기준이라서# 왼쪽 위로 갈수록 x, y 좌표가 작아진다.import cv2import numpy as npvideo_path = 'son_puskas.mp4'# video_path = 'april.mp4'cap = cv2.VideoCapture(video_path)# 비디오를 읽는다.# cv2.VideoCapture : 비디오를 읽는 함수# output_size = (187, 333)# (width, height) 가로, 세로# 비디오 파일 저장을 위해서 구현하는 코드# fourcc = cv2.VideoWriter_fourcc('m', 'p', '4', 'v')# out = cv2.VideoWriter('%s_output.mp4' % (video_path.split('.')[0]), fourcc, cap.get(cv2.CAP_PROP_FPS), output_size)# cap.isOpened() : cap라는게 잘 열렸나요?if not cap.isOpened():    exit()    # exit() : 그게 아니라면 Python 프로그램을 종료# Object Tracker# csrt : cv2.TrackerCSRT_create# kcf : cv2.TrackerKCF_create# boosting : cv2.TrackerBoosting_create# mil : cv2.TrackerMIL_create# tld : cv2.TrackerTLD_create# medianflow : cv2.TrackerMedianFlow_create# mosse : cv2.TrackerMOSSE_create# 우리는 여기서 csrt를 사용할 것이다.tracker = cv2.TrackerCSRT_create()ret, img = cap.read()# 1번째 프레임을 읽어온다.# ret은 True / False 여부가 적혀있는 것.# img가 실제로 우리가 읽는 영상 파일cv2.namedWindow('Select Window')cv2.imshow('Select Window', img)# ROI - Region of Interest 사용자가 관심있어하는 ROI# Object Tracking을 위해서는 ROI가 필요하겠지.rect = cv2.selectROI('Select Window', img, fromCenter=False, showCrosshair=True)# 중심점에서 시작하지 말고 / 중심점을 보여라 (십자가 모양으로 보인다)# 그냥 맨뒤에꺼 True 없애도 될듯.# rect = cv2.selectROI('Select Window', img, False)cv2.destroyWindow('Select Window')# tracker 물체추적 세팅# tracker.init(img, rect) : Object Tracker가 img와 rect를 따라가도록 설정한다.tracker.init(img, rect)# rect로 설정한 부분을 내가 쫓아가면 되구나!while True:    ret, img = cap.read()    # 비디오를 읽어서 img라는 변수에 저장한다.    if not ret:        # 비디오 프로그램 다 읽으면 ret이라는 변수가 False가 된다.        exit()    # tracker.update(img)    # img에서 rect로 설정한 이미지와 비슷한 물체의 위치를 찾아 반환한다.    success, box = tracker.update(img)    # success : 성공 여부    # box : rect 형태의 데이터로 나온다.    x, y, w, h = [int(v) for v in box]    #왼쪽, 오른쪽, 너비, 높이    # center_x = x + w / 2    # center_y = y + h / 2    # result_top = int(center_y - output_size[1] / 2)    # result_bottom = int(center_y + output_size[1] / 2)    # result_left = int(center_x - output_size[0] / 2)    # result_right = int(center_x + output_size[0] / 2)    # result_img = img[result_top:result_bottom, result_left:result_right]    # 여기 뒤에다가 .copy()를 붙이면 사각형 그게 사라진다.    # out.write(result_img)    cv2.rectangle(img, pt1=(x, y), pt2=(x + w, y + h), color = (255, 255, 255), thickness=3)    # cv2.rectange(img, (x, y), (x+w, y+h), (255, 255, 255), 3)    # cv2.imshow('result_img', result_img)    cv2.imshow('img', img)    if cv2.waitKey(1) == ord('q'):        break        # 키보드 q 누르면 프로그램 종료 손흥민 선수 객체 선정을 해주고 (ROI 영역 설정)​ 그러면 OpenCV에서 Object Tracker로 알아서 손흥민 선수를 따라간다.​ 끝까지 따라감 짱신기 ㅋㅋㅋ​OpenCV로 Object Tracker를 구현해보았으면,Object Detection도 구현해보아야겠다.​#OpenCV #컴퓨터비전 #개발 #딥러닝 #머신러닝 #비전 #눈 #computervision #computer #vision #오픈CV #컴퓨터 "
[Paper] Extending IOU Based Multi-Object Tracking by Visual Information (2018 15th IEEE on AVSS) ,https://blog.naver.com/cho-yani/222120219343,20201019,"원문 : https://ieeexplore.ieee.org/document/8639144 Abstract오늘날의 multi-object tracking approach는 인기 있는 tracking-by-detection 방식을 따를 때 거의 완벽한 object detection의 혜택을 크게 누린다. 이를 통해 input detection을 고속 IOU tracker로 완전히 의존하는 매우 간단하지만 정확한 tracking 방법을 사용할 수 있다. 실제 응용의 경우, 소수의 누락된 detection이 많은 ID 전환과 fragmentation을 유발하여  track의 품질을 현저히 떨어뜨린다. 우리는 tracker가 object detection을 이용할 수 없는 경우에 visual single-object tracking으로 되돌아간다면 이 문제를 효율적으로 극복할 수 있다는 것을 보여준다. 여러 실험에서 우리는 서로 다른 visual tracker에 대해 높은 tracking 속도를 유지하고 UA-DETRAC 및 VisDrone dataset의 최첨단 기술을 능가하면서 ID 전환과 fragmentation의 수가 큰 폭으로 감소할 수 있다는 것을 보여준다. 1. Introduction가장 성공적인 multi-object tracking(MOT) approach는 tracking-by-detection 패러다임을 따른다. 일반적인 방법은 정교한 appearance 모델을 사용하여 긴 시간 범위[25, 4]에 걸쳐 object instance를 다시 식별하거나 복잡한 global optimization [2, 29, 3]에서 각 객체의 track을 계산한다.​그러나 최근 몇 년 동안 object detector가 크게 개선되었다. 근본적인 발전은 적어도 현재의 딥러닝 시대[16, 13]와 large-scale object detection 벤치마크[18, 8]에 의해 촉진되지 않는다. 따라서 Fast(er)/Mask R-CNN[11, 24, 12], FCN[20] 또는 SSD[19]와 같은 매우 정확한 object detection 방법이 도출된다.​이는 tracking 알고리즘을 위한 요구사항의 변화를 야기한다. 이처럼 점점 더 정확한 object detection에 의존할 수 있게 되면 훨씬 더 간단한 tracking-by-detection 접근법을 사용할 수 있다[6, 5, 30]. 이들은 모두 detection을 individual track과 높은 spatio-temporal overlap과 연관시키는 핵심 원리를 공유한다. 이 overlap은 연속된 프레임 사이의 detection bounding box의 IOU에 의해 측정된다.​Simple Online Realtime Tracker(SORT)[5]는 Kalman filter motion model을 채용하고 Hungarian algorithm을 사용하여 detection의 할당 문제를 최적으로 해결한다. 이 방법은 [30]에서 긴 기간의 occlusion을 처리하기 위해 deep association metric를 통해 appearance 정보를 통합함으로써 확장되었다. SORT와 마찬가지로 IOU Tracker[6]도 detection에만 의존하며 이미지 정보를 사용하지 않는다. 이 단순한 tracker는 motion model을 사용하지 않으며 detection을 greedy manner로 tracking에 연관시킨다. 그 결과 IOU tracker는 훨씬 더 복잡한 최첨단 방법을 능가하면서 초당 수천 프레임에서 작동할 수 있다 [21]. 이 간단한 접근법의 주요 단점은 underlying detector의 높은 recall이다. 하나 또는 몇 개의 누락된 detection에 의해 야기되는 모든 gap은 false negative 뿐만 아니라 track의 종료와 재시작으로 이어져 높은 비율의 fragmentation 및 ID 전환의 원인이 된다.​본 연구에서는 누락된 detection에 대한 robustness를 높이기 위해 visual single-object tracker를 IOU tracking 계획에 통합함으로써 이 문제에 접근한다. 우리의 아이디어는 새로운 detection이 연관되지 않을 경우 visual tracker로 각 track을 계속하여 track 사이의 간격을 메우는 것이다. 이것은 track의 fragmentation과 ID 전환의 수를 감소시킨다. 부작용으로는, 방법이 더 이상 모든 프레임에 걸친 연속적인 detection에 의존하지 않기 때문에 더 높은 detection confidence threshold을 적용해야 한다는 것이다. 제안된 접근방식의 정확성과 속도는 주로 obejct detector와 visual singla-object tracker의 성능에 따라 달라진다. UADETRAC [28] 및 VisDrone[31] dataset에 대한 실험에서, 우리는 서로 다른 최첨단 multi-object detector와 single-object detector가 우리의 프레임워크의 성능에 미치는 영향을 조사한다. 우리는 이러한 접근방식이 전체적인 정확도를 향상시키고 ID 전환과 fragmentation의 양을 효과적으로 상당 부분 줄이는 동시에 낮은 computational footprint를 유지하며 두 데이터셋 모두에서 최첨단 기술을 능가한다는 것을 보여준다. 2. MethodV-IOU(Visual Intersistance-over-Union) tracker는 [6]에 제시된 IOU tracking 접근법의 개념적으로 일관적인 연속이다. 이는 baseline method track의 결과인 많은 ID 전환과 높은 fragmentation rate을 줄이기 위해 설계되었다. 이는 visual single-object tracker를 tracking 프레임워크에 통합하여 누락된 object detection를 보완함으로써 달성된다. IOU Tracker우선 baseline IOU Tracker의 개념과 한계를 간략하게 검토하겠다. 이 접근법의 주요 개념은 현재의 object detector가 false positive/negative detection에 의해 유발된 failure을 무시할 수 있을 정도로 충분히 신뢰할 수 있다(reliable enough)는 것이다. 이러한 가정에 근거해 tracking-by-detection 패러다임에 따르는 multi-object tracking의 작업은 하찮게 된다.​IOU tracker는 spatial overlap에 의해서만 후속 프레임의 detection을 track에 연관시킨다. 원래의 제안된 접근방식에서, 이것은 greedy way로 이루어진다: Track은 마지막으로 지정된 object position(즉, track의 이전 detection)에 가장 높은 IOU를 가진 detection을 얻는다. 비록 최적이지는 않지만, 이러한 heuristic 접근방식은 충분할 수 있다. 또는 Hungarian algorithm을 사용하여 [5]와 같은 선형 할당 문제를 해결함으로써 association를 수행할 수 있다. 실제 적용에서는, false positive/negative detection이 발생하며 tracking 프로세스를 방해한다. 따라서 결과 track은 각 track에 높은 detection confidence(≥σh)를 만족하는 detection 1개 이상을 포함하고 최소 tmin 프레임의 최소 길이를 갖도록 요구하여 필터링된다. 이렇게 하면 false positive 검출로 인한 많은 failure case를 효과적으로 골라낸다. 반면에 false negative detection을 track을 즉시 멈추게 한다. IOU tracker가 마지막 detection을 전파하지 않으므로 다음 가용 track에 새 track이 생성된다. 이는 ID 전환의 높은 비율과 track의 fragmentation로 이어진다. Visual Tracking Extension특히 IOU tracker의 경우 누락된 detection이 전파되지 않기 때문에, false negative detection은 tracking-by-detection 접근 방식에 일반적인 문제를 제기한다. 따라서 우리는 association에 대한 detection을 사용할 수 없는 경우 visual single-object tracking으로 다시 돌아감으로써 IOU tracker를 확장할 것을 제안한다. 전체 visual tracking extension은 그림 1에 나타나 있다. 그림 1. 확장된 IOU Tracker의 기본 원리: IOU track은 일반적으로 누락된 detection(왼쪽)로 인해 크게 fragmented된다. 누락된 물체 위치(노란색, 가운데)를 보정하기 위해 visual tracker를 사용하여 간격을 채울 수 있다. 그 결과 track은 덜 fragment된다(오른쪽).​Visual tracking은 다음 두 가지 방향으로 수행된다. 첫째, track에 대해 [6]의 σIOU threshold를 만족하는 detection 이 없을 경우, visual tracker는 마지막으로 확인된 물체 위치(이전 프레임에서의 detection)에서 초기화되어 최대 ttl 프레임의 물체를 detect하는 데 사용된다. 새로운 detection이 이러한 ttl 프레임 내에서 σIOU threshold를 만족하면 visual tracking이 중지되고 IOU tracker가 계속된다. 그렇지 않으면 track이 종료된다. 이 정도면 누락된 detection이 거의 없는 경우 그를 신뢰성 있게(reliably) 보상하기에 충분하다.​그러나 visual tracked frame의 수가 증가함에 따라 visual tracker가 track을 느슨하게 하거나 다른 물체로 점프할 가능성이 높아진다. Visual cue에 의해서만 객체가 tracking되는 연속 프레임의 수를 제한하기 위해서, 우리는 또한 visual tracking을 각각의 새로운 track에 대한 마지막 ttl 프레임을 통해서 역방향으로 수행한다. 기존 완료된 track에 대해 overlap 기준이 충족되면 이를 병합한다. 이러한 방식으로 최대 2·ttl 프레임의 길이 간격을 좁힐 수 있는 반면 single visual object tracker는 최대 ttl 프레임에 대해서만 사용된다.​개체의 visual foreward 및 background tracking이 중단된 track을 병합하는 데 도움이 되기는 하지만 그림 1(b)에서 볼 수 있는 것처럼 완성된 각 track의 시작과 끝에 visually tracked된 stub을 추가한다. Track은 객체가 현장에 진입할 때 시작되고 객체가 떠날 때 끝나야 하므로, 이러한 stub은 병합 간격에 기여할 수 없으며 관심 객체가 현장에 없을 수 있으므로 오류를 포함하기 쉽다. 이러한 이유로 우리는 gap closure에 대한 visual tracking의 사용을 제한하고 track에서 visually tracked된 bounding box의 stub을 절단한다. 3. Experiments우리는 두 개의 공통 multi-object tracking dataset에서 제안된 V-IOU tracker를 평가한다. 먼저 UA-DETRAC [28] dataset에서 IOU 및 V-IOU tracker의 성능을 비교하고 false negative detection을 처리하는 능력을 조사한다. 또한 다양한 visual single-object tracker가 VisDrone [31] dataset에 미치는 영향을 연구한다. 마지막으로, 두 dataset에 대해 우리의 방법을 최신 기술과 비교한다. 각 실험에 대한 최적의 tracker 구성은 tracker parameter의 grid search에 의해 결정되었다. 3.1. UA-DETRACUA-DETRAC dataset은 vehicle tracking을 위해 10시간 분량의 완전한 annotation이 달린 비디오 자료로 구성되어 있으며, 60개의 교육용 비디오와 40개의 테스트 비디오로 나뉜다. 전형적인 교통 모니터링 뷰에서 기록되었으며, 다양한 scale, pose, illumination, occlusion 및 기상 조건을 다룬다. 평가는 [28]에 제시된 PR 기반 지표를 따르며, tracker는 detector의 모든 confidence level에서 잘 실행되어야 한다.​Detection​우리는 두 가지 다른 방법의 detection을 사용하여 실험을 수행한다. 첫째, 우리는 dataset과 함께 제공되는 CompACT[7]의 detection을 평가한다. 이 detector는 77.37%의 AP를 달성하는 UA-DETRAC train split에 대해 훈련되었다. Test set의 경우 AP는 53.23%이다.둘째, COCO minival set에서 46.5의 mAP@IoU=0.50:0.95를 달성하는 COCO trainval35k dataset[18]에서 훈련된 모델을 사용하여 Mask R-CNN[12]의 detection를 계산했다. 자동차, 버스, 트럭 class만을 detect한 실험에서 우리는 UA-DETRAC train set에서 80.53%, test set에서 80.48%의 AP를 달성했다. 그 detection는 공개적으로 이용할 수 있도록 되어 있다(https://github.com/bochinski/iou-tracker). CompACT에 비해 높은 평균 precision 외에도, tracker parameter는 train set에서만 조정할 수 있기 때문에 중요한 UADETRAC train 및 test data 모두에서 detector 성능은 동일하다.​Tracking​Baseline IOU 및 확장 V-IOU tracker는 앞서 설명한 두 detector에 대한 UA-DETRAC train 시퀀스 상의 최상의 PR-MOTA 값에 최적화되어 있다. V-IOU의 평가에는 fast KCF Tracker[14]만 고려되었다. 최상의 성능을 발휘하는 구성에 대한 매개변수는 표 1에 나와 있다. 벤치마크의 PR 기반 평가에서는 파라미터 σl이 0에서 1.0까지 다양하다는 점에 유의하라.​Results​KCF를 사용한 baseline IOU tracker와 우리의 extended V-IOU tracker에 대한 평가 결과는 표 2에서 최신 기술과 비교된다. CompACT detection을 사용할 때, PR-MOTA는 1.6% (상대적으로 10%), PR-FM은 3배, PR-ID는 6배 향상된다. Mask R-CNN detection의 경우 PR-MOTA 값은 IOU와 V-IOU가 다르지 않지만 PR-ID와 PR-FM의 수는 각각 4와 2.5배 향상된다. 표 2. easy, medium, hard split을 포함한 전체 DETRAC-Test dataset에서 각 Tracker에 대한 최상의 결과. #의 결과는 [28]에서 얻었다.MOTA metric은 track의 false positive, negative bounding box 수와 ID 전환 횟수에 영향을 받는다. 세 성분은 모두 균등하게 가중된다(weighted equally). 이는 ID 전환 횟수가 전체 MOTA 점수에 미치는 영향은 미미하다는 것을 의미한다. Tracker에 대한 입력 detection이 좋을수록 V-IOU 접근방식에 의해 시각적으로 추적되지 않는(the less visually tracked) bounding box를 삽입해야 한다. 따라서 CompACT와 Mask R-CNN detection을 사용할 때 PR-MOTA 점수는 소폭 개선되었을 뿐이지만 PR ID와 PR-FM 측면에서는 여전히 큰 진전이 있다. 이러한 동작은 그림 2에서 PR 기반 평가에 사용된 모든 detector threshold에 대해 각각의 metric을 플로팅한 것을 볼 수 있다. 그림 2. 다양한 detection method에 대한 서로 다른 낮은 detector threshold σ(l)에서 MOTA(↑), #Fragmentation(↓), #ID 전환(↓) 측면에서 baseline IOU와 우리의 extended V-IOU tracker의 비교. IOU tracker를 시각적 정보로 보완하면 fragmentation과 ID 전환이 훨씬 적게 발생하기 때문에 더 유용한 track이다.그림 2(a)는 0.0에서 1.0 사이의 모든 threshold σl에 대해 양호한 성능을 보이는 Mask R-CNN의 결과를 보여준다. Baseline IOU tracker에 대한 ID 전환 및 fragmentation의 수는 less-confident detection이 필터링됨에 따라 detector threshold가 높을수록 상당히 증가한다. 따라서 object와 frame당 하나의 detection을 한다는 초기 가정은 frequency가 증가하면서 위배된다. 반면에 우리의 extended V-IOU tracker는 이러한 누락된 detection을 신뢰성 있게(reliably) 보완할 수 있다. 비록 이것이 MOTA 값에 미미하게만 영향을 미치지만, track의 주관적인 품질은 대부분의 application에서 상당히 개선된다.​그림 2(b)에서 CompACT detection을 사용할 때도 이와 유사한 것을 볼 수 있다. 피크 MOTA의 위치와 최악의 ID 전환/fragmentation 횟수는 그림 3과 같이 각 detector에 대한 confidence score의 분포가 서로 다르기 때문에 발생한다. Confidence threshold가 증가함에 따라 Mask R-CNN에 비해 V-IOU의 시각적 추적으로 보정할 수 있는 더 많은 detection이 제거된다. 이것은 σIOU > 0.2의 MOTA 점수에서도 알 수 있다. 그림 3. UA-DETRAC test set에 대한 Mask R-CNN 및 CompACT detection에 대한 confidence score 분포.일반적으로 UA-DETRAC dataset에 대한 평가는 IOU tracker의 visual tracking extension에 의해 tracking 성능이 상당히 향상된다는 것을 보여준다. Baseline method와는 다르게 tracker가 image 정보를 사용하기 때문에 런타임이 감소한다. CompACT 및 Mask R-CNN detection을 모든 confidence threshold에 대해 평균적으로 사용하는 1,000 및 350 fps 이상이 있는 경우, 이 방법은 여전히 고속으로 간주될 수 있다. 3.2. VisDroneVisDrone 데이터 집합은 다양한 종류의 비디오 드론을 사용하여 도시 및 국가 환경에서 캡처되었다. 평가 벤치마크는 비디오와 image의 object detection, single-object tracking, multi-object tracking 등을 대상으로 한다.​multi-object tracking을 위한 시퀀스는 56개의 training, 7개의 validation, 16개의 test 비디오로 나뉜다. Faster R-CNN [24]의 detectoin이 제공된다. UADETRAC와 대조적으로, VisDrone MOT dataset은 track할 4가지 class의 객체(자동차, 버스, 트럭, 밴, 보행자)를 포함한다. IOU tracking 접근방식은 원래 서로 다른 객체 class를 동시에 처리하도록 설계되지 않았다. 따라서 우리는 tracking하는 동안 class를 무시하고 detection 수가 가장 많은 class를 track에 배정한다. 마찬가지로, 우리는 overlap threshold nmst에 대해 class label을 무시하고 non-maximum suppression을 수행하여 가장 높은 confidence score만 사용한다.​Detector가 dataset의 training split에 대해 train되었기 때문에, 우리는 validation split에 대한 parameter를 조정하기로 결정했다. 평가된 visual single-object tracker는 UA-DETRAC와 같은 KCF [14] 및 Midedflow [15]이다. Parameter tuning은 두 가지 다른 목표를 가진 baseline IOU 및 extended V-IOU tracker에 대해 수행된다. 첫째, 우리는 전반적으로 최상인 성능에 대한 parameter를 결정한다. 둘째로, 모든 시퀀스에 걸쳐 최상의 평균 MOTA에 대한 parameter를 최적화하여 서로 다른 길이와 object 수를 가진 scene의 weight을 동일하게 부여한다. 두 번째 전략에 대해서만 시퀀스 uav0000268_05773_v를 제외한다. 왜냐하면 시퀀스 uav0000268_05773_v는 recording의 overexposure로 인해 발생할 수 있는 다수의 false positive detection을 포함하고 있기 때문이다. 결과는 표 3에 제시되어 있다. 여기서 첫 번째 행은 overall MOTA에 관한 best configuration과 performing을 보여주고 두 번째 행은 각 tracker에 대한 best average MOTA를 보여준다. [6]에서 설명한 것처럼 UA-DETRAC에 대한 실험에 대해 행해진 detection을 greedy manner로 track에 할당하는 대신, [5]에서 제안한 Hungarian algorithm을 사용하여 최적의 할당을 수행한다. Computational complexity는 증가하지만, VisDrone dataset의 전체 결과는 약간 더 좋다. 표 3. KCF 및 Midedflow visual single object tracker를 사용하여 baseline IOU tracker와 proposed extention에 대한 VisDrone validation set에 대한 best configuration 비교. AVG는 uav0000268_05773_v를 제외한 평균 MOTA를 나타낸다.Validation set의 경우 MOTA는 약 1-2% 향상되는 반면 ID 전환 횟수는 최대 60% 감소한다. 따라서 ID 기반 metric IDF1, IDP 및 IDR은 visual tracking을 사용할 때 일관되게 개선된다. Mediumflow 구성은 best general performing이어야 하며 test set의 평가를 위해 선택된다. 표 4는 첨단기술과 비교한 최종 결과를 보여준다. 보다 자세한 분석은 [31]에서 확인할 수 있으며, 이 분석에서는 10개 metric 전체에 대한 average 순위 측면에서 V-IOU를 최고의 tracker로 보고한다. 표 4. 최첨단과 비교한 VisDrone-VDT 2018 test set에 대한 multi-object tracking 결과 [31].4. Conclusion우리는 IOU tracker에 visual tracking을 통합하는 직관적인 방법을 제시했다. 몇몇 실험에서 우리는 false negative detection이 강력하게 보상될 수 있다는 것을 보여주었다. 그 결과 ID 전환과 fragmentation 수가 감소하여 track의 품질이 크게 향상된다. 제안된 tracking 프레임워크에서는 아무 visual single-object tracker나 사용할 수 있다. 인기 있는 KCF[14]를 채용할 때, UA-DETRAC 및 VisDrone dataset에서 최첨단 성능을 발휘하면서 고화질 비디오 영상에 대해 200 fps 이상을 달성할 수 있다. 제안된 tracking 접근 방식의 고속, 고 정확도 및 simplicity는 향후 접근방식에 대한 강력한 baseline뿐만 아니라 많은 use cases에 적합하다. 5. Acknowledgements이 연구는 허가 계약 번호 03VP01940(SiGroViD)에 따라 BMBF-VIP+로부터 자금을 지원받았다. "
코딩 고속 보조배터리 5000 아이폰 미니 맥세이프 무선충전 후기 ,https://blog.naver.com/princess02/223024765586,20230223,"평소에 아이폰을 사용하면서 배터리가 너무 빨리 닳아서 외출할 때마다 불편함을 많이 느꼈었어요.그래서 매번 보조배터리를 가방에 챙겨다니고 있는데 제가 기존에 사용하던 건 유선이라 케이블 선이 자주 고장나서 몇 개째 구입하고 있는지 셀 수 없을 정도예요.​ ​이번에 고속 보조배터리를 고르면서 고려했던 부분은 무선 충전이 가능한 것, 가벼워 휴대성이 좋은 것, 아이폰과 갤럭시 모두 호환 가능한 것, 디자인도 이쁜 것 이렇게 4가지 기능을 중점적으로 생각해서 골랐어요. ​​ ​코딩 맥파워 맥세이프 무선 보조배터리팩기본 구성품은 본품, 사용 설명서, C타입 충전 케이블로 구성돼요.​​ ​무게는 157g으로 가벼웠어요. 디자인은 전체적으로 화이트와 라이트그레이 컬러로 심플한 느낌이라 마음에 들었어요.색상은 제가 사용하고 있는 화이트 컬러 외에도 파스텔톤의 핑크와 블루, 심플한 블랙까지 총 4가지 색상이 있어요.​​  ​사용방법1. 먼저 뒷면을 펼쳐서 삼각형 모양으로 세워줍니다.2. 맥세이프 지원되는 스마트폰을 부착합니다.3. 오른쪽 측면에 버튼을 누르면 충전이 시작돼요.4. 미지원 디바이스는 눕혀서 사용할 수 있어요.​​ ​뒷면을 펼쳐서 세우는 폴딩 방식이라 안정적으로 세워둘 수 있었는데요. 받침대가 스탠드를 펼치는 방식이라면 부러질 수도 있다는 단점이 있는데 그럴 염려가 없어요.​​ ​측면에 충전 버튼을 눌러줘야 시작하는데 버튼을 누르지 않으면 거치대로만 사용할 수 있다는 점이 장점이예요.스마트 FOD(Foreign Object Detection) 센서를 적용해 충전 상태와 이물질을 감지해 발열, 전력소비 방지해 과전압, 과전류, 과열 등으로부터 기기를 안전하게 보호해주는 기능도 해요.​​ ​이렇게 부착하면 바로 연결돼요. 최신형 MACSAFE를 완벽 호환한다는 장점이 있어요.또한 세로모드 뿐만 아니라 가로모드로 거치도 가능해서 유튜브나 넷플릭스 등 동영상을 보면서 충전하기도 편리해요.​​ ​맥세이프란 스마트폰에 마그네틱을 탑재해 자력에 의해 무선 충전이 가능하게 만든 기술을 말하는데요.애플에서 아이폰12 시리즈부터 새롭게 추가한 기능이예요.​핸드폰은 아이폰 12,13,14 프로, 맥스, 미니 등 기종이 지원되며 에어팟, 버즈 이어폰도 충전할 수 있어요.미지원 기기는 맥세이프 케이스나 그립톡, 액세서리 등을 장착해서 사용할 수 있어요.​​ ​휴대폰을 올려두는 부분은 부드러운 실리콘으로 되어있어서 케이스를 빼고 쓸 때 기기를 보호해주는 역할도 해요.보조배터리 5000mAh 용량이라 완충 시 약 1.6회 충전할 수 있어요. (아이폰 14프로 기준)핸드폰 뿐만 아니라 태블릿, 에어팟, 노트북까지 다양한 디바이스에 연결해서 사용가능하니까 실용적이예요.​​  ​배터리 잔량이 표시되니까 얼마나 남았는지 확인할 수 있어요.빨간색은 에러 발생, 파란색은 무선 충전중이라는 표시예요. LED 1개 점등은 배터리 잔량이 5~25% 남았다는 것을 의미하고 LED 2개는 25~50%, 3개는 50~75%, 4개는 75~100%를 뜻해요.​게다가 유무선 동시에 충전도 할 수 있어요. 무선충전을 하면서 C타입 포트를 연결해서 기기 2개를 동시에 충전할 수 있답니다.​​ ​애플 정품 대비 약 1.5배 강한 자력을 가지고 있어서 스마트폰을 흔들어도 딱 붙어 떨어지지 않아요.​가우스 측정치로 측정한 결과 정품은 약 1300가우스, 코딩 맥파워는 약 2100가우스로 측정되었어요.(가우스란 1㎠ 단면에 1맥스웰 자기 선속이 통과하는 자기장으로 값이 클수록 자성이 높고 부착력이 좋다는 것을 의미)​​ ​초강력 마그네틱을 사용했기 때문에 내 핸드폰을 안정적으로 고정해줘요. 약 900g의 하중을 견뎌낼 정도이니 정말 단단히 고정된다는 점을 알 수 있어요.거치대 기능으로도 활용할 수도 있어서 충전+거치를 동시에 사용가능하니 가방이 가벼워지죠.​​ ​아이폰만 되나? NO!삼성 갤럭시도 가능!세워서 사용하지 않고 눕혀서 쓸 땐 무선 충전을 지원하는 모든 기기가 호환돼요.​​ ​보조배터리로 충전할 때 속도가 느리면 참 답답하죠? 계속 달고 다녀야하니 불편하기도 하고요.이 제품은 MAX 15W 고속 충전이 지원된답니다. MAX 20W 유선 PD 충전도 지원되니까 더 빠르게 하고 싶다면 케이블 선을 연결해서 사용해도 돼요.​​ ​약 13mm 크기의 컴팩트한 사이즈라서 휴대성이 좋은 점도 장점이예요.슬림하고 가벼운 미니 보조배터리이기 때문에 가방에 쏙- 넣어서 다니기 좋아요.연결한 채로 한 손으로 전화를 받을 수도 있고, 셀카를 찍거나 동영상을 시청하는 등 자유롭게 조작할 수 있어요.​​ ​기능이 아무리 좋아도 안전한 제품인지가 가장 중요한데요.KC 전자파 적합, 전기안전확인 등 테스트를 거쳐 인증 받은 제품이라 안전성도 입증되었어요.​​ ​특히나 여행 다닐 땐 사진, 동영상을 많이 찍게되니까 배터리 용량이 빨리 닳아요.유선은 케이블을 주렁주렁 달고 다녀야해서 거추장스럽고 단자가 금방 고장나기도 하는데 무선은 자력으로 부착해서 주머니에 넣고 다니면 되니까 간편했어요.​​ 제품 스펙모델명 코딩 맥파워 보조배터리 KMPB05 용량 5000mAh 리튬 이온 배터리크기 105 X 66 X 135mm중량 157g입력 Type C(5V-3A/ 9V-2A), LIGHTNING(5V 2.4A)출력 Type C(5V-3A/ 9V-2A/ 12V-1.67A)Wireless(Max. 15W), PD(Max. 20W)재질 ABS, PC, 실리콘, PU코딩 맥파워 아이폰 고속 무선 충전 미니 맥세이프 보조배터리 5000mAh KMPB05 : 코딩[코딩] Smile with Kordingsmartstore.naver.com 제품을 팔고 끝이 아닌 향후 서비스까지 제공한다는 점이 좋은 브랜드라고 생각하는데요.코딩(Kording)은 무조건 새상품만 배송하며 1년 무상 AS 서비스도 지원이 된답니다. 사용 중 문제가 있다면 언제든지 고객센터로 연락할 수 있어요.​​  고속 충전도 빠르게 잘되고 거치대로 쓰기에 디자인도 심플해서 이뻐요. 콤팩트한 미니 사이즈에 다양한 디바이스를 안전하게 충전할 수 있는 코딩 맥세이프 보조배터리 추천해요. 이상 솔직 사용후기였습니다.​​​ ​ "
【프로그래밍 「 API 편」】 TensorFlow의 Object Detector API로 자신의 Object Detector를 훈련하는 방법 ,https://blog.naver.com/dnflsmsskek/222847569754,20220813,"이것은 ""Tensorflow 및 OpenCV를 사용하여 실시간 객체 인식 앱 빌드"" 에 대한 후속 게시물로 , 여기서 저는 제 수업을 교육하는 데 중점을 둡니다. 특히, 직접 수집하고 레이블을 지정한 데이터 세트에 대해 자체 Raccoon 감지기를 훈련했습니다. 전체 데이터 세트는 내 Github repo 에서 사용할 수 있습니다 .그건 그렇고, 여기에 너구리 감지기가 작동합니다. 너구리 탐지기.자세한 내용을 알고 싶다면 계속 읽어야 합니다!동기 부여내 마지막 게시물 이후에 많은 사람들이 TensorFlow의 새로운 Object Detector API 를 사용하여 자신의 데이터세트로 객체 감지기를 훈련시키는 방법에 대한 가이드를 작성해달라고 요청했습니다 . 할 시간을 찾았습니다. 이 게시물에서는 자신의 탐지기를 훈련하는 데 필요한 모든 단계를 설명합니다. 특히 비교적 좋은 결과로 라쿤을 인식할 수 있는 물체 감지기를 만들었습니다. 이런 젠장? 왜 너구리 🐼 ????별거 없어요😄 제가 제일 좋아하는 동물이기도 하고 어찌보면 이웃이기도 한데요! 맹세컨대, 너구리 감지기에는 많은 잠재적인 사용 사례가 있습니다. 예를 들어, 이제 집에 없는 동안 너구리가 문을 두드리는지 감지할 수 있습니다. 시스템에서 휴대전화로 푸시 메시지를 보내 방문자가 있다는 것을 알 수 있습니다. 전체 영상: https://youtu.be/Bl-QY84hojs데이터세트 만들기그럼 본격적으로 가자! 가장 먼저 해야 할 일은 나만의 데이터세트를 만드는 것이었습니다.Tensorflow Object Detection API는 TFRecord 파일 형식 을 사용하므로 결국 데이터세트를 이 파일 형식으로 변환해야 합니다.TFRecord 파일을 생성하는 몇 가지 옵션이 있습니다. PASCAL VOC 데이터 세트 또는 Oxford Pet 데이터 세트 와 유사한 구조를 가진 데이터 세트가 있는 경우 이 경우에 대해 기성 스크립트가 있습니다( create_pascal_tf_record.py및 참조 create_pet_tf_record.py). 이러한 구조 중 하나가 없으면 TFRecord를 생성하기 위해 고유한 스크립트를 작성해야 합니다(또한 이에 대한 설명 도 제공함 ). 이것이 내가 한 일이다!API용 입력 파일을 준비하려면 두 가지를 고려해야 합니다. 첫째, jpeg 또는 png로 인코딩된 RGB 이미지가 필요하고 두 번째로 xmin, ymin, xmax, ymax이미지에 대한 경계 상자 목록( )과 경계 상자에 있는 객체의 클래스가 필요합니다. 저 같은 경우에는 수업이 하나밖에 없었기 때문에 쉬웠습니다.Google 이미지 와 Pixabay 에서 200개의 너구리 이미지(주로 jpeg와 몇 개의 png)를 스크랩 하여 이미지의 크기, 포즈 및 조명에 큰 변화가 있는지 확인했습니다. 다음은 내가 수집한 Raccoon 이미지 데이터 세트의 하위 집합입니다. 너구리 이미지 데이터 세트의 하위 집합입니다.그 후 LabelImg 를 사용하여 수동으로 레이블을 지정했습니다 . LabelImg는 Python으로 작성되고 그래픽 인터페이스에 Qt를 사용하는 그래픽 이미지 주석 도구입니다. Python 2 및 3을 지원하지만 Python 3 및 Qt5에 문제가 있으므로 Python 2 및 Qt4로 소스에서 빌드했습니다. 사용이 매우 간편하고 주석이 PASCAL VOC 형식의 XML 파일로 저장되므로 스크립트를 사용할 수도 create_pascal_tf_record.py있지만 나만의 스크립트를 만들고 싶었기 때문에 이 작업을 수행하지 않았습니다.어쨌든 LabelImg는 MAC OSX에서 jpeg를 여는 데 문제가 있어서 png로 변환한 다음 나중에 jpeg로 다시 변환해야 했습니다. 사실, API도 이것을 지원해야 할 뿐만 아니라 png로 둘 수 있습니다. 나는 이것을 너무 늦게 깨달았다. 이것은 내가 다음에 할 일입니다.마지막으로 이미지에 레이블을 지정한 후 XML 파일을 csv로 변환한 다음 TFRecord를 생성하는 스크립트를 작성했습니다. 훈련( train.records)에 160개의 이미지를 사용하고 테스트( test.records)에 40개의 이미지를 사용했습니다. 스크립트는 내 repo에서도 사용할 수 있습니다 .메모:라는 다른 주석 도구를 찾았습니다.FIAT(빠른 이미지 데이터 주석 도구)그것도 좋은 것 같습니다. 앞으로는 이것을 시도해 볼 수 있습니다.여러 이미지를 다른 파일 형식으로 변환하는 것과 같은 명령줄에서 이미지를 처리하는 경우 ImageMagick 은 매우 좋은 도구입니다. 사용하지 않은 경우 시도해 볼 가치가 있습니다.일반적으로 데이터 세트를 만드는 것이 가장 고통스러운 부분입니다. 이미지를 분류하고 레이블을 지정하는 데 2시간이 걸렸습니다. 그리고 이것은 한 수업만을 위한 것이었습니다.이미지 크기가 중간인지 확인합니다(매체의 의미는 Google 이미지 참조). 이미지가 너무 크면 특히 기본 배치 크기 설정을 변경하지 않을 때 훈련 중에 메모리 부족 오류가 발생할 수 있습니다.모델 훈련API에 필요한 입력 파일을 만든 후 이제 모델을 훈련할 수 있습니다.교육을 위해서는 다음이 필요합니다.객체 감지 훈련 파이프라인 . 또한 repo에 샘플 구성 파일 을 제공합니다. 내 훈련을 위해 나는 ssd_mobilenet_v1_pets.config기초로 사용했습니다. 1로 조정하고 모델 체크포인트, 학습 및 테스트 데이터 파일, 레이블 맵에 대한 num_classes경로( )도 설정해야 했습니다. PATH_TO_BE_CONFIGURED학습률, 배치 크기 등과 같은 다른 구성의 경우 기본 설정을 사용했습니다.참고: 데이터 세트에 다른 크기, 포즈 등과 같은 가변성 이 data_augmentation_option많지 않은 경우 매우 흥미롭습니다. 옵션의 전체 목록은 여기 에서 찾을 수 있습니다 ( 참조 PREPROCESSING_FUNCTION_MAP).데이터세트(TFRecord 파일) 및 해당 레이블 맵. 레이블 맵을 만드는 방법의 예는 여기 에서 찾을 수 있습니다 . 다음은 클래스가 하나만 있었기 때문에 매우 간단한 레이블 맵입니다.​ item {  id: 1  name: 'raccoon'} 참고: 레이블 맵은 항상 id 1에서 시작해야 합니다. 인덱스 0은 자리 표시자 인덱스입니다(이 주제에 대한 자세한 내용은 이 토론 참조 ).(선택 사항) 사전 학습된 모델 체크포인트. 처음부터 훈련하면 좋은 결과를 얻기까지 며칠이 걸릴 수 있으므로 사전 훈련된 모델에서 시작하는 것이 항상 더 좋으므로 체크포인트를 사용하는 것이 좋습니다. 그들은 자신의 리포지토리에 여러 모델 체크포인트 를 제공 합니다. 제 경우에는정확도보다 모델 속도가 더 중요 했기 때문에 ssd_mobilenet_v1_coco 모델을 사용했습니다.이제 교육을 시작할 수 있습니다.교육은 로컬 또는 클라우드 (AWS, Google Cloud 등)에서 수행할 수 있습니다. 집에 GPU(최소 2GB 이상)가 있는 경우 로컬에서 수행할 수 있습니다. 그렇지 않으면 클라우드를 사용하는 것이 좋습니다. 제 경우에는 이번에 Google Cloud 를 사용했으며 기본적으로 설명서 에 설명된 모든 단계를 따랐습니다 .GCP의 경우 YAML 구성 파일을 정의해야 합니다. 샘플 파일도 제공 되는데 기본적으로 기본값을 그대로 사용했습니다.또한 교육 중에 평가 작업을 시작하는 것이 좋습니다. 그런 다음 로컬 머신 에서 Tensorboard 를 실행하여 교육 및 평가 작업의 프로세스를 모니터링할 수 있습니다 . tensorboard — logdir=gs://${YOUR_CLOUD_BUCKET} 다음은 교육 및 평가 작업의 결과입니다. 전체적으로 24개의 배치 크기로 약 1시간/22k 단계 이상 실행했지만 이미 약 40분 만에 좋은 결과를 얻었습니다.총 손실은 다음과 같이 발전했습니다. 사전 훈련된 모델로 인해 총 손실이 상당히 빠르게 감소했습니다.클래스가 하나밖에 없었기 때문에 총 mAP(평균 정밀도)만 보는 것으로 충분했습니다. mAP는 약 20,000단계에서 0.8을 기록했는데 이는 상당히 좋은 수치입니다.다음은 모델을 훈련하는 동안 하나의 이미지를 평가하는 예입니다. 너구리 주변에서 감지된 상자는 시간이 지남에 따라 훨씬 좋아졌습니다.모델 내보내기훈련을 마친 후 추론에 사용할 수 있도록 훈련된 모델을 하나의 파일(Tensorflow graph proto)로 내보냈습니다.제 경우에는 GCP 버킷에서 로컬 머신으로 모델 체크포인트를 복사한 다음 제공된 스크립트 를 사용하여 모델을 내보내야 했습니다. 프로덕션에서 실제로 사용하려는 경우를 대비 하여 내 repo 에서 모델을 찾을 수 있습니다 .)보너스:YouTube에서 찾은 동영상에 학습된 모델을 적용했습니다.비디오를 본 경우 모든 너구리가 감지되지 않거나 일부 잘못된 분류가 있음을 알 수 있습니다. 이것은 우리가 작은 데이터 세트에서만 모델을 훈련했기 때문에 논리적입니다. 예를 들어 지구상에서 가장 유명한 너구리인 가디언즈 오브 갤럭시의 로켓 라쿤을 감지할 수 있는 보다 일반화되고 강력한 너구리 감지기를 만들려면 훨씬 더 많은 데이터가 필요합니다. 그것은 바로 지금 AI의 한계 중 하나일 뿐입니다! 지구상에서 가장 유명한 너구리.결론이 게시물이 마음에 드셨으면 합니다. 하시면 ❤️ 주세요. 이제 자신만의 물체 감지기를 훈련할 수 있기를 바랍니다. 이 기사에서는 더 많은 데이터에 레이블을 지정하는 것이 귀찮았기 때문에 하나의 클래스만 사용했습니다. 데이터 레이블링 서비스를 제공하는 CrowdFlowe r, CrowdAI 또는 Amazon의 Mechanical Turk 와 같은 서비스가 있지만 이 기사에서는 너무 많습니다.짧은 훈련 시간 동안 꽤 괜찮은 결과를 얻었지만 이는 검출기가 단일 클래스에서만 훈련되었기 때문입니다. 더 많은 클래스의 경우 총 mAP가 내가 얻은 것만큼 좋지 않으며 좋은 결과를 얻으려면 확실히 더 긴 교육 시간이 필요합니다. 사실 Udacity에서 제공하는 주석이 달린 주행 데이터 세트(Dataset 1) 에 대한 객체 감지기도 훈련했습니다.. 자동차, 트럭 및 보행자를 제대로 인식할 수 있는 모델을 훈련하는 데 꽤 오랜 시간이 걸렸습니다. 다른 많은 경우에 내가 사용한 모델도 여러 클래스의 모든 변동성을 캡처하기에는 너무 단순하여 더 복잡한 모델을 사용해야 합니다. 또한 고려해야 하는 모델 속도와 모델 정확도 사이에는 트레이드오프가 있습니다. 그러나 이것은 다른 이야기이며 실제로는 또 다른 독립 기사일 수 있습니다. "
DINO 효과-Self-Supervised Transformers for Unsupervised Object Discovery using Normalized Cut ,https://blog.naver.com/cobslab/222849991268,20220816,"안녕하세요 콥스랩(COBS LAB)입니다.오늘 소개해 드릴 논문은 ‘Self-Supervised Transformers for Unsupervised Object Discovery using Normalized Cut’입니다.해당 내용은 유튜브 ‘딥러닝 논문 읽기 모임' 중 ‘Self-Supervised Transformers for Unsupervised Object Discovery using Normalized Cut' 영상 스크립트를 편집한 내용으로, 영상으로도 확인하실 수 있습니다. (영상링크:https://youtu.be/JCEK5nD4MKM)   먼저 Self-Supervised에 대해서 말씀드리겠습니다. 자연어 처리에서의 Self-Supervised learning과 vision Transformer의 차이를 살펴보겠습니다. 자연어 처리에서는 이렇게 my [mask] is cute [sep] king [mask] dogs 이런 식으로 일반적인 문장에서 일정 부분의 단어들을 마스킹하고 그 단어들을 예측하도록 하고 있습니다. 그래서 BERT는 기본적으로 모든 데이터를 학습할 수 있습니다. label이 없더라도 데이터에서 마스크 하면 되기 때문입니다. 반대로 vision Transformer는 label 된 이미지 입력을 작게 쪼개서 입력을 하고 그 다음에 거기에 대해서 label을 맞추는 방식으로 학습을 하게 됩니다. 그래서 label한 데이터만 가능하게 됩니다. 그래서 자연어 처리와 vision Transformer의 Pretrain의 가장 큰 차이는 label된 데이터를 사용하느냐, 사용하지 않느냐 혹은 label된 데이터를 사용한다는 건 데이터의 활용 폭이 적어진다 이렇게 볼 수 있습니다.  이를 해결하기 위해서 Self-Supervised learning들이 시도되고 있습니다. 그중에서 SiT라는 방법을 살펴보겠습니다. SiT는 보시면 이미지가 있는데 이 이미지를 쪼개서 나올 때 노이즈를 가하게 됩니다. Random Drop, Random Replace, Color Distortion, Bluring, Grey-Scale을 통해서 이미지를 왜곡시킨 다음에 마지막에 다시 원본 이미지를 예측하도록 하는 게 이 Image Reconstruction입니다. 이렇게 해서 로스는 원본 이미지와 예측된 이미지에 L1 로스를 가지고 L1 로스가 줄어들도록 학습을 하게 됩니다.  두 번째 방법은 이미지를 Rotation을 시킵니다. 0도 90도 180도 270도 Rotation을 시키고 Rotation에 대한 것이 4가지가 되고 4가지에 대한 확률분포를 구해서 0도인지 90도인지 180도인지 270도인지 맞추는 맞추는 classification 문제로 Rotation Prediction을 하도록 하고 있습니다.  SiT에서 세 번째로 쓰는 방법은 Contrastive learning 각각에 대해서 vector가 비슷한 것과 배치 내에서 다른 것들을 이미지에서 비슷한 vector들을 찾아내도록 하는 Contrastive learning을 통해서 학습을 하게 되면 좋은 특징들을 얻을 수 있는 모델을 학습할 수 있다고 설명하고 있습니다.  다음으로 볼 논문은 DINO라는 아키텍처입니다. 오늘 설명드릴 모델도 DINO를 기반으로 해서 만들어진 모델입니다. 원본 이미지가 있으면 augmentation를 각각 다른 방법으로 합니다. Augmentation을 하고 그다음에 이미지를 얻습니다. 여기서 보시면 이쪽에 Teacher, Student 이렇게 이미지를 만들고 각각 이미지를 집어넣습니다. 다음 나온 곳에서 소프트맥스를 각각 하게 되는데 Teacher는 Teacher 예측값에서 센터를 뺀 값, Student는 예측값 그대로 해서 나올 두 개의 확률에 대해서 크로스엔트로피를 구하고 엔트로피 로스를 줄여주는 식으로 학습을 하게 됩니다. 그리고 Student만 backpropagation을 하고 그 다음에 EMA(Exponential Moving Average)를 통해서 Student에 있는 웨이트를 피쳐로 옮겨가는 방식으로 Teacher를 학습하지 않고 Student에 있는 웨이트로 옮겨가는 방식으로 학습을 합니다.  Psuedo code입니다. 원본 이미지가 있을 때 Augment를 각각 다른 방법을 해서 x1과 x 2가 있으면 Student에도 x1과 x2를 넣고 그다음에 Teacher에도 x1과 x2를 넣고 나온 친구들을 로스 함수를 통해서 t1과 s2 그리고 t2와 s1 이렇게 다르게 해서 로스를 구하고 이 로스를 minimize 하게 됩니다. 그리고 gradient를 구하고 Teacher의 파라미터에다가 l 값에다가 1-l에서 Student 파라미터를 해줍니다. 이때 람다 값 l은 0.996에서 1 사이기 때문에 매우 작은 값입니다. 그래서 Student가 매우 작게 반영돼서 지속적으로 Teacher가 반영된다 보시면 됩니다. 그다음에 센터값은 중앙값을 의미하는데 Teacher에 있는 값의 중앙값인데 이전 센터값과 새로운 센터 값을 averaging 해가면서 센터값을 계산하게 되고 이렇게 하면 모델이 어떤 학습이 안 되는 것들을 막을 수 있다고 합니다. h함수를 보게 되면 Teacher는 약간 backpropagation하지 않기 때문에 detach를 하고 그다음에 Student는 바로 템퍼러쳐라고 해서 소프트맥스 취했는데 Teacher는 센터값을 빼준 상태에서 temperature를 나눠서 했습니다. 그리고 나서 negative로 log likelihood를 구한 것을 알 수 있습니다.  이렇게 했더니 신기하게도 이 오브젝트에서 보시면 attention Map이 이렇게 잘 잡아내고 있습니다. 또 칫솔도 attention Map을 잘 잡아내고 있습니다.  이런 사진에서도 주요한 이미지들을 잘 뽑아내고 있는 것을 볼 수 있습니다. 이렇게 DINO 방법으로 학습을 하게 되면 중요한 이미지들이 좋은 vector를 갖게 됩니다. Supervised 방식과 DINO를 비교했을 때도 DINO가 훨씬 더 좋은 attention Map을 가지고 있는 것을 알고 있습니다. 그중에 비슷한 vector들이 같은 곳을 잘 향하고 있다 이렇게 볼 수 있을 것 같습니다.  그다음에 LOST 함수입니다. LOST 모델인데 DINO backbone을 그대로 사용하고 있고 DINO backbone에서 attention Map보다 좀 더 좋은 방법으로 어떤 객체를 뽑아내는 그런 방법을 제시했습니다. 예를 들어 배 이미지가 있을 때 이미지를 넣어서 쪼개는데 좀 크게 쪼개보겠습니다. 우리가 봤을 때 1 2 3은 아마 비슷한 vector를 갖게 될 것 같고 이미지가 비슷하니까 또 5 6 번도 비슷할 것 같습니다. 그리고 4 7 8 9가 비슷할 것 같고 6번과 4번 은 비슷하지 않을까 이렇게 가정을 해보고 통과하게 되고 이때 두 vector들간의 1번과 2번,1번과 3번 이렇게 두 vector에 dot product를 하고 그 다음에 그 친구가 0보다 크면 90도 이내라는 얘기입니다. 90도 이내에 있으면 1이고 그렇지 않으면 0이라고 했습니다. 이것들을 쭉 그려보면 Adjacency Matrix가 있습니다. 1번의 경우에는 2 3번과 비슷하고 2번의 경우 1 3번과 비슷하고 3번의 경우에는 2번과 비슷하고 4번의 경우에는 6 7 8 9번 이런 식으로 매트릭스를 구할 수 있을 것 같습니다. 그리고 다 썸 하면 첫 번째 몇 개가 있는지 연결되는지 볼 수 있습니다. 아래쪽에 보시면 Degree Matrix를 구할 수 있습니다. 1번은 2개 하고 연결되어 있고 2번은 두 개 이런 식으로 구할 수가 있게 되고 이 값 중에서 가장 작은 값 seed라고 얘기하고 있으면서 Degree matrix를 만들고 이 중에 가장 작은 것을 Seed로 잡습니다.  전체를 쭉 보면 6번까지 들어가 있습니다. 그래서 6번까지를 이런 식으로 여기 0보다 큰 것들을 잡으면 5번과 6번이 Seed가 되고 이걸 가지고 Box Extraction를 하게 되면 5 6번에 있는 모든 1인값들이 4 5 6번 이렇게 있으니까 4 5 6 번 이렇게 해서 거기다가 bounding box를 치겠다는 게 LOST가 제시하는 모델입니다.  LOST가 이렇게 했더니 위 같은 이미지가 있을 때 히든 값들이 잡히고 오브젝트가 잘 잡히고 표현되는 것을 알 수가 있습니다. 그래서 DINO 보다 LOST를 썼더니 더 좋아졌다. 이렇게 설명하고 있습니다.  다음은 Token Cut입니다. LOST 하고 비슷하니까 동일하게 이렇게 이미지를 9 개를 쪼개서 넣으면 similarity 함수가 달라졌습니다. 여기 보시면 이런 식으로 Cosine 세타 값을 구합니다. 그래서 Cosine 세타가 하이퍼 파라미터를 줬는데 0.2보다 클 때는 1이고 나머지는 le-5승 이런 작은 값을 줬습니다. fully connected 그래프 만들겠다고 얘기를 하고 있는데 0이라고 가정을 해도 크게 문제가 없을 것 같습니다. 이렇게 했을 때 이런 그래프를 그릴 수 있고 아까 하고 동일하게 Adjacency 매트릭스와 Degree 매트릭스를 만들 수 있습니다. 고양이 사진이 첫 번째 과정인데 이렇게 이미지를 넣고 통과한 다음에 이렇게 그래프를 만드는 과정을 얘기를 하고 있습니다.  그래프를 만들었으면 이 그래프까지 만들어졌는데 4번에 연결돼 있다고 가정해보겠습니다. 이렇게 가정을 하고 봤을 때 나누는 방법을 좀 살펴보면은 여기서 2nd smallest eigen value & vector Adjacency 매트릭스와 Degree 매트릭스를 이용해서 2nd smallest eigen value & vector를 구하면 됩니다. 자세한 건 뒤에 가서 설명드리겠습니다. 다음에 보시면 eigen vector에 i값을 여기서 각 성분들을 곱하면 평균값이 이렇게 위치하게 된다고 얘기를 하고 있습니다. 그러면 y햇보다 작은 건 A, Y햇 보다 큰 건 B 이런 식으로 나눌 수 있습니다. 이것이 두 번째 그림에 있는 2nd smallest eigen value & vector를 구하는 방법과 같다고 하고 있습니다.  foreground냐 background냐 결정을 해야 되는데 아까 나눴으니까 평균값이 나오고 그러면 이 합이 같습니다. 그러면 B 쪽에 있는 값이 절댓값이 훨씬 큽니다. 그래서 이 절대값이 가장 큰 것이foreground일 것이다. 여기서는 아마 5가 갖고 있을 것 같습니다. 여기 있는 게 foreground에 있는 그 가장 max value에 있는 위치 즉, Foreground의 Seed와 같은 그런 위치라 볼 수 있습니다. 그래서 절대값 Vmax를 포함하고 있는 largest connected foreground 즉, 가장 크게 연결된 것을 foreground라고 볼 수 있다라고 얘기를 하고 있습니다.  Experiment입니다. 여기서 IoU는 CorLoc라는 걸 사용했는데 여기서는 IoU가 0.5 이상인 bounding box 개수를 TP로 보고 precision을 보이는데 보시면 아까 봤던 DINO, LOST 그다음에 지금 하고 있는TokenCut을 봤을 때 앞에 있는 것보다는 앞에 있는 DINO나 LOST에 비해서 훨씬 더 잘 잡아내고 있다 이렇게 얘기를 하고 있고 이 성능도 Token Cut이 좀 더 좋은 성능을 내고 있는 것을 볼 수 있습니다.  다음에 두 번째 Weakly Supervised Object입니다. 마지막 layer하나만 학습이 끝나고 나서 마지막 linear layer 하나만 학습했을 때 얘기를 하고 있습니다. 이때 매트릭을 세 가지로 얘기를 하고 있습니다. Top-1 Cls는 이미지 classification에서 첫 번째로 예측한 것이 맞을 확률, 그다음에 방금 봤던 GT Loc IoU 0.5인 bounding box, Top-1 Loc는 두 가지가 모두인 경우 얘기를 하고 있는데 보시면 테스트는 두 가지로 진행했습니다. ImageNet을 pretraining했고 그 다음에 ImageNet으로 Fine tune했을 때고 그 다음에 CUB라는 데이터셋으로 Fine tune했을 경우인데 Supervised의 경우는 ImageNet으로 pretraining을 하고 ImageNet에서 Fine tune을 하는 좋은 결과를 내고 있습니다. 그런데 Unsupervised의 경우에는 Token Cut이 Supervised보다 더 좋은 성능을 내고 있습니다. 이 의미는 뭐냐면 더 일반화가 잘 돼 있다 그래서 학습하지 않은 데이터를 Token Cut이 더 잘 채우고 있다고 얘기를 하고 있습니다.  그 다음에 Salient object는 가장 중요한 오브젝트를 잘 찾아냈느냐 얘기를 하고 있는데 F-measure, IoU, Accuracy 등으로 하고 있습니다. 보시면 이렇게 인풋이 있고 groundtruth가 있는데 이 친구들을 잘 잡아내고 있는 것을 보고 있습니다. 여기다가 Bilateral Solver라는 Token Cut 그러니까 Object detection을 썼더니 더 잘 된다고 얘기를 하고 있습니다.  Ablation Study입니다. τ를 0부터 이렇게 변화를 줬을 때 0.2가 가장 좋은 결과를 냈다고 얘기를 하고 있습니다. 그리고 아래쪽은 인터넷에서 받을 수 있는 이미지입니다. 이런 이미지들이 있을 때 꽤나 어려운 것임에도 불구하고 TokenCut이 잘 detection 하고 있는 것을 볼 수 있습니다. 오른쪽 그림 은은 잘 안 되는 경우를 얘기를 하고 있습니다. 그래서 보시면 LOST는 잘 됐습니다. 파란색이 정답이고, 빨간색이 예측한 건데 이런 한계들도 좀 있었다 얘기하고 있습니다.  Laplacian Matrix를 간단하게 설명드리겠습니다. 이런 그래프가 있다고 가정을 해보겠습니다. 그러면 이걸 가지고 Degree 매트릭스와 Adjacency 매트릭스를 구할 수 있습니다. 그리고 이걸 가지고 Degree 매트릭스 - Adjacency 매트릭스를 하면 이런 매트릭스를 구할 수 있습니다. 이것을 Laplacian 매트릭스라고 얘기를 하고 있습니다. 그리고 보시면 i와 j 같은 경우는 Degree 값인 거고 다음에 i와 j가 다르고 Adjacency면 - 나머지는 0인 Laplacian 매트릭스를 구할 수 있습니다.  Laplacian 매트릭스 특징은 일단 Symmetric 매트릭스입니다. 대각선 기준으로 양쪽이 같고 다음에 eigen value는 non negative real numbers이고 Eigen Vector는 real이고 Orthogonal 한 그러니까 Eigen vector들은 다 직교한다고 얘기하고 있습니다.  첫 번째가 smallest eigen vector는 모두 1이면 구할 수 있습니다. 왜냐하면 이 vector 하고 첫 번째와 곱하면 각 성분들은 다 더한 게 됩니다. 항상 다 0이 됩니다. 0이 되기 때문에 Eigen vector는 모두 x이 되고 Eigen value는 람다 값이 0이 되는 Laplacian 매트릭스에서 가장 smallest eigen value가 되겠습니다.  second smallest 구하는데 symmetric 매트릭스에서 eigen value를 구하는 것은 위 수식으로 구할 수 있다고 합니다. 그리고 아래 수식에 따라서 전개를 하면 됩니다. 기본적으로 밑에 식과 각각의 성분의 차이의 값을 최소가 되도록 하는 게 목표입니다.  보시면 여기서 가정을 두 가지를 넣어 보겠습니다. 일단은 unit vector가 맞을 수 있으니까 모든 2nd smallest eigen vector는 unit vector다. 즉 제곱의 합이 1이다라고 볼 수 있습니다. 아까 orthogonal 하다고 했기 때문에 smallest eigen vector는 모두 2였기 때문에 그래서 orthogonal 하다는 얘기는 Dot product가 0이기 때문에 Dot product가 0이 되고 결국은 모든 성분의 합이 0이 되어야만 된다는 두 가지 조건이 있습니다.  이 조건을 가지고 이렇게 볼 수 있습니다. 0을 기준으로 이 합들이 제곱합이 다 1이 달려있기 때문에 일정 범위에서 잘 분포가 되어 있어야 되고 이 사이에 이쪽에는 거리 들 만큼이 곱해질 것이기 때문에 i 제곱의 차이입니다. xi와 xj 사이기 때문에 이 선들이 최소화되도록 해야지 2nd smallest를 구할 수 있습니다. 이 선이 최소가 되는 그룹으로 나눠야 된다 이런 식으로 나눠지는 해를 구하는 게 결국은 2nd smallest eigen vector를 구하는 것과 같다고 볼 수 있습니다. "
"아이폰 고속 무선 충전기 거치대 추천, LED 탁상시계 기능까지 ",https://blog.naver.com/518_518/223009042048,20230208,"시계가 없을 때와 스마트폰 배터리가 없을 때 불안감을 느끼곤 합니다. 그래서 외출 전 스마트폰 배터리 충전과 손목시계는 필수로 준비해야 하는데요. 꼭 외부에서뿐만 아니라, 집에 있을 때에도 마찬가지로 불안하더라고요. 오늘은 이 두 가지를 모두 해소할 수 있는 제품을 추천드리겠습니다. LED 탁상시계와 알람시계로 이용함과 동시에 고속 충전이 가능한 아이폰 무선 충전기로 함께 사용할 수 있습니다.  무아스의 미니 스퀘어 무선 충전기입니다. 가로 85cm, 세로 88cm이며 무게 133g으로 매우 콤팩트한 사이즈이며 컬러는 다크 그레이와 베이지 두 가지 컬러가 있는데요. 제가 사용하는 베이지 컬러는 콤팩트한 사이즈와 더불어 어떤 곳에 놓아도 주변 물건과 잘 어울리는 색감을 가지고 있습니다. 산업통상자원부와 한국디자인진흥원에서 우수 디자인 상품으로 선정이 되었다고 하더라고요. 제품 구성품은 무선 충전기 겸 탁상시계 본체, C 타입 케이블, 매뉴얼이 있습니다.   제품의 후면에는 탁상시계의 시간을 맞추거나 알람시계 시간 설정을 할 수 있는 네 개의 버튼과 USB-C 타입 포트가 위치해 있고, 아래면은 안정적인 거치를 위해서 논슬립 패드가 부착되어 있습니다. 전면은 LED 탁상시계와 알람시계로, 상단은 무선 충전기 패드로 사용이 가능하며, 위와 같이 패드를 펼쳐주어 무선 충전 거치대로까지 활용성이 다양하다는 것이 제품의 특징입니다.  LED 밝기가 무척 밝아서 빛이 많은 공간에서도 시계를 보는 데에는 전혀 문제가 없는데요. 반대로 취침 시에는 밝은 LED가 수면에 방해를 줄 수 있겠죠. 무아스 LED 탁상시계는 간단한 조작으로 3단계 LED 밝기 조절을 할 수 있어서 침실이나 책상 위 어디에서도 사용하기에 편리했습니다.  시간 및 알람 설정 방법. 시간은 24H, 12H 두 가지로 설정을 해줄 수 있습니다. 제품 후면에 시계 버튼을 길게 누른 후 '+'또는 '-' 버튼으로 시간을 설정하고, 시계 버튼을 한 번 더 누르면 분 설정으로 이동할 수 있습니다. 알람시계 설정도 같은 방법으로 제품 후면에 알람 버튼을 길게 누른 후에 '+'와 '-'로 시간을 설정하고, 알람 버튼을 한 번 더 눌러 분을 설정할 수 있습니다.  알람이 울리는 도중에 상단 무선 충전 패드를 짧게 누르면 알람이 멈추고 디스플레이에는 'Zzz'표시와 함께 스누즈 기능이 활성화되는데요. 스누즈 간격은 9분이며 반복 횟수는 무제한입니다. 다시 충전 패드를 길게 누르고 있으면 스누즈가 비활성화로 바뀌게 됩니다.   15W 고속 무선 충전 기능을 제공하고 있다는 게 무엇보다 큰 장점입니다. 저는 무선 충전이 가능한 이어폰과 스마트폰을 주로 이곳에 올려놓고 사용하고 있는데요. 무선 충전이 가능한 기기 외에 금속물이나 이물질을 감지하여 자동으로 충전을 차단해 주는 FOD(Foregn Object Detection), 과전압과 과전류로부터 기기를 보호하는 OVP(Over Voltage Protection), 기기 발열 시 센서가 탐지하여 보호 기능을 해주는 TSC(Temperature Sensor Control)까지 갖추고 있어 무선 이어폰도 안심하고 사용할 수 있었습니다.   거치대로 사용할 때는 아이폰 12mini의 경우 스마트폰을 세워서 거치하여도 흔들림 없이 고정은 잘 되었으나, 동시에 충전을 해주기 위해서는 스마트폰을 가로로 놓고 사용하는 것이 편리했습니다.   LED 탁상시계나 알람시계로 아울러 무선 충전기로 함께 사용할 수 있으니 책상 위에 놓거나 침실 머리맡에 놓아 정말 유용하게 사용할 수 있었고, 동영상 시청 시 무선 충전 거치대로 충전과 거치를 한 번에 할 수 있어서 유선으로 충전을 하면서 동영상을 시청하는 것보다 훨씬 편리했습니다.   ​디자인부터 다양한 활용성까지 갖추어 사무실 책상 위나 침실에 하나씩 두고 사용하기에 좋은 무아스 LED 탁상시계 & 무선 충전기, 합리적인 가격에 다양한 기능까지 갖춘 무아스 15W 거치형 미니 스퀘어 무선 충전 알람시계, 여러분들께 추천드립니다. 무아스 15W 미니 스퀘어 거치형 무선충전 탁상시계 : 무아스[무아스] 단순한 소모품을 넘어서 일상의 즐거움이 되는 미니멀하고 모던한 감성 소품, 무아스smartstore.naver.com ​ "
"[Paper] Performance Evaluation of Object Tracking Algorithms (Proc. IEEE Int. Workshop PETS, 2007) ",https://blog.naver.com/cho-yani/222121282358,20201020,"원문 : https://www.semanticscholar.org/paper/Performance-evaluation-of-object-tracking-Yin-Makris/ad76bdc7d06a7ec496ac788d667c6ad5fcc0fe41?p2df  Fei Yin, Dimitrios Makris, Sergio VelastinDigital Imaging Research Centre, Kingston University London, UK{fei.yin, d.makris, sergio.velastin} @ kingston.ac.uk Abstract이 논문은 motion tracking의 성능 평가라는 비견할 수 없는(non-trivial) 문제를 다룬다. 우리는 motion tracking의 성과에 대한 다양한 측면을 평가하기 위한 풍부한 측정 기준을 제안한다. 우리는 다양한 도전(challenges)을 나타내는 6가지 다른 비디오 시퀀스를 사용하여 두 개의 motion tracking 알고리즘을 평가하고 비교함으로써 제안된 metric의 실질적인 가치를 설명한다. 우리 프레임워크의 기여는 특정 모듈의 성능이나 특정 조건에서의 고장 등 motion tracker의 특정 약점을 식별할 수 있게 해주는 것이다. 1. Introduction상당한 연구 노력이 비디오 기반 motion tracking[1] [2] [3] [4]에 집중되어 업계의 관심을 끌고 있다. Motion tracking의 성능 평가는 연구자의 알고리즘 비교 및 추가 개발뿐만 아니라 영국의 i-LIDS 프로그램에 의해 정형화된 기술의 상용화 및 표준화를 위해서도 중요하다[5].​본 논문에서는 motion tracking 성능의 다른 측면을 강조하기 위해 사용되는 motion tracking metric 셋을 선택했다. 다양한 dataset을 사용하여 두 개의 motion tracking 알고리즘 평가에서 제안된 metric의 목적을 설명한다.​Ellis[6]는 보안 감시 시스템의 효과적인 성능 분석을 위한 주요 요구사항을 조사하고 비디오 dataset의 특성화를 위한 몇 가지 방법을 제안했다.​Nascimento와 Markes[7]는 서로 다른 motion detection 알고리즘의 output을 지정된 ground truth와 비교하고 Correct Detections, False alarm, Detection failure, Merges 및 Splits와 같은 객관적인 지표를 추정하는 프레임워크를 제안했다. 그들은 또한 다양한 매개변수에 걸쳐 알고리즘을 특성화할 수 있는 ROC와 같은 곡선을 제안했다.​Lazarevic-McManus 외 연구진[8]은 ROC와 유사한 곡선 및 F-measure에 기초한 motion detection 평가를 가능하게 하는 object-based 접근법을 개발했다. 후자는 application 도메인을 고려한 단일 값을 사용하여 straight-forward comparison을 허용한다.​Needham과 Boyle[9]은 trajectory를 비교하고 motion 시스템을 추적하기 위한 일련의 지표와 통계를 제안했다.​Brown 외 연구진[10]은 True Positive(TP), False Positive(FP) 및 False Negative(FN), Merged and Split trajectory의 수를 추정하는 motion tracking 평가 프레임워크를 제안한다. 그러나 시스템 track centroid와 확대된 ground truth bounding box의 비교에 기초한 그들의 정의는 큰 object의 track을 선호한다.​Bashir와 Porikli[11]는 ground truth의 spatial overlap과 큰 object에 치우치지 않는 시스템 bounding box에 근거하여 위의 지표를 정의하였다. 그러나 그것들은 프레임 샘플 단위로 계산된다. 이러한 접근법은 성능평가의 목적이 object detection일 때 정당화된다[7] [8]. Object detection에서, 프레임보다는 track의 관점에서 TP, FP, FN을 측정하는 것은 최종 사용자의 기대에 부합하는 자연스러운 선택이다.​본 논문은 다음과 같이 정리되어 있다. Section 2는 motion tracking과 track에 대한 정의를 제공한다. Section 3에서는 성능평가 방법론을 설명하고 다른 측정기준에 대한 정의를 설명한다. 결과는 section 4에서 제시되고 논의된다. Section 5는 논문을 마무리짓는다. 2. Motion Tracking우리는 motion tracking을 비디오 시퀀스의 각 프레임에 대한 non-background object의 위치와 spatial extent를 추정하는 문제로 정의한다. Motion tracking의 결과는 scene의 모든 M개의 moving object에 대한 track Tj (j = 1, ... , M)의 집합이다. Track Tj = {xij, Bij} (i = 1, ... , N)로 정의되며, 이때 xij와 Bij는 각각 프레임 i에 대한 object j의 center와 spatial extent(대개 bounding box로 표시됨)이며, N은 프레임의 개수다. 3. Performance Evaluation3.1 Preparation우리는 시스템의 성능을 평가하기 위해 motion tracking 시스템의 output을 Ground Truth(GT)와 비교하는 일련의 측정 기준을 제안한다.​평가 지표를 소개하기 전에, GT track과 시스템(ST) track 사이의 matching 수준을 공간과 시간적으로 모두 정량화하는 데 필요한, track 간 spatial 및 temporal overlap의 개념을 정의하는 것이 중요하다.​Spatial overlap은 특정 프레임 k에서 GTi track과 STj track 사이의 overlapping level A(GTi , STj)로 정의된다(그림 1). Eq.1그림 1. GT(ik)와 ST(jk)의 교집합과 합집합또한 우리는 threshold Tov(여기서는 20%)를 기반으로 binary variable O(GTi , STj)를 정의한다. Eq.2Temporal overlap TO(GTi , STj)는 ST track j와 GT track i 사이의 frame span의 overlap을 나타내는 숫자다. Eq.3여기서 TOS는 두 track의 첫 번째 frame index 중 더 큰 값이고 TOE는 두 track의 마지막 frame index 중 더 작은 값이다. Track 사이의 temporal, spatial overlap은 그림 2에 도해되어 있다. 그림 2. Track overlapping의 예우리는 다음 조건에 따라 ST track을 GT track에 연결하기 위해 temporal-overlap 기준을 사용한다: Eq.4여기서 Length는 프레임 수이고 TRov는 임의의 threshold이다. Eq 4가 참이면 ST track을 GT track과 연결하고 ST track의 성능을 평가하기 시작한다. 3.2 Metrics이 section에서는TP, FP 및 FN track과 같은 높은 수준의 metric에 대한 정의를 제시한다. 그러한 metric은 Specificity 및 Accuracy와 같은 metric을 추정하기 위한 기초가 되며 [11] ROC 같은 곡선을 구성할 수 있기 때문에 [8] 유용하다. Track Fragmentation 및 ID 변경과 같은 metric은 트랙의 무결성(integrity)을 평가한다. 마지막으로, 위치(Track Matching Error), spatial extent(Closeness), 완전성 및 temporal latency를 추정할 때 motion tracking의 accuracy를 측정하는 metric을 정의한다.​Correct detected track(CDT) 즉, TPGT track은 다음 두 가지 조건을 모두 만족하는 경우 올바르게 detect된 것으로 간주된다.​조건 1: GT track i와 ST track j 사이의 temporal overlap이 track overlap threshold TRov(여기서는 15%)보다 크다. Eq.5조건 2: ST track j는 GT track i와 충분한 spatial overlap이 있다. Eq.6각 GT track은 위의 조건에 따라 모든 ST track과 비교된다. 둘 이상의 ST track이 하나의 GT track에 대한 조건을 충족한다고 해도(fragmentation 때문일 가능성이 있음), 우리는 여전히 GT track이 정확하게 detect되었다고 생각한다. Fragmentation error는 별도로 계산한다(아래 참조). 따라서 각 GT track이 (하나 이상의 ST track에 의해) 올바르게 detect되는 경우, CDT의 수는 GT track의 수와 동일하다.​False Alarm Track(FAT) 즉, FP​사람은 복잡한 상황에서도 FAT(event)이 무엇인지 깨닫기 쉽지만, 자동화된 시스템으로는 이를 실현하기 어렵다. 여기서 FAT에 대한 실질적인 정의를 제시한다(그림 3). ST track이 다음 조건 중 하나라도 충족하면 ST track을 false alarm으로 간주한다.​조건 1: ST track j와 GT track i의 temporal overlap이 TRov보다 작다. Eq.7조건 2: ST track j는 GT track i와 temporal overlap은 충분하지만, 어떠한 GT track과도 spatial overlap은 충분하지 않다. Eq.8그림 3. 두 개의 FAT의 예. 왼쪽 ST는 Eq.7을 만족하지 않고, 오른쪽 ST는 Eq.8을 만족하지 않는다.FAT는 중요한 metric인데, operator에 의해 TP 성능에 관계 없이 false alarm 발생률이 0에 가깝지 않은 시스템이 꺼질 가능성이 있다(a system which does not have a false alarm rate close to zero is likely to be switched off, not matter its TP performance)는 것을 일관되게 나타내기 때문이다.​Track detection failure(TDF)​GT track은 다음 조건 중 하나라도 만족하는 경우 detect되지 않은 것으로 간주된다(즉, TDF로 분류된다).​조건 1: GT track i는 어떠한 ST track j와도 TRov보다 큰 temporal overlap을 가지지 않는다. Eq.9조건 2: GT tracl i는 ST track j와 temporal overlap은 충분하지만, 어떠한 ST track과도 충분한 spatial overlap을 가지고 있지 않다. Eq.10위의 지표에 대한 유사한 정의가 [10]과 [11]에 제시되었다. [10]에서, spatial overlap은 ST track의 중심이 GT track의 영역 내에 있는지 확인하여 정의한다. 그러나 그러한 정의는 large object의 track을 선호한다. [11]에서, spatial overlap은 a) 중심 사이의 거리, b) GT 영역 내의 ST track 중심 및 c) area ratio overlap의 세 가지 방법으로 정의된다. 우리의 접근법은 후자의 정의와 비슷하다. 단, [11]에서는 프레임 샘플의 관점에서 TP, FP, FN을 추정하는데, 우리는 최종 사용자에게 보다 유용한 정보를 제공하기 위해서는 (temporal overlap의 개념을 사용하여) track의 관점에서 측정해야 한다고 생각한다.​Track Fragmentation(TF)​Fragmentation은 단일 GT track에 대한 ST track의 연속성이 없음을 나타낸다. 그림 4는 TF error의 예를 보여준다. 그림 4. TF = 2(ST track이 두 번 fragmented됨)최적의 조건에서, TF error는 0이어야 하며 이는 tracking 시스템이 GT object에 대해 지속적이고 안정적인 tracking을 생성할 수 있음을 의미한다.​앞에서 언급한 바와 같이, 우리는 GT track과 ST track 사이의 multiple association을 허용하므로, fragmentation은 track correspondence 결과로부터 측정된다.​ID 변경(IDC)우리는 각 STj track의 ID 변경 횟수를 세기 위해 IDCj metric을 도입한다. 이러한 metric은 ID swap metric보다 더 많은 elementary한 정보를 제공한다.​각 프레임 k에 대해 ST track STj의 bounding box Dj,k는 NDj,k개의 GT 영역과 overlap될 수 있으며, 여기서 NDj,k는 다음과 같다. Eq.11우리는 NDj,k = 1인 프레임(즉, track STj가 이 프레임 각각에 대해 하나의 GT Track에만 연결(spatially overlap)됨)만 고려한다. 우리는 STj의 ID 변경을 관련 GT track의 변경 횟수로 추정하기 위해 이 프레임들을 사용한다.​비디오 시퀀스에서 IDC의 총 변경 횟수를 다음과 같이 추정할 수 있다. Eq.12ID 변경 횟수를 세는 절차는 그림 5와 같다. ID 변경 추정의 몇 가지 예는 그림 6에 나와 있다. 그림 5. IDC estimation pseudo-code그림 6. ID 전환과 ID 변경의 예ST track의 latency(LT)​ST track의 latency(time delay)는 obejct가 시스템에 의해 track되기 시작하는 시간과 object의 첫 번째 appearance(그림 7) 사이의 시간 간격이다. 최적의 latency은 0이어야 한다. Latency가 매우 크다는 것은 시스템이 제때 tracking을 trigger할 만큼 sensitive하지 않다는 것을 의미하거나 detection이 tracking을 trigger할 만큼 좋지 않음을 나타낸다. 그림 7. ST track의 Latency 예시Latency는 ST track의 첫 번째 프레임과 GT track의 첫 번째 프레임 간의 프레임 차이로 추정한다. Eq.13Track의 Closeness(CT)​Associated된 GT track과 ST track 쌍의 경우, spatial overlap의 시퀀스(그림 2)는 temporal overlap 동안 Eq.2에 의해 추정한다. Eq.14Eq.14로부터 특정 쌍의 GT와 ST에 대한 평균 closeness를 추정할 수 있다. 하나의 비디오 시퀀스에서 모든 M개의 쌍을 비교하기 위해, 이 비디오의 closeness를 모든 M 쌍의 CT의 weighted average로 정의한다. Eq.15전체 시퀀스에 대한 CT의 weighted 표준 편차는 다음과 같다. Eq.16Track Matching Error(TME)이 측정기준은 ST track의 위치 오차를 측정한다. 그림 8은 한 쌍의 track의 위치를 보여준다. 그림 8. track 쌍의 예TME는 ST track과 GT track 사이의 average distance error다. TME가 작을수록 ST track의 정확도(accuracy)가 향상된다. Eq. 17여기서 Dist()는 GT와 ST의 중심 사이의 거리다.TMED는 distance error의 표준 편차로서 다음과 같이 정의된다. Eq. 18마찬가지로 전체 비디오 시퀀스에 대한 TME(TMEMT)는 각 track 쌍이 overlap된 기간에 대한 weighted average로 정의된다. Eq. 19전체 시퀀스에 대한 TME의 표준 편차는 Eq. 20Track Completeness(TC)이는 ST track이 GT track과 겹치는 시간 범위를 GT track의 총 시간 범위로 나눈 값으로 정의된다. 이 값이 100%이면 fully complete track이다. Eq. 21GT track과 관련된 ST track이 두 개 이상일 경우 각 GT track에 대한 최대 completeness를 선택한다.​또한 비디오 시퀀스의 평균 TC는 다음과 같이 정의한다. Eq. 22여기서 N은 GT track의 수이며 전체 시퀀스에 대한 TC의 표준 편차는 다음과 같다. Eq. 234. Results두 개의 motion tracking 시스템(BARCO의 experimental industrial tracker와 OpenCV1.0 blobtracker[12])을 평가하여 제안된 metrics의 실제 가치를 입증한다. 우리는 조명 변화, 그림자, 눈보라, 빠르게 움직이는 object, FOV의 blurring, 느리게 움직이는 물체, 물체의 거울상, multiple object intersection 등 다양한 challenge를 나타내는 6개의 비디오 시퀀스(그림 9 ~ 그림 14 참조)에 tracker를 실행한다. 모든 비디오에 대한 ground truth은 Viper GT를 사용하여 수동으로 생성되었다 [13]. 그림 9. PETS2001 PetsD1TeC1.avi 시퀀스는 2686 프레임(00:01:29)으로 사람 4명, 사람 2그룹과 차량 3개를 그린다. 이것의 주된 challenge는 multiple object intersection이다.그림 10. i-LIDS SZTRA103b15.mov 시퀀스는 5821 프레임(00:03:52)이며 사람 1명을 묘사한다. 이것의 주요 challenge는 조명의 변화와 빠르게 움직이는 물체다.그림 11. i-LIDS SZTRA104a02.mov 시퀀스는 4299 프레임(00:02:52)으로 사람 한 명을 묘사한다.그림 12. i-LIDS PVTRA301b04.mov 시퀀스는 7309 프레임(00:04:52)으로 사람 12명과 90대의 차량을 묘사하고 있다. 이것의 주요 challenge는 그림자와 시퀀스 초반의 움직이는 물체 그리고 multiple object intersection이다.그림 13. BARCO 060306_04_Parkingstab.avi는 7001 프레임으로 3명의 보행자와 1대의 차량을 묘사하고 있다. 이것의 주요 challenge는 빠른 조명 변화다.그림 14. BARCO 060306_02_Snowdivx.avi는 8001 프레임으로 3명의 보행자를 묘사하고 있다. 이것의 주요 challenge는 눈폭풍, FOV의 blurring, 천천히 움직이는 물체, 그리고 물체의 거울상 등이다.성능 평가 결과는 표 1 ~ 8에 제시되어 있다. 일반적으로 CDT, FAT, TDF와 같은 높은 값의 metrics는 BARCO Tracker가 거의 모든 경우 OpenCV Tracker를 능가한다는 것을 보여준다. 유일한 예외는 i-Lids 시퀀스 PVTRA301b04.mov이다(그림 12, 표 6). 반면에 OpenCV Tracker는 일반적으로 물체의 위치를 추정하는데 더 정확하다(TMEM 낮음).​또한 OpenCV Tracker는 눈보라 scene(그림 14, 표8)에서 물체의 위치와 spatial 및 temporal extent를 더 잘 추정했으며(TMEM 낮음, CTM 및 TCM 높음), 이는 이 장면에 더 나은 object segmentation 모듈을 의미한다. 그러나 BARCO tracker는 metric이 더 좋아서(FAT 낮음, TF 낮음)를 가지고 있어 tracking policy가 더 낫다는 것을 시사한다.​여기에서 사용되는 풍부한 metrics set 없이는 서로 다른 tracker의 성능 저하의 가능한 원인을 식별하기가 매우 어렵다는 점에 유의하라. 표 1. PETS2001 Seq에 대한 평가 결과표 2. Pets2001에 대한 BARCO track association 결과표 3. PETS2001에 대한 OpenCV track association 결과표 4. i-LIDS SZTRA103b15에 대한 tracking PE 결과표 5. i-LIDS SZTRA104a02에 대한 tracking PE 결과표 6. i-LIDS PVTRA301b04에 대한 tracking PE 결과표 7. BARCO Parkingstab에 대한 tracking PE 결과표 8. BARCO Snowdivx에 대한 tracking PE 결과5. Conclusions우리는 motion tracking의 performance에 대한 다른 측면을 평가하기 위한 새로운 metrics set을 제시했다. 물체의 위치, spatial 및 temporal extent 추정의 정확성(accuracy)을 나타내는 TME(Track matching error), CT(Closeness of Tracks), TC(Track Completeness)와 같은 statistical metrics를 제안했으며, 그것들은 tracker의 motion segmentation 모듈과 밀접하게 관련되어 있다.​​CDT(Correct Detection track), FAT(False Alarm Track) 및 TDF(Track Detection Failure)와 같은 metrics은 알고리즘 성능에 대한 general overview를 제공한다. TF(Track Fragmentation)는 track의 시간적 일관성을 보여준다. IDC(ID Change)는 multi-target tracker의 data association 모듈을 테스트하는 데 유용하다.​우리는 조명 변화, 그림자, 눈보라, 빠른 이동 물체, FOV의 blurring, 느린 이동 물체, 물체의 거울상, multiple object intersection 등 다양한 challenge를 제공하는 6개의 비디오 시퀀스를 사용하여 tracker 2개를 테스트했다.​이는 모듈과 challenge의 orthogonality을 가정했을 때, 다양한 metrics와 데이터셋을 통해 우리는 특정 challenge에 대한 tracker의 특정 모듈의 약점을 파악할 수 있다. 이러한 접근방식은 motion tracker의 단점을 이해하는 현실적인 방법이며, 그 단점을 개선하는데 중요하다.​향후 작업에서는 이 프레임워크를 사용하여 더 많은 tracker를 평가할 것이다. 이벤트 detection, action recognition 등 high level task에 대한 평가가 가능하도록 프레임워크도 확대할 것이다. 6. Acknowledgements우리는 벨기에 BARCO View와 EPSRC(Engineering and Physical Science Research Council) RISION 프로젝트에서 보조금 번호 EP/C533410에 따른 재정적 지원을 받았다. "
Azure ,https://blog.naver.com/nicsdiary/223074153678,20230414,"​ 클라우드 서비스란 인터넷을 통해 제공되는 컴퓨터 시스템 및 리소스(하드웨어, 소프트웨어, 데이터 등)를 대여하는 서비스입니다. 이 서비스를 이용하면 고객은 클라우드 제공업체가 보유한 컴퓨터 자원과 서비스를 인터넷을 통해 사용할 수 있습니다. 이를 통해 기업이나 개인 사용자는 자신의 데이터를 저장하고, 애플리케이션을 실행하고, 인터넷 서비스를 제공하는 서버를 구축하거나 운영하는데 드는 물리적인 자원, 비용, 인력 등을 절약할 수 있습니다.​클라우드 서비스는 대부분 인터넷을 통해 제공되는 소프트웨어 서비스, 인프라스트럭처 서비스, 플랫폼 서비스 등으로 분류됩니다. 대표적인 클라우드 서비스로는 아마존 웹 서비스(AWS), 마이크로소프트 애저(Microsoft Azure), 구글 클라우드 플랫폼(Google Cloud Platform) 등이 있습니다.  Cognitive Services Microsoft에서 제공하는 클라우드 기반 인공지능(AI) 플랫폼입니다. 이 서비스는 각종 인공지능 기술과 API(Application Programming Interface)를 제공하여, 개발자들이 손쉽게 인공지능 기능을 구현할 수 있도록 지원합니다.​Cognitive Services는 이미지 분석, 음성 인식, 언어 이해, 자연어 처리(NLP), 지식 추출 등 다양한 인공지능 기능을 제공하며, 이를 활용하면 애플리케이션, 봇, IoT(Internet of Things) 기기 등에서 인간과 자연스럽게 상호작용할 수 있는 기능을 개발할 수 있습니다.​ Cognitive Services - AI 솔루션용 API | Microsoft AzureAzure Cognitive Services는 기계 학습 전문 지식이 필요하지 않은 API를 통해 개발자에게 AI를 제공합니다. 30일 만에 AI 솔루션을 빌드하는 방법을 알아보세요.azure.microsoft.com  컴퓨터 비전은 이미지와 비디오 데이터를 분석하고 이해하는 분야이며, 다음과 같은 3가지 기술을 포함합니다:​객체 감지(Object Detection): 컴퓨터 비전 기술 중 가장 일반적으로 사용되는 기술 중 하나입니다. 객체 감지는 이미지나 비디오에서 특정 객체를 인식하고, 해당 객체가 있는 위치를 식별하는 기술입니다. 객체 감지는 자율 주행 차량, 보안 시스템, 로봇 및 의료 이미징 분야 등 다양한 분야에서 활용됩니다.얼굴 인식(Face Recognition): 얼굴 인식은 인간의 얼굴을 인식하고 분류하는 기술입니다. 얼굴 인식은 보안 시스템, 디지털 마케팅, 인적 자원 관리 등 다양한 분야에서 활용됩니다.이미지 분할(Image Segmentation): 이미지 분할은 이미지를 작은 구성 요소로 나누는 프로세스입니다. 이러한 작은 구성 요소는 개별적으로 처리될 수 있으며, 이미지에서 특정 개체 또는 패턴을 인식하는 데 사용됩니다. 이미지 분할은 의료 이미징 분야에서 종양 검출, 자율 주행 분야에서 도로 인식 등에 활용됩니다.  ​​​endpoint는 API 요청을 보낼 대상 서버의 주소를 나타냅니다. Computer Vision API를 사용하려면, 먼저 API 서버의 주소를 알아야 합니다. 이 주소는 일반적으로 API 제공 업체에서 제공하는 문서에서 확인할 수 있습니다.Computer Vision API는 이미지를 분석하여 이미지에 대한 정보를 추출하는 데 사용됩니다. 이를 위해 두 가지 주요 기능인 ""분석(analyze)""과 ""감지(detect)""가 제공됩니다.​analyze 기능은 이미지에서 객체, 태그, 색상 등의 정보를 추출하여 이미지의 콘텐츠를 자동으로 분석합니다.detect 기능은 이미지에서 얼굴, 물체, 텍스트 등을 감지하고 분석합니다. 이를 통해 얼굴 감지, 이미지에서 특정 물체 감지, 문자 감지 등을 수행할 수 있습니다.  여행사에서 사용자에게 여행지 추천을 위한 API를 사용하는 예시 1.request 모듈 임포트​ import requests 2. API endpoint와 API key를 변수에 저장합니다.​ api_endpoint = ""https://example.com/travel""api_key = ""abcdefghijklmnopqrstuvwxyz"" 3. API를 호출할 때 필요한 header를 정의합니다. 보통 API key를 포함시키기 위해 header에 ""Authorization"" 필드를 사용합니다. headers = {    ""Authorization"": f""Bearer {api_key}"",    ""Content-Type"": ""application/json""} 4. API를 호출할 때 필요한 parameter를 정의합니다. 예를 들어, ""country""라는 파라미터에 ""France""를 넘기면 프랑스의 여행지를 추천받을 수 있습니다. params = {    ""country"": ""France""} 5. API를 호출할 때 전달할 데이터를 정의합니다. 이번 예제에서는 아무 데이터도 전달하지 않습니다. data = None 6. requests 모듈의 get() 함수를 사용해 API를 호출합니다. 이때 endpoint, header, parameter, data를 전달합니다. response = requests.get(api_endpoint, headers=headers, params=params, data=data) 7.API 호출 결과를 분석합니다. 여기서는 JSON 형태로 반환된 결과를 출력해봅시다. result = response.json()print(result) **json()은 requests 모듈에서 제공하는 메소드 중 하나로, HTTP 응답(Response) 객체의 내용을 JSON 형식으로 디코딩하는 메소드입니다. JSON 형식은 데이터를 구조화하여 전달하기 때문에, 서버로부터 전달받은 JSON 데이터를 디코딩하여 원하는 데이터를 추출하고 가공하는 데 사용됩니다. response.json() 메소드를 사용하여 JSON 응답을 파이썬에서 다룰 수 있는 딕셔너리나 리스트와 같은 데이터 타입으로 변환할 수 있습니다.  ""GET""과 ""POST""는 HTTP 프로토콜을 사용하여 인터넷에서 정보를 전송하는 데 사용되는 두 가지 기본적인 메소드입니다.​""GET"" 메소드는 URL에 정보를 포함시켜 서버로 요청을 보내고, 서버는 해당 정보를 응답으로 반환합니다. 일반적으로 브라우저 주소창에 입력된 URL이 GET 요청을 보내는 것입니다. 예를 들어, 검색어를 입력한 후 검색 버튼을 누르면 브라우저는 해당 검색어를 GET 요청으로 서버로 보내고, 검색 결과 페이지를 응답으로 받아 표시합니다.​""POST"" 메소드는 URL에 정보를 노출시키지 않고 서버로 요청을 보내며, 요청하는 데이터는 HTTP 요청의 본문에 포함됩니다. 이 방식은 보안성이 높은 정보를 전송하는 데 주로 사용됩니다. 예를 들어, 로그인 정보를 전송하거나, 글을 작성할 때 작성한 내용을 서버로 전송하는 것 등이 POST 방식으로 이루어집니다.  # 필요한 라이브러리를 import 합니다. import requestsfrom io import BytesIOfrom PIL import Image# 이미지를 가져오는 URL 주소를 변수에 저장합니다.image_url = 'https://menu.mt.co.kr/moneyweek/thumb/2017/05/17/06/2017051709058028746_1.jpg'# 이미지를 가져와 PIL 이미지 객체로 만듭니다.image = Image.open(BytesIO(requests.get(image_url).content))# Azure Computer Vision API를 호출하기 위해 필요한 값들을 변수에 저장합니다.key = '3$$%^'endpoint = 'https://$%^&**(.cognitiveservices.azure.com/' + 'vision/v2.0/'analyze_endpoint = endpoint + 'analyze'detect_endpoint = endpoint + 'detect'# API 호출에 필요한 Header와 Parameter, 그리고 데이터를 변수에 저장합니다.header = {'Ocp-Apim-Subscription-Key': key}params = {'visualFeatures':'Categories,Description,Color'}data = {'url':image_url}# analyze_endpoint를 POST 방식으로 호출하고, Header, Parameter, 데이터를 함께 보냅니다.response = requests.post(analyze_endpoint,               headers=header,              params=params,              json=data)# API의 응답 결과를 JSON 형식으로 파싱합니다.result = response.json() ​ Computer Vision REST API 참조 - Azure Cognitive ServicesAzure Cognitive Services Computer Vision API에 대한 REST 호출을 시작하는 방법을 알아봅니다.learn.microsoft.com  language: 'ko' - 이 값은 해당 텍스트가 한국어임을 나타냅니다.textAngle: 0.0 - 이 값은 감지된 텍스트의 각도를 나타내며, 이 경우에는 0도입니다.orientation: 'Up' - 이 값은 텍스트의 방향을 나타내며, 이 경우에는 수직으로 표시됩니다.regions: [{'boundingBox': '45,125,95,36', 'lines': [{'boundingBox': '45,125,95,17', 'words': [{'boundingBox': '45,125,46,17', 'text': '평화와'}, {'boundingBox': '95,125,45,17', 'text': '번영의'}]}, {'boundingBox': '70,144,46,17', 'words': [{'boundingBox': '70,144,46,17', 'text': '한반도'}]}]}] - 이 값은 이미지에서 감지된 텍스트가 포함된 영역(region)의 목록을 나타냅니다. 이 경우 하나의 영역만 감지되었으며, 해당 영역의 바운딩 박스는 (45,125)에서 시작하여 가로 95픽셀, 세로 36픽셀 크기입니다.​  Custom Vision는 이미지와 간단한 텍스트를 분류하는 데 사용되는 클라우드 기반 서비스입니다. Custom Vision에서 사용할 수 있는 프로젝트 유형은 두 가지가 있습니다.​Classification Projects 이 프로젝트 유형은 이미지를 분류하는 데 사용됩니다. 이미지를 분류하려면 Custom Vision 모델에 이미지와 해당 이미지에 대한 레이블(클래스)을 제공해야 합니다. Custom Vision 모델은 이 정보를 사용하여 이미지 분류 작업을 수행하고 새로운 이미지를 자동으로 분류할 수 있습니다.Object Detection Projects 이 프로젝트 유형은 이미지에서 객체를 탐지하고 객체의 경계 상자(bounding box)를 그리는 데 사용됩니다. 이 유형의 프로젝트를 만들려면 Custom Vision 모델에 이미지와 해당 이미지에서 각 객체의 경계 상자 좌표, 그리고 객체의 클래스(레이블)를 제공해야 합니다. Custom Vision 모델은 이 정보를 사용하여 객체 탐지 작업을 수행하고 새로운 이미지에서 객체를 자동으로 탐지할 수 있습니다.  pip install azure-cognitiveservices-vision-customvision  파이썬 패키지 관리자(pip)를 사용하여 Microsoft Azure Cognitive Services Custom Vision API를 사용하는 데 필요한 파이썬 패키지를 설치하는 명령어입니다.설치되는 패키지는 ""azure-cognitiveservices-vision-customvision""이며, 이 패키지는 Custom Vision 서비스에서 사용할 수 있는 API 기능을 파이썬 코드에서 사용할 수 있게 해줍니다. 이 패키지를 설치하면 Custom Vision API를 사용하여 이미지를 분류하거나 객체를 탐지하는 등의 작업을 파이썬 코드에서 수행할 수 있습니다.즉, 이 명령어를 실행하면 Custom Vision 서비스에서 제공하는 API를 파이썬에서 사용할 수 있는 환경을 구축할 수 있습니다.  ""credential 가지고 인증한다는 것""은 보안을 유지하면서 웹 서비스나 API를 사용할 때 사용자의 신원을 확인하는 과정입니다.웹 서비스나 API를 사용하려면 일반적으로 사용자가 서비스 제공자로부터 발급받은 인증 정보를 제공해야 합니다. 이 인증 정보는 사용자의 ID와 비밀번호, API 키, 인증 토큰 등의 정보를 포함할 수 있습니다. 이러한 인증 정보를 사용하여 서비스 제공자는 사용자의 요청이 유효한지 확인하고, 사용자의 계정과 권한이 적절한지 확인할 수 있습니다.""credential""은 이러한 인증 정보를 의미합니다. 예를 들어, Microsoft Azure에서는 Azure 계정과 관련된 ""credential""로서 Azure Active Directory에서 발급된 인증 토큰, 클라이언트 ID와 클라이언트 시크릿 키, 또는 액세스 키 등을 사용합니다. 이러한 credential 정보를 사용하여 Azure 서비스에 대한 API 요청을 보내고, 사용자의 신원을 확인할 수 있습니다. "
opencv Face detection ,https://blog.naver.com/jinho381/222443901852,20210724,"오늘은 opencv로 FaceDetection을 진행하겠다.​우선 mediapipe라는 라이브러리를 다운로드 한다. pip install mediapipe mediapipe는 여러모로 쓸모가 있음.  나중에 포스팅할 예정​나는 영상이 좋음. import cv2 import mediapipe as mpface_detection_object=mp.solutions.face_detectiondrawing=mp.solutions.drawing_utilscap = cv2.VideoCapture(0) with face_detection_object.FaceDetection(    model_selection=2, min_detection_confidence=0.5) as face_detection:    while True:        ret,img=cap.read()        if not ret:            break                img=cv2.cvtColor(img,cv2.COLOR_BGR2RGB)        results=face_detection.process(img)        if results.detections:            for catch in results.detections:                drawing.draw_detection(img,catch)        cv2.imshow('face',img)        cv2.waitKey(3) 이거 돌리면 깜짝 놀람.​ 내 얼굴 보고 깜짝 놀랐음.opencv는 BGR순서로 처리한다는 것을 잊지말자​ 잘 동작함 ㅎㅎ "
"ESP32-Cam과 OpenCV, Yolo를 이용한 객체검출 ",https://blog.naver.com/jcosmoss/222976730180,20230106,"Yolo를 이용한 객체 인식 예제를 만들어 보기로 하자. ​준비물ESP32-Cam (그냥 웹캠의 영상을 이용해도 된다.)윈도우 PC에 OpenCV 설치(라즈베리파이3에 할려고 했으니 설치가 드럽게 느려서 포기)​ESP32-Cam 카메라 웹서버 설치설치 방법은 많이 다루었다. IDE에 보드관리자와 라이브러리를 설치 하면 예제를 사용할 수 있다. camerawebserver 예제 코드를 열어서 몇가지 수정(카메라 모델과 공유기 정보)하고 업로드 해주면 된다. 다른 개발자의 웹서버 코드를 업로드 해도 된다. 어떻게든 카메라 스트리밍이 되도록 해주면 된다.  camera webserver의 IP로 접속해서 위와 같이 영상이 나오면 성공이다. 얼굴인식기능은 이 자체에서 제공을 하고 있다. ​뒤에 OpenCV에서는 이 웹페이지 주소를 사용하는 것이 아니다.http://esp32-cam_ip:81/stream를 사용하면 스트리밍 영상만 사용이 가능하다.​2. OpenCV 설치파이썬이 설치된 윈도우에서 python-opencv를 설치 한다.​pip install opencv-python설치 완료 한다. main 모듈과 extra 모듈이 포함된 풀버전의 차이가 있는데 그 차이는 아래에서 참고 하자. OpenCV: OpenCV modules​3. Yolo 필수 파일 다운로드이건 따로 설치하는 과정이 필요 없는 것 같다. 객체 인식을 위해서는 3가지 파일이 필요하다. ​cgf & weights YOLO: Real-Time Object Detection (pjreddie.com) <자신의 시스템에 맞는 사이즈로 받으면 된다>coco.namesdarknet/coco.names at master · pjreddie/darknet · GitHub위 세가지 파일을 다운롣 받아서 작업 중인 파이썬 프로젝트 폴더에 같이 복사해 넣어 둔다.이제 코드를 작성하자.​4. 코드 작성 구글링 해보면 다양한 코드들이 있다. 그중에 하나 실행 시켜 보기로 하자. # -*- coding: utf-8 -*-""""""Objects Detection in Real Time with YOLO v3 and OpenCVFile: yolov3-camera-realtime-detection.py""""""import numpy as npimport cv2import timeprint (cv2.__version__)camera = cv2.VideoCapture('http://esp32-cam-ip:81/stream')h, w = None, Nonewith open('coco.names') as f:    labels = [line.strip() for line in f]network = cv2.dnn.readNetFromDarknet('yolov3-tiny.cfg',                                     'yolov3-tiny.weights')layers_names_all = network.getLayerNames()layers_names_output = \    [layers_names_all[i - 1] for i in network.getUnconnectedOutLayers()]probability_minimum = 0.5threshold = 0.3colours = np.random.randint(0, 255, size=(len(labels), 3), dtype='uint8')while True:    _, frame = camera.read()    if w is None or h is None:        h, w = frame.shape[:2]    blob = cv2.dnn.blobFromImage(frame, 1 / 255.0, (416, 416),                                 swapRB=True, crop=True)    network.setInput(blob)  # setting blob as input to the network    start = time.time()    output_from_network = network.forward(layers_names_output)    end = time.time()    print('Current frame took {:.5f} seconds'.format(end - start))    bounding_boxes = []    confidences = []    classIDs = []    for result in output_from_network:        for detected_objects in result:            scores = detected_objects[5:]            class_current = np.argmax(scores)            confidence_current = scores[class_current]            if confidence_current > probability_minimum:                box_current = detected_objects[0:4] * np.array([w, h, w, h])                x_center, y_center, box_width, box_height = box_current                x_min = int(x_center - (box_width / 2))                y_min = int(y_center - (box_height / 2))                bounding_boxes.append([x_min, y_min, int(box_width), int(box_height)])                confidences.append(float(confidence_current))                classIDs.append(class_current)    results = cv2.dnn.NMSBoxes(bounding_boxes, confidences,                               probability_minimum, threshold)    if len(results) > 0:        for i in results.flatten():            x_min, y_min = bounding_boxes[i][0], bounding_boxes[i][1]            box_width, box_height = bounding_boxes[i][2], bounding_boxes[i][3]            colour_box_current = colours[classIDs[i]].tolist()                        cv2.rectangle(frame, (x_min, y_min),                          (x_min + box_width, y_min + box_height),                          colour_box_current, 2)            text_box_current = '{}: {:.4f}'.format(labels[int(classIDs[i])],                                                   confidences[i])            cv2.putText(frame, text_box_current, (x_min, y_min - 5),                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, colour_box_current, 2)    cv2.namedWindow('YOLO v3 Real Time Detections', cv2.WINDOW_NORMAL)    cv2.imshow('YOLO v3 Real Time Detections', frame)    if cv2.waitKey(1) & 0xFF == ord('q'):        breakcamera.release()cv2.destroyAllWindows() 파일경로를 제대로 수정하고 비디오 소스를 넣어 주면 된다.​5. 실행 정상적으로 인식하는 것을 확인 할 수 있다.  ​ "
"RCNN모델, SSD모델, YOLO모델 개념 ",https://blog.naver.com/gayeon6423/223107762347,20230521,"1. RCNN(Regions with CNN) object Detection모델 RCNN은 Regions proposal을 CNN모델에 결합하여 Object Detection을 수행하는 모델입니다.하나의 Region Proposal마다 Feture Extractor이 각각 들어가게 되므로 오랜 학습시간이 걸리게 됩니다.​​- One-Stage Detector : Object Detection(분류)와 Region Proposal(지역화)를 동시에 수행- Two-Stage Detector : Object Detection(분류)와 Region Proposal(지역화)를 순차적으로 수행 ​1. Stage1 : Selective Searach로 Region Proposal Selective Search을 통해 2000개의 객체가 있을만한 영역을 추천해주는 Region Prosal을 진행합니다. 이 2000개의 Region Propasal마다 하나씩 Stage2의 Feature Extractor로 입력하게 됩니다.이때 Region Proposal들의 사이즈를 동일하게 맞추기 위해서 Crop, Wrap등을 적용해줍니다.​2. Stage2-1 : Softmax로 학습하고 SVM으로 최종클래스 분류 Region Proposal을 기반으로 입력된 이미지에서 객체가 어떤 물체를 나타내는지 클래스를 분류합니다.먼저 Softmax layer로 객체가 어떤 클래스로 분류되는지에 대한 확률 Score을 얻습니다. 이 확률 Score을 얻으면서 파라미터가 학습됩니다. 이러한 Softmax layer로 파리미터를 학습한 후 SVM Classifier로 탐지된 객체의 클래스가 무엇인지 최종 분류하게 됩니다.​3. Stage2-2 : Bounding Box 좌표 찾기 객체가 어디에 있는지 Detection하기 위해 객체 주변 사각형 Bounding Box의 좌표를 찾아야 합니다. 최적의 좌표를 찾는 과정은 Loss함수를 이용해 역전파를 사용합니다. 예측된 파란색 박스의 좌표를 실제 좌표값인 gx,gy로 얼마나 이동시키는지 구하게 됩니다. 정규화를 적용하여 Loss값을 최소화하는 값을 구합니다.​2. SSD(Single Shot Detector) Object Detection 모델SSD는 One-Stage모델로 Region Proposal과 Object Detection을 한번에 수행하는 모델입니다.SSD모델이 나오기 전 One-Stage모델로 YOLO version1이 개발되었지만 mAP값이 현저히 낮은 문제가 있었습니다. 이를 개선하고자 나온 모델이 SSD모델입니다.​1. Multi-Scale Feature Layer멀티 스케일 Feature Layer은 여러 번의 컨볼루션을 적용해 나온 여러 개의 Feature Map을 Object Detection을 수행시키고 그 결과를 통합해 최종 Detection하는 과정입니다.사이즈가 32,16,8,4일 때 각각에 대해 Object Detection을 수행합니다.​ 2. Default Box(Anchor Box)를 통한 Object Detection컴볼루션을 통해 나온 각각의 Feature Map의 포인트 마다 여러개의 Anchor Box를 씌우고 Ground Turth와 비교를 하면서 학습을 진행합니다.서로 크기가 다른 Feature Map이 입력 이미지 내의 크기가 서로 다른 객체들을 각각 마크하여 탐지를 합니다. Feature Map1번에서 4번으로 갈수록 입력 이미지를 더 상징적으로 잘 나타내는 Feature Map이게 됩니다. 중심 Object는 강아지들이므로 1번일 때보다 2번~4번에서 강아지들을 상징하는 Feature Map이 존재하게 됩니다.32X32 Feature Map에서는 강아지들보다 작은 '사람의 손'을 탐지하고 있습니다.16X16 Feature Map에서 좀 더 큰 object인 강아지를 탐지하고 있습니다.즉, 컨볼루션 과정을 거친 Feature Map일 수록 이미지의 중심이되는 Object를 잘 탐지하게 됩니다. ​=> 즉, SSD는 입력 이미지 내에 여러개의 객체들이 있을 때, 서로 다른 여러개의 Feature Map이 입력 이미지 내의 크기가 다른 객체들을 전담해서 탐지하도록 만듭니다.​3. NMS(Bon-Max Suppression)Default Box로 Object Detection을 끝내게 되면 많은 Bounding Box들이 생겨납니다. Ground Truth와 가장 IoU가 높은 Bounding Box만 남기기 위해서 NMS기법을 사용하게 됩니다.​4. SSD의 Cost Function ​3. YOLO(You Only Look) Object Detection모델YOLO모델의 공통점은 입력이미지 또는 Feature Map을 특정 그리드로 나누고 각 Cell마다 Object Detection을 수행하는 것입니다. SSD모델에서 Feature Map의 각 포인트마다 Object Detection을 수행하는 원리와 동일합니다. ​1. YOLO - Version1컨볼루션을 거친 Feature Map이 아닌 입력 이미지 자체를 특정 그리드 SxS로 나눕니다. 나눈 그리드의 각 Cell마다 Anchor Box를 2개씩 씌우고 이를 Ground Truth와 비교하면서 Object Detection을 수행합니다.  입력 이미지를 xXx그리드로 나누고 나눈 각 cell당 하나의 Objet Detection을 수행하고 그 결과를 통합해서 최종적인 Object Detection을 수행합니다.Anchor Box 2개 + Pascal VOC Dataset으로 구성된 벡터가 됩니다. Cell마다 Object Detection을 수행해주고 나면 수많은 Bounding Box들이 도출됩니다. 이 때 NMS과정을 통해서 최적의 Bounding Box들만 남깁니다.하지만 그리드를 나눈 각 Cell마다 Anchor BOX가 2개밖에 없습니다. 그만큼 ROI(객체 후보)가 적게 되므로 Object를 잘 탐지하지 못하는 문제가 발생할 수도 있습니다. 즉, 하나의 그리드 Cell에 여러개 Object가 겹쳐있으면 단순히 하나의 Object로만 탐지하고 넘어가게 될 수도 있습니다. Anchor Box에 대한 벡터5개, 이 Box가 2개있으므로 10개, 클래스20개 벡터=>총30개의 벡터2. YOLO-Version2다음은 YOLO-version1에 추가된 특징입니다.입력 이미지가 아닌 Feature Map에서 13X13그리드로 나누고 각 Cell마다 Object Detection을 수행합니다.각 Cell당 씌우는 Anchor Box개수를 5개로 늘렸습니다.동이한 이미지이더라도 크기만 다르게 해서 모델을 학습했습니다.(Multi Scaling)Batch Normaliztion을 적용했습니다.분류 모델을 Fine Tuning을 적용했습니다.Darknet19라는 개별 Feature Extractor을 사용했습니다. Version1과 비교해보면 FC Layer가 사라졌고 Feature Map에서 13X13그리드로 나누고 각 cell마다 object detection을 수행했다는 것을 알 수 있습니다.5개의 서로 다른 Anchor Box의 사이즈 기준은 입력되는 이미지 데이터의 Ground Truth의 Bouding Box를 분석해 비슷한 부분끼리 그룹핑되도록 K-means Clustering을 사용하게 됩니다.​3. YOLO-Version3YOLO-version3의 특징은 다음과 같습니다.SSD의 Multi-Scale Feature Layer와 유사한 기법을 적용했습니다.Multi-label Classification을 해결하기 위해 클래스 분류 시 Softmax가 아닌 독립적인 여러개의 Sigmoid Layer을 사용했습니다.하나의 그리드 Cell당 3개의 Anchor Box를 씌웠습니다.클래스 종류가 80개인 COCO Dataset을 사용했습니다.Darknet-53이라는 개별 Feature Extractor을 사용했습니다.동일한 사이즈의 이미지들에 다른 학습을 적용한는 Multi-Scaling을 진행했습니다. Multi Scale Feature Layer을 그리드에 적용해주는 셈입니다.Layer중간에 Feature Map사이즈 축소를 막기 위한 Up Sampling을 사용했습니다.Gradient Vanishing을 방지하기 위해 Skip Connection을 사용했습니다. 독립적인 여러개의 Sigmoid로 Multi-label Classification을 해결하였습니다.Multi label Classification은 동시에 여러개의 레이블을 가질 수 있는 것이고 Multi class Classification은 무조건 하나의 레이블만 가질 수 있는 것입니다.YOLO v3전에는 softmax Layer를 사용해서 Multi class classification문제만을 해결하였습니다.YOLO v3에서는 Sigmoid(Logistic함수) Layer을 사용해 Multi-label Classification 문제도 함께 해결하였습니다.​예를들어 '남자'가라는 객체가 들어있는 이미지를 입력하게 된다고 가정해봅니다.클래스의 종류가 [사람,남자,강아지]일 때 Softmax Layer을 사용하게 되면 각 클래스에 대한 Score이 [0.3,0.6,0.1]로 세 값의 총합은 1이 됩니다. 그러므로 예측 모델은 Score이 가장 높은 남자로만 예측합니다.여기서 독립적인 Sigmoid Layer을 사용하게 되면 Score은 [0.8,0.8,0.2]가 됩니다. 이때 Score의 임곗값을 0.5로 하고 이보다 크면 1, 이보다 작으면 0으로 분류해서 Multi label로 예측하게 됩니다. 즉, 해당 이미지를 사람이면서 남자인 2개의 레이블으르 가질 수 있게 됩니다.이렇게 더욱 정확한 확률로 예측을 할 수 있게 됩니다.​4. YOLO-version4YOLOv4는 기존 YOLO시리즈의 작은 객체 검출 문제를 해결하고자 한 것이 특징입니다.다양한 작은 object들을 잘 검출하기 위해 input 해상도를 크게 사용하였습니다. 기존에는 224, 256 등의 해상도을 이용하여 학습을 시켰다면, YOLOv4에서는 512을 사용하였습니다.receptive field를 물리적으로 키워 주기 위해 layer 수를 늘렸으며, 하나의 image에서 다양한 종류, 다양한 크기의 object들을 동시에 검출하려면 높은 표현력이 필요하기 때문에 parameter 수를 키워주었습니다.Architecture : 정확도를 높이는 반면에 있어서의 속도문제는 아키텍처의 변경을 통해 해결하였습니다.FPN(Feature Pyramid Network)과 PAN(Path Aggregation Network) 기술을 도입하였고 CSP (Cross Stage Partial connections) 기반의 backbone 연결과 SPP (Spatial Pyramid Pooling) 등의 새로운 네트워크 구조를 도입하여 성능을 향상하였습니다. YOLOv4 = YOLOv3 + CSPDDarknet53 + SPP + PAN + BoF + BoS​YOLO-version5BottleneckCSP를 사용하여 각 계층의 연산량을 균등하게 분배해서 연산 bottleneck을 없애고 CNN layer의 연산 활용을 업그레이드 시켰습니다.다른 Yolo 모델들과 다른 점은 backbone을 depth multiple과 width multiple를 기준으로 하여 크기별로 yolo v5 s, yolo v5 m yolo v5 l, yolo v5 x로 나눈다는 것입니다. 이는 small, medium,large, xlarge라고 생각하면 구분하기 쉽습니다.따라서 아래 그림과 같이 yolo v5 s가 가장 빠르지만 정확도가 비교적 떨어지며 yolo v5 x는 가장 느리지만 정확도는 향상됩니다. 정확도와 속도는 상충관계이기 때문에 모두 잡을 순 없어 s 가 가장 빠른 대신 정확도는 떨어지고 x가 가장 느린 대신 정확도는 향상됩니다.​ "
Fair MOT: A Simple Baseline for Multi-Object Tracking  ,https://blog.naver.com/yeajin522/222279454274,20210318,"arxiv.org/abs/2004.01888​​Multiple Object Tracking 에서 detection 과 re-id 부분을 균형있게 학습한 논문 , idea 는 Anchor-free 방식을 사용했다.    Abstract 기존 방법은 inference speed 를 올리기 위해 one-shot network 에서 object detection 과 re-identification 두가지의 성능을 모두 올리는 부분은 취약했다.특히, re-identification 에서 학습이 제대로 되지 않는다는 것이 문제이다.이 논문에서는 잘 되지 않는 이유에 대한 연구와 해결 방법을 제시했다. 특히 30fps를 유지하며 SOTA 성능을 냈다.  Introduction 기존 SOTA 방법들 중에는 두 가지 별개의 모델로 나뉜다 (two-step 방법)1. detction model: bounding box를 잡고 ,2. Re-identification model: 각 bounding box 에 대해 Re-Identification feature 를 추출하여 특정 track 중 하나로 연결한다.그러나 detection model 과 Re-ID 부분이 서로 연동되지 않기 때문에 inference time이 실시간으로 잘 나오지 않는다.   1) Unfairness Caused by Anchors   기존 One-shot tracker 들과 비교  Red: Positive anchorsGreen: target objects (GT)(I)모든 positive anchor 로 부터 re-ID Features 를 다 뽑는다 (ROI Align)(II)모든 positive anchor 의 center 로 부터 re-ID features 를 뽑는다(III)Object 의 center 로부터 re-ID Features 를 뽑는다.     -> One anchor corresponds to multiple identities(b) 와 같이 하나의 RoI에 다른 instance 나 background 가 들어있다.   -> Multiple anchors correspond to one identity•하나의 object 에 여러 anchor 가 쳐져 있음•같은 instance 의 image patches 가 너무 다름 -> training 시 ambiguity 발생•기존처럼 8배씩 down-sampled 한 후의 ROI 는 re-id 에 적용하기에 너무 coarse 한 feature 를 사용하게 됨 (feature 의 ROI align 이 object center 와 맞지 않음)•위와 같이 instance 가 겹칠 경우, augmentation 등 전처리를 했을 시 다른 instance 로 바뀌는 등 많은 영향을 받게 됨    2) Unfairness Caused by Features•기존의 One-shot tracker 의 경우, 대부분의 feature 들이 object detection 과 re-ID task 에서 공유됨•Object detection 에서는 deep 한 feature 를 추출하여 class 와 position 을 잡을 수 있지만, 이 뒤에 붙여지는 Re-ID 는 low-level appearance 에만 집중하게 됨•Detection 에 bias 가 생기며, low-quality 의 re-ID Features 가 생김•Small, large object 모두 잡기 위해서 다양한 resolution의 피쳐 필요•따라서 multi-layer feature aggregation 을 사용하여 해결하고자 하였음    3) Unfairness Caused by Feature Dimension•기존에는 high-dimension을 사용하여 re-ID 부분 학습•본 논문에서는 lower-dimensional 하였음•High-dimensional 은 object 를 구별하는 능력은 향상될 수 있지만, detection 과 re-ID 부분이 서로 경쟁하며 최종 tracking accuracy 가 낮아지는데 영향을 미침•Training data 가 적을 때는, low-dimensional 을 학습하면 over-fitting 을 막아줌•Inference speed 가 향상됨     Fig.2.: overview of one-shot MOT tracker. 먼저 input image 가 encoder-decoder 에 주어진다. high resolution feature maps 를 추출한다. (stride = 4). 그리고 두 개의 parallel heads 로 bounding boxes 와 Re-ID features 를 각각 예측한다. features 는 object 의 center 를 예측하고, temporal bounding box 와 linking 한다.  •One-shot network: detection + Re-ID•Anchor-free ( object centers and sizes) + Re-ID(high-quality feature)•Encoder-decoder 로 high-resolution feature 를 추출, (stride:4) 기존엔 stride:32 였음•두 개의 branch 로 갈라짐  MOT challenge benchmark 로 평가하였으며 2DMOT15 에서 1위를 하였다. 그럼에도 불구하고 30fps 로 사용 가능하다.2021 기준 MOT 1위   Related Work MOT 를 two-steps와 one-shot methods 로 나누어 장단점을 비교하도록 한다. 2.1 Two-Step MOT Methods[38,41,23,47,11] Two-step Methods 에서는 detection 과 Re-ID 를 두개의 분리된 task로 구분한다.먼저, CNN detector 를 사용하여 object 가 있을만한 곳을 localize 하고 box를 추출한다. 그 다음, 별개의 단계로 image 를 crop 하여 identity 를 embedding 하고 Re-ID features 를 추출한다. 그리고 box 정보와 link 하여 tracking 하는 형태이다.link 방법은 먼저 Re-ID feature 와 bounding box 의 Intersection over Unions(IoU) cost matrix 를 계산해준다.그 다음 Kalman filter 와 Hungarian Algorithm 을 사용하여 linking 을 마무리 해준다.[23,47,11] 처럼 RNN을 쓰는 조금 더 복잡한 방법들도 있다.Two-step methods 의 장점은 분리된 각 task 가 가장 적합한 모델로 각각 사용할 수 있다는 것이다.게다가, detection 된 bbox 에 따라 image patches 를 crop 하고 Re-ID를 하는 동일한 크기로 조정할 수 있다. (??)이런 방법을 사용하면 object 의 variation 을 커버할 수 있게 된다. 따라서 이러한 방법은 성능이 높다.하지만 Re-ID 와 embedding 을 서로 공유하지 않고, computation 이 많기 때문에 실시간 성능을 보장하기 어렵다. 2.2 One-shot MOT MethodsOne-shot Methods 핵심 아이디어는 object detection 과 Re-ID 부분을 single network 로 만들어 computation 공유를 통해 inference time 을 줄이는 것이다.Track-RCNN[33]은 Mask-RCNN[12]에 Re-ID 부분만을 추가하여 bounding box 와 Re-ID 를 regression 시킨다.JDE[36] 의 경우도 YOLOv3 에 identity 부분을 추가하여 inference 속도를 높였다.그러나, One-shot Methods 의 tracking accuracy 는 대부분 two-step methods 보다 낮은데, 학습 된 Re-ID features 가 최적화 되지 않아 ID switches 가 많이 일어난다.그 이유는 anchor 에서 추출 된 identity embedding features 와 object 의 center 와 align 되지 않아 ambiguities 를 가져오기 때문이다.이를 해결하기 위해, anchor-free 방식을 제안했음  FairMOT     3.1 Backbone NetworkResNet-34[13] 사용 (accuracy 와 speed balance 가 좋다) + DLA(Deep Layer Aggregation)[45]원래의 DLA 와 달리 FPN 과 유사하게 low-level 과 high-level 사이의 skip connections 이 더 많다.또한, up-sampling module 의 모든 convolution layer는 object scale 과 pose 에 따라 receptive field 를 dynamically 하게 조절할 수 있도록 deformable convolution 을 사용한다. 이는 alignment issue 에도 좋다.모델 이름은 DLA-34로, input image 는 H x W 이며, output feature map shape 은 H/4 x W/4 x C 이다.      3.2. Object Detection Branchobject detection 은 high-resolution feature map 위주로 처리된다.세 개의 parallel regression heads는 각각 heatmaps, object center offsets, bounding box size 를 추정한다.각 head 는 backbone network 의 output feature map 에 3x3 convolution 을 적용한 후 (256개 채널 포함) 최종에는 1x1 convolution 레이어를 구현하여 적용된다.  1) Heatmap HeadHeatmap Head 는 object 의 center 위치를 추정하는 역할이다.Dimension: 1 x H x W 아래 식을 한 줄씩 설명하면, (1) GT box 의 x,y, 좌표(2) image에서 object center(3) feature map 의 object center. stride 가4니까 4로 나눠준다(4) Mxy는 (x,y) 위치에 대한 (feature map) Heatmap response로, Gaussian distribution 식이다.GT와 거리가 멀어질 수록 Exponetially 하게 decay 된다.(5) Focal loss 를 적용하여 덜 학습이 된 부분에 집중하여 학습해준다. Mxy=1 일 경우 아래 식이 -무한대로 발산하므로 1일 경우 위 식으로 대체해준다.    Heatmap head 에서는 focal loss 를 이용해 loss 를 계산해주고, heat map을 학습시켜준다.      2) Center Offset HeadCenter Offset Head 는 object 의 위치를 더 정확하게 추정한다. Stride 는 Quantization error 를 일으킨다.Re-ID feature는 정확한 object center에 따라 수행되어야 한다. ReID features 와 object center를 alignment 하는것이 매우 중요하다.s로 height 와 width 를 예측해준다.       3) Box Size, center offset Head•offset head: object 의 위치를 정확하게 예측하기 위함 •The box size head: target 의 height 와 width 예측하기 위함•Feature map 에서 quantization error 가 생길 수 있음•Down-sampling 하며 생긴 object center 의 픽셀 단위 offset을 추정한다. •GT 의 box 값이 bi •Si, Oi 는 GT 값 , Si^, Oi^ 는 예측 값•L1 loss 로 정의된다.       3.3 Identity Embedding BranchIdentity Embedding branch 는 서로 다른 object 가 구별 가능하도록 feature map 을 생성하는 것이다.이상적으로는 서로 다른 object 간의 distance 가 서로 같은 object 의 distance 보다 커야한다.identity embedding features 의 각 위치를 위해 backbone feature 의 top 으로부터 128 kernels 의 convolution layer를 적용했다.결과 feature map 은 E 이며 , (x,y)에 있는 객체의 Re-ID Feature 는 E(x,y) 이다.    •   E(cxi~, cyi~)  를 추출하여 fully connected 를 통하여 P로 추출 한 후 L과 cross entropy loss 로 정의된다.     Fair MOT 의 Loss 는 다음과 같이 정의된다.  •Ltotal 에서 detection 과 re-ID task 의 balance 를 학습하게 된다.•W1과 w2 는 learnable parameters이다.    FairMOT TrainingFairMOT network•Input: 1088x608•Heatmap 에서, NMS 를 통해 heatmap score 가 threshold 보다 peak keypoints 를 걸러냄•Box size 와 center offset 부분에서, Bounding box를 계산 함•그리고, 예측한 object center로 identity embeddings 를 추출함 -> tracker 와 연결기존 tracking 들과 마찬가지로•첫번째 프레임에서 예측된 box 를 기반으로 tracklets 초기화 •다음 프레임에서 Re-ID feature 에서 IoU로 측정한 거리에 따라 box 를 기존 tracklets 과 연결한다. Kalman filter 를 이용하여 현재 프레임에서 tracklet 위치를 예측한다. 이부분은 SORT 논문을 참고   Experiments    •Re-ID feature 추출 방식에 따른 성능 비교•Center BI는 Center Bilinear interpolation•One-shot 방법임에도 불구하고 Two-stage 보다 성능이 더 높음        •Backbone Network 에 따른 성능 비교•Deep Layer Aggregation 을 사용했을 때 MOTA 성능이 가장 높고 ID switches 가 가장 적음   Green: same id targetRed: Query Image (테스트 이미지)Re-id feature 추출 방법에 따른 비교   핵심은, Anchor-free 방식으로 center 위주로 학습했다는 것, center heat map을 사용했다는 것이다.  FairMOT: On the Fairness of Detection and Re-Identification in Multiple Object TrackingThere has been remarkable progress on object detection and re-identification (re-ID) in recent years which are the key components of multi-object tracking. However, little attention has been focused on jointly accomplishing the two tasks in a single network. Our study shows that the previous attempt...arxiv.org ​ "
인공지능 분야의 다양한 분야에서 사용되는 오픈소스 라이브러리 개요 ,https://blog.naver.com/dreamxpeed/223041245028,20230311,"인공지능 관련 오픈소스 라이브러리는 크게 머신러닝, 딥러닝, 자연어 처리, 강화학습, 컴퓨터 비전 등 다양한 분야로 나뉘어집니다. 이러한 라이브러리들은 각각의 분야에서 필요한 기능들을 제공하며, 누구나 자유롭게 사용하고 공유할 수 있습니다.​머신러닝 라이브러리머신러닝은 데이터를 기반으로 패턴을 학습하는 기술입니다. 이를 위해 데이터 전처리, 모델 선택, 학습 및 평가 등의 과정이 필요합니다. 머신러닝 라이브러리는 이러한 과정을 보다 쉽게 수행할 수 있도록 도와줍니다. 예를 들어, Scikit-learn은 데이터 전처리, 분류, 회귀, 군집화 등 다양한 머신러닝 모델을 제공합니다.​딥러닝 라이브러리딥러닝은 인공신경망을 사용하여 데이터를 학습하는 기술입니다. 딥러닝 라이브러리는 이러한 인공신경망을 쉽게 구성하고 학습할 수 있는 도구들을 제공합니다. 예를 들어, TensorFlow, PyTorch 등은 딥러닝 모델을 구성하고 학습할 수 있는 다양한 기능들을 제공합니다. 자연어 처리 라이브러리자연어 처리는 인간의 언어를 기계가 이해하고 처리할 수 있도록 하는 기술입니다. 이를 위해 텍스트 전처리, 토큰화, 형태소 분석, 품사 태깅, 개체명 인식, 문장 분류 등의 과정이 필요합니다. 자연어 처리 라이브러리는 이러한 과정을 보다 쉽게 수행할 수 있도록 도와줍니다. 예를 들어, NLTK, SpaCy, KoNLPy 등은 자연어 처리에 필요한 다양한 기능들을 제공합니다.​강화학습 라이브러리강화학습은 에이전트가 환경과 상호작용하며 보상을 최대화하는 방향으로 학습하는 기술입니다. 강화학습 라이브러리는 이러한 환경과 상호작용하는 에이전트를 구현하고 학습할 수 있는 도구들을 제공합니다. 예를 들어, OpenAI Gym, PySC2 등은 강화학습에 필요한 다양한 환경과 보상을 제공하며, 이를 바탕으로 강화학습 알고리즘을 구현할 수 있도록 도와줍니다.​컴퓨터 비전 라이브러리컴퓨터 비전은 컴퓨터가 이미지나 비디오 데이터를 분석하고 이해하는 기술입니다. 이를 위해 이미지 전처리, 객체 검출, 분할, 인식 등의 과정이 필요합니다. 컴퓨터 비전 라이브러리는 이러한 과정을 수행할 수 있는 다양한 알고리즘과 도구들을 제공합니다. 예를 들어, OpenCV, DLIB, TensorFlow Object Detection API 등은 컴퓨터 비전 분야에서 필요한 다양한 기능들을 제공합니다. 일반적인 인공지능 라이브러리인공지능 분야에는 머신러닝, 딥러닝, 자연어 처리, 강화학습, 컴퓨터 비전 외에도 다양한 분야가 있습니다. 이를 위해 일반적인 인공지능 라이브러리는 이러한 분야에서 필요한 다양한 기능들을 제공합니다. 예를 들어, Hugging Face Transformers는 자연어 처리 분야에서 최신 딥러닝 모델들을 제공하며, GPT-3와 같은 대규모 언어모델을 사용할 수 있도록 도와줍니다.​이러한 오픈소스 라이브러리들은 누구나 자유롭게 사용하고, 수정하여 개선할 수 있습니다. 이를 통해 많은 개발자들이 함께 협력하여 인공지능 분야의 기술 발전을 이끌어내고 있습니다.​이외에도 많은 오픈소스 라이브러리들이 존재합니다. 예를 들어, Keras는 TensorFlow와 같은 딥러닝 프레임워크 위에서 동작하는 높은 수준의 인터페이스를 제공하여 딥러닝 모델을 더 쉽게 구성하고 학습할 수 있도록 도와줍니다. 그리고 PyTorch Geometric은 그래프 머신러닝을 위한 도구들을 제공하여 그래프 데이터를 분석하고 모델링할 수 있도록 도와줍니다. 이러한 라이브러리들은 인공지능 분야에서 더욱 다양한 연구와 개발이 가능하도록 해주고 있습니다. 또한, 인공지능 기술은 다양한 분야에서 적용되기 때문에, 이를 위한 다양한 라이브러리들도 존재합니다. 예를 들어, Dask는 대용량 데이터를 분산하여 처리하는 분산 컴퓨팅 라이브러리입니다. 이를 통해 머신러닝 모델 학습과 같은 대규모 연산 작업을 빠르게 처리할 수 있습니다. 그리고 PySpark는 Apache Spark 위에서 동작하는 파이썬 API로, 대용량 데이터 처리와 머신러닝 분석을 위한 다양한 기능들을 제공합니다.​인공지능 분야의 기술 발전은 계속되고 있으며, 이에 따라 새로운 오픈소스 라이브러리들도 지속적으로 등장하고 있습니다. 이러한 라이브러리들은 인공지능 분야에서 적극적으로 사용되고 있으며, 더욱 발전된 기술과 알고리즘을 위한 도구들이 필요할 때마다 자유롭게 공유되고 개선될 것입니다.​​ "
자율주행차 강의 ,https://blog.naver.com/asdf200162/223085493978,20230426,​​초등학교에서자율주행자동차 특별강의 있는 날​요즘 학교에서다양한 체험들을 아이들에게 제공하고 있어서좋다는 생각이 듭니다.​4차 산업혁명 시대​미래를 대비해야겠죠.​​ ​날이 맑고 좋습니다​​​  ​교내에 전시된각종 귀여운 작품들도구경하고요.​저도 학교 수업을​처음 했을땐긴장이 되어서이런것도 안보이고오로지 내가 할것만바라보던 때가 있었는데 ㅎㅎㅎ이젠 여유좀 보세요이런 구경도 하고있고 ㅎㅎㅎ  ​자 이제 시작해볼까요-!​​​  자율주행자동차의단계와 원리핵심 기술들최신 발전 동향​설명하면서​제가 작업했었던인공지능 라이다 데이터도소개했어요​​ 라이다 기반 3D Object Detection(그림 출처 : Innoviz에서 그림 발췌)​​그리고또 한가지!​마침 이번에 뽑은저희집 자동차가​반자율주행 기능이 있어요~​여기 올때에도자율주행차 타고 왔다며​운전대에서 손 놓은실제 영상을 보여주니까^^​관심 폭발~~  ​​​ ​이제 이쯤이면마무리 하고드디어 직접 해보는데요!​​​ Previous imageNext image ​2명이 짝이 되어서머리를 맞대고 하는 중 입니다.​​  ​아이들 완전빠져들었지요?​​​ ​ 누워서 하십니다ㅋㅋㅋ​골똘히 생각도 해야되고머리를 쓰느라 지쳤나봐요 ㅋㅋㅋ​​ ​근데 마치고 인사하고 가려니까 앵콜~~! 또 오세요!!하는 아이들ㅋㅋ​앵콜이라니~~~​노래라도 불렀어야 했나?ㅋㅋㅋ​​​초등학생 뿐만 아니라어른도 재미있는~​자율주행차 강의였습니다!​​​ 자율주행자동차 (빙 이미지 크리에이터)자율주행자동차의 연구과제 자율주행자동차의 주요 연구과제는 다음과 같습니다: - 자율주행 기술 개발 - ...m.blog.naver.com ​​​ 
모노즈쿠리 인스트럭터 능력향상및 현장개선 지도 노하우 공유 ,https://blog.naver.com/leenameun00/222950453461,20221212,"제38회 인스트럭터 SKILL-UP교육및 모노즈쿠리 현장개선 지도 성과발표회, 2022.12.08​1.쉽게 배우는 제조현장 적용 인공지능 기술: 한국생산기술연구원 윤종필 박사   최근 인공지능 기술은 어디까지 왔을까? 어떤 일을 할 수 있을까?   동영상에 소리 입히기, 이미지 합성(도로 등),자율주행, 카운팅(수량 등).  딥러닝 개요--사람의 신경세포의 정보처리 및 전달 과정을 수학적으로 모델링.  깊게 쌓을 수록 복잡한 문제를 풀 수 있지만 수 많은 피라미터를 학습시킬 충분한 양의   데이터가 필수이다.  영상만 입력하면 스스로 정답에 필요한 특징을 찾는다.  딥러닝이 잘 풀 수 있도록 충분한 양의 입력 데이터 와 딥러닝이 잘 풀 수 있도록 정확한  정답 데이터를 확보하는 것이 중요하고, 딥러닝은 우리가 만든 정답 데이터대로 학습한다.​  생산현장 AI 적용 분야 5가지 -설계 지능화--실험계획으로 물리적인 실행시간을 단축하고 정확도를 높여 개발기간 단축. -예지보전 지능화--다변량 모델기반 제조설비 이상감시로 설비고장 예지및 효율향상. -검사 지능화--딥러닝 결함 검출/ 분류 자동화로 품질향상및 인력 효율화. -공정 지능화--공정과 불량데이터를 연계하여 불량 원인 분석. -SCM 지능화--전 공장 물류설계/ 계획 최적화로 생산성 향상.​ 주요 제조 데이터 유형 -정형 데이터--공정 조건등의 데이터를 정량적으로 테이블화 할 수 있는 데이터. -이미지 데이터--불량검사,계측,진단 등의 목적으로 획득된 비전,SEM,X-ray등의 데이터. -시계열 데이터--앞뒤순서(시간적)의미가 중요한 진동,변위,온도,압력 등의 데이터. -다양한 데이터--3D 데이터,Text data,Multi modal signal 등 양질의 Raw data와 Labeled data가 많다면 다양한 일을 할 수 있다. 제조 AI-한계점 극복 이슈사항 -데이터 관점--데이터부족,데이터 불균형,과도한 레이블링,레이블링 오류. -상용화 관점--확장성,일반화 성능 확보,원인 분석 가능,연산량. 설비 이상 상태 진단. AI가 성공적으로 적용될 수 있도록 지속적인 노력 필요함. 제조 AI의 추진전략--문제 정의,데이터 수집,데이터 가공,모델 개발,활용.    데이터(입력+출력) 협업.​2. 스마트 제조에서 DX 전환으로 : 이명열 (주)동광사우 대표E/L 비상추략방지장치를 만드는 회사가 IT회사로 변신한 사례-Object Detection(객체 탐색,물체검출)기술을 활용하여 2톤 키트박스내에 무게를 달아 무게센서를데이터 연결하여 일정한 무게가 감지되면 부품 자동발주되게 함.(몸무게 재는 저울,로드셀 4개 50kg)-레이저가공기(5억,트럼프)기계 가동율을 알고싶음--기계에 레이져가 가동되면 램프에 불이 들어오는것을 조도 센서를 부착하여 데이터를 구축함. 목적은 기계 더 구입할 필요유무,제작리드타임등 분석.-고객사(H사) 제품 출하 박스에 제품이 누락되는 것을 알고싶음--제품 포장하기전에 사진을 찍어서보내는 데 적외선 센서로 자동촬영,시간날짜,QR코드 포함.상기내용을 현대E/L 고객사가 알고 전 협력사100개사에 지시하여 확대 적용 2년치 오더 받음.사무실과 현장 Data 일치 with RPA(사무자동화,동일한 패턴),화이트 칼라의 자동화.DX를 왜 해야 하는가? 숙련도를 줄이고 비용도 줄이고,인력난을 대비한 자동화, 업무 몰입을 하기위하며, 구축순서는 조직문화,새로운 BM 발굴,DX화 입니다.​2023년에는 무엇을 향상하시겠습니까?생산 비용 절감?효율성 향상?고품질 제품?리소스 최적화?직원 만족도 향상? ...모두 해당해요? 이러한 문제는 하프 자동화를 통해 해결할 수 있습니다. 인터컨티넨탈 서울 코엑스서울특별시 강남구 봉은사로 524 인터컨티넨탈 서울 코엑스 ​ "
"트루엔 공모주 청약 : 수요예측, 공모가, 상장일 ",https://blog.naver.com/harukomm/223097153470,20230509," 기업개요트루엔은 Edge AI, IoT 융합 영상감시 플랫폼(Platform)전문기업으로 영상감시 분야에서 축적된 독보적 기술력, 경험을 바탕으로 도시방범, 도로, 공항, 산업, 군사 시설 등 다양한 분야에서 영상 보안 사업을 선도하며, 사회 안전과 편의를 높이는데 기여하고 있다.​트루엔은 4차 산업 혁명으로 빠르게 진화하는 사회, 기술, 시대적 요구를 충족하고 확장성 있는 다양한 서비스를 구현하고 공급할 수 있는 새로운 제품 개발로 세계 영상 보안 산업의 혁신을 선도하고자 한다.​기존 영상감시 장치의 제한적인 영상 분석 처리(Video Analysis)에서 Deep Learning 기반의 영상 처리(AI Analysis)로 차량번호인식(ALPR), 객체탐지(Object Detection), 객체추적(Object Tracking), 지정객체 모자이크 처리(Privacy Masking) 등 다양한 AI기능을 Edge 시스템에서 지원하는 제품을 개발 출시하였다.  공모일정공모청약일 : 5.8(월) ~ 5.9(화)공모가 : 12,000원상장일 : 5.17 (수)​주관사미래에셋증권​기관경쟁률은 1689 : 1최소 청약 주식수 : 10주최소 청약 증거금 : 60,000원  주주구성보호예수 물량 680만주유통가능 물량 418만주​상장시 공모가 기준 시가총액 : 1300억원​의무보유 확약비율 : 8.24%​유통물량이 절반 가까이 되기 떄문에 매력적이지 않으며 사업의 재료 또한 투자 매력도가 떨어진다고 생각함.  재무제표지난해 매출은 388억원으로 전년 대비 7.8% 증가했다. ​영업이익이 90억원으로 같은 기간 8.2% 줄었지만 3개년 연속 90억원 이상을 유지하고 있다.​지난해 IP카메라의 매출은 약 318억원으로 전체 매출의 82% 비중을 나타냈다. ​이 밖에 2018년 공급하기 시작한 홈 카메라, 비디오 도어벨 등 IoT 기기를 통해 약 33억원(8%), 비디오인코더를 포함한 영상 스트리밍 솔루션 매출이 약 31억원(8%), 기타 서비스 매출이 약 6억원(2%)의 비중을 보였다.  안재천 트루엔 대표이사안재천 트루엔 대표이사는 ""인터넷프로토콜(IP) 카메라 시장의 성장 수혜 기대감과 함께 회사의 AI 카메라 원천 기술력을 바탕으로 지속적인 신규 솔루션 개발로 성장성을 증명하겠다""고 말했다.  트루엔 공모주 청약 일정​공모청약일 : 5.8(월) ~ 5.9(화)공모가 : 12,000원상장일 : 5.17 (수)​주관사미래에셋증권​기관경쟁률은 1689 : 1최소 청약 주식수 : 10주최소 청약 증거금 : 60,000원 "
EdgeSharing: Edge Assisted Real-time Localization and Object Sharing in Urban Streets ,https://blog.naver.com/bjoon1993/222477986787,20210820,"본 리뷰는 제가 공부하기 위해서 정리하는 것으로 내용이 부정확한 부분이 많이 있고, 정확하게 이해하지 못 한 부분이 있습니다. 비전공자가 컴공에 발을 담구는 과정으로 혹시 관련된 부분에 대해서 조언 해주시면 감사하겠습니다​이 논문은 2021 IEEE에 accepted되었으며, 선정한 사유는 Real-Time Localizaion에 대해서 좀 더 알아보고자 선정한 논문입니다.​Abstract교차로 위에서 각기 다른 카메라로 물체의 위치를 공유하는 것은 교통사고의 위험에서 벗어나는데  도움이 될 것입니다. 하지만 클라이언트와 탐지된 개체는 움직이기 때문에 정확하게 위치를 공유를 하는 것은 어렵습니다. 그래서 이 논문에서는 EdgeSharing이라는 방법을 제시합니다.3D 실시간 맵을 보유하여 이 영역을 통과하는 클라이언트 장치의 정확한 위치 측정과 객체 공유 서비스를 제공합니다.그 결과 평균 차량 위치 오차는 0.28 ~ 1.27m, 객체 공유 정확도 82.3 ~ 91.4%를 달성했으며, 제안된 최적화 기술로 대역폭은 70.12%감소,  end-to-end 지연시간은 40.09%까지 줄입니다.​I. Introduction미국 교통부 조사 결과 교차로에서 대다수의 교통사고 발생한 것으로 확인되었습니다. 개체를 공유하는 시스템은 움직이는 Object의 위치를 파악할 수 있고, 사각을 줄여 사고 발생 위험을 줄일 수 있습니다. 물론 움직이는 클라이언트에게 움직이는 Object의 정확한 위치를 알려주는 것은 굉장히 어렵습니다. 물론 관련된 연구는 있었지만, 메모리 사용량이 많아 모바일 디바이스에 적용하는 것은 어려웠습니다. 클라우드 기반으로 운용하기에는 latency가 존재하여 적합하지 않았습니다.이 논문에서 제시한 EdgeSharing은 최초로 localizaion과 object sharing system을 collaborate했습니다.edge cloud는 vehicle 또는 client로 부터  수집한 depth reading과 지역 이미지를 가진 3D feature map을 저장합니다. EdgeSharing은 또한 참가한 클라이언트로부터 object location을 탐지합니다. 이 논문에서는 Context-Aware Feature Selection(모서리의 계산 리소스를 사용하여 오프로드된 이미지에서 잠재적인 이동 객체에 대한 피쳐 포인트를 필터링하여 SLAM 위치화 정확도를 높입니다.)과 Collaborative Local Tracking mechanism(선택한 키프레임만 에지 클라우드로 오프로드하여 프레임 전송의 대역폭 사용량을 크게 줄이는 동시에 클라이언트 및 탐지된 객체의 위치를 최종 장치에서 추적)을 활용합니다. 또 Parallel Streaming and Processing model(parallel video streaming과 cloud processing)을 활용하여 latency를 줄입니다.​​II. Edgesharing Design클라이언트는 visual sensor를 가진 node로 cloud server에 연결되어 있어야 합니다. 모바일 클라이언트는 producer와 consumer로 나눠집니다. Producer는 자율주행차량, 카메라, 드론 등과 같이 지속적으로 server가 localizaion을 할 수 있도록 RGB 프레임을 지속적으로 보내줍니다.Consumer는 스마트폰, 스마트글래스등과 같이 지속적으로 주변 Object를 받는 Client입니다.Edge Server는 sensor로 부터 수집한 참가한 모바일 클라이언트, 다양한 각도에서 탐지한 Object 등의 정보를 종합하고, 종합한 데이터를 다시 모바일 클라이언트에 보내주는 역활을 수행합니다.Edge Server는 Device Localizaion, Object Detection, Object Sharing으로 이루어져 있습니다.Edge Server가 정확하게 3D pose information(location and orientation)정보를 정확하게 측정해야합니다. Sensor에서 GPS신호나 inertial sensor data가 정보를 제공하지만 도심에서 수신하는 정보로 오차가 많이 있습니다. Device Localizaion은 visual odometry techniques(?)기법을 활용하여 클라이언트에서 수집한 정보를 3D 변환 매트릭스로 추정합니다. 그리고 ORB-SLAM(?)알고리즘을 활용하여 재설계합니다. 쉽게 이야기하면 모바일클라이언트에서 수집한 정보로 3D map을 만들고, 이 정보(프레임)를 다시 클라이언트로 보냅니다.Object Detection은 state-of- the-art object detection algorithm(?)를 활용하여 이미지 프레임에서 새 객체의 위치를 탐색합니다. 그리고 EdgeSharing은 그 정보를 활용하여 프레임에 좌표정보를 예측하여 투영합니다.Object Sharing service는 저장된 Object를 인근 클라이언트와 공유하는 서비스입니다.​Vehicle에서 수집한 데이터를 Edge server로 전송하면 ORB-SLAM알고리즘을 통해 맵을 생성합니다. 맵이 생성되면, Edge server는 Device Localizaion은 모바일 device에 ORB feature 포인트를 3D맵에 알려줍니다. 이 시스템은 RANSAC 및 모션 모델 제약 조건을 사용하여 장치의 pose를 더욱 최적화합니다. Device Localization 서비스의 최종 출력은 World to Camera Transformation 매트릭스로 카메라에서 출력한 위치 좌표계까지 합칩니다.​Object Detection에는 크게 2가지 요소가 있습니다.하나는 Object Bounding Box Detection과 Object 3D Localization입니다.Object Bounding Box Detection은 Res-Net 50 based Faster RCNN Object Detection Model을 활용하여 각 프레임에서 Object location을 확인합니다. 이때 Microsoft Coco dataset을 활용하여 트레이닝 시킵니다.Object 3D Localization은 수집한 Object를 Consumer의 시각으로 제공하는 방법입니다. (1) 라이다, 레이더 등을 통하여 Vehicle로 부터 Object의 깊이를 측정합니다.(2) 측정한 2D 픽셀좌표계를 3D world 좌표계로 변환합니다.(equation1)(3) 모든 탐지된 Object의 위치는 즉시 Shared Object Database에 저장됩니다.Shared Object Database은 각 Object의 이미지를 근처의 클라이언트에게 제공합니다.Object SharingEdgeSharing은 Shared Object Database 저장된 Object를 클라이언트에 공유하여 가시선(Line-of-sight) 이상의 추가 정보를 제공합니다.특히 시스템은 장치 로컬라이제이션 서비스에서 계산한 변환 매트릭스(Tcw)를 기준으로 탐지 범위 내의 모든 공유 개체를 세계 좌표계에서 클라이언트 카메라의 좌표계로 변환합니다.서버는 공유 객체 위치를 결합하여 소비자 클라이언트로 다시 보냅니다.​C. System OptimizaionContext Aware Feature Selection method을 활용하여 각 프레임에서 stable feature를 선정합니다.Collaborative Local Tracking mechanism를 활용하여 높은 tracking accuracy를 보여주는 동안 지연시간을 줄입니다.a Parallel Streaming and Processing pipeline를 통해 전체 시스템의 지연시간을 줄입니다.​III. CONTEXT AWARE FEATURE SELECTION이 접근법의 직관적 관점은 이동 가능한 객체의 감지된 객체 경계 상자 안에 위치한 피쳐 포인트가 다른 배경 영역보다 이동할 가능성이 높다는 것입니다.EdgeSharing은 Object Detection 서비스를 활용하여 잠재적인 이동 객체의 경계 상자에 있는 피쳐 점을 직접 필터링합니다. 녹색 마커는 3D 지도에서 프레임과 피쳐 포인트 사이의 피쳐 일치입니다.파란색 마커는 이전 프레임과 일치함을 나타냅니다. 빨간색 마커는 이동하는 Object로 여기서는 폐기합니다.​IV. COLLABORATIVE LOCAL TRACKINGLocal Device Tracking.EdgeSharing은 모든 프레임을 Edge 서버로 오프로드할 필요 없이 클라이언트 디바이스의 위치를 실시간으로 로컬화할 수 있습니다.로컬 처리된 프레임의 경우 모바일 클라이언트는 장치의 업데이트된 위치만 Edge 서버로 전송하면 되며, 서버는 이 정보를 사용하여 데이터베이스에서 이 클라이언트로 개체를 공유할 수 있습니다.​Local Object Tracking.생산자 클라이언트가 프레임을 에지 서버로 계속 업로드하지 않고도 이미 탐지된 개체의 위치를 업데이트할 수 있습니다.​Adaptive Offloading.Adaptive Offloading은 마지막 오프로드된 프레임과 마지막 프레임 간의 피쳐 일치 수를 사용하여 마지막 프레임이 마지막 오프로드된 프레임과 상당한 양의 피쳐 일치 여부를 확인합니다. 그리고 클라이언트 장치가 현재 프레임을 에지 서버로 오프로드해야 하는지 아니면 로컬 추적을 위해 유지해야 하는지 여부를 결정합니다.​V. PARALLEL STREAMING AND PROCESSING(1) 클라이언트 디바이스에서 에지 서버로 프레임 전송,(2) 오프로드된 프레임의 객체 감지,(3) 오프로드된 프레임에서의 피쳐 처리(즉, 피쳐 추출 및 매칭) 및(4) 카메라 포즈 추정, 객체 위치 추정, 객체 공유 등 사후 처리.에지 서버에서 전송 및 이미지 처리 모두 상당한 시간이 소요되고 순차적으로 실행되므로 전체 시스템에 대해 엔드 투 엔드 지연 시간이 길어지는 것을 관찰했습니다.EdgeSharing에서 클라이언트 장치는 오프로드된 각 프레임을 4개의 슬라이스로 분할하여 하나씩 전송합니다.각 슬라이스가 에지 서버에 도착하면 시스템은 전체 프레임이 도착할 때까지 기다리지 않고 즉시 기능 추출 및 개체 탐지 작업을 시작할 수 있습니다.모든 개체 탐지 및 기능 처리 작업이 완료되면 시스템이 사후 처리 단계를 시작하고 나머지 작업을 완료합니다.Parallel Streaming and Process는 기본 접근 방식에 비해 거의 절반의 대기 시간을 단축할 수 있습니다.​VI. IMPLEMENTATIONClient Device : Nvidia Jetson TX2Edge Cloud : PC equipped with an Intel i7-6850K CPU and an Nvidia Titan XP GPU ​VII. EVALUATIONdevice localization accuracy, object sharing latency, bandwidth consumption, and end-toend latency​Dataset DescriptionCARLA라는 자율 주행을 위한 오픈 소스 시뮬레이터를 사용하여 두 개의 데이터셋을 수집합니다.CARLA는 이를 위해 만들어진 오픈 디지털 자산(도시 레이아웃, 건물, 차량)을 무료로 이용할 수 있도록 제공한다.우리는 실험을 위한 데이터 집합을 수집하기 위해 CARLA의 지도 3과 지도 5에서 4방향 정지 신호 교차로와 다른 도시 교차로를 신중하게 선택합니다.​실제 트래픽 시나리오에서 EdgeSharing의 성능을 추가로 평가하기 위해 우리는 LA의 교차로에서 120시간 데이터 세트를 추가로 수집합니다.이 데이터 세트에서는 윈드실드 아래 중앙에 장착된 GoPro를 사용하여 1280×720 해상도의 전체 드라이버 전면 뷰 비디오를 30fps로 녹화합니다.데이터 수집 기간 동안 10명의 운전자들이 30차례에 걸쳐 도시 거리에서 함대로 함께 운전했다.그림 7(c)는 비행대의 세 번째 차량에 의해 캡처된 비디오 프레임을 보여줍니다.​C. Accuracy of Device Localizationthe baseline solution (Baseline), our solution with the Context-aware Feature Selection method (+ Feature Selection), and our solution with both the Contextaware Feature Selection and the Collaborative Local Tracking method (+ Local Tracking).을 비교하여 측정 "
Yolov3-Object Prediction ,https://blog.naver.com/polpolie95/221851473887,20200313,"#Yolov3 #Yolo #Windows #ObjectDetection #CPU #GPU​안녕하세요, 오늘은 Object Prediction에 대해 설명하겠습니다.​ Class Prediction 원리​Yolov3는 Class Prediction을 통해 Class를 분류하고, ​해당 Class에 얼마나 맞는지 확인하는(Detection)작업을 수행합니다.​이때, Regression(회귀)와 Classification(분류)를 동시에 하는 멀티테스크를 합니다.​보통 80개의 클래스가 80개다라고 가정했을때, ​그 80개의 클래스에 대해서 SoftMax(소프트맥스)를 합니다.​그리고 각각 클래스를 Sigmoid(시그모이드) 취해서 BinaryClassfication으로 바꿔서 ​멀티 Classification을 쉽게 할 수 있도록 구성했습니다.​그러니까 0과 1로 쉽게 구분하게 처리를 한다라는 것이죠.​ Predictions Across Scales그렇다면 좀더 자세히 Prediction을 살펴보겠습니다.​   ​이전 포스팅에 BoundingBox에 대해 설명한 적이 있습니다. ​미리 Predefined한  BoundingBox를 AnchorBox라고 부르고, ​그 AnchorBox를 몇개를 사용하는지에 따라 물체를 감지합니다.​실제로 3가지 스케일에 대해서 3개의 바운딩박스를 사용하게 됩니다.​스케일에 따라서 박스의 갯수가 resolution(해상도)에 따라 바뀌어 정확하게 3+3+3 = 9개라고 하기에는 애매한 부분이 있습니다.​위에 보시면 강아지 그림에 그리드로 셀이 있습니다. ​하나의 스케일에서 보면은 빨간색 한개의 셀에서 박스를 3개를 뽑게 됩니다.​가로 세로가 N x N 일때 위의 그림처럼 Prediction Features Map 3D형태로 나오게 됩니다.​이때 빨간색 박스 부분의 채널이 255가 됩니다. ​그 이유는 4(BoundingBox 일루션 x(가로), y(세로), w(너비), h(높이)에 대해서 각각 하나씩) ​1(Confidence - Objectness Score)​80(Yolov3에서 제공되는 coco데이터 소스의 클래스 갯수)​즉, 이 스케일에서 하나의 바운딩박스 3개씩 뽑을 거라서 곱하기 3을 하는 것이죠.​N x N x(3 x (4 + 1 + 80)) = 255​ Anchor Boxes그렇다면 Anchor Boxs는 뭘까요?​Anchor Box를 통해 Object Detection(물체감지)를 한다고 했습니다.​Anchor Boxes는 K-means 클러스터링을 통해 바운딩 박스중에서 결정됩니다.​즉, 내가 가지고 있는 trainning set을 분석을 해서 ​k개의 클러스터로 묶어 각 클러스터와 거리 차이의 분산을 최소화한 Anchor box를 만듭니다.​  K-means Clustering 예제 그림 출처. 위키백과 예를 들어 CCTV를 통해 보행자만 찾으면 된다라고 했을 때, ​대부분 서 있는 사람이 많기에 가로로 긴 바운딩박스를 많이 가져갈 필요가 없습니다.​즉, 내가 찾고자 하는 물체의 특징을 잘 반영하고 있는 것이 ​실제 Trainning Set에 정답(Ground Truth) 바운딩박스가 잘 반영되어 있을 것입니다.​그렇다면 그 특징을 분석해서 어떤 Anchor Box를 정하는 것이 가장 좋은 방법입니다.​9개씩 클러스터링 한 다음 이것을 스케일 3개로 각 3개씩 나눕니다.​가장 작은 물체를 찾는 쪽에서는 앞에서 3개를 쓰고,​중간 사이즈는 가운데 3개,​가장 큰 물체를 찾을 때에는 뒤에서 3개 바운딩박스를 씁니다.​ Yolov3 Architecture다음은 Yolov3 구조입니다.​피라미드 구조와 비슷합니다.​가장 작은 Feature Map에서 가장 큰 Feature Map으로 점점 키워가며 물체를 인식합니다.​ 딥러닝 CNN의 경우 ResNet을 사용하는데, ​Yolov3와 반대로 Feature Map을 줄여나가면서 물체를 인식합니다.​이렇게 하면 아래쪽에 있는 것들은  Feature에 대한 성숙도가 떨어져 성능의 문제가 있었고​Yolov3는 이것을 보완하기 위해 반대로 Feature 사이즈를 키워나가면서 Highlevel extraction을 뽑고​위치정보를 가지고 와서(이전에는 줄어드면서 위치정보가 사라졌음) 덧셈을 해 좋은 성능을 내려고 했습니다.​   여기서도 사이즈를 다시 키우고  빨간색 화살표 처럼  사이즈를 다시 가지고 와서 같이 쓰는 방식으로 진행합니다.​아래 그림을 보면 Scale 3의 해상도(Resolution)이 가장 크니까 여기서 가장 작은 물체를 찾을 것이고​Scale 1에서 가장 큰 물체를 찾게 되는 구조로 되어 있습니다.​   ​오늘은 여기까지 포스팅 하겠습니다.​다음에는  Yolov3를 사용하는 이유에 대해 설명드리겠습니다. "
"가성비 좋은 차량용 고속 무선충전기&시거잭 추천, 초텍 15W 핸드폰 거치대  ",https://blog.naver.com/supapa13/222970007714,20221230,"요즘 운전할 때마다, 아이폰 14 PRO의 다이내믹 아일랜드 덕을 톡톡히 본다. 최근 티맵에서 다이내믹 아일랜드와 AOD를 묶어 라이브 액티비티 기능을 업데이트했다. 덕분에 기존보다 티맵을 활용하는 비중이 크게 증가했다. ​ ​이때 당연히 꼭 있어야 할 아이템, 차량용 고속충전기다. 대부분 차량용 핸드폰 거치대 추천 품목과 일치한다고 보면 되는데, 한 가지 꼭 짚어봐야 할 점도 있다. 차량용 핸드폰 거치대를 구입했으나, 꼭 고속충전기라고 할 수는 없다는 점이다. ​ ​일단 제품 자체가 차량용 고속충전기여야 하며, 동시에 시거잭 또한 고속충전을 지원해야 한다. 마지막으로 케이블까지 그에 맞는 제품을 갖춰야 한다. 그렇게 3박자가 다 맞아떨어질 때, 가장 빠른 무선 충전 속도를 즐길 수 있다. ​ ​필자가 본 포스팅을 통해 추천하고자 하는 제품은 초텍의 15W 차량용 고속충전기, T202-F다. 15W 고출력을 지원하며, 심플하고 컴팩트한 디자인은 차량 내 인테리어 드레스업 효과도 가질 수 있다. 물론, 핸드폰 거치대로서의 역할은 기본이다.  ​ ​위에서 언급한 대로, 제대로 고속충전을 하기 위한 준비물이 있다. 바로, 시거잭과 그에 맞는 케이블이다. 일단 케이블은 T202-F 제품의 기본 구성품으로 포함되어 있으니 걱정할 것 없다.​ ​그 외 차량용 시거잭만 구비하면 되는데, 이때도 초텍의 38W 듀얼 포트 제품 TC0005를 추천한다. QC & PD 3.0/2.0 지원, 아이폰과 갤럭시까지 모두 고속 충전할 수 있다. 게다가 듀얼 포트다. Type-C & A 포트가 각 1개씩 총 2개 듀얼 구성이며 최대 38W의 고출력을 뿜어낸다. ​ ​다시 차량용 고속충전기로 돌아와서, 실 사용기부터 전해보자면 이렇다. 우선, 거치가 쉽다. 매우 심플한 방식이다. 기본 구성품으로 송풍구 집게를 제공하는데, 가격 대비 퀄리티가 상당히 높다. 저가형 제품들 중, 기본적인 마감부터 현저히 낮은 경우가 참 많다. 그런 제품들과 크게 비교될 만큼, 마감성이 상당히 우수하다. ​ ​초텍은 북미에서 시작되어 글로벌 스마트 기기 충전 액세서리 브랜드로 우뚝 선 기업이다. 그저 자사 브랜드만 덧대어 판매하는 OEM 유형과는 전혀 다르다. 자체 R&D(연구개발) 시스템과 QC 체계를 구축하고 있으며, 직접 제품을 개발하고 생산하고, 판매까지 병행한다. 그에 따른 무선충전 기술력을 보유한 전문 브랜드인데, 제품을 볼 때마다 확연히 다른 퀄리티와 마감 등을 보면 브랜드의 아이덴티티를 분명히 느낄 수 있다. ​ ​아무튼 송풍구 집게는 기본적인 고정력도 상당하고, 꽤 깊은 형태라 필자의 제네시스 GV80과 같은 슬림한 에어벤트 차량에도 사용하기 좋다. 또한 하단부에 고무그립이 덧대어진 형태라, 혹여나 모를 스크래치에 대한 걱정도 덜어낼 수 있었다. 참고로 4천 원을 추가하면 대시보드 거치대도 같이 구입할 수 있다. ​ ​초텍 차량용 무선 고속충전기 T202-F는 일단 컴팩트하다. 사이즈가 꽤 작은 편이라 마음에 쏙 들었는데, 성능은 그와는 반대다. 최소 4인치 스마트폰에서 최대 6.5인치까지 폭넓게 거치할 수 있으며, 필자는 주로 아이폰 14 PRO 모델을 사용한다. ​ ​충격에 강한 타입인 PC+ABS 소재를 사용, 빼어난 내구성을 자랑한다. 디자인도 마음에 쏙 드는데, 심플하고 깔끔한 블랙 하이그로시 컬러가 차량 드레스업 효과를 연출해 준다. ​ ​갤럭시는 10W, 아이폰은 7.5W를 지원한다. 최근에 주말 가족 나들이에도, 평일 출퇴근 길에도 매일 사용해 보고 있는데, 충전 속도와 안정성 모두 합격점 그 이상이었다. 늘 빠르게 충전되는 걸 볼 수 있었고, 주행 중 흔들림도 1도 없다. ​  ​물론 FOD 센서도 탑재되어 있다. 스마트폰만을 자동으로 인식하는 기술이며, Foreign Object Detection의 약자다. 스마트폰을 제외한 다른 어떤 금속 재질의 물질, 혹은 이물질이 있다면 반응하지 않는다. 안전과 편의성을 위해서도 꼭 필요하지만, 제품 보호를 위해서도 반드시 체크해야 하는 제원 중 하나다. ​ ​목적지에 도착했을 때 아직 폰을 꺼내지 않았는데 (습관적으로) 시동부터 꺼버리는 경우가 있다. 괜찮다. 그럴 때도(전력 공급 중단) 잔류 전력을 보유하고 있어 스마트폰을 꺼낼 수 있다. ​ ​빠른 탈부착도 장점이다. 한 손으로 쉽게 꺼낼 수 있고, 당연히 한 손으로 거치할 수도 있다. 언제나 올리기만 하면 고속무선충전이 시작되는데, 굳이 충전이 필요 없다면 케이블을 제거해서 차량용 핸드폰 거치대로 쓸 수도 있다. 안정적으로 잡아주는 건 똑같으니까.​ ​제품 보증 기간은 2년이다. 꽤 넉넉하다. 그리고 무엇보다, 가성비가 뛰어나다고 말할 수밖에 없다. 심플하고 고급스러운 디자인에 크지 않은 부피, 안정적인 거치대로서의 역할과 깔끔한 고속무선충전기로서 활용할 수 있는 아이템이다. 그럼에도 가격은 2만 원 초반대다. ​- 바로 가기 -​ ​한번 찾아보자. 이 정도의 퀄리티를 제공하면서, 이만한 가격의 제품이 있는지를. 있어도 디자인, 안정성, 충전 효율 등 모든 면에서 본 제품을 따라가긴 어려울 것이다. 그만큼 가성비 위주로 접근하고자 하는 분들께 최고의 선택지가 될 수 있는, 강력히 추천하는 차량용 핸드폰 무선 충전 거치대다. ​​​   ​ "
"성수에서 열리는 2022 대한민국 과학축제, 이번 주말 방문해보세요! ",https://blog.naver.com/with_msip/222851839370,20220818,"2022 대한민국 과학축제가 오늘(18일)부터 21일 일요일까지 4일간 서울 성수동 에스팩토리에서 개최됩니다.​이번 축제는 '페스티벌 어스'라는 주제로, 지속가능한 지구와 인간의 공존을 위한 과학기술의 역할을 생각해보자는 취지로 열리는데요. 목요일부터 토요일까지는 오후 12시부터 오후 9시까지(오후 8시 입장마감), 마지막 날인 일요일은 오후 12시부터 오후 5시까지(오후 4시 입장마감) 운영되니 참고해주시기 바랍니다. 부대행사도 서울어린이대공원 열린무대 일원에서 진행되고 있습니다.    행사 첫 날인 18일 목요일 방문한 후기와 어떤 프로그램을 즐길 수 있는 지 소개드리니, 이번 주말 여유가 되신다면 성수동 에스팩토리에 방문하셔서 2022 대한민국 과학축제를 즐겨보세요! 이번 대한민국 과학축제의 테마가 지구이다 보니, 에스팩토리 A동 야외에는 녹아내리는 빙하가 얼음으로 표현되어 6년 38일 남짓 남은 빙하의 시간과 함께 얼음이 녹는 것을 보여줍니다. 녹아내리는 빙하를 통해 지구와 인류 앞에 놓인 위기를 직시하고, 지속가능한 지구와 인류의 공존을 위한 과학기술의 역할과 노력에 관심을 가지도록 독려하는 해당 전시물이 인상적이었습니다. A동에서는 강연 프로그램(렉처와 토크랩)과 함께 과학컨텐츠들을 제공하는 사이언스올, 지역과학문화거점센터, 국립대구과학관과 국립부산과학관, 국립과천과학관, 생활과학교실, 과학문화활동, YTN 사이언스로 구성된 러닝랩이 있습니다. AR, VR 기기들이 비치되어 있어 행사진행위원들의 도움을 받아 AR은 태블릿과 카드를 이용하여, VR은 VR 전용 기기를 이용하여 직접 체험할 수 있습니다. 한 군데가 아니라 여러 부스에서 VR 체험을 진행하고 있으니 관심이 가는 주제를 선택하여 VR 체험을 해보면 더욱 흥미로울 것 같습니다. 과학기술 전시물 관람도 빠질 수 없겠죠? 디지털 신호와 아날로그 신호를 전환해주는 인버터와 컨버터, 풍력발전기의 원리 등을 포함한 버튼을 누르면 작동하는 전시물부터, 직접 수력이나 풍력발전기를 키트로 만들어볼 수도 있습니다. 해당 체험들은 전시 장소에서 선착순 명단 작성을 통해 진행되니 부스에 먼저 들러서 예약하시는 것을 추천드립니다. 국립과천과학관 부스에서는 간단하게 흥미로운 과학을 눈으로 볼 수 있는 전시물들을 체험할 수 있도록 해두었는데요. 행사진행위원분들께서 친절하게 과학적 원리에 대해 설명해주시니, 태풍의 경로가 바뀌는 이유, 평평한 데 볼록렌즈의 효과를 내는 프레넬 렌즈, 디지털 현미경, 두 개의 추를 이용하면 랜덤한 경로로 움직이는 전시물 등을 직접 체험해보세요. 휴식이 필요하시다면 A동 3층 강연장소에서 휴식할 수도 있습니다. 과학기술정보통신부와 한국과학창의재단에서 선정한 우수과학도서를 비치해두었습니다. 아동/어린이부문부터 초등, 중/고등학생, 대학/성인부문으로 다양화하여 비치해두었으니, 휴식을 취하며 책을 읽어보는 것도 좋겠습니다. 과학기술과 사회라는 책을 무료나눔하고 있으니 관심이 가신다면 자유롭게 가져가시면 되고, 감상문을 쓰는 이벤트도 진행 중이니 참고하시면 좋겠습니다. B동 3층 루프탑에서는 오후 5시 이후 스크린을 통해 영상이 송출되고, 과학문화 공연 및 성인을 위한 과학 체험 이벤트가 열립니다.  A동 건물의 우측 길을 통해 D동으로 이동하는 주차장 한 쪽 부분에서는 대국민 기상교육 기상과학축전으로 기상기후 체험존이 운영되고 있으니, 이번 축제의 작은 부분이라도 놓치지 않고 관람을 원하시는 경우에는 이 체험존에도 방문해보세요. D동 1층에서는 어스&테크랩이라는 테마로, 지속가능한 지구를 위한 출연연(한국원자력연구원, 한국에너지기술연구원, 한국기계연구원, 한국전자통신연구원), 과학기술원(대구경북과학기술원(DGIST), 광주과학기술원(GIST), 울산과학기술원(UNIST), 한국과학기술원(KAIST)), 3층에서는 퓨처랩이라는 테마로 대기업, 벤처/스타트업의 과학기술 성과를 전시합니다.  어스&테크랩에는 지속가능한 지구를 위해 연구되고 있는 기술들을 주로 전시하고 있어, 어린 학생들에게는 조금 어려운 내용일 수도 있겠습니다. 여러 신기술들이 연구되고 있음을 소개하는 공간이라고 생각하시면 되시는데요, 몇 가지 소개해드릴테니 직접 축제장에 방문하셔서 실물을 보시면 더욱 의미있을 것 같습니다. ​한국원자력연구원에서는 사람과 동물을 치료하는 방사성동위원소를 생산 및 공급하고 있고, 한국전자통신연구원에서는 3년 전에 자체개발한 인공지능 NPU 서버인 ArtBrain-K로 실시간 물체감지(Object detection)를 태블릿을 통해 보이고 있습니다. D동에서도 기술을 모식화한 키트로 만들기 프로그램을 운영하고 있으니, 참여해보아도 좋을 것 같습니다.  대구경북과학기술원(DGIST)에서는 생체모방로봇 시연과 나무 섬유소와 한천을 이용한 바닷물(해수)을 사용할 수 있는 물인 담수로 바꾸는 기술을 선보이고 있었습니다. 나무섬유소에 광열입자를 섞어 한천과 결합하면 해수가 태양광만으로도 증발하여 높은 효율로 담수를 얻을 수 있는 기술입니다. 광주과학기술원(GIST)에서는 탄소가 2개인 이산화탄소를 탄소가 4개인 부탄과 부탄올로 합성하는 데에 높은 효율을 보이는 나노물질을 활용한 전기화학 촉매를 개발하여, 해당 내용을 전시하고 있었습니다. 한국과학기술원(KAIST)에서는 펨토초 레이저를 이용해 탄소를 포함한 부도체를 전기가 통하게 하는 기술을 소개하고 있었습니다. 마지막으로 퓨처랩에서는 대기업과 벤처, 스타트업의 성과를 소개합니다. 지속가능한 지구를 위해 재활용하는 브랜드들이 많이 있었는데요, 119 방화복을 활용하는 119REO, 국내산 페트병을 100% 재활용해 재생 단섬유를 생산하는 건백, 삼성의 카본 트러스트 탄소발자국 인증 제품, 실험실에서 만들어내는 고기인 배양육 스타트업 씨위드, 비건 포뮬러 처방으로 생산되며 투명페트를 무한재활용하는 캠페인을 진행하는 아로마티카, 우리 삶에 녹아 있는 측정과학을 테마로 한 2021 대국민 과학 굿즈 아이디어 공모전에서 대상을 수상한 유레카(A, B, D동의 도장을 다 모아 D동 1층 안내데스크로 가면 유레카의 L자 화일로 교환해줍니다) 등 여러 기업들을 만나볼 수 있습니다.​이번 주말 성수동에서 대한민국 과학축제를 함께 즐겨보시는 것은 어떨까요?   ​ "
yolo object ,https://blog.naver.com/hera0love/222099477552,20200925,​​https://www.pyimagesearch.com/2020/01/27/yolo-and-tiny-yolo-object-detection-on-the-raspberry-pi-and-movidius-ncs/ YOLO and Tiny-YOLO object detection on the Raspberry Pi and Movidius NCS - PyImageSearchLearn how to utilize YOLO and Tiny-YOLO for near real-time object detection on the Raspberry Pi with a Movidius NCS.www.pyimagesearch.com ​깃허브 yolo objecthttps://github.com/yehengchen/Object-Detection-and-Tracking yehengchen/Object-Detection-and-TrackingYOLO & RCNN Object Detection and Multi-Object Tracking - yehengchen/Object-Detection-and-Trackinggithub.com ​ 
MMRotate 설치 및 Demo 실행 ,https://blog.naver.com/ziippy/222826031210,20220723,"Background MMRotate 는 openMMlab 에서 제공하는 오픈 소스 중 하나다.openMMlab 은 https://openmmlab.com/ 에서 확인할 수 있듯이, 2018.10 월에 MMCV (Computer Vision) 를 시작으로 많은 것을 하고 있다.​MMDetection, MMClassification, MMSegmentation, MMPose, MMOCR, MMTracking, MMGeneration, MMDetection3D, MMFlow, MMRotate 등 많은 Project 를 하고 있고,유명하고 최신의 프레임워크를 제공하며, SOTA (State Of The Art) 모델들도 빠른 시일내에 적용되고 있다.​어쨌든 Rotated Object Detection 에 대한 MMRotate 도 제공하고 있고,MMRotate 는 PyTorch 기반 rotated object detection 을 수행할 수 있는 open source 프로젝트이다.(https://github.com/open-mmlab/mmrotate 에 자세히 설명되어 있다.) 참고로, Rotated Object Detection 이 뭐냐하면? 아래 그림처럼 찾고자 하는 object 를 사각형 형태로 찾을 때90도 기준 사각형이 아니라, 회전해서 찾는다는 것을 말한다. 그런데 왜 굳이 Rotated 해서 찾는걸까?그건 위 그림에서 회색 영역이 아닌 부분이 학습되는 것을 방지해서 object 를 더 잘 찾기 위한 것이다.""에이~ 별로 넓지도 않은데 그게 문제가 되나요?"" 라고 묻는다면.찾고자 하는 object 가 긴 직사각형 형태라면, 네모 박스 안에 object 영역보다 쓸모없는 영역이 더 넓을 수 있기 때문이다.​아래 그림 예시를 보면 확실히 알 수 있을 것이다. 이정도로 간략히 설명하고, 여튼 저렇게 rotated 된 object 를 detection 하기 위한 open-source 프레임워크인MMRotate 를 설치 및 데모 테스트 까지 해보자.​ 가상 환경 구축 우선 conda init bash 를 해서 conda 를 shell 에 initialize 후 $ conda init bash﻿$ conda create -n mmrotate python=3.8 -y$ conda activate mmrotate﻿ ​ CUDA 버전 확인 $ nvcc --versionnvcc: NVIDIA (R) Cuda compiler driverCopyright (c) 2005-2022 NVIDIA CorporationBuilt on Tue_Mar__8_18:18:20_PST_2022Cuda compilation tools, release 11.6, V11.6.124Build cuda_11.6.r11.6/compiler.31057947_0 난 11.6 버전이 설치되어 있다.​ MMCV 설치 전 환경 확인 MMRotate 도 내부적으로 MMCV 를 사용하고 있다. 어쨌든 CV 즉, Computer Vision 이기 때문이다.https://github.com/open-mmlab/mmcv여기서 mmcv 의 버전에 맞는 CUDA 와 torch 버전을 확인할 수 있다.(22.07.23 현재 기준 아래 테이블과 같다.) 즉, CUDA 11.5 에 torch 1.11 을 이용해서 설치하면 되겠다.엇? 그런데 나는 지금 CUDA 1.16 이 설치되어 있는데? 어차피 하위 호환이 될 것이라고 생각하고 진행해보자.​ PyTorch 설치 PyTorch 설치 Command 는 https://pytorch.org/ 에서 PyTorch 버전과 OS, Package, Language, Computer Platform 을 선택하면 Run time command 영역에서 확인할 수 있다. Comuter Platform 에 11.5 는 없지만, 느낌적인 느낌으로 아래와 같이 Command 를 작성해서 해 보았다. $ conda install pytorch==1.11.0 torchvision==0.12.0 torchaudio==0.11.0 cudatoolkit=11.5 -c pytorch 설치가 잘 되었는지 확인 $ conda list | grep torchpytorch                   1.11.0          py3.8_cuda11.5_cudnn8.3.2_0    pytorchpytorch-mutex             1.0                        cuda    pytorchtorchaudio                0.11.0               py38_cu115    pytorchtorchvision               0.12.0               py38_cu115    pytorch Good!​ MMCV 설치 나는 CUDA 11.5 에 torch 1.11 버전을 이용해서 설치할 것이므로 아래 command 를 이용한다.(pip install mmcv-full -f https://download.openmmlab.com/mmcv/dist/{cu_version}/{torch_version}/index.html 이런 형식이기 때문이다.)mmcv-full 도 1.6.0 버전을 설치한다. $ pip install mmcv-full==1.6.0 -f https://download.openmmlab.com/mmcv/dist/cu115/torch1.11.0/index.html ​ MMDetection 설치 MMRotate 도 detection 기능을 수행하므로, MMDetection 을 설치한다. $ pip install mmdet MMDetection 설치 도중 오류ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.jupyter-client 7.3.4 requires entrypoints, which is not installed.=> pip install jupyter-client 수행​​자 이제 드디어​ MMRotate 설치 $ git clone https://github.com/open-mmlab/mmrotate.git$ cd mmrotate$ pip install -r requirements/build.txt$ pip install -v -e . # or ""python setup.py develop"" 여기까지 오기 까지 로그 내용들은 다 생략했지만, 생각보다 오래 걸리는 명령어들도 있다. 10분 이상 걸린것도..암튼 느긋하게 기다리고 차근차근 하면 잘 설치될 것이다.​22.08.26AWS 에서 하니 금방 되네~​ MMRotate 폴더 구조 확인 앞서 git clone 으로 받은 mmrotate 폴더 구조는 다음과 같다. 참고로 checkpoint, data 폴더는 내가 필요해서 만든 것이고, work_dirs 폴더는 학습 결과가 담기는 폴더다.- checkpoint 폴더는 내가 만든 폴더로 Model Zoo 에서 다운 받은 pre-trained 모델을 담기 위한 폴더다.- data 폴더는 내가 만든 폴더로 학습 시 사용할 데이터를 담아두려고 만든 폴더다.- work_dirs 폴더는 학습하면 자동으로 생기는 폴더다. checkpoint 파일, config 파일, log 파일이 위치한다.​ 데모 실행 앞서 보여준 폴더 구조에서 demo 폴더에 가면 demo 용 .jpg 파일과 추론용 파일들이 위치해 있다.demo.jpg 은 다음 그림과 같다. (이미지가 커서 일부분만 보여준다.) 아무래도 Rotate 객체 검출은 상공 이미지에서의 객체 검출 분야에서 사용될 것 같았는데, demo 도 마찬가지다.이 파일에 대해 추론하는 코드인 image_demo.py 를 이용해서 객체를 검출 해 보겠다.​앞서도 잠깐 언급했지만, 학습/추론 하기 위한 환경설정 파일들은 configs 폴더 내에 있다.그 중 demo 에서는 oriented_rcnn 에 있는 oriented_rcnn_r50_fpn_1x_dota_le90.py 를 이용할 것이다.또한, 사전에 pre-trained 된 모델 파일은 이용할텐데https://github.com/open-mmlab/mmrotate 에 언급된 Model Zoo 항목 중 - Oriented R-CNN 의 링크를 타고 들어가서, - ResNet50 백본에서- oriented_rcnn_r50_fpn_1x_dota_le90 Config 에 해당하는 model 을 다운 받는다. 다운받은 .pth 파일을 앞서 설명한 checkpoint 폴더에 넣어둔다. (폴더가 없으면 만든다.) $ wget https://download.openmmlab.com/mmrotate/v0.1.0/oriented_rcnn/oriented_rcnn_r50_fpn_1x_dota_le90/oriented_rcnn_r50_fpn_1x_dota_le90-6d2b2ce0.pth 이후 아래 명령어를 이용해서 demo 를 해본다. (난 6번 GPU를 사용해서 하려고 앞에 CUDA_VISIBLE_DEVICES=6 을 붙였다.) $ pwd/home/ziippy/mmrotate$ CUDA_VISIBLE_DEVICES=6 python demo/image_demo.py    \             ./demo/demo.jpg     \              configs/oriented_rcnn/oriented_rcnn_r50_fpn_1x_dota_le90.py    \              checkpoint/oriented_rcnn_r50_fpn_1x_dota_le90-6d2b2ce0.pth   \             --out-file=demo/demo_output.jpg/opt/conda/envs/mmrotate/lib/python3.8/site-packages/mmdet/models/dense_heads/anchor_head.py:116: UserWarning: DeprecationWarning: `num_anchors` is deprecated, for consistency or also use `num_base_priors` instead  warnings.warn('DeprecationWarning: `num_anchors` is deprecated, 'load checkpoint from local path: checkpoint/oriented_rcnn_r50_fpn_1x_dota_le90-6d2b2ce0.pth/opt/conda/envs/mmrotate/lib/python3.8/site-packages/mmdet/models/dense_heads/anchor_head.py:123: UserWarning: DeprecationWarning: anchor_generator is deprecated, please use ""prior_generator"" instead  warnings.warn('DeprecationWarning: anchor_generator is deprecated, '$ 위와 같이 오류 없이 끝났다면 정상이다. demo 폴더에 생긴 demo_output.jpg 파일을 살펴보자. (역시 이미지가 커서 일부분만 보여준다.) large-vehicle 과 small-vehicle 을 잘 잡음을 확인할 수 있다.​ 마무리 이렇게 해서 MMRotate 를 Install 하고 demo 를 해 보았다.앞으로 이어서 Custom Dataset 을 이용해서 학습하는 과정을 해 보겠다.​일단 여기까지.​[끝] "
FortiGate Device Detection를 사용한 정책 적용 ,https://blog.naver.com/tcanon/222977147740,20230106,[Device Detection]FortiGate에서 네트워크를 모니터링하고 해당 네트워크에서 작동하는 단말에 대한 정보를 수집할 수 있도록 Device Detection을 활성화할 수 있다. 이 기능을 통해 수집된 단말에 대해 정책 적용이 가능하다.​[단말에 대해 수집 가능한 정보]- MAC 주소- P 주소- 운영 체제- 호스트 이름- 사용자 이름- FortiOS가 장치와 인터페이스를 감지한 경우​[설정 방법]1. Network -> Interfaces -> 내부 스위치와 연결된 포트 설정2. Networked Devices -> Device Detection enable → 이 기능을 활성화 하지 않으면 내부 단말들의 MAC을 수동으로 다 등록해야 됨 3. User & Device -> Device Inventory  → 내부에서 사용중인 단말 확인 가능  → 새 장치 개체(MAC 기반 주소)를 장치에 추가하여 장치 주변의 정책을 관리할 수 있습니다.       MAC 기반 주소를 추가하면 장치를 주소 그룹 또는 정책에서 직접 사용할 수 있습니다.   → 바로 등록 가능 ​4. 등록된 내부 단말 기준으로 방화벽 정책 적용 가능  - Source 설정 부분에서 device 항목 클릭스 등록된 단말 확인 가능 ​[참고]V6 버전에서는 address object에서 MAC을 등록하는 기능이 없고 device쪽에서 바로 등록이 된다. 하지만 V7로 펌웨어 업그레이드 되면서 다시 address object에서 device(Mac  address) 등록이 가능하게 변경된 것 같다. 
[컴퓨터 비전] Python + OpenCV 따라하기 - Object Tracking ,https://blog.naver.com/real_world0222/222291995823,20210329,"https://pysource.com/2021/01/28/object-tracking-with-opencv-and-python/ Object Tracking with Opencv and Python - PysourceIn this tutorial we will learn how to use Object Tracking with Opencv and Python. First of all it must be clear...pysource.com <본 글은 위 링크를 그대로 따라한 후기>​​https://yongku.tistory.com/entry/PYTHON-No-module-named-cv2-%EB%AC%B8%EC%A0%9C-%ED%95%B4%EA%B2%B0%ED%95%98%EA%B8%B0 [PYTHON] No module named 'cv2' 문제 해결하기Python을 사용하다가 보면 import 할 때 No module named 'cv2'가 뜨면서 에러가 발생한다. 그럴 경우 아래와 같이 따라하면 해결이 됩니다. 1. Anaconda가 설치되어 있을 경우 Anaconda Prompt를 실행합니다. 2..yongku.tistory.com (base) C:\Users\KIMJUNGRYUL>import cv2'import'은(는) 내부 또는 외부 명령, 실행할 수 있는 프로그램, 또는배치 파일이 아닙니다.(base) C:\Users\KIMJUNGRYUL>(base) C:\Users\KIMJUNGRYUL>pip install opencv-pythonRequirement already satisfied: opencv-python in c:\users\kimjungryul\anaconda3\lib\site-packages (4.5.1.48)Requirement already satisfied: numpy>=1.14.5 in c:\users\kimjungryul\anaconda3\lib\site-packages (from opencv-python) (1.19.2)(base) C:\Users\KIMJUNGRYUL>pythonPython 3.7.10 (default, Feb 26 2021, 13:06:18) [MSC v.1916 64 bit (AMD64)] :: Anaconda, Inc. on win32Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.>>> import cv2>>> ^Z  File ""<stdin>"", line 1    ^Z    ^SyntaxError: invalid syntax>>> https://needneo.tistory.com/46 ModuleNotFoundError: No module named 'cv2에러 내용 ModuleNotFoundError: No module named 'cv2.cv2' 단순히 cv2 모듈을 다시 설치하면 될 것으로 판단하여, pip install opencv-python을 시도해보았으나 이미 설치가 되어 있었다. (base) E:\Project\uni..needneo.tistory.com https://yongku.tistory.com/entry/PYTHON-No-module-named-cv2-%EB%AC%B8%EC%A0%9C-%ED%95%B4%EA%B2%B0%ED%95%98%EA%B8%B0 [PYTHON] No module named 'cv2' 문제 해결하기Python을 사용하다가 보면 import 할 때 No module named 'cv2'가 뜨면서 에러가 발생한다. 그럴 경우 아래와 같이 따라하면 해결이 됩니다. 1. Anaconda가 설치되어 있을 경우 Anaconda Prompt를 실행합니다. 2..yongku.tistory.com (base) C:\Users\KIMJUNGRYUL>pip uninstall opencv-pythonFound existing installation: opencv-python 4.5.1.48Uninstalling opencv-python-4.5.1.48:  Would remove:    c:\users\kimjungryul\anaconda3\lib\site-packages\cv2\*    c:\users\kimjungryul\anaconda3\lib\site-packages\opencv_python-4.5.1.48.dist-info\*Proceed (y/n)? y  Successfully uninstalled opencv-python-4.5.1.48ERROR: Exception:Traceback (most recent call last):  File ""C:\Users\KIMJUNGRYUL\Anaconda3\lib\site-packages\pip\_internal\cli\base_command.py"", line 189, in _main    status = self.run(options, args)  File ""C:\Users\KIMJUNGRYUL\Anaconda3\lib\site-packages\pip\_internal\commands\uninstall.py"", line 91, in run    uninstall_pathset.commit()  File ""C:\Users\KIMJUNGRYUL\Anaconda3\lib\site-packages\pip\_internal\req\req_uninstall.py"", line 456, in commit    self._moved_paths.commit()  File ""C:\Users\KIMJUNGRYUL\Anaconda3\lib\site-packages\pip\_internal\req\req_uninstall.py"", line 296, in commit    save_dir.cleanup()  File ""C:\Users\KIMJUNGRYUL\Anaconda3\lib\site-packages\pip\_internal\utils\temp_dir.py"", line 205, in cleanup    rmtree(ensure_text(self._path))  File ""C:\Users\KIMJUNGRYUL\Anaconda3\lib\site-packages\pip\_vendor\retrying.py"", line 49, in wrapped_f    return Retrying(*dargs, **dkw).call(f, *args, **kw)  File ""C:\Users\KIMJUNGRYUL\Anaconda3\lib\site-packages\pip\_vendor\retrying.py"", line 212, in call    raise attempt.get()  File ""C:\Users\KIMJUNGRYUL\Anaconda3\lib\site-packages\pip\_vendor\retrying.py"", line 247, in get    six.reraise(self.value[0], self.value[1], self.value[2])  File ""C:\Users\KIMJUNGRYUL\Anaconda3\lib\site-packages\pip\_vendor\six.py"", line 703, in reraise    raise value  File ""C:\Users\KIMJUNGRYUL\Anaconda3\lib\site-packages\pip\_vendor\retrying.py"", line 200, in call    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)  File ""C:\Users\KIMJUNGRYUL\Anaconda3\lib\site-packages\pip\_internal\utils\misc.py"", line 130, in rmtree    onerror=rmtree_errorhandler)  File ""C:\Users\KIMJUNGRYUL\Anaconda3\lib\shutil.py"", line 516, in rmtree    return _rmtree_unsafe(path, onerror)  File ""C:\Users\KIMJUNGRYUL\Anaconda3\lib\shutil.py"", line 400, in _rmtree_unsafe    onerror(os.unlink, fullname, sys.exc_info())  File ""C:\Users\KIMJUNGRYUL\Anaconda3\lib\shutil.py"", line 398, in _rmtree_unsafe    os.unlink(fullname)PermissionError: [WinError 5] 액세스가 거부되었습니다: 'c:\\users\\kimjungryul\\anaconda3\\lib\\site-packages\\~v2\\cv2.cp37-win_amd64.pyd'(base) C:\Users\KIMJUNGRYUL>pip uninstall opencv-pythonWARNING: Skipping opencv-python as it is not installed.(base) C:\Users\KIMJUNGRYUL>pip install opencv-pythonCollecting opencv-python  Using cached opencv_python-4.5.1.48-cp37-cp37m-win_amd64.whl (34.9 MB)Requirement already satisfied: numpy>=1.14.5 in c:\users\kimjungryul\anaconda3\lib\site-packages (from opencv-python) (1.19.2)Installing collected packages: opencv-pythonSuccessfully installed opencv-python-4.5.1.48 ​ ​ C:\Users\KIMJUNGRYUL\AppData\Local\Programs\Python\Python37-32\python.exe C:/Users/KIMJUNGRYUL/AppData/Local/Temp/main.py/main.pyHi, PyCharmProcess finished with exit code 0 Process finished with exit code 0​은 잘 뛴다는 소리...​주피터노트북으로 가져와서 한번 뛰어 본다.  import cv2from tracker import *# Create tracker objecttracker = EuclideanDistTracker()cap = cv2.VideoCapture(""highway.mp4"")# Object detection from Stable cameraobject_detector = cv2.createBackgroundSubtractorMOG2(history=100, varThreshold=40)while True:    ret, frame = cap.read()    height, width, _ = frame.shape    # Extract Region of interest    roi = frame[340: 720,500: 800]    # 1. Object Detection    mask = object_detector.apply(roi)    _, mask = cv2.threshold(mask, 254, 255, cv2.THRESH_BINARY)    contours, _ = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)    detections = []    for cnt in contours:        # Calculate area and remove small elements        area = cv2.contourArea(cnt)        if area > 100:            #cv2.drawContours(roi, [cnt], -1, (0, 255, 0), 2)            x, y, w, h = cv2.boundingRect(cnt)            detections.append([x, y, w, h])    # 2. Object Tracking    boxes_ids = tracker.update(detections)    for box_id in boxes_ids:        x, y, w, h, id = box_id        cv2.putText(roi, str(id), (x, y - 15), cv2.FONT_HERSHEY_PLAIN, 2, (255, 0, 0), 2)        cv2.rectangle(roi, (x, y), (x + w, y + h), (0, 255, 0), 3)    cv2.imshow(""roi"", roi)    cv2.imshow(""Frame"", frame)    cv2.imshow(""Mask"", mask)    key = cv2.waitKey(30)    if key == 27:        break 이렇게 궤적도 뽑힌다. ​  생각보다 검지가 잘 안된다​생각해보니 이건 딥러닝이 아니라 배경모델링 같은 거였다 ㅎㅎㅎ​https://pysource.com/object-detection-opencv-deep-learning-video-course/ Object Detection - Opencv & Deep learning | Video Course - PysourceGet your hands on Object Detection as quickly as possible to work on your project, no matter what's your experience.pysource.com ​ "
[AIDE공부] 인공지능 알고리즘 ,https://blog.naver.com/mori0714/223012553675,20230211,"인공지능 알고리즘딥러닝의 표현방식   1) 딥러닝(Deep Learning) : 여러 층을 가진 인공신경망(ANN)을 사용하여 머신러닝 학습을 수행       - 기계가 자동으로 대규모 데이터에서 패턴과 규칙을 학습       - 학습을 기반으로 의사결정이나 예측 등을 수행하는 기술       - 딥러닝의 성능은 학습 데이터의 품질과 영향이 높음       - 인공지능이 학습을 위해서는 많은 양의 데이터가 필요하며, 학습에 필요한 데이터를 전처리하여 제공해야 함​2. 딥러닝의 동작원리   1) 딥러닝의 학습을 위한 데이터는 훈련데이터(Train)와 평가 데이터(Test)로 분류하여 사용. 대략 8:2 비율   2) 에포크(Epochs) : 수 많은 훈련셋을 반복적인 학습을 하는 것   3) 동작원리 요약       - 훈련셋(반복적 에포크) → 시험셋(평가) → 실전(실데이터)로 구성​3. 인공지능 프로그램의 개발 절차   1) 1단계 : 라이브러리 읽어 오기   2) 2단계 : 데이터를 읽어 들이고 전처리 하기   3) 3단계 : 신경망 만들기   4) 4단계 : 모델 만들기(학습하기)   5) 5단계 : 모델 적용하기(예측하기)      ※ 학습 데이터(데이터 전처리)는 2, 4단계      ※ 인공지능 개발(데이서셋 활용)은 3, 5단계​4. 인공지능 객체검출 방법의 이해   1) 사진 속 객체의 수에 따라 single object와 multi object로 인식   2) Single Object = Classification + Localization      - 학습을 통해 1개의 object를 찾는 것을 분류라고 함      - 분류(Classification) : 데이터셋을 학습하여 이를 토대로 새로운 이미지를 식별하게 되는 과정. 학습되지 않은 class는 인식하지 못함      - 영역표시(Localization) : 분류를 통해 검출한 객체의 정보가 있는 위치를 보기 쉽게 box형태로 지정하는 것   3) Multi Object      - 객체검출(Object Detection) : 학습을 통해 여러 개의 객체를 인식하고 인식된 개체를 바운딩박스와 색을 이용하여 영역을 표시      - 의미적 분할(Instalce Segmentation) : 객체인식에서 이미지 내의 의미 있는 단위로 분할하는 작업(포토샵 라인따기와 비슷)​5. 핵심 딥러닝 알고리즘 이해   1) CNN(합성곱신경망,Convolutional Neural Network) : 영상처리에 많이 활용, 합성곱을 사용   2) RNN(순환신경망, Recurrent Neural Network) : 음성처리에 많이 사용, 계층의 출력이 순환구조   3) GAN(생성적 적대 신경망, Generative Adversarial Network) : 이미지생성, 복원, 편집, 변환 등 활용, 신경망끼리 경쟁​​ "
아이폰 맥세이프 무선충전 보조배터리 QUXX(쿽스) ,https://blog.naver.com/realnogun/222995348771,20230126,"내게 스마트폰은 매우 소중한 존재가 되었다.​  블로그 포스팅을 주로 하고, 주문이 들어오는 걸 확인하는 것도 스마트폰으로 함. 그 외에 짧은 동영상 편집이나 업무 메일을 주고받는 것 역시 폰으로 다 하는중이다.​​ 그래서 당연하게도 아이폰의 배터리의 용량이 매일 부족하다. 유선 핸드폰이 아닐까 싶을 정도로, 사용하지 않을 땐 항상 전원 케이블에 연결시켜서 충전중임.​그런고로 이번 시간엔 무선충전 보조배터리인 QUXX(쿽스)를 소개하고자 한다.​  쿽스 보조배터리 특징QUXX는 맥세이프를 기반으로 자석과 무선충전, 그리고 고속으로 배터리 충전이 가능한 제품이다.​케이블이 불필요한 제품이고 거치까지 되는 기능성을 지녀, 일반적인 보조배터리와 확연한 차이를 보여준다.​  ​기본 구성품은 요정도. 가볍고 컴팩트한 사이즈를 지녀 휴대하기 매우 용이하다.​ 크기는 세로 102mm X 가로 66mm X 폭 13mm 정도이고,무게는 154g 이다.​무식하게 몸집만 키운 제품들과는 확실히 다른 사이즈와 무게 덕분에, 가볍게 들고다니기 편한 제품임.​  ​매력적인 디자인내부가 훤히 들여다 보이는 투명한 디자인을 지녀, 일반적인 무선충전 보조배터리보다 매력적인 외관이 특징이다.​​리뉴얼 완료2023년 신규 리뉴얼이 완료된 QUXX 이시다.​ 과충전 보호 기술과 2가지 기기 동시 멀티 충전, FOD 지원, 30cm 충전 케이블, 부착형 링을 제공한다.​ ​배터리의 총 용량은 5000mAh이다. 내가 현재 사용하고 있는 아이폰 14 pro가 3,200mAh 니까, 완충을 한 번 하고도 남는 제품이다.​  최대 20W PD 3.0 고속충전PD 3.0을 지원하는 기기라서, 표준 5W 보다 높은 수준의 전력을 스마트폰에 공급하여 최대 75% 더 빠른 속도와 양방향 충전, 거기에 안전하고 효율적인 기능성을 지녔다.​​ ​ 입출력 스펙입력: PD 18W(5V 3A / 9V 2A)유선출력: PD 20W(5V 3A / 9V 2.22A / 12V 1.5A 지원)무선출력: 5W / 7.5W / 10W​  ​ 무선충전 호환기기삼성15W 고속 차지: S10, S10+, S20, S20+, S20 FE, S21Ultra, S22, S22+, S22 Ultra, 노트10, 노트10+, 노트20, 노트20 울트라, 폴드​11W 고속 차지: Z폴드 2, Z 폴드 3​9W 고속 차지: S9, S9+, S8, S8+, S7, S7엣지, S7 Active, S6엣지+, 노트9, 노트8, 노트 7 FE, 노트5, Z플립 5G, Z플립 3​5W 일반 차지: 갤럭시 S6, S6 엣지, S6 Active​ ​​애플7.5W 고속 차지: iPhone 13, 13 Pro, 13 Max, iPhone 12, 12 pro, 12 Max, 12 Mini, iPhone 11, 11 Pro, 11 Max, iPhone 8, 8+, iPhone X, Xr, Xs, Xs Max, SE 2​엘지15W 고속 차지: V40 ThinQ, V50 ThinQ, G7 ThinQ, G8 ThinQ​10W 고속 차지: V30 ThinQ, V30S ThinQ, V35 ThinQ​5W 일반 차지: G6+  ​가볍지만 강력한 자력아이폰 보다 가벼운 무게를 지녔음에도, 맥세이프로 접지시키면 잘 떨어지지 않는 자력을 보여준다.​  ​맥세이프 기능이 탑재되어 있는 아이폰 12 프로 이후 시리즈 부터 호환이 되고, 그 외 기종은 기본으로 제공되는 링을 활용해 부착할 수 있다.​  무선충전 보조배터리로 추천할 만한 QUXX(쿽스)의 제품 영상은 아래에서 확인해 보자.​  무선충전 보조배터리 ​거치하면서 충전하자바야흐로 동영상의 시대라 나는 항상 무언가를 보고 있다. 무선충전을 하면서 거치대를 활용할 수 있는 제품이라 더욱 매력적.​  ​​성능은 기본 안전은 확실하게과충전 보호 기능으로 장시간 동안 사용시, 출력을 자동으로 최소화하며 수명을 보호한다.​과전압보호로 출력 전압이 불안정할시, 안전한 전압 상태를 유지한다.​   ​높은 온도가 발생할 경우 자동으로 감지하여 작동을 멈추는 과열 보호 기능도 탑재되어 있음.​과전류 발생시 출력을 차단하는 과전류 보호와, 기기와 충전부 사이에 이물질이 있는 경우 이물질 감지로 스마트폰을 보호한다.​​ ​FOD 센서로 더 안전하게스마트 FOD(Foreign Object Detection) 센서를 적용해, 발열과 전력 소비 방지를 할 수 있다.​  ​ 총평한 손에 쏙 들어오는 작은 디자인이라 휴대성이 좋고, 용량이 5000mAh라서 더 믿음이 가는 제품이다.​ ​무엇보다 맥세이프 기능으로 아이폰에 부착해서 사용할 수 있는게 가장 큰 특징이며, 무식하게 사이즈만 커서 무서운 타사 제품들 보다 디자인도 예쁘고 가볍기 때문에 요즘 자주 애용하는 머스트해브 아이템이시다.​  ​​ 얼리버즈일상 속 특별함, 얼리버즈!e-birds.co.kr ​ ​#무선충전보조배터리#맥세이프 #무선충전 #아이폰보조배터리#보조배터리 #보조배터리추천#거치대 #가벼운보조배터리​ "
Salient Object   vs   Camouflage Object ,https://blog.naver.com/lcs5382/222190612442,20201230,Salient Object Detection 영상 속에서 가장 중요하고 집중된 object를 찾아내어 해당 객체의 전체 범위를 segment하는 것이다. 그렇게 영상 속 각 픽셀들에 salient object가 속할 확률을 intensity 값으로 표현한 saliency map을 알아낼 수 있다. 출처: https://cvml.tistory.com/2​​ Camouflage Object Detection 중요하다고 판단되지 않는 주변 배경의 object 또는 숨겨진 object를 찾아내는 방식​ 
Sony Xperia PRO-I with 1.0″ type sensor & phase detection autofocus(영문) ,https://blog.naver.com/1967jk/222552104749,20211029," Sony Xperia PRO-I with 1.0″ type sensor & phase detection autofocus ​​ By Matthew Allard ACS​​ ​Sony has announced the new Xperia PRO-I which features a 1.0″ type sensor, phase detection autofocus, and the ability to record 4K 120p video. The PRO-I was developed for and is being targeted directly at content creators.​ ​The smartphone wars are certainly heating up, especially with more devices being aimed squarely at content creators. We now have some pretty impressive video capabilities in smartphones that allow them to be integrated into certain professional workflows.​​KEY FEATURES​•1.0-type Exmor RS sensor•phase detection autofocus•First Xperia to support Eye AF and Object Tracking during video shooting•4K 120p•BIONZ X image processor•ZEISS® Tessar Optics with T* anti-reflective coating and dual aperture F2.0/F4.0•Corning Gorilla Glass Victus•4K HDR OLED 120Hz refresh rate display•High quality audio features, including Dolby Atmos, 3.5mm audio jack, Full-stage Stereo Speakers, 360 Reality Audio ​​Same sensor as the RX100 VII ​According to Sony, the Xperia PRO-I is the World’s First Smartphone to include a 1.0-type image sensor with phase detection autofocus. Other companies have previously released smartphones with 1″ type sensors, however, none of them are using phase detect autofocus. The 1.0-type Exmor RS sensor is apparently the same one used in the RX100 VII camera, however, it has been optimized for the PRO-I smartphone. Whatever ‘optimized’ refers to I am not sure. The pixel pitch of the sensor is 2.4µm.​ ​The Xperia PRO-I uses a BIONZ X image processor and a front-end LSI, which Sony claims helps to provide clear and noise-free images even when shooting in the dark.​​Phase Detection AF ​315 phase-detection AF points cover 90% of the frame. Sony states that this allows users to continually capture subjects at high speed with high accuracy, even in scenes where focusing is difficult.​The PRO-I is the first Xperia to support Eye AF and object tracking when shooting video.​​Video Features ​Sony claims that the Xperia PRO-I is the world’s first smartphone that is capable of capturing 4K at 120fps.​The Xperia PRO-I includes two new video features for video and filmmaking. For quick, high-quality videos and vlogging, the new Videography Pro feature includes creative settings in a single location. Users can intuitively and accurately adjust settings such as focus, exposure, and white balance.​The Xperia PRO-I also adopts a high-precision Level meter that has been calibrated one-by-one at the factory. By assigning the “Videography Pro” application to the “shortcut key” on the side of the body, you can start shooting video immediately like a dedicated camera. You can also assign other applications or features to the shortcut key. ​ ​For filmmaking, the Cinematography Pro feature pulls from Sony’s professional video camera technology to easily create a cinematic look with eight different color settings inspired by Sony’s VENICE digital cinema camera, including a 21:9 recording ratio.​The new Xperia PRO-I also features the latest Optical SteadyShot with FlawlessEye.​In addition to stereo microphones, the Xperia PRO-I features a built-in monaural microphone next to the main camera. The smartphone also includes Sony’s industry-leading audio separation technology to effectively filter wind noise, both for stereo and monaural microphones.​Sharing video files can be done using a 5G network or by connecting the Xperia PRO-I to a computer by USB 3.2 Gen 2 to transfer data twice as fast from the Xperia PRO-I to a PC.​​Lenses ​The Xperia PRO-I includes three lenses, and all three feature ZEISS T* anti-reflective coating and a 3D iToF sensor. The newly developed 24mm lens utilizes ZEISS Tessar Optics with less peripheral image distortion and more intense contrast and sharpness.​ ​The Xperia PRO-I’s 24mm lens is complemented by two further lens options, 16mm and 50mm. The device’s 3D iToF sensor instantly calculates the distance between the camera and the subject, to help ensure fast, accurate autofocus.​The Xperia PRO-I can capture 12-bit RAW still images at up to 20fps AF/AE burst shooting. There is also an anti-distortion shutter. With Photography Pro on the Xperia PRO-I, users can customize manual settings or easily access automatic settings and shoot in RAW format.​ ​The Xperia PRO-I features a dedicated shutter button that boasts the same shutter switch module as Sony’s RX100 series cameras and requires similar button strokes to operate AF and shutter release. The user can also long-press the shutter button to quickly launch Photography Pro and start shooting immediately, even when the display is off. In this new model, there is also a strap hole.​​6.5” 4K HDR OLED displayThe Xperia PRO-I lets the user edit and share video content directly from the device. It has a wide 6.5” 4K HDR OLED display and accurate, professional-level color reproduction with creator mode. As an added benefit, the color setting of the display can be adjusted according to the user’s monitor or printed photos.​The 4K HDR 10-bit equivalent OLED display also features a 120Hz refresh rate.​​Vlog Monitor ​Along with the phone, Sony has introduced a mobile vlogging kit called Vlog Monitor. This is a new accessory you can pair with the Xperia PRO-I, with an aspect ratio of 16:9 on a 3.5-inch LCD (1280×720) screen and a metal holder. This new accessory also features a magnetic detachable design, making it easy to attach and detach to the holder.​ ​When shooting with the Xperia PRO-I and the Vlog Monitor, the user can connect a Bluetooth shooting grip, such as the GP-VPT2BT which allows you to start/stop recording without touching any buttons on the Xperia PRO-I itself. ​ ​The Vlog Monitor also features a 3.5mm 3-pole microphone jack so users can connect their own external microphone by mounting a microphone on the accessory shoe mount on the top of the holder.​The user can also display the screen when using “Photography Pro” or “Videography Pro” by connecting the Vlog Monitor to a smartphone with the supplied connection cable. In addition, since it is equipped with a USB Type-C port for supplying power from an external power source, users can shoot while supplying power to both the smartphone and accessories.​​Size ​The Xperia PRO-I has a length of approximately 6.5 in (166 mm), a width of approximately 2.8 in (72 mm), and a thickness of approximately 8.9 mm. ​ ​The new Xperia PRO-I has a frosted glass back panel and matt finish. It is water and dust resistant with an IP65/68 rating and features Corning Gorilla Glass Victus which is both drop and scratch resistance.​​BatteryThe smartphone has a 4,500mAh battery and users can fast charge the device up to 50% in just 30 minutes with the included 30W charger XZQ-UC1.​​Internal & External MemoryThe Xperia PRO-I has a large internal memory, with 12GB RAM to handle intensive computing tasks and 512GB ROM offering ample storage for photos, videos, and other files to support micro SDXC media up to 1TB. ​​Pricing & AvailabilityThe new Xperia PRO-I will be available in early December starting from $1,798 USD for the 512GB version.​The new Vlog Monitor will also be available in early December for approximately $198 USD.​​출처: https://www.newsshooter.com/2021/10/26/sony-xperia-pro-i-with-1-0-type-sensor-phase-detection-autofocus/ Sony Xperia PRO-I with 1.0"" type sensor & phase detection autofocus - NewsshooterSony has announced the new Xperia PRO-I which features a 1.0"" type sensor, phase detection autofocus, and the ability to record 4K 120p video.www.newsshooter.com ​ "
YOLOv1 ~ YOLOX  ,https://blog.naver.com/ziippy/222796643913,20220701,"Introduction to YOLO Family YOLO (You Only Look Once) is a single-stage object detector introduced to achieve both goals (i.e., speed and accuracy).And today, we will give an introduction to the YOLO family by covering all the YOLO variants (e.g., YOLOv1, YOLOv2,…, YOLOX, YOLOR).​2010년 까지는 전통적인 컴퓨터 비전 방식을 이용해서 Object Detection 했다면,2012년에 AlexNet 이 나오면서 판도가 바뀌었다. (출처: https://arxiv.org/pdf/1905.05055.pdf)​single-stage detector 인 YOLO 의 진화 그래프 single-stage object detector 와 two-stage object detector 의 차이는? ​YOLOv1 은?In 2016 Joseph Redmon et al. published the first single-stage object detector,You Only Look Once: Unified, Real-Time Object Detection, at the CVPR conference.​YOLO achieved 63.4 mAP (mean average precision), as shown in Table 1 ​YOLOv2 는?Redmon and Farhadi (2017) published the YOLO9000: Better, Faster, Stronger paper at the CVPR conference.The authors proposed two state-of-the-art YOLO variants in this paper: YOLOv2 and YOLO9000; both were identical but differed in training strategy.​YOLOv2 는 Pascal VOC 나 MS COCO 같은 detection dataset 으로 학습YOLO9000 은 9000가지의 서로 다른 카테고리를 위해 MS COCO 나 ImageNet 으로 학습​YOLOv2 에서는 multi-scale training 을 했다. ​YOLOv3 은?Redmon and Farhadi (2018) published the YOLOv3: An Incremental Improvement paper on arXiv.This paper introduced a new network architecture called Darknet-53. The Darknet-53 is a much bigger network than before and is much more accurate and faster.inference time 은 빨라졌으나, mAP 는 그닥이다. ​YOLOv4 는?YOLOv4 is a product of many experiments and studies that combine various small novel techniques that improve the Convolutional Neural Network accuracy and speed.In 2020, Bochkovskiy et al. (the author of a renowned GitHub Repository: Darknet) published the YOLOv4: Optimal Speed and Accuracy of Object Detection paper on arXiv. EfficientDet 보다 2배 이상 빠르고, YOLOv3 보다 mAP 는 10% 증가, FPS 도 12% 올랐다. Convolutional Neural Network (CNN) performance depends greatly on the features we use and combine.Bochkovskiy et al. leverage this idea and assume a few universal features, including- Weighted-Residual-Connections (WRC)- Cross Stage Partial connections (CSP)- Cross mini-Batch Normalization (CmBN)- Self-adversarial training (SAT)- Mish activation- Mosaic data augmentation- DropBlock regularization- CIoU loss​YOLOv5 는?In 2020, after the release of YOLOv4, within just two months of period, Glenn Jocher, the founder and CEO of Ultralytics, released its open-source implementation of YOLOv5 on GitHub. YOLOv5 offers a family of object detection architectures pre-trained on the MS COCO dataset.It was followed by the release of EfficientDet and YOLOv4.  $ import torch$ model = torch.hub.load('ultralytics/yolov5', 'yolov5s')  # or yolov5m, yolov5l$ img = 'https://ultralytics.com/images/zidane.jpg'  # or file, Path, PIL, OpenCV$ results = model(img)$ results.print()  # or .show(), .save() - ​Training on Custom Dataset- ​Multi-GPU Training- Exporting the trained YOLOv5 model on TensorRT, CoreML, ONNX, and TFLite- Pruning the YOLOv5 architecture- ​Deployment with TensorRT ​Mosaic Data Augmentation 은?The idea of mosaic data augmentation was first used in the YOLOv3 PyTorch implementation by Glenn Jocher and is now used in YOLOv5.4개의 이미지를 하나로 합치는 방식​The benefit of using mosaic data augmentations is- The network sees more context information within one image and even outside their normal context.- Allows the model to learn how to identify objects at a smaller scale than usual.- Batch normalization would have a 4x reduction because it will calculate activation statistics for four different images at each layer. This would reduce the need for a large mini-batch size during training. Quantitative BenchmarkIn Table 2, we show the performance (mAP) and speed (FPS) benchmarks of five YOLOv5 variants on the MS COCO validation dataset at 640×640 image resolution on Volta 100 GPU. ​[참고 주소]https://pyimagesearch.com/2022/04/04/introduction-to-the-yolo-family/https://lynnshin.tistory.com/47​ "
Computer Vision with Embeded Machine Learning 강좌 메모 - 3 ,https://blog.naver.com/kjy6135/222846754819,20220812,"Obejct Detection​​Image Classifiaction 의 경우, 객체에 대한 위치 정보를 주지 않는다.Object Localization : 이미지 안에서 객체에 대한 위치 정보를 제공함. 하지만 어떤 클래스인지는 모름Object Detection : 이미지 안에 있는 객체에 대해 위치와 클래스 정보를 제공함.​위치정보를 제공할 때 Bounding Box 방법으로 표시하는 것을 생각해보자.​좌상단에 최초로 일정 크기의 Windows 를 만든 뒤 객체가 있는지 판단하고, stride 값에 따라 +y축으로 +stride 만큼 평행 이동을 한 Window로 다시 판단을 할 것이다.​ㅁ ㅁ ㅁ ㅁㅁ ㅁ ㅁ ㅁㅁ ㅁ ㅁ ㅁㅁ ㅁ ㅁ ㅁ​stride 가 띄어쓰기 한칸 + ㅁ 라면 저렇게 띄어쓰기 한 칸의 공간이 남을 것이다.따라서 stride가 Windows의 길이보다 작다면 overlap이 발생할 것이며, 이는 객체를 더욱 오밀조밀하게 탐색할 수 있을 것이다.​만약 하나의 Window에서 객체인지 아닌지 탐색하는 시간이 0.1s = 100ms 라면, 위 예제에서는 1.6s 가 소요될 것이다. 즉 1.6fps​1.6초에 한번 이미지를 촬영해서 판단한다면 문제가 없겠지만, 우리는 real-time, 실시간으로 객체를 탐지해야 한다. 따라서 stride 를 잘 설정해야 함..​​​Intersection over Union (IoU) Intersection : 같은 클래스로 탐지된 두 객체의 바운딩박스의 겹치는 부분Union : 두 객체 바운딩박스의 합친 면적​=> 0.5 이상일경우 True Positive (TP)이하일 경우  False Positive (FP)안잡혔는데 실제로 있는 경우 False Negative (FN) 사진출처 : https://research.aimultiple.com/machine-learning-accuracy/ Machine Learning Accuracy: True vs. False Positive/NegativeExplore how to measure machine learning accuracy from a business perspective. Get beyond precision, recall, F1 score. Measure the model's business impactresearch.aimultiple.com ​위 표에서 Predicted와 Actual 로 나뉜것으로 보아 예측을 했을때는 predicted positive, 못했을때는 negative이고 Actual은 실제의 경우를 말하는 것 같다.​TP일 경우 실제 둘 다 있는 경우이다. 따라서 IoU를 진행했을때, 레이블링 한 바운딩박스와 예측한 바운딩박스가 0.5 이상이므로 잘 바운딩박스를 그린것이고​FP일 경우 바운딩 박스를 그렸지만, 실제 레이블링한 바운딩 박스 영역에 한참 못미치는 정도로 그린 경우.​FN은 실제로 있는데 포착을 못한 경우이다.​​  만약 3개의 클래스 ball, dog, toy 를 object detection 의 클래스로 삼은 모델이 있다고 하자.​각각에 대한 객체에 레이블링을 다 진행했고 실제 테스트 데이터 셋을 넣어 돌렸을 때 정확도를 보고자 한다.​어떻게 평균 정확도를 측정할까? (mAP = mean Average Precision)​바로 위에 나온 IoU를 이용한다. IoU가 0.5 이상일 때, Recall, Precision의 관계를 0에서 1까지 적분한 값이 바로 해당 클래스에 대한 AP 값이 된다. 3개의 클래스가 있으니 다음과 같은 수식으로 구할 수 있다.​ ​COCO 2017 Challenge 에서는 IoU Threshold 값을 0.5 부터 시작해서 0.05간격으로 0.95 까지 설정하고 각각의 mAP를 모두 더한 값을 사용하기도 했다.​Threshold 값이 커지면 커질수록 미리 레이블링한 바운딩박스 영역과 겹치는 부분이 많아야 하기 때문에mAP는 상대적으로 떨어질 수 밖에 없을 것이다. "
트루엔 공모주 진행 할까요? 말까요?  ,https://blog.naver.com/hhy0818/223096478187,20230509,"안녕하세요. 긴 연휴 보내고 다시 현실 복귀에 문제 없으신가요?​연휴 끝나고 바로 오랫만에 괜찮은 공모주들이 있어서 소소하게 진행 해 보려 합니다. ​그중 눈에 들어온 회사는  트루엔이었습니다. ​이름만 들어서는 무엇을 하는 회사인지 감을 잡기 어려우시죠?​트루엔은  기존 영상감시 장치의 제한적인 영상 분석 처리(Video Analysis)에서 Deep Learning 기반의 영상 처리(AI Analysis)로 차량번호인식(ALPR), 객체탐지(Object Detection), 객체추적(Object Tracking), 지정객체 모자이크 처리(Privacy Masking) 등 다양한 AI기능을 Edge 시스템에서 지원하는 제품을 개발 출시하였으며, 지속적인 AI 연구개발(R&D)로 스마트 시티 구축은 물론 시민의 안전하고 편리한 생활을 위한 서비스를 제공하겠습니다.  트루엔은 영상감시분야와 Edge AI, IoT 융합을 통해 ‘영상감시 플랫폼(Platform) 전문기업’으로 진화(성장)하고 있습니다.​가장 많은 매출을 차지 하고 있는 분야는 영상감시분야인데요... ​지나다니다 이런 모양의 카메라를 보신 기억이 있으실 거에요...  ​이외에도 AI 연구개발을 내세워 회사의 이미지를 올리려 하는 것 같기도 합니다. 좀 더 AI 와 엮이려면 좀 더 지켜봐야할것 같아요. ​하지만 요즘 상황상 AI 에 관련된 회사에 대한 비젼과 기관의 수요예측을 보면 아주 높은 성과는 살짝 접어도 수익 내는데는 문제가 없을 것으로 보입니다. ​매출 성장도 나쁘지 않아 보이구여.. ​ 2022년 감사보고서당기순이익만 보면 잘 크고 있는 회사 인것 같죠?​그렇다면 트루엔의 공모일정을 알아 볼까요? * 확정공모가: 12,000원* 공모청약일: 23년5월8일-9일*환불일: 5월11일*상장일 : 5월17일*청약수수료: 2,000원* 최소청약주수: 10주(60,000원)*주관사: 미래에셋증권사의무확약비중도 8.2%정도로 나쁘지 않습니다.​제가 가장 중요하게 보고 있는 기관의 수요예측 경쟁률입니다. 요즘 사실, 돈 잘버는 회사이든 아니든 기관의 경쟁률에 맞춰 인기가 있고 없고가 갈리더라구여... 기관의 경쟁률이 별로 좋지 않음 상장 폐지도 일어나니까요. ​ ​제 개인적인 생각으로는 공모가도 부담스럽지 않아서 따상 까지는 아니어도 더블만 되어도... 점심값은 버니까... 가족계좌로 해 볼랍니다. ​ 공모가 : 12,000원더블: 24,000원따상: 31,200원 ​​* 모든 투자는 개인적인 판단으로 진행 되어야 하니 참고만 해주세요.  "
객체 인식 (Object recognition) ,https://blog.naver.com/paulcyp/222187882398,20201228,"객체 인식은 이미지 또는 비디오 상의 객체를 식별하는 컴퓨터 비전 기술입니다. 객체 인식은 딥러닝과 머신 러닝 알고리즘을 통해 산출되는 핵심 기술입니다. 사람은 사진 또는 비디오를 볼 때 인물, 물체, 장면 및 시각적 세부 사항을 쉽게 알아챌 수 있습니다. 이 기술의 목표는 이미지에 포함된 사항을 이해하는 수준의 능력과 같이 사람이라면 당연히 할 수 있는 일을 컴퓨터도 할 수 있도록 학습시키는 것입니다. 객체 인식을 사용하여 다른 카테고리의 객체 식별하기. 객체 인식은 무인 자동차에서 활용되는 핵심 기술로, 자동차가 정지 신호를 인식하고 보행자와 가로등을 구별할 수 있도록 합니다. 또한 객체 인식 기술은 바이오이미징에서의 질병 식별, 산업 검사 및 로봇 비전과 같은 여러 분야에서 활용할 수 있습니다.​객체 감지(Object detection)와 객체 인식(Object recognition)은 서로 유사한 객체 식별 기술이지만, 실행 방식은 서로 다릅니다. 객체 감지는 이미지에서 객체의 인스턴스를 찾아내는 프로세스입니다. 딥러닝의 경우 객체 감지는 이미지에서 객체를 식별할 뿐만 아니라 위치까지 파악되는 객체 인식의 서브셋입니다. 이를 통해 하나의 이미지 에서 여러 객체를 식별하고 각 위치를 파악할 수 있습니다. 객체 인식(왼쪽)과 객체 감지(오른쪽). 객체 인식에는 다양한 접근 방식을 사용할 수 있습니다. 최근 머신 러닝과 딥러닝 기술이 객체 인식 문제에 접근하는 방식으로 널리 사용되고 있습니다. 두 기술은 이미지에서 객체를 식별하는 방법을 학습하지만, 실행 방식은 서로 다릅니다.  객체 인식에 사용되는 머신 러닝과 딥러닝 기술. ​​딥러닝을 사용한 객체 인식​딥러닝 기술은 객체 인식에 널리 사용되는 방법이 되었습니다. CNN(컨벌루션 뉴럴 네트워크)과 같은 딥러닝 모델은 객체를 식별하기 위해 해당 객체 고유의 특징을 자동으로 학습하는 데 사용됩니다. 예를 들어 CNN에서는 수천 장의 훈련용 이미지를 분석하고 고양이와 개를 구분하는 특징을 학습하여 고양이와 개의 차이점을 식별하는 방법을 학습할 수 있습니다.딥러닝을 사용하여 객체 인식을 실시하는 두 가지 접근 방식이 있습니다.기초부터 모델 훈련시키기: 기초부터 딥 네트워크를 훈련시키기 위해서는 레이블이 지정된, 매우 방대한 데이터 세트를 모으고, 네트워크 아키텍처를 설계하여 특징을 학습하고 모델을 완성합니다. 이를 통해 뛰어난 결과물을 얻을 수 있지만, 이러한 접근 방식을 위해서는 방대한 분량의 훈련 데이터가 필요하고 CNN에 레이어와 가중치를 설정해야 합니다.사전 훈련된 딥러닝 모델 사용하기: 대다수 딥러닝 응용 프로그램은 사전 훈련된 모델을 세밀하게 조정하는 방법이 포함된 프로세스인 전이 학습 방식을 사용합니다. 이 방식에서는 AlexNet 또는 GoogLeNet과 같은 기존 네트워크를 사용하여 이전에 알려지지 않은 클래스를 포함하는 새로운 데이터를 주입합니다. 이 방법을 사용하면 수천 또는 수백만 장의 이미지로 모델을 미리 훈련한 덕분에 시간 소모가 줄게 되고 결과물을 빠르게 산출할 수 있습니다.딥러닝은 높은 수준의 정밀도를 제공하지만, 정확한 예측을 위해서는 대량의 데이터를 필요로 합니다.​ 식당 음식을 인식하는 데 객체 인식을 사용하는 딥러닝 응용 분야.​머신 러닝을 사용한 객체 인식​머신 러닝 기술도 객체 인식에 널리 사용되고 있으며, 딥러닝과는 다른 접근 방식을 제공합니다. 머신 러닝 기술이 사용되는 일반적인 사례는 아래와 같습니다.SVM 머신 러닝 모델을 사용한 HOG 특징 추출SURF 및 MSER과 같은 특징을 사용한 단어 주머니(Bag-of-Words) 모델얼굴과 상반신을 포함하여 다양한 객체를 인식하는 데 사용할 수 있는 Viola-Jones 알고리즘머신 러닝 워크플로표준 머신 러닝 방식을 사용하여 객체 인식을 수행하려면 이미지(또는 비디오)를 모아 각 이미지에서 주요 특징을 선택합니다. 예를 들어 특징 추출 알고리즘을 사용하면 데이터에서 클래스 간의 구분에 사용할 수 있는 가장자리 또는 코너 특징이 추출됩니다.그런 다음, 이러한 특징을 머신 러닝 모델에 추가하여 각 특징을 고유 카테고리로 나눈 후 새로운 객체를 분석 및 분류할 때 이 정보를 사용합니다.정확한 객체 인식 모델을 만들기 위해 다양한 머신 러닝 알고리즘과 특징 추출 방법을 조합하여 사용할 수 있습니다. 객체 인식에 사용되는 머신 러닝 워크플로.​객체 인식에 머신 러닝을 사용하면 다양한 특징과 분류기를 최적으로 조합하여 학습에 사용할 수 있습니다. 최소의 데이터로도 정확한 결과를 얻을 수 있습니다.​객체 인식을 위한 머신 러닝과딥러닝의 차이점​최적의 객체 인식 접근 방식은 응용 분야와 해결하려는 문제에 따라 다릅니다. 많은 경우, 특히 객체의 클래스를 구분하기 위해 이미지의 어떤 특징을 사용하는 것이 가장 좋은지 알고 있을 때는 머신 러닝이 효과적인 기술일 수 있습니다.머신 러닝과 딥러닝 중 무엇을 선택할지 고민되는 경우에는 고성능 GPU와 대량의 레이블이 지정된 학습용 이미지를 보유하고 있는지를 중점적으로 고려해야 합니다. 이러한 조건이 갖추어지지 않았다면 머신 러닝 방식이 보다 나은 선택일 수 있습니다. 딥러닝 기술은 이미지가 많을 때 더 나은 결과를 보여주는 경향이 있으며, GPU는 모델을 학습시키기 위해 필요한 시간을 줄이는 데 도움이 됩니다. 딥러닝과 머신 러닝 중에서 선택할 때 고려해야 하는 주요 요소.​출처  객체 인식MATLAB을 활용한 컴퓨터 비전에 객체 인식을 사용하는 방법을 알아봅니다. 리소스에는 객체 인식, 컴퓨터 비전, 딥러닝, 머신 러닝 및 기타 주제를 다루는 비디오, 예제 및 문서가 포함됩니다.kr.mathworks.com ​ "
모노즈쿠리 인스트럭터를위한 장비장착 IoT 스마트 모듈 패키징및 실례 세미나 ,https://blog.naver.com/leenameun00/222902231596,20221019,"강사: 한국생산기술연구원 황태진 박사세미나 주관: (재) 한일협력기술협력재단, 2022년 10월 13일.​DX로 가기위한 기법의 하나, 데이터 취득 패키징.프로토타이핑 기술: 센서시스템의준비,통신기술, 앱제작기술,PCB 제작기술,하우징및 키트.최근 중소제조기업의 이세 CEO들은 IoT,DX에 관심이 많음.돈을 버는 것은 Low Tech에서 업종을 하향해서 AI,IoT 최근기술을 접합하는 것이 돈을 번다.데이터 취득 패키징 만드는 것은 어떻게 하면 돈을 안드리고 할것이냐?,최대한 쉬운 방법으로.다시말하면 블럭쌓기를 무한하게 하는 것이다. 어떻게 하면 돈을 안드리고 할것이냐?는 간편자동화(Low Cost Intelligent Automation)의 철학이다.간편자동화는 작업자에게 있어서 간단하고 편리하고 간소하고 지혜가 들어간 구조로, 작업자에게 편리성 있는 스마트화를 실현시키기 위한 것이다. 즉 최소의 비용으로 사람과 설비의 베스트 매칭을 통해 생산성과 품질을 최적화 시킨다.LCIA= Do It Yourself(內製化)+자사의 노하우+현장아이디어+무동력 누구나 간단하게 할 수 있는 스마트화.​E/L 비상추략방지장치를 만드는 회사가 IT회사로 변신한 D사 사례-Object Detection(객체 탐색,물체검출)기술을 활용하여 2톤 키트박스내에 무게를 달아 무게센서를  데이터 연결하여 일정한 무게가 감지되면 부품 자동발주되게 함.(몸무게 재는 저울,로드셀 4개 50kg)-레이저가공기(5억,트럼프)기계 가동율을 알고싶음--기계에 레이져가 가동되면 램프에 불이 들어오는 것을 조도 센서를 부착하여 데이터를 구축함. 목적은 기계 더 구입할 필요유무,제작리드타임등 분석.-고객사(H사) 제품 출하 박스에 제품이 누락되는 것을 알고싶음--제품 포장하기전에 사진을 찍어서  보내는 데 적외선 센서로 자동촬영,시간날짜,QR코드 포함.상기내용을 고객사가 알고 전 협력사에 지시하여 확대 적용 예정이라고 함.데이터 취득 시스템 프로토타이핑 기술:원천기술은 있어야 하지만 부가가치는 적음 10달러, 자동온도조절장치에 경우 100~500달러,제네시스 자동차에 경우 8천만원 부가가치가 크다.그러므로 D사에 경우도 E/L 완제품에 가까워야 비즈니스 부가가치를 높일수 있습니다.​2022년 4분기에는 무엇을 향상하시겠습니까?생산 비용 절감?효율성 향상?고품질 제품?리소스 최적화?직원 만족도 향상? ...모두 해당해요? 이러한 문제는 하프 자동화를 통해 해결할 수 있습니다. 한국공학대학교경기도 시흥시 산기대학로 237 한국공학대학교 ​ "
AS9100 요구사항 중 FOD(Foreign Object Damage) 란? ,https://blog.naver.com/spade3832/221809079877,20200214,"안녕하세요MES consulting/ 컨설턴트 한창민입니다.​이번 포스팅은 AS9100 Rev.D 요구사항 중  FOD에 대한 포스팅입니다.​우주항공 품질관리시스템(QMS)을 제·개정함에 있어 필요한 요건으로 AS9100 Rev.D 요구사항을 확인하시다 보면 FOD(Foreign object Damage)에 관한 항목을 쉽게 찾아볼 수 있습니다. ​기존에 AS9100 인증을 취득한 경험이 있거나 현재 유지 중인 기업이라면 제·개정된 FOD 관련 요구사항을 처음 마주하였을 때 다소 생소하였을 거라 생각합니다. 본 포스트가 참고되셨으면 합니다.​ 그렇다면 AS9100 요구사항에서 FOD에 대한 요구사항이 무엇을 의미하며, 해당 조직에 어떻게 적용해야 할 것인지 알아보도록 하겠습니다.  AS9100 요구사항에서 FOD 관련 항목 확인! MES Consulting먼저 AS9100 요구사항 속 FOD관련 항목을 체크해 보겠습니다.​FOD와 관련된 내용은 AS9100 요구사항 중 8.운영(Operation) 요구사항에서 자주 언급되고 있습니다. (8.1항, 8.5.1항, 8.5.4.b항)​첫 번째 8.1항에서는 제품 및 서비스에 대한 요구사항을 결정할 때 이물질 고려​두 번째 8.5.1항에서는 제품과 서비스에 대한 관리 조건을 결정할 때 이 물질 보호​세 번째 8.5.4 b) 항에서는 조직의 제품을 보존하기 위한 조직의 활동에서 이물질 고려​   출처 : simexperts.com / F.O.D (Foreign Object Damage) ​AS9100 요구사항을 확인하셨다면 위 3가지 항목에 대해서 한번쯤 고민해 보셨을 요구사항입니다.​해당 3가지 요구사항은 모두 8. 운영(Operation) 상에서 확인할 수 있으며, FOD 활동 또한 운영(Operation)에서 이루어지는 활동입니다.​  AS9100 요구사항에서 FOD 란? MES ConsultingFOD는""Foreign Object Detection"", ""Foreign Object Debris"", ""Foreign Object Damage"" 이렇게 세 가지 정의를 가지고 있습니다. (이 일맥상통하는 세 가지 정의는 FOD를 언급할 때 상호교환적으로 사용됩니다.)​​FOD 즉, 이물질에 관한 개념은!!이물질(FOD)로 인한 피해가 인간의 삶에 영향을 미치는 안전 요소가 될 수 있다는 항공산업에서 시작되었습니다. ​FOD 잔해에 대한 항공 정의 중 하나는_ 도구 및 부속품이 원위치 되지 않음으로써 위험을 발생시킬 수 있는 이물질이 되는 것을 의미합니다.​만약 작업자가 하드웨어를 설치하거나 수리하기 위해 좁은 구역으로 들어가 렌치를 놓고 나왔다면, 이것은 항공기에 손상을 입힐 수 있다는 가정을 할 수 있습니다. ​제품에 손상을 줄 수 있는 원치 않은 물체를 관리하기 위한 통제 장치를 마련하는 것에 대한 이와 같은 생각은 모든 우주항공 제조 산업에 통용되고 있습니다.​정리하면,AS9100 요구사항에서 FOD는공정 중 이물질(FOD) 감지여부,  제거 가능 여부 등을 포함하여 피해를 줄 수 있는 모든 이물질을 통제하고 예측, 제거, 방지하는 것에 관한 모든 것을 요구하고 있습니다.​​   출처 : https://duotechservices.com / FOD(Foreign Object Damage) 좀 더 자세히 설명해보겠습니다.​위 사진의 붉은색 화살표를 보시면 아시겠지만 작업자가 작업 후 finset을 제품에 그래도 남겨두고 작업을 마쳐 발생한 사건입니다. ​이런 경우 다른 검사자 또는 후속 공정 작업자가 발견하지 못하고 제품 커버를 덮었다면 자칫 큰 인명사고로 이러질 수 있습니다.​이런 경우를 예방하기 위하여 AS9100에서는 FOD 관리를 중요시하고 있습니다.​   출처 : https://duotechservices.com / FOD(Foreign Object Damage) ​다음 사진은 이물질로 인해 발생한 제품 손상입니다.​해당 사진은 항공기 엔진 부분인 것으로 추정되며, 이물질로 인하여 날개 부분이 파손된 것을 확인할 수 있습니다. 이런 FOD 관련 사건은 검색을 통해 쉽게 확인할 수 있습니다.​​  FOD를 관리하기 위한 방법(예시) MES ConsultingFOD를 관리 방법 중 - 제어 방법에 관한 여러 예시들 가운데 이번 포스팅에서는 일반적인 예시를 정리하였습니다.​​1. 우주항공 제품의 이물질을 방지하기 위한 설계 기능​   해당 사진은 FOD 설계 기능과 무관합니다._MES Consulting ​최초 우주항공 제품을 설계하는 단계에서 이물질 유입을 고려한 설계가 이루어져야 합니다. ​항공기의 운행 중에 사고로 이어질 수 있는 이물질을 사전에 차단하기 위해서는 수많은 리스크적 관점을 고려하여 설계를 해야 이물질을 방지할 수 있습니다.​​​2. 작업자가 공구를 잘못된 위치에 두는 것을 방지하기 위한 공구 관리​   ​작업자가 작업을 마친 뒤 작업에 필요한 공구를 제품과 함께 조립하는 경우!!​이런 경우 작업자는 해당 공구가 제품과 함께 조립되었을 거라는 생각하지 못하고 조립되는 경우 큰 인명 피해가 발생할 수 있어, 이러한 경우를 예방하기 위한 절차 및 활동이 필요합니다.  (4번 예시 참조)​​3. 설치 시 발생하는 볼트를 추적하여 남은 볼트가 없도록 관리   작업자가 작업 후 볼트를 하드웨어 안에 볼트를 남겨두고 작업을 마쳤다면, 해당 볼트는 그때부터 이물질로 간주해야 합니다. ​FOD 관련 사고에서 가장 많이 발생하는 예시가 볼트 및 동전과 같은 작은 이물질이 하드웨어에 남아 있어 발생하는 사건입니다.​이러한 경우를 예방하기 위하여 작업자는 작업에 필요한 정확한 볼트 수량만을 가지고 작업을 수행해야 하며, ​만약 작업 중에 볼트가 부족하거나 남는 경우 해당 작업에 문제가 발생할 것을 즉시 인식하고 조치를 취할 수 있는 절차가 필요합니다.​​​4. 작업에 사용되는 도구 또는 장비를 보여주는 Shadow boards.   출처 : https://www.flexpipeinc.com  /  Shadow board_MES Consulting   출처 : https://www.flexpipeinc.com  /  Shadow board_MES Consulting ​Shadow board는 2번 예시에서 설명드린 작업자의 실수로 작업에 사용한 공구를 하드웨어에 방치하는 경우를 예방하기 위함입니다.​작업자가 작업을 마친 후 정리를 위해서 공구를 Shadow board에 원위치 할 때 공구가 없다면 작업에 문제가 있었다는 것을 즉시 인지할 수 있습니다.​   ​다음 사진은 Tool tethers입니다. ​Shadow board의 경우 작업을 마친 후 공구의 분실을 확인할 수 있지만 Tool teters를 사용하는 경우 공구는 작업자와 함께 연결되어 있어 공구 분실에 대한 리스크를 감소시킬 수 있습니다. ​앞서 참고하신 예시와 같이 이물질은 다양한 상황에서 발생할 수 있으며,발생가능한 여러가지 상황을 사전에 파악하여 FOD 제어를 위한 절차를 마련해야 합니다. ​표준에 따라 이물질을 통제하는 수단으로서 제품이나 서비스에 필요한 것이 무엇인지 고려할 필요가 있습니다.​이것으로 AS9100요구사항에서 FOD에 대한 기본적인 설명을 마치겠습니다.  MES Consulting은 AS9100 컨설팅을 통한 다양한 사례를 바탕으로 기업에 적합한 FOD 관리 방법을 컨설팅하고 있으며, AS9100 심사를 위한 컨설팅을 진행하고 있습니다.​본 포스트는 AS9100 요구사항 항목 중 FOD의 가장 기본적인 내용을 참고하실 수 있도록 작성하였으며 ​AS9100 인증 취득을 위한 심화과정은 컨설팅을 통해서 전달하고 있습니다.​​▼▼기타 AS9100관련 문의사항은 ""블로그보고 전화했다""고 하시면 당담 컨설턴트가 바로 상담해드립니다▼▼   ​  ​ 감사합니다. ​ 본 포스트는 직접 작성한 글로 MES consulting에 저작권이 있음 알려드립니다. ​ ​"
5월 둘째 주(2주차) 공모주 청약 상장 일정 분석 ,https://blog.naver.com/nasuk007/223097039462,20230509,"5월 공모주 및 상장 일정 한눈에 보기5월 둘째 주(2주차) 상장 및 공모주 일정​ 5월 둘째 주(2주차) 공모주 일정트루엔■ 기업개요 : Edge AI, loT 융합 영상 감시 플랫폼 전문 기업 (AI 영상감시 솔루션 전문 기업)■ 사업분야 주식애소리차량번호인식(ALPR), 객체 탐지(Object Detection), 객체 추적(Object Tracking), 지정 객체 모자이크 처리(Privacy Masking) 등 다양한 AI 기능을 Edge 시스템에서 지원하는 제품을 개발​■  공모 개요 종목명 : 투르엔 공모 일정 : 23.05.08 ~ 23.05.09희망 공모가 : 10,000원 ~ 12,000원구주매출 : 없음우리사주 : 없음상장 예정일 :  23.05.17.(수)​■ 수요예측기관투자자 경쟁률 : 1689 : 1의무보유확약 비율 : 8.2%확정공모가 : 12,000원시가총액 : 1,320억 원예상 유통물량 : 34.58 %예상 유통 금액 : 457억 원​■ 청약 정보주관사 : 미래에셋증권계좌개설일 : 청약당일청약수수료 : 2,000원일반 청약한도 : 62,000주 (3억 7,200만 원)최소 청약수량 : 10주 (60,000원) ​■ 분석 👍 기관투자자 경쟁률 : 1689 : 1  👍AI 관련 사업의 유망성  👍 유통 가능 물량 양호              👎 의무보유확약 비율 : 8.2% ​■ (팩트Seok의 솔직리뷰) 엣지 AI 카메라는 AI PC 서버가 필요 없다는 면에서 경쟁력이 있어 보인다. AI 기술 솔루션 기업으로 평가를 한 것인지. 수요예측 결과도 좋다. 확약비율이 높아져서 유통물량 좀 적어지면 더욱 좋겠다. ​​ 씨유박스■ 기업개요 : AI(인공지능) 얼굴인식 전문 기업■ 사업분야AI System AI Solution ​■  공모 개요 종목명 : 씨유박스공모 일정 : 23.05.09 ~ 23.05.10희망 공모가 : 17,200원 ~ 23,200원구주매출 : 없음우리사주 : 없음상장 예정일 :  23.05.19.(금)​■ 수요예측기관투자자 경쟁률 : 86 :1의무보유확약 비율 :  3.7%확정공모가 : 15,000원시가총액 : 1,495억 원예상 유통물량 : 48.6%예상 유통 금액 : 727억 원​■ 청약 정보주관사 : 신한투자증권(80%), SK증권(10%), 신영증권(10%)계좌개설일 : 신한투자증권(청약당일), SK증권(청약당일), 신영증권(5월 2일까지)청약수수료 : 2,000원일반 청약한도 : 신한투자증권(20,000주), SK증권(3,500주), 신영증권(1,200주)최소 청약수량 : 50주 (375,000원) ​■ 분석 👍 AI 관련 사업으로 성장성이 있다. 👍공모가가 밴드 하단보다도 할인             👎 유통 가능 물량이 많다. 👎상장 일정이 겹친다. 👎기존 주주 비율이 높다.  👎기술특계상장기업​■ (팩트Seok의 솔직리뷰)  공모 일정이 겹치고, 유통 가능 물량도 많고, 수요예측도 좋지 않다. 보안검색대 등 AI 얼굴인식 사업은 성장성은 있어 보인다. 공모가가 밴드 하단보다도 더 낮은 15,000원에 형성되었다는 점 때문에 수익 가능성도 있다. ​​ 모니터랩■ 기업개요 : 보안솔루션 기업 (국내 웹 방화벽 시장 1위)■ 사업분야 ​■  공모 개요 종목명 : 모니터랩공모 일정 : 23.05.09 ~ 23.05.10희망 공모가 : 7,500원 ~ 9,800원구주매출 : 없음우리사주 : 15,650주(0.78%)상장 예정일 :  23.05.19.(금)​■ 수요예측기관투자자 경쟁률 : 1715 : 1의무보유확약 비율 : 6.06%확정공모가 : 9,800원시가총액 : 1,215억 원예상 유통물량 : 24.74 %예상 유통 금액 : 300억 원​■ 청약 정보주관사 : 미래에셋증권계좌개설일 : 청약당일청약수수료 : 2,000원일반 청약한도 : 50,000주 (2억 4500만 원)최소 청약수량 : 10주 (49,000원) ​■ 분석 👍 웹 방화벽 국내 시장점유율 1위  👍유통 가능 물량이 작다 👍 기관투자자 경쟁률 1715 : 1              👎 기술 특례상장기업 👎상장 일정 겹침 👎  의무 확약비율 6%​■ (팩트Seok의 솔직리뷰) 국내 1위 기업이면 경쟁력이 있고, 유통 가능 물량이 작은 것도 큰 이점이다. 다만 상장일이 씨유박스와 겹치고, 확약비율이 너무 낮지만 수요예측 결과가 생각보다 좋게 나왔기 때문에 무안할 것으로 보인다. ​​​#5월둘째주공모주 #공모주청약 #투르엔 #씨유박스 #모니터랩 #수요예측결과 #균등청약 #공모주일정 #상장일정 "
Computer Vision with Embeded Machine Learning 강좌 메모 - 1 ,https://blog.naver.com/kjy6135/222846754528,20220812,"​coursera.org 에서 제공하는 머신러닝 강좌를 수강중이다.​image classification 과 object detection을 임베디드 디바이스에서 사용할 수 있게끔 알려주는 강의이다.​배우면서 알게된 점을 메모하도록 하겠다.​​​   image classification (하나의 객체 판별) 과 다르게 object detection (여러 클래스를 두고 판별) 을 현재 상황에서 임베디드 장치에서 진행하려면 적어도 linux os가 올라갈 정도의 인프라는 구축이 되어야 함.​이미지 파일은 3가지 RGB 채널이 있으며, 투명도를 나타내는 데이터는 알아보면 좋겠지만, 이 강의에서는 다루지 않음. ​데이터셋을 모을 때 클래스당 최소 50장의 이미지가 필요한데, 이 이미지는 최대한 같은 밝기, 같은 배경, 비슷한 사이즈 등 비슷한 사진으로 구성이 되어야함. 만약 다른 배경, 다른 밝기의 사진에서 같은 객체를 학습시키고자 한다? 그럼 그 사진 역시 최소 50장의 이미지가 필요함. 따라서 이런식으로 학습을 시키면 천장, 이천장씩 클래스당 데이터가 늘어나게됨.​사람이든 사물이든 아무것도 없는 empty set(배경) 역시 클래스로 만드는 것이 필요함.​BMP 파일이 압축을 하지않아 사용하기 좋지만, PNG 파일로 edge impulse에 업로드 해야함.​​image classification 데이터의 어려운 점 : Light, Occlusion(특징점을 가리는 경우) ​ "
"인하대 배승환 교수, 컴퓨터·전기전자 공학 분야 최고 권위 저널지 논문 게재 ",https://blog.naver.com/inhanuri/223080152317,20230420,"안녕하세요.인하대학교 홍보팀 인하인스타 6기 입니다.  인하대학교 컴퓨터공학과 배승환 교수가 컴퓨터 및 전기전자 공학 분야 최고 권위 SCI(E) 저널지인 IEEE TPAMI에 논문을 게재할 계획이라는 소식입니다. 전기전자 공학 및 인공지능 분야 최고 권위 학술지 논문 게재하기로시스템 구현·실험부터 논문 작성까지 단독 수행 의미인하대학교(총장 조명우)는 컴퓨터공학과 배승환 교수가 컴퓨터 및 전기전자 공학 분야 최고 권위 SCI(E) 저널지인 IEEE TPAMI(IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE)에 논문을 게재할 계획이라고 20일 밝혔다. IEEE TPAMI는 전기전자 공학 분야 상위 0.5% 수준(Impact Factor: 24.3) 저널지로, 인공지능 분야에서도 최고 권위 해외 학술지로 평가받고 있다. 배승환 교수는 ‘객체 감지를 위한 변형 가능 부품 영역 학습 및 형상 집계 트리 표현’(Deformable Part Region Learning and Feature Aggregation Tree Representation for Object Detection)을 주제로 한 논문을 발표한다. 해당 논문은 다양한 환경에서 효과적으로 영상 기반 물체 검출을 하기 위해 객체 외형 모델을 자유자재로 변화시킬 수 있는 부분 모델 분리 및 모델 결합 학습 기술, 물체의 외형 어텐션(Attention) 메커니즘을 고효율로 학습시킬 수 있는 어텐션 트리 네트워크를 제안했다. 기존의 검출 기술은 물체 외형 모델을 하나의 전체 모델(예:사각형 모양)로 나타내고 해당 영역 안에 물체의 형상 정보를 학습해 검출한다. 이와 비교했을 때 배승환 교수가 제안한 기술은 물체 전체 모델을 여러 개의 부품 모델로 분리하고, 분리된 부품 모델을 트리 형태로 재조립해 물체 검출을 할 수 있다. 특히, 물체 검출 챌린지 데이터세트(set)에서 최고 성능 수준을 달성해 해당 기술의 우수성과 상용화 가능성을 입증했다. 배승환 교수의 이번 논문은 논문의 작성, 시스템 구현·실험까지 직접 진행한 것이어서 의미가 크다. 이번 연구 결과는 한국 연구재단의 우수신진연구, 4단계 BK21 사업, 중점 연구소 사업 지원과 정보통신기획평가원의 사람 중심 인공지능 핵심원천기술개발사업, 인공지능융합대학원의 지원으로 만들어졌다. 한편 배승환 교수는 지난해에도 인공지능 분야 최고 권위 학술대회 중 하나인 전미 인공지능 학회(AAAI)에서 단독 저자로 영상 객체 검출 관련 논문을 게재한 바 있다. 배승환 인하대학교 컴퓨터공학과 교수는 “연구팀이 가지고 있는 물체 인식 및 생성 기술을 온디바이스(On-Device) AI 기술과 결합해 실제 산업 제품에 적용할 수 있도록 연구를 이어갈 계획”이라고 말했다. ​ "
TIL_0830_YOLO ,https://blog.naver.com/mm323/222862800666,20220831,"YOLOv4 https://arxiv.org/pdf/2004.10934.pdfYOLOv4 · DarkNet Book (jjeamin.github.io)YOLOv4는 YOLOv3이후에 나온 딥러닝 정확도를 개선하는 다양한 방법들(Weighted-Residual-Connections(WRC), Cross-Stage-Partial-connections(CSP), Cross mini-Batch Normalization (CmBN), Slef-adversarial-training(SAT) and Mish-activation, etc)을 적용해서 YOLO의 성능을 극대화하고자 함ContributionsTo develop a fast and accurate object detection model which can be trained on 1080Ti or 2080Ti GPUTo verify the influence of SOTA Bag-of-Freebies(BOF) and Bag-of-Specails(BOS) methods of object detection during the detector trainingTo modify SOTA moethods and make them more efficient and suitable for single GPU training, including CBN, PAN, SAM, etc.Related workObject detection models: a backbone which is pretrained on ImageNet and a head which is used to predict classes and bounding boxes of object, head part는 one-stage object dector(YOLO, SSD and RetinaNet)와 two-stage object detector(R-CNN계열)이 주로 사용. 최근에는 backbone과 header사이에 서로 다른 stage들로부터 feature map을 수집하는 neck이라고 불리는 layer존재 (Feature Pyramid Network(FPN), Path Aggregation Network(PAN), BiFPN, and NAS-FPN) from 2page of 2004.10934.pdf (arxiv.org)Back of Freebies: inference 속도는 유지하되 training strategy를 바꾸거나 training cost를 증가시켜 정확도를 높이는 방법Data Augmentation: 원본 dataset의 overfit을 막고 적은 dataset의 효과를 극대화하기 위한 방법,Photometric Distortion: brightness, contrast, hue, saturation, noiseGeometric Distortion: random scaling, cropping, flipping, rotatingCutout: 제거한 영역을 0으로 채움Random Erase: 제거한 영역을 random한 값으로 채움MixupCutMixGANSemantic Distribution Bias: dataset에 특정 label이 많거나 하는 경우에 대한 불균형(data imbalance between different classes)을 해결하기 위한 방법Hard Negative Example Mining: Hard negative란 negative를 positive라고 예측하기 쉬은 data. Hard data mining이란 hard negative data를 모아서 원래 data에 추가해서 학습함으로써 False negative 오류에 robust한 model을 만드는 방법이지만, dense prediction architecture에 속하는 one-stage detector에는 적용하기 어려움 -> focal lossFocal Loss: 분류하기 쉬운 sample의 경우 학습에 대한 기여도가 낮기 때문에 비효율적인데, 이러한 문제를 해결하기 위한 새로운 손실 함수가 Focal loss임. 기존 cross entropy에 (1-pi) 라는 factor가 포함되어 있고, 이 factor의 scale은 γ 로 조절가능하므로 쉬운 예제(?)의 경우 손실에 기여도를 낮출 수 있음Crossentropy = -log(pi)Focal Loss = -(1-pi)γ log(pt), γ ≥0Label Smoothing: one-hot hard representation으로 서로 다른 category간의 상관 정도를 표현하기 어려운 문제를 해결하기 위해 dataset labeling 실수 가능성을 포함하는 방법, 고양이 사진이 있는 경우 라벨을 [고양이: 1 | 개: 0] 으로 정답을 라벨링하는 것이 아니라 [고양이: 0.9 | 개: 0.1]로 합니다new labels- one-hot hard labels * (1-label smoothing value) + label smoothing value/ num classesBounding Box RegressionGIOU (Generalized Intersection over Union)CIOU (Complete Intersection over Union)DIOU (Distance Intersection over Union)​ Scaled-YOLOv4 https://arxiv.org/pdf/2011.08036.pdf​ YOLOR https://arxiv.org/pdf/2105.04206v1.pdf "
"[vision, pytorch] vision Transformer 계열 훑어보기 ",https://blog.naver.com/ehdrndd/223085824663,20230426,"NLP에서 시작된 transformer가 vision에서도 기존 CNN계열보다 성능이 좋다. CNN보다 Transformation이 좋은이유는 CNN의 inductive bias가 없기때문이라고 하는데,,​inductive bias는 두가지가 있다.translation invariance : 물체가 이동해도, 같은 결과를 내는 것. 이것은 물체의 위치특성을 무시한다. 반대어는 translation equivariance(TE)인데, CNN자체는 TE하지만, Max pooling, weight sharing(동일한 filter를 sliding window 연산하는 것)등이 TI하게 만든다. TE하면, 적은데이터로도 좋은 성능을 내지만, TI하면, 많은 데이터가 필요하다.locally restricted receptive field : 커널(필터)에 의해 주변값들만 바로보는 현상.​그리고 object detection 분야에서 sota계열에 속하는 SWIN 또한 transformer 기반이다. Transformer 기반은 데이터가 적으면 성능이 낮지만 데이터가 많을수록(특히, 많은데이터로의 pretrained 모델의 유무) 성능이 일반적으로 좋다.​  Image classificationViT : An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale(2020) 이미지를 패치로 쪼갠다.패치를 flatten한다.패치를 특정(작은)차원으로 linear embedding(projection)한다.패치마다 positional embedding을 더한다.패치 시퀀스를 transformer encoder에 넣는다.​ : Training data-efficient image transformers & distillation through attention(2021) : Distillation이라는 기법( teacher network(CNN)의 output으로부터 배우는 student network(Transformer). + labeled data와 teacher의 두가지 목표를 이루도록 설계)CaiT : Going deeper with Image Transformers(2021) : Deeper ViT,DeiT (left) ViT. class embedding이 처음에 들어가면 성능 안좋음.(middle) class를 중간에 넣으니 성능 증가(right) class를 더욱 후반부에 삽입TNT : Transformer in Transformer(2021) : uses a nested transformer architecture to capture global and local image features simultaneouslyResViT : ResViT: Residual Vision Transformers for Multimodal Medical Image Synthesis(2021)  Object detectionDETR : End-to-End Object Detection with Transformers(2020) DETR의 input shape 변화과정.1. imgs(N,C,H,W), mask(N,C,H,W) : mask는 미니배치 이미지크기 차이로부터 생겨나는 padding영역을 지칭하기 위함.  ex. (2, 3, 768, 1151)​2. Backbone(CNN, position embedding)에 imgs,mask가 통과 : imgs → src(N,C',H',W'), mask → mask(N,H',W'), pos(N,C'',H',W'). ex. (2, 3, 768, 1151) → (2, 2048, 24, 36)​3. src가 Conv2(C',C'', kernel_size=1)를 통과 : src(N,C',H',W') → proj_src(N,C'',H',W') ex. (2, 2048, 24, 36) → (2, 256, 24, 36)​4. transformer에 proj_src, mask, query_embed(=nn.Embedding(num_queries=100, C').weight), pos가 들어가게됨.proj_src, pos flatten,permute. : proj_src(N, C'', H', W') → src(H'W', N, C'') ex. (2, 256, 24, 36) → (864,2,256). mask flatten : mask(N,H',W') → src_key_padding_mask(N, H'W') ex. (2, 24, 36) → (2, 864)query_embed(num_queries, C')을 (num_q, N, C')로 변경. query_embed와 size는 같은 0텐서 tgt생성. tgt(num_queries, N, C'') 텐서 생성(향후 decoder에 들어감) ex. (100, 2, 256)6개의 encoder layer를 src, src_key_padding_mask, pos가 통과encoder layer 내부에서 src와pos를 더한값을 k,q로 여기고, v는 src로 하여 self attention에 k,q,v, src_key_padding_mask를 넣음. : q,k,v(H'W', N, C'') → src2(H'W', N, C'') ex. (864,2,256) → (864,2,256). 이후 dropput, add&norm진행후 linear layer 2개(C''→dim_feedforward=2048→C'')를 통과한 결과와 dropput, add&norm를 진행한 최종결과가 encoder layer개수만큼 반복되고, norm을 거쳐 output memory(H'W', N, C'') 반환.5. 6. 7. ​​  Both(also instance segmentation)PVT series(v1,v2) : Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions(2021)CoaT : CoaT: Co-Scale Conv-Attentional Image Transformers(2021)Swin Transformer : Swin Transformer: Hierarchical Vision Transformer using Shifted Windows(2021) "
[OpenCV] Object Traking (0) ,https://blog.naver.com/cho-yani/222116401183,20201015,이 포스트는 아래 사이트의 번역 & 정리본임을 밝힌다.https://www.pyimagesearch.com/2018/07/23/simple-object-tracking-with-opencv/  Object Tracking 과정1. object detection의 초기값(ex) bounding box의 좌표) 가져오기2. 각각의 initial detection에 대해 unique ID 생성하기3. unique ID를 유지하면서 각각의 object를 tracking하기​각각의 object에 대해 unique ID를 부여하기 때문에 object counting도 가능하다.​ Object Tracking algorithm- object detection 단계는 딱 한 번만 필요함 (object가 처음으로 detect되었을 때)- 매우 빠름 (object detector보다 빠름)- traking하는 object가 사라지거나 영상 밖으로 나가는 것도 처리 가능- occlusion에 강함- frame 사이사이에서 놓친 물체 복구 가능  [OpenCV] Object Traking (1) - Centroid Tracking이 포스트는 아래 사이트의 번역 & 정리본임을 밝힌다.https://www.pyimagesearch.com/2018/07/23/simp...blog.naver.com 
트루엔 상장 첫날 공모가 대비하여 60% 상승 출발 ,https://blog.naver.com/dktkahtk1212/223113232181,20230527,"사진을 누르시면 전화 연결 됩니다.오늘의 기업  트루엔 Edge AI, IoT 융합 영상감시 플랫폼(Platform) 전문 기업​트루엔은 영상감시 분야에서 축적된 독보적 기술력, 경험을 바탕으로도시방범, 도로, 공항, 산업, 군사 시설 등 다양한 분야에서 영상 보안 사업을 선도하며,사회 안전과 편의를 높이는데 기여하였습니다.  트루엔은 4차 산업 혁명으로 빠르게 진화하는 사회, 기술, 시대적 요구를 충족하고확장성 있는 다양한 서비스를 구현하고 공급할 수 있는 새로운 제품 개발로세계 영상 보안 산업의 혁신을 선도하고자 합니다.  기존 영상감시 장치의 제한적인 영상 분석 처리(Video Analysis)에서Deep Learning 기반의 영상 처리(AI Analysis)로 차량번호인식(ALPR), 객체 탐지(Object Detection),객체 추적(Object Tracking), 지정 객체 모자이크 처리(Privacy Masking) 등다양한 AI 기능을 Edge 시스템에서 지원하는 제품을 개발 출시하였으며, 지속적인 AI 연구개발(R&D)로스마트 시티 구축은 물론 시민의 안전하고 편리한 생활을 위한 서비스를 제공하겠습니다.  트루엔은 영상감시 분야와 Edge AI, IoT 융합을 통해‘영상감시 플랫폼(Platform) 전문 기업’으로 진화(성장) 하고 있습니다.​ 코스닥 시장에 신규 상장한 기업 트루엔이 첫 거래일 공모가 대비 약 60%에 웃돌면서 상승 출발을 하였습니다.​17일 한국거래소는 이날 오전 9시 16분을 기준하여 트루엔은 현재 시초가[1만 9340원] 대비하여 240원[1.24%] 오른 가격 1만 9580원에 거래되었다고 전하였습니다.​트루엔은 이날의 공모가 보다 61% 웃도는 가격에 시초가를형성한 뒤 2만 원 선을 두고 등락하고 있다고 밝혔습니다.​ 트루엔은 지난 8일 이틀간 일반 투자자를 대상으로 공모주 청약을 실시한 결과 최종 경쟁률 1471.8 대 1을 기록하였으며 증거금으로는 총 5조 5569억 원이 들어와 당시 기준 올해 신규 상장기업 중 최대치를 기록했습니다.​지난 수요예측에서도 희망가 최상단인 1만 2000원에 공모가를 결정하였었습니다.​트루엔은 인공지능(AI) 기술을 기반으로 지능형 영상 감시와 스마트 사물인터넷(IoT) 기기 및 서비스를 제공합니다.​트루엔은 국내 공공 영상 감시 시장점유율 1위를 달리고 있으며 지난해 매출액은 388억 3200만 원, 영업이익은 90억 900만 원으로 최근 8년 연속 성장세를 보이고 있습니다.​최근 상장한 트루엔의 무한한 성장과 발전을 축하드립니다.더욱 발전하는 세계 최강의 기업을 희망하며 오늘의 기업 소식은 마치겠습니다.​장외 주식 정보마당에서 드리는 오늘의 기업 소식을 통하여 발전되는 주식투자 안전한 주식투자하시길 바람 하겠습니다.​ 사진을 누르시면 전화 연결 됩니다.​본 정보는 기업 추천하는 글이 아닙니다.노력하고 발전 가능성 있는 기업의 정보와 소식으로 좀 더 친밀하게 접근할 수 있도록 도움드리는 정보 소식이오니 참고 바라오며무분별한 주식투자 불안전한 주식투자는 위험하오니 올바른 선택하여 좋은 주식을 안전하게 매도 매수할 수 있도록 해주십시오.  "
[졸업작품] 마무리 ,https://blog.naver.com/drunken_thispath/222749843782,20220528,"어제 부로 졸업작품 발표회를 진행하면서 정말 졸업작품이 끝났다.아직 서류 제출이라던가가 조금 남았지만, 그래도 신경쓸 것이 하나 줄어서 좋다.​  기왕 졸업작품 발표회고, 윤이도 학교 온다고 하고, 저녁엔 규태 송별회가 있어서 차려입었다.몸이 더 컸으면 더 이뻐보였을텐데오랜만에 카라티를 입으니까 운동 욕구가 솟아올랐다.​  이전 포스팅에서도 언급하였지만,우리 프로젝트는 라즈베리 파이와 Object Detection, Tracking 기능을 제공하는 Web, App 서비스이다.​ 정말 감사하게도 은상을 수상했다.상장은 다시 인쇄하여 주신다고 하셨다.​부족한 실력이었지만, 훌륭한 조장과 조원들 덕에 좋은 결과를 받을 수 있었다고 생각한다.​  기훈이형과 창재형이 꽃을 사다주었다.내가 사진 많이 찍자고 졸랐다.​​지난 1년을 회고해보자면,아이디어 선정부터 마무리까지 우여곡절도 많았고, 에러도 정말 많았다.하지만 다행히도 팀원들 간에 불화는 없었고, 이는 아마 팀장의 역할이 크지 않았나 싶다.나랑 재현이형, 혜민이는 같은 동아리라 우리끼리만 놀 수 있을 수도 있었는데,팀장이 우리를 잘 이끌어주었기에 이런 결과를 받을 수 있지 않았나 싶다.​동상만 받아도 고마울 것 같다는 생각을 했는데,정말 감사하게도 은상까지 받게 되었다.더불어 다른 팀들도 퀄리티가 매우 높았다.​이 졸업작품 발표회로 인해 나의 열정에 불을 지필 수 있길🔥​졸업작품을 끝마치는 데 도움을 준 모든 스택 오버플로우, 깃허브, 블로그 등에게 감사를,수상을 축하해준 사람들에게 사랑을, 꽃을 선물해준 기훈이형과 창재형은 special thanks,1년간 고생한 비범한 Be Bomb 팀원들에게는 박수👏를 드립니다! "
칩스앤미디어 간략 정리  ,https://blog.naver.com/inee1104/223057180143,20230327,"올해 매출액 + 20%(YoY) 성장과 OPM 30%의 기대하고 있습니다. 회사에 대해 알아보니 앞으로도 이런 숫자를 계속 보여줄수 있을 듯 싶습니다.  ​영상 처리 특화된 IP를 칩리스 업체나 빅테크 업체에 판매합니다. 웹상에서 영상 데이터가 더 많아지고 자율주행 관련한 영상 데이터 처리의 필요성이 커짐에 따라  앞으로 미래가 더 유망해 보입니다. ​모범생 느낌의 회사입니다. 주가가 저렴할때 매수해서 꾸준히 들고 가면 좋을 듯 싶습니다. :)   첨부파일칩스앤미디어 IR BOOK-1.pdf파일 다운로드 다양한 디바이스 안에는 Application precessor가 있으며 , AP 안에는 CPU, Graphic 그리고 동사가 설계하는 IP인 video codec, image processing, computer vision도 들어있음. 현재는 NPU도 개발하고 있음. ​​웹상의 80%가 영상 데이터. 영상은 양도 커지고 화질이 좋아지면서 데이터 센터에서 처리해야 할 데이터가 영상이 주가 될 것임. 현재 Chat GPT는 텍스트 형식이지만 차후에는 영상 형식이 될거라 예상함. 데이터센터에서 영상처리 중요성이 커질 것.   ​​NPU는 CPU, GPU 다음의 데이터를 처리하는 프로세스. NPU는 인공지능을 처리하는 데 최적화 되어 있음. 동사는 video IP를 하드웨어로 잘 구현하는 기술력으로 하드웨어 NPU IP를 개발하고 있음. 기존의 Edge용 NPU에 비해 10-20%정도의 사이즈로 만들려고 개발 중. 영상 처리에 특화된 NPU.  ​Noise reduction ( 카메라 센서에 들어온 영상에는 기본적으로 노이즈가 있음. 노이즈 없애주는 알고리즘 ) Super resolution ( 고화질로 upscaling해주는 알고리즘. ) Object detection ( 자율주행할때 사람, 오토바이 등을 인식해주는 알고리즘 ) 이 알고리즘을 같이 공급하거나 고객이 알고리즘이 있을 경우 NPU만 공급할 예정. 모듈화 해서 공급 가능해짐.  현재는 연구개발 중. 내년에 프로모션. 빠르면 내년에 라이센스 매출 기대 가능할 듯.  라이센스 했던 IP들이 로얄티로 이어지는 경향이 높아지고 있음. ​고객사가 다양해짐. 기존 칩리스 업체뿐만 아니라 유명한 빅테크 기업들도 자체칩을 생산하기 시작하면서 고객사 늘어남. 19,20년부터 자체 칩 니즈에 따라 팹리스 이외의 고객사가 계속 늘어나고 있음.   ​작년 기준으로 차량용 매출 비중이 가장 컸음.  작년부터 자율주행향 프로젝트 2개 시작했음. 자율주행이 본격적으로 개화되면 카메라와 칩이 더 많이 사용 될 것.  가정용과 산업용이 23-30% 자지했음. ​21년부터 모바일향 신규 라이센스 시작했음. 향후 로얄티 기대. ​​​​​작년 로얄티 130억원, 라이센스 100억원. 올해는 라이센스가 더 높을 듯. 새로운 고객사가 늘어났다는 뜻.  작년에 계약 후 인식 하지 못한 라이센스 매출이 80-90억원. ​기존 고객사인 NXP해 말이나 내년에 새로운 라이센스 계약 할듯. ​오포 자회사 zeku에 21년도 모바일향 라이센스 했음. 올해 신규 라이센스 이야기 나누고 있음. 현재  BBK 그룹의 4개의 브랜드 (오포, 비보, 원플러스, 리얼 미)는 삼성이나 퀄컴의 미디어텍 칩을 사용하고 있지만우리가 라이센스 아웃한 zeku가 칩을 잘 만들어서 대체할 가능성 있음. BBK 그룹의 글로벌 MS가 20%가 넘음. zeku 칩이 성공할 경우 우리에게 로얄티를 많이 줄수 있음. 빠르면 올해 말. 올해는 초기 물량. 내년부터 본격적 매출 기대.  미국 G사(안드로이드)에도 라이센스 아웃.  올해 신규 라이센스 아웃 및 로얄티 발생 기대.  AI, GPU Solution 칩 관련해서 중국에서 이미 하거나 이야기가 많이 나오고 있음. ​전방산업이 커지고 있음. 예전에는 영상을 사용하는 기기가 모바일과 티비밖에 없었으나 서버나 카메라도 늘어나고 요즘은 검색도 영상으로 함.  ​​Q. 자율주행 고객사와 매출 시기는? A. 작년 자율주행 프로젝트 2개.      중국의 AI 유니콘 기업과 1개 가져감. 폭스바겐과 칩 개발하고 있는 회사.      국내 ASIC 업체가 자율주행 레벨 4 라이센스 가져감. 현대차와 42닷이 공동개발하고 있는 회사.     라이센스 아웃 후 로얄티을 받을려면 5년 정도 걸림. ​Q. 텔레칩스 통해서 모비스향으로 전장 IP 납품은 계속 하고 있나? A.  ㅇㅇ. 앞으로도 계속 협업 할듯....  Q. 현금이 계속 쌓이는 구조이다. M&A는? A. 여러가지 검토 중.   Q.  라이센스와 로얄티 구조는? A. 칩리스 회사에 IP를 판매하면 라이센스로 바로 매출 인식.      칩리스 회사가 우리 IP을 포함해서 여러 IP를 모아서 칩 설계.      파운드리에 맡겨 양산 후 판매하게 되면 판매에 따라 로얄티 발생함.  Q.  로얄티는 칩당 얼마? A. 보통 퍙균적으로 칩당 1센트 정도. 분기마다 정산. 칩 가격의 비중은 1% 정도.  ​Q.  사업군마다 라이센스 아웃 후 로얄티가 발생하는 기간은? A.  전장은 5년, 다른 사업군은 2-3년.       전장은 로얄티가 발생하는 시간은 오래 걸리지만 칩을 더 오래 사용하기 때문에 로얄티 매출 기간이 김.  ​Q. 로얄티 라이프 사이클은?  A. 제품의 사용 주기와 같음. 자동차는 5년 이상. 모바일은 2-3년 정도 사용하는 기간 만큼 로얄티 발생. ​Q. 경쟁사는? A. 진입장벽이 높음. 각 IP마다 검증된 제품만 사용할려고 함.     새로운 회사한테서 라이센스를 받을려고 하지 않음.      프랑스 Allegro는 우리보다 저급의 IP 생산.  대형업체 상대하기 어려움 잇음.     중국 Verisilicon은 다양한 IP확보. 턴키 서비스 가능. 우리보다 규모가 큼.     우리는 기술적으로 가장 앞서 있음.       미국에서는 중국 기술 꺼려함. 재작년부터 미국 고객들이 우리 제품 라이센스 아웃함.     중국에서는 미국으로부터 반도체 독림을 위해 반도체 투자 붐 일고 있음. 팹리스 많음.     우리의 고객이 될 만한 잠재적 고객이 늘어나고 있음.     현재 Verisilicon과 양분할 듯. ​Q. 23년, 24년 기대되는 부분? A. 작년부터 AI & 데이터센터 칩 프로젝트가 계속 이어지고 있음.      올해 말이나 내년부터 모바일 칩은 10년만에 매출이 일어날 듯.       미국 대형 플랫폼 업체의 AR/VR향 영업하고 있음.      G사와 Q사에서 관심을 가지고 있음. 조만간 계약을 할수 있을 듯.   ​ "
"디지털트윈 기술 기반 통합관제플랫폼, 엠폴시스템 IMP ",https://blog.naver.com/powerkorea0505/223104664386,20230517,"​ 디지털트윈 기술 기반 통합관제플랫폼, IMP국내 최초 보안 및 안전 분야 통합관리솔루션 제공디지털 트윈 기술 기반 통합관제플랫폼실제와 3D가상 환경에서 실시간 관제, AI 객체탐지 기술 뿐만 아니라 라이다, 다양한 센싱기술을 기반한 사고 및 재난-재해 관리 전문 솔루션빠르게 진보하는 기술과 4차산업혁명이라는 시대적 변화 속에 ‘보안과 안전’의 영역이 산업 전반에 걸쳐 넓은 범위로 빠르게 영향력을 높여가고 있다. 보안과 안전은 바라보는 관점에 따라 다양한 의미로 해석될 수 있으나, 상호간에 굉장히 밀접한 관계를 가지고 있다. 보안이라는 단어 그 자체를 단순히 풀이하면 ‘보안은 안전을 지키는 것’이고, ‘보안활동의 결과로 안전한 상태가 된다’는 의미를 가지고 있다. 즉 안전한 상태를 지키는 것이 보안의 시작이고, 결국보안과 안보는 뗄래야 뗄 수 없는 불가분의 관계라고 볼 수 있다.최근에는 ‘초연결’, ‘초지능’, ‘초융합’으로 대표되는 4차산업혁명 시대를 맞아서 진보적 정보통신기술과 전통적 통제수단 전체를 포괄하는 물리보안 통합관제 플랫폼 ‘PSIM(Physical Security Information Management)’에 대한 실질적인 적용 방안이 적극적으로 모색되고 있다. 이런 가운데 디지털트윈과 라이다 기술을 기반으로 물리적 보안은 물론 안전에 대한 시스템까지 안정적으로 강화할 수 있는 통합관제플랫폼을 개발하여 많은 주목을 받는 국내 중소기업이 있다. 본지에서는 그 주인공인 ‘㈜엠폴시스템(대표 송승찬)’을 찾아 집중취재를 진행하였다.   디지털 트윈 기술디지털트윈이란 현실 세계를 3D 모델로 가상화하고, 가상의 현실에 현실 속 다양한 데이터를 연계 및 시각화하여 실시간 자동관제 및 시뮬레이션 기분 분석, 예측, 최적화를 구현하는 융합기술 서비스다. 디지털 대상을 시공간으로 동기화하고, 다양한 목적에 따라 상황을 분석하고, 모의 결과를 기반으로 예측해 물리적인 대상을 최적화하기 위한 융합 기술이다.​디지털 트윈으로 현실 세계를 디지털 세계로 복제·재현하여 복잡한 문제를 해결하고 새로운 산업·서비스 생태계를 창출하는 기술융합 플랫폼화 할 수 있다. 이 플랫폼에는 현실 세계를 3D 모델로 가상화하고 다양한 데이터를 연계·시각화하여 실시간 자동관제 및 시뮬레이션 기반 분석·예측·최적화를 구현하는 융합기술이 포함되어 있다고 한다. 디지털 트윈을 통해 현실 세계의 문제해결을 위한 시간, 비용, 공간, 정보의 한계를 극복 가능케 하는 미래 지향적인 새로운 비즈니스 모델을 창출할 수 있다고 한다.  디지털 트윈 기술 기반의 통합관제플랫폼현재, 보안관제는 물리보안 공간과 사이버보안 공간의 개별적인 운영으로 인한 보안 사각지대발생(물리보안 또는 정보보안 특정의 문제로만 단정짓는 현상 또는 어느 한가지로만 해결)으로 침해사고 탐지를 하지 못하는 경우가 발생되거나 경보가 없으면 수집된 보안이벤트는 활용 소멸 된다. 논리적인 공간 또는 2D 전자지도 기반의 기본 보안관제 인터페이스 제공으로 직관성이 낮다고 한다.하지만, ㈜엠폴시스템은 디지털 트윈 기술을 보안관제에 접목시켜 시·공간의 제약 없이 가상의 디지털 트윈과 현실 세계 간 상호 피드백을 통한 동반 최적화를 실현하고 있다. ​2012년부터 현재까지 융합보안 산업에서 보안솔루션과 서비스를 전문으로 사업을 영위하고 있는 ‘㈜엠폴시스템(대표 송승찬)’은 실감콘텐츠, 네트워크 장비, 웨어러블 디바이스, XR, 스마트시티·팩토리, 지능형 CCTV, 센싱 기술 등을 응용해 최적화된 디지털 관제환경과 플랫폼을 제공하는 디지털 트윈 기반의 융합보안 솔루션 전문 업체로 국내 PSIM(Physical Security Information Management) 산업을 선도하는 기업이다. 다양한 산업에서 고객을 형성하고 다양한 기술접목 및 응용서비스를 통해 기술적으로 검증된 통합관제플랫폼을 양산화하고 있고, 3년 전부터 디지털 트윈 기술을 플랫폼에 적용하기 시작했다.  ㈜엠폴시스템의 통합관제플랫폼 ‘IMP’, 산업현장의 안전과 물리보안을 통합한 관제 솔루션그렇게 개발된 ㈜엠폴시스템의 통합관제플랫폼 ‘IMP(Integrated Managment Platgorm)’는 산업 현장의 안전과 물리보안을 통합해서 관리하는 관제 솔루션 플랫폼이다. 디지털트윈 기반으로 개발했기에 현장과 동일한 실사 영상 송출 기능을 시스템에 탑재했다. 발전소, 산업 현장, 공공시설 및 기타 시설에서 다양한 보안 안전 솔루션(CCTV, 출입통제, 화재감시, IOT센서 등)을 통합하는 플랫폼으로 사용되고 있으며, 이는 SOP(standard Operation Process)에 따른 Rule 기반의 Work Flow 엔진기반 기술을 이용한 디지털 트윈 통합관제 서비스로 활용되고 있다. ​㈜엠폴시스템의 통합관제플랫폼은 세 가지 목적성을 가지고 솔루션화를 추구한다. 첫째, 통합화로 기존 인프라(하드웨어, 소프트웨어)를 활용하여 하나의 시스템으로 통합을 하여 효율적인 관리-관제-제어가 가능하게 한다. CCTV, 화재감지, 출입통제, 온습도관리센터, 일산화탄소 및 이산화탄소 측량 센서 등을 실시간으로 탐지하게 제어할 수 있는 통합솔루션으로 안전뿐 아니라 전력제어 조명제어 등의 기능까지 모두 통합할 수 있다. ​둘째, 호환성-연결성-응용성으로 하드웨어, 소프트웨어 간 상호 연결성을 제공하기 때문에 호환성으로 보다 쉽게 시스템 통합화를 추구할 수 있고, 기능적 관제를 위해 센서, 디지털 트윈과 같은 기술이 내재화된다. 셋째, 디지털 트윈 기술로 특정 관제, 제어, 관리를 목적으로 객체를 똑같이 3D가상화하여 해당 객체에 대한 시뮬레이션, 배치, 개선화, 문제점 발굴, 예방-예측 등으로 관제가 가능하다.  가상의 트윈 공간을 활용함으로써 신제품 개발 기간의 단축 효과 예측 불가능한 사고에 대한 예방과 피해 최소화, 현실 기반의 가상현실에서 순찰 가능이러한 통합관제플랫폼은 현실 세계의 제품이나 기기, 서비스·문제 발생 예측 대응이 가능하며, 가상의 트윈 공간을 활용함으로써 신제품 개발 기간 단축에도 활용된다. 또한, 육안으로 확인이 불가하거나 위험성이 높은 작업을 실시간 모니터링 및 제어가 가능하며 사전 예측 불가능한 사고에 대한 예방 및 피해 최소화 가능하다 했다. 여기에 사용되는 것이 디지털 트윈 기술을 이용한 시뮬레이션이다. 디지털 트윈 기술을 활용해 IMP가 설치되는 건물의 모든 정보를 입력하는데, 외부 인물의 침입을 막는 펜스 및 벽 등의 방어 시설은 물론이고 내부에 설치된 소방 및 각종 배관 배선까지 그대로 입력되어 가상공간에 실제 공간과 같은 똑같은 공간을 만든다. 또한 만일 현장에서 보안과 안전에 관련된 어떤 이벤트가 발생하면, 그와 관련된 정보를 IMP가 실시간으로 출력하고 관제실로 알림을 보낸다. ​컴퓨터나 웹 환경을 통해서 가상공간에서 CCTV 등의 카메라 정보와 영상 정보를 확인할 수도 있다. 현장에 직접 가지 않더라도 디지털 트윈으로 구현된 현실 기반의 가상의 현실에서 공간 내부를 순찰하고, 각종 배선 및 안전 장비를 점검할 수 있으며, 이를 자동으로 설정해놓으면 솔루션이 스스로 순찰하는 기능까지 지원한다. 가상의 현실 속에서 발전기, 온도, 계측기 등의 다양한 정보를 실시간으로 확인할 수 있으며, 모든 정보는 실시간으로 바로 바로 업데이트 되기 때문에 문제가 발생하면 즉각적으로 대처할 수 있다. ​이 기능은 보안과 안전 분야 양쪽으로 모두 활용할 수 있으며, 실시간으로 대처가 가능해 이벤트 발생 시 빠르게 대응할 수 있다.   실시간으로 발생하는 위급 상황과 대응 체계 등을 복합적으로 관제, 관리, 제어㈜엠폴시스템의 통합관제플랫폼 적용분야는 국민 생활과 밀접한 도시와 안전, 국방, 에너지, 농축수산, 제조 산업 등이며, 다양한 응용서비스에서의 데이터 분석과 예측, 시뮬레이션을 통해 현실 세계의 다양하고 복잡한 문제에 대한 최적의 솔루션을 제공하는 것을 최종 목표로 하고 있다. 특히, 온디멘드(On-demand) 관점에서 ㈜엠폴시스템의 플랫폼은 다양한 고객사의 니즈에 맞게끔 각종 기능을 커스트마이징 할 수 있다. 서로 다른 기술 및 산업간 유기적 연계의 어려움을 디지털 트윈 기술 기반 통합관제플랫폼을 구성하여 원활한 융·복합 생태계 조성하고 이를 통해 다양한 산업에서 활용할 수 있는 솔루션으로 맞춤형 서비스를 제공하고 있다. 화재수신반 연동의 통합관제로 화재 발생 시 신속한 알림 및 정확한 대응이 가능한 소방 설비 연동솔루션, 화재 발생 시 감지 센서에 도달하기 전에 이상고온을 미리 감지해 경호하는 불꽃 감지 센서 내장형 카메라, CCTV 카메라에 입력되는 실시간 영상으로 광범위한 영역의 화재를 조기 감지하는 영상분석 기반 화재 연기감시 서비스 등 다양한 상황에 대처할 수 있는 솔루션이 가능하다. 이러한 기능을 통해 다양한 분야에서 실시간으로 발생하는 여러 가지 위급 상황과 대응 체계 등을 복합적으로 관제, 관리, 제어할 수 있도록 돕는다.   효율적이고 효과적인 라이다 기반, 고정형 구축 방식의 외각 방호 시스템국내 최초 보안과 안전 분야 양쪽을 모두 통합해서 관리할 수 있는 통합관제플랫폼㈜엠폴시스템의 통합관제플랫폼 솔루션이 지닌 세 가지 목적성 중, 가장 핵심적인 요인은 호환성-연결성-응용성이다. 여기에는 이종 도메인의 디지털 트윈 간에 상호 연계된 복합 문제해결을 위해 목적 지향 협업 역할의 객체들이 연합적 연동을 하는 기술을 표준화하는 목적도 있지만, 다양한 응용기술을 세밀하게 접목시키는 목적도 있다.​특히, 센싱기술 중 라이다(LiDar)를 활용하여 공간을 분리 감시와 차별된 기술 기반으로 높은 효과와 효율의 외곽 방호 시스템 구축할 수 있다. 라이다 기술은 최근 자율주행에 분야에 가장 먼저 적용되며, 비약적으로 발전을 해왔다. 라이다 기술을 활용하면 물체까지의 거리 측정뿐 아니라 움직이는 물체의 속도, 방향, 크기 등 다양한 정보를 오차 없이 분석이 가능하다. ㈜엠폴시스템은 3D라이다 기반 외곽 침입 감지 시스템은 고정형 구축 방식을 적용했다. 이를 통해 △공간 인식 △객체 인식 및 추적 △3D공간 지오펜싱 △현장 환경 학습을 통한 맞춤형 기술에 특화된 시스템을 구축하였으며, 기존의 다른 기술에 비해 오차가 매우 적어 보안 시스템에 가장 적합한 솔루션이라고 평가받고 있다. 라이다의 응용으로 보안관제범위 내 다양한 객체(진입로 및 주변에서 움직이닌 인적-비인적 물체)의 1차적인 침범-침입이 되는 구간을 단계별로 정밀하게 탐지하고 통합관제플랫폼과 연동하여 빠른 상황대응과 조치가 이루어질 수 있도록 한다고 한다. 디지털트윈과 라이다 기술을 기반으로 개발된 IMP의 가장 큰 장점은 보안과 안전 분야 양쪽을 모두 통합해서 관리할 수 있다는 점이다. 보안과 안전에 관해 통합해서 관리할 수 있는 통합관제시스템 플랫폼은 국내 최초이며, 이는 ㈜엠폴시스템만이 갖춘 최고의 경쟁력이다. 라이더를 활용한 ㈜엠폴시스템의 보안시스템은 국내 최초로 실제 현장에 도입했으며, 현재 한국 서부 발전소를 포함한 여러 발전소와 LG전자, 군부대 등 여러 보안시설 및 관공서, 기업 등에서 적극적으로 활용되고 있다.  AI 객체탐지 기술을 적용해 CCTV와 연동, 인적 개입의 필요 없이 24시간 보안 관리실화상에 열화상을 오버레이한 듀얼 열화상카메라, 사물의 식별력 대폭 높여또한, ㈜엠폴시스템의 플랫폼은 AI 객체탐지 기술을 적용하여 관제 범위 내에 설치되어 있는 CCTV와 연동하고, 영상을 통해 보여지는 객체에 대하여 다양한 사건-사고, 재난-재해 등을 인적 개입 필요없이 24시간 보안을 관리할 수 있다. AI 객체탐지의 대상은 주로 화재-연기, 사람의 쓰러짐, 이상행동, 일정 진입로의 인적-비인적 객체에 대한 출입관리 등이다. AI 객체탐지의 오탐과 미탐을 최소화하기 위해 화재-연기 센서, 열화상카메라, 기타 센싱기술 등을 활용하여 발생되는 이벤트 관리의 효과를 극대화한 것도 큰 장점이다.​열화상 카메라의 경우, 보안에 사용되고 있는 기존의 열화상 카메라는 단순한 열 감지로만 사물을 표현하기 때문에 사물에 식별이 어렵고, 실제로 외부인의 무단진입을 인지하더라도 그 대상이 누군지 정확히 확인할 수 없다는 단점이 있었다. ㈜엠폴시스템은 이러한 단점을 해결하기 위해 실화상에 열화상을 오버레이한 듀얼 열화상카메라 적용하여 사물에 식별력을 높였다. 또한 온도 포인트를 6분할하였기 때문에 구역별 온도 이벤트 영역을 분리 할 수 있어 산업 현장에서의 사용도 적합하게 적용 하였다. ㈜엠폴시스템 송승찬 대표는 “만일 외부인의 침입 혹은 화재 등의 각종 이벤트가 발생하면, 이벤트 발생 프로세서 Rule을 상황별 마이그레션 적용이 가능하여 상황 전파, 방소, 알림 메시지를 발송함과 함께 CCTV 영상을 실시간으로 송출하기 때문에 관리자가 빠르게 대처할 수 있고, 화재가 발생하면 정확한 위치와 탈출 가능한 최적의 동선을 안내하여 빠르게 대피할 수 있도록 돕는다. 또한 IMP에 탑재된 라이다는 365도 모든 공간을 감지하기에 영역을 임의대로 나눌 수 있어 최적화된 보안 시스템이라는 평가를 받고 있다. 프로세스에 맞춰 일정 구역에 접근하는 침입자에게 사전 경고방송으로 침입을 사전에 예방하고, 경계 구역 내에 침입 시에는 관제실로 알림 전파를 하는 프로세스를 제공하고 있다. 관리자가 알림을 접수하지 못하는 경우 혹은 더욱 빠른 대처를 위해 SOP(STANDARD OPERATING PROCEDURE)가 119나 112에 자동으로 신고를 접수하는 프로토콜을 작동시킬 수도 있다”고 설명했다.   특화된 솔루션의 확장성과 호환-응용성을 지닌 엠폴시스템의 사업 방향성㈜엠폴시스템의 관계자는 “당사의 솔루션은 다양한 산업에 적용될 수 있는 만큼, 다양한 형태로 구성되는 솔루션으로 확장성과 호환-응용성을 지니고 있다. 이것이 ㈜엠폴시스템의 특화된 기술적 장점이라 자부한다”고 언급했다. IMP가 지니고 있는 다양한 기능은 많은 산업 현장에서 적극적으로 활용될 수 있다. 안전과 물리적 1차 외곽방어시스템은 물론이고 CCTV관리, 스피드게이트와 엑스레이를 활용한 출입관리시스템에도 적용이 가능하다. 또한 디지털트윈 기반 IMP는 모든 상황을 3D 화면 내에서 식별할 수 있도록 동작 모션 및 색상으로 건물 내에서 얼마나 전력을 사용하고 있고, 어느 곳에서 전력을 사용하고 있는지 조명이 켜져 있는지의 여부 등도 디지털 트윈 기반의 맵을 통해 육안으로 바로 확인 제어가 가능하기 때문에 FMS나 BMS 등에도 적용이 가능한 모두 통합 솔루션이라고 볼 수 있다. ​특히, 에너지 플랜트 산업 및 다양한 형태의 발전소에 특화된 통합관제솔루션으로 사업화 서비스를 이루고 있는데, 에너지 플랜트 산업 및 다양한 발전소 등에 특화된 관제솔루션은 디지털 트윈 기반으로 3D모델링을 통한 가상환경 조성, 가상환경에서의 예측 SOP 발현, 시뮬레이션, 응용기술 기반 시설-설비 관리/관제 등이 복합적으로 수행될 수 있는 최적의 솔루션을 제공하고 있다. 송승찬 대표는 “차별화된 다양한 기능을 유연하게 연동화하여 하나의 플랫폼 안에서 모두 관리하고 제어할 수 있다. 어떠한 사고가 발생하더라도 즉각적으로 대응할 수 있게 구성했다. 위급상황 발생 시 빠르게 판단하고 대처할 수 있으며, 최소 인원의 관리자로도 플랫폼 운영이 가능하다”고 언급하며, 마지막으로 “당사는 디지털 트윈을 기반으로 하는 관제 플랫폼 최고의 기업이 되는 것을 목표로 하고 있다. 현재 생산 쪽에도 적용이 가능한 통합 플랫폼을 개발 중이다. 이 플랫폼은 보안과 안전, 그리고 생산에 특화된 스마트팩토리 통합관제플랫폼이 될 것이다”고 언급하며, 앞으로의 계획을 밝혔다.   Introducing the lidar-based digital twin integrated control platform: IMPNation’s first integrated management solution for security and safetyIntegrated control platform based on digital twin technology In addition to real-time control and AI object detection technology in real and 3D virtual environments, Lidar provides accident and disaster management solutions based on various sensing technologiesIn the era of the Fourth Industrial Revolution, represented by “super-connectivity, super-intelligence, and superconvergence”, PSIM (Physical Security Information Management), which encompasses information and communication technology and control, is emerging. In the midst of this, Mpole System, which has developed an integrated control platform with digital twin and lidar technology, is attracting attention.  Digital twin-based integrated control platformDigital twin is a convergence technology that virtualizes the real world into a 3D model to realize real-time automatic control, simulation analysis, prediction, and optimization. Digital twins can create new future-oriented business models that can overcome the limitations of time, cost, space, and information to solve real-world problems. Mpole System combines digital twin technology with security to provide optimization services without time and space limitations. Mpole System, which has been in the security solution business since 2012, is a leading company in the domestic PSIM industry as a security solution company that provides optimized digital control and platform by applying realistic content, network equipment, wearable devices, XR, smart cities and factories, intelligent CCTV, and sensing technologies. It is 3 years ago that the company loaded digital twin technology in its cutting edge security control platforms.  Mpole System’s integrated control platform ‘IMP’The company’s Integrated Managment Platgorm (IMP) provides a surveillance solution that integrates safety and security. Since it was developed based on a digital twin, it is equipped with the same live-action video transmission function on the site. It is used as an integrated security and safety solution (CCTV, access control, fire surveillance, IOT sensor, etc.) platform in power plants, industrial sites, public facilities, and other facilities, and is used as a digital twin integrated control service using a rulebased work flow engine according to SOP (Standard Operation Process). This allows for efficient management-control by integrating into one system. CCTV, fire detection, access control, temperature and humidity control center, carbon monoxide and carbon dioxide measurement can be controlled in real time. It also makes it easier to pursue system integration through connectivity between hardware and software, and provides simulated, deployed, improved, problem, preventive-prediction of objects by 3D virtualizing.  Efficient virtual space controlThe company’s integrated control platform enables prediction of problems in real-world products or services, and is also used to shorten the development period of new products by utilizing a virtual twin space. It also enables real-time monitoring and control of tasks that are invisible to the naked eye or are highly dangerous. Using digital twin technology, all the information of the building where the IMP is installed is inputted, and not only the fence facilities that prevent the intrusion of outside people, but also the fire fighting and piping and wiring installed inside are input, creating a space identical to the real space in the virtual space. The relevant information is output in real time by the IMP and sends a notification to the control room. Even if you don’t go to the site in person, you can patrol in a virtual space, check safety equipment, and patrol automatically if you set it to the auto function. In virtual reality, the information for generations, temperature, and measuring instruments can be checked in real time, and all the information is updated in real time, so if a problem occurs, you can respond immediately.  Real time control, management and respondThe company aims to provide optimal solutions to a variety of complex real-world problems through data analysis, prediction, and simulation in the urban safety, defense, energy, agriculture, fisheries, and manufacturing industries. The platform can customize various functions to meet the needs of clients. The difficulty of linking different technologies and industries can be solved by using the digital twin integrated control platform. In the case of fire break out, for example, it detects abnormal temperature or sparks and sends alarm.  LiDar-driven defense systemThe core of Mpole System’s integrated control platform solution is compatibility, connectivity, and applicability. In particular, LiDar can be used to build space separation monitoring and perimeter protection systems. Using lidar technology, it is possible not only to measure the distance to the object, but also to analyze various information such as the speed, direction, and size of the moving object without error. The company’s 3D lidar system is specialized in △spatial awareness △object recognition and tracking △3D spatial geofencing △customized technology, and is evaluated as the best solution for security systems due to its very low error rate. The integrated control system platform that can integrate and manage the security and safety of the Mpole System is the first in Korea. The company introduced a security system that uses lidar into the actual field for the first time in Korea, and it is currently used in Korea Western Power, LG Electronics, military bases, government offices, and companies.  Application of AI object detection technologyThe platform can also apply AI object detection to provide 24-hour security by linking with CCTV from movement of persons to fire break-out. In addition, dual thermal imaging cameras have been applied to increase the ability to identify objects. Since the temperature point is divided into 6 zones, the temperature event area can be separated by zone, making it suitable for industrial use. ​“The system detects any abnormal activities and sends the information and rings the alarm at the same time so that the managers can move fast. The lidar detects all areas in 365 degrees and the SOP (STANDARD OPERATING PROCEDURE) reports the event not only to the managers but also to 119 or 112.” explains Song Seung-chan, CEO of Mpole System.   Expansion of solution and linking and applicationAn official of the company says “Our solutions are scalable, compatible, and applicable as they can be applied to a wide range of industries. This is the technical advantage of Mpole System.” IMP can be applied not only to defense systems, but also to CCTV management and access control systems. In addition, the digital twinbased IMP can be applied to FMS or BMS by displaying motion and color to identify all situations within the 3D screen, and also displaying power usage and lighting status. In particular, through digital twin 3D modeling specialized for energy plants and power plants, it provides an optimal solution for complex virtual environment creation, predictive SOP expression, simulation, facility management and control. Song says “Various functions can be linked to manage and control all from one platform. Any accident can be responded to immediately. Platform operation is possible with a minimum number of managers. We aim to become the best digital twin-based control platform company. We are also currently developing a smart factory integrated control platform specialized in production.  엠폴mpole official sitenaver.me  . : 네이버 블로그How we’re steering the built world into the futurem.blog.naver.com  엠폴엠폴은 지능형 디지털 트윈 전문 기업으로 디지털 트윈 기반 페더레이션 구축 및 솔루션을 제공합니다.youtube.com ​​​​​​​  글 | 신태섭 기자 tss79@naver.com온라인 업로드 | 백지원 발행 | 5월 호 CEO<저작권자 © 월간파워코리아 / 파워코리아데일리 무단전재 및 재배포금지> "
Human-Object Interactions Papers[2](CVPR 2022) ,https://blog.naver.com/kingjykim/222885598764,20220927,"https://openaccess.thecvf.com/CVPR2022 CVPR 2022 Open Access RepositoryCVPR 2022 open access These CVPR 2022 papers are the Open Access versions, provided by the Computer Vision Foundation. Except for the watermark, they are identical to the accepted versions; the final published version of the proceedings is available on IEEE Xplore. This material is presented to ensu...openaccess.thecvf.com ​​8. Efficient Two-Stage Detection of Human–Object Interactions with a Novel Unary–Pairwise Transformer저자 : Frederic Z. Zhang, Dylan Campbell, Stephen Gouldhomepage : https://fredzzhang.com/unary-pairwise-transformers/Abstract : 시각 데이터에 대한  transformer 모델의 최근 발전은 인식 및 감지 작업에서 상당한 개선을 가져왔다. 특히,  region proposals 대신 학습 가능한 쿼리를 사용하는 것은 Detection Transformer(DETR)가 주도하는 새로운 종류의 one-stage 감지 모델을 탄생시켰다. 이 1단계 접근 방식의 변화는 이후 human–object interaction(HOI) 감지를 지배했다.​그러나 이러한 1단계 HOI 검출기의 성공은 transformer의 표현력에 영향을 미쳤다. 저자는 동일한 transformer를 장착할 때, two-stage 방식을 훈련하는 데 약간의 시간이 걸리면서도 더 성능 좋고 메모리 효율적일 수 있다는 것을 발견했다.​본 연구에서는 HOI에 대한 단일 및 쌍 표현을 활용하는 2단계 검출기인 Unary–Pairwise Transformer를 제안한다. transformer 네트워크의 단일 및 쌍으로 구성된 부분이 전문화되어 전자는 긍정적인 예제의 점수를 우선적으로 증가시키고 후자는 부정적인 예제의 점수를 감소시킨다는 것을 관찰한다.​저자의 방법은 HICO-DET 및 V-COCO 데이터 세트를 사용하여 성능이 높아졌다는 것을 증명했다. 추론 시간에 ResNet50을 사용하는 우리의 모델은 단일 GPU에서 실시간 성능에 접근한다.  9. Learning Transferable Human-Object Interaction Detector with Natural Language Supervision저자 : Suchen Wang, Yueqi Duan, Henghui Ding, Yap-Peng Tan, Kim-Hui Yap, Junsong Yuanhomepage : https://github.com/scwangdyd/promting_hoiAbstract : human-object interactions(HOI)의 조합 특성으로 인해 인간 행동과 interaction 하는 사물의 가능한 모든 조합을 포함하는 데이터 컬렉션을 구성하는 것은 어렵다. 이 연구에서, 보이지 않는 interaction을 위한 전이 가능한 HOI 검출기를 개발하는 것을 목표로 한다. 기존 HOI 검출기는 종종 interaction을 discrete labels로 처리하고 미리 결정된 범주 공간에 따라 분류기를 학습한다. 이것은 사전 정의된 범주에서 벗어난 보이지 않는 interaction을 탐지하는 데 본질적으로 부적합하다.​반대로, 저자는 독립적인 HOI 레이블을 interaction의 natural language supervision으로 취급하고 그것들을 공동 시각 및 텍스트 공간에 내장하여 상관관계를 포착한다. 구체적으로, interaction 하는 인간과 사물을 감지하고 그것들을 공동 특징 공간에 매핑하여 interaction 인식을 수행하기 위한 새로운 HOI 시각적 인코더를 제안한다. 저자의 시각적 인코더는 새로운 학습 가능한 HOI 토큰과 시퀀스 파서를 통해 Vision Transformer로 인스턴스화되어 고유한 HOI 예측을 생성한다. 사전 훈련된 CLIP 모델에서 전송 가능한 지식을 증류하고 활용하여 zero-shot interaction 탐지를 수행한다.​SWIG-HOI와 HICO-DET라는 두 데이터 세트에 대한 실험은 저자가 제안한 방법이 보이는 HOI와 보이지 않는 HOI를 모두 탐지하는 데 있어 주목할 만한 mAP 개선을 달성할 수 있음을 검증한다.  10. NeuralHOFusion: Neural Volumetric Rendering under Human-object Interactions저자 : Yuheng Jiang, Suyi Jiang, Guoxing Sun, Zhuo Su, Kaiwen Guo, Minye Wu, Jingyi Yu, Lan XuAbstract : human-object interactions의 4D 모델링은 수많은 응용 프로그램에서 매우 중요하다. 그러나 특히 적은 입력에서 복잡한 interaction 시나리오의 효율적인 볼륨 캡처 및 렌더링은 여전히 어렵다.​본 논문에서는 sparse consumer RGBD sensors를 사용하여 volumetric human-object 캡처 및 렌더링을 위한 신경 접근법인 NeuralHOFusion을 제안한다. 기존의 non-rigid fusion을 최근의 neural implicit modeling 및 blending advances과 결합하며, 여기서 포착된 인간과 사물은 층별로 분리된다. geometry modeling을 위해, 저자는 템플릿 보조 강력한 사물 추적 파이프라인뿐만 아니라 non-rigid key-volume fusion이 있는 neural implicit inference scheme를 제안한다. 계획은 복잡한 interactions과 occlusions에서 상세하고 완전한 기하학적 생성을 가능하게 한다. 또한 공간 및 시간 영역 모두에서 볼륨 및 이미지 기반 렌더링을 결합하여 사진 현실적 결과를 얻는 레이어 별로 human-object 텍스처 렌더링 체계를 도입한다.​광범위한 실험은 복잡한 human-object interactions에서 사진-현실적 프리뷰 결과를 합성하는 접근 방식의 효과와 효율성을 보여준다.  11. Interactiveness Field in Human-Object Interactions저자 : Xinpeng Liu, Yong-Lu Li, Xiaoqian Wu, Yu-Wing Tai, Cewu Lu, Chi-Keung Tanggit : https://github.com/Foruck/Interactiveness-FieldAbstract : Human-Object Interaction(HOI) 감지는 활동 이해에서 핵심 역할을 한다. 최근의 two/one-stage 방법이 인상적인 결과를 달성했지만, 필수적인 단계로서 대화형 human-object  쌍을 발견하는 것은 여전히 어려운 일이다. one/two-stage 방법 모두 redundant negative pairs을 생성하는 대신 extract interactive pairs을 효과적으로 추출하지 못한다.​이 연구에서, 우리는 이전에 간과되었던 interactiveness bimodal prior을 소개한다. 이미지에서 사물은 인간과 쌍을 이룬 뒤 생성된 쌍은 대부분 non-interactive이거나 대부분 interactive 하며, 전자는 후자보다 더 빈번하다. 이러한 interactiveness bimodal prior에 기초하여 “interactiveness field”를 제안한다. 학습된 필드가 실제 HOI 이미지 고려 사항과 호환되도록 하기 위해,  저자는 interactive와 non-interactive pairs에 기초하는 고유한 “interactiveness field”의 cardinality 및 차이를 기반으로 하는 새로운 에너지 제약을 제안한다.​결과적으로, 우리의 방법은 더 정확한 쌍을 감지하여 HOI 감지 성능을 크게 향상시킬 수 있으며,  이는 최첨단보다 상당한 개선을 달성하는 널리 사용되는 벤치마크에서 검증된다.  12. Category-Aware Transformer Network for Better Human-Object Interaction Detection저자 : Leizhen Dong, Zhimin Li, Kunlun Xu, Zhijun Zhang, Luxin Yan, Sheng Zhong, Xu ZouAbstract : 인간과 관련 사물의 interaction을 인식하면서 localize 하는 것을 목표로 하는 Human-Object Interactions (HOI) 감지는 이미지를 이해하는 데 중요하다. 최근  tranformer 기반 모델은 HOI 검출의 진전을 크게 진전시켰다. 그러나 모델의 Object Query는 항상 단순히 0으로 초기화되어 성능에 영향을 미치기 때문에 이러한 모델의 기능은 완전히 탐구되지 않았다.​본 논문에서는 카테고리 인식 의미 정보로 Object Query를 초기화하여 transformer 기반 HOI 검출기를 촉진하는 문제를 연구하려고 한다. 이를 위해 Category-Aware Transformer Network(CATN)를 혁신적으로 제안한다. 특히, Object Query는 더 나은 성능을 내기 위해 외부 object 감지 모델로 대표되는 category priors들을 통해 초기화된다. 또한, 이러한  category priors은 attention mechanism을 통해 특징의 표현 능력을 향상시키는 데 추가로 사용될 수 있다. 저자는 먼저 groundtruth 카테고리 정보로 Object Query를 초기화함으로써 Oracle 실험을 통해 아이디어를 검증했다. 그런 다음 아이디어를 갖춘 HOI 탐지 모델이 기준치를 큰 폭으로 능가하여 새로운 최첨단 결과를 달성한다는 것을 보여주기 위한 실험이 수행되었다.  13. Consistency Learning via Decoding Path Augmentation for Transformers in Human Object Interaction Detection저자 : Jihwan Park, SeungJun Lee, Hwan Heo, Hyeong Kyu Choi, Hyunwoo J.Kimgit : https://github.com/mlvlab/CPC_HOIAbstract : Human-Object Interaction(HOI) 감지는 interaction 분류 뿐만 아니라 객체 감지를 수반하는 holistic한 시각 인식 작업이다. 이전 HOI 감지는  Image → HO → I, Image → HI → O와 같은 다양한 구성의 subset 예측을 다뤘다. 최근 transformer 기반 HOI 아키텍쳐는 end-to-end 방식(Image → HOI)을 통해 HOI 삼중항을 직접 예측한다. ​저자는 다양한 HOI 감지 방법 추론에 영감을 받아 증강된 디코딩 경로를 활용하여 transformer의 HOI 탐지를 개선하기 위한 새로운 end-to-end 학습 전략인 Cross-Path Consistency learning(CPC)을 제안한다. CPC 학습은 순열 추론 시퀀스에서 가능한 모든 예측이 일관되도록 한다. 이 간단한 체계는 모델이 일관된 표현을 학습하도록 하여 모델 용량을 늘리지 않고 일반화를 개선한다.​저자의 실험은 방법의 효과를 보여주며, 기준 모델에 비해 V-COCO와 HICO-DET에서 상당한 개선을 이루었다.  14. MSTR: Multi-Scale Transformer for End-to-End Human-Object Interaction Detection저자 : Bumsoo Kim, Jonghwan Mun, Kyoung-Woon On, Minchul Shin, Junhyun Lee, Eun-Sol KimAbstract : Human-Object Interaction (HOI) 감지는 이미지에서 ⟨human, object, interaction⟩ 삼중항 세트를 식별하는 작업이다. 최근의 연구는  end-to-end 훈련을 통해 HOI 검출에서 많은 hand-designed 구성 요소의 필요성을 성공적으로 제거한 transformer encoder-decoder 아키텍처를 제안했다. 그러나 그것들은 single-scale 특징 해상도로 제한되어 humans, objects 및 매우 다른 스케일 및 거리와의 interactions을 포함하는 장면에서 차선의 성능을 제공한다.​이 문제를 해결하기 위해 Dual-Entity attention와 Entity-conditioned Context attention라는 두 개의 새로운 HOI 인식 deformable attention 모듈로 구동되는 HOI 감지를 위한 Multi-Scale TRansformer(MSTR)를 제안한다. 기존의 변형 가능한 attention는 HOI 감지 성능에서 막대한 비용이 발생하지만, MSTR의 제안된 attention modules은 interaction을 식별하는 데 필수적인 샘플링 지점에 효과적으로 주의를 기울이는 방법을 학습한다.​실험에서, 두 개의 HOI 탐지 벤치마크에서 새로운 최첨단 성능을 달성했다.​​​2022.09.27 "
