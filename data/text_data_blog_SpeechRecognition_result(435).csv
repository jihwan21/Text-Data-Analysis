title,link,postdate,description
SpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition 논문 리뷰 ,https://blog.naver.com/grow_bigger/222996165337,20230127,"1. Introduction심층학습은 자동 음성인식(Automatic Speech Recognition)에 잘 적용되고 있습니다. 다만 이러한 심층학습 모델들은 과적합이 쉽게 일어나는 문제가 있었습니다. 이러한 시점에서 나온 이 논문은 과적합되는 문제를 세가지의 주요 데이터 증강기법으로 과소적합 문제로 바꿉니다. 즉, 학습을 더 시키는 등의 기존에 흔히 쓰던 과소적합 문제를 해결하는 방식들을 적용하면 되는 것입니다. ​물론 심층학습을 이용한 방식 자체가 기본적으로 데이터 증강과 밀접합니다. 이미지의 경우 방향도 돌렸다가 좌우 반전을 시키기도 하는데, 음성인식분야에서도 비슷하게 다양한 시도들이 있었습니다. Vocal Tract Length Normalization, Noisy Audio(잡음을 추가하는 방법), speed perturbation(속도를 조절하는 방식), pitch perturbation(음높이를 변경하는 방법) 또는 multi stream ASR을 훈련시키기 위한 Feature drop-outs같은 방식들이 고안되었습니다. 일반적으로 이미지 처리 분야에서 적용되던 방식들을 음성인식에도 사용하고 있었던 것입니다. ​논문제서 제안하는 SpecAugment는 입력 음성신호를 그대로 사용하지는 않고 입력 소리 신호의 로그 멜 스팩트로그램을 마치 이미지를 다루듯이 변형시키는 증강기법입니다. 세부적으로는 time warping, frequency masking, time masking으로 세 가지 기법이 있고 전반적으로는 계산 비용이 많이 들지는 않습니다. 물론 이 세 가지 중에서는 뒤에 소개될 time warping이 비용 대비 성능 향상 효과는 좀 덜 하다고 합니다.​2. Augmentation PolicySpecAugmentation에는 세 가지 기법이 있는데 신경망이 학습할 유용한 feature들이 시간에 대한 변형이나 주파수 정보 및 음성의 일부 손실에 대해 강건(크게 영향을 받지 않는)하도록 증강을 하려고 한 결과물들입니다. 즉, 다음 세 가지 증강 기법들은 위의 'policy'를 따릅니다. ​각 기법들이 적용된 모습은 아래의 사진과 같습니다.  다음은 각 기법에 대한 설명입니다. Time warping이미지의 중앙을 가로지르는 수평선에 임의의 한 점을 잡고 왼쪽이나 오른쪽으로 이 점을 잡고 당기는 방식입니다. ​2.Frequency maskingmel frequency의 영역 [f0, f0+f)을 가리는 방법입니다. ( 0<f<F, F는 파라미터. f0는 [0, v-f)에서 선택, v는 mel frequency의 채널 수) 3. time maskingtime step [t0, t0 + t)을 가리는 방법입니다. ( 0<t<T, T는 파라미터. t0는 [0, τ  - t)에서 선택, v는 mel frequency의 채널 수) time mask는 time step수에 비해 너무 커지면 안된다고 합니다.  ​이때 masking들이 여러개가 적용될 수도 있습니다. 논문에서는 4개의 정책을 직접 만들었습니다.  LB정책이라면 각 masking이 1개씩만 적용된 것이고 SM은 각각 두개씩 적용된 데다 LB에 비해 masking될 수 있는 최대 두께가 작습니다. 추가로 파라미터 p의 경우 time masking에서 time mask가 time step에  비해 너무 커지면 안된다고 했는데, 이때의 상한선을 정하는데 time mask두께 < time step * p의 조건으로 이용됩니다. ​3. Model본 논문에서는 SpecAugment의 성능 확인을 위해 Listen, Attend and Spell network를 이용합니다.​3.2. Learning Rate Schedules학습률은 ASR 신경망에 있어 중요한 요소라고 합니다. 특히 데이터 증강이 있으면 더욱 중요하다고 합니다. 학습률 스케줄은 ramp-up, hold, exponentially decay단계로 세분화되어 있습니다. decay시에는 최곳값의 100분의 1이 될 때까지 하고 그 뒤부터는 일정하게 학습률을 유지 시킵니다.​아래는 저자들이 사용한 기본적인 스케줄입니다. 이떄 각 S는 각 단계의 종료시점입니다.  Sr은 ramp-up종료시점입니다. 그리고 Snoise는 학습률에 표준편차가 0.075인 noise를 섞는 구간입니다. 이후 구간에서도 훈련종료까지 계속 노이즈를 유지합니다. (we turn on a variational weight noise of standard deviation 0.075 at step snoise and keep it constant throughout training.)​추가로, 저자들은 label smoothing기법을 추가로 적용합니다. 완전히 학습률과 관련이 된 건 아닌데, label을 1 또는 0과 같이  one-hot인코딩을 하는 대신 0.9와 0.1로 만드는 것 입니다. 즉, 확신도와 불확신도로 변형을 하는 것 입니다. 이 부분을 이 문단에 같이 포함시킨 이유는 label smoothing이 학습률이 작을 때 적용하면 모델의 학습이 불안정해지는 점을 강조하기 위함인 것 같습니다. 저자들은 종종 훈련 초반에만 적용하고 learning rate가 감소하는 시점에는 적용하지 않았다고 합니다.​학습률을 어떻게 설계할지에 대한 중요한 지표 중 하나는 저자들이 실험했을때 길면 대체적으로 좋은 성능을 얻을 수 있었다는 점입니다. 위 Long schedule은 위의 기본 schedule보다 훨씬 긴데, 가장 큰 모델의 성능 향상을 위해 사용한 스케줄이라고 합니다. label smoothing은 LibriSpeech 960h에 대해서는 140k시점 이전까지 적용했고, Switchboard 300h에 대해서는 훈련내내 적용했다고 합니다.​3.3 Shallow Fusion with Language Models저자들은 데이터 증강만으로도 SOTA를 달성했지만, 그럼에도 추가적인 성능향상을 위해 language model을 사용할 수 있습니다. 기본 ASR모델과 주로 RNN기반의 language model을 조합하여 결과를 내는 방식입니다. 아래 식처럼 language model에 람다 파라미터를 곱해 합산하는 방식을 채택하고 있습니다. ​  [참고자료]https://arxiv.org/abs/1904.08779 [원 논문]SpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition (notion.so) "
[딥러닝] Attention-Based Models for Speech Recognition (논문 리뷰) ,https://blog.naver.com/andrew9909/222810055983,20220713,"2015년 발표되었고 인용 수가 현재(22/7/13) 2340회를 달성한 논문입니다.  0. 들어가며​Seq2seq 모델은 기존 모델에 비해서 SOTA를 달성하지 못했지만, 가능성을 제시했고 Attention의 도입으로 인해 퍼포먼스가 매우 좋아졌습니다. 하지만 이는 번역 모델에 대한 이야기인데요, 번역 모델에서 입력이 몇십 개의 단어로 이루어지는 반면, 음성 인식 모델에서는 매우 긴 시퀀스가 입력 데이터로 들어가게 됩니다. 그럴 경우 생겨나는 다양한 문제점으로 인해 이 논문에서는 논문 제목에 써 있듯 '음성 인식을 위한 어텐션 모델'을 제시했습니다.  1. General Framework - ARSG 기본 구조​Attention-based recurrent sequence generator (ARSG)의 기본적인 방식에 대해 살펴보겠습니다. 이전에 소개했던 논문들에서도 자주 나왔던 식입니다. 가중치를 고려해 새로운 context vector를 만들어 주의깊게 보아야 할 input에 더욱 attention할 수 있습니다. 에너지 e_i,j는 이전 decoder의 state인 s_i-1과 encoder의 hidden state인 h_j를 이용해 만듭니다. 각 토큰별로 이번 output을 위한 에너지를 가지고 있게 되는 것입니다. 이를 softmax 함수에 넣어 각 가중치 α를 구합니다. 이는 해당 hidden state에 얼마나 attention할 지 결정하는 데에 활용합니다. 이 논문에서는 가중치를 alignment라는 이름으로 사용했습니다. 여기에서는 새로운 context vector를 c가 아닌 g로 표기했습니다. glimpse의 약자라고 합니다. context vector와 이전 decoder의 state를 활용해 output y를 만들어냅니다.  이 과정을 표현한 그림입니다. 보통 그림이 encoder와 decoder를 나누어 그렸는데, 여기서는 encoder의 h까지 한 번에 그려서 조금 복잡하게 생긴 감이 있습니다.​  2. General Framework - Attention의 방법​​ 출처: SooftwareAttention하는 방법은 다양한데요, 기본적인 dot product 방식 말고도 다양한 방법이 존재합니다. 이 논문에서 참고한 방법에 대해 살펴보겠습니다. Content-based attention mechanism은 다음과 같은 수식으로 에너지를 구합니다. 여기서 W, V, w는 모두 가중치이고, h와 s는 각각 encoder, decoder의 state입니다. b는 편향(bias)입니다.  기존 dot product 방식이 단순히 s, h 행렬을 내적했다면 이 방식에서는 s, h에 모두 가중치를 부여하고 편향이 존재합니다. 이를 합해 hyperbolic tangent 함수를 적용하고,  마지막으로 이에 대한 가중치 역시 존재합니다. 어쨌거나 이 경우에도 마지막에 w의 transpose된 행렬을 곱했기 때문에 에너지는 스칼라 값 하나로 나오게 됩니다. 가중치들이 더 많아져 보다 섬세한 attention이 가능하겠으나, 이 방식의 문제점은 sequence에서 자신의 위치에 상관없이 scoring을 한다는 점입니다. 이를 'similar speech fragments' 문제라고 합니다.​다른 방법 하나는 'location based attention' 방법입니다. 이 방식에서는 이전 가중치와 decoder의 output s만을 이용해 곧바로 가중치를 만들어냅니다. 하지만 이 방식은 encoder의 hidden state를 고려하지 않기 때문에 한계가 존재합니다. 애초에 attention이라는 개념 자체가 버려지는 encoder의 hidden state가 너무 아깝다는 생각에서 비롯된 아이디어입니다.​연구진들은 location based attention의 아이디어를  content based  attention 기법에 적용함으로써 위치 정보까지 담고 있는 개선된 attention 기법을 만들고자 했습니다. 논문의 표현으로는 'location-aware'한 기법입니다.k×r차원의 행렬 F와 이전 가중치 α_i-1을 합성곱하여 새로운 k개의 vector f를 만들어줍니다. 이를 통해 위치 정보를 반영하였고, 앞선 식에 이 f, 그리고 f에 대한 가중치 U를 새롭게 도입해 에너지를 만들어 줍니다.  ​  3. Score Normalization: Sharpening and Smoothing​기본적인 attention은 모든 에너지에 대해 softmax 함수를 적용하여 이루어집니다. 이 방식에 대해 논문에서는 세 가지 문제점을 지적하고 있습니다.​1) input sequence가 길다면, glimpse(context vector)에는 노이즈 정보가 포함되어 있을 가능성이 크다.특히나 음성 인식에서는, 한사랑산악회 정광용 선생님처럼 말을 엄청 느리게 하시는 분이 아니라면 현재보다 특정 시점 이후의 음성  input은 현재 input과 연관이 없을 것입니다. 하지만 softmax 함수의 기본 구조가 '나 / 전체' 이기 때문에, 현재 내 입장에서 쓸모없는 input들의 에너지, 즉 노이즈까지 모두 고려할 수 밖에 없습니다. 그래서 내가 음성 인식에 필요한 몇 개의 프레임들, 논문의 표현에 의하면 'a few relevant frames'에 집중하기가 힘듭니다.​2) 시간 복잡도가 크다.input sequence가  L 만큼의 길이를 가지면, 디코더는 모든 계층에서  L개의 frame을 모두 고려해 attention해야 합니다. 그래서 T만큼의 디코딩 길이를 가지면, T번만큼 attentio을 반복하기에 O(LT)라는 높은 시간 복잡도를 가지게 됩니다. ​3) Softmax 함수는 Single Vector에만 집중 (focus) 하는 경향이 있다.이러한 경향은 top-score를 받은 여러 프레임을 고려할 수 없게 합니다.​이러한 문제점을 개선하기 위해 연구진들은 'Sharpening', 나아가 'Smoothing' 기법을 제안합니다. 하나씩 살펴보겠습니다.​1. Sharpening when β > 1β > 1인 inverse temperature을 각 에너지에 곱해주는 방식을 제안했습니다. 하지만 이 방식은 시간 복잡도를 줄이지 못했는데요, 시간 복잡도를 줄이기 위한 방법으로 'windowing' 역시 제안했습니다. 이 방식은  이전 alignment의 중간값을 기준으로 윈도우 크기 만큼만 고려해주는 방식입니다. 이 방법을 통해 시간 복잡도를 O(LT)에서 O(L+T)로 낮출 수 있습니다.Sharpening은 long-utterance에서의 퍼포먼스는 개선했지만, 전체적인 퍼포먼스는 하락했습니다. 하지만 해당 실험은 multiple top-scored frames들을 선택하여 집계하는 방식이 좋을 것이라는 가정을 하도록 만들었다고 합니다. 이를 통해 나오게 된 기법이 smoothing 기법입니다.​2. Smoothing Smoothing 기법은 각 에너지에 exponential 함수를 씌우는 것이 아니라 sigmoid 함수를 씌우는 기법입니다.   4. 실험 결과 새로운 attention 기법과 smoothing까지 적용했을 때 에러율은 17.6%로, RNN Transducer보다는 좋았지만 기존 SOTA에는 성능이 미치지 못하는 모습을 보였습니다.  참고자료​원논문https://arxiv.org/abs/1506.07503?context=cs Attention-Based Models for Speech RecognitionRecurrent sequence generators conditioned on input data through an attention mechanism have recently shown very good performance on a range of tasks in- cluding machine translation, handwriting synthesis and image caption gen- eration. We extend the attention-mechanism with features needed for speec...arxiv.org 참고자료1https://github.com/sooftware/Speech-Recognition-Tutorial/blob/master/seminar/Attention-Based%20Models%20for%20Speech%20Recognition.pdf Speech-Recognition-Tutorial/Attention-Based Models for Speech Recognition.pdf at master · sooftware/Speech-Recognition-Tutorial한국어 음성인식 튜토리얼. Contribute to sooftware/Speech-Recognition-Tutorial development by creating an account on GitHub.github.com ​ "
TIL_0624_Speech_Recognition ,https://blog.naver.com/mm323/222787089692,20220624,"Efficient on-device acoustic models On-device speech recognition and DRAM bandwidth bottleneck problemOn-device 장점: serverlessOn-device 문제점: processing power, battery life issueArithmetic bound: ARM CPU에서 감당할 수 있는 연산량인가?DRAM BW bound: Cache 이용률 높이고, DRAM access BW줄여야 함Memory BW줄이는 방법Model compression: FP optimization, Pruning, SVD transformation등Parallelization: computing multiple output samples at a time(multi-time-step parallelization), LSTM-RNN에는 적용할 수 없음Multi-time step parallel and linear recurrent RNNsIdea: DRAM access 줄이기 위해 하나의 parameter를 mutiple time step에 적용하고자 함, 그러나 end-to-end acoustic modeling에 많이 사용되는 LSTM RNN의 경우 ht 계산시 이전 time의 feedback(ht-1)이 필요하므로 parallelize어려움 -> compute output one by oneLinear recurrent RNNs (no U matrices): matrix연산이 필요한 부분은 병렬진행하고 scalar곱 필요한 부분만 직렬로 진행QRNNs (Quasi-RNN): LSTM RNN without output feedbackSRU (Simple Recurrent Unit)QRNN+1-d time-depth convolution based modelsQRNN의 performance(accuracy) issue: LSTM RNN은 long-term (ct-1) and short-term feedback(ht-1)을 제공하는데 반해 QRNN은 long-term feedback만 가지고 있어서 LSTM RNN에 비해 성능이 떨어짐 -> time-context의 depth를 추가함(SRU앞에 1D Conv) 성능향상 꾀함Word-piece model에서는 AM(Acoustic Model)은 LM에 비해 더 낮은 속도로 처리해도 괜찮다?? -> Down sampled word piece models (reducing the frame rates by inserting time-domain pooling at CNN or RNN layers)Gated ConvNet and Simple Gated ConvNet based modelsCNN계열은 time을 길게 봐야만 성능이 나오는데, 이러면 parameter이 늘어남 -> shorter 2D convolution을 사용하는 대신 context length를 늘리기 위해 1D-time depth convolution을 추가​ Recent Deep Neural Networks for Automatic Speech Recognition Sequence Learning DNN Models learned so farLSTM RNN Adv: RNN중에서는 비교적 훈련이 잘됨, 성능이 좋다단점: 출력이 한 샘플씩 계산됨. 병렬처리 환경에서 불리QRNN-long term memory만 사용하고 short term feedback사용하지 않음, 대신에 입력을 추가 처리장점: 출력을 여러 개 한번에 계산가능단점: 성능 문제가 있음CNN, gated Convnet: 정해진 길이의 convolution사용장점: 출력을 여러 개 한번에 계산가능단점: 좋은 성능을 위해서는 convolution길이를 늘려야함(mobile에서는 time-depth-wise 1-D convolution을 이용해서 해결하기도 함)Transformer model: 시간축의 연산을 모두 풀어놓고, 입력사이의 관계를 attention을 이용해 파악장점: 좋은 성능(과거를 길게 본다). 출력을 여러 개 한번에 계산(특히 훈련시 유리), 훈련이 잘됨(LSTM RNN대비 2~3배 빠름), model size가 정해진 경우 성능이 좋고, 병렬 처리하기 좋다. 멀리있으나 가까이 있으나 동일하게 본다단점: 길이가 길 경우 계산에 불리(attention은 길이의 제곱 계산), 실시간 모델 만들기 어렵다(내부 메모리 사용량 많다)Attention Is All You Need (1706.03762.pdf (arxiv.org))Transformer-TransducerConformerA variant of Transformer with additional convolution moduleTime-domain 1D convolutionContextNet기본적으로 CNN구조​Wav2Vec 2.0Self-supervised learning: 문장(raw waveform)을 주면서 random하게 단어를 지우고(masking) 그 단어(frame)를 예측하도록 trainingPretrained model + Fine-tuning for ASR​​SpecAugmentAudio augmentation methodsSpectrogram augmentationTime WarpingFrequency MaskingTime Masking​CTC DecodingConditional Independence in CTCGreedy Decoding: Argmax of each frame매 frame에서 가장 확률이 높은 단어를 고름만들어진 문장에서 collapse and removeCTC beamsearch DecodingBeamsearch = Keep track of highest probability prefix한 frame씩 봐가면서 가장 확률 높은 Top-B개의 prefix후보군을 선택하고 collapse했을 때 확률이 높아지는 path가 선택됨 ​ MLCC EmbeddingsMotivation From Collaborative FilteringInput: 1,000,000 movies that 500,000 users have chosen to watchTask: Recommend movies to users           To solve this problem some method is needed to determine which movies are similar to each other.Organizing Movies by Similarityd-Dimensional EmbeddingsAssumes user interest in movies can be roughly explained by a d aspectsEach movie becomes a d-dimensional point where the value in dimension d represents how much the movie fits that aspectEmbeddings can be learned from dataLearning Embeddings in a Deep NetworksNo separate training process needed - the embedding layer is just a hidden layer with one unit per dimensionSupervised information (e.g. users watched the same two movies) tailors the learned embeddings for the desired taskIntuitively the hidden units discover how to organize the items in the d-dimensional space in a way to best optimize the final objectiveInput RepresentationEach example (a row in this matrix) is a sparse vector of features(movies) that have been watched by the userDense representation of this example as: (0, 1, 0, 1, 0, 0, 0, 1)            Is not efficient in terms of space and timeBuild a dictionary mapping each feature to an integer from 0, ..., #movies-1Efficiently represent the sparse vector as just the movies the user watched. This might be represented as: (1,3,999999)Corresponding to Geometric ViewDeep NetworkEach of hidden units corresponds to a dimension(latent feature)Edge weights between a movie and hidden layer are coordinate valueGeometric view of a single movie embeddingA move would be embedded point in a 3 dimensional space Selecting How Many Embeddings DimsAs with any deep neural network there are hyperparameters and one of the hyperparameters we have in the embedding layer is how many embedding dimensionsHigher-dimensional embeddings can more accurately represent the relationships between input valuesBut more dimensions increase the chance of overfitting and leads to slower trainingEmpirical rule-of-thumb (a good starting point but should be tuned using the validation data): Embeddings as a ToolEmbeddings map items (e.g. movies, text, ...) to low-dimensional real vectors in a way that similar items are close to each otherEmbeddings can also be applied to dense data (e.g. audio) to create a meaningful similarity metricJointly embedding diverse data types (e.g. text, images, audio,...) define a similarity between themTranslating to a Lower-Dimensional SpaceObtaining EmbeddingsStandard Dimensionality Reduction Techniques PCA has been used to create word embbedingsWord2vec "
미아보청기 어음지각(speech perception)라고도 불리는 어음인지(speech recognition) 어음인지검사(speech recognition testing) ,https://blog.naver.com/sad097235/222960929410,20221228,"미아보청기 어음지각(speech perception)라고도 불리는 어음인지(speech recognition) 어음인지검사(speech recognition testing)​​​대한민국 상위 1% 사후관리 시스템을 구축하고 있는 보청기 전문 센터 스탠다드보청기 청능재활전문센터에요.​  스탠다드보청기 전문센터  청각 평가와 어음인지​  대화 유창성 & 어음인지검사(speech recognition testing)대화 유창성의 향상과 청력과 관련된 장애의 감소에 있어 중요한 요소는 어음을 인지하기 위해서 개인의 능력을 최대한으로 활용하는 것이다. ​​대화 유창성이 종종 감소하는 이유는 청력손실을 가진 사람들이 의사소통 파트너의 구어 메시지를 인지할 수 없기 때문이다. ​​어음지각(speech perception)라고도 불리는 어음인지(speech recognition)는 사람들이 구어 메시지를 이해하기 위하여 청각과(혹은) 시각 정보를 얼마나 잘 이용하는가를 반영한다. ​​어음인지검사(speech recognition testing)는 사람들이 음소, 단어 및 문장과 같은 말소리 단위들을 얼마나 잘 인지할 수 있는지에 대한 평가를 포함한다. ​​대부분의 청능재활 계획 중 첫 번째 단계는 환자의 청력을 평가하고, 말을 얼마나 잘 인지할 수 있는지를 평가하는 것이다. ​사람들은 청력의 상태와 의사소통의 필요성에 따라 보청기나 인공와우 와 같은 청각 보조 기기를 사용하게 되고, 청능훈련이나 독화 훈련 혹은 그 두 가지 모두에 참여하게 된다.​​어음인지력을 최적화할 수 있는 범위 내에서 대화 유창성의 향상과 청력과 관련된 장애의 감소가 동시에 일어날 것이다. ​ 스탠다드보청기 전문센터스탠다드보청기 전문센터   청력도(audiogram)청력도(audiogram)는 개인의 청력손실 정도에 대한 일반적인 설명을 나타내고 있다.   그러나 개인이 경험하는 의사소통의 문제와 청능재활에 대한 필요성을 항상 적절히 반영하는 것은 아니다.   어음인지에 대한 측정은 청력손실이 개인의 삶과 의사소통의 상호작용에 어떤 영향을 미치는가에 대한 평가에 있어 중요한 요소이다. ​목적, 환자 변인, 자극 단위, 검사 절차   스탠다드보청기 전문센터스탠다드보청기 전문센터   인지적 가소성 VS 인지적 예비성인지적 가소성 VS 인지적 예비성입니다.​​인지적 가소성, 즉 뇌와 행동의 플라스틱 변화는 시스템의 1차적 변화에 대한 2차적 변화를 반영합니다(Bengtson et al, 2005).  ​​인지 작업에 대한 향상된 성과를 포함한 개인의 인지 가소성은 잠재된 인지 잠재력과 현재 수행 수준에 기초해 정의됩니다(Willis et al, 2009). ​​인지 가소성은 맥락에 따라 다르며 신경 가소성과 통합되어 있는 비분할적 과정입니다. 이것은 신경 심리학에서 사용되는 인지 기능의 개념과 밀접하게 관련되어 있습니다. ​​인지 기능은 신경 수준의 결손이나 병리에도 불구하고 적절한 인지 기능 수준에서 계속 기능하는 개인의 능력을 말합니다. (Willis et al, 2009). ​​윌리스 외 연구진 (2009)에 따르면, 인지 가소성이 발생하는 세 가지 다른 단계가 있습니다: 뇌 내의 신경 가소성; 행동 수준에서의 인지 가소성; 그리고 문화 자원과 지식이 보존되고 전달되는 사회 문화 수준에서의 인지 가소성입니다.   스탠다드보청기 전문센터스탠다드보청기 전문센터   인지 가소성비록 인지 가소성이 분파 내 과정이지만, 인지 예비군은 신경 병리학이 주어진 인지 기능을 수행하는 뇌의 부분을 방해하면 보상하는 능력 측면에서 개인 간에 차이가 있다고 제안합니다.   Willis et al (2009)에 따르면, 인지적 가소성에 기여하는 신경적 가소성이 손상되어도 인지적 가소성은 계속 존재합니다. 노화와 관련된 인지 변화를 보상하는 능력에서 개인마다 상당한 차이가 있습니다.    LACE와 같은 청각 기반 개입을 통한 청각 가소성의 개념과 유사하게, 신경 심리학자들은 인지 기능이 사람을 능동적인 변화의 매개체로 하여 목적적인 활동에 의해 강화될 수 있다고 믿습니다.   인지 예비력에서 개인 간의 차이는 선천적인 지적 기능, 환경적 요인, 사전 인지 활동 수준, 교육적 추구, 직업적 지위, 그리고 다양한 다른 고려 사항의 차이 때문일 수 있습니다.   마틴과 고렌스타인(2010)에 따르면, 인지 예비 모델은 정상적인 노화 또는 질병과 관련된 변화에 의해 인지 예비력이 고갈될 때 인지 장애가 발생하기 시작한다고 합니다.​​ 스탠다드보청기 전문센터​ 스탠다드보청기 의정부센터경기도 의정부시 신흥로240번길 26 동양레쉬빌II 2층 205호   ​ "
[Deep Learning] DLHLP #2 Speech Recognition (1/7) ,https://blog.naver.com/after-glow-/222999736987,20230130,"이번 포스팅은 국립 타이완 대학교 Hung-Yi Lee(李宏毅) 교수님의 강의 중Speech Recognition에 관한 내용을 다룰 예정이다.​강의 링크: Speech Recognition (음성인식)음성인식이란?- 인간의 발성하는 음성을 이해하여 컴퓨터가 다룰 수 있는 문자(코드) 정보로 변환하는 기술. Speech:  a sequence of vector (length T, dimension d) Text:  a sequence of token (length N, V different tokens) Usually T > N ​​음성을 구성하는 요소별 단위를 살펴본다:  Token​​- Phoneme (음소) :  a unit of sound어떤 언어에서 의미 구별 기능을 갖는 음성상의 최소 단위. Lexicon : word to phonemes 이러한 사전이 있다면 phoneme과 단어의 상관관계를 파악할 수 있다.Phoneme과 음성의 관계가 매우 뚜렷해서 딥러닝이 생기기 전에는 음성인식에 많이 사용되었지만 음소 사전이 있어야 단어에 적용할 수 있다는 단점이 있다. 이 사전은 만들기 어렵고 전문 언어학자에 의해서만 가능하다,,게다가 어떤 언어의 Phoneme는 아직 찾기 힘들다( •︠ˍ•︡ )​​- Grapheme (문자소) : smallest unit of a writing system ​영어로 예를 들자면: 26 English alphabet+ { _ } (space) + {punctuation marks} (문장부호) 반면에 중국어는 ""space""가 필요 없다. 그래서 Phoneme보다는 훈련시키기 어려운 부분이 있다.​​- Word: 어떤 언어에서는 V의 값이 엄청 커질 수가 있다.(예를 들면 중국어는 space로 단어 사이를 나누지 않기 때문에 중국어의 경우 V가 엄청 크다.)​​- Morpheme (형태소) : the smallest meaningful unit​예를 들면:unbreakable → “un” “break” “able” rekillable → “re” “kill” “able” ​​- Bytes: The system can be language independent!​이진법으로 언어를 표시하는 것. ​​2019년도의 논문 통계에 의하면 :(Go through more than 100 papers in INTERSPEECH’19, ICASSP’19, ASRU’19) Grapheme의 비중이 제일 큰 것으로 볼 수 있다.  Acoustic Feature(음향 특성)보편적으로 음성의 길이는 T,  차원은 d의 특징 벡터로 표시한다.​ 만약 위와 같은 음성 시그널을 크기 25ms의 네모 기준으로 스캔한다면하나의 frame 벡터를 얻을 수 있다 : frame 벡터를 계산하는 3가지 방법즉 모든 네모의 간격이 10ms일 경우 초당 100개의 frame 벡터가 존재한다는 것이다.​​아래와 같이 음성의 특징을 추출 해내려면 우선 DFT(Discrete Fourier Transform) 을 통해서 spectrogram을 얻어야 한다.( spectrogram은 음성 식별을 하는 데 있어서 매우 중요한 역할을 한다,많은 데이터를 기반으로 train 시킨 후 오로지 spectrogram만 봐도 발음할 수도 있다. )그다음엔 spectrogram을 filter bank를 통해 각각의 데이터를 얻은 후log를 거쳐서 DCT(Discrete Cosine Transform)를 적용한다면  MFCC 특징을 얻을 수 있다. ​사실상 filterbank output, MFCC, spectrogram, waveform 등모두 음성 식별에 바로 활용 가능하지만 이 중에서도 filterbank ouput이 제일 많이 사용되고 있으며 중요한 부분이기도 하다.  Speech Recognition (음성 식별)의 두 가지 모델 분류:Seq-to-seq과 HMM ​Hung-Yi LEE 교수님의 강의에서는 seq2seq 중 5개의 모델만 다루셨다.​• Listen, Attend, and Spell (LAS) • Connectionist Temporal Classification (CTC) • RNN Transducer (RNN-T) • Neural Transducer • Monotonic Chunkwise Attention (MoChA) 위 그래프는 2019년도 논문을 바탕으로 각 모델의 사용 빈도수를 통계한 것이다.LAS가 가장 많이 사용되고 있는 것을 볼 수 있다.​​다음 포스팅에서 seq2seq 모델들을 본격적으로 다룰 예정이다(~˘▾˘)~ "
크롬 웹스토어 확장 프로그램 - Speech Recognition Anywhere ,https://blog.naver.com/rheecastro/222726533682,20220509,"[크롬 브라우저]에서 자판을 손가락으로 입력하지 않고 음성으로 말하면 텍스트로 전환하여 구글검색 등 자판으로 해야 할 모든 일들을 대신할 수 있다. 크롬 브라우저의 Google 검색창에 [크롬 웹스토어]라고 입력하여 검색한 후Chrome 웹 스토어를 클릭한다.[Chrome 웹 스토어] 왼쪽 위의 입력창에 [speech recognition anywhere]를 입력하고 Enter키를 눌러 앱을 검색한 후 앱을 클릭한다.우측의 [Chrome에 추가]를 클릭한다.[확장 프로그램 추가]를 클릭한다.[Speech Recognition Anyshere이(가) Chrome에 추가됨]이라는 메시지가 나타나며 Chrome 화면에 마이크 아이콘이 생성된다. [ x ]를 눌러 상자를 닫는다. 구글 검색창 오른쪽의 핀 모양으로 생긴 [확장 프로그램] 아이콘을 클릭하면 검색창 옆에 마이크모양의 [Speech Recognition Anywhere] 아이콘이 활성화된다. 이제 크롬 브라우저 안에서 키보드(자판)을 타이핑하지 않고 음성으로 말하여 원하는 내용을 검색하거나 입력하기 위해서 [Speech Recognition Anywhere] 아이콘을 클릭한 후, 말을 하면 제대로 음성을 텍스트로 변환하여 타이핑이 되는지 테스트하기 위해서 컴퓨터 마이크나 노트북에서 말을 하면 텍스트가 입력되는 것을 확인할 수 있다. 입력된 텍스트를 복사하여 워드, 이메일 등에 붙여넣기 하면 텍스트를 자판으로 입력하지 않고 음성으로 편리하게 입력할 수 있다.​Speech Recognition Anywhere는 처음 1달은 무료로 사용할 수 있고, 그 다음부터 유료버전은 1년에 2만원 정도 하는데, 유료버전을 사용하지 않더라도 1달이 지나고 나서 음성으로 타이핑을 하면 [Free Trial Expired]라는 텍스트가 자동으로 입력되는데, 이 텍스트만 지우면 무료버전을 사용하더라도 전혀 문제없이 음성입력기능을 사용하는데에 문제가 없다. ​구글 검색창이나 구글 앱의 번역 등에서 마이크 버튼을 클릭한 후 음성으로 말을 하면 텍스트로 전환되어 원하는 내용을 찾아준다.  "
Speech Recognition Threshold(SRT) ,https://blog.naver.com/ylhearing/222578429769,20211125,"안녕하세요.​​​​와이엘 보청기 입니다.​​​​오늘은 어음인지역치검사(Speech recognition threshold, SRT)에 대해 ​​알아보겠습니다​​어음인지역치검사란 제시된 이음절어를 50%가량 인지할 수 있는 ​​최소강도레벨을 측정 하는 것입니다.​​어음청각검사의 한가지 종류로써 ​​단어인지도검사(Word recognition score, WRS)와​​ 함께 가장 많이 쓰이고 있는 검사 중 하나 입니다.​​이 검사의 목적은 어음인지 시 필요한 민감성을 측정하여 순음청력검사의 신뢰도를​​ 확인하고 단어 및 문장인지도검사의 기초 자료로써 사용 됩니다.​​검사의 방법으로는 우선 검사용으로 표준화한 어표를 사용합니다.​​만 13세 이상의 경우 일반용 이음절어표(KS-BWL-A)​​만 6~12세의 경우 학령기용 이음절어표(KS-BWL-S)​​만 3~5세의 아동의 경우 학령전기용 이음절어표(KS-BWL-P)를 사용합니다.​​검사를 할때 청력이 더 좋은 귀를 우선 검사하며, 양이 차이가 없을 경우 ​​오른쪽을 먼저 검사합니다.​​목표음은 육성과 CD음원 두가지를 사용할 수 있는데​​목표음을 육성으로 제시할 경우 VU(Volume units) 미터의 눈금이 0 dB에​​위치하게끔 주의 해야합니다.​​다음으로 충분한 강도에서 어표 내 단어를 제시하여 검사에 사용할 ​​목표 단어를 모두 아는지 친숙화 과정을 진행하는데 피검자가 단어를 몰라서 ​​따라 할 수 없는 경우를 배제하기 위해서 입니다.​​본격적인 검사에서는 친숙화 과정에서 사용한 단어를 무작위 순서로 제시하며​​첫 단어의 제시 레벨은 평균순음역치보다 약 30 dB를 더한 강도에서​​시작하여 수정상승법을 사용하여 SRT를 측정합니다.​​보통 SRT와 PTA가 10 dB 이내일 경우 순음청력검사의 신뢰성이 좋다고 판단하며​​그 이상의 차이를 보일 경우 순음청력검사의 신뢰도가 떨어지기 때문에​​순음청력역치를 재측정하는것이 옳습니다.​​하지만 예외도 있는데 급경사형 난청인의 경우 SRT 보다 PTA가 더 나쁠수 있으며​​중추청각처리장애를 가진경우 PTA보다 SRT가 더 나쁠 수 있습니다.​​​​​​​​​​​​​더 궁금하신 사항은​​와이엘 보청기로​​연락주시기 바랍니다.​​​​ ​ "
"감정인식(SER, Speech Emotion Recognition)에서 generalization 기술 ",https://blog.naver.com/ssj860520/222845546936,20220811,"감정인식 데이터셋이 적다.데이터셋으로 성공적으로 학습을 했다고 하자.그런데, 실전에서는 수많은 화자들을 만날텐데 성능을 보장할 수 있을까?training data set과 test data set 이 달라서 생기는 문제를 어떻게 하면 해결 할 수 있을까?​두가지 방법으로 나눌수 있겠다.여러기법을 사용해서 데이터 양을 늘린다. (data augmentation)데이터셋의 domain 이 바껴도 robust 한 성능을 갖도록 학습한다. (cross domain recognition)​data augmentation 방법은 아래와 같이 나눌 수 있겠다.speech 를 perturbation 하는 방법이 있다. spectrogram상의 주파수 축을 스케일링하는 방법GAN 을 이용하여 새로운 도메인에서 새로운 데이터셋을 생성하는 방법이 있다.Adversarial autoencoder 를 이용하여 실제 디스트리뷰선 내의 데이터를 생성하는 방법이 있다.​cross domain recognition 은 딥러닝 기법을 많이 사용한다. 데이터를 건드리기보단 여러 도메인의 데이터를 모아두고 domain 이 바껴도 성능을 보장하는 robust한 classifier를 학습하는 기법이라 볼 수 있겠다.대표적으로autoencoder, DANN, MPGLN 등이 있다.  언어에 따라 감정표현 방식이 달라지는데 이것은 감정인식의 문제이다. 이것을 풀기위해 하는 방법은 언어를 하나로 고정시키고 그 언어에 대해서만 학습을 하고 그 언어 화자에 대해서만  활용하는 방식이 있다. 다른 방법은 아예 다양한 언어 데이터셋을 이용하여 학습하는 방식이있다.​출처A Review on Speech Emotion Recognition Using Deep Learning and Attention Mechanism by Eva Lieskovská *,Maroš Jakubec,Roman JarinaORCID andMichal Chmulík​​ "
Voice Verify& Speech Recognition ,https://blog.naver.com/kenmin/222737493800,20220518,"The world is changing more quickly than ever before, and customers expect technology to help them manage their fast-paced lives. Traditional payments no longer have a place in today's world - what customers really want to do is make instant payments whenever they want. Voice Verify& Speech Recognition with a spoken command can be a game changer. "
(python)openAI의 whisper 음성인식기(speech recognition) 사용방법 ,https://blog.naver.com/ludalee_/222999799143,20230130,"음성인식기 whisper의 사용법을 알아보겠습니다.설치하는 방법은 이 글(음성인식기(speech recognition) OpeanAI whisper 설치 방법)에서 확인해주시고요.본격적으로 음성인식 작업을 하겠습니다. import whispermodel = whisper.load_model(""base"") whisper를 import 하고 음성인식에 사용할 모델을 import 합니다. audio_path = 오디오경로audio = whisper.load_audio(audio_path)audio = whisper.pad_or_trim(audio) 첫째줄에서 audio 경로를 지정하고둘째줄에서 whisper.load_audio 를 사용해서 audio 를 불러옵니다.셋째줄에서 audio 에 대해서 padding 이나 trim 을 합니다. mel = whisper.log_mel_spectrogram(audio).to(model.device) mel_spectrogram 을 만들고 device에 넣어줍니다. _, prob_language = model.detect_language(mel)print(prob_language)//{'en': 0.9966489672660828, 'zh': 0.00024542718892917037, 'de': 0.00010944104724330828, 'es': 0.0006348636816255748, 'ru': 9.441580914426595e-05, 'ko': 0.00048077202518470585, 'fr': 6.433062662836164e-05, 'ja': 0.00021320662926882505, 'pt': 0.00028872702387161553, 'tr': 0.00018219789490103722, 'pl': 2.5662566258688457e-05, 'ca': 2.3241564122145064e-06, 'nl': 5.503377542481758e-05, 'ar': 1.5788715245435014e-05, 'sv': 2.3678128854953684e-05, 'it': 1.9652597984531894e-05, 'id': 5.877604417037219e-05, 'hi': 2.0770034097949974e-05, 'fi': 1.0036293133453e-05, 'vi': 7.173624180722982e-05, 'he': 5.5535883802804165e-06, 'uk': 1.6711479702280485e-06, 'el': 3.549623579601757e-05, 'ms': 7.796740828780457e-05, 'cs': 2.1607336293527624e-06, 'ro': 1.3968771099825972e-06, 'da': 4.708929282060126e-06, 'hu': 1.6291186284433934e-06, 'ta': 1.8439473933540285e-05, 'no': 4.534306299319724e-06, 'th': 1.996626815525815e-05, 'ur': 9.703408977657091e-06, 'hr': 5.190391902942793e-07, 'bg': 2.950136774870771e-07, 'lt': 7.769128842483042e-07, 'la': 4.890768832410686e-05, 'mi': 1.0069128620671108e-05, 'ml': 1.4083244650464621e-06, 'cy': 0.00032019600621424615, 'sk': 3.547139328929916e-07, 'te': 3.733412995643448e-06, 'fa': 1.2975701793038752e-06, 'lv': 2.7308593075758836e-07, 'bn': 3.7048257581773214e-06, 'sr': 1.4403110526473029e-07, 'az': 2.499587878901366e-07, 'sl': 2.3362210868071998e-06, 'kn': 3.00724707358313e-07, 'et': 1.4229432565571187e-07, 'mk': 4.320105162491927e-08, 'br': 4.33654668086092e-06, 'eu': 7.450969405908836e-07, 'is': 2.982015701036289e-07, 'hy': 1.3042999569279345e-07, 'ne': 5.055792371422285e-07, 'mn': 1.6974202310393594e-07, 'bs': 4.2825249124689435e-07, 'kk': 7.433343540697024e-08, 'sq': 1.2784445857505489e-07, 'sw': 5.352275366021786e-06, 'gl': 1.010005235002609e-06, 'mr': 3.3079576411410017e-08, 'pa': 3.3296208812316763e-07, 'si': 2.2592914774577366e-06, 'km': 1.5914507457637228e-05, 'sn': 1.5957174355207826e-06, 'yo': 2.4500509425706696e-06, 'so': 3.7947467390608836e-09, 'af': 2.0474433313211193e-06, 'oc': 1.347752771607702e-07, 'ka': 5.878766984324102e-08, 'be': 8.47744274778961e-07, 'tg': 3.295650419232743e-09, 'sd': 9.263393252467722e-08, 'gu': 7.887302189146794e-08, 'am': 3.786463409483076e-08, 'yi': 6.375511247824761e-07, 'lo': 6.077619474353924e-08, 'uz': 4.0757511254874146e-10, 'fo': 1.0526152038892178e-07, 'ht': 1.5956778725012555e-06, 'ps': 4.1022410357527406e-08, 'tk': 1.6018236737025404e-10, 'nn': 5.645578494295478e-05, 'mt': 1.5313868573230138e-07, 'sa': 3.974182618549094e-06, 'lb': 2.1357160484569704e-09, 'my': 1.6394284330090159e-06, 'bo': 8.573867489758413e-06, 'tl': 1.67225043696817e-05, 'mg': 1.8365102494311003e-10, 'as': 2.885973060529068e-08, 'tt': 4.071099013458479e-09, 'haw': 1.128640906244982e-05, 'ln': 7.000900303921753e-09, 'ha': 6.143791031476553e-10, 'ba': 2.2453207348949178e-10, 'jw': 1.6005236830096692e-05, 'su': 1.5685698295797579e-09} 언어가 무엇인지 분석해봅니다.prob_language에는 어떤 언어일 확률이 높은지 나옵니다.어떤 언어인지 보려면 가장높은 확률을 갖는 언어를 추출하면 되죠 아래와 같이요 language = max(prob_language, key=prob_language.get)print(language) //en 여기서는 en = english 영어이네요 options = whisper.DecodingOptions()result = whisper.decode(model, mel, options)print(result.text)//And it's going to help you to do your marketing. 이제 음성인식을 해보겠습니다.첫번째 줄에서 옵션을 불러오고두번째 줄에서 모델, mel, options을 decode함수에 넣어줍니다.결과가 잘나오는 것을 확인할 수 있습니다.​ "
[논문 요약] 4-bit conformer with Native Quantization Aware Training for Speech Recognition ,https://blog.naver.com/woal1975/222896531504,20221010,"#AI #Native #Quantization #speech #recognition #ASR​저자 : Shaojin Ding, Phoenix Meadowlark, Yanzhang He, Lukasz Lew소속 : Google LLC, USA발표 : Interspeech 2022​초록모델의 크기와 latency를 줄이는 것은 음성인식 응용 시나리오에서 매우 중요한 연구분야이다. 이러한 측면에서 모델 양자화 기술은 뉴럴 네트워크를 압축하고 연산 비용을 줄이는 매우 인기 많은 접근 방법이다. 현재 유행하는 많은 ASR 시스템은 훈련 후 8-bit로 양자화하는 기술이다. 성능 하락 없이 모델을 더 줄이기 위해서 우리는 native quantization aware training 방법을 이용한 4-bit ASR 모델을 제안한다. 이 방법은 native integer operation을 이용하여 훈련과 추론 과정을 모두 효과적으로 최적화할 수 있다. 우리는 이 방법의 효과성을 입증하기 위해 Conformer 모델을 사용하여 두 가지 실험을 진행했다. 첫째, LibriSpeech 훈련 셋을 이용해서 weight와 actication 양자화에 대한 다른 정밀도의 효과를 실험했고 이를 통해 손실 없이 4-bit conforemr 모델을 얻었으며 크기는 float32 모델 대비해서 7.7배나 줄일 수 있었다. 실제 large-scale 훈련 세트에 대해서 4-bit과 8bit의 weight를 섞어서 손실 없는 Conformer ASR을 생성할 수 있었다. ​Introduction​E2E ASR은 과거 hybrid ASR에 비해 크기는 줄고 성능은 더 좋아졌다. 모델의 크기와 반응시간을 줄이는 것은 서버 기반이든 On-Device 기반이 든 중요한 연구주제이다. 이러한 기술로는 networdk pruning, knowledge distilklation, model quantization 등이 있다. 이 중에 quantization의 경우 int8 기반의 post training quantization(PTQ)이 쉽게 구현할 수 있어 많이 사용되었다. TFLite에서 쉽게 변환이 가능하다. PTQ의 단점은 성능 하락이 있을 수 있다는 점과 모델 양자화 과정에서의 통제가 어렵다는 점이다. 즉 int4에 대한 지원이 안되고 특정 레이어에 대한 선택적인 양자화가 불가능하다는 점이다. 이러한 점이 Quantization aware training(QAT)에 대한 필요성을 야기한다. QAT는 몇 가지 옵션이 있다. ""fake""와 native가 그것이다. 양자화 과정에서 quantized operation을 수행하기 위해서 native integer operation을 사용한다면(즉 행렬 곱셈 연산에서) 정확하다고 말할 수 있다. 이 말은 다르게 말해서 훈련과정에서와 추론 과정에서 정확도에서 차이가 없다는 것을 의미한다. 반면에 훈련과정에서는 float 연산을 사용하고  추론과에에서는 integer 연산을 사용함으로써 둘 간의 수치적인 차이가 있다면 ""fake quantization""이라고 말 수 있다. 만일 훈련과정에서 float 연산이 23bit의 가수부를 갖지 않기 때문에 이를 fake라고 부른다. ""당신이 훈련하는 것이 곧 당신이 서비스하는 것이다""라는 접근 방식을 따를 때 이를 native QAT라고 표현한다. Native integer operation이라 함은 훈련과 추론 과정에서의 forward propagation 간에 수치적인 차이가 없는 것을 의미한다. ​본 논문에서는 native QAT만을 사용한다. 이전 연구에서는 fake quantization은 일반적으로 사용했는데, native QAT가 클라우드나 모바링 응용프로그램에서 모두 사용할 수 있는 반면에 ""fake"" quantization은 대부분 모바일 응용 프로그램에서밖에 사용할 수 없고 또한 특별하게 TFlite에 의한 변환 과정을 거쳐야 한다. quantization을 위해 필요한 연산의 수를 최소로 줄였다. int4 weight와 float32 activation 혹은 int4 weight와 int8 activation quantization의 양자화를 사용했을 Librispeech에서 정확도의 하락이 없었다. 다만, Librispeech에서 달리 대용량의 데이터베이스에서는 성능의 하락이 있었다. 이를 보완하기 위해 다양한 전략을 탐구했다. ​이전 연구와의 차이는 아래와 같다. 대용량의 데이터 셋을 사용했다는 점. 다른 연구들은 ""fake"" quantization을 사용했지만 본 연구에서는 native quantization을 이용했다는 점. Native Quantization의 장점은 다음과 같다. a) 모바일과 클라우드에서 동일한 모델을 사용할 수 있다는 점, b) 정수 연산을 지원하는 하드웨어에서는 속도 향상이 보다 크다는 점, 기존의 연구들은 8 혹은 6 bit에 대해서 연구를 하거나 conformer가 아닌 다른 모델을 이용했다는 점이다. ​2. Method​ [그림 1] 연구 사용된 multi-pass conformer 모델의 구조​ LibriSpeech 실험을 위해서는 여러 개의 레이어를 가지는 하나의 encoder와 1개의 LSTM 레이어를 가지는 conformer를 이용했다. 실제 응용을 위한 모델의 경우 multi-pass 구조를 사용했고 첫 번째 causal encoder pass와 그 후에 두 번째 non-causal encoder를 사용했다. 여기서 causal encoder의 경우는 왼쪽 context만을 사용하는 것이고 non-causal의 경우 보다 정확한 결과를 왼쪽과 오른 context를 모두 사용한다. 원 구조와는 달리 우리는 각 encoder에 대해 개별 decoder를 사용했다. decoder는 하나의 embedding prediction network와 1개의 fully-connected 레이어로 구성된 joint network를 가진다. QAT에 대한 TensorFlow(TF)는 ""fake"" QAT에 기반한다. tf.quantization.fake_quant_* 함수에 의존한다. 이렇게 하면 연구자들은 훈련과 sever-side 추론 과정에서 부득이 이 함수들을 사용할 수밖에 없다.  on-device의 경우 fake quantization 연산을 integer 연산으로 변환하기 위해 TFLite를 사용한다. 그런 후 결과 모델을 TFLite와 함께 모바일 폰에서 수행한다. 이러한 과정에 end-to-end 경험을 제공하기는 하지만 몇 가지 단점을 가지고 있다. 1) fake quantization 연산을 integer 연산으로 추가적으로 변환 과정을 거쳐야 한다. 2) tf.quantization.fake_quant_with_min_max_vars_per_channel은 마지막 차원에 대한 채널당의 min과 max만을 지원한다. 그러나,  채널의 차원이 마지막의 것이 아닌 경우도 있을 수 있다. 이러한 경우, 우리는 채널의 차원을 마지막 것에 맞추기 위해 입력 텐서의 차원을 변형시켜야 한다. 그 후에 tf.quantization.fake_quant_with_min_max_vars_per_channel을 이용해서 텐서를 ""fake"" 양자화하고 그 후에 입력 텐서를 원래의 차원으로 복원해야 한다. 이러한 추가적인 연산은 훈련시간을 단순 float 훈련과정에 비해 늘리게 된다. 위와 같은 문제점을 해결하기 위해 우리는 native QAT를  native tf 연산을 이용하는 방식을 제안하다. 이러한 방식을 우리는 Accurate Quantized Training이라는 부른다. 4-bit 모델 양자화의 방법들 중 하나는 QAT fine-tuning에 기반하고 있다. 그것은 몇 가지의 단계를 가진다. float 모델을 훈련한 뒤 그것을 QAT fine tuning을 위해 사용한다. 이러한 접근법은 항상 훈련시간을 늘린다. 반대로, 우리는 quantization aware mode에서는 훈련하는 방식을 이용했고 이는 기존의 float 훈련보다 단지 7% 정도의 속도 증가만 보였다. 우리는 그림 2에 보인 것과 같이 dynamic quantization에 기반한 native QAT에 의해 그러한 결과를 이룰 수 있었다. ​ [그림 2] int8 native quantization in TensorFlow​[그림 2]의 소스 코드를 보면, 우선 양자화되는 축에 대해 최댓값을 찾아낸다. 이것은 채널에 따른 양자화를 지원한다. 그런 후에 int8을 위해 127로 최댓값을 나눔으로써 scale을 계산한다. 그 후에, 입력 텐서는 scale로 나누고 int로 casting 됨으로써 양자화된다. Dequantization은 하나의 텐서를 scale로 곱함으로써 이루어진다. 전체 연산량을 줄이기 위해서 우리는 [22]에 소개된 ""zero point""를 사용하지 않았다. 즉 우리는 입력 텐서 값들의 분포가 대칭적이라고 가정했다. 그것이 매우 비현실적인 가정임에도 불구하고 섹션 4.1에서 보이는 것처럼 정확도의 하락 없이 native 4-bit QAT에 대한 모델의 weight들이 양자화됨을 보여준다. ​3. Experimental setups​훈련셋 : Librispeech, 400,000시간의 영어 테스트 셋: Voice Search 12K voice search utterance. Librispeech 셋을 위한 conformer 모델과 Large-scale 셋을 위한 conformer 모델은 앞서 이야기 한 것과 같이 다르게 구성되었다. ​Conformer model with large-scale data 구성128 차원의 log Mel-filterbank energy4개의 연속 프레임은 쌓고 쌓은 sequence을 3배수로 서브 샘플링한다. causal convolution을 위해서는 15 커널 사이즈를 가진다. causal convolution encoder는 7개의 conformer layers를 가진다. 이때 처음 3개의 레이어는 self-attention이 없다. causal encoder의 경우 23개의 frame으로 왼쪽 context를 사용한다. 오른쪽 context는 사용하지 않는다. non-causal encoder는 6개의 conformer layer를 가진다. 추가적으로 30개의 오른쪽 컨텍스트를 사용한다. 즉 900msec의 미래 값을 사용한다. 모든 self-attention 레이어는 8개의 head를 가진다. 각 RNN-T 디코더는 320차원의 임베딩 prediction network와 384차원의 fully-connected joint network로 구성된다. ​모델 평가는 Tensorflow 그랩을 TFLite 포맷으로 변환한 상태에서 on-device 추론 과정을 통해 이루어졌다. 우리는 어떤 언어 모델도 사용하지 않았다. 오직 encoder에 대해서만 양자화했다. 이는 decoder는 상대적으로 매우 작기 때문이다. 우리는 동일한 이유로 convolution 레이어의 kernel에 대해서는 양자화하지 않았다. ​ [그림 3] convolution kernel 그림. https://towardsdatascience.com/types-of-convolution-kernels-simplified-f040cb307c37 Types of Convolution Kernels : SimplifiedAn intuitive introduction to different variations of the glamorous CNN layertowardsdatascience.com 4. Result​우리는 2가의 실험을 했다. 첫 번째, 다른 양자화 정밀도를 가지고 LibriSpeech 데이터 셋에 대한 다섯 개의 베이스 라인와 비교했다. 이는 conformer ASR 모델에 대한 native QAT의 유효성과 효과성을 보이기 위함이다. 두 번째로 이를 실환경 데이터 셋인 Large scale 데이터 셋에 적용했다. ​LibriSpeech에 대한 실험아래와 같은 종류에 대한 실험을 진행했다. ​Float : float32 weight, float32 activation(baseline)I8W : int 8 weight, float32 activationI4W : int 4 weight, float32 activationI8WA : int8 weight, int8 activationI4WI8A : int4 weight, int8 activationI4WA : int4 weight, int4 activationFakeI4W : fake in4 weight, float32 activation​weight만을 양자화함으로써 모델 사이즈를 줄일 수 있다. 그러나 추론 단계에서는 weight를 float으로 변환해 줘야만 한다. 반대로 activation을 양자화하는 것은 모델의 크기를 줄이고 런타임 효율성을 높일 수 있다. 실험 결과는 아래와 같다. ​ Exploring the limit of 4-bit quantization on large-scale data우리는 LibriSpeech에 대한 성능 저하 없는 int4 quantized model을 쉽게 획득했지만 Large scale에 대해서는 그렇지 못하다는 사실을 알게 되었다. 그뿐만 아니라 성능 저하는 weight quantization만 했을 때도 일어난다. Table 2에 그 결과가 있다.  PTQ는 E0에서는 좋은 성능을 보이지만 E2에서는 측정이 불가능할 정도로 성능이 떨어지는 것을 확인할 수 있다. 반대로 native QAT는 int8 대비해서 각각 0.6/0.4의 성능 하락이 있다. 이를 통해 native QAT의 효용성을 입증된다. LibriSpeech의 경우 높은 over-parameter 모델이라고 할 수 있다. 즉 훈련량 대비 파라미터의 개수가 많다. 즉 overfitting 문제를 내포하고 있다. 반대로 Large-scale 실험의 경우, 훈련량 대비해서 파라미터의 개수가 너무 작다. 때문에 overfitting 문제는 더 이상 없다고 할 수 있다. ​성능 저하를 없애는 3가지 전략을 아래와 같이 제시한다. ​첫 번째와 마지막 layer를 int8로 둔다. 왜냐하면 이 두 개의 레이어는 양자화에 가장 민감하기 때문이다. 그러나 E6의 모델에서 볼 수 있는 바 처럼, 이러한 경향이 더 이상 발견되지 않는다. 즉 전체 causal encoder를 int4로 양자화한 것이랑 차이가 없다. 두 번째는 self-attention layer를 int8로 두는 것이다. 왜냐하면 이것들은 conformer 모델의 가장 본질적이고 근원적인 레이어이기 때문이다. 이를 통해 0.2/0.1의 하락만 있었다. 마지막으로 바닥에서 꼭대기까지의 레이를 순차적으로 양자화한 실험을 통해 처음 3개의 레이어만을 양자화했을 때 0.1의 하락이 있음을 확인했다. 이를 통해 아래쪽의 레이어들이 양자화에 더 강인하다는 사실을 확인할 수 있었다. ​5. Conclusion​본 연구에서 native QAT에 기반한 새로운 접근법을 제안했다. LibrSpeech 실험을 통해서 양자화 구성을 달리했을 때를 비교함으로써 QAT 방식의 효율성과 유효성을 입증했다. Large scale 실험에서 나타난 인식 저하를 방어하기 위한 세 가지 전략을 제안했으며 cost와 accuracy 간의 tradoff에 대해 논의했다. ​ "
(딥러닝 16주차) Speech Recognition ,https://blog.naver.com/z1z11009/222443652124,20210724," Speech recognition - audio clip은 기압의 미세한 차이를 기록한다. 사람의 귀는 아래의 그림과 같이 spectrogram을 만드는 것과 같은 전처리 작업을 진행한다.- 초기에는 음소 단위로 speech recognition이 이루어졌지만, 오늘날에는 더 큰 데이터 셋을 활용하여 end-to-end 알고리즘이 만들어졌다. : Speech recognition에도 attention model이 활용되기도 한다. - 일반적으로 Speech recognition은 input data가 output data보다 많다(초당 100Hz를 활용하면 10초에 1000개의 input이 발생, output은 1000개의 alphabet이 없다)- CTC는 중복되는 단어는 합치고 blank는 제거하여 이를 잘 처리한다.​  Trigger Word Detection : 각종 음성 인식 기계들은 각각 이를 깨우는 키워드들을 지니고 있어 이를 잘 이해할 필요가 있다. - 알고리즘은 Trigger word 직후에 output label이 1이 되도록 한다. 이 때 1보다 0의 개수가 너무 많아 imbalanced이므로, 1을 한 번에 여러개 만들도록 할 수 있다. "
[쉽게 만들어 보세요!] AI 음성인식 API로 음성 변환 서비스 만들기 (feat. CLOVA Speech Recognition) ,https://blog.naver.com/n_cloudplatform/222583556230,20211201,"​ ​AI 음성인식(Speech Recognition) 기능을 활용해서 사람의 목소리를 텍스트로 바꿔주는 서비스를 간단히 구현하는 과정을 설명드리겠습니다. 음성인식 기능은 네이버 클라우드 플랫폼에서 제공하는 클로바 음성인식 (CLOVA Speech Recognition, CSR) 을 활용했습니다.​ 음성인식 서비스 구조도​테스트 가능한 코드 실행 터미널아래의 두 개의 인자는 서비스 활성화가 필요합니다. X-NCP-APIGW-API-KEY-IDX-NCP-APIGW-API-KEYhttps://beta.ryugod.com/pages/shared/python/YAhqBRh_sQ.template<<<2번과 3번 과정을 참조하세요>>​ CLOVA Speech Recognition(CSR) 서비스란?네이버 클라우드 플랫폼에서 제공하는 CLOVA Speech Recognition(CSR)은 어떤 서비스일까요?​사람의 목소리를 인식하여 작동하는 비서 애플리케이션, 챗봇, 음성 메모 등 서비스를 만들 때 활용할 수 있는 음성 인식 API 서비스입니다. 음성 데이터는 API를 통해 CLOVA Speech Recognition(CSR) 엔진으로 전송되며, 해당 음성 데이터를 인식해서 텍스트로 변환하여 전달해 줍니다.​음성 인식(CSR) API를 활용하면 음성을 텍스트로 변환하는 서비스를 만들 수 있는데요, 구축 방법은 아래와 같이 간단합니다.   ​1. 네이버 클라우드 플랫폼 '콘솔(console)'에 접속합니다.​- 콘솔 접속 : https://console.ncloud.com/ * 회원가입 및 로그인이 필요합니다. 네이버 클라우드 플랫폼 콘솔 > Product > AI-Application Service​​2. CLOVA Speech Recognition(CSR) 서비스를 활성화합니다.​저는 테스트를 위해서 사용될 Web 서비스 URL를 임의로 http://csr.csong.kr로 명시하였습니다.​해당 서비스 환경 등록에 대해서는 아래의 공식 매뉴얼 페이지를 참조 바랍니다.https://guide.ncloud-docs.com/docs/naveropenapiv3-application​ CLOVA Sentiment 서비스를 AI Naver service에서 활성화시키는 화면​​3. 생성한 csr-sample-test 를 클릭하여 인증정보를 확인합니다.​Client ID (X-NCP-APIGW-API-KEY-ID) 와 Client Secret (X-NCP-APIGW-API-KEY) 정보를 메모장 등에 복사합니다.​ CLOVA Speech Recognition 서비스에 생성된 인증 정보 확인창​​4. API를 사용해 python으로 서비스를 구현합니다.​하나의 문서를 RESTful API 방식으로 전달하면 서버에서 인식해 하나의 요약 결과를 리턴해주는 API를 사용할 예정이며, python를 통해 간단히 구현을 하였습니다.​아래와 같이 앞에서 메모 해둔 X-NCP-APIGW-API-KEY-ID 와 X-NCP-APIGW-API-KEY 를 치환합니다.language 변수와 content_url 는 음성을 변환하고자 하는 언어와 음성을 변환하고자 하는 파일의 위치에 따라서 달라질 수 있습니다. 녹음된 파일은 mp3, aac, ac3, ogg, flac, wav 이 지원하고 있습니다.​다만 아래와 같이 WINDOWS 10에서 Linux와 동일한 코드로 구동했을 때, 아래와 같은 에러가 발생하였습니다. OSError: [WinError 6] 핸들이 잘못되었습니다File ""C:\Users\user\AppData\Local\Programs\Python\Python39\lib\site-packages\requests\models.py"", line 319, in prepareself.prepare_body(data, files, json)File ""C:\Users\user\AppData\Local\Programs\Python\Python39\lib\site-packages\requests\models.py"", line 485, in prepare_body File ""C:\Users\user\AppData\Local\Programs\Python\Python39\lib\site-packages\requests\utils.py"", line 130, in super_len total_length = os.fstat(fileno).st_sizeOSError: [WinError 6] 핸들이 잘못되었습니다 ​따라서, 아래와 같이 OS 별로 처리 방법을 달리 선언하였습니다. ​LinuxWindows 10URL에서 처리 차이data = ​urlopen(content_url)data =​ requests.get(content_url, stream=True) ​<Linux에서 사용 가능한 예제> #!/usr/bin/env python3#-*- codig: utf-8 -*-import sysimport requestsimport jsonfrom urllib.request import urlopenclient_id = ""﻿X-NCP-APIGW-API-KEY-ID""client_secret = ""﻿X-NCP-APIGW-API-KEY""headers = {    ""X-NCP-APIGW-API-KEY-ID"": client_id,    ""X-NCP-APIGW-API-KEY"": client_secret,    ""Content-Type"": ""application/octet-stream""}language = ""Kor"" # 언어 코드 ( Kor, Jpn, Eng, Chn )csr_rest_api_url= ""https://naveropenapi.apigw.ntruss.com/recog/v1/stt"" content_url = ""https://kr.object.ncloudstorage.com/clova-speech/sample01.ogg"" # 음성 파일 urlurl = csr_rest_api_url +""?lang="" +languagedata = urlopen(content_url)response = requests.post(url, data=data, headers=headers)rescode = response.status_codeif(rescode == 200):    print (response.text)else:    print(""Error : "" + response.text) ​​<WINDOWS 10에서 사용 가능한 예제> #!/usr/bin/env python3#-*- codig: utf-8 -*-import sysimport requestsimport jsonfrom urllib.request import urlopenclient_id = ""﻿X-NCP-APIGW-API-KEY-ID""client_secret = ""﻿X-NCP-APIGW-API-KEY""headers = {    ""X-NCP-APIGW-API-KEY-ID"": client_id,    ""X-NCP-APIGW-API-KEY"": client_secret,    ""Content-Type"": ""application/octet-stream""}language = ""Kor"" # 언어 코드 ( Kor, Jpn, Eng, Chn )csr_rest_api_url= ""https://naveropenapi.apigw.ntruss.com/recog/v1/stt"" content_url = ""https://kr.object.ncloudstorage.com/clova-speech/sample01.ogg"" # 음성 파일 urlurl = csr_rest_api_url +""?lang="" +languagedata = requests.get(content_url, stream=True)response = requests.post(url, data=data, headers=headers)rescode = response.status_codeif(rescode == 200):    print (response.text)else:    print(""Error : "" + response.text) ​5. 파이썬 예제 파일을 실행한 내용을 확인합니다.​위의 예제에서 json body 내용을 같이 출력하여 내용을 확인하였습니다. # python3 sample.py{""text"":""당신의 제안에 실행 가능한 이유는 무엇인가""} ​6. 아래 결과가 추출되었습니다.현재 예시로 아래와 같이 변환하고자 하는 언어 와 음성을 TEXT로 변환하고자 하는 파일을 사용한 결과 아래와 같이 추출되었습니다. 변환하고자 하는 언어 (language)한국어음성을 TEXT로 변환하고자 하는 파일 위치 (content_url)​https://kr.object.ncloudstorage.com/clova-speech/sample01.oggTEXT로 변환된 내용 (text)당신의 제안에 실행 가능한 이유는 무엇인가 ​RESTful API에 대한 각각의 필드에 대해서는 아래 문서를 참고 부탁드립니다.https://api.ncloud-docs.com/docs/ai-naver-clovaspeechrecognition-stt stt (Speech-To-Text) - CLOVA Speech Recognition(CSR)Korean 서비스 이용약관 개인정보처리방침 사업자등록번호: 129-86-31394 통신판매업신고번호: 제2009-경기성남-0510호 대표이사: 박원기 주소: 경기도 성남시 분당구 분당내곡로 117 10층 및 11층 네이버클라우드,​ 13529 고객지원 대표전화: 1544-5876 © NAVER Cloud Corp. All Rights Reserved.api.ncloud-docs.com ​2021년 12월 현재 요금 정책은 아래와 같습니다.   ​ * 본 포스팅은 네이버클라우드 Cloud Advocate 송창안 님이 작성해 주셨습니다.​​ ​​ "
TIL_0622_Speech_Recognition ,https://blog.naver.com/mm323/222784371687,20220622,"Speech Recognition using Neural Network Digital Signal ProcessingSamplingFourier TransformFiltering​ MLCC Training Neural Networks: Best PracticesFailure casesVanishing Gradients: The gradients for the slower layers (closer to the input) can become very small. In deep networks, computing these gradients can involve taking the product of many small terms. When the gradients vanish toward 0 for the lower layers, these layers train very slowly, or not at all. The ReLU activation function can help prevent vanishing gradientsExploding Gradients: If the weights in a network are very large, then the gradients for the lower layers involve products of may large terms. In this case you can have exploding gradients: gradients that get too large to converge. Batch normalization can help prevent exploding gradients, as can lowering the learning rate. Dead ReLU Units: Once the weighted sum for a ReLU unit falls below 0, the ReLU unit can get stuck. It outputs 0 activation, contributing nothing to the network's output, and gradients can no longer flow through it during backpropagation. With a source of gradients cut off, the input to the ReLU may not ever change enough to bring the weighted sum back above 0. Lowering the learning rate can help keep ReLU units from dying.Dropout Regularization: Yet another form of regularization, called Dropout, is useful for neural networks. It works by randomly ""dropping out"" unit activations in a network for a single gradient step. The more you drop out, the stronger the regularization:0.0 = No dropout regularization.1.0 = Drop out everything. The model learns nothing.Values between 0.0 and 1.0 = More useful.​​ "
네이버 클라우드 플랫폼에서 CLOVA Speech Recognition(CSR) 를 이용하여 사람의 목소리를 텍스트로 바꿔주는 서비스 간단히 구현하기 ,https://blog.naver.com/casong99/222547776945,20211025,"​안녕하세요? 오늘은 네이버 클라우드 플랫폼에서 제공하는 CLOVA Speech Recognition(CSR) 를 서비스를 가지고 사람의 목소리를 텍스트로 바꿔주는 서비스를  간단히 구현 해보겠습니다.  테스트 가능한 코드 실행 터미널 ​ 아래의 두개의 인자는 서비스 활성화가 필요 합니다.  X-NCP-APIGW-API-KEY-ID X-NCP-APIGW-API-KEY <<<2번과 3번 과정을 참조 하세요>> https://beta.ryugod.com/pages/shared/python/YAhqBRh_sQ.template ​먼저, 네이버 클라우드 플랫폼에서 제공하는 CLOVA Speech Recognition(CSR) 는 어떤 서비스 일까요?  CLOVA Speech Recognition(CSR) 서비스란? 사람의 목소리를 인식하여 작동하는 비서 애플리케이션, 챗봇, 음성 메모 등의 서비스를 만들 때 활용할 수 있는 음성 인식 API 서비스입니다. 음성 데이터는 API를 통해 CLOVA Speech Recognition(CSR) 엔진으로 전송되며, 해당 음성 데이터를 인식해서 텍스트로 변환하여 전달해줍니다.1. 네이버 클라우드 플랫폼 포탈에 접속을 합니다. (Product>AI-Application Service)  네이버 클라우드 플랫폼 포탈에서 Product>AI-Application Service 접속 하는 화면​2. CLOVA Speech Recognition(CSR) 서비스를 활성화 합니다.저는 테스트를 위해서 사용될 Web 서비스 URL를 임의로 http://csr.csong.kr로 명시 하였습니다.해당 서비스 환경 등록에 대해서는 아래의 공식 메뉴얼 페이지를 참조 바랍니다.https://guide.ncloud-docs.com/docs/naveropenapiv3-application​  CLOVA Sentiment 서비스를 AI Naver service에서 활성화 시키는 화면​3. 생성한 csr-sample-test 를 클릭 하여 인증정보를 확인 합니다.Client ID (X-NCP-APIGW-API-KEY-ID) 와 Client Secret (X-NCP-APIGW-API-KEY) 정보를 기억하거나 메모장에 복사합니다.​  CLOVA Speech Recognition 서비스에 생성된 인증정보 확인창​4. 하나의 문서를 RESTful API 방식으로 전달하면 서버에서 인식해 하나의 요약 결과를 리턴해주는 API를 사용 할 예정이며, python를 통해 간단히 구현을 하였습니다.아래와 같이 앞에서 메모 해둔 X-NCP-APIGW-API-KEY-ID 와 X-NCP-APIGW-API-KEY 를 치환 합니다.language 변수 와 content_url  는 음성을 변환 하고자 하는 언어 와 음성을 변환 하고자 하는 파일의 위치에 따라서 달라 질수 있습니다. 녹음된 파일은 mp3, aac, ac3, ogg, flac, wav 이 지원 하고 있습니다.​아래와 같이 WINDOWS 10에서 Linux와 동일한 코드로 구동 했을때 아래와 같은 에러가 발생 하였습니다.  OSError: [WinError 6] 핸들이 잘못되었습니다 File ""C:\Users\user\AppData\Local\Programs\Python\Python39\lib \site-packages\requests\models.py"", line 319, in prepare self.prepare_body(data, files, json) File ""C:\Users\user\AppData\Local\Programs\Python\Python39\lib \site-packages\requests\models.py"", line 485, in prepare_body  File ""C:\Users\user\AppData\Local\Programs\Python\Python39\lib \site-packages\requests\utils.py"", line 130, in super_len  total_length = os.fstat(fileno).st_size OSError: [WinError 6] 핸들이 잘못되었습니다 그리하여 아래와 같이 OS 별로 처리 방법을 달리 선언 하였습니다.  ​ Linux Windows 10 URL에서 처리 차이 data = urlopen(content_url) data = requests.get(content_url, stream=True) <Linux 에서 사용 가능한 예제> #!/usr/bin/env python3#-*- codig: utf-8 -*-import sysimport requestsimport jsonfrom urllib.request import urlopenclient_id = ""﻿X-NCP-APIGW-API-KEY-ID""client_secret = ""﻿X-NCP-APIGW-API-KEY""headers = {    ""X-NCP-APIGW-API-KEY-ID"": client_id,    ""X-NCP-APIGW-API-KEY"": client_secret,    ""Content-Type"": ""application/octet-stream""}language = ""Kor"" # 언어 코드 ( Kor, Jpn, Eng, Chn )csr_rest_api_url= ""https://naveropenapi.apigw.ntruss.com/recog/v1/stt"" content_url = ""https://kr.object.ncloudstorage.com/clova-speech/sample01.ogg"" # 음성 파일 urlurl = csr_rest_api_url +""?lang="" +languagedata = urlopen(content_url)response = requests.post(url, data=data, headers=headers)rescode = response.status_codeif(rescode == 200):    print (response.text)else:    print(""Error : "" + response.text) <WINDOWS 10에서 사용 가능한 예제> #!/usr/bin/env python3#-*- codig: utf-8 -*-import sysimport requestsimport jsonfrom urllib.request import urlopenclient_id = ""﻿X-NCP-APIGW-API-KEY-ID""client_secret = ""﻿X-NCP-APIGW-API-KEY""headers = {    ""X-NCP-APIGW-API-KEY-ID"": client_id,    ""X-NCP-APIGW-API-KEY"": client_secret,    ""Content-Type"": ""application/octet-stream""}language = ""Kor"" # 언어 코드 ( Kor, Jpn, Eng, Chn )csr_rest_api_url= ""https://naveropenapi.apigw.ntruss.com/recog/v1/stt"" content_url = ""https://kr.object.ncloudstorage.com/clova-speech/sample01.ogg"" # 음성 파일 urlurl = csr_rest_api_url +""?lang="" +languagedata = requests.get(content_url, stream=True)response = requests.post(url, data=data, headers=headers)rescode = response.status_codeif(rescode == 200):    print (response.text)else:    print(""Error : "" + response.text) ​5. 파이썬 예제 파일을 실행한 내용 입니다. 예제 에서 json body 내용을 같이 출력 하여 내용을 확인 하였습니다. # python3 sample.py{""text"":""당신의 제안에 실행 가능한 이유는 무엇인가""} ​6. 현재 예시로 아래와 같이 변환하고자 하는 언어 와 음성을 TEXT로 변환 하고자 하는 파일을 사용한 결과 아래와 같이 추출 되었습니다.  변환 하고자 하는 언어 (language) 한국어 음성을 TEXT로 변환 하고자 하는 파일 위치 (content_url)​ https://kr.object.ncloudstorage.com/clova-speech/sample01.ogg TEXT로 변환된 내용 (text) 당신의 제안에 실행 가능한 이유는 무엇인가 RESTful API에 대한 각각의 필드에 대해서는 아래 문서를 참고 부탁 드립니다.https://api.ncloud-docs.com/docs/ai-naver-clovaspeechrecognition-stt stt (Speech-To-Text) - CLOVA Speech Recognition(CSR)Korean 서비스 이용약관 개인정보처리방침 사업자등록번호: 129-86-31394 통신판매업신고번호: 제2009-경기성남-0510호 대표이사: 박원기 주소: 경기도 성남시 분당구 분당내곡로 117 10층 및 11층 네이버클라우드,​ 13529 고객지원 대표전화: 1544-5876 © NAVER Cloud Corp. All Rights Reserved.api.ncloud-docs.com 현재 요금 정책은 아래와 같이 설정되어 있습니다.  ​ "
Speech Recognition ,https://blog.naver.com/luyttt4/222910520476,20221025,"음성 인식(Speech recognition)AI가 인간의 말을 이해할 수 있도록 돕는 기술.​음성 인식 시스템은 인간이 말하는 것을 인식하도록 훈련되었습니다.​이러한 시스템은 음성 인식 받아쓰기 소프트웨어는 물론 번역 도구와 음성 인식 스마트 스피커의 근간을 이루는 핵심 기술이라고 할 수 있습니다.​기계는 음성을 인식할 수는 있지만 인간이 말을 이해하는 방식으로 이해한다고 할 수는 없습니다.​인간은 문맥에 맞지 않거나 뒤죽박죽 말한 문장도 이해할 수 있지만 기계는 그렇지 않습니다.​'자연어 처리'는 음성 인식 분야의 최근 업적으로, AI가 문법 규칙을 따르고 실제 음성을 분석해 사람들의 복잡한 발화 방식을 더욱더 잘 이해할 수 있도록 합니다. 이를 통해 AI 시스템은 어조나 유머와 같은 요소가 문장의 의미를 어떻게 바꿀 수 있는지를 파악할 수 있습니다.​이러한 음성 인식 기술은 우리가 말하는 것을 글자 그대로가 아니라 그 안에 내포된 실제 의미를 이해할 수 있도록 꾸준한 진화를 거듭하고 있습니다. AI 설계팀이 시스템에 더 다양한 뉘앙스를 학습시키는 방법을 지속적으로 모색하고 있기에, 사람들은 AI와 그 어느 때보다 매끄럽고 자연스럽게 상호작용하고 있습니다.​검색 키워드- 음성인식- 자연어처리- 음성인식 자연어처리 차이- 음성인식 공부- 자연어처리 공부- 음성인식 분야- 자연어처리 분야​신호처리, 언어학, 통계학​스탠포드 대학의 CS224S (Spoken Language Processing)​https://lynnshin.tistory.com/42 음성인식에 필요한 기초개념 1****************************************************************************************************************************** 이 글은 ReadyToUseAI에서 무료로 제공하는 강의를 듣고 정리한 내용입니다...lynnshin.tistory.com https://www.youtube.com/channel/UCpWrFUlwUGZSHVlOT1eD-Wg/videos Ready-To-Use Tech기업에서 의사결정을 하는 투자자, CEO, 기획자들이 기술에 대해 잘 모르는 경우가 많습니다. 예를들어 인공지능을 알고 싶어도 구글에 검색해보면 모두 개발자를 위한 자료들 뿐입니다. 비개발자들을 대상으로 한 기술의 간단한 원리와 비즈니스 관점에서 활용도를 이야기 합니다.www.youtube.com https://drive.google.com/drive/folders/1VPjvaF8KTTufxyC_5pstWiuwXCgyM2eC 21년 4월_음성인식(심규홍) - Google DriveYou are using an unsupported browser. If you see some unexpected behavior, you may want to use a supported browser instead. Learn more Files 210421 Deep Learning for Speech Recognition (Part 1).pdf 210428 Deep Learning for Speech Recognition (Part 2).pdfdrive.google.com ​https://ratsgo.github.io/speechbook/ Homearticles about speech recognitionratsgo.github.io http://dsba.korea.ac.kr/seminar/?mod=document&uid=1883 [Paper Review] Speech to Speech Translation[ 발표 요약 ] 1. Topic Speech to Speech Translation 2. Overview Speech to Speech Translation(S2ST)이란? 특정 언어의 음성을 목표로 하는 언어의 음성으로 변환하는 시스템을 의미한다. 초기 시스템은 기능을 나누어 음성 인식(ASR), 번역(MT), 음성 생성(TTS)을 차례로 수행함으로써 음성 번역을 수행하였다. 최근에 딥러닝과 관련된 방법론들이 발달되면서 음성 번역 과업을 하나의 모델로 수행할 수 있는 End-to-End 방법론이 개발되었다. 오늘 세미나는 구글에...dsba.korea.ac.kr https://jonathan-hui.medium.com/speech-recognition-series-71fd6784551a Speech Recognition SeriesIn this speech recognition series, we start our discussion with the basic phonetics. In the first article, we understand the core…jonathan-hui.medium.com https://ratsgo.github.io/machine%20learning/2017/03/18/HMMs/ 은닉마코프모델(Hidden Markov Models) · ratsgo's blog은닉마코프모델(Hidden Markov Models) 18 Mar 2017 | HMMs 이번 글에선 은닉마코프모델(Hidden Markov Models, HMMs) 을 다루어 보도록 하겠습니다. 순차적인 데이터를 다루는 데 강점을 지녀 개체명 인식, 포스태깅 등 단어의 연쇄로 나타나는 언어구조 처리에 과거 많은 주목을 받았던 기법입니다. 이 글은 고려대 강필성 교수님 강의와 역시 같은 대학의 정순영 교수님 강의, 서울대 언어학과 신효필 교수님 저서, 위키피디아, Speech and Language Processing 3rd edit...ratsgo.github.io https://untitledtblog.tistory.com/133 [머신 러닝/군집화] 가우시안 혼합 모델 (Gaussian Mixture Model, GMM)Gaussian Mixture Model (GMM)은 이름 그대로 Gaussian 분포가 여러 개 혼합된 clustering 알고리즘이다. 현실에 존재하는 복잡한 형태의 확률 분포를 [그림 1]과 같이 $K$개의 Gaussian distribution을 혼합하여..untitledtblog.tistory.com 음성 인식의 개념, MFCC 및 음성 Feature 추출, GMM(Gaussian Mixture Model), HMM(Hidden Markov Model) 등등 음성 인식과 딥러닝 관련해서 시리즈로 정리 되어 있습니다.https://medium.com/@flowergeoji/deep-learning-study-1-9f1c52068a0a Deep Learning StudyPyTorchmedium.com http://neuralnetworksanddeeplearning.com/ Neural networks and deep learningNeural Networks and Deep Learning What this book is about On the exercises and problems Using neural nets to recognize handwritten digits How the backpropagation algorithm works Improving the way neural networks learn A visual proof that neural nets can compute any function Why are deep neural netwo...neuralnetworksanddeeplearning.com https://dbstndi6316.tistory.com/category/%EA%B3%B5%EB%B6%80/ML%26DL?page=7 '공부/ML&DL' 카테고리의 글 목록 (7 Page)dbstndi6316.tistory.com https://wiserloner.tistory.com/1294 딥러닝 speech recognition 모델 개념 정리- 갑자기 생각이 나서 찾아봤더니 좋은자료가 많고 생각보다 이해가 되는것 같아서 이론과 간단히 모델 구조만 정리해봅니다.찾아본 자료는 구글링에서 speech recognition deeplearning을 치면 상위에 올라와있는 angrypark.github.io/speech%20recognition/Deep-Learning-in-Speech-Recognition/를 먼저 봤는데,이곳에서 소개하는 영상 강의인 www.youtube.com/watch?v=RBgfLvAOrss를 보고, 여러 자료들을 검색하고 개인적 지식들로 정리합니다. ...wiserloner.tistory.com ​ "
"강력한 음성 인식 기능과 문서 편집 기능 합치기 ""Combine Powerful Speech Recognition and Document Editing"" ",https://blog.naver.com/hyubwooinfotech/222873565567,20220913,"Speech Recognition and Document Editor​LEADTOOLS Document Editor 는 웹 애플리케이션을 위한 강력한 제로 풋프린트 편집 솔루션(zero-footprint editing solution) 입니다. 이제 음성 인식을 통해 편집기를 사용하여 오디오를 텍스트로 변환하고, 결과를 편집하고, PDF, DOCX 및 TXT를 포함한 다양한 형식 중 하나로 저장할 수 있습니다. ​서버 컴퓨팅이 필요하지 않습니다!​​ 이 모든 기능은 브라우저에서 로컬로 수행되므로 네트워크 연결이 느려지는 문제가 웹 응용 프로그램에 발생하지 않습니다.!!!!!!!!!!!!!!​​ ​​​LEADTOOLS에 대한 Speech Recognition and Document Editing""​자세한 내용이 필요하시면 당사 sales1@hyubwoo.com 로 문의 주시면자세히 안내드리겠습니다.  ​​협우인포테크(주) 2022LEADTOOLS 국내 총판​ 견적요청하기Copyright © 2021 by Hyubwoo. Info. Tech. Co., Ltd. All rights reserved.www.hyubwoo.com ​ "
[Kaggle] TensorFlow Speech Recognition Challenge (Part 1) ,https://blog.naver.com/bandi12424/222931037369,20221117,"https://www.kaggle.com/competitions/tensorflow-speech-recognition-challenge/data​대회목적: 데이터를 다음 12개의 라벨로 분류yes, no, up, down, left, right, on, off, stop, go, silence, unknown​ 라이브러리 임포트import pandas as pd# Mathimport numpy as npfrom sklearn.decomposition import PCA#오디오 분석을 위한 라이브러리 from scipy.fftpack import fft #fas Fourier transform from scipy import signal #신호처리 from scipy.io import wavfileimport librosa #music and audio 분석 from scipy.fftpack import fft# 시각화from PIL import Imageimport matplotlib.pyplot as pltfrom matplotlib.backend_bases import RendererBaseimport seaborn as snsimport IPython.display as ipdimport librosa.displayimport plotly.offline as pypy.init_notebook_mode(connected=True)import plotly.graph_objs as goimport plotly.tools as tls%matplotlib inline 음성 데이터 분석 소리: 특정 주파수를 가지는 sin함수들의 집합 ⇒  특정 시간에 주파수 성분이 어떻게 구성되어있는지를 확인하여 분석할 수 있습니다.​Step 1. 아날로그 데이터인 음성 데이터를 디지털 신호로 변환합니다. train_audio_path = '/kaggle/working/train/train/audio/'filename_yes = 'yes/0a7c2a8d_nohash_0.wav' #label이 yes인 파일 filename_right = 'right/0a7c2a8d_nohash_0.wav' #label이 no인 파일 sample_rate_yes, samples_yes = wavfile.read(str(train_audio_path) + filename_yes)sample_rate_right, samples_right = wavfile.read(str(train_audio_path) + filename_right)print(f""sample_rate = yes: {sample_rate_yes}, right: {sample_rate_right}"")   #samples/sec print(f""samples = yes: {samples_yes}, right: {samples_right}"") #WAV format = 16-bit integer PCM sample rate = 16000 ⇒ 초당 16000개 (16000Hz 주파수)을 가지고 있는 데이터 임을 확인할 수 있습니다. PCM을 이용, 음성 파일을 numpy array 형태로 읽어온 것을 확인할 수 있습니다.  파형 (Waveform )시간에 따른 소리이 진폭을 나타내는 그림, x축: 시간, y축: 진폭전체적으로 단어에 따른 waveform을 관찰해줍니다.   같은 단어라도 발화자에 따라 다른 양상이 나타날 수 있으므로, 이를 관찰해봅니다.  스펙트로그램 (Spectrogram)소리나 파동을 시각화하여 파악하기 위한 도구로, 파형+(waveform)과 스펙트럼(spectrum)의 특징의 조합음향 신호를 주파수, 진폭(강도), 시간으로 분석하여 얻어진 그림으로, x축: 시간, y축: 주파수를 나타내며 시간축과 주파수 축의 변화에 따른 진폭의 차이를 색상의 차이로 나타냅니다.흑백이면 어두울수록, RGB 3색이라면 파랑-초록-빨강 순으로 신호가 강하게 나타나며, 주파수 축의 최대치는 신호의 샘플링 주파수의 절반입니다. 원활한 시각화를 위해 log를 취해준 값을 반환하는 함수를 구성해줍니다.  def log_specgram(audio, sample_rate, window_size=20,                 step_size=10, eps=1e-10):    nperseg = int(round(window_size * sample_rate / 1e3))    noverlap = int(round(step_size * sample_rate / 1e3))    freqs, times, spec = signal.spectrogram(audio,                                    fs=sample_rate,                                    window='hann',                                    nperseg=nperseg,                                    noverlap=noverlap,                                    detrend=False)    return freqs, times, np.log(spec.T.astype(np.float32) + eps) 전체적으로 spectrogram을 관찰해줍니다.  fig = plt.figure(figsize=(15,15))# for each of the samplesfor i, filepath in enumerate(sample_audio[:30]):    # Make subplots    plt.subplot(10,3,i+1)        # pull the labels    label = filepath.split('/')[-2]    plt.title(label)        # create spectogram    samplerate, test_sound  = wavfile.read(filepath)    freqs, times, spectrogram = log_specgram(test_sound, samplerate)        plt.imshow(spectrogram.T, aspect='auto', origin='lower')    plt.axis('off') 같은 단어라도 발화자에 따라서 다양한 양상으로 나타날 수 있으므로, 이 또한 관찰해봅니다. dog_samples = [audio_path + 'dog/' + y for y in os.listdir(audio_path + 'dog/')[:6]]fig = plt.figure(figsize=(10,10))for i, filepath in enumerate(dog_samples):    # Make subplots    plt.subplot(3,3,i+1)        # pull the labels    label = filepath.split('/')[-1]    plt.title('""dog"": '+label)        # create spectogram    # create spectogram    sample_rate, samples  = wavfile.read(filepath)    freqs, times, spectrogram = log_specgram(samples, sample_rate)        plt.imshow(spectrogram.T, aspect='auto', origin='lower')    plt.axis('off') Mel-Spectrogram주파수를 mel-scale로 변환한 형태  오디오 신호에서 FFT를 이용해 구한 스펙트럼에 Mel Filter Bank를 적용한 값Mel Scale: 사람의 청각기관은 고주파수보다 저주파수 대역에 더 민감하므로, 이를 고려해 물리적인 주파수와 실제 사람이 인식하는 주파수의 관계를 표현한 것Mel Filter Bank: Mel Scale에 기반한 Filter Bank Mel--Spectrogram에 Cepstral분석을 이용하여 MFCC를 구하게 됨  #librosa 사용을 위해서 파일을 다시 읽어옵니다. (librosa가 16 bit wav 파일을 처리하는데 문제가 있었습니다.)samples_yes, sample_rate_yes = librosa.load(str(train_audio_path) + filename_yes)S = librosa.feature.melspectrogram(samples_yes, sr=sample_rate_yes, n_mels=128)log_S = librosa.power_to_db(S, ref=np.max)plt.figure(figsize=(12, 4))librosa.display.specshow(log_S, sr=sample_rate_yes, x_axis='time', y_axis='mel')plt.title('Mel power spectrogram ')plt.colorbar(format='%+02.0f dB')plt.tight_layout() MFCC(Mel-Frequency Cepstral Coefficients)오디오 신호에서 추출할 수 있는 feature, 소리의 고유한 특징  Mel Spectrum(멜 스펙트럼)에서 Cepstral(켑스트럴) 분석을 통해 추출된 값mfcc = librosa.feature.mfcc(S=log_S, n_mfcc=13)# Let's pad on the first and second deltas while we're at itdelta2_mfcc = librosa.feature.delta(mfcc, order=2)plt.figure(figsize=(12, 4))librosa.display.specshow(delta2_mfcc)plt.ylabel('MFCC coeffs')plt.xlabel('Time')plt.title('MFCC')plt.colorbar()plt.tight_layout() ​참고문헌https://www.kaggle.com/code/davids1992/speech-representation-and-data-exploration/notebookhttps://www.kaggle.com/code/timolee/audio-data-conversion-to-images-edahttps://en.wikipedia.org/wiki/Temporal_theory_(hearing)  https://en.wikipedia.org/wiki/Place_theory_(hearing)https://en.wikipedia.org/wiki/Spectrogram    https://en.wikipedia.org/wiki/Pulse-code_modulationhttps://brightwon.tistory.com/11https://en.wikipedia.org/wiki/Mel-frequency_cepstrum  https://m.blog.naver.com/sooftware/221661644808https://jonathan-hui.medium.com/speech-recognition-feature-extraction-mfcc-plp-5455f5a69dd9https://www.youtube.com/watch?v=iNbOOgXjnzE ​ "
어떤 단어가 말했는지 결정하기 위해 반응에서 const 페이지를 내보내는 Speech Recognition Hook 추가 ,https://blog.naver.com/geadsff/222698314357,20220412,"반응 앱에 음성 인식 기능을 추가하려고 합니다. 사용자가 '개구리'라는 특정 단어를 말하면 다음 페이지를 열 수 있습니다. 그러나 허용되지 않는 후크이기 때문에 이 줄을 내 페이지에 추가하는 데 어려움을 겪고 있습니다. const { transcript, resetTranscript } = useSpeechRecognition(); ​이 페이지의 버튼은 말한 내용과 사용자가 계속 진행할 수 있는지 여부를 알아야 하므로 새 구성 요소를 만들지 않고 추가할 수 있는 방법이 있습니까?아래는 내 코드입니다. export const AuthPage = () => {    const navigate = useNavigate();    //THIS LINE IS CAUSING THE HOOK ERROR    // const { transcript, resetTranscript } = useSpeechRecognition();const navigateToNextPage = () => {    navigate(`/frogPage/${name}`);};return (    <main className='simple-wrapper'>        <button onClick={SpeechRecognition.startListening}>Say Word</button>        //ONLY SHOW NEXT BUTTON IF THEY SAID THE WORD FROG CORRECTLY        {transcript.includes(""frog"") && (        <button onClick={navigateToNextPage}>Next</button>)    </main>); ​}; ​​귀하의 피드백으로 판단하면 클래스 구성 요소 내부에서 후크를 사용하려는 것 같습니다.솔루션: 래퍼 구성 요소 만들기 function WrapperComponent({ renderChild }) { const result = useSpeechRecognition(); return renderChild(result)}class MyComponent { render() {  return <WrapperComponent    renderChild={({ transcript, resetTranscript }) => (      <main className='simple-wrapper'>          <button onClick={SpeechRecognition.startListening}>Say Word</button>          //ONLY SHOW NEXT BUTTON IF THEY SAID THE WORD FROG CORRECTLY          {transcript.includes(""frog"") && (          <button onClick={navigateToNextPage}>Next</button>)      </main>    )}  /> }} ​원한다면 props.children(함수로)을 직접 사용할 수도 있습니다. 편안한 것을 사용하십시오. ​​확인! 그래서, 나는 이것을 시도했고 유일한 오류가 발생했습니다. 나는 어떤 이유로 useNavigate를 만났습니다. 그래서 react-router-dom@5.2.0더 친숙해서 바꿨습니다.NPM 문서를 읽을 때( doc ). 이것은 브라우저에 따라 다를 수 있습니다. 따라서 폴리필을 추가할 것을 권장하지만 업데이트된 크롬은 이를 지원해야 합니다. (Web Speech API로 인해)나는 react-speech-recognition(3.9.0)과 react version17.0.2를 사용하고 있습니다. 또한 react-speech-recognition 라이브러리는 React hooks를 사용하려면 React 16.8 이상이 필요합니다.아래는 나를 위해 일한 전체 코드입니다. // AuthPage.jsimport SpeechRecognition, {  useSpeechRecognition} from ""react-speech-recognition"";import { useHistory } from ""react-router-dom"";const AuthPage = () => {  const history = useHistory(); // usehistory in place of useNavigate   // const navigate = useNavigate();  const { transcript, resetTranscript } = useSpeechRecognition();  const navigateToNextPage = () => {    console.log(""This works"");    history.push(""/homepage"");    // navigate(`/frogPage/${name}`);  };  return (    <main className=""simple-wrapper"">      <button onClick={SpeechRecognition.startListening}>Say Word</button>      {/* //ONLY SHOW NEXT BUTTON IF THEY SAID THE WORD FROG CORRECTLY */}      {transcript.includes(""frog"") && (        <button onClick={navigateToNextPage}>Next</button>      )}    </main>  );};export default AuthPage; ​만약 당신이 그것을 직접 테스트하고 싶은 경우를 대비하여. 아래는 내가 추가한 앱 페이지로, Say Word 버튼이 표시되어야 하며 '개구리'라고 말하여 음성 인식 후 홈페이지로 이동할 수 있는 방법을 제공합니다. // App.jsimport React from ""react"";import AuthPage from ""./AuthPage"";import Homepage from ""./Homepage"";import { BrowserRouter, Switch, Route } from ""react-router-dom"";// const SpeechlySpeechRecognition = createSpeechlySpeechRecognition(appId);// SpeechRecognition.applyPolyfill(SpeechRecognitionPolyfill);export default function App() {  return (    <div className=""App"">      <h1>Hello CodeSandbox</h1>      <h2>Start editing to see some magic happen!</h2>      <div>{/* <Dictaphone /> */}</div>      <BrowserRouter>        <Switch>          <Route exact path=""/"">            <AuthPage />          </Route>          <Route exact path=""/homepage"">            <Homepage />          </Route>        </Switch>      </BrowserRouter>    </div>  );} ​ // Homepage.jsimport React from ""react"";const Homepage = (props) => {  return <div>Homepage</div>;};export default Homepage; ​또한 솔루션 작업 링크 에 대한 링크 . 또한 웹 페이지에서 마이크를 허용해야 합니다. :)​ "
감정인식(Speech Emotion Recognition) SER의 미래 연구 방향 ,https://blog.naver.com/ssj860520/222832559378,20220728,"감정표현의 요소감정을 표현하는 요소는 무엇일지 연구​발화로부터 감정라벨 결정데이터를 수집하고 라벨링을 할때, 발화를 듣고 라벨링을 하는데 라벨링을 한다는 것이 문화에 따라 달라질 수 있다.​라벨의 주관성라벨을 할 때 주관적인 판단에 의해 라벨링이 되므로 이 주관성을 없애는 방향은?​말하는 스타일에 따른 도메인 mismatch/adaption 한 데이터셋에 대해 학습된 모델이 다른 데이터셋에 적용이 되지 않는 문제가 있다. 트랜스퍼 learning이 이를 해결하는 하나의 방법일 수 있다.​여러개의 corpora사용데이터셋의 양이 작다면 여러 corpora를 쓸수 있다. 근데 corpora마다 차이가 있는것은 사실이다. 이것을 해결하는 방법은 무엇일까? multi task learning과 semi supervised learning을 사용하기도 한다.​라벨링 되지 않는 발화 사용데이터를 수집하고 라벨링하는것은 어려운 작업이다. 라벨링되지 않은 데이터로 학습하는 방법은?​Adversarial learning데이터셋이 동일한 분포를 갖는다고 가정하곤 하는데 이 가정을 틀렸다는 생각에서 출발한것이 adversarial learning 이다. adversarial learning으로 감정인식의 성능을 개선하려는 시도가 있다.​​출처:Youddha Beer Singh, Shivani Goel,A systematic literature review of speech emotion recognition approaches,Neurocomputing,Volume 492,2022,Pages 245-263,ISSN 0925-2312, "
Multi-Dialect Speech Recognition With A Single Sequence-To-Sequence Model 리뷰 ,https://blog.naver.com/smhan3333/222693533677,20220406,"Bo Li, Tara N. Sainath, Khe Chai Sim, Michiel Bacchiani,Eugene Weinstein, Patrick Nguyen, Zhifeng Chen, Yonghui Wu, Kanishka Rao, Multi-Dialect Speech Recognition With A Single Sequence-To-Sequence Model, 2017​1. Introduction7개의 영어권 국가의 영어를 분류하여 인지할 수 있는 single seq2seq 모델을 제안했다.음성 데이터를 Listen, Attend, Spell 의 조합인 LAS 특성을 바탕으로 어느 국가의 영어인지 분리할 수 있다. 기존에는 LM(language model), AM(acoustic model), PM(pronounce model)을 따로 따로 적용하여 음성 데이터를 학습해야했다. 하지만 LAS 특성에 대하여 한 번에 seq2seq를 적용함으로써 모델을 간소화했다.또한 전처리 과정에서 모든 dialect들을 한 번에 하나의 LAS model로 pooling하니, 각 dialect를 각각의 LAS로 따로 fine tuning하여 학습시켰을 때보다 성능이 떨어졌다. 때문에 학습 모델(seq2seq)에 각 방언들의 특성 정보를 넣어 training target을 변경시켰다. 이는 dialect symbol을 original grapheme sequence에 넣어줌으로써 가능했다. 또한, 각 layer에 들어갈 때마다 dialect 특성에 맞게 one-hot-representation 시켜주었다. 이로써 하나의 dialect를 각각의 LAS로 변환하여 학습한 모델보다 성능이 3%~16%가량 더 좋은 모델을 만들게 되었다. ​ "
Conformer: Convolution-augmented Transformer for Speech Recognition ,https://blog.naver.com/woal1975/222806023889,20220710,"https://arxiv.org/pdf/2005.08100.pdf 번역 및 요약, 논리 정리#conformer #speech #recognition #transformer #ASR #end-to-end #convolution #recurrent #CNN #RNN #트랜스포머​저자 : Anmol Gulati, etc. @ Google Inc.​Abstract​최근 Transformer와 Convolution nerual network(CNN)에 기반한 모델들이 음성인식 분야에서 좋은 결과를 보여주고 있다. Recurrent neural network 대비해서도 좋은 성능이다. (※ 역주: transformer란 무엇인가? 트랜스포머 이전에 seq2seq 모델이 있었다. 입력으로 들어오는 벡터 시퀀스를 인코더를 거치면서, 입력 벡터 시퀀스의 하나의 단일 벡터로 압축해 정보를 저장하여 출력하고 이를 디코더가 입력으로 받아 출력 시퀀스를 생성한다. 입력 시퀀스 → [인코더] → 하나의 벡터 → [디코더] → 출력 시퀀스의 과정을 거친다. 입력 시퀀스가 하나의 벡터로 압축되는 과정에서 많은 정보가 소실되는데 이를 보안하기 위해 어텐션이 사용되었다. 어텐션은 입력으로 들어오는 시퀀스를 출력 벡터로 바꾸는 과정에서 입력 시퀀스의 어느 위치에 주의를 주어야 하는지를 스코어로 계산해 내는 것을 의미하는데  다양한 방식으로 구현될 수 있고 그 성능 또한 좋았다. 어텐션 적용이 성공하자 많은 연구자들이 다양한 어텐션 방식을 음성인식 및 번역 시스템에 적용했고, 이후 어텐션만으로 인코더와 디코더를 만들어 내는 것에 대한 연구를 진행했다. 그것이 바로 transformer이다.) Transformer 모델은 콘텐츠 기반의 글로벌 상호작용을  분석하는데 좋은 성능을 보인다. 반면에 CNN은 지역적인 특징을 효과적으로 분석한다. 이번 연구에서는 파라미터를 이용한 효과적인 방법으로 지역적고 전역적인 오디오 시퀀스의 의존성을 모델링 하기 위해 CNN과 transformer를 결합하는 방식으로 최고의 결과를 획득했다. 이와 관련하여, 음성인식을 위한 convolution으로 강화된 transformer를 제안한다. 이름하여 Conformer이다. Conformer는 이전의 Transformer와 CNN에 기반한 모델들이 이룩한 성과를 압도한다. Librispeech를 이용한 평가에서 우리 모델은 언어 모델의 사용 없이도 2.1%/4.3%  WER을 성취했고, 언어 모델을 사용했을 때는 1.9%/3.9%의 WER을 성취했다. 각각 test/testother이다.  또한 10M 파라미터의 작은 모델에서도 2.7%/6.3%의 경쟁력 있는 성능을 보였다. Index Terms : #Speech #recongition, #attention, #convolution neural networks, #Transformer, #end-to-end​1.Introduction​  뉴럴네트워크 기반의 엔드 투 엔드 음성인식 시스템(End-to-end ASR)은 최근 엄청난 성능의 향상이 있었다. 리커런트 뉴럴네트워크 (RNNs)은  사실상의 ASR의 필수 선택지였다. 왜냐하면 음향 시퀀스 안에 존재하는 시계열의 의존성을 매우 효과적으로 모델링 할 수 있기 때문이다[5]. 최근 셀프-에텐션(self-attention)에 기반한 트랜스포머(Transformer) 구조는 긴 거리 사이에 존재하는 상용호작을 정확하게 잡아낼 수 있다는 점과 모델 훈련의 높은 효율성으로 넓은 영역에서 많이 적용되고 있다. 또 다른 방향에서 컨볼류션(convolution) 역시 ASR 분야에서 성공적이었는데, 이는 레이어 간의 지역적인 수용장을 기반으로 좁은 영역의 맥락들을 점진적으로 분석하고 잡아낼 수 있었기 때문이다.   그러나 셀프 에테이션과 컨볼류션를 이용한 모델은 각각 그들 고유의 한계점을 가지고 있다. 트랜스포머가 비교적 길게 퍼진 맥락을 모델링 하기에 좋은 반면, 좁은 영역의 잘게 퍼진 특징 패턴을 추출하는 데는 덜 효과적이다. 다른 한편으로 컨볼루션 뉴럴 네트워크(CNN)은 지역적인 정보를 잘 탐색하기 때문에 비전 분야의 사실상의 필수적인 블록으로 사용되고 있다. 그것들은 지역적인 분석 프레임에 대한 위치 기반으로 공유되는 커널을 학습한다. 그것은 변환 과정에서 분산을 동일하게 유지하고 엣지와 모양과 같은 특징을 잡아낼 수 있다. 지역적인 연결성을 사용하는 것의 한계점은 전역의 특징을 잡기 위해서는 훨씬 더 많은 레이어와 파라미터가 필요하다는 것을 의미한다. 이러한 문제를 해결하기 위해서, ContextNet[10]의 경우, 더 긴 맥락을 찾아내기 위해 레지두얼 블록(residual block) 안에 압착해서 폭발시키는 모듈(squeeze-and-excitation)을 적용했다(참고 그림1 참조).  참고 그림 1. 1차원 Sequeeze-and excitation module, 입력벡터는 처음 컨볼루션 레이어로 들어간다. 그 다음에 배치 정규화와 활성화 함수가 적용된다. 평균풀링이 conv 결과를 1차원 벡터로 응축시키기 위해 적용된다. 그것은 두개의 풀 커넥티드 레이어로 구성된 하나의 bottlenect에 의해 처리된다. 그 출력벡터는 Sigmoid 함수로 들어가서 0과 1사이의 수자로 변환된다. 그 후 타일식으로 이어붙여지고 conv 출력에 포인트단위로 곱해진다. (https://arxiv.org/pdf/2005.03191.pdf 참고)그러나 전체 시퀀스에 대한 전역의 평균으로만 적용했기 때문에 여전히 전역에서 역동적으로 변화하는 컨텍스트를 찾내는 것은 쉽지 않았다.   최근의 연구들은 컨볼류션과 셀프-어텐션을 결합했을 때가 개별적으로 각각 적용했을 때보다 성능이 좋다는 사실을 밝혀냈다[14]. 위치 단위의 지역 특징과 콘텐츠 기반의 전범위의 상호작용을 함께 학습할 수 있다. 동시에 [15, 16]의 논문들에서 분산을 동일하게 유지하는 상대적 위치 정보를 이용하는 증강된 셀프-어텐션을 적용했다. [17]은 입력을 두 가지의 축(self-attention과 conlvolution)으로 쪼개는 다중 가지 구조(multi-branch architecdture)를 제안했다(참고 그림 2 참조).  참고 그림 2: 트랜스포머 블럭의 병목을 납작하게 만든다는 것은 어텐션 대 FFN의 비율을 늘려나가는 것을 의미한다. 이렇게 하는 것이 Long-Short Range Attention(LSRA)안의 어텐션을 좀 더 최적화하기에 좋았다. (https://openreview.net/pdf?id=ByeMPlHKPH 참고)즉 입력은 셀프 어텐이션과 컨볼류션으로 쪼개서 들어가 처리되고 그 결과는 짜깁기된다. [17]의 연구는 모바일 적용을 목적으로 했으며, 기계번역에 큰 향상을 보여주었다.   이런 연구는 ASR 모델에서 어떻게 셀프-어텐션과 컨볼류션을 조직적으로 결합할 수 있는지 보여준다. 우리는 파라미터가 효과적이기 위해서는 지역적인 상호작용과 전역적인 상호작용 둘 다 중요하고 가정했다. 이를 위해, 우리는 셀프 어텐션과 컨볼류션의 새로온 결합을 제안한다. 이는 두 세계를 통틀어 최고의 성능을 보일 것이다. 셀프-어텐션은 전역의 상호작용을 학습할 수 있고, 반면에 컨볼류션은 상대적 시작점을 기반으로 한 지역적인 상호 관련성을 학습할 수 있다. [17, 18]로부터 영감을 받아 우리는 피드 포워드 모듈(feed-forward module) 한 쌍으로 샌드위치처럼 감싸인 구조를 갖는 셀프-어텐션과 컨볼류션의 새로운 조합을 제안한다. 이는 그림 1에 나타나 있다.  그림 1. Conformer 인코더 모델의 구조. 멀티 헤드 셀프 어텐션과 컨볼류션 모둘을 마치 마카롱(※ 역주: 마카롱은 두 겹의 빵이 크림을 감싼고 있는 과자이다) 샌드위치 구조로 감싸고 있고, 각각 레지두얼 연결을 가지고 있다. (※ 역주: 레지두얼 연결은 건너뛰기 연결이라고 해도 된다. 이를 사용하는 이유는 훈련과정의 수렴이 빠르다는 장점 때문이다) 이후 후처리 레이어놈이 적용된다(※ 역주: 레이어놈은 배치놈과 달리 벡터들의 차원별로 분산/평균으로 정규화를 한다)   우리가 제안한 모델을  Conformer로 명명하였고, LibriSpeech에서 세계 최고의 성능을 보였다. 이는 기존에 최고 성능을 보인  Transformer Transducer[7]보다도 test-orther 데이터 셋에 대해 15% 상대적 오류 감소율을 보주었다(※역주: 외부 언어 모델을 사용했을 때 수치이지만, 사용하지 않을 때 역시 큰 폭의 인식률 상승이 있었다 ).  우리는 모델의 크기를 세 가지로 구분했다. 파라미터의 수로 10M, 30M 그리고 118M로 구분하여 훈련했다. 10M 모델의 경우, 비슷한 크기의 다른 연구[10]와 비교했을 때 성능 향상이 있었다. 중간 크기인 30M는 [7]에서 이야기한 139M 모델인 트랜스포머에 대비해서도 우수한 성능을 보였다. 118M 모델의 경우, 언어 모델이 없을 때 2.1%/4.3%, 언어 모델을 사용했을 때 1.9%/3.9%의 성능을 보였다.   우리는 어텐션 헤드의 개수, 컨볼류션 커널의 크기, 활성화 함수, 피드포워드의 위치, 그리고 컨볼류션 모듈을 트랜스포머 기반의 네트워크에 결합하는 전략의 차이가 성능에 어떤 영향을 주는지를 주의 깊게 연구했다. 그리고 각각의 요소들이 성능 향상에 어떤 기여를 하는지 살펴보았다. ​2. Conformer Encoder​음향신호를 처리하는 인코더는 입력 시퀀스를 컨볼루션 서브 샘플링 레이어로 우선 처리를 하고 그 후 다수의 컨포머 블록으로 처리를 한다. 이에 대한 설명은 그림 1에 묘사되어 있다. 우리가 제안한 모델의 특별한 점은 트랜스포머 블록[7, 19]의 위치에 컨포머 블록을 사용한다는 점이다. (※역주 : 참고 그림 3을 참조하라) 참고 그림 3. RNN/Transformer Transducer architecture[7]컨포머 블록은 네 개의 모듈을 함께 쌓아서 만든다. 즉 피드포워드 모듈, 셀프 어텐션 모듈, 컨볼루션 모듈, 두 번째 피드포워드 모듈이다. 섹션 2.1.1 그리고 2.3은 셀프 어텐션, 컨볼류션, 그리고 피드 포워드 모듈을 각각 설명한다. 마지막으로 2.4는 이러한 하위 블록들이 어떻게 결합되는지를 설명한다. ​2.1 Multi-headed self-attention module​우리는 Transformer-XL[20]의 중요한 특징인 상대적인 정형파 위치 인코딩 기술(relative sinusoidal positional encoding)을 이용하면서 동시 다중 셀프 어테이션을 활용했다. 상대적 위치 인코딩 기술은 셀프 어텐션 모듈이 다른 길의 입력에 대해 잘 적용될 수 있도록 하고 가변적인 길이를 가지는 입력들에 대해 인코더의 성능을 보다 안정적으로 만들어준다. 우리는 깊은 모델의 정규화 및 훈련에 도움이 되도록 드롭아웃(dropout)과 함께 선행 정규화 여기 유닛(pre-norm regularizing units)[21,22]을 사용했다(※역주: 선행 정규화 여기 유닛이란 하나의 루트는 레이어놈을 거치며 이후의 레이어로 들어가고 다른 루트는 모든 레이어를 스킵 한다는 의미이다. 이렇게 만들어진 결괏값을 유닛이라고 표현했다). 그림 3은 다중 셀프 어텐션 블록을 나타낸다.  그림 3: 다중 셀프 어텐션 모듈(Multi-headed self attention module). 레이어놈을 거친 유닛들에 대해 상대적인 위치 임베딩과 함께 다중 셀프 어텐션 모듈을 사용했다. 2.2 Convolution module​[17]로부터 영감을 받아, 컨볼류션 모듀리은 게이팅 메커니즘으로 시작한다. 즉 포인트 단위의 컨볼류션과 게이트 선형 유닛(Gated linear unit)을 의미한다. 이후 1차원의 깊이 기반 컨볼루션 레이터를 지난다. 깊은 모델의 훈련에 도움을 주기 위해 배치놈이 적용된다. 그림 2는 컨볼루션 블록에 묘사되어 있다.  그림 2: 컨볼루션 모듈 2.3 Feed Forward Module​  [6]에서 제안한 트랜스포머 구조는 MHSA 레이어 이후에 피드포워드 모듈을 배치하고 두 개의 선형 변환과 한 개의 비선형 활성화를 조합을 그 사이에 집어넣는다. 피드포워드 간의 레지듀얼 연결을 추가하고 이후 레이어놈을 한다. 이러한 구조는 또한 트랜스포머 ASR에 모델에도 적용되었다[7, 24].  우리는 프리놈 레지듀얼 유닛[21, 22]을 따랐고 레이어 정규화를 정규화 유닛과 입력 사이에 첫 번째 선형 레이어 전에 적용했다. 또한 Swish 활성화 함수(참고 그림 2)와 네트워크의 정규화를 돕는 드롭아웃을 적용했다. 그림 4는 피드 포워드 모듈을 나타낸다.  참고 그림 2 : Swich 함수 (참고: https://velog.io/@iissaacc/Swish-function)2.4 Conformer Block​  우리가 제안한 컨포머 블록은 두 개의 피드 포워드 모듈이 MHSA 모듈과 컨볼루션 모듈을 샌드위치처럼 감싸는 구조이다.   이러한 샌드위치 구조는 Macron-Net[18]에서 영감을 받았다. 이 모델은 트랜스포머 블록 안의 오리지널 피드포워드 레이어를 두 개의 절반값으로 두 번 처리하는 피드포워드로 레이어로 바꾸었다. 하나는 어텐션 레이어 이전에 또 다른 하나는 그 후에 위치한다. Macron-Net과 같이, 우리도 피드포워드 모듈 안의 레지두얼 가중치를 절반씩 두 번 가는 구조로 적용했다. 두 번째 피드포워드 모듈은 마지막 레이어놈 레이어 뒤를 따른다. 수학적으로 이것은 컨포머 블록에 들어오는 입력값에 대해 블록의 출력이 아래와 같아진다는 것을 의미한다.    위 수식에서 FFN은 피드포워드 모듈을 의미한다. MHSA는 다중 구조의 셀프 어텐션을 의미한다. Conv는 컨볼류션 모듈을 의미한다.   3.4.3 섹션에서 기존의 FFN 모듈과 절반씩 두 번 처리하는 FFN 모듈 간을 비교하고 논의한다. 우리는 Macaron을 따르는 구조가 그렇지 않고 하나의 피드포워드를 사용하는 Confomer 구조보다 뛰어나다는 점을 발견했다.   컨볼류션과 셀프 어텐션의 조합은 이전에도 연구되어 왔다.  조합하는 방식은 다양한다. 셀프 어텐션으로 컨볼루션을 강화시키는 다양한 옵션은 3.4.2 섹션에서 이야기한다. 우리는 음성인식에서는 셀프 어텐션 이우에 컨볼루션을 쌓는 것이 최고의 성능을 낸다는 사실을 밝혔다. ​3. Experiments​3.1 Data​  LibriSpeech 데이터 셋[26]에 대해 제안된 모델을 평가했다. 이 데이터 셋은 970시간의 레이블 된 음성과 추가적으로 언어 모델 훈련을 위한 800M 개의 단어 토큰으로 구성된 텍스트 코퍼스로 구성되어 있다. 우리는 스트라이드 10ms로 25ms 윈도로부터 80채널의 필터 뱅크 특징 벡터를 추출했다. 또한 마스크 파라미터(F=27)을 가지는 SpecAugment[27, 28]와 최대 마스킹 시간 비율을 (ps=0.05)로 설정한 후 10번의 마스크를 진행했다. 이때 최대 마스킹 시간 비율은 음성의 길이에 ps 배 만큼을 마스킹 하겠다는 의미이다. ​3.2 Conformer Transduce​  우리는 세 가지 모델을 만들었다. 10M의 파라미터를 가지는 작은 모델, 30M을 가지는 중간 모델, 그리고 118M를 가지는 큰 모델이다. 각각은 네트워크의 깊이, 모델의 차원, 멀티 헤드 어텐션의 개수를 가진다. 사이즈라는 조건 안에서 최선의 조합을 선택했다. 모든 모델에서 디코더는 오직 하나의 LSTM 레이어를 가진다. 표 1은 각각의 구조에서 사용된 파라미터를 설명하고 있다.  표 1: S, M 그리고 L 모델에 대한 모델 하이퍼 파라미터. 파라미터의 크기에 대한 조건 안에서 최고의 성능을 내는 조합과 모델을 선택했다.   정규화를 위해 컨포머의 개별 레지두얼 유닛 안에서 출력값으로 나가기 전 즉 모듈의 입력에 더해지기 전에 드롭아웃을 적용했다[29]. 우리는 P=0.1을 사용했다. 다양한 노이즈를 정규화 과정으로써 모델 훈련에 사용했다. 1e-6의 가중치를 가지는 l2 정규화를 모든 훈련 파라미터에 적용했다. 베타1=0.9, 베타2=0.98 그리고 엡실론=10^-9 가지는 아담 옵티마이저[31]를 사용했고, 트랜스포머 러닝 레이트 스케줄을 사용했는데 이는 10k씩 키워나가다가 0.05/(d^0.5)에서 최고값을 찍는 스케줄이다. 이때 d는 컨포머 인코더의 모델 차원이다.   우리는 LibriSpeech의 960시간 전사 문장을 더한 LibriSpeech 언어 모델 코퍼스에 대해 4096 폭을 가지는 3층짜리 LSTM 언어 모델을 사용했다. 960시간의 Libri 전자 문장으로부터 1k 짜리 워드 피스 모델(word piece model)를 이용해서 토크나이징 했다. LM은 dev-set 전사 문장에 대해  단어 레벨 퍼플랙시티(perplexity)가 63.9였다. 얕은 결합을 위한 LM의 웨이트 람다 값은 욕심쟁이 알고리즘 서치를 이용해 dev-set에 대해 튜닝했다. 모든 모델은 Lingvo toolkit[32, https://github.com/tensorflow/lingvo]으로 구현했다.  GitHub - tensorflow/lingvo: LingvoLingvo. Contribute to tensorflow/lingvo development by creating an account on GitHub.github.com ​3.3 Result on LibriSpeech​  표 2는 LibriSpeech test-clean/test-other을 통해 세계 최고의 수준의 모델들과 비교를 한 것이다. 비교에는 WER(Word Error Rate)을 사용했다. 비교 모델은 ContextNet[10], Transformer transducer[7], 그리고 QuartzNet[9]이 포함된다. 모든 결과는 소수점 1자리에서 라운드업했다.  표 2: 최근 발표된 모델들과의 비교 결과. 컨포머는 다양한 모델 파라미터의 크기 제약 상황에서도 일관성 있게 우월한 결과를 보여준다. 10.3M 파라미터 모델에서 동시대의 최고 모델이라 할 수 있는 ContextNet(S) 대비 0.7% 우수한 결과를 보여준다. 30.7M 모델은 139M 파라미터의 Transformer Transducer의 결과보다도 월등하게 우수한 결과를 보여주고 있다.   언어 모델 없이, 중간 크기의 모델의 성능은 Transformer의 성능에 도달한다. 언어 모델을 장착한 상태에서는 비교 대상인 모든 모델 대비 가장 낮은 오류율을 보여주고 있다. 이거시은 단일 네트워크 안에서 트랜스포머와 컨볼루션을 결합했을 때의 효과성을 명백하게 입증하는 것이라 할 수 있다. ​3.4 Ablation Studies​3.4.1 Conformer Block vs. Transformer Block​  컨포머 블록은 다양한 측면에서 트랜스포머 블록과 차이가 있다. 특히 컨볼루션 블록을 포함하는 것과 마카롱 스타일로 FFN으로 블록을 감싸는 것은 뚜렷한 차이점이라고 할 수 있다. 이제부터 트랜스포머와 컨포머모델간의 차이점으로 인한 효과에 대한 추가 연구결과를 소개한다. 표 3은 컨포머 블록에 주어진 각각의 변화의 효과를 보여준다. 모든 변수 중에서 컨볼루션 하위 블록이 가장 중요한 변수이다. 동시에 동일한 파라미터 크기를 가질 때는 단일 FFN보다는 마카롱 스타일의 두 개의  FFN을 사용하는 것이 훨씬 더 효과적이다. swish 활성화 함수를 사용하면 보다 빨리 수렴된다.  표 3: 개별 특징들이 성능에 미치는 효과. 컨포머 블록으로부터 시작하여 우리는 그것의 특징을 제거해 나갔는데 종국에는 심플한 트랜스포머 블록으로 변경된다. (1) Swish를 ReLU로 바꾼다; (2) 컨볼루션 서브 블록을 제거한다. (3) 마카롱 스타일을 단일 FFN으로 바꾼다. (4) 상대적인 위치 임베딩을 가진 셀프 어텐션을 일반적인 셀프 어텐션 레이어로 바꾼다. 모든 추가 연구는 외부 언어 모델 없이 진행했다.  3.4.2 Combination of Convolution and Transformer Models​  우리는 MHSA를 컨볼루션 모듈과 조합하는 다양한 방식의 효과에 대해서도 연구를 했다. 첫째, 컨볼루션 모듈 안의 깊이 단위 컨볼루션을 가벼운 컨볼루션[35]으로 바꾸어 보았는데 dev other 데이터 셋에 대해 특히 성능이 심각하게 하락하는 것을 확인했다. 두 번째, 우리는 컨볼루션 모듈을 MHSA 모듈 앞에 위치시켜 보았는데, dev-other에서 0.1까지 결과가 좋지 않은 것을 확인했다. 또 다른 가능한 구조의 변경은 [17]에서 제안한 것과 같이 입력을 MHSA와 Conv 모듈로 병렬적으로 보내고 그 결과를 짜깁기 하는 것이다. 하지만 그 결과는 우리가 제안한 방식에 비해 성능이 좋지 않았다.   이러한 결과는 표 4에 나와 있으며 컨포머 블록의 컨볼루션 모듈은 셀프 어텐션 후에 위치하는 것이 가장 좋다는 결론을 도출하게 하였다.  표 4:컨포머 컨볼류선 블록에 대한 추가 실험 결과. MHSA과 컨볼루션 블록의 다양한 조합; (1) 컨포머 구조;  (2) 가벼운 컨볼루션 블록의 사용; (3) MHSA 앞에 컨볼루션을 두기; (4) 컨볼루션과 MHSA를 병렬적으로 처리하고 그 결과를 짜깁기 하기[17]3.4.3 Macaron Feed Forward Modules​  트랜스포머 모델에서 사용하는 단일 FFN 대신에 컨포머 모델에서는 마카롱처럼 샌드위치형의 FFN을 사용한다. 더 나가 컨볼루션 피드 포워드 모듈은 절반씩 두 번 가는 레지두얼 방식을 사용한다. 표 5는 단일 FFN을 사용하거나 한방에 건너가는 방식의 레지두얼을 사용하는 것을 현 모델의 방식과 비교하였다.  표 5: 마카롱 피드포워드 모듈의 추가 연구결과. 트랜포머 모델 안에서 사용된 단일 FFN과 현 컨포머 모델에서 사용된 피드포워드 모듈 간의 차이에 대한 추가 실험 결과 (1) 컨포머; (2) 한방에 건너가는 방식의 피드포워드 방식; (3) 마카롱 스타일을 단일 방식으로 바꾸었을 때3.4.4 Number of Attention Heads​  셀프 어텐션 안에 각각의 어텐션 헤드는 입력의 다른 부분에 초점을 두고 학습을 한다. 그것은 단순한 가중치 평균 방식을 넘어서는 향상을 가져왔다. 우리는 큰 모델을 나에서 4에서 32까지 헤드의 개수를 달리하며 그 효과를 연구했다. 한 모델 안의 모든 레이어는 동일한 헤드 개수를 가진다. 16개까지 헤드의 개수를 느려가면 성능이 점진적으로 좋아진다. 결과는 표 6에 나와 있다.  표 6: MHSA의 어텐션 헤드 개수에 따른 실험 결과3.4.5 Convolution Kernel Sizes​  깊이 중심의 컨볼루션안에서 커널의 크기에 따른 효과를 연구하기 위해  큰 모델의 커널 개수를 {3, 7, 17, 32, 65}로 바꿔가면 실험했다. 커널 사이즈가 17과 32일 때 성능 향상이 있었다. 그러나 커널 사이즈가 65가 되었을 때 오히려 성능이 떨어졌다. 이는 표 7에 나와 있다. WER의 소수점 두 자리까지 두고 비교했을 때, 32개의 커널 개수가 가장 좋은 성능을 보였다.  표 7: 커널 개수에 따른 효과 실험 결과4. Conclusion​이 연구에서 컨포머를 소개했다. 엔드 투 엔드 음성인식을 위해 CNN과 transformer의 요소들을 결합한 구조이다. 우리는 각각의 컴포넌트의 중요성을 연구했으며 컨볼루션 모듈의 포함이 컨포너 모델의 성능에 아주 중요하다는 점을 밝혔다. 모델은 더 작은 파라미터로도 기존의 모델 대비해서 좋은 성능을 보였으며 LibriSpeech dataset에서 1.9%/3.9%의 세계 최고의 성능을 보였다. ​​​5. References ​[1] C.-C. Chiu, T. N. Sainath, Y. Wu, R. Prabhavalkar, P. Nguyen, Z. Chen, A. Kannan, R. J. Weiss, K. Rao, E. Gonina et al., “Stateof-the-art speech recognition with sequence-to-sequence models,” in 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2018, pp. 4774–4778. ​[2] K. Rao, H. Sak, and R. Prabhavalkar, “Exploring architectures, data and units for streaming end-to-end speech recognition with rnn-transducer,” in 2017 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU). IEEE, 2017, pp. 193–199. ​[3] Y. He, T. N. Sainath, R. Prabhavalkar, I. McGraw, R. Alvarez, D. Zhao, D. Rybach, A. Kannan, Y. Wu, R. Pang, Q. Liang, D. Bhatia, Y. Shangguan, B. Li, G. Pundak, K. C. Sim, T. Bagby, S.-Y. Chang, K. Rao, and A. Gruenstein, “Streaming End-to-end Speech Recognition For Mobile Devices,” in Proc. ICASSP, 2019. ​[4] T. N. Sainath, Y. He, B. Li, A. Narayanan, R. Pang, A. Bruguier, S.-y. Chang, W. Li, R. Alvarez, Z. Chen, and et al., “A streaming on-device end-to-end model surpassing server-side conventional model quality and latency,” in ICASSP, 2020. ​[5] A. Graves, “Sequence transduction with recurrent neural networks,” arXiv preprint arXiv:1211.3711, 2012. ​[6] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin, “Attention is all you need,” 2017.​[7] Q. Zhang, H. Lu, H. Sak, A. Tripathi, E. McDermott, S. Koo, and S. Kumar, “Transformer transducer: A streamable speech recognition model with transformer encoders and rnn-t loss,” in ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2020, pp. 7829–7833. ​[8] J. Li, V. Lavrukhin, B. Ginsburg, R. Leary, O. Kuchaiev, J. M. Cohen, H. Nguyen, and R. T. Gadde, “Jasper: An end-to-end convolutional neural acoustic model,” arXiv preprint arXiv:1904.03288, 2019.​[9] S. Kriman, S. Beliaev, B. Ginsburg, J. Huang, O. Kuchaiev, V. Lavrukhin, R. Leary, J. Li, and Y. Zhang, “Quartznet: Deep automatic speech recognition with 1d time-channel separable convolutions,” arXiv preprint arXiv:1910.10261, 2019. ​[10] W. Han, Z. Zhang, Y. Zhang, J. Yu, C.-C. Chiu, J. Qin, A. Gulati, R. Pang, and Y. Wu, “Contextnet: Improving convolutional neural networks for automatic speech recognition with global context,” arXiv preprint arXiv:2005.03191, 2020. ​[11] T. N. Sainath, A.-r. Mohamed, B. Kingsbury, and B. Ramabhadran, “Deep convolutional neural networks for lvcsr,” in 2013 IEEE international conference on acoustics, speech and signal processing. IEEE, 2013, pp. 8614–8618. ​[12] O. Abdel-Hamid, A.-r. Mohamed, H. Jiang, L. Deng, G. Penn, and D. Yu, “Convolutional neural networks for speech recognition,” IEEE/ACM Transactions on audio, speech, and language processing, vol. 22, no. 10, pp. 1533–1545, 2014. ​[13] J. Hu, L. Shen, and G. Sun, “Squeeze-and-excitation networks,” in Proceedings of the IEEE conference on computer vision and pattern recognition, 2018, pp. 7132–7141. ​[14] I. Bello, B. Zoph, A. Vaswani, J. Shlens, and Q. V. Le, “Attention augmented convolutional networks,” in Proceedings of the IEEE International Conference on Computer Vision, 2019, pp. 3286– 3295. ​[15] B. Yang, L. Wang, D. Wong, L. S. Chao, and Z. Tu, “Convolutional self-attention networks,” arXiv preprint arXiv:1904.03107, 2019. ​[16] A. W. Yu, D. Dohan, M.-T. Luong, R. Zhao, K. Chen, M. Norouzi, and Q. V. Le, “Qanet: Combining local convolution with global self-attention for reading comprehension,” arXiv preprint arXiv:1804.09541, 2018. ​[17] Z. Wu, Z. Liu, J. Lin, Y. Lin, and S. Han, “Lite transformer with long-short range attention,” arXiv preprint arXiv:2004.11886, 2020. ​[18] Y. Lu, Z. Li, D. He, Z. Sun, B. Dong, T. Qin, L. Wang, and T.-Y. Liu, “Understanding and improving transformer from a multi-particle dynamic system point of view,” arXiv preprint arXiv:1906.02762, 2019. ​[19] S. Karita, N. Chen, T. Hayashi, T. Hori, H. Inaguma, Z. Jiang, M. Someki, N. E. Y. Soplin, R. Yamamoto, X. Wang et al., “A comparative study on transformer vs rnn in speech applications,” arXiv preprint arXiv:1909.06317, 2019. ​[20] Z. Dai, Z. Yang, Y. Yang, J. Carbonell, Q. V. Le, and R. Salakhutdinov, “Transformer-xl: Attentive language models beyond a fixed-length context,” 2019. ​[21] Q. Wang, B. Li, T. Xiao, J. Zhu, C. Li, D. F. Wong, and L. S. Chao, “Learning deep transformer models for machine translation,” in Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics, Jul. 2019, pp. 1810–1822. ​[22] T. Q. Nguyen and J. Salazar, “Transformers without tears: Improving the normalization of self-attention,” arXiv preprint arXiv:1910.05895, 2019. ​[23] Y. N. Dauphin, A. Fan, M. Auli, and D. Grangier, “Language modeling with gated convolutional networks,” in Proceedings of the 34th International Conference on Machine Learning-Volume 70. JMLR. org, 2017, pp. 933–941.​[24] L. Dong, S. Xu, and B. Xu, “Speech-transformer: a no-recurrence sequence-to-sequence model for speech recognition,” in 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2018, pp. 5884–5888.​[25] P. Ramachandran, B. Zoph, and Q. V. Le, “Searching for activation functions,” arXiv preprint arXiv:1710.05941, 2017.​[26] V. Panayotov, G. Chen, D. Povey, and S. Khudanpur, “Librispeech: an asr corpus based on public domain audio books,” in 2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2015, pp. 5206–5210. ​[27] D. S. Park, W. Chan, Y. Zhang, C.-C. Chiu, B. Zoph, E. D. Cubuk, and Q. V. Le, “Specaugment: A simple data augmentation method for automatic speech recognition,” arXiv preprint arXiv:1904.08779, 2019.​[28] D. S. Park, Y. Zhang, C.-C. Chiu, Y. Chen, B. Li, W. Chan, Q. V. Le, and Y. Wu, “Specaugment on large scale datasets,” arXiv preprint arXiv:1912.05533, 2019. ​[29] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov, “Dropout: A simple way to prevent neural networks from overfitting,” Journal of Machine Learning Research, vol. 15, no. 56, pp. 1929–1958, 2014. ​[30] K.-C. Jim, C. L. Giles, and B. G. Horne, “An analysis of noise in recurrent neural networks: convergence and generalization,” IEEE Transactions on neural networks, vol. 7, no. 6, pp. 1424– 1438, 1996. ​[31] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,” arXiv preprint arXiv:1412.6980, 2014. ​[32] J. Shen, P. Nguyen, Y. Wu, Z. Chen, and et al., “Lingvo: a modular and scalable framework for sequence-to-sequence modeling,” 2019. ​[33] Y. Wang, A. Mohamed, D. Le, C. Liu, A. Xiao, J. Mahadeokar, H. Huang, A. Tjandra, X. Zhang, F. Zhang et al., “Transformerbased acoustic modeling for hybrid speech recognition,” arXiv preprint arXiv:1910.09799, 2019. ​[34] G. Synnaeve, Q. Xu, J. Kahn, T. Likhomanenko, E. Grave, V. Pratap, A. Sriram, V. Liptchinsky, and R. Collobert, “End-toend asr: from supervised to semi-supervised learning with modern architectures,” 2019. ​[35] F. Wu, A. Fan, A. Baevski, Y. N. Dauphin, and M. Auli, “Pay less attention with lightweight and dynamic convolutions,” arXiv preprint arXiv:1901.10430, 2019. "
Presentation 4 - Speech Recognition ,https://blog.naver.com/imhotep/222998142746,20230129,"SRS  Service Routine System, 서비스 루틴 시스템SRS  Speech Recognition System, 음성 인식 시스템 (음성)SRS  Student Response System "
TIL_0623_Speech_Recognition ,https://blog.naver.com/mm323/222786719358,20220624,"HMM based speech recognitionCTC end to end ASREnd-to-end Automatic Speech Recognition 개요CTC algorithm: Sequence Modeling with CTC (distill.pub)OverviewTrainingTensorflow implementationDecodingCTC implementation ExamplesDeepspeech2CTC GramCT WordCTC on FPGANeural TransducersCTC의 fieble : 입력(음성 frame)과 출력(text)의 갯수가 다름에도 training가능하도록 만들어준 혁신적인 algorithm이었으나 단어간 conditional independence를 가정하면서  AM(acoustic model) 에서 output label얻은 후 LM(language model)을 이용한 beam-search decoding시 beam width를 키워야 성능이 좋아짐Listen Attend Spell (LAS)Comparision of CTC, Neural Transducers, LASImplementation Considerations​Attention and Augmented Recurrent Neural Networks (distill.pub)​ MLCC Multi-Class Neural NetsOne vs. AllSoftmax​ "
"감정인식(SER, Speech Emotion Recognition) 의 미래 연구과제 ",https://blog.naver.com/ssj860520/222833553930,20220729,"감정인식 중에 특히 Speech Emotion Recognition은 음성신호만을 가지고 감정을 인식하는 태스크이다.감정인식에 주요한 요소로는 네가지가 있다.DatabaseFeature감정인식 모델​Database는 어떤 데이터셋을 이용할 것인지Feature는 감정을 인식하는데 있어서 어떤 수치를 활용할 것인지어떤 classifier를 사용할것인지는 감정인식모델과 관련이 있다.딥러닝 시대 이전에는 고전적인 머신러닝 기법이 사용되었으며 최근에는 딥러닝 기법이 많이 사용된다.​미래 연구과제로는 아래와 같이 정리할 수 있다.databasesfeaturesclassifierscross-lingual emotion recognitionreal-time emotion recognition​1,2,3은 봤던 얘기이고4번은 언어가 달라져도 감정인식 성능을 어떻게 유지할것인지를 5번은 감정인식을 실시간으로 한는 방법에 관한것이다. 감정인식은 어떤 일에 적용되냐에 따라 real time 일 필요가 있고(자동차) 필요가 없을 때(우울증 검증)도 있다.​출처아래의 논문을 참고하고 내생각을 얹힘출처:Youddha Beer Singh, Shivani Goel,A systematic literature review of speech emotion recognition approaches,Neurocomputing,Volume 492,2022,Pages 245-263,ISSN 0925-2312, "
Self-Supervised Learning with Random-Projection Quantizer for Speech Recognition ,https://blog.naver.com/woal1975/222848647235,20220815,"https://arxiv.org/pdf/2202.01855.pdf 번역 및 요약정리저자: Chung-Cheng Chiu, 외 동료들 @ Google Brain in 2022​Abstract우리는 음성인식을 위한 단순하고 효과적인 self-supervised 학습 접근을 소개한다. 이 방식은 마스킹 된 음성신호를 예측하도록 모델을 학습시킨다. 랜덤 주사 양자생성기로 생성된 디스크리트 레이블의 형식을 취한다. 특히, 양자생성기는 랜덤으로 초기화된 행렬을 가지고 음성 입력을 주사하고, 랜덤으로 초기화된 코드 북안에서 가장 가까운 정보를 찾는다(nearnest-neighbor lookup). 자기 지도학습 기간 동안 행렬과 매트릭스는 업데이트되지 않는다. 랜덤 주사 양자생성기는 학습되지 않고 음성인식 모델과는 분리되어 있기 때문에 보다 유연하고 유니버설 음성인식 구조와 잘 호환된다. LibriSpeech에 대해서 우리의 방식은 비스트리밍 모델에서 자기 지도 학습을 했던 인식률 만큼의 성능을 보여주었고, 스트리밍 방식의 w2v-BERT이나 wav2vec 2.0보다 낮은 WER과 latency를 보였다. 다중언어 태스크에서 우리의 방식은 wav2vec이나 w2v-BERT보다 우수한 성능을 보였다. ​Introduction자기 지도 학습(Self-supervised Learning)은 최근에 음성인식 모델링의 질에서 인상적인 향상을 보여주었다. 이러한 학습 방식은 모델이 비지도 데이터로부터 학습할 수 있게 하고 지도학습과 결합해서 음성인식의 정확도를 높여준다. 비지도 데이터로부터 학습할 수 있는 능력은 지도학습 데이터가 부족할 때 특히 도움이 된다. 또한 낮은 자원을 가진 언어나 도메인의 학습에 도움을 준다. 음성인식을 위한 자기 지도 학습의 설계 원칙은 학습 표상에 집중되어 있다. BERT의 성공에 영향을 받아, 음성 분야의 한 가지 연구 경향은 BERT에 영향을 받은 알고리즘을 제작하는 것이다. 음성을 위한 BERT 스타일의 자기 지도 학습에 존재하는 난제는 연속적인 음성신호와 불연속적인 텍스트 토큰 간의 갭을 어떻게 줄일 수 있냐는 것이다. 이러한 문제를 해결하는 한 가지 방안은 음성 표상의 학습을 통해서이다. 또는 양자화된 표상을 학습하는 것이다. 많은 연구들이 음성 표상을 학습하기 위한 효과적인 알고리즘을 제안했다. 그리고 그러한 학습된 표상의 양자화 결과는 발화의 음소들의 연관성(correlation)을 높여주므로 음성인식에 도움을 준다고 밝혀졌다. 표상 학습(representation learning)은 음성 분야에 매우 중요한 주제이지만 그것은 자기 지도 학습과 결합할 때 두 가지의 문제점이 나타난다: (1) 모델 구조의 제한. 표상 학습과 자기 지도 학습을 결합할 때 모델이 다운스트림 태스크(downstream task?)에 효과적임을 유지한 상태에서 음성 표상을 제공하는 역할을 해야 한다. 그러나 효과적인 표상 모델은 다운스트림 테스트에 항상 효과적인 것은 아니다. 예를 들어, 좋은 표상 학습 모델은 발화의 미래 시점의 컨텍스트에 접근할 필요가 있다. 반면에 다운스트림 태스크는 낮은 지연시간이 필요하고 이를 위해 미래 시점에 접근하는 것을 금지하고 있다. (2) 복잡도의 증가. 표상 학습과 자기 지도 학습의 목적 값들은 항상 할당되어 있는 것이 아니고 또한 두 알고리즘을 설계하고 그들의 균형을 찾는 것의 복잡함은 연구 개발을 어렵게 만드는 주요 요인이다. 이러한 복잡성은 단순하고 효과적인 대안을 찾는 것 대신에 보다 복잡한 알고리즘은 설계토록 만들 수 있다.  이 연구에서 우리는 BERT_based Speech pre-Training with Random-projection Qunatizer(BEST_RQ)를 제안한다. 이는 음성인식을 위한 단순하고 효과적인 자기 지도 학습 알고리즘이다. 본 알고리즘은 음성신호를 마스킹하고 그들을 음성인식 모델의 인코더부에 입력한다. 인코더는 마스킹 되지 않은 영역을 기반으로 마스킹 된 영역을 예측하도록 학습한다. 학습의 목표는 랜덤 프로젝션 양자생성기에 의해 제공되는 레이블이다. 랜덤 프로젝션 양자생성기는 음성신호를 랜덤으로 초기화된 행령에 주사를 하고(projection) 랜덤으로 초기화된 코드 북안에서 가장 가까운 벡터를 찾는다. 그 벡터의 인덱스가 타깃 인덱스이다. 프로젝트 행렬이나 코드 북은 학습 단계에서 전혀 업데이트되지 않는다. 양자생성기는 표상 학습을 요구하지 않는다. 그리고 그것은 모델과 분리되어 있어 모델의 구조설계의 제한점 등이 없다. 이러한 단순성에도 불구하고 LibriSpeech에 대해서 알고리즘은 비스트리밍 모델에서의 성능과 유사한 결과를 얻었다. 또한 이전 방식의 스트리밍 방식보다 우수한 결과를 보였다. 다중언어 태스크에서 본 알고리즘은 wav2vec 2.0이나 w2v-BERT보다 우수한 결과를 얻었다. 우리는 표상 학습의 질과 자기 지도 학습의 질 사이의 관계에 대한 분석을 진행했으며 두 개의 목적값이 서로 본질적으로 연관되어 있지 않음을 시연할 것이다. 이러한 관측이 우리가 표상 학습 없이 자기 지도 학습을 설계한 주요한 이유이다. 그리고 자기 지도학습에 대한 새롭고 덜 복잡한 연구 방향을 제시한다. ​Related Work음성인식을 위한 자기 지도 학습에 대한 이전 연구들은 음성 표상을 학습하는 것에 초점을 맞추었었다. wav2vec은 과거의 컨텍스트에 기반한 미래 표상을 학습하기 위해 대조 학습(contrastive learning)을 적용했다.  Contrastive Representation Learninghttps://lilianweng.github.io/posts/2021-05-31-contrastive/contrastive represention learning의 목적은 임베딩 스페이스를 학습하는 것이다. 임베딩 스페이스안에서 유사한 샘플의 쌍은 서로 가까이 존재하고 반면에 유사하지 않은 샘플은 거리를 두게 된다. Contrastive learning은지도학습과 비지도학습 모두에 적용할 수 있다. 비지도 데이타에 적용할때 Constrastive learning은 자기지도학습 방법안에서는 가장 강력한 접근법중하나다.  Contrastive Representation LearningThe goal of contrastive representation learning is to learn such an embedding space in which similar sample pairs stay close to each other while dissimilar ones are far apart. Contrastive learning can be applied to both supervised and unsupervised settings. When working with unsupervised data, contr...lilianweng.github.io vq-wav2vec은 wav2vec을 표상을 학습하는데 이용하고 그것들을 불연속적인 토큰으로 양자화한다. 그리고 표상 학습을 더욱 향상시키기 위해서 BERT-style의 선행학습을 수행한다. Discrete-BERT는 BERT 식의 선행학습모델을 실제 음성인식(downstream task)에 최적화하기 위해서 vq-wav2vec을 확장했다. wav2vec2.0은 마스킹 된 부분의 표상을 예측하기 위해 과거와 현재 양쪽을 이용하는 contrastive learning을 사용했다. HuBERT는 음성신호를 불연속의 레이블에 매핑하는 양자 초깃값을 학습하기 위해 k-meas 알고리즘을 이용한다. 그리고 BERT 스타일의 선행학습을 수행한다. 여기서 입력은 마스킹 된 음성신호이고 예측 타깃은 불연속의 레이블이다. HuBERT는 모델의 새로운 이터레이션을 훈련하기 위한 새로운 양자로써 선행 훈련된 모델을 사용한다. 그리고 선행 훈련된 결과를 반복적으로 증진시키기 위한 반복처리를 수행한다. w2v-BERT는 음성 표상을 학습하기 위한 contrastive 학습을 수행하기 위해 모델의 하위 네트워크를 사용한다. 그리고 BERT-style의 선행 훈련을 수행하기 위해 네트워크의 나머지 부분을 사용한다. w2v-BERT는 representation learning을 훈련과 BERT-style의 선행 훈련을 동시에 진행한다. 우리의 접근법은 representation learning의 필요성이 없다는 점과 음성은 신 모델로부터 양자를 분리한다는 점에서 위의 선행연구들과는 다르다. 우리의 양자화 방식은 입력신호를 랜덤 매트릭스에 주사한다. 그것은 입력 차원을 줄이는 과정과 유사하다고 할 수 있다. 그러한 양자화를 사용하는 것은 자기 지도 학습을 Maked Auto Encoder (MAE)와 구조적으로 유사하다. 그것은 마스킹 된 입력신호를 직접적으로 재생성한다. 컴퓨터 비전 분야에서 또 다른 유사한 연구는 BEit이다. 그것은 VQ-VAE를 양자로써 훈련을 하고 VQ-VAE를 BERT-Style의 자기 지도 학습을 수행하기 사용한다. 이러한 접근법과의 차이는 우리의 알고리즘은 양자를 훈련하지 않는다는 점이고 이는 훈련과정을 더 단순화 시키는 것이다.  BERT style의 훈련법이란? Word Embedding은 NLP를 위한 딥러닝의 기본 기술이다. Word Embedding은 일반적으로 공동출현(co-occurence)에 근거해서 학습된다. 하지만 Word Embedding은 컨텍스트정보를 전혀사용하지 않는다. 컨텍스트 프리 방식으로 인한 문제를 어떻게해결할 수 있을까? 초기에는 Semi-Superviesd Sequence 학습을 사용했따. 이는 일차적으로자기 자신에 대해 LSTM을 훈련한 후 파인튜닝하는 방식이었다. (Semi-Supervised Sequence Learning, Google 2015)후에 ELMo라는 방식이 나왔고 이는 left-to-right와 right-to-left를 분리해서 훈련한 후에 pre-trained Embedding으로 이를 이후 다른 모델링에 사용하는 것이었다. (ELMo: Deep Contextual Word Embeddings, AI2 & Unviers-ity of Washington, 2017)또 Deep Transformer를 이용한  훈련방식이 나왔다. (Improving Language Understading by Generative Pre-Training, OpenAI, 2018)위에서 제시한 다양한 방법의 문제는 언어 모델이 left-context 혹은 right-context만을 사용한다는 점이다. 이에 대한 해결책은 입력 단어 중 K% 만큼을 마스킹하고 이 마스킹된 단어를 예측할 수 있는 모델을 만든 것이다. 하지만 이 방식에도 문제는 있다. 파인튜닝단계에서는 마스킹된 단어는 결코 나타나지 않는다는 것이다. 해결책은 단어들의15%만을 예측하고 모든 시간의 [MASK]로 대치하지 않는 것이다. 대신 훈련의 80%는 [MASK]로 대치하고 10%는 랜던 단어로 대치하고 10%는 원래의 단어를유지하는 것이다.  ​3. Self-superviesd Learning with Random-projection Quantizer 그림 1. BEST-RQ의 전체 그림. 이 방식은 입력신호를 랜덤하게 초기화된 코드 북에 주사하기 위해 랜덤 프로젝션을 활용한다. 그리고 그것들은 코드 북 내에서 가장 가까운 벡터를 찾음으로써 불연속의 레이블에 매핑된다. 선행 훈련의 목적은 ASR의 인코더가 마스킹 된 입력 신호를 받은 후 그에 대응되는 불연속의 레이블을 찾는 데 있다. BEST-RQ는 ASR의 인코더를 위해 BERT-style의 선행 훈련을 가능하게 하기 위해  음성신호를 불연속의 레이블에 매핑하기 위해 랜덤 프로젝션 양자를 사용한다는 점이다. 양자는 하나의 매트릭스와 코드 북을 랜덤하게 초기화한다. 그리고 그 매트릭스를 입력신호를 주사하기 위해 사용하고 코드 북은 가장 가까운 벡터를 찾는데 사용한다. 이때 벡터의 인덱스가 곧 레이블이다. 선행 훈련 과정은 음성신호를 마스킹하고 그것을 ASR 인코더에 넣고 마스킹 된 부분의 레이블을 예측하기 위해 ASR 인코더를 훈련한다. 랜덤하게 초기화된 매트릭스와 코드 북은 선행 훈련과정 동안 모두 고정된다. 입력 데이터는 평균값 0과 표준편차 1을 가지는 값들로 정규화된다. 정규화 과정은 랜덤 주사가 코드의 작은 하위 집안으로 붕괴되지 않도록 해주기 때문에 매우 중요한 과정이다. 전체적인 프레임워크는 그림 1에 묘사되어 있다. 선행 훈련 후에 ASR 인코더는 실시간 음성인식 과제를 수행하기 위한 최적화 훈련을 받게 된다. 이 접근 방식은 음성신호를 직접적으로 마스킹 한다. 마스킹 전략은 모든 프레임에서 고정된 확률 값으로 샘플링을 하는 것이다. 각각의 마스킹 넓이는 시작하는 프레임에서 고정된 길이를 가진다. 마스킹 된 부분은 평균 0과 0.1의 표준편차를 가지는 노이즈 값들도 대치된다. ​3.1 Random-projection Quantizer입력 벡터 x가 주어질 때, x는 d차원의 벡터이고 랜덤 주사 양자는 x를 불연속의 레이블 y에 매핑한다.  A는 h x d 크기의 매트릭스로 랜덤 초기화된다. C는 h 차원을 가지는 랜덤으로 초기화된 벡터들의 집합이다. norm2()는 벡터를 unit l2 norm을 가지는 벡터로 정규화한다. 주사 매트릭스 A는 Xavier 초기화를 사용하고 코드 북 C는 표준 정규 분포를 따른다. 파라미터들은 선행 훈련 동안 고정된다. 그러므로 양자는 훈련 내내 상숫값으로 존재한다. ​3.2 Pre-training선행 훈련 과정에서 ASR 인코더의 꼭대기에 양자화된 음성 레이블을 예측하기 위해 소프트맥스 레이어를 추가한다. 랜덤 주사 양자는 음성인식 인코더에 대해 독립적이기 때문에, 선행 훈련은 유연하고 ASR 인코더가 다른 구조를 가진다 하더라 작동 가능하다. 우리는 실시간 모델과 비실 시간 모델 모두에 대해 알고리즘의 효과성은 검증했다. 그리고 우리의 실험에는 빌딩 블록으로 conformer를 사용했다. ​3.2.1 Non-streaming modelsBERT-style의 선행 훈련은 비실시간 훈련 방식으로 설계되었기 때문에 양자화된 레이블의 예측을 위해 현재와 과거의 컨텍스트를 문제없이 모두 사용한다. ​3.2.2 Streaming models비실시간모델에 덧붙여, 실시간간 구조는 음성인식에서 중요하다. 반응시간의 속도는 음성인식 분야에서 중요한 성능지표이다. 그러나 실시간 구조는 비실 시간 방식에 비해 자기 지도학습에 대한 연구가 많지 않았다. 이전의 자기 지도학습 방식 중 대다수가 과거와 미래의 컨텍스트를 활용하는 방식이었다. 이를 어떻게 실시간 분야로 일반화시킬 수 있는지는 중요한 연구과제이다. 우리는 실시간 구조에서 활용할 수 있는 두 개의 선행 훈련 알고리즘을 제안한다. ​Streaming-pre train우리의 알고리즘은 양자화 과정에 대한 학습이 필요 없고 단지 ASR의 훈련에만 초점을 맞추기 때문에 실시간 ASR에 매우 큰 도움이 된다. 실시간 모델에 대한 선행학습은 비실 시간 모델에서와 동일하게 진행하고 ASR 인코더는 단지 과거의 컨텍스트에 기반하여 마스킹 된 부분에 대한 양자화 레이블을 예측하는 것만 학습을 한다. ​Non-Streaming pre-trainTransformer/Conformer와 같은 뉴럴네트워크의 구조가 미래의 컨텍스트를 추가함으로써 비실 시간 모델에서 실시간 모델로 바꿀 수 있다면 실시간 모델에 대한 비실시간 셋업으로 선행학습을 수행할 수 있다. 우리의 알고리즘은 비실시간과 실시간 선행학습을 이용할 수 있다는 점에서 실시간 모델에 대해 큰 장점을 가지고 있다. ​3.3 Fine-tuning선행학습 이후에, 선행학습으로 ASR의 인코더를 초기화한다. 그리고 지도학습 셋을 이용해서 정밀 훈련을 한다. 선행학습에서는 사용하지 않았던 소프트맥스 레이어를 추가한다. 우리는 RNN-Transducer로 end-to-end 모델을 이용했다. 예측 네트워크에는 LSTM을 사용했다. 인코더에는 ASR 태스크의 적응에 도움이 되도록  추가적인 주사 레이어를 더했다. 지도학습 기반의 정밀 학습과정에서 인코더는 업데이트된다. ​3.4. Understanding the Effectiveness of the Random-projection Quantizer우리의 알고리즘은 자기 지도 학습을 위해서 랜덤 주사 양자를 사용하는데 이러한 방식은 두 가지 의문점을 내포하고 있다. 양자 변환기의 양자화 질이 얼마나 좋아야 하는가? 그리고 양자화의 질은 자기 지도 학습의 효율성에 어느 정도의 영향을 주는가? 우리는 VQ-VAE와의 비교를 통해 이러한 질문에 대한 답을 찾았다. 음성신호를 양자화하기 위해 랜덤 주사방식을 사용하는 것은 VQ-VAE와 동일하다. 랜덤 주사방식은 음성신호에 대한 차원 감축을 수행한다. 반면에 랜덤 코드 북은 음성데이터의 분포의 불연속적인 표현을 제공한다. VQ-VAE 또한 음성신호에 대한 불연속적인 표상을 제공한다. 그러나 그것은 음성데이터를 가장 잘 보존하는 잠재 공간(latent space)에 대한 표상을 학습함으로써 이를 수행한다. 그러므로 VQ-VAE와의 비교 작업은 양자의 질과 자기 지도 학습에 대한 표상 학습의 효과성에 대한 통찰을 제공해 줄 수 있다. 우리는 4.3절에서 랜덤 주사 양자의 양자의 질이 이상적이지는 않지만 VQ-VAE와 비교했을 때 자기 지도학습에 꽤 유용하다는 점을 보여줄 것이다. 우리는 또한 양자의 질의 차이가 비지도 데이터의 양이 커질수록 큰 문제가 되지 않는다는 점을 보여줄 것이다. 음성인식을 위한 자기 지도학습의 가장 중요한 목적은 모델이 컨텍스트 정보를 학습할 수 있도록 학습하게 하는 것이다. 랜덤 주사 양자는 음성데이터의 분포를 보존한다. 그리고 모델이 마스킹 된 신호에 기반하여 양자화 토큰을 예측하도록 학습할 수 있도록 하기 위해서는 모델이 원신호를 처리하도록 학습해야 하고 음성데이터 사이에서 컨텍스트 정보를 추론해야 한다. 그러한 규정은 모델이 랜덤 주사 양자를 이용하여 효과적으로 자기 지도 학습을 수행할 수 있게 한다. ​4. Experiments비실시간과 실시간 모델을 이용해서 LibriSpeech에 대한 자기지도 학습 실험을 수행했다. 그리고 비실시간 모델을 이용해서는 다중언어 태스크 실험도 수행했다. 우리는 VQ-VAE로 학습된 양자와 우리의 방식을 비교함으로써 랜덤 주사 양자의 양자 실에 대한 연구도 수행했다. 이를 수행키 위해 Linbo library를 사용했다.  Lingvo library란?Tensorflow내의 seuqnce model의 뉴럴 네트워크를 제작하기 위한 제공되는프레임워크이다.  ​4.1. LibriSpeechLibriLight 데이터 셋을 이용해서 pre-training 수행  ﻿※LibriLight 데이타비지도학습 혹은 제한된 지도학습을 위한 데이타 셋이다. 영어로 구성되었으며, 60K 시간의 오디오로 구성되었고 이는 현존하는 가장큰 무료데이타 셋이다. 오디오는 voice activity detection, SNR로 태깅되어 있고, 화자 ID와 장르표식이 되어 있다. 크게 아래와 같은 3가지 셋팅으로 구성되어 있다. (1) zero resource/unsupervised setting(ABX)(2) the distant supervision setting(PER, CER)(3) limited textual Setting(10 minutes to 10 hours) aligned with the speech 정밀 학습을 위해 LibriSpeech 훈련 셋을 사용80-d log-mel filter bank coefficients10ms frame shiftvocab size 10241024 toke WordPiece model​4.1.1 Non-Streaming models2 convolution layers4times temporal dimention reductiona stack of conformer models0.6B model size24 layers of conformer​Pre-train400ms with masking probability of 0.01transformer learning rate scheduleAdam optimizer with 0.004 peak learning rate 25000 warm up stepsbatch size 2048vocab size of codebook is 8192, and dimention is 16​Fine-tune2 layers of unidirectional LSTM, hidden dimension of LSTM are 1280Transformer learning rate schedule인코더가 미리 초기화되기 때문에 디코더보다 인코더의 learning rate은 더 작다. 인코더는 0.0003 peak learning rate을 사용했고 5000 warmup step을 가진다. 반면에 디코더의 경우 0.0001 peak learning rate을 사용했고, 1500 warmup step을 가진다. ​LM에 대해서는 shallow fusion을 이용해서 결합했다. LM은 0.1B의 Transformer model을 LibriSpeech LM 코퍼스를 이용해서 훈련했다. 모델은 8개의 레이어를 가지고 1024의 모델 차원을 가진다. 4096 feed-forward 차원을 가진다. ​4.1.2 Streaming modelsstreaming 실험은 Yu et al. 2021을 따랐다. 모델의 크기는 non-streaming 실험에서 사용했던 것과 동일하게 0.6B로 키웠다. 3개의 conformer 레이어를 아래에 두었고 그 바로 위에 시간압축을 위한 stacking 레이어를 두었다. 20개의 conformer 레이어를 stacking 레이어 위에 두었다. confomer는 self-attention을 위한 1024개의 hidden 차원을 두었고  feed-forward레이에는 4096차원을 두었다. self-attention 레이어는 현재와 이전 64 frame에 대해 attend 한다. 컨볼류션은 현재와 이전 3개의 프레임을 커버하는 커널을 가진다.  참조: FastEmit: Low-latency streaming ASR with Sequence-level emission regularization @ Jiahui Yu, Googl LLC, USA훈련 셋업은 대부분은 non-streaming 실험에서와 동일하다. 다만 다른 pre-training 접근법을 위해서 마스킹 비율을 일부 변경되었다. ​0.6B 크기의 모델3개의 conformer layer2times temporal deimension reduction20 conformer layers on the top of the stacking layerConformer has 1024 hidden dimension for the self-attention layer4096 for feed-forward layersself attention layer attend to current and the previous 64 framesconvolution has a kernel that covers the current and the past 3 frames​Streaming pretrainstreaming pre-training은 원래의 구조와 동일한 셋업을 사용했고 마스킹 길이는 300ms이고 마스킹 확률은 0.02이다. 랜덤 주사 양자생성자는 2개의  프레임을 쌓아서 진행했다. 300ms masking length0.02 masking probabilityrandom projection quantizer stack every 2 frames​Non-streaming pre-trainnon-streaming pretraining은 미래의 3개의 프레임에 접근할 수 있는 conformer layer를 가지는 convolution 커널을 이용해서 미래의 컨텍스트에 접근할 수 있도록 확장했다. self-attention은 이전 컨텍스트에 대해서만 접근하도록 제약했다. self-attention을 위한 미래 컨텍트 사용하는 것에 대한 탐구도 진행했다. 마스킹의 길이는 400ms이고 마스킹 확률은 0.02이다. ​Fine-tuneASR의 훈련에는 디코더로서 일방향 LSTM의 레이어를 가지는 RNN-T가 사용된다. LSTM의 은닉층의 차원은 640이다. 훈련 셋업은 non-streaming 실험에서와 동일한 0.6B 모델을 사용했다. non-streaming pre-training된 모델로부터 초기화를 했으며 convolution은 모델의 streaming이 가능하도록 이전 컨텍스트만 활용하도록 했다. ​Latency Measurementstreaming 모델은 미래의 컨텍스트에 접근하도록 예측을 지연하도록 학습할 수 있고 예측의 정확도를 높일 수 있다. 그러므로 동일한 지연시간을 가지는지를 확인하기 위해 streaming 모델의 지연시간을 측정하는 것은 매우 중요하다. 이러한 정보는 예측의 정확도와 지연시간간의 trade off 대신에 실질적으로 향상이 있는지 가늠할 수 있도록 한다. 지연 식간 비교는 두 모델로부터 나온 각각의 예측에 대해 각 단어의 시작 시간과 끝 시간을 계산을 통해 이루어진다. 두 모델로부터 나온 예측을 단어별로 할당을 하고 동일한 단어를 찾고 그들 간의 시간과 끝 시간의 차이를 비교한다. 상대적인 지연시간 측정은 모든 매칭된 단어들 간의 시간 차이의 평균값이다. 상대적인 지연시간은 아래와 같이 계산된다.  i는 두 개의 예측 사이의 매칭되는 단어들의 인덱스이다. j는 발화 음성의 인덱스이고 s[i][j]와 e[i][j]는 baseline 모델로부터의 단어들의 시작과 끝 시간을 의미한다. s'[i][j]와 e'[i][j]는 비교할 모델의 시작과 끝 시간을 의미한다. N은 모든 발화 음성에서 매칭되는 단어의 개수이다. 상대적 지연시간이 음수를 가진다면 이는 비교되는 모델이 baseline 모델 대비해서 낮은 지연시간을 가진다는 의미이다.  표 2. LibriSpeech를 이용한 동일한 streaming 구조를 가지는 이전 연구들과 비교 결과, LibriLight는 pretrain을 위해서 사용했고 LibriSpeech 960h는 find-tuning을 위해서 사용했다. 상대적인 지연시간은 0.1B의 conformer baseline 모델과의 비교를 통해 이루어졌다. BEST-QR이 wav2vec 2.0과 w2v-BERT 대비해서 WER과 지연시간에서 우수한 성능을 보이는 것을 확인할 수 있다. WER과 상대적인 지연시간을 표 2에 제시한다. 이 비교 실험에서 wav2vec 2.0과 w2v-BERT는 BEST-RQ와 동일한 구조를 가지고 동일한 마스킹과 훈련 셋업에서 진행했다. wav2vec 2.0과 w2v-BERT에 일반적인 마스킹 셋업을 사용했을 때 성능이 더 떨어지는 것을 확인했다. 바닥에 convolution 레이어가 없기 때문에 contrastive learning은 음성 그 자체를 타깃으로 사용한다. w2v-BERT 모델은 contrastive 모듈을 위해 12 레이어를 사용하고 마스킹 예측 모듈을 위한 또 12개의 레이어를 사용한다. 이는 non-streaming setup과 동일하게 맞추기 위해서이다.  w2v-BERT pretraing framework. w2v-BERT는 feature encoder, constrastive module 그리고 masked LM(MLM) 모듈이고 구성되어 있다. 뒤쪽 두 개의 모듈은 모두 conformer block들을 쌓아서 만든다. N과 M은 각각 두 모듈의 conformer block의 개수이다. 참고 논문:  w2v-BERT: Combining constrastive learning and masked language modeling for self-supervised speech pre-training , @MIT, Google Brain, 2021우리의 알고리즘은 wav2vec 2.0과 w2v-BERT 대비해서 streaming과  non-streaming pre-traing 모두에서 우수한 성능을 보였다. 이는 앞선 두 방식이 모두 non-streaming 구조에 더 적합하기 때문이 아닌가 한다. 모델의 크기를 0.1B에서 0.6B로 늘렸을 때 지연시간에서 약간의 상승이 있었다. 자기 지도 학습 알고리즘으로 훈련된 모델들은 streaming pre-training이 가장 유의미한 지연시간 절감에 영향을 주는 것과 함께 모두 낮은 지연시간을 보였다. 이것은 자기 지도 학습 방법이 성능은 높이면서도 지연시간은 늘리지 않는다는 것을 보여준다. ​4.2 Multiingual Tasks우리는 이번 섹션에서 다중언어 결과를 보여주고자 한다. 우리는 LibriSpeech non-streaming 실험과 동일한 모델 셋업을 이용했다. ​4.2.1 DataMultilingual LibriSpeech(MLS-10hrs)다중언어 LibriSpeech 데이타셋은 lIBRIVOX의 오디오북을 읽은 것으로부터 파생된 매우 큰 코퍼스이다. 이는 8개의 언어로 구성되어 있다. Dutch, English, French, German, Italian, Polish, Protuguese, Spanish이다. 이 코퍼스의 가장 최근 버전은 영어 44k 시간을 포함해 총 50k 시간으로 구성되어 있다. 우리는 *few-shot learning 능력치를 평가하기 위해 훈련 데이터의 10시간 조각을 이용했다. (*역주 : few-shot learning은 meta learning의 한 종류)​Multlingual Voice Search(VS-1000hrs)ASR의 훈련을 위해 Voice Search Dataset을 사용했다. 15개의 언어 DB로부터 1000시간을 랜덤 샘플링했다. 15개의 언어는 English(US), English(IN), Spanish(US), Portuguese(BR), Spanish(ES), Arabic(GULF), Arabic(EG), Hindi(IN), Marathi(IN), Bengali(BD), Chinese(TW), Russian(RU), Turkish(TR), Hungarian(HU), Malay(MY)이다. ​XLS-R unsupervised data(XLS-R -U)공개된 레이블 되지 않은 음성 데이터는 XLS-R을 위해 사용된 pre-training 데이터를 사용했다. 이때 라이선스 문제로 VoxLingua-107은 사용하지 않았다. 전체적으로 51개 언어에 대해 429k 시간의 레이블 없는 음성 데이터를 사용했다. 결과적으로 우리의 모델은 51개의 언어의 음성데이터로 pre-train 했다. XLS-R는 128개의 언어로 훈련되었다. 우리의 pre-training 셋은 6.6k 시간이 더 작다. 우리는 공개된 결과와 비교하기 위해 MLS-10hrs에 대한 pretrain 데이터를 사용했다. ​Youtube unsupervised data(YT-U)pretrain을 위해 다국어 유튜브 데이터 셋을 수집했다. 각각의 언어를 위해 VAD(아래 그림 참조)를 이용한 세그먼티드된 레이블 되지 않은 유튜브 데이터 셋을 준비했다.  참고 : Feature Learning with Raw-Waveform CLDNNs for Voice Activity Detection @Google, 2016언어당 시간은 다음과 같다. English(800k hrs), Spanish(800k hrs), Marathi(600k hrs), Portuguese(800k hrs), Russian(800k), Arabic(800k), Hindi(800k), Chinese(800k), Malay(250k), Turkish(800k), Bengali(800k), Hugarian(300k). 실질적으로 우리는 이 데이터가 XLS-R-U보다 보다 우수한 성능을 보이는 것을 확인했다. 그러므로 우리는 이 데이터를 VS-1000hrs에 대한 이 전 연구들과의 비교를 위해 선행 훈련 데이터로 사용했다. ​4.2.2 Result on MLS-10hrs우리는 다국어의 작은 리소스를 이용한 MLS-10hrs에 대한 음성인식 실험을 진행했다. 우리는 XLS-R-U를 pretraining 훈련 데이터로 사용했다. 그리고 그것으로 MSL-10hrs에 대해 파인튜닝했다. 표 3에서 보이는 것처럼, 우리의 baseline w2v-BERT는 이미 XLS-R로부터 생성된 이전 모델 대비해서 우수한 성능을 보였다. 평균 WER이  "
자폐와 말 인지(Speech Recognition) 1 ,https://blog.naver.com/worldviews/221692290035,20191029,"요즘 나는 자폐와 말 인지에 대해서 공부를 하고 있다. 뭐. 공부라기보다는 관련된 연구들을 살펴보고 내가 초은이를 더 잘 도울 방법을 생각하고 있다. 초은이를 어렸을 때부터 쭉 지켜보니, 아주 어렸을 때는 아예 귀머거리인 것처럼 보였다. 불러도 대답도 없었고, 돌아보지도 않았다. 마치 소리를 못 듣는 아이 같았다. 시간이 지나고 수용 언어가 아주 조금씩 늘면서, 초은이가 듣기는 듣는다는 것을 확인할 수 있었다. 하지만 말을 제대로 알아듣지 못했고, 지시사항을 잘 이해하지 못했다. 그렇게 시간이 흘러 초은이가 10살이 되었고, 이제 초은이는 웬만한 말을 알아듣는 아이가 되었다. 정상 발달 아동이었다면 만 2세 정도에 거의 모든 말을 알아들었을 것이다. 정상 발달 아동과 비교하면 초은이의 말 인지는 비교할 수 없을 정도로 발달이 느렸다. ​초은이와 같은 자폐 아동이 말 인지가 느리거나 문제가 있는 이유는 뭘까? 우선 결론부터 말해서 이 문제에 대한 답은 아직 존재하지 않는다. 또한 자폐 아동이 말 인지에 문제가 있는 근원적 원인에 대한 연구가 부족한 것도 현실이다.   말 인지(Speech Recongnition)이란?말 인지라는 것은 말의 뜻만 보면 소리로 된 언어정보를 해석해서 의미를 형성하게 되는 과정을 말한다. 그렇다면 인간은 어떻게 말 인지를 하는 걸까? 정상적인 언어발달을 이룬 사람들은 누구나 아무런 노력을 하지 않아도 자신의 모국어를 듣고 순식간에 그 의미를 파악하는 능력을 가지고 있다. 아이들도 만 2세 정도가 되면, 어휘나 사고의 한계가 있기는 하지만 모국어에 대한 말 인지 능력은 이미 상당한 수준으로 발달한다. 즉, 말 인지라는 것은 거의 모든 인간에게 자연스럽게 주어진 능력이다. ​언어학자들은 오래전부터 이 신비에 대해서 연구를 해왔지만, 인간이 어떻게 말을 듣고 인지하는지, 그 방식과 원리에 대해서는 아직도 정확하게 밝히지 못했다. 언어학자들은 다양한 학문적 기반을 중심으로 인간의 말 인지를 설명하려고 하지만 어느 이론도 하나 만족스러운 결과를 제시하지 못했다. ​언어학의 여러 분야 중에 컴퓨터 언어학Computational Linguistics라는 분야가 있다. 이 분야의 목표는 언어 구조와 말 인지를 체계화해서 컴퓨터가 인간의 말을 이해하게 만드는 것이다. 현재 컴퓨터 언어학자와 컴퓨터 공학자들은 컴퓨터가 문어written language를 이해하는 기술을 거의 완성했다. 물론 완벽하다고 말할 수는 없지만 컴퓨터는 문어를 이해하는 번역할 수 있는 어느 정도의 능력을 갖게 되었다. 그래서 현재 컴퓨터 언어학자들의 목표는 컴퓨터가 인간의 말을 듣고 이해하게 만드는 것이다. 이 과정은 다음과 같은 도식을 사용한다. 컴퓨터가 인간의 말소리를 마이크를 통해서 듣고, 그 소리를 정확하게 문어로 전환한다. 그리고 컴퓨터는 문어를 해석한다. 그래서 최근의 말 인지는 컴퓨터가 정확하게 구어를 문어로 바꾸는 것에 집중하고 있다. ​하지만 이것은 인간의 말 인지와 상당히 다르다. 인간의 말 인지는 말소리를 바로 해석하는 것이 핵심이다. 인간이 말을 이해하기 위해서 문어, 즉 글자가 필요하지 않기 때문이다. 어느 누구나 한글을 배우기 전에 말을 배우고, 평생 한글을 깨우치지 못하는 사람들도 말소리를 통해서 언어를 완벽하게 이해하고 사용한다. 즉, 언어의 본질은 소리이지, 글자가 아니다. 그렇기 때문에 인간의 말 인지는 구어를 문어로 바꾸는 과정 없이, 말소리를 바로 이해하는 과정이다. ​결과적으로 인간이 이런 방식으로 말 인지를 할 수 있는 것은 인간의 고유한 청각 기관과 뇌를 가지고 있기 때문이다. ​#자폐 #언어치료 #말인지 #컴퓨터언어학 #언어학 #speechrecognition   자폐 아동의 청각적인 문제로 고민이 있다면 아래 링크를 클릭하세요. 필터드 사운드 트레이닝 홈페이지로 이동할 수 있습니다.   AIT 청각통합훈련 필터드 사운드 트레이닝AIT 청각통합훈련은 청각적 문제를 해결하는데 도움이 되는 훈련입니다. 자폐스펙트럼장애, 발달지연, 전반적발달장애, 청각과민, 청각처리장애로 인한 문제들을 완화하는데 도움이 됩니다.filtered.co.kr ​ "
"(감정인식, Speech Emotion Recognition, SER) speaker independent SER, speaker dependent SER  ",https://blog.naver.com/ssj860520/222843783734,20220809,"감정인식에 대해 알아봅시다.감정인식에서 화자가 다르면 어떤 일이 벌어질까요?말하는 사람에 따라 감정을 표현 하는 방식이 달라집니다.화자가 갖고 있는 배경? 성향? 성대 구조? 등에 따라서 스펙트로그램상에 찍히는 감정적인 요소가 달라질 수 있다는 얘기입니다.​이렇게 화자에 따라 감정인식 성능이 달라지곤 하는데요. 화자에 대해 dependent하냐 independent 하냐에 다라 감정인식을 두가지로 나눌 수 있습니다.화자와 연관있는 speaker dependent SER이 있고, 화자와 연관없이도 성능을 발휘하면 좋은 speaker independet SER이 있습니다.​말만 들어도 spekar independant SER이 어려워 보이죠? 화자가 누구이든 상관없이 감정적 요소를 뽑아내야 하기 때문이죠.​정리해서 보면speaker independant SER은 source domain 에 나타난 화자와 target domain 에 나타난 화자가 다른 SER을 의미합니다.speaker dependant SER은 source doamin 에 나타난 화자와 target domain 에 있는 화자가 같은(혹은 부분적으로 같을 때)  SER을 의미합니다.​출처C. Lu, Y. Zong, W. Zheng, Y. Li, C. Tang and B. W. Schuller, ""Domain Invariant Feature Learning for Speaker-Independent Speech Emotion Recognition,"" in IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 30, pp. 2217-2230, 2022, doi: 10.1109/TASLP.2022.3178232. "
파이썬을 이용한 음성인식 - Speech Recognition ,https://blog.naver.com/jcosmoss/221720733555,20191128,파이썬을 이용한 음성인식을 해보자. ​- 윈도우10- 파이썬 3.7​먼저 명령프롬프트를 관리자 모드로 실행한다. ​pip install SpeechRecognitionpip install pipwinpipwin install pyaudio​간단히 코드를 작성하자.  import speech_recognition as srr =  sr.Recognizer()with sr.Microphone() as source:    print('Speak Anything : ')    audio = r.listen(source)    try:        text = r.recognize_google(audio)        print('You said : {}'.format(text))    except:        print('Sorry could not recognize yur voice') 영어만 된다. 다른 음성인식을 위해서는 recoginize_google 대신에 recoginize_google_cloud를 이용해야 할것 같은데 결제수단을 등록하라고 해서 빠른 포기 했다.   ​​​ 
"원어민 수준으로 발음 개선! neo CEFR ""Speech Recognition 기능 활용"" ",https://blog.naver.com/seein3/222295156175,20210401,"​​연습을 통해서만 확보되는 유창한 영어 실력,너무나 당연한 말!​효과적인 연습을 돕는 많은 도구들...저마다 ""내가 최고지!""를 말하기에""무엇이 다른가요?""라는 반복된 질문을 받는다.​솔직한 나의 대답은잘 몰라요. 저희 것 소개도 버거워요저희 도구를 열심히 설명드릴테니다른 도구와 비교하시고직접 결정해 주시면 어떨까요?​​자신감 부족? 무책임?아니... 솔직한 마음을 표현한 것이고어떤 도구든 많은 고민 속에서 개발되었을 터,적당히 아는 척 말하는 것을 피하고 싶을 뿐.​ ​​게임 기술이 결합된 neo Study AppCEFR 인증 레벨업이 목표​1단계 Preview 듣고 이해, 예습2단계 Repeat 구체적인 이해, 퀴즈3단계 Record 발음과 스피킹​그리고 소개하고 싶은4단계 Speak 원어민 수준 발음 연습!이 단계가 전체 5단계 중긴장, 몰입도가 가장 높다. 5단계 레슨은 순서대로 진행되며,앞 선 단계를 완료해야 다음 단계가 진행됨(레슨별 5~25분) ​음성인식 기술을 활용한 4단계 Speak 레슨을 완료하면5단계 Review 배운 내용을 체득 레슨은 흔한 말대로 ""껌!!"" ​CEFR 레벨업이 목표인 neo.30년 이상 학습 데이터 분석이 바탕이 된Speech Recognition 기술을 활용한다.​문장별로 3번의 Speech Recognition 기회. 정확히 발음하고, 반복적인 도전을 하도록계속된 시각, 청각 자극을 주는 방법으로Study App 스스로 학습 동기 유지에 기여한다.​​""Can do...""CEFR 설명에서 많이 강조되는활용 가능한(can do..) 영어 실력.​Speech Recognition 기술을적극 활용하면 영어 발음과 표현을유창하게 해낼 수 있다.  누구나 Can do....​하나 더Practice makes perfect!neo CEFR 음성인식 기술 소개 영상  ​ 대학생, 직장인 3개월 CEFR 레벨업 자유토커 제안서​ "
한국어 음성 인식 - Korean Speech Recognition [Project] ,https://blog.naver.com/sooftware/221700966254,20191107,"Korean Speech Recognition Application < 한국어 음성 인식 >""Sooftware""​​Korean Speech Recognition using PyTorchNaver 2019 Hackathon - SpeechTeam Kai.Lib​​Korean Speech Recognition GitHub Repositoryhttps://github.com/sh951011/Korean-Speech-RecognitionDemo Application Repositoryhttps://github.com/sh951011/My-Capstone-Application  Model ArchitectureBased on ""Seq2seq""  seq2seq ​Architecture  Model Architecture ​Model Based on IBM Pytorch-seq2seq  Our Model  Seq2seq with Attention  Hyper Parameters  Hyperparameters  Data​A.I Hub에서 제공한 1,000시간의 한국어 음성데이터 사용​Data format​● 음성 데이터 : 16bit, mono 16k sampling PCM, WAV audio                                                ● 정답 스크립트 : 코드와 함께 제공되는 Character level dictionary를 통해서 인덱스로 변환   Dataset folder structure          FeatureMFCC (Mel-Frequency Cepstral Coefficient)    SpecAugmentation    Score    Reference    Requirements    License    Demo Application​※  시연 영상  ※  ​학습되지 않은 팀원의 목소리로 테스트 진행기존에 학습했던 단어들에 대해서는 꽤 정확하지만,처음보는 단어들에 대해서는 부정확한 정확도를 보임​​100시간의 한정된 데이터로 진행한 한계라고 판단 후이후 1000시간의 AI Hub에서 제공한 한국어 음성 데이터로 프로젝트 진행중 "
"음성인식 실습 - 파이썬(Speech Recognition, PyAudio) ",https://blog.naver.com/bananacco/221852534315,20200314,"쉬는 동안 팀원들과 간단하게 회의를 했는데기존 Final project 주제에 음성인식이나 챗봇 기능을 넣어보자는 의견이 있었습니다.​그래서 음성인식에 관심이 생겨 Youtube에서 간단한 내용을 따라 해봤는데너무 신기해서 음성인식 관련 포스팅을 진행하려고 합니다.​다음 주부터 자격증, OpenCV, 음성인식을 공부해야 해서 정신없을 것 같은데잘 정리해보겠습니다. Speech Recognition / PyAudio 설치저는 맥을 사용하기 때문에 Mac 버전 설치 방법을 알려드리겠습니다.1. Speech Recognition파이썬의 음성인식 라이브러리인 Speech Recognition을 사용하겠습니다. 이 라이브러리는 Converting Speech To Text 개념으로 입력된 음성을 text로 변환해 줍니다.​Speech Recognition을 설치하는 방법은 간단합니다. jupyter notebook / terminal에서 pip을 이용해 설치해 주면 됩니다.  출처 : https://www.simplifiedpython.net/speech-recognition-python/ 2. PyAudio PyAudio는 microphone을 활용하는 기능을 제공합니다.홈페이지에서 설치 방법이 나옵니다. https://people.csail.mit.edu/hubert/pyaudio/ mac은 Homebrew 환경에서 설치해야 합니다.터미널에서 아래와 같이 입력해 주시면 됩니다. brew install portaudiopip install pyaudio 간단하게 설치가 끝났습니다.​ 음성 텍스트 변환 실습내용은 내용에 첨부한 유튜브 영상을 따라서 진행했습니다.입력된 음성을 문자로 변환해 주는 코드입니다. import speech_recognition as srimport pyaudior = sr.Recognizer()with sr.Microphone() as source:    print('Speack Anything :')    audio = r.listen(source)        try:        text = r.recognize_google(audio)        print('You said : {}'.format(text))    except:        print('Sirry could not recignize your voice') 위의 코드는 영문자로 변환해 줍니다.그렇다면 한글 텍스트로 변환해 주는 것은 어떻게 할까요?간단합니다. language='ko-KR'를 추가해 주면 됩니다.  신기하게 코드 몇 줄로 제가 한 말을 텍스트로 변환이 가능합니다.​API는 구글을 활용했습니다.​​  참고https://www.youtube.com/watch?v=K_WbsFrPUCk ​ "
Pushing the Limits of Semi-Supervised Learning for Automatic Speech Recognition ,https://blog.naver.com/sooftware/222277974745,20210317,"Pushing the Limits of Semi-Supervised Learning for Automatic Speech Recognition  Yu Zhang et al., 2020Google Research, Brain Team  Reference​- Wav2vec 2.0 Summary- Conformer Summary  Summary​- 현재 Papers with Code 기준 ASR 부문 State-Of-The-Art- Conformer + Wav2vec 2.0 + Noisy Student Training- 1.3%/2.6%/1.4%/2.6% on the dev/dev-other/test/test-other sets ​Wav2vec 2.0 Pre-training - 53,000이라는 대량의 Unlabeled speech data로 학습- Pre-training 과정  - Waveform에서 CNN을 이용해서 피쳐를 뽑음  - 이를 Vector Quantization을 통해 one-hot-vector로 만들고 Embedding matrix를 내적하여      token화 함  - 일정 비율로 Masking하고 다음 Token이 뭔지 알아맞추게하는 Masked Language Modeling      (MLM) 학습 방식 적용​Vector Quantization - Z를 선형변환하여 logit을 만듦 - 여기에 Gumbel Softmax와 argmax를 취해 one-hot vector를 만듦 - 이후 Embedding matrix를 내적해 Z^를 만듦​Conformer​- Self-attention 기반한 트랜스포머는 global-context 정보를 잘 표현하지만, local-context에서는   부족하다는 단점이 있음- 반면, CNN 기반 모델은 local-context는 잘 표현하지만 global-context를 반영하기 위해서는   적당한 dilation과 깊은 구조를 가져야 함- 이 두 방법을 결합하여 global-context와 local-context 모두 잘 표현할 수 있도록 하기 위한   transformer + CNN 결합구조인 Conformer 구조 제안  Method​본 논문에서 실험한 모델 구조 및 트레이닝 방법​Model Architecture​Conformer Encoder + LSTM decoder로 이루어진 Transducer 구조 - 기존 Conformer 구조에서 Wav2vec 2.0 Pre-training을 도입하기 위해 Masking하는 과정과 Linear Layer (Quantization 대체) 추가- 사이즈별로 L, XL, XXL로 구분- XXL+는 Conformer XXL에 Conformer block을 stack (XXL보다 50M 파라미터 추가) ​Wav2vec 2.0 Pre-training- 60k Libri-Light 데이터셋 사용- 기존 논문과 달리, 인풋으로 log-mel spectrogram 사용- Masking 된 인풋과 예측한 context vector 간의 contrastive loss로 학습​Noisy Student Training with SpecAugment Experiiments - 결과적으로 Pre-training + NST가 좋은 성적을 냄 "
"[Speech synthesis and recognition by Holmes & Holmes, (2001)] - Summary of ch.8 ",https://blog.naver.com/dorin0123/222186409081,20201226,"Ch.8 Intro to automatic speech recognition General principles of pattern matching● early methods (e.g., Hyde (1972): rule-based approaches, which was not very successful due to co-articulation and difficulty of phone identification● pattern-matching techniques: one way is to store example acoustic patterns (called templates) for all the wordsDistance metrics● Filter-bank analysis: describes the speech as a sequence of feature vectors, which can be compared with stored templates for all the words in the vocabulary using a suitable distance metric● Normalization- adding a small consonant to the measured level before taking logarithms- the square of the Euclidean distance in the multi-dimensional space● Dynamic time warping (DTW)- can deal with sequences of connected words ( ⇒ can solve the end-point detection problem) by matching one word on to another in a way which applies the optimum non-linear timescale distortion to achieve the best match at all points- Score pruning: not allowing paths from relatively badly scoring points to propagate further in the DTW calculation "
「Attention-Based Models for Speech Recognition」 Paper Review ,https://blog.naver.com/sooftware/221811795770,20200217,"「Attention-Based Models for Speech Recognition」 Paper Review    http://papers.nips.cc/paper/5847-attention-based-models-for-speech-recognition.pdf​​Introduction  본 논문에서는 최근 도입된 (당시는 최근이였음) 어텐션 매커니즘이 여러 분야에서 좋은 성능을 보였지만, 음성 인식 분야의 특성을 충분히 반영한 매커니즘은 없었다고 주장한다.음성 인식은 NMT 등의 task에 비해 상당히 긴 input sequence를 가진다.단어 단위로 수개에서 수십개의 인풋을 가지는 NMT에 비해 음성 인식에서는 20 ~ 40ms로 자른 프레임들이 수백~수천개의 인풋으로 들어가게 된다본 논문은 이러한 음성 인식 분야의 특성에 맞게 새로운 어텐션 매커니즘을 제안한다.    참고로 본 논문은 2015년 당시 음성 인식 분야에서 ""Listen, Attend and Spell"" 논문과 함께 Innovation이라고 불릴만큼 큰 파장을 준 논문이였다. 기존 CTC 방식이 압도적이였던 당시에, End-to-End 방식의 포문을 열어준 논문이였기 때문이다.​​​General Framework    기본적인 어텐션에 대한 큰 그림이다.(본 논문에서는 α는 alignment, g는 glimpse라고 칭함 )어떠한 매커니즘을 거쳐서 alignment (α) 를 구하고 나면, alignment와 인코더의 아웃풋들을 곱해서 glimpse를 구한다.  Attention의 개념​본 논문에서는 나와 있지 않지만 간단하게 개념을 정리하고 가자면, ""alignment는 어떤 인코더를 고려해야 할까?""를 수치화해준 벡터이고, glimpse는 수치화 된 alignment와 인코더의 아웃풋들을 각각 곱해서 현재 디코딩에 필요한 인코더의 정보를 압축한 벡터이다. 그리고 glimpse와 디코더의 아웃풋을 고려해서 현재 스텝의 값을 예측한다.    그럼 alignment는 어떻게 구하지? 란 물음에 답해주는 부분이다.특정 방식으로 Score를 구한 뒤, 해당 점수를 Softmax 함수에 넣어서 전체 값을 0~1의 값으로, 전체 합을 1로 만들어 준다.=> 각 인코더 아웃풋을 얼마씩 참고할지를 수치화하는 것이다.그럼 Score를 구하는 특정 방식은 무엇이냐??   어텐션 스코어를 구하는 방법은 위와 같이 다양하다. 사실 위는 정말 몇 개만 뽑아온 것이다.어텐션 매커니즘의 종류는 이 스코어 함수가 무엇이냐에 따라 달라진다.그리고, 본 논문은 새로운 ""스코어 함수""를 제안한 논문인 것이다.  본 논문에서는 2가지 어텐션 방식에 주목했다.​1. Content-Based Attention​2. Location-Based Attention​​Content-Based Attention    아마 어텐션을 처음 공부할 때에 대부분 Dot-Product Attention으로 배웠을 것이다.해당 스텝의 디코더의 출력과 인코더의 모든 출력들을 내적하여 어텐션 스코어를 구하는 방식이다.​Content-Based Attention은 Dot-Product보다 조금 더 복잡한 수식으로 점수를 낸다.단순한 내적이 아닌, 해당 스텝의 디코더의 출력과 인코더의 모든 출력들에 웨이트를 준다.그리고 편향 및 Hyperbolic tangent를 걸어주고, 마지막으로 웨이트를 다시 걸어준다.​Dot-Product Attention에 비해서는 진보된 방법이지만, Content-Based 방식의 문제점은 시퀀스에서의 자신의 위치에 상관없이 스코어링을 한다는 점이다. 이를 ""similar speech fragments"" 문제라고 한다고 한다.​​Location-Based Attention    그럼 이번에는 Location-Based 방식을 살펴보자.이 방식은 alignment 계산시, 해당 스텝 디코어의 출력과, 이전 alignment를 고려해줌으로써, 현재 시퀀스에서 어느 위치인지를 알 수 있게끔 해주는 방식이다.하지만 이 방식은 인코더의 아웃풋을 전혀 고려하지 않고, 디코더의 아웃풋만을 가지고 예측하기 때문에 분명한 한계점이 존재한다.​​Hybrid Attention    본 논문은 이러한 2 방식의 어텐션을 적절히 결합한 음성 인식용 어텐션을 제안한다.( 해당 어텐션을 Hybrid, Location-Aware, Location-Sensitive 등 여러 이름으로 불린다 )​   기존 Content-Based 방식에서 약간의 수식만이 추가됐을 뿐이다.기존 Content-Based 방식에서 이전 스텝의 alignment를 고려해준다.이때 이전 alignment에 웨이트를 주기 이전에, Convolution으로 1xC의 형상에서 KxC의 형상으로 늘려준다. (C: Classfication Number)그리고 해당 행렬에 웨이트를 주어서 Content + location 방식을 완성한다.​​3 Potential Issue    앞에서 살펴봤던 위의 수식에는 3가지의 이슈가 있다.​​1. 인풋 시퀀스가 길다면, glimpse에는 노이즈가 섞여있을 가능성이 크다.​만약 인풋 시퀀스가 길다면, 어떤 시점 t에서 멀리 떨어져 있는 t + k라는 시점에서의 음성과는 서로 관련이 없을 것이다. 하지만 Softmax 함수 특성상, 모든 인풋들에 값을 부여한다. 이러한 Softmax의 특성에 의해 많은 관련없는(irrelevant) 인코더의 출력들이 고려될 것이다. 이는 Noise로 작용된다.​​2. 시간 복잡도가 크다.​인풋 시퀀스의 길이가 L이라고 할 때, 디코더는 매 타임 스텝마다 이 L개의 frame을 고려해주어야 한다. 그리고, 디코딩 길이를 T라 할 때, 위의 과정을 T만큼 반복하게 된다. 이는 O(LT) 라는 높은 시간 복잡도를 만들게 된다.​​3. Softmax 함수는 Single Vector에만 집중 (focus) 하는 경향이 있다.​이러한 경향은 top-score를 받은 여러 프레임을 고려할 수 없게 한다.​​Sharpening & Windowing​본 논문은 위의 문제를 간단하게 해결하기 위해 ""Sharpening""이라는 개념의 제안했다. Softmax 수식을 약간 수정하는 것이다.   when, β > 1​본 논문에서는 inverse temperature를 걸어준다고 표현했다.위의 수식이 왜 1번 문제를 해결해 주는지에 대해서는 아직 이해를 하지 못하였다.그리고 본 논문은 위의 방식이거나, top-k개의 프레임만을 뽑아서 re-normalization을 해주는 방식으로도 해결 가능하다고 말한다. 하지만, 위의 2 방식 모두 2번째 시간복잡도의 문제는 해결하지 못했으며, 2번째 방법의 경우는 오히려 시간 복잡도를 더 늘리게 된다.그리고 Windowing이라는 방법이 나오게 되는데, 이전 alignment의 중간값(median)을 기준으로 윈도우 크기 만큼만 고려해주는 방식이다. 해당 방법은 O(L+T)로 시간 복잡도를 낮춰준다.  Sharpening은 long-utterance (긴 발화)에서의 퍼포먼스는 개선했지만, 전체적인 퍼포먼스면에서는 좋지 못한 결과로 이어졌다.(짧은 발화에서는 퍼포먼스가 별로였다)하지만 해당 실험은 최상위 점수를 받은 프레임들을 선택하여 집계하는 방식이 좋을 것이라는 가정을 하도록 만들었다고 한다.​​Smoothing​그래서 나오게 된 방법이 Smoothing 방법이다.   위의 식처럼 기존 Softmax 식에 Sigmoid를 추가해준 방식이다.이러한 방식은 다양성을 가져온다고 본 논문은 말한다.​​Result    본 논문에서 진행한 실험의 결과이다.기본 모델보다는 Convolution을 적용한 모델이 더 좋은 결과를 내었고,Smoothing까지 적용한 모델이 최상의 성적을 내었다.   ​Implementation  Dot-Product Attention, Content-Based Attention, Hybrid-Attention까지 구현한 링크입니다.https://github.com/sh951011/Korean-Speech-Recognition/blob/master/models/attention.py오류, 피드백, 질문 무엇이든지 환영합니다.​email1 : sh951011@gmail.comemail2 : sooftware@naver.com "
Korean Speech Recognition Document ,https://blog.naver.com/sooftware/221837798897,20200304," Korean Speech Recognition Document ""Sooftware""​https://sh951011.github.io/Korean-Speech-Recognition/ Welcome to Korean Speech Recognition’s documentation! — Korean Speech Recognition 1.0 documentationWelcome to Korean Speech Recognition’s documentation! Notes Introduction Preparation before Training Architecture Models Package Reference Feature Dataset Loss Hparams sh951011.github.io Sphinx라는 라이브러리를 이용해서 GitHub 프로젝트 페이지 쉽게 작성.파이썬 코드의 주석을 일정 형식에 맞춰 쓰면 주석 내용을페이지에 이쁘게 정리해준다.​해당 방법은 포스팅 예정.   ​   ​ "
종단간 자동 음성 인식 알고리즘(End-to-End automatic speech recognition algorithms) ,https://blog.naver.com/towards-ai/222164271527,20201206,"국내외 학회에 참가해보면, 딥러닝 모델만으로 입력된 음성을 즉시 문자로 표현하는 종단간 자동 음성 인식 모델에 대한 많은 연구 발표를 볼 수 있고 현재는 음성 인식 처리를 서버가 아닌 On-Device에서 처리할 수 있도록 노력 중입니다.​글로벌로 보면 구글 어시스턴트, 아마존 알렉사, 마이크로소프트 코타나, 애플 시리 등이 치열한 경쟁 중이고 국내 주요 기업으로는 삼성전자, LG전자, SKT, KT, LG U+, 네이버, 카카오가 음성 AI 플랫폼을 출시하였습니다.​최근 여러 학회를 통해 음성인식 처리 개요에 대해서 배웠고 이에 대한 개요를 정리를 해보았습니다. 추후에 각 모델에 대한 논문들을 읽어보고 정리해볼 계획입니다. 음성인식 처리 방법음성인식 처리하는 방법에는 크게 2가지로 나뉩니다. 아래 그림에서 보면 알 수 있듯이 전통적인 음성인식 모델과 종단 간 음성인식 모델이 있습니다. ​특징전통적인 음성인식 모델- 음성 입력값을 음향 모델(AM), 언어 모델(LM), 발음 모델(PM)과 같은 여러 구성 요소로 나눠 각각을 별도 모델로 처리함- 언어 모델과 발음 모델의 크기는 많은 양의 단어 사용하면 커지기 때문에 많은 양의 메모리를 사용해야 한다는 단점종단간 음성인식 모델- 딥러닝 모델을 통한 종단 간 자동 음성 인식 모델이 사용- 한 개의 Neural Network로 구성 가능해서 기존 접근 방식보다 콤팩트하게 만들 수 있음- 작은 크기의 모델로도 음성인식 프로세스 처리가 가능하다는 장점 종단간 음성인식 모델 종류: CTC (Connectionist Temporal Classification), LAS(Listen, Attend Spell)ex) Google은 2018년 종단간 음성인식 모델 상용화, Samsung은 2019년 상용화특징CTC (Connectionist Temporal Classification)- 연결주의 시간 분류- 입력 음성 프레임 시퀀스와 타겟 단어/음소 시퀀스 간에 명시적인 얼라인먼트(alignment) 정보 없이도 음성 인식 모델을 학습할 수 있는 기법입니다.LAS(Listen, Attend Spell)- Sequence to sequence framework와 Attention 알고리즘을 이용하여 음향 입력과 레이블 출력 사이의 정렬(Alignment)을 계산합니다.- Listener (encoder) 와 Speller (decoder) 로 이루어져 있습니다. 종단간 자동 음성 인식 모델들의 한계점: Streaming 음성인식에 적합하지 않은 단점주요 모델로는 MoCha (Monotonic Chunkwise Attentions), RNN-T (Recurrent Neural Network Transducer)이 있음딥러닝 모델을 이용한 종단간 자동 음성 인식 모델들도 한계점이 제기되었습니다. 전체 문장을 입력으로 넣고 처리하다 보니 Streaming 음성인식에 적합하지 않은 단점이 존재해서 Streaming을 처리할 수 있는 Online Model이 등장하였습니다. 음성인식 모델 압축 기술: On-Device 음성 인식에서는 모델 압축 기술들을 통해 On-device 실시간 구현이 가능지식 증류법(Knowledge Distillation), 양자화(Quantization), 낮은 계수 근사법(Low-Rank Approximation), Pruning(가지치기)특징1. 지식 증류법(Knowledge distillation)실제로 사용하고자 하는 작은 Network(Student Network)에게 미리 잘 학습된 큰 Network(Teacher)의 지식을 전달하는 것입니다.2. 양자화(Quantization)조금 더 Compact 하게 저장. 파라미터를 위하여 32bit 또는 64bit 부동 소수점 수준의 Precision이 필요하지 않은 경우가 많이 있습니다. Quantization은 파라미터의 Precision을 적절히 줄여서 연산 효율성을 높이는 방법입니다.3. 낮은 계수 근사법(Low-Rank Approximation)행렬 곱 연산 시 Rank를 줄여 연산함으로써 근사해를 구하더라도 더 빠른 속도로 연산을 할 수 있게 합니다. Rank를 줄이기 위해 사용할 수 있는 대표적인 방법으로 SVD(Singular Vector Decomposition)가 있으며, 4차원 데이터와 3차원 Filter-Bank의 형태나 특성을 고려하여 Rank를 줄이는 방법입니다.4. 가지치기(Pruning)Neural Network의 parameter 중에서 중요도가 낮은 parameter를 찾아 제거하는 방법입니다. 가지치기 해서 작은 파라미터는 영으로 만들어 저장 안 합니다. 최근 음성인식 동향삼성전자에서는 기존 MoCha 모델을 통한 음성인식 처리를 하였지만, Latency가 크다는 단점으로 현재 상용제품들을 MoCha모델에서 RNN-T로 바꾸고 있습니다.​개체명 인식 도메인의 사용으로 On-device의 WER(단어오류율)을 상당히 감소시켰고, Server 단의 음성인식 처리와 유사한 성능을 보이고 있습니다. 하지만, Open domain에서는 음성인식이 낮은 성능을 보여 길이가 긴 단어 처리, 지역적인 방언 처리 등 많은 음성인식 연구들은 현재 노력 중입니다.​​[References]- 2020 인공지능학회 추계학술대회 A Review of on-device fully neural end-to end automatic speech recognition algorithms by Chanwoo Kim- 삼성 AI 포럼 Towards End-to-End Speech Recognition by Tara Sainath- https://www.samsungsds.com/kr/insights/Neural-Network-AI.html 인공지능 경량화 기술 동향인공지능 경량화 기술 동향www.samsungsds.com ​ "
LASR: Lightning Automatic Speech Recognition ,https://blog.naver.com/sooftware/222310080251,20210414,"LASR: Lightning Automatic Speech Recognition ""sooftware""​https://github.com/sooftware/lasr sooftware/lasrPyTorch Lightning implementation of Automatic Speech Recognition - sooftware/lasrgithub.com ​PyTorch-Lightning으로 음성인식 트레이닝 코드를 구현해봤습니다.​PyTorch-Lightning은 기존 PyTorch 코드를 더욱 구조적이고 코드를 깔끔하게 할 수 있으며,TPU Training, FP16 Training 등 학습 최적화 등을 자동으로 처리해주고.to(device), loss.backward() 등의 여러 연산을 자동으로 연산해줍니다.​연습삼아 구현해봤는데, PyTorch-Lightning을 앞으로 계속 사용할 것 같네요. "
Technical Report : KoSpeech: Open-Source Toolkit for End-to-End Korean Speech Recognition ,https://blog.naver.com/sooftware/222083736991,20200908,"Technical Report : KoSpeech: Open-Source Toolkit for End-to-End Korean Speech Recognition  ""Sooftware""​​https://arxiv.org/abs/2009.03092 KoSpeech: Open-Source Toolkit for End-to-End Korean Speech RecognitionWe present KoSpeech, an open-source software, which is modular and extensible end-to-end Korean automatic speech recognition (ASR) toolkit based on the deep learning library PyTorch. Several automatic speech recognition open-source toolkits have been released, but all of them deal with non-Korean la...arxiv.org ​처음으로 논문 형식으로 글을 써봤습니다.논문이라기 보다는 Technical Report가 맞을 것 같네요.영어로 글을 쓴다는게 진짜 어렵네요 ㅎㅎ..​따로 논문을 내지는 않고 arXiv에 올려놨습니다.​ 11쪽 분량으로 저희가 담을 수 있는 내용은 최대한 담았습니다.한국어 음성인식을 입문 혹은 연구하시는 분들께 도움이 됐으면 좋겠습니다. "
2020 Accented English Speech Recognition Challenge (AESRC) 온라인 세미나  ,https://blog.naver.com/datatang007/222161992880,20201203,"무료로 참가하는 AESRC 온라인 세미나 많은 참여 부탁드립니다 ~.#세미나#음성인식#딥러닝#데이터#음성데이터 #AI#머신 러닝#NLP#VISION#data#datascience#deeplearning#seminar#speechdata#speech#데이터가공#인공지능  Dr. Jinyu Li from Microsoft and Professor Shinji Watanabe from The Johns Hopkins University will attend 2020 Accented English Speech Recognition Challenge (AESRC) Online Seminar​Time: Dec. 5th, 2020 10am-13:20pmZoom Webinar ID: 89956910990For online seminar inquires, please email : info@datatang.com​ ​ "
How to start with Kaldi  and speech recognition ,https://blog.naver.com/kkyy3402/221573074995,20190628,"https://towardsdatascience.com/how-to-start-with-kaldi-and-speech-recognition-a9b7670ffff6 How to start with Kaldi and speech recognition – Towards Data ScienceThe best way to achieve a state of the art speech recognition systemtowardsdatascience.com Spearker Recognition , Speech Recognition 에 사용되는 오픈소스 라이브러리인데, 워낙 소스 규모가 큰 라이브러리이기에, 카테고리를 새로 만들어서, 공부해보고자 한다.  "
System.Speech.Recognition.SpeechRecognitionEngine에 대한 스트리밍 입력 ,https://blog.naver.com/peachflame05781/222678908747,20220321,"TCP 소켓에서 C#으로 ""스트리밍"" 음성 인식을 수행하려고 합니다. 내가 겪고 있는 문제는 SpeechRecognitionEngine.SetInputToAudioStream()이 찾을 수 있는 정의된 길이의 스트림이 필요한 것 같습니다. 지금 당장은 이 작업을 수행할 수 있는 유일한 방법은 더 많은 입력이 들어올 때 MemoryStream에서 인식기를 반복적으로 실행하는 것입니다.다음은 설명할 몇 가지 코드입니다.             SpeechRecognitionEngine appRecognizer = new SpeechRecognitionEngine();            System.Speech.AudioFormat.SpeechAudioFormatInfo formatInfo = new System.Speech.AudioFormat.SpeechAudioFormatInfo(8000, System.Speech.AudioFormat.AudioBitsPerSample.Sixteen, System.Speech.AudioFormat.AudioChannel.Mono);            NetworkStream stream = new NetworkStream(socket,true);            appRecognizer.SetInputToAudioStream(stream, formatInfo);            // At the line above a ""NotSupportedException"" complaining that ""This stream does not support seek operations."" 누구든지 이 문제를 해결하는 방법을 알고 있습니까? SetInputToDefaultAudioDevice()를 사용하여 마이크와 잘 작동하기 때문에 일종의 스트리밍 입력을 지원해야 합니다.고마워, 숀System.IO.BufferedStream에서 네트워크 스트림을 래핑해 보셨습니까? NetworkStream netStream = new NetworkStream(socket,true);BufferedStream buffStream = new BufferedStream(netStream, 8000*16*1); // buffers 1 second worth of dataappRecognizer.SetInputToAudioStream(buffStream, formatInfo);  입력을 버퍼링한 다음 음성 인식 엔진에 연속적으로 더 큰 덩어리로 보냈습니다. 예를 들어 처음에는 처음 0.25초를 보낸 다음 처음 0.5초를 보낸 다음 처음 0.75초를 보내는 식으로 결과를 얻을 때까지 보낼 수 있습니다. 이것이 가장 효율적인 방법인지 확실하지 않지만 만족스러운 결과를 얻었습니다.행운을 빕니다, 션  이것이 내 솔루션입니다. class FakeStreamer : Stream{    public bool bExit = false;    Stream stream;    TcpClient client;    public FakeStreamer(TcpClient client)    {        this.client = client;        this.stream = client.GetStream();        this.stream.ReadTimeout = 100; //100ms    }    public override bool CanRead    {        get { return stream.CanRead; }    }    public override bool CanSeek    {        get { return false; }    }    public override bool CanWrite    {        get { return stream.CanWrite; }    }    public override long Length    {        get { return -1L; }    }    public override long Position    {        get { return 0L; }        set { }    }    public override long Seek(long offset, SeekOrigin origin)    {        return 0L;    }    public override void SetLength(long value)    {        stream.SetLength(value);    }    public override int Read(byte[] buffer, int offset, int count)    {        int len = 0, c = count;        while (c > 0 && !bExit)        {            try            {                len = stream.Read(buffer, offset, c);            }            catch (Exception e)            {                if (e.HResult == -2146232800) // Timeout                {                    continue;                }                else                {                    //Exit read loop                    break;                }            }            if (!client.Connected || len == 0)            {                //Exit read loop                return 0;            }            offset += len;            c -= len;        }        return count;    }    public override void Write(byte[] buffer, int offset, int count)    {        stream.Write(buffer,offset,count);    }    public override void Close()    {        stream.Close();        base.Close();    }    public override void Flush()    {        stream.Flush();    }} 사용하는 방법: //client connect inTcpClient clientSocket = ServerSocket.AcceptTcpClient();FakeStreamer buffStream = new FakeStreamer(clientSocket);...//recognizer initm_recognizer.SetInputToAudioStream(buffStream , audioFormat);...//recognizer endif (buffStream != null)    buffStream.bExit = true;  스트림 클래스를 재정의하여 실시간 음성 인식이 작동합니다. class SpeechStreamer : Stream{    private AutoResetEvent _writeEvent;    private List<byte> _buffer;    private int _buffersize;    private int _readposition;    private int _writeposition;    private bool _reset;    public SpeechStreamer(int bufferSize)    {        _writeEvent = new AutoResetEvent(false);         _buffersize = bufferSize;         _buffer = new List<byte>(_buffersize);         for (int i = 0; i < _buffersize;i++ )             _buffer.Add(new byte());        _readposition = 0;        _writeposition = 0;    }    public override bool CanRead    {        get { return true; }    }    public override bool CanSeek    {        get { return false; }    }    public override bool CanWrite    {        get { return true; }    }    public override long Length    {        get { return -1L; }    }    public override long Position    {        get { return 0L; }        set {  }    }    public override long Seek(long offset, SeekOrigin origin)    {        return 0L;    }    public override void SetLength(long value)    {    }    public override int Read(byte[] buffer, int offset, int count)    {        int i = 0;        while (i<count && _writeEvent!=null)        {            if (!_reset && _readposition >= _writeposition)            {                _writeEvent.WaitOne(100, true);                continue;            }            buffer[i] = _buffer[_readposition+offset];            _readposition++;            if (_readposition == _buffersize)            {                _readposition = 0;                _reset = false;            }            i++;        }        return count;    }    public override void Write(byte[] buffer, int offset, int count)    {        for (int i = offset; i < offset+count; i++)        {            _buffer[_writeposition] = buffer[i];            _writeposition++;            if (_writeposition == _buffersize)            {                _writeposition = 0;                _reset = true;            }        }        _writeEvent.Set();    }    public override void Close()    {        _writeEvent.Close();        _writeEvent = null;        base.Close();    }    public override void Flush()    {    }} ... 그리고 그 인스턴스를 SetInputToAudioStream 메서드에 대한 스트림 입력으로 사용합니다. 스트림이 길이를 반환하거나 반환된 개수가 요청된 것보다 적으면 인식 엔진이 입력이 완료되었다고 생각합니다. 이것은 결코 끝나지 않는 순환 버퍼를 설정합니다.  분명히 할 수 없습니다(""설계에 따라""!). http://social.msdn.microsoft.com/Forums/en/netfxbcl/thread/fcf62d6d-19df-4ca9-9f1f-17724441f84e 참조 "
The global speech and voice recognition market was valued at usd 5.21 billion in 2016 and is project ,https://blog.naver.com/khanr635/221463644068,20190212,"Voice and Speech Recognition is the ability of a machine or program that can identify, distinguish and authenticate the voice of an individual speaker, understand and carry out spoken commands. Benefits of voice and speech recognition includes - encourages natural, human-like conversations, enables organizations save agents for more important tasks, delivers a great customer experience, and has the ability to convert speech to text and vice-versa.Request For Free Sample- https://www.kennethresearch.com/sample-request-10059792​Sample Infographics:Market Dynamics:1. Market Drivers1.1 Development of speech and voice recognition software for micro-linguistics and local languages1.2 Increasing usage of speech and voice recognition in service robotics and autonomous cars1.3 Adoption of speech and voice recognition system in the education of temporarily and permanently disabled students1.4 Increasing demand for voice authentication in mobile banking application1.5 Growing demand for speech-based biometric systems for multifactor authentication1.6 Integration of AI and cloud in speech and voice recognition system2. Market Restraints2.1 Poor accuracy of speech and voice recognition system in noisy and harsh working environments2.2 Slower network speed in some regions2.3 Lack of standardized platform for the development of customized speech and voice recognition software products​Market Segmentation:The Global Speech and Voice Recognition Market is segmented on the technology, vertical, and region.​1. Technology:1.1 Speech Recognition1.2 Voice Recognition ​2. By Vertical:2.1 Healthcare2.2 ConsumerBuy Now- https://www.kennethresearch.com/report-purchase-id-10059792​2.3 Military2.4 Legal2.5 Education2.6 Automotive2.7 Enterprise2.8 Banking, financial services, and insurance2.9 Government2.10 Retail 2.11 Others​3. By Region:3.1 North America (U.S., Canada, Mexico)3.2 Europe (Germany, UK, France, Rest of Europe)3.3 Asia Pacific (China, India, Japan, Rest of Asia Pacific)3.4 Latin America (Brazil, Argentina, Rest of Latin America) 3.5 Middle East & Africa​Competitive Landscape:The major players in the market are as follows:1. Microsoft Corporation 2. Sensory, Inc. 3. Readspeaker Holding B.V. 4. Iflytek Co., Ltd. 5. Voicebox Technologies Corp. 6. Acapela Group Sa7. Nuance Communications, Inc. 8. Alphabet Inc. 9. Cantab Research Limited 10. Pareteum Corporation 11. Voicevault Inc. 12. Lumenvox, LLC Read More- https://www.kennethresearch.com/report-details/global-speech-and-voice-recognition-market-/10059792​Contact Us​1412 Broadway,21st Floor Suite MA111, New York, NY 10018​Name:DavidEmail:sales@kennethresearch.comPhone: +1 313 462 0609 "
「SpecAugment: A Simple Data Augmentation Method for Automatic    Speech Recognition」 Paper Review ,https://blog.naver.com/sooftware/221849072822,20200311,"SpecAugment:​「A Simple Data Augmentation Method for Automatic   Speech Recognition」 Review    https://arxiv.org/abs/1904.08779해당 글은 이곳​에서 더 깔끔한 화면으로 보실 수 있습니다.​​Abstract  모델의 Overfitting을 막기 위해 가장 좋은 방법은 데이터가 많은 것입니다. 하지만 데이터가 뿅! 하고 생기는 것이 아니기 때문에 기존 데이터를 활용하여 새로운 데이터를 만들어내는 Augmentation이라는 기법을 사용합니다. 본 논문에서는 음성인식을 위한 간단한 Data-Augmentation을 제안하고, 이를 SpecAugment라고 명명했습니다. 본 논문은 오디오에서 뽑은 피쳐 벡터 (MFCC or Mel-Spectrogram etc ..) 를 input으로 Time warping, Frequency masking, Time masking 3가지 방법으로 Augmentation을 적용했습니다. 성능 테스트를 위한 모델로는 「Listen, Attend and Spell」 (LAS) 모델을 사용했으며, Language Model과의 Shallow Fusion을 통해 인식률 개선을 이뤄냈다고 밝히고 있습니다. 본 논문의 모델은 LibriSpeech 960h 데이터셋과 Swichboard 300h 데이터셋에서 State-Of-The-Art (SOTA) 를 달성했습니다. 달성한 결과는 아래 표에 정리했습니다. DatasetLibriSpeech 960hLibriSpeech 960hSwichboard 300hSwichboard 300hMethodNo LMWith LMNo LMWith LMPrevious-7.5-8.3 / 17.3LAS + SpecAugment6.85.87.2 / 14.66.8 / 14.1 ​Data Augmentation​자세히 들어가기 앞서, Data-Augmentation이 뭔지 부터 살펴봅시다.   Augmentation이란, 데이터를 부풀려서 모델의 성능을 향상시키는 기법입니다.이미지 인식 분야에서 많이 쓰이는 방법으로, 좌우 반전, 사진의 일부 발췌, 밝기 조절 등을 적용하여 한정된 데이터를 조금씩 변형시켜 새로운 데이터처럼 활용하는 방법입니다.​​Augmentation을 하는 이유​  1. Preprocessing 및 .Augmentation을 하면 대부분의 경우 성능이 향상된다.  2. 원본 데이터를 활용하여 추가하는 개념이므로 성능이 저하될 염려가 없다.  3. 방법이 간단하며 패턴이 정해져 있다. ​단기간에 성능 향상을 원한다면, Ensemble, Augmentation을 활용하라는 말이 있을 정도로 그 효과가 검증됐다고 합니다. 저번 네이버 해커톤 - Speech 대회 참여 당시에도, 상위권 팀들은 Ensemble, Augmentation을 거의 모두 적용했었습니다. 또한 Augmentation을 적용하는 방법은 매우 다양하기 때문에, 여러 방법도 적용이 가능하다는 장점이 있습니다.​​​Introduction  딥러닝은 음성인식 분야에 성공적으로 적용이 되었습니다. 하지만, 음성 인식 분야의 연구는 대부분 모델에 초점이 맞춰져서 진행이 되었는데, 본 논문은 이러한 모델들은 쉽게 Overfitting 현상이 발생하며, 많은 양의 데이터를 필요로 한다고 지적하고 있습니다.​​Traditional Data-Augmentation for Audio​그리고 본 논문은 기존의 Augmentation이 어떤 방식으로 적용되었었는지에 대한 설명을 간략하게 합니다.​​Noise injection   기존 데이터에 임의의 난수를 더하여 Noise를 추가해주는 방법입니다.​​Shifting Time   임의의 값만큼 음성 신호를 좌/우로 shift하고 빈 공간은 0으로 채우는 방법입니다.​​Changing Pitch   기존 음성 신호의 Pitch(음높이, 주파수)를 랜덤하게 변경해주는 방법입니다.​​Changing Speed   기존 음성 신호의 속도를 빠르게 / 느리게 바꿔주는 방법입니다.​​기존 음성 신호에 대한 Augmentation은 위와 같이 raw audio를 변형하는 방법들이었습니다.하지만 본 논문에서는 이와 같이 주장합니다.   또한 이러한 방법을 이와 같이 표현합니다.   log mel spectrogram을 이미지처럼 다루는 겁니다. 이렇게 계산 비용이 적게 들기 때문에 학습을 하면서 바로바로 Augmentation을 적용할 수 있었다고 합니다. SpecAugment는 앞에서 언급했듯이 3가지 종류의 변형을 적용했습니다.​  1. Time Warping  2. Frequency Masking  3.Time Masking​​​Augmentation Policy  그럼 이제 본 논문에서 제안하는 SpecAugment에 대해 상세하게 알아봅시다.별로 어렵지 않은 내용이라, 쉽게 이해가 되실거라 생각합니다.​​Time Warping   Computer Vision에서 사용되는 Image Warping을 응용한 방법입니다.축의 중심을 이동한다(?)라고 생각하시면 되는데 아마 감이 잘 안오실 겁니다.쉽게 생각해보자면 다음과 같습니다.   위와 같이 보자기의 중심에 손가락을 가져다가 한쪽으로 밀게되면 우측의 이미지와 같이 보자기가 꾸겨지게 됩니다. (보자기가 없어 수건으로 사진을 찍었습니다 ㅎㅎ..) 하지만, 우측 이미지를 보더라도 우리는 보자기라는 것을 알 수 있습니다. 이러한 점을 이용해서 Vision에서는 Image Warp라는 Augmentation 방법을 성공적으로 적용하였고, 본 논문은 여기에 영감을 받아, log mel spectrogram을 이미지라 생각하고, Time Warp를 적용합니다.​​Frequency Masking   굉장히 간단한 방법입니다. 주파수와 시간 축으로 이루어진 Spectrogram의 주파수 축을 따라 일정 영역을 0으로 마스킹해버립니다.   ​​Time Masking   Frequency Masking과 똑같습니다. 다만, 주파수 축기 아닌, 시간 축에 대해서 일정 영역을 0으로 마스킹해버립니다.   ​Frequency Masking과 Time Masking 적용 시 주의점은, 마스킹하는 영역의 범위를 적당하게 지정해주어야 합니다. 너무 많이 / 적게 적용한다면 Augmentation의 효과가 덜하거나 심한 경우 Noise가 될 수 있습니다.   Figure 1은 위에서 아래 방향으로 기존 Spectrogram, Time Warp, Frequency Mask, Time Mask가 각각 적용된 Spectrogram입니다.   본 논문은 Frequency Masking과 Time Masking을 동시에 적용하는 것을 고려했다고 합니다. 2 마스킹을 동시에 적용하게 되면 Figure 2와 같은 Spectrogram이 나오게 됩니다.본 논문은 각각 적용하는 것과 동시에 적용하는 실험을 진행했고, 결과로 나온 파라미터는 다음과 같습니다.   LB : LibriSpeech BasicLD : LibriSpeech DoucleSM : Switchboard MildSS : Switchboard String​Frequency Masking과 Time Masking을 동시에 적용하는 코드는 아래와 같이 사용하시면 됩니다.   ​​Model  본 논문은 「Listen, Attend and Spell」 모델을 사용했습니다. LAS 모델 같은 경우는 음성 인식 분야에서 end-to-end의 대표적인 모델로써, 구조가 간단하며, 관련 연구도 많이 진행된 구조입니다. 첫번째 절에서 이 모델에 대한 Review 및 파라미터들에 대해 소개하고, 2번째 절에서는 Learning Rate Schedules에 대해 다룹니다. 이 Learning Rate Schedule은 퍼포먼스에 많은 영향을 미쳤다고 소개합니다. 또한 앞에서 언급했던 shallow fusion에 대해서 3번째 절에서 다룹니다.​​LAS Network Architectures​본 논문은 LAS Network 중 「Model Unit Exploration for Sequence-to-Sequence Speech Recognition」에서 사용된 구조를 사용했다고 밝힙니다. ( 제가 진행하고 있는 한국어 음성인식 프로젝트도 역시 LAS Network를 사용하기 때문에 해당 논문도 읽고 리뷰를 쓸 예정입니다. )   해당 논문은 log mel spectrogram을 입력으로 받아, 2-Layer의 maxpooling이 적용된 CNN을 거칩니다. (stride = 2) 그리고 이렇게 CNN을 거쳐서 나온 아웃풋을 인코더의 stacked Bi-LSTM의 입력으로 넣습니다. 그리고 인코딩을 거친 아웃풋을 어텐션 기반의 디코더에 넣어 예측 시퀀스를 뽑아냅니다. (디코더 레이어 사이즈 = 2)​​Learning Rate Schedules​이 섹션에서는 학습율을 어떻게 관리했는지에 대해서 소개하고 있습니다. 이렇게 하나의 학습율을 사용하는 것이 아닌, 학습 도중 학습율을 조정하면서 사용하는 것을 Multi-step Learning Rate라고 합니다. 본 논문에서는 총 4단계의 Learning Rate Scheduling을 적용했습니다.다음 그림으로 보시면 이해가 조금 더 쉬울 겁니다.   좌측의 lr의 특정 값은 제가 진행하고 있는 프로젝트에서 적용한 값이므로 무시하셔도 좋습니다.​​Ramp-up:: 학습율이 0부터 시작하여 특정 값까지 급격하게 증가시키는 구간입니다. [0, s_r]High Plateau:: 특정 값에 다다르면 학습율을 유지시키는 구간이 High Plateau입니다. [s_r, s_i]Exponential Decay:: 스텝이 s_i에 다다르면, s_f까지 High Plateau에서 사용한 학습율의 1 / 100로 지수적으로 감소시키면서 진행합니다. [s_i, s_f]Low Plateau:: 이 시점 이후에는 학습률을 계속 유지합니다. [s_f, ~]​High Plateau 구간 중 [s_r, s_noise]까지는 학습율에 deviation이 0.075인 noise를 끼워서 진행하고, s_noise 이후에는 기존 학습율을 유지한다고 합니다. 학습율이 가장 중요한 하이퍼파라미터라는 말답게 상당히 많은 고민을 한 모습입니다.​그리고 본 논문에서는 이러한 구간을 총 3개로 나눠서 실험을 진행했습니다.​  B(asic): (s_r, s_noise, s_i, s_f) = (0.5K, 10K, 20K, 80K)  D(ouble): (s_r, s_noise, s_i, s_f) = (0.5K, 20K, 40K, 160K)  L(ong): (s_r, s_noise, s_i, s_f) = (1K, 20K, 140K, 320K)​이에 대한 실험의 결과는 뒤에서 살펴보겠습니다.​​Label-Smoothing또한 본 논문은 Label-Smoothing을 적용했다고 밝힙니다. Label-Smoothing은 데이터에 대한 Over-Confidence를 조금 덜어주는 역할을 합니다. 아마 Overfitting은 많이 봤겠지만, Over-Confidence는 생소한 분들이 많으실 겁니다. Over-Confidence란 데이터를 너무 믿는다는 겁니다. 아무래도 레이블링이라는 작업이 결국은 사람이 하는 것이다 보니, 어느 정도의 오류가 있습니다. 이러한 오류가 있는 데이터를 학습하다보면 아무래도 정확한 학습하기가 힘듭니다. 그래서 이러한 Over-Confidence를 줄여주기 위하여 Label-Smoothing이라는 개념이 있습니다.정확히 말하자면 Label-Smoothing loss입니다. loss를 계산할 때 적용이 됩니다. loss 계산시에, 원-핫 인코딩 되어 있는 레이블링에 의해 정답에 대해서만 loss가 계산되지만, 이때 정답 레이블은 1, 나머지 레이블은 0으로 되어 있는 것이 아니라, 정답 레이블은 confidence, 나머지 레이블은 uncertainty로 바꾸어 loss 계산을 합니다.confidence + uncertainty = 1.0이 되도록 설정을 합니다.아래는 이를 PyTorch로 이를 구현한 코드입니다.   본 논문은 confidence는 0.9, uncertainty는 0.1을 적용했다고 합니다.​​Shallow Fusion with Language Model​Augmentation만으로도 State-Of-The-Art를 달성했지만, 조금 더 개선하기 위해 Language Model과 Shallow Fusion을 진행했다고 합니다.   ASR 모델에서 나온 log-probability와 LM 모델에서 나온 log-probability를 적절히 고려해주어서 y_hat을 결정하게 됩니다. 앞에서 언급했었던 성능향상을 위해 적용하는 기법 중 하나인 Ensemble과 비슷한 효과를 내는 방법이라고 합니다.​​Experiments  실험 결과에 대한 자세한 설명은 생략하겠습니다.아래 표를 참고 혹은 본 논문을 참고하시면 자세한 결과를 보실 수 있습니다.       ​​​Discussion  자, 이제 얻어진 결과에 대해 해석해보는 시간입니다.​​Time waiping contributes, but is not a major factor in improving performance.​제안한 Time Warp, Frequency Masking, Time Masking 중 Time Warp는 계산은 오래 걸리는데 반하여, 성능이 그리 좋지는 않습니다. 그래서 학습시간이 넉넉치 않다면 Frequency Masking, Time Masking만을 적용하더라도 충분한 결과를 얻을 수 있을 것이라고 언급하고 있습니다.​​Label smoothing introduces instability to training.​Label Smoothing은 Augmentation과 같이 적용될 때 눈에 띄는 성과를 냈다고 언급합니다. 그 이유에 대해 추측해보자면, Masking, Warp와 같은 조작이 들어가게 되면 어느 정도의 변형이 된 것이기 때문에 완벽하게 ~~한 데이터라고 표현할 수는 없을 것입니다. 그래서 이러한 Confidence를 줄여주는 Label-Smoothing과 Collaboration이 되면 더 큰 효과를 내는 것이 아닐까 추측해봅니다 ㅎㅎ..​​Augmentation converts an over-fitting problem into an under-fitting problem.​Augmentation은 오버피팅 되는 문제를 언더피팅으로 바꿔주는 효과가 있다는 말입니다. Augmentation이 적용 되지 않은 데이터셋으로만 학습을 하게 되면, 아무래도 오버피팅이 날 확률이 높습니다. 하지만, Augmentation을 적용해주게 되면 아무래도 기존의 Training 데이터셋에 대하여 Overfitting이 나기 힘든 환경이 될 것입니다. 본 논문에서는 이를 over-fitting => under-fitting 되는 효과가 있다고 표현했습니다.​​Common methods of addressing under-fitting yield improvements.​그럼 이때 발생하는 under-fitting 문제를 어떻게 해결했는지에 대한 답입니다. 간단합니다. 네트워크를 깊게 만들고 학습을 오래시키면 됩니다. 보통 over-fitting이 문제지, under-fitting이 문제라면 전통적인 방법인 네트워크를 깊게하고, 학습을 오래시키면 해결 가능합니다.​​​Conclusion  다른 논문들은 인식률 개선을 위해 Network에 집중할 때, Augmentation, Learning Rate Schedule, Loss 계산 등에 집중해서 State-Of-The-Art를 달성한 ""기본에 충실하자""라는 깨달음을 준 논문입니다. 또한 제가 진행하고 있는 한국어 음성 인식 프로젝트에 많은 영감을 줬고, 실제로 논문에 등장한 거의 대부분의 내용을 적용하여 학습을 진행중입니다. 기회가 된다면 해당 모델로 나온 결과에 대해서도 리뷰하겠습니다. 읽어주셔서 감사합니다. "
「State-Of-The-Art Speech Recognition with Sequence-to-Sequence Models」 Paper Review ,https://blog.naver.com/sooftware/221871527881,20200325,"「State-Of-The-Art Speech Recognition     with Sequence-to-Sequence Models 」       Paper Review   https://arxiv.org/abs/1712.01769​본 논문은 제가 진행하는 프로젝트의 Contributor 분께서 추천해주신 논문으로, 본 논문에서 적용한 Multi-Head Attention을 적용하여 인식률이 향상되었습니다. 또한 본 논문에서는 Word-Piece를 사용하지만, 한국어에서는 Word-Piece 적용시 성능이 저하된다고 합니다. ​​​Abstract  논문 이름에서부터 밝히듯이, 500h Voice Search 분야에서 State-Of-The-Art (SOTA) 를 달성한 논문입니다.  어텐션 기반의 Seq2seq구조인 Listen, Attend and Spell (LAS) 아키텍쳐를 사용했습니다. LAS 구조는 이전에 음향 모델, 발음 모델, 언어 모델로 구성된 방식에서 하나의 Neural Network로 End-to-End 학습이 가능한 구조입니다. 본 논문에서는 grapheme (문자 단위) 모델이 아닌 word-piece (단어 단위) 모델을 사용했으며, Multi-Head Attention을 도입했다고 합니다. 그 외에도 최적화를 위해 Synchronous training, scheduled sampling, label smoothing 등을 사용했다고 밝힙니다. 특이한 점으로 빠른 인식 및 학습을 위해 인코더 부분에 Bidirectional-LSTM이 아닌 Unidirectional-LSTM을 사용했다고 합니다.​​​Introduction  Sequence-to-Sequence 모델은 Autonomic Speech Recognition (ASR) Task에서 탁월한 성능을 보여주었습니다. 2015년에 「Listen, Attend and Spell」 논문에서 Seq2seq 아키텍처를 도입한 이후로 이 글을 쓰는 지금 2020년까지도 관련 논문들이 많이 나오고 있고 SOTA 모델에서도 Sequence-to-Sequence 모델이 많이 점유하고 있습니다. Neural Machine Translation 분야에서 엄청난 성능을 자랑하는 Transformer가 Speech 분야에서는 Transformer 모델이 다소 부진한 듯 합니다. (NMT에서의 압도적인 성능에 비해서입니다 ㅎㅎ..) 제가 네이버 AI 해커톤 참여 당시, 멘토분께서 Transformer는 데이터가 적을 때 성능이 그렇게 좋지 않다고 하셨습니다. 아마 데이터가 적은 음성 분야라 더 두드러지는 특징이 아닐까 싶습니다. 실제로 네이버 대회 당시 100시간이라는 한정된 데이터로 진행을 하다보니, Transformer를 사용한 팀들이 어텐션 기반의 Seq2seq 모델을 사용한 팀들에게 밀리는 현상이 있었습니다. 본 논문도 Transformer보다 뒤에 나온 논문이지만, Transformer가 아닌 Seq2seq 기반으로 모델을 구성한 것을 볼 수 있습니다. 단, Transformer를 제안한 「Attention Is All You Need」 논문에서 제안된 Multi-Head Attention을 사용했습니다. 또한 뒤에서 다룰 Word-Piece Model (WPM), Scheduled Sampling (SS), label smoothing, Asynchronous SGD, language model 등을 사용했습니다. 자세한 내용은 뒤에서 다루겠습니다.​​​System Overview  Basic LAS Model  LAS 모델을 간략하게 설명해줍니다. Encoder(Listener)는 입력으로 들어온 특징 벡터를 h라는 higher-level feature로 변환해줍니다. 이러한 h를 가지고 Decoder(Speller)는 어텐션 매커니즘과 함께 출력을 만들어 냅니다. 이에 대한 자세한 설명은 「Listen, Attend and Spell」 Paper Review을 참고하시면 좋을 것 같습니다.​​Word-Piece Models​LAS 모델은 아웃풋의 단위가 보통 grapheme (character)이였습니다.하지만 이러한 구조는 OOV (Out-Of-Vocabulary) 문제가 발생시킬 수 있습니다. 이를 위한 대안으로, grapheme 단위가 아닌 phoneme (음소) 단위가 있습니다만, phoneme 단위의 단점은 추가적인 발음 모델과 언어 모델이 필요하다는 점입니다. 또한 본 논문에서 실험한 바로는, phoneme 단위는 grapheme 단위 모델보다 성능이 좋지 못했다고 합니다.그래서 본 논문은 Word-Piece Model (WPM)을 사용했다고 합니다. WPM은 OOV 문제를 해결할 수 있습니다. 또한, 일반적으로 word-level의 언어 모델은 graphme-level의 언어 모델보다 Perplexity가 낮다고 합니다. (Perplexity가 낮을수록 우수한 모델입니다.) 그래서 본 논문에서는 이러한 경향을 봤을 때, WPM을 사용하게 되면 디코딩 과정에서 더 우수한 성능이 나오지 않을까라고 예상했다고 합니다.또한 이러한 WPM으로 진행하게 되면, 기존 문자 단위에 비해 더 적은 디코딩 스텝으로 계산되기 때문에 학습 및 추론 속도가 향상되는 장점도 가지게 됩니다. 그리고 결과적으로, WPM을 사용한 모델이 다른 모델보다 더 좋은 성능을 보였다고 합니다.주의할 점으로 Word-Piece 같은 경우, 한국어에서는 오히려 성능이 저하된다고 합니다.​​Multi-Head Attention  Multi-Head Attention (MHA)은 유명한 「Attention Is All You Need」 논문에서 기계번역 분야를 위해 제안되었습니다. 본 논문은 이러한 Multi-Head Attention을 Speech 분야로 확장해보았다고 합니다.  MHA는 기존의 어텐션을 multiple head로 확장한 구조입니다. 기존 어텐션이 1개의 어텐션 분포를 만들었다면, MHA의 각 head는 서로 다른 어텐션 분포를 만들어 내게 됩니다. 이러한 구조는 각 head가 encoder output의 서로 다른 곳을 Attend하게 해주는 효과가 있습니다.본 논문에서는 MHA를 적용 전, 후를 따로 비교하지는 않을 것 같습니다만, 제가 진행하는 음성 인식 프로젝트에서 비교해본 결과 MHA를 적용했던 모델이 압도적으로 좋은 성능을 보였습니다.​​Scheduled Sampling​본 논문에서 적용한 Scheduled Sampling이라는 개념에 대해 서술합니다.  Seq2seq 구조에서는 학습을 빠르게 시키기 위해 티쳐포싱이라는 기법을 사용합니다. Seq2seq구조는 원래 이전 타임스텝의 추론 char / word를 다음 타임스텝의 입력으로 넣어야 합니다만, 학습 초기에는 대부분 잘못된 추론 결과가 나오게 됩니다. 이러한 부분을 개선해주기 위하여 이전 타임스텝 추론 결과가 아닌, Ground Truth를 넣어줌으로써 빠른 학습을 가능하게끔 해주는 기법입니다.​​Exposure Bias Problem​하지만 이러한 티쳐포싱 기법에는 단점이 있습니다. 학습 중에는 Ground Truth를 가지고 있지만, 실제 추론 과정에서는 Ground Truth가 없습니다. 그렇기 때문에 학습 과정과 추론 과정에서 차이(discrepancy)가 발생하게 됩니다.  본 논문은 이러한 Exposure Bias Problem의 차이를 줄이기 위해서 스케쥴링을 해줍니다.학습 초기에는 티쳐포싱 100%로 진행이 되지만, 학습이 진행될수록 비율을 점점 낮춰서 최종적으로는 티쳐포싱 60%까지 줄여서 학습을 진행했다고 합니다.이렇게 스케쥴링 해줌으로써 실제 추론과 학습 단계에서의 차이를 줄였다고 합니다.​​​Label-Smoothing​또한 본 논문은 Label-Smoothing을 적용했다고 밝힙니다. Label-Smoothing은 데이터에 대한 Over-Confidence를 조금 덜어주는 역할을 합니다. 아마 Overfitting은 많이 봤겠지만, Over-Confidence는 생소한 분들이 많으실 겁니다. Over-Confidence란 데이터를 너무 믿는다는 겁니다. 아무래도 레이블링이라는 작업이 결국은 사람이 하는 것이다 보니, 어느 정도의 오류가 있습니다. 이러한 오류가 있는 데이터를 학습하다보면 아무래도 정확한 학습하기가 힘듭니다. 그래서 이러한 Over-Confidence를 줄여주기 위하여 Label-Smoothing이라는 개념이 있습니다.​정확히 말하자면 Label-Smoothing loss입니다. loss를 계산할 때 적용이 됩니다. loss 계산시에, 원-핫 인코딩 되어 있는 레이블링에 의해 정답에 대해서만 loss가 계산되지만, 이때 정답 레이블은 1, 나머지 레이블은 0으로 되어 있는 것이 아니라, 정답 레이블은 confidence, 나머지 레이블은 uncertainty로 바꾸어 loss 계산을 합니다.​confidence + uncertainty = 1.0이 되도록 설정을 합니다.​아래는 이를 PyTorch로 이를 구현한 코드입니다.​ class LabelSmoothingLoss(nn.Module):    def __init__(self, vocab_size, ignore_index, smoothing=0.1, dim=-1):        super(LabelSmoothingLoss, self).__init__()        self.confidence = 1.0 - smoothing        self.smoothing = smoothing        self.vocab_size = vocab_size        self.dim = dim        self.ignore_index = ignore_index    def forward(self, logit, target):        with torch.no_grad():            label_smoothed = torch.zeros_like(logit)            label_smoothed.fill_(self.smoothing / (self.vocab_size - 1))            label_smoothed.scatter_(1, target.data.unsqueeze(1), self.confidence)            label_smoothed[target == self.ignore_index, :] = 0        return torch.sum(-label_smoothed * logit)>>> criterion = LabelSmoothingLoss(vocab_size, ignore_index, smoothing=0.1, dim=-1)   ​Second-Pass Rescoring​물론 Seq2seq의 Decoder가 어느 정도의 language model의 성격을 갖습니다만, 훈련 데이터의 텍스트만이 반영되기 때문에 language model로서의 한계점은 분명합니다. 그래서 다른 논문에서도 그러하듯이, 방대한 텍스트 데이터로 학습한 external language model과 결합을 합니다. 다만 이러한 결합은 훈련 과정이 아닌, 추론 과정에서만 결합을 합니다.  위의 식과 같이, Acoustic Model에서 나온 확률과 Language Model에서 나온 확률, 단어의 개수를 고려하여 Rescoring을 해줍니다. "
[Coursera] Speech recognition-Audio data ,https://blog.naver.com/thdakfwn/222007142269,20200620,Speech recognition-Audio data​sequnce to sequnce models applied to audio data​  ​  ​in speech recognition usually the number of input time steps is much bigger than output time steps​  Trigger Word Detection​  ​ 
"SER Speech Emotion Recognition Dataset | 음성 감정 분석 데이터셋 | 한국어, 영어, 독일어 ",https://blog.naver.com/hannaurora/222642934425,20220209,"음성 감정 분석(Speech Emotion Recognition)에서 사용 할 수 있는 데이터셋을 서치에 정리하였습니다.독일어, 영어, 한국어 데이터셋이 있으며 종류에 따라 신청 과정을 거쳐야 합니다.8개 종류가 있어 표를 두 개로 나누어 올립니다~​python을 이용해 SER 인공지능(ai) 모델을 학습할 때 사용하기 위해 찾은 자료입니다.정리내용에 어느정도 오류가 있을 수 있으니 이 자료를 기반으로 자신의 테이블을 다시 만드는 것을 추천합니다.나라에 따라 감정을 나타내는 패턴이 다를 수 있기 때문에 독일어로 학습한 모델을 한국어에 바로 적용하는 데는 무리가 있다는 점을 알아두세요! 인덱스1234데이터셋 이름Berlin Emotion DatabaseRAVDESS Emotional speech audioIEMOCAPKESDy18언어독일어영어영어한국어신청 여부신청필요없음신청필요없음신청필요신청필요용량46.4MB563MB255MB207MB감정[7개]행복, 슬픔, 지루함, 중립, 분노, 혐오, 두려움[8개]중립, 차분, 행복, 슬픔, 분노, 두려움, 혐오, 놀람[5개]happiness, anger, sadness, frustration, neutral[4개]중립, 행복, 슬픔, 분노설명10명의 배우가 연기한 음성셋RAVDESS에서 오디오만 있는 파일.(01 = ""Kids are talking by the door"", 02 = ""Dogs are sitting by the door"")두 개의 문장만 발화함얼굴과 모션까지 수집한 데이터셋에서 음성만 가져옴.2명이 연기하는 시나리오에서 발췌함.3명 이상이 라벨링에 참여함동적, 정적으로 해서 녹음.20개의 문장을 사용화자10명 전문배우24명 배우10명의 배우30명의 성우오디오 포맷16 kHz, 16 bit, mono16bit, 48kHz .wav48kHz16kHz링크http://www.emodb.bilderbar.info/download/https://www.kaggle.com/uwrfkaggler/ravdess-emotional-speech-audiohttps://sail.usc.edu/iemocap/iemocap_info.htmhttps://nanum.etri.re.kr/share/kjnoh/SER-DB-ETRIv18?lang=ko_KR자세한 설명http://emodb.bilderbar.info/docu/https://sail.usc.edu/iemocap/Busso_2008_iemocap.pdf관련 논문: https://doi.org/10.3390/s21051579파일 개수535144018232880(동적 480,정적 2400) 인덱스5678데이터셋 이름감정 분류를 위한 대화 음성 데이터셋감정 음성합성 데이터셋emotiontts_open_db감성 대화 말뭉치언어한국어한국어한국어한국어신청 여부신청필요신청필요신청필요신청필요용량모두 합해 12GB 정도4.97GB3.61GB감정[7개]행복, 분노, 혐오, 두려움, 중립, 슬픔, 놀람[7개]분노, 혐오, 두려움, 행복, 중립, 슬픔, 놀라움[4개]일반, 기쁨, 화남, 슬픔 [대분류 6가지]분노, 슬픔, 불한, 상처, 당황, 기쁨[소분류 각 10개씩 총 60개]설명감성대화 어플리케이션을 이용해 수집하였으며5명이 라벨링 작업함음성 합성을 위해 동일 인물에 대한 다감정 데이터셋로봇의 감정 및 개성을 표현할 수 있는 대화음성합성 원천기술총 1만 문장의 음성 녹음 데이터.감정 라벨링은 텍스트로 진행함(일반인 50명).녹음 후 라벨링 작업이 없었음.화자20-40대 남성 여성 일반인30대 여성 성우 1인감정 표현 기술 연구를 위한 연구용 DB (일반 대본) : 여성 2인, 남성 3인 / 감정 표현 기술 연구를 위한 연구용 DB (감정 대본) : 여성 5인, 남성 5인여성 10명 각 500문장 = 5000문장남성 동일함오디오 포맷16bit, mono, 16KHz, PCM format16bits, 22.05kHz, mono44.1kHz, 32bit링크https://aihub.or.kr/opendata/keti-data/recognition-laguage/KETI-02-002https://aihub.or.kr/opendata/keti-data/definition-tech/KETI-05-001https://github.com/emotiontts/emotiontts_open_db/tree/master/Dataset/SpeechCorpushttps://aihub.or.kr/aidata/7978자세한 설명angry, disgust, fear, sadness 의 감정만 있음한 명이 녹음함일반 대본에 감정을 부여하여 녹음, 감정 대본을 사용하여 녹음,개성 표현 음성합성기 개발 고도화를 위한 음성 DB (16명): : 10대, 3명: 20~40대, 8명: 50대이상, 3명: 사투리, 2명영상 설명: https://www.youtube.com/watch?v=8e5JE-MeiuM파일 개수5만개 이상 예상21000크라우드 소싱 수행으로 1500명 대상으로 한 텍스트 27만 문장을 구축하고 그 중 1만개를 녹음하여 음성 데이터를 만들음 ​ "
음성인식기(speech recognition) OpeanAI whisper 설치 방법 ,https://blog.naver.com/ludalee_/222999772144,20230130,음성인식기인 OpenAI 의 whisper 설치방법에 대해 알아보겠습니다.OpeanAI의 github page는 여기를 누르면 방문할 수 있습니다.​OpeanAI를 설치하기 위해서는1.pytorch 설치 해놓으세요(최신버전..)2.terminal 혹은 command에 아래으 ㅣ명령어를 입력합니다. pip install -U openai-whisper 위의 방법이 통하지 않는다면 아래의 코드를 입력합니다. pip install git+https://github.com/openai/whisper.git  3.whisper 를 사용하다가 업데이트가 필요하면 아래의 명령어를 입력합니다 pip install --upgrade --no-deps --force-reinstall git+https://github.com/openai/whisper.git 4. 그리고 ffmpeg를 사용해야 하니까 ffmpeg도 설치해주세요 sudo apt update && sudo apt install ffmpeg ​ 
감정인식(SER) 에서 accoustic features #speech emotion recognition ,https://blog.naver.com/ssj860520/222840350826,20220805,감정인식 성능 향상을 위해 어떤 feature 를 사용할지 중요하다.별여별 방법이 이쓰이는 것 같다.temporal 하게 feature 를 추출하되 이것이 global 하게 표현되도록 (representing) 뽑는 경우도 잇다.감정인식을 위해 사용된 대표적인 feature 로는 MFCC 와 MFCC 의 미분등이 있다.(Mel spectrum과 MFCC(Mel-Frequency-Cepstral-Coefficient)의 의미)MFCC 이외에도 여러 feature 들을 사용한다.많은 노력이 있었다.​이런 음성 feature 를 뽑아주는 toolkit 으로는 opensmile 이 있다.opensmile를 이용하면 감정인식을 위해 제안된 feature 를 뽑을 수 있다고 한다.​여러 feature가 있지만 자주 나오는 것들에 대해 인지를 하자.그리고 내가 어떻게 feature 를 만들 수 있을지도 고민해볼만 하다.!​referenceEva Lieskovska and etc. A Review on Speech Emotion Recognition Using Deep Learning and Attention Mechanism 
"Speech and Voice Recognition Market: Global Size, Share, Growth, Analysis & Demand 2024 ",https://blog.naver.com/bharatbook/221458422697,20190204,"Bharat Book Bureau Provides the Trending Market Research Report on “Speech and Voice Recognition Market by Technology (Speech and Voice Recognition), Vertical (Automotive, Consumer, Government, Enterprise, Healthcare, BFSI), Deployment (On Cloud & On-Premises/Embedded), and Geography - Global Forecast to 2024”under Media & Technology Category. The report offers a collection of superior market research, market analysis, competitive intelligence and Market reports.Speech and Voice Recognition Market is expected to grow at CAGR of 19.18% from 2018 to 2024.The speech and voice recognition market is expected to grow from USD 7.5 billion in 2018 to USD 21.5 billion by 2024, at a CAGR of 19.18%. The high growth potential in healthcare application, growing demand for voice authentication in mobile banking application, and rapid proliferation of multifunctional devices or smart speakers are a few major factors driving the growth of speech and voice recognition market. However, high cost of high-end voice recognition systems and lack of accuracy in speech and voice recognition systems in noisy and harsh environments is restraining the market growth.Request a free sample copy of Speech and Voice Recognition Market Report@ https://www.bharatbook.com/marketreports/Sample/Reports/763758   On-premises/Embedded expected to grow at highest CAGR during the forecast period.​The on-premise/embedded segment is expected to witness high growth owing to the recent advancement of speech and voice recognition as a multifactor authentication application in BFSI and other enterprise application, whereas on-premise voice recognition is used for high degree of consumer data security.Speech and Voice Recognition market in APAC is expected to grow at highest CAGR during forecast period.The large consumer electronics industry in China and India offers significant expansion opportunities for the speech recognition market in APAC during the forecast period. Moreover, the growing focus on productivity in developed countries of APAC such as Japan and Singapore is further expected to drive the speech and voice recognition market in the enterprise segment during the forecast period. Furthermore, rapid technological advancements and a high degree of R&D in the country are expected to create several opportunities for the players in the speech and voice recognition market in this country.Breakdown of profile of primary participants:• By Company Type: Tier 1 = 27%, Tier 2 = 41%, and Tier 3 = 32%• By Designation: C-Level Executives = 26%, Directors = 40%, and Others = 34%• By Region: North America = 47%, Europe = 28%, APAC = 19%, and RoW = 6%Nuance (US), Microsoft (US), Alphabet (US), IBM (US), Amazon (US), Sensory (US), Cantab Research (UK), iflytek (China), Baidu (China), and Raytheon BBN Technologies (US) are a few key players in the speech and voice recognition market.Research Coverage:- The study covers the speech and voice recognition market. It aims at estimating the market size and growth potential of this market, across different segments, such as by technology, deployment type, vertical, and region. - The study also includes an in-depth competitive analysis of key market players, along with their company profiles, key observations related to product and business offerings, recent developments, and key market strategies.Key Benefits of Buying the Report:The report will help leaders/new entrants in this market with information on the closest approximations of revenue numbers for the overall speech and voice recognition market and its subsegments. This report will help stakeholders understand the competitive landscape and gain more insights to better position their businesses and plan suitable go-to-market strategies. The report will also help stakeholders understand the pulse of the market and provide them with information on key market drivers, restraints, challenges, and opportunities.Browse our full report with Table of Contents : https://www.bharatbook.com/marketreports/speech-and-voice-recognition-market-by-technology-speech-and-voice-recognition-vertical-automotive-consumer-government-e/763758About Bharat Book Bureau:Bharat Book is Your One-Stop-Shop with an exhaustive coverage of 4,80,000 reports and insights that includes latest Market Study, Market Trends & Analysis, Forecasts Customized Intelligence, Newsletters and Online Databases. Overall a comprehensive coverage of major industries with a further segmentation of 100+ subsectors.Contact us at:Bharat Book BureauTel: +91 22 27810772 / 27810773Email: poonam@bharatbook.comWebsite: www.bharatbook.comFollow us on : Twitter, Facebook, LinkedIn, Google Plus "
논문리뷰A systematic literature review of speech emotion recognition approaches 감정인식 접근 리뷰 ,https://blog.naver.com/ssj860520/222865926786,20220903,"감정인식에 대해 알아보죠.감정인식을 하려는 시도는 굉장히 많았습니다.speech 를 이용해 감정인식을 하는 것을 SER(Speech enhancement recognition)이라고 하죠.SER에 대해서 자세히 알고 싶다면 아래의 글을 보도록 해주세요감사합니다. 사랑합니다 키키​ [논문리뷰]A systematic literature review of speech emotion recognition approaches - 딥러닝을 이용한 감정인식리뷰논문을 계속 리뷰하고 있다. 이 전에 리뷰했던 것을 살펴보면 아래와 같다. 음성감정인식 분류기의 종류(SER, Speech Emotional Recognition classifiers) Voice quality features Teager energy operator(TEO..qwertyuioop.tistory.com ​ "
[감정인식논문리뷰]Stochatic Process Regression for Cross Cultural Speech Emotion Recognition ,https://blog.naver.com/ssj860520/222829784876,20220726,논문 그냥 읽으려고 하면 너무나도 힘들죠.Stochatic Process Regression for Cross Cultural Speech Emotion Recognition 을 읽으려고 하다가그냥 읽자니 너무 힘들었어요그래서 관련된 리뷰가 있나 찾아봤죠.너무나도 고맙게도 이 논문에 대해서 리뷰하신 블로거가 있더라고요.그 블로거에게 너무나도 감사하다는 말을 전하고 싶네요그 리뷰는 아래글에서 확인할 수 있답니다.여러분들도 많이 이용하세요논문정리-Stochatic-Process-Regression-for-Cross-Cultural-Speech-Emotion-Recognition 
(논문정리) Stochatic Process Regression for Cross Cultural Speech Emotion Recognition ,https://blog.naver.com/ssj860520/222832460879,20220728,"논문을 읽고 싶었다.감정인식과 관련된 논문을 읽고 싶었다.어떤 논문이 있나 찾아보다가 아래의 논문을 찾게되었다Stochatic Process Regression for Cross Cultural Speech Emotion Recognition영어이다 보니 읽다가 지쳤다.한글로 된 좋은 리뷰가 없을까?찾아보니 있었다.​ [논문정리] Stochatic Process Regression for Cross-Cultural Speech Emotion Recognition감정을 연속적인 수치로 나타내는 arousal, valence, dominance는 여러요인에 의해 달라진다. 녹음을 할경우 화자의 문화, 언어의 특징등에 의해 감정을 판단하기 어려운 경우도 생기고 라벨러가 감정을 라벨링할..qwertyuioop.tistory.com stochastic process 로 회귀를 하다니그리고 문화적 차이에 의해 생긴 감정의 모호함을표현하는 것 같다.재밌다 "
음성 인식 DB 데이터 무료  (Speech Recognition free DB) - 모빌리오 mobilio ,https://blog.naver.com/takion7/221367121022,20180928,"1. LibriSpeech • 대규모 (약 1,000시간) 공개 영어 음성 데이터로, 최근 많은 음성인식 연구에서 활용 • Kaldi의 egs/librispeech 디렉토리에 recipe 존재 • http://www.openslr.org/12 - 음성-문장 데이터 • http://www.openslr.org/11 - 언어모델, 발음사전, 단어목록 ​​2.Mini-LibriSpeech • 위 LibriSpeech 데이터셋을 음성인식 시스템 입문자들이 활용하기 좋게 줄인 데이터 • Kaldi의 egs/mini_librispeech 디렉토리에 recipe 존재 • http://www.openslr.org/31 - 음성-문장 데이터 ​3.Project Zeroth • 한국어 음성인식을 위한 공개 한국어 음성 데이터 및 Kaldi 기본 recipe • https://github.com/goodatlas/zeroth - Kaldi recipe (data download 포함) • Kaldi 공식 배포판에도 Project Zeroth를 위한 기본 recipe 수록 • Kaldi의 첫 설치 및 사용이 어려울 경우 참고할 만한 한국어 리소스 goodatlas/zerothKaldi-based Korean ASR (한국어 음성인식) open-source project - goodatlas/zerothgithub.com • 대규모 (약 1,000시간) 공개 영어 음성 데이터로, 최근 많은 음성인식 연구에서 활용 • Kaldi의 egs/librispeech 디렉토리에 recipe 존재 • http://www.openslr.org/12 - 음성-문장 데이터 • http://www.openslr.org/11 - 언어모델, 발음사전, 단어목록 • Mini-LibriSpeech • 위 LibriSpeech 데이터셋을 음성인식 시스템 입문자들이 활용하기 좋게 줄인 데이터 • Kaldi의 egs/mini_librispeech 디렉토리에 recipe 존재 • http://www.openslr.org/31 - 음성-문장 데이터 • Project Zeroth • 한국어 음성인식을 위한 공개 한국어 음성 데이터 및 Kaldi 기본 recipe • https://github.com/goodatlas/zeroth - Kaldi recipe (data download 포함) • Kaldi 공식 배포판에도 Project Zeroth를 위한 기본 recipe 수록 • Kaldi의 첫 설치 및 사용이 어려울 경우 참고할 만한 한국어 리소스​​#LibriSpeech #무료 #음성 #DB  #Mini-LibriSpeech #Zeroth #Kaldi #Speech #Recognition #MiniLibriSpeech #deep #learning #mobilio #모빌리오 #인고지능 #AI #스마트팩토리​www.mobilio.io MobilioMobilio smart factory solution 모빌리오 스마트팩토리www.mobilio.io ​ "
음성감정인식 분류기 종류 SER speech emotion recognition ,https://blog.naver.com/kekyake/222824603012,20220722,"speech 를 분석하여 많은 것을 할 수 있죠speech 를 분석하여 한느 task 중에 하나가 감정인식입니다.(speech emotion recognition)감정인식을 하려고 하는 시도는 많았죠이 감정인식 SER 을 하는 머신을 classifier 라고 부릅니다.SER의 classifier에는 어떤 것들이 있을까요?SER 분류기에 대해 보도록 합시다.SER 분류기 종류를 보려면 아래 글을 보세요​ 음성감정인식 분류기의 종류(SER, Speech Emotional Recognition classifiers)음성감정인식을 통해 음성으로부터 화자의 감정을 추론한다. 추론결과를 통해 감정을 분류하는데 분류를 하는 친구를 분류기(Classifier)라고 한다. 이 분류기의 종류에는 무엇이 있을까 SER classifier 종류 SER..qwertyuioop.tistory.com SER 이랬습니다​ "
[논문 브리핑]Speech Emotion Recognition Using Deep Learning Techniques: A Review ,https://blog.naver.com/hannaurora/222401936345,20210618,"논문 제목: Speech Emotion Recognition Using Deep Learning Techniques: A Review저자: RUHUL AMIN KHALIL, EDWARD JONES, MOHAMMAD INAYATULLAH BABAR,TARIQULLAH JAN, MOHAMMAD HASEEB ZAFAR, AND THAMER ALHUSSAIN주제: Speech Emotion Recognition(SER)목적: 130개 이상의 논문을 활용하고 리뷰하여 SER분야에서 부상하고 있는 딥러닝기법에 대해 소개하고 딥러닝기법의 한계와 앞으로의 방향에 대해 제시한다.---SER에 딥러닝을 활용하는 것에 관심이 있으신 분들이 읽으면 좋은 논문입니다.​깃허브에 정리하였습니다.https://github.com/YoungriKIM/YESPEECH/blob/main/%5B%EB%85%BC%EB%AC%B8%20%EB%A6%AC%EB%B7%B0%5DSpeech_Emotion_Recognition_Using_Deep_Learning_Techniques_A_Review.md YoungriKIM/YESPEECH신기술 개발팀. Contribute to YoungriKIM/YESPEECH development by creating an account on GitHub.github.com ​ "
Conformer: Convolution-augmented Transformer for Speech Recognition. ,https://blog.naver.com/sooftware/222217846451,20210124,"Conformer: Convolution-augmented Transformer for Speech Recognition. ""sooftware"" ​구글에서 작년에 공개한 CNN + Transformer인 Confor를 구현해봤습니다.https://github.com/sooftware/conformer sooftware/conformerPyTorch implementation of Conformer: Convolution-augmented Transformer for Speech Recognition - sooftware/conformergithub.com ​ "
음성논문 정리 Stochastic Process Regression for Cross Cultural Speech Emotion Recognition ,https://blog.naver.com/kekyake/222804870099,20220708,"음성논문중에 Stochastic Process Regression for Cross Cultural Speech Emotion Recognition 이있다.감정인식의 어려운 부분이 있다. 그것은 감정이란 주관적이라는 것이다. 이것을 어떻게 해결해야할까?이 논문에서는 감정을 stochastic process로써 본다.이것을 regression 해서 음성의 애매함을 해결하려고 한다.좀더 많은 얘기를 하고 싶지만아래의 논문을 보면 더 좋을 것 같다.​ [논문정리] Stochatic Process Regression for Cross-Cultural Speech Emotion Recognition감정을 연속적인 수치로 나타내는 arousal, valence, dominance는 여러요인에 의해 달라진다. 녹음을 할경우 화자의 문화, 언어의 특징등에 의해 감정을 판단하기 어려운 경우도 생기고 라벨러가 감정을 라벨링할..qwertyuioop.tistory.com ​ "
Simple Speech Recognition in Keras ,https://blog.naver.com/kkyy3402/221558644650,20190610,"https://blog.manash.me/building-a-dead-simple-word-recognition-engine-using-convnet-in-keras-25e72c19c12b Building a Dead Simple Speech Recognition Engine using ConvNet in KerasSo you’ve classified MNIST dataset using Deep Learning libraries and want to do the same with speech recognition! Well continuous speech…blog.manash.me cnn을 통해 bed, cat, happy를 구별하는 음성인식 엔진을 keras로 구현하는 튜토리얼이다. 크게,오디오 데이터에서 MFCC 구함 -> 패딩을 통해 벡터 크기를 동일하게 맞춤 -> CNN 을 통해 classification을  하는 과정을 거치며, 95%의 정확도를 얻을 수 있다. ​위 소스가 올라가있는 repo 링크https://github.com/manashmndl/DeadSimpleSpeechRecognizer manashmndl/DeadSimpleSpeechRecognizerCNN based Minimal model for recognizing word. Contribute to manashmndl/DeadSimpleSpeechRecognizer development by creating an account on GitHub.github.com ​ "
[논문리뷰] Deepfake speech detection through emotion recognition: a semantic aprroach ,https://blog.naver.com/ssj860520/222866407590,20220904,"deepfake 기술이 발전해 감에 따라 부작용 도 있다.deepfake에 등장하는 인물에 동의를 받지 않는 영상, 음성들이 마구마구 제작되기 때문이다.그런 불법 deepfake를 잡아내기 위해서 deepfake detection 기술도 많아지고 있다.deepfake detection 을 위한 많은 시도는 있지만 이 논문 ""Deepfake speech detection through emotion recognition: a semantic aprroach""은 감정인식 관점에서 deepfake detection 을 한다. 어떻게 한다는 것일까? 아래의 글을 보면 잘 알 수 있다.​​ (논문 시스템 구조 정리) Deepfake speech detection through emotion recognition: a semantic aprroach지난 글에 이어 계속해서 논문을 정리하고 있다. 지난 글([논문리뷰]Deepfake Speech Detection Through Emotion Recognition: A Semantic Approach - Introduction) 에서는 introduction 부만 정리했고, 이번에는..qwertyuioop.tistory.com ​ "
speech emotion recognition(SER) ,https://blog.naver.com/kimsjpk/222202472225,20210110,"계속 음성쪽을 하게 된다. 감정 인식이고 시계열 다변수 데이터 분류라서 하던거에서 별로 변한게 없다.mel spectrogram을 써서 특징을 뽑아낸다. 40개 feature x length가 되는데 주파수 특성을 많이 배우질 못해서 왜 이렇게 뽑혔는지는 알지 못한다. 그냥 인터넷에서 찾아서 소스에 적용하였다.https://kaen2891.tistory.com/39 Python Mel-Spectrogram(Mel scaled Spectrogram) 얻기딥러닝을 이용하여 음성 인식, 음성 처리, 화자 인식, 감정 인식 등에서 많이 쓰이는 음성의 특징 추출 방법은 1.Mel-Spectrogram, 2. MFCC가 있다. 오늘은 Mel-Spectrogram에 대하여 어떻게 추출하여 쓸 수 있는..kaen2891.tistory.com 특징 추출 소스는 다음과 같다. import globimport osimport librosaimport numpy as npframe_length = 0.025frame_stride = 0.010audio_only_data = []audio_only_labels = []folders = os.listdir('./TESS/')for ii in range(len(folders)):    files = glob.glob('./TESS/' + folders[ii] +'/*.wav')    for ij in range(len(files)):        print(str(ii) + ' / ' + str(ij) + ' / ' + str(len(files)))        wavfiles = files[ij]        y, sr = librosa.load(os.path.join(wavfiles))        input_nfft = int(round(sr*frame_length))        input_stride = int(round(sr*frame_stride))        S = librosa.feature.melspectrogram(y=y, n_mels=40, n_fft=input_nfft, hop_length=input_stride)        S2 = np.asarray(S, dtype = np.float32)        S2 = np.transpose(S2, (1, 0))        audio_only_data.append(S2)        if wavfiles.find('angry') != -1:            labels = 0        elif wavfiles.find('disgust') != -1:            labels = 1        elif wavfiles.find('Fear') != -1 or wavfiles.find('fear') != -1:            labels = 2        elif wavfiles.find('happy') != -1:            labels = 3        elif wavfiles.find('neutral') != -1:            labels = 4        elif wavfiles.find('Pleasant_surpise') != -1 or wavfiles.find('pleasant_surpise') != -1:            labels = 5        else:            labels = 6        audio_only_labels.append(labels)np.save('audio_only_data.npy', audio_only_data)np.save('audio_only_labels.npy', audio_only_labels) TESS는 (toronto emotional speech set)이다. 음성 감정 분류 데이터셋인데 예전에도 있던 데이터셋인데 2020년도에 업데이트를 한 것 같다. 데이터셋은 다음 사이트에서 구할 수 있다.https://www.kaggle.com/ejlok1/toronto-emotional-speech-set-tess Toronto emotional speech set (TESS)A dataset for training emotion (7 cardinal emotions) classification in audiowww.kaggle.com 데이터를 저장하고 40개의 랜덤 인덱스를 뽑아서 그 후 40프레임을 저장하는 데이터 증대를 사용하였다. 소스는 hand gesture recognition (skeleton) 예전 글을 참고하길 바란다.rnn 네트워크 역시 mcfly를 쓰지 않고 예전 글에 썼었던(emotional recognition) conv1d+transformer+bidirectional gru 를 썼다. 성능은 99.49%가 나왔다. 여기에 그치지 않고 savee(Surrey Audio-Visual Expressed Emotion) 데이터셋을 사용해서 한 번 더 실험하였다.https://www.kaggle.com/ejlok1/surrey-audiovisual-expressed-emotion-savee Surrey Audio-Visual Expressed Emotion (SAVEE)A dataset for training emotion (7 cardinal emotions) classification in audiowww.kaggle.com 데이터셋의 숫자가 480개 밖에 안되어서 40개 랜덤 인덱스를 해서 19200개로 실험했을 때에는 정확도가 79%밖에 안나왔다. 그래서 200개 랜덤 인덱스 샘플링으로 500epoch까지 늘려서 학습을 하니 94.67% 가 나왔다.https://arxiv.org/pdf/2009.08909.pdf 이 논문에 보니깐 97.49%가 나왔는데 논문을 보진 않았지만 ...음 나의 패배다. 근데 계속 음성쪽을 건드리면 음성변조도 할 수 있을까??? 아님 기초가 없으니 제자리 걸음일까???​[업데이트]94.67% 결과를 올리기 위해 투표 방법을 택했다. 하나의 wav 파일에서 10개의 랜덤 인덱스를 뽑고 (1, 40, 40)데이터를 모델에 추론한 후 10개의 클래스를 모아 가장 많은 투표를 받은 클래스를 wav의 클래스로 정했다.결과는 100% 나왔다. 하나 하나의 인식률이 94%인데 어찌보면 당연한 결과이기도 하다. 소스를 기록으로 남겨둔다. import randomfrom collections import Counterimport numpy as npimport tensorflow.keras as kerasfrom tensorflow.keras import layersimport tensorflow as tffrom tensorflow.keras.models import load_modelfrom sklearn.metrics import accuracy_scoreNUM_CLASSES = 7def most_common(lst):    data = Counter(lst)    return max(lst, key=data.get)def overlapping_percentage(x, y):    return (100.0 * len(set(x) & set(y))) / len(set(x) | set(y))class MultiHeadSelfAttention(layers.Layer):    def __init__(self, embed_dim, num_heads=8, **kwargs):        super(MultiHeadSelfAttention, self).__init__(**kwargs)        self.embed_dim = embed_dim        self.num_heads = num_heads        if embed_dim % num_heads != 0:            raise ValueError(                f""embedding dimension = {embed_dim} should be divisible by number of heads = {num_heads}""            )        self.projection_dim = embed_dim // num_heads        self.query_dense = layers.Dense(embed_dim)        self.key_dense = layers.Dense(embed_dim)        self.value_dense = layers.Dense(embed_dim)        self.combine_heads = layers.Dense(embed_dim)    def attention(self, query, key, value):        score = tf.matmul(query, key, transpose_b=True)        dim_key = tf.cast(tf.shape(key)[-1], tf.float32)        scaled_score = score / tf.math.sqrt(dim_key)        weights = tf.nn.softmax(scaled_score, axis=-1)        output = tf.matmul(weights, value)        return output, weights    def separate_heads(self, x, batch_size):        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))        return tf.transpose(x, perm=[0, 2, 1, 3])    def call(self, inputs):        # x.shape = [batch_size, seq_len, embedding_dim]        batch_size = tf.shape(inputs)[0]        query = self.query_dense(inputs)  # (batch_size, seq_len, embed_dim)        key = self.key_dense(inputs)  # (batch_size, seq_len, embed_dim)        value = self.value_dense(inputs)  # (batch_size, seq_len, embed_dim)        query = self.separate_heads(            query, batch_size        )  # (batch_size, num_heads, seq_len, projection_dim)        key = self.separate_heads(            key, batch_size        )  # (batch_size, num_heads, seq_len, projection_dim)        value = self.separate_heads(            value, batch_size        )  # (batch_size, num_heads, seq_len, projection_dim)        attention, weights = self.attention(query, key, value)        attention = tf.transpose(            attention, perm=[0, 2, 1, 3]        )  # (batch_size, seq_len, num_heads, projection_dim)        concat_attention = tf.reshape(            attention, (batch_size, -1, self.embed_dim)        )  # (batch_size, seq_len, embed_dim)        output = self.combine_heads(            concat_attention        )  # (batch_size, seq_len, embed_dim)        return output    def get_config(self):        # Implement get_config to enable serialization. This is optional.        config = {'embed_dim': self.embed_dim,                  'num_heads': self.num_heads}        base_config = super(MultiHeadSelfAttention, self).get_config()        return dict(list(base_config.items()) + list(config.items()))class TransformerBlock(layers.Layer):    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1, **kwargs):        super(TransformerBlock, self).__init__(**kwargs)        self.att = MultiHeadSelfAttention(embed_dim, num_heads)        self.ffn = keras.Sequential(            [layers.Dense(ff_dim, activation=""relu""), layers.Dense(embed_dim),]        )        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)        self.dropout1 = layers.Dropout(rate)        self.dropout2 = layers.Dropout(rate)        self.embed_dim = embed_dim        self.num_heads = num_heads        self.ff_dim = ff_dim    def call(self, inputs, training):        attn_output = self.att(inputs)        attn_output = self.dropout1(attn_output, training=training)        out1 = self.layernorm1(inputs + attn_output)        ffn_output = self.ffn(out1)        ffn_output = self.dropout2(ffn_output, training=training)        return self.layernorm2(out1 + ffn_output)    def get_config(self):        # Implement get_config to enable serialization. This is optional.        config = {'embed_dim' : self.embed_dim,                  'num_heads' : self.num_heads,                  'ff_dim': self.ff_dim}        base_config = super(TransformerBlock, self).get_config()        return dict(list(base_config.items()) + list(config.items()))class TokenAndPositionEmbedding(layers.Layer):    def __init__(self, maxlen, vocab_size, embed_dim):        super(TokenAndPositionEmbedding, self).__init__()        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)     def call(self, x):        maxlen = tf.shape(x)[-1]        positions = tf.range(start=0, limit=maxlen, delta=1)        positions = self.pos_emb(positions)        x = self.token_emb(x)        return x + positionsdata = np.load('audio_only_data2.npy', allow_pickle=True)labels = np.load('audio_only_labels2.npy')model = load_model('savee.h5', custom_objects={'TransformerBlock':TransformerBlock,                                              'MultiHeadSelfAttention':MultiHeadSelfAttention})classes2 = []for ii in range(len(data)):    data2 = data[ii]    numlist = [random.randint(0, len(data2) - 41) for _ in range(10)]    classes = []    for ij in range(10):        ddata = data2[numlist[ij]: numlist[ij] + 40, :]        ddata = np.reshape(ddata, (1, 40, 40))        dataclass = model.predict(ddata)        dataclass = list(dataclass[0])        dataclass2 = dataclass.index(max(dataclass))        classes.append(dataclass2)    classes2.append(most_common(classes))print(""accuracy: "" + str(accuracy_score(classes2, labels))) [업데이트 2]논문으로 남기기 위해서 다른 논문이랑 결과를 비교해 보았다. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7583996/숭실대에서 나온 논문인데 데이터셋으로 ravdess, emodb를 추가하였고 나도 또한 이 데이터셋을 추가로 실험하였다. emodb는 200개 샘플링, 40 length로 했을 때 99.57%가 나왔다.특이한게 ravdess 인데 데이터가 1440로 audio only 데이터셋만을 대상으로 했는데 갯수가 충분해서 샘플링 수를 100개로 했는데 성능이 200epoch에서 80%를 못 넘어서 length 를 80으로 하니깐 100epoch 에서 82%가 나왔고 120으로 length를 늘리고 샘플링을 40으로 줄였다. 그래서 500epoch에 분류 성능이 98.61%가 나왔다. 시계열 데이터라고 하지만 length를 조절함으로써 성능이 이렇게 변할 수도 있다는게 좀 신기하다. 이번 학술대회에는 두 편의 논문을 제출하였고 다음 하계학술대회에 위 내용을 정리해서 논문으로 내야겠다.모든 데이터셋에 대해 분류 성능이 98%가 넘어서 내는데는 문제가 없을거 같다. 선배 이름도 넣어주고​[업데이트 3]논문을 쓰기 전 실험tess: length:40, sampling:40개, 200 epoch, accuracy 99.60%savee:length:120, sampling:40개, 200 epoch, accuracy: 99.32%ravdess:length:120, sampling:40개, 200 epoch, accuracy:97.28% emodb:length:120, sampling:40개, 200 epoch, accuracy:99.86% "
"ASR(Automative Speech Recognition, 자동 음성 인식) ",https://blog.naver.com/lcs5382/222138796810,20201108,"사람이 말하는 음성 언어를 컴퓨터가 해석해 그 내용을 문자 데이터로 전환하는 처리를 말한다. ​STT(Speech-to-Text)라고도 한다. ​키보드 대신 문자를 입력하는 방식으로 주목을 받고 있다. ​로봇, 텔레매틱스 등 음성으로 기기제어, 정보검색이 필요한 경우에 응용된다. ​대표적인 알고리즘은 HMM(Hidden Markov Model)으로서, 다양한 화자들이 발성한 음성들을 통계적으로 모델링하여 음향모델을 구성하며 말뭉치 수집을 통하여 언어모델을 구성한다.​미리 기록해 둔 음성 패턴과 비교해 개인 인증 등의 용도로 사용하기도 하는데 이를 화자 인식이라고 한다.​​https://ko.wikipedia.org/wiki/%EC%9D%8C%EC%84%B1_%EC%9D%B8%EC%8B%9D 음성 인식 - 위키백과, 우리 모두의 백과사전위키백과 아시아의 달 온라인 에디터톤 이 11월 1일부터 30일까지 열립니다. 코로나19 에디터톤 이 11월 13일부터 30일까지 열립니다. 음성 인식 위키백과, 우리 모두의 백과사전. 음성 인식 ( Speech Recognition )이란 사람 이 말하는 음성 언어를 컴퓨터 가 해석해 그 내용을 문자 데이터로 전환하는 처리를 말한다. STT (Speech-to-Text)라고도 한다. 키보드 대신 문자를 입력하는 방식으로 주목을 받고 있다. 로봇 , 텔레매틱스 등 음성으로 기기제어, 정보검색이 필요한 경우에 응용된다. 대표적인 알고...ko.wikipedia.org ​ "
how do you plan to market your new speech recognition software? ,https://blog.naver.com/decentlife996/222720459758,20220504,새로운 음성인식 소프트웨어를 어떻게 마케팅 할계획입니까?​We're developing a strategy 전략을 짜고 있습니다.​some boxes have been stacked on shelves.상자들이 선반에 쌓여있다.​who do we call to report a problem with the copy machine?복사기 문제를 신고하려면 누구에게 전화해야합니까? 
OpenSpeech: Open-Source Toolkit for End-to-End Speech Recognition ,https://blog.naver.com/sooftware/222388903196,20210607,"OpenSpeech ""sooftware""​주말동안 틈틈이 + 대학교 후배의 도움으로 OpenSpeech라는 라이브러리를 공개했습니다.​https://github.com/sooftware/openspeech sooftware/openspeechOpen-Source Toolkit for End-to-End Speech Recognition leveraging PyTorch-Lightning and Hydra. - sooftware/openspeechgithub.com PyTorch-Lightning + Hydra 이 2개 라이브러리를 이용해서 만들었으며,총 17개의 모델과 영어, 중국어, 한국어 3개에 대한 레시피를 제공합니다.​많은 관심 & 스타 부탁드려요! "
■ 디지털신호처리 : Chapter 22: Audio Processing - Speech Synthesis and Recognition ,https://blog.naver.com/kasbel/222989408246,20230120,"22장: 오디오 처리음성 합성 및 인식​컴퓨터 생성 및 음성 인식은 만만치 않은 문제입니다. 약간의 성공으로 많은 접근 방식이 시도되었습니다. 이것은 DSP 연구의 활발한 영역이며 의심할 여지없이 앞으로도 오랫동안 그렇게 될 것입니다. 음성 합성 및 인식 회로를 구축하는 방법을 설명하는 이 섹션을 기대했다면 매우 실망할 것입니다. 여기서는 일반적인 접근 방식에 대한 간략한 소개만 제공할 수 있습니다. 시작하기 전에 사람이 말하는 음성을 생성하는 대부분의 상용 제품은 음성을 합성 하지 않고 단순히 사람이 말하는 디지털 녹음 부분을 재생 한다는 점을 지적해야 합니다 . 이 접근 방식은 뛰어난 음질을 제공하지만 미리 녹음된 단어와 구로 제한됩니다.​거의 모든 음성 합성 및 인식 기술은 그림 22-8에 표시된 인간 음성 생성 모델을 기반으로 합니다. 대부분의 사람의 말소리는 유성음 또는 마찰음 으로 분류할 수 있습니다 . 유성음은 공기가 폐에서 성대를 통해 입 및/또는 코 밖으로 나올 때 발생합니다. 성대는 두 개의 얇은 조직 플랩입니다. 아담의 사과 바로 뒤에 있는 공기 흐름을 가로질러 뻗어 있습니다. 다양한 근육 긴장에 반응하여 성대가 50~1000Hz의 주파수로 진동하여 주기적인 공기 퍼프가 목구멍으로 주입됩니다. 모음은 유성음의 예입니다. 그림 22-8에서 유성음은 조정 가능한 매개변수인 피치(즉, 파형의 기본 주파수)와 함께 펄스 트레인 생성기로 표현됩니다.​이에 비해 마찰음 은 성대의 진동이 아닌 임의의 소음에서 발생합니다. 이것은 공기 흐름이 혀, 입술 및/또는 치아에 의해 거의 차단되어 협착부 근처에서 난기류가 발생할 때 발생합니다. 마찰음에는 s , f , sh , z , v 및 th 가 포함 됩니다. 그림 22-8의 모델에서 마찰음은 잡음 발생기 로 표현된다 .​이 두 음원은 혀, 입술, 입, 인후 및 비강에서 형성된 음향 공동에 의해 수정됩니다. 이러한 구조를 통한 사운드 전파는 선형 프로세스이므로 임펄스 응답이 적절하게 선택된 선형 필터로 나타낼 수 있습니다. 대부분의 경우 재귀 필터는 필터의 특성을 지정하는 재귀 계수와 함께 모델에 사용됩니다. 음향 공동의 크기가 수 센티미터이기 때문에 주파수 응답은 주로 킬로헤르츠 범위의 일련의 공명입니다. 오디오 처리 전문 용어로 이러한 공명 피크를 형식 주파수 라고 합니다.. 혀와 입술의 상대적 위치를 변경하여 형식 주파수를 주파수와 진폭 모두에서 변경할 수 있습니다.​그림 22-9는 음성 신호, 음성 스펙트로그램 또는 성문 을 표시하는 일반적인 방법을 보여줍니다 . 오디오 신호는 짧은 세그먼트(예: 2~40밀리초)로 나뉘며 FFT는 각 세그먼트의 주파수 스펙트럼을 찾는 데 사용됩니다. 이러한 스펙트럼은 나란히 배치되어 그레이스케일 이미지로 변환됩니다(낮은 진폭은 밝아지고 높은 진폭은 어두워짐). 이것은 음성의 주파수 내용이 시간에 따라 어떻게 변하는지를 관찰하는 그래픽 방식을 제공합니다. 세그먼트 길이는 주파수 분해능 (더 긴 세그먼트가 선호)과 시간 분해능 (짧은 세그먼트가 선호 ) 간의 균형으로 선택됩니다 .​비 의 a 에 의해 입증된 것처럼 유성음은 (a)에 표시된 주기적인 시간 영역 파형과 (b)에 표시된 일련의 규칙적인 간격의 고조파인 주파수 스펙트럼을 갖습니다. 이에 비해 storm 의 s 는 마찰음이 (c)와 같은 잡음이 있는 시간 영역 신호와 (d)에 표시된 잡음이 있는 스펙트럼을 가짐을 보여줍니다. 이 스펙트럼은 또한 두 사운드에 대한 형식 주파수에 의한 형태를 보여줍니다. 또한 비라 는 단어의 시간-주파수 표시가 두 번 모두 비슷하게 보입니다.​예를 들어 25밀리초라는 짧은 시간 동안 세 가지 매개변수를 지정하여 음성 신호를 근사화할 수 있습니다. 3) 성도 응답을 모방하는 데 사용되는 디지털 필터의 계수. 그런 다음 이 세 가지 매개변수를 초당 약 40회 지속적으로 업데이트하여 연속 음성을 합성할 수 있습니다. 이 접근 방식은 DSP의 초기 상업적 성공 중 하나인 어린이용 전자 학습 보조 장치인 Speak & Spell 을 담당했습니다. 이러한 유형의 음성 합성의 음질은 열악하여 매우 기계적이고 인간적이지 않은 소리로 들립니다. 그러나 일반적으로 몇 kbits/sec에 불과한 매우 낮은 데이터 전송률이 필요합니다.​이것은 또한 음성 압축의 LPC ( Linear Predictive Coding ) 방법의 기초이기도 합니다. 디지털로 녹음된 사람의 음성은 짧은 세그먼트로 나뉘며 각각은 모델의 세 가지 매개변수에 따라 특성화됩니다. 일반적으로 세그먼트당 약 12바이트 또는 초당 2~6kbytes가 필요합니다. 세그먼트 정보는 필요에 따라 전송되거나 저장되며 음성 합성기로 재구성됩니다.​음성 인식 알고리즘은 추출된 매개변수의 패턴 인식을 시도함으로써 이를 한 단계 더 발전시킵니다. 여기에는 일반적으로 구어를 식별하기 위해 세그먼트 정보를 이전에 저장된 소리의 템플릿과 비교하는 것이 포함됩니다. 문제는 이 방법이 잘 통하지 않는다는 것입니다. 일부 응용 프로그램에는 유용하지만 인간 청취자의 기능에는 훨씬 못 미칩니다. 컴퓨터에서 음성 인식이 왜 그렇게 어려운지 이해하기 위해 누군가가 예기치 않게 다음 문장을 말하는 것을 상상해 보십시오. 더 큰 의료용 구매 개는 거의 언제 운이 좋았습니다.​물론 이 문장에는 아무 의미가 없기 때문에 이 문장의 의미를 이해하지 못할 것입니다. 더 중요한 것은 아마도 말한 모든 개별 단어를 이해하지 못할 것입니다. 이것은 인간이 살아가는 방식의 기본입니다. 음성을 인식하고 이해합니다. 단어는 소리로 인식되지만 문장 의 맥락 과 듣는 사람의 기대치 로도 인식됩니다. 예를 들어 다음 두 문장을 듣는다고 상상해 보십시오.그 아이는 할로윈에 거미 반지 를 끼고 있었다.그는 전쟁 중에 미국 스파이였습니다 .​밑줄 친 단어를 전달하기 위해 정확히 같은 소리가 생성되더라도 청취자 는 문맥에 맞는 올바른 단어를 듣게 됩니다. 세상에 대한 당신의 축적된 지식을 통해 당신은 아이들이 비밀 요원을 착용하지 않고 사람들이 전시 중에 으스스한 보석이 되지 않는다는 것을 알고 있습니다. 이것은 일반적으로 의식적인 행동이 아니라 인간 청각의 고유한 부분입니다.​대부분의 음성 인식 알고리즘은 문맥이 아닌 개별 단어의 소리에만 의존합니다. 그들은 단어를 인식 하려고 시도 하지만 말을 이해 하지는 못합니다 . 이것은 인간 청취자와 비교할 때 엄청난 불리한 위치에 있습니다. 세 가지 성가심은 음성 인식 시스템에서 일반적입니다. (1) 인식된 음성에는 단어 사이에 뚜렷한 일시 중지가 있어야 합니다. 이렇게 하면 소리는 비슷하지만 다른 단어로 구성된 구문을 처리하는 알고리즘이 필요하지 않습니다(예: 스파이더 링 및 스파이). 겹치는 흐름으로 말하는 데 익숙한 사람들에게는 느리고 어색합니다. (2) 어휘는 종종 수백 단어로 제한됩니다. 이는 알고리즘이 최상의 일치를 찾기 위해 제한된 세트만 검색하면 됨을 의미합니다. 어휘가 커질수록 인식 시간과 오류율이 모두 증가합니다. (3) 알고리즘은 각 화자에 대해 훈련 되어야 합니다. 이를 위해서는 시스템을 사용하는 각 사람이 인식할 각 단어를 말해야 하며 종종 5~10회 반복해야 합니다. 이 개인화된 데이터베이스는 단어 인식의 정확도를 크게 증가시키지만 불편하고 시간이 많이 걸립니다.​성공적인 음성 인식 기술 개발에 대한 상금은 엄청납니다. 음성은 인간이 의사소통을 할 수 있는 가장 빠르고 효율적인 방법입니다. 음성 인식은 쓰기, 타이핑, 키보드 입력, 스위치와 손잡이가 제공하는 전자 제어를 대체할 가능성이 있습니다. 상업적인 시장에서 받아들여지려면 조금 더 잘 작동하면 됩니다. 음성 인식의 발전은 DSP 자체를 통해서 뿐만 아니라 인공 지능 및 신경망 영역에서도 이루어질 것입니다. 이것을 기술적인 어려움 으로 생각하지 마십시오 . 기술적인 기회 라고 생각 하세요.​​ "
STT 오픈소스 검색 결과 정리 ,https://blog.naver.com/websearch/223110240285,20230524,"STT 관련 오픈소스 검색을 시작하였고 본 포스트에 계속 정리할 계획입니다.​먼저 관련 용어 정리는 다음과 같습니다.​STT = Speech To TextASR = Automatic Speech Recognition  현재까지 검색된 STT 관련 오픈 소스 프로젝트는 다음과 같습니다.​* https://github.com/goodatlas/zeroth  - Kaldi 기반 한국어 ASR 오픈소스 프로젝트​* https://github.com/kaldi-asr/kaldi  - Kaldi is a toolkit for speech recognition written in C++ and licensed under the Apache License v2.0. Kaldi is intended for use by speech recognition researchers.  - http://kaldi-asr.org/doc/​* VOSK ( https://alphacephei.com/vosk/ )  - Supports 20+ languages and dialects - English, Indian English, German, French, Spanish, Portuguese, Chinese, Russian, Turkish, Vietnamese, Italian, Dutch, Catalan, Arabic, Greek, Farsi, Filipino, Ukrainian, Kazakh, Swedish, Japanese, Esperanto, Hindi, Czech, Polish, Uzbek, Korean. More to come  - 기본 제공되는 한국어는 인식률이 0% 인 것 같음  - docker, websocket 등을 지원하므로 편리한 연동이 가능한 것 같음  - https://github.com/alphacep/vosk-server  - https://alphacephei.com/vosk/models​* https://github.com/homink/kaldi-asr.ko​* Whisper ( https://github.com/openai/whisper )  - OpenAI 에서 개발한 STT  - Whisper is a general-purpose speech recognition model. It is trained on a large dataset of diverse audio and is also a multitasking model that can perform multilingual speech recognition, speech translation, and language identification.  - https://openai.com/research/whisper  - https://towardsdatascience.com/unlock-the-power-of-audio-data-advanced-transcription-and-diarization-with-whisper-whisperx-and-ed9424307281​ ​[참고 자료]​* https://fosspost.org/open-source-speech-recognition/​ "
How to install speeach_recognition package ,https://blog.naver.com/flower3962/222979977498,20230110,"#speeach_recognition​The Ultimate Guide To Speech Recognition With Pythonby David Amos 126 Comments advanced data-science machine-learningTweet Share EmailTable of ContentsHow Speech Recognition Works – An OverviewPicking a Python Speech Recognition PackageInstalling SpeechRecognitionThe Recognizer ClassWorking With Audio FilesSupported File TypesUsing record() to Capture Data From a FileCapturing Segments With offset and durationThe Effect of Noise on Speech RecognitionWorking With MicrophonesInstalling PyAudioThe Microphone ClassUsing listen() to Capture Microphone InputHandling Unrecognizable SpeechPutting It All Together: A “Guess the Word” GameRecap and Additional ResourcesAppendix: Recognizing Speech in Languages Other Than English  Remove ads  Watch Now This tutorial has a related video course created by the Real Python team. Watch it together with the written tutorial to deepen your understanding: Speech Recognition With Python Have you ever wondered how to add speech recognition to your Python project? If so, then keep reading! It’s easier than you might think.Far from a being a fad, the overwhelming success of speech-enabled products like Amazon Alexa has proven that some degree of speech support will be an essential aspect of household tech for the foreseeable future. If you think about it, the reasons why are pretty obvious. Incorporating speech recognition into your Python application offers a level of interactivity and accessibility that few technologies can match.The accessibility improvements alone are worth considering. Speech recognition allows the elderly and the physically and visually impaired to interact with state-of-the-art products and services quickly and naturally—no GUI needed!Best of all, including speech recognition in a Python project is really simple. In this guide, you’ll find out how. You’ll learn:How speech recognition works,What packages are available on PyPI; andHow to install and use the SpeechRecognition package—a full-featured and easy-to-use Python speech recognition library.In the end, you’ll apply what you’ve learned to a simple “Guess the Word” game and see how it all comes together. Free Bonus: Click here to download a Python speech recognition sample project with full source code that you can use as a basis for your own speech recognition apps. How Speech Recognition Works – An OverviewBefore we get to the nitty-gritty of doing speech recognition in Python, let’s take a moment to talk about how speech recognition works. A full discussion would fill a book, so I won’t bore you with all of the technical details here. In fact, this section is not pre-requisite to the rest of the tutorial. If you’d like to get straight to the point, then feel free to skip ahead.Speech recognition has its roots in research done at Bell Labs in the early 1950s. Early systems were limited to a single speaker and had limited vocabularies of about a dozen words. Modern speech recognition systems have come a long way since their ancient counterparts. They can recognize speech from multiple speakers and have enormous vocabularies in numerous languages.The first component of speech recognition is, of course, speech. Speech must be converted from physical sound to an electrical signal with a microphone, and then to digital data with an analog-to-digital converter. Once digitized, several models can be used to transcribe the audio to text.Most modern speech recognition systems rely on what is known as a Hidden Markov Model (HMM). This approach works on the assumption that a speech signal, when viewed on a short enough timescale (say, ten milliseconds), can be reasonably approximated as a stationary process—that is, a process in which statistical properties do not change over time.In a typical HMM, the speech signal is divided into 10-millisecond fragments. The power spectrum of each fragment, which is essentially a plot of the signal’s power as a function of frequency, is mapped to a vector of real numbers known as cepstral coefficients. The dimension of this vector is usually small—sometimes as low as 10, although more accurate systems may have dimension 32 or more. The final output of the HMM is a sequence of these vectors.To decode the speech into text, groups of vectors are matched to one or more phonemes—a fundamental unit of speech. This calculation requires training, since the sound of a phoneme varies from speaker to speaker, and even varies from one utterance to another by the same speaker. A special algorithm is then applied to determine the most likely word (or words) that produce the given sequence of phonemes.One can imagine that this whole process may be computationally expensive. In many modern speech recognition systems, neural networks are used to simplify the speech signal using techniques for feature transformation and dimensionality reduction before HMM recognition. Voice activity detectors (VADs) are also used to reduce an audio signal to only the portions that are likely to contain speech. This prevents the recognizer from wasting time analyzing unnecessary parts of the signal.Fortunately, as a Python programmer, you don’t have to worry about any of this. A number of speech recognition services are available for use online through an API, and many of these services offer Python SDKs.  Remove adsPicking a Python Speech Recognition PackageA handful of packages for speech recognition exist on PyPI. A few of them include:apiaiassemblyaigoogle-cloud-speechpocketsphinxSpeechRecognitionwatson-developer-cloudwitSome of these packages—such as wit and apiai—offer built-in features, like natural language processing for identifying a speaker’s intent, which go beyond basic speech recognition. Others, like google-cloud-speech, focus solely on speech-to-text conversion.There is one package that stands out in terms of ease-of-use: SpeechRecognition.Recognizing speech requires audio input, and SpeechRecognition makes retrieving this input really easy. Instead of having to build scripts for accessing microphones and processing audio files from scratch, SpeechRecognition will have you up and running in just a few minutes.The SpeechRecognition library acts as a wrapper for several popular speech APIs and is thus extremely flexible. One of these—the Google Web Speech API—supports a default API key that is hard-coded into the SpeechRecognition library. That means you can get off your feet without having to sign up for a service.The flexibility and ease-of-use of the SpeechRecognition package make it an excellent choice for any Python project. However, support for every feature of each API it wraps is not guaranteed. You will need to spend some time researching the available options to find out if SpeechRecognition will work in your particular case.So, now that you’re convinced you should try out SpeechRecognition, the next step is getting it installed in your environment.Installing SpeechRecognitionSpeechRecognition is compatible with Python 2.6, 2.7 and 3.3+, but requires some additional installation steps for Python 2. For this tutorial, I’ll assume you are using Python 3.3+.You can install SpeechRecognition from a terminal with pip:$ pip install SpeechRecognition Once installed, you should verify the installation by opening an interpreter session and typing:>>>>>> import speech_recognition as sr >>> sr.__version__ '3.8.1' Note: The version number you get might vary. Version 3.8.1 was the latest at the time of writing. Go ahead and keep this session open. You’ll start to work with it in just a bit.SpeechRecognition will work out of the box if all you need to do is work with existing audio files. Specific use cases, however, require a few dependencies. Notably, the PyAudio package is needed for capturing microphone input.You’ll see which dependencies you need as you read further. For now, let’s dive in and explore the basics of the package.The Recognizer ClassAll of the magic in SpeechRecognition happens with the Recognizer class.The primary purpose of a Recognizer instance is, of course, to recognize speech. Each instance comes with a variety of settings and functionality for recognizing speech from an audio source.Creating a Recognizer instance is easy. In your current interpreter session, just type:>>>>>> r = sr.Recognizer()Each Recognizer instance has seven methods for recognizing speech from an audio source using various APIs. These are:recognize_bing(): Microsoft Bing Speechrecognize_google(): Google Web Speech APIrecognize_google_cloud(): Google Cloud Speech - requires installation of the google-cloud-speech packagerecognize_houndify(): Houndify by SoundHoundrecognize_ibm(): IBM Speech to Textrecognize_sphinx(): CMU Sphinx - requires installing PocketSphinxrecognize_wit(): Wit.aiOf the seven, only recognize_sphinx() works offline with the CMU Sphinx engine. The other six all require an internet connection.A full discussion of the features and benefits of each API is beyond the scope of this tutorial. Since SpeechRecognition ships with a default API key for the Google Web Speech API, you can get started with it right away. For this reason, we’ll use the Web Speech API in this guide. The other six APIs all require authentication with either an API key or a username/password combination. For more information, consult the SpeechRecognition docs. Caution: The default key provided by SpeechRecognition is for testing purposes only, and Google may revoke it at any time. It is not a good idea to use the Google Web Speech API in production. Even with a valid API key, you’ll be limited to only 50 requests per day, and there is no way to raise this quota. Fortunately, SpeechRecognition’s interface is nearly identical for each API, so what you learn today will be easy to translate to a real-world project. Each recognize_*() method will throw a speech_recognition.RequestError exception if the API is unreachable. For recognize_sphinx(), this could happen as the result of a missing, corrupt or incompatible Sphinx installation. For the other six methods, RequestError may be thrown if quota limits are met, the server is unavailable, or there is no internet connection.Ok, enough chit-chat. Let’s get our hands dirty. Go ahead and try to call recognize_google() in your interpreter session.>>>>>> r.recognize_google()What happened?You probably got something that looks like this:Traceback (most recent call last): File ""<stdin>"", line 1, in <module> TypeError: recognize_google() missing 1 required positional argument: 'audio_data'You might have guessed this would happen. How could something be recognized from nothing?All seven recognize_*() methods of the Recognizer class require an audio_data argument. In each case, audio_data must be an instance of SpeechRecognition’s AudioData class.There are two ways to create an AudioData instance: from an audio file or audio recorded by a microphone. Audio files are a little easier to get started with, so let’s take a look at that first.  Remove adsWorking With Audio FilesBefore you continue, you’ll need to download an audio file. The one I used to get started, “harvard.wav,” can be found here. Make sure you save it to the same directory in which your Python interpreter session is running.SpeechRecognition makes working with audio files easy thanks to its handy AudioFile class. This class can be initialized with the path to an audio file and provides a context manager interface for reading and working with the file’s contents.Supported File TypesCurrently, SpeechRecognition supports the following file formats:WAV: must be in PCM/LPCM formatAIFFAIFF-CFLAC: must be native FLAC format; OGG-FLAC is not supportedIf you are working on x-86 based Linux, macOS or Windows, you should be able to work with FLAC files without a problem. On other platforms, you will need to install a FLAC encoder and ensure you have access to the flac command line tool. You can find more information here if this applies to you.Using record() to Capture Data From a FileType the following into your interpreter session to process the contents of the “harvard.wav” file:>>>>>> harvard = sr.AudioFile('harvard.wav') >>> with harvard as source: ... audio = r.record(source) ...The context manager opens the file and reads its contents, storing the data in an AudioFile instance called source. Then the record() method records the data from the entire file into an AudioData instance. You can confirm this by checking the type of audio:>>>>>> type(audio) <class 'speech_recognition.AudioData'>You can now invoke recognize_google() to attempt to recognize any speech in the audio. Depending on your internet connection speed, you may have to wait several seconds before seeing the result.>>>>>> r.recognize_google(audio) 'the stale smell of old beer lingers it takes heat to bring out the odor a cold dip restores health and zest a salt pickle taste fine with ham tacos al Pastore are my favorite a zestful food is the hot cross bun'Congratulations! You’ve just transcribed your first audio file!If you’re wondering where the phrases in the “harvard.wav” file come from, they are examples of Harvard Sentences. These phrases were published by the IEEE in 1965 for use in speech intelligibility testing of telephone lines. They are still used in VoIP and cellular testing today.The Harvard Sentences are comprised of 72 lists of ten phrases. You can find freely available recordings of these phrases on the Open Speech Repository website. Recordings are available in English, Mandarin Chinese, French, and Hindi. They provide an excellent source of free material for testing your code.Capturing Segments With offset and durationWhat if you only want to capture a portion of the speech in a file? The record() method accepts a duration keyword argument that stops the recording after a specified number of seconds.For example, the following captures any speech in the first four seconds of the file:>>>>>> with harvard as source: ... audio = r.record(source, duration=4) ... >>> r.recognize_google(audio) 'the stale smell of old beer lingers'The record() method, when used inside a with block, always moves ahead in the file stream. This means that if you record once for four seconds and then record again for four seconds, the second time returns the four seconds of audio after the first four seconds.>>>>>> with harvard as source: ... audio1 = r.record(source, duration=4) ... audio2 = r.record(source, duration=4) ... >>> r.recognize_google(audio1) 'the stale smell of old beer lingers' >>> r.recognize_google(audio2) 'it takes heat to bring out the odor a cold dip'Notice that audio2 contains a portion of the third phrase in the file. When specifying a duration, the recording might stop mid-phrase—or even mid-word—which can hurt the accuracy of the transcription. More on this in a bit.In addition to specifying a recording duration, the record() method can be given a specific starting point using the offset keyword argument. This value represents the number of seconds from the beginning of the file to ignore before starting to record.To capture only the second phrase in the file, you could start with an offset of four seconds and record for, say, three seconds.>>>>>> with harvard as source: ... audio = r.record(source, offset=4, duration=3) ... >>> r.recognize_google(audio) 'it takes heat to bring out the odor'The offset and duration keyword arguments are useful for segmenting an audio file if you have prior knowledge of the structure of the speech in the file. However, using them hastily can result in poor transcriptions. To see this effect, try the following in your interpreter:>>>>>> with harvard as source: ... audio = r.record(source, offset=4.7, duration=2.8) ... >>> r.recognize_google(audio) 'Mesquite to bring out the odor Aiko'By starting the recording at 4.7 seconds, you miss the “it t” portion a the beginning of the phrase “it takes heat to bring out the odor,” so the API only got “akes heat,” which it matched to “Mesquite.”Similarly, at the end of the recording, you captured “a co,” which is the beginning of the third phrase “a cold dip restores health and zest.” This was matched to “Aiko” by the API.There is another reason you may get inaccurate transcriptions. Noise! The above examples worked well because the audio file is reasonably clean. In the real world, unless you have the opportunity to process audio files beforehand, you can not expect the audio to be noise-free.  Remove adsThe Effect of Noise on Speech RecognitionNoise is a fact of life. All audio recordings have some degree of noise in them, and un-handled noise can wreck the accuracy of speech recognition apps.To get a feel for how noise can affect speech recognition, download the “jackhammer.wav” file here. As always, make sure you save this to your interpreter session’s working directory.This file has the phrase “the stale smell of old beer lingers” spoken with a loud jackhammer in the background.What happens when you try to transcribe this file?>>>>>> jackhammer = sr.AudioFile('jackhammer.wav') >>> with jackhammer as source: ... audio = r.record(source) ... >>> r.recognize_google(audio) 'the snail smell of old gear vendors'Way off!So how do you deal with this? One thing you can try is using the adjust_for_ambient_noise() method of the Recognizer class.>>>>>> with jackhammer as source: ... r.adjust_for_ambient_noise(source) ... audio = r.record(source) ... >>> r.recognize_google(audio) 'still smell of old beer vendors'That got you a little closer to the actual phrase, but it still isn’t perfect. Also, “the” is missing from the beginning of the phrase. Why is that?The adjust_for_ambient_noise() method reads the first second of the file stream and calibrates the recognizer to the noise level of the audio. Hence, that portion of the stream is consumed before you call record() to capture the data.You can adjust the time-frame that adjust_for_ambient_noise() uses for analysis with the duration keyword argument. This argument takes a numerical value in seconds and is set to 1 by default. Try lowering this value to 0.5.>>>>>> with jackhammer as source: ... r.adjust_for_ambient_noise(source, duration=0.5) ... audio = r.record(source) ... >>> r.recognize_google(audio) 'the snail smell like old Beer Mongers'Well, that got you “the” at the beginning of the phrase, but now you have some new issues! Sometimes it isn’t possible to remove the effect of the noise—the signal is just too noisy to be dealt with successfully. That’s the case with this file.If you find yourself running up against these issues frequently, you may have to resort to some pre-processing of the audio. This can be done with audio editing software or a Python package (such as SciPy) that can apply filters to the files. A detailed discussion of this is beyond the scope of this tutorial—check out Allen Downey’s Think DSP book if you are interested. For now, just be aware that ambient noise in an audio file can cause problems and must be addressed in order to maximize the accuracy of speech recognition.When working with noisy files, it can be helpful to see the actual API response. Most APIs return a JSON string containing many possible transcriptions. The recognize_google() method will always return the most likely transcription unless you force it to give you the full response.You can do this by setting the show_all keyword argument of the recognize_google() method to True.>>>>>> r.recognize_google(audio, show_all=True) {'alternative': [ {'transcript': 'the snail smell like old Beer Mongers'}, {'transcript': 'the still smell of old beer vendors'}, {'transcript': 'the snail smell like old beer vendors'}, {'transcript': 'the stale smell of old beer vendors'}, {'transcript': 'the snail smell like old beermongers'}, {'transcript': 'destihl smell of old beer vendors'}, {'transcript': 'the still smell like old beer vendors'}, {'transcript': 'bastille smell of old beer vendors'}, {'transcript': 'the still smell like old beermongers'}, {'transcript': 'the still smell of old beer venders'}, {'transcript': 'the still smelling old beer vendors'}, {'transcript': 'musty smell of old beer vendors'}, {'transcript': 'the still smell of old beer vendor'} ], 'final': True}As you can see, recognize_google() returns a dictionary with the key 'alternative' that points to a list of possible transcripts. The structure of this response may vary from API to API and is mainly useful for debugging.By now, you have a pretty good idea of the basics of the SpeechRecognition package. You’ve seen how to create an AudioFile instance from an audio file and use the record() method to capture data from the file. You learned how to record segments of a file using the offset and duration keyword arguments of record(), and you experienced the detrimental effect noise can have on transcription accuracy.Now for the fun part. Let’s transition from transcribing static audio files to making your project interactive by accepting input from a microphone.  Remove adsWorking With MicrophonesTo access your microphone with SpeechRecognizer, you’ll have to install the PyAudio package. Go ahead and close your current interpreter session, and let’s do that.Installing PyAudioThe process for installing PyAudio will vary depending on your operating system.Debian LinuxIf you’re on Debian-based Linux (like Ubuntu) you can install PyAudio with apt:$ sudo apt-get install python-pyaudio python3-pyaudio Once installed, you may still need to run pip install pyaudio, especially if you are working in a virtual environment.macOSFor macOS, first you will need to install PortAudio with Homebrew, and then install PyAudio with pip:$ brew install portaudio $ pip install pyaudio WindowsOn Windows, you can install PyAudio with pip:$ pip install pyaudio Testing the InstallationOnce you’ve got PyAudio installed, you can test the installation from the console.$ python -m speech_recognition Make sure your default microphone is on and unmuted. If the installation worked, you should see something like this:A moment of silence, please... Set minimum energy threshold to 600.4452854381937 Say something! Go ahead and play around with it a little bit by speaking into your microphone and seeing how well SpeechRecognition transcribes your speech. Note: If you are on Ubuntu and get some funky output like ‘ALSA lib … Unknown PCM’, refer to this page for tips on suppressing these messages. This output comes from the ALSA package installed with Ubuntu—not SpeechRecognition or PyAudio. In all reality, these messages may indicate a problem with your ALSA configuration, but in my experience, they do not impact the functionality of your code. They are mostly a nuisance.  Remove adsThe Microphone ClassOpen up another interpreter session and create an instance of the recognizer class.>>>>>> import speech_recognition as sr >>> r = sr.Recognizer()Now, instead of using an audio file as the source, you will use the default system microphone. You can access this by creating an instance of the Microphone class.>>>>>> mic = sr.Microphone()If your system has no default microphone (such as on a Raspberry Pi), or you want to use a microphone other than the default, you will need to specify which one to use by supplying a device index. You can get a list of microphone names by calling the list_microphone_names() static method of the Microphone class.>>>>>> sr.Microphone.list_microphone_names() ['HDA Intel PCH: ALC272 Analog (hw:0,0)', 'HDA Intel PCH: HDMI 0 (hw:0,3)', 'sysdefault', 'front', 'surround40', 'surround51', 'surround71', 'hdmi', 'pulse', 'dmix', 'default']Note that your output may differ from the above example.The device index of the microphone is the index of its name in the list returned by list_microphone_names(). For example, given the above output, if you want to use the microphone called “front,” which has index 3 in the list, you would create a microphone instance like this:>>>>>> # This is just an example; do not run >>> mic = sr.Microphone(device_index=3)For most projects, though, you’ll probably want to use the default system microphone.Using listen() to Capture Microphone InputNow that you’ve got a Microphone instance ready to go, it’s time to capture some input.Just like the AudioFile class, Microphone is a context manager. You can capture input from the microphone using the listen() method of the Recognizer class inside of the with block. This method takes an audio source as its first argument and records input from the source until silence is detected.>>>>>> with mic as source: ... audio = r.listen(source) ...Once you execute the with block, try speaking “hello” into your microphone. Wait a moment for the interpreter prompt to display again. Once the “>>>” prompt returns, you’re ready to recognize the speech.>>>>>> r.recognize_google(audio) 'hello'If the prompt never returns, your microphone is most likely picking up too much ambient noise. You can interrupt the process with Ctrl+C to get your prompt back.To handle ambient noise, you’ll need to use the adjust_for_ambient_noise() method of the Recognizer class, just like you did when trying to make sense of the noisy audio file. Since input from a microphone is far less predictable than input from an audio file, it is a good idea to do this anytime you listen for microphone input.>>>>>> with mic as source: ... r.adjust_for_ambient_noise(source) ... audio = r.listen(source) ...After running the above code, wait a second for adjust_for_ambient_noise() to do its thing, then try speaking “hello” into the microphone. Again, you will have to wait a moment for the interpreter prompt to return before trying to recognize the speech.Recall that adjust_for_ambient_noise() analyzes the audio source for one second. If this seems too long to you, feel free to adjust this with the duration keyword argument.The SpeechRecognition documentation recommends using a duration no less than 0.5 seconds. In some cases, you may find that durations longer than the default of one second generate better results. The minimum value you need depends on the microphone’s ambient environment. Unfortunately, this information is typically unknown during development. In my experience, the default duration of one second is adequate for most applications.  Remove adsHandling Unrecognizable SpeechTry typing the previous code example in to the interpeter and making some unintelligible noises into the microphone. You should get something like this in response:Traceback (most recent call last): File ""<stdin>"", line 1, in <module> File ""/home/david/real_python/speech_recognition_primer/venv/lib/python3.5/site-packages/speech_recognition/__init__.py"", line 858, in recognize_google if not isinstance(actual_result, dict) or len(actual_result.get(""alternative"", [])) == 0: raise UnknownValueError() speech_recognition.UnknownValueErrorAudio that cannot be matched to text by the API raises an UnknownValueError exception. You should always wrap calls to the API with try and except blocks to handle this exception. Note: You may have to try harder than you expect to get the exception thrown. The API works very hard to transcribe any vocal sounds. Even short grunts were transcribed as words like “how” for me. Coughing, hand claps, and tongue clicks would consistently raise the exception. Putting It All Together: A “Guess the Word” GameNow that you’ve seen the basics of recognizing speech with the SpeechRecognition package let’s put your newfound knowledge to use and write a small game that picks a random word from a list and gives the user three attempts to guess the word.Here is the full script:import random import time import speech_recognition as sr def recognize_speech_from_mic(recognizer, microphone): """"""Transcribe speech from recorded from `microphone`. Returns a dictionary with three keys: ""success"": a boolean indicating whether or not the API request was successful ""error"": `None` if no error occured, otherwise a string containing an error message if the API could not be reached or speech was unrecognizable ""transcription"": `None` if speech could not be transcribed, otherwise a string containing the transcribed text """""" # check that recognizer and microphone arguments are appropriate type if not isinstance(recognizer, sr.Recognizer): raise TypeError(""`recognizer` must be `Recognizer` instance"") if not isinstance(microphone, sr.Microphone): raise TypeError(""`microphone` must be `Microphone` instance"") # adjust the recognizer sensitivity to ambient noise and record audio # from the microphone with microphone as source: recognizer.adjust_for_ambient_noise(source) audio = recognizer.listen(source) # set up the response object response = { ""success"": True, ""error"": None, ""transcription"": None } # try recognizing the speech in the recording # if a RequestError or UnknownValueError exception is caught, # update the response object accordingly try: response[""transcription""] = recognizer.recognize_google(audio) except sr.RequestError: # API was unreachable or unresponsive response[""success""] = False response[""error""] = ""API unavailable"" except sr.UnknownValueError: # speech was unintelligible response[""error""] = ""Unable to recognize speech"" return response if __name__ == ""__main__"": # set the list of words, maxnumber of guesses, and prompt limit WORDS = [""apple"", ""banana"", ""grape"", ""orange"", ""mango"", ""lemon""] NUM_GUESSES = 3 PROMPT_LIMIT = 5 # create recognizer and mic instances recognizer = sr.Recognizer() microphone = sr.Microphone() # get a random word from the list word = random.choice(WORDS) # format the instructions string instructions = ( ""I'm thinking of one of these words:\n"" ""{words}\n"" ""You have {n} tries to guess which one.\n"" ).format(words=', '.join(WORDS), n=NUM_GUESSES) # show instructions and wait 3 seconds before starting the game print(instructions) time.sleep(3) for i in range(NUM_GUESSES): # get the guess from the user # if a transcription is returned, break out of the loop and # continue # if no transcription returned and API request failed, break # loop and continue # if API request succeeded but no transcription was returned, # re-prompt the user to say their guess again. Do this up # to PROMPT_LIMIT times for j in range(PROMPT_LIMIT): print('Guess {}. Speak!'.format(i+1)) guess = recognize_speech_from_mic(recognizer, microphone) if guess[""transcription""]: break if not guess[""success""]: break print(""I didn't catch that. What did you say?\n"") # if there was an error, stop the game if guess[""error""]: print(""ERROR: {}"".format(guess[""error""])) break # show the user the transcription print(""You said: {}"".format(guess[""transcription""])) # determine if guess is correct and if any attempts remain guess_is_correct = guess[""transcription""].lower() == word.lower() user_has_more_attempts = i < NUM_GUESSES - 1 # determine if the user has won the game # if not, repeat the loop if user has more attempts # if no attempts left, the user loses the game if guess_is_correct: print(""Correct! You win!"".format(word)) break elif user_has_more_attempts: print(""Incorrect. Try again.\n"") else: print(""Sorry, you lose!\nI was thinking of '{}'."".format(word)) breakLet’s break that down a little bit.The recognize_speech_from_mic() function takes a Recognizer and Microphone instance as arguments and returns a dictionary with three keys. The first key, ""success"", is a boolean that indicates whether or not the API request was successful. The second key, ""error"", is either None or an error message indicating that the API is unavailable or the speech was unintelligible. Finally, the ""transcription"" key contains the transcription of the audio recorded by the microphone.The function first checks that the recognizer and microphone arguments are of the correct type, and raises a TypeError if either is invalid:if not isinstance(recognizer, sr.Recognizer): raise TypeError('`recognizer` must be `Recognizer` instance') if not isinstance(microphone, sr.Microphone): raise TypeError('`microphone` must be a `Microphone` instance')The listen() method is then used to record microphone input:with microphone as source: recognizer.adjust_for_ambient_noise(source) audio = recognizer.listen(source)The adjust_for_ambient_noise() method is used to calibrate the recognizer for changing noise conditions each time the recognize_speech_from_mic() function is called.Next, recognize_google() is called to transcribe any speech in the recording. A try...except block is used to catch the RequestError and UnknownValueError exceptions and handle them accordingly. The success of the API request, any error messages, and the transcribed speech are stored in the success, error and transcription keys of the response dictionary, which is returned by the recognize_speech_from_mic() function.response = { ""success"": True, ""error"": None, ""transcription"": None } try: response[""transcription""] = recognizer.recognize_google(audio) except sr.RequestError: # API was unreachable or unresponsive response[""success""] = False response[""error""] = ""API unavailable"" except sr.UnknownValueError: # speech was unintelligible response[""error""] = ""Unable to recognize speech"" return responseYou can test the recognize_speech_from_mic() function by saving the above script to a file called “guessing_game.py” and running the following in an interpreter session:>>>>>> import speech_recognition as sr >>> from guessing_game import recognize_speech_from_mic >>> r = sr.Recognizer() >>> m = sr.Microphone() >>> recognize_speech_from_mic(r, m) {'success': True, 'error': None, 'transcription': 'hello'} >>> # Your output will vary depending on what you sayThe game itself is pretty simple. First, a list of words, a maximum number of allowed guesses and a prompt limit are declared:WORDS = ['apple', 'banana', 'grape', 'orange', 'mango', 'lemon'] NUM_GUESSES = 3 PROMPT_LIMIT = 5Next, a Recognizer and Microphone instance is created and a random word is chosen from WORDS:recognizer = sr.Recognizer() microphone = sr.Microphone() word = random.choice(WORDS)After printing some instructions and waiting for 3 three seconds, a for loop is used to manage each user attempt at guessing the chosen word. The first thing inside the for loop is another for loop that prompts the user at most PROMPT_LIMIT times for a guess, attempting to recognize the input each time with the recognize_speech_from_mic() function and storing the dictionary returned to the local variable guess.If the ""transcription"" key of guess is not None, then the user’s speech was transcribed and the inner loop is terminated with break. If the speech was not transcribed and the ""success"" key is set to False, then an API error occurred and the loop is again terminated with break. Otherwise, the API request was successful but the speech was unrecognizable. The user is warned and the for loop repeats, giving the user another chance at the current attempt.for j in range(PROMPT_LIMIT): print('Guess {}. Speak!'.format(i+1)) guess = recognize_speech_from_mic(recognizer, microphone) if guess[""transcription""]: break if not guess[""success""]: break print(""I didn't catch that. What did you say?\n"")Once the inner for loop terminates, the guess dictionary is checked for errors. If any occurred, the error message is displayed and the outer for loop is terminated with break, which will end the program execution.if guess['error']: print(""ERROR: {}"".format(guess[""error""])) breakIf there weren’t any errors, the transcription is compared to the randomly selected word. The lower() method for string objects is used to ensure better matching of the guess to the chosen word. The API may return speech matched to the word “apple” as “Apple” or “apple,” and either response should count as a correct answer.If the guess was correct, the user wins and the game is terminated. If the user was incorrect and has any remaining attempts, the outer for loop repeats and a new guess is retrieved. Otherwise, the user loses the game.guess_is_correct = guess[""transcription""].lower() == word.lower() user_has_more_attempts = i < NUM_GUESSES - 1 if guess_is_correct: print('Correct! You win!'.format(word)) break elif user_has_more_attempts: print('Incorrect. Try again.\n') else: print(""Sorry, you lose!\nI was thinking of '{}'."".format(word)) breakWhen run, the output will look something like this:I'm thinking of one of these words: apple, banana, grape, orange, mango, lemon You have 3 tries to guess which one. Guess 1. Speak! You said: banana Incorrect. Try again. Guess 2. Speak! You said: lemon Incorrect. Try again. Guess 3. Speak! You said: Orange Correct! You win!  Remove adsRecap and Additional ResourcesIn this tutorial, you’ve seen how to install the SpeechRecognition package and use its Recognizer class to easily recognize speech from both a file—using record()—and microphone input—using listen(). You also saw how to process segments of an audio file using the offset and duration keyword arguments of the record() method.You’ve seen the effect noise can have on the accuracy of transcriptions, and have learned how to adjust a Recognizer instance’s sensitivity to ambient noise with adjust_for_ambient_noise(). You have also learned which exceptions a Recognizer instance may throw—RequestError for bad API requests and UnkownValueError for unintelligible speech—and how to handle these with try...except blocks.Speech recognition is a deep subject, and what you have learned here barely scratches the surface. If you’re interested in learning more, here are some additional resources. Free Bonus: Click here to download a Python speech recognition sample project with full source code that you can use as a basis for your own speech recognition apps. For more information on the SpeechRecognition package:Library referenceExamplesTroubleshooting pageA few interesting internet resources:Behind the Mic: The Science of Talking with Computers. A short film about speech processing by Google.A Historical Perspective of Speech Recognition by Huang, Baker and Reddy. Communications of the ACM (2014). This article provides an in-depth and scholarly look at the evolution of speech recognition technology.The Past, Present and Future of Speech Recognition Technology by Clark Boyd at The Startup. This blog post presents an overview of speech recognition technology, with some thoughts about the future.Some good books about speech recognition:The Voice in the Machine: Building Computers That Understand Speech, Pieraccini, MIT Press (2012). An accessible general-audience book covering the history of, as well as modern advances in, speech processing.Fundamentals of Speech Recognition, Rabiner and Juang, Prentice Hall (1993). Rabiner, a researcher at Bell Labs, was instrumental in designing some of the first commercially viable speech recognizers. This book is now over 20 years old, but a lot of the fundamentals remain the same.Automatic Speech Recognition: A Deep Learning Approach, Yu and Deng, Springer (2014). Yu and Deng are researchers at Microsoft and both very active in the field of speech processing. This book covers a lot of modern approaches and cutting-edge research but is not for the mathematically faint-of-heart.Appendix: Recognizing Speech in Languages Other Than EnglishThroughout this tutorial, we’ve been recognizing speech in English, which is the default language for each recognize_*() method of the SpeechRecognition package. However, it is absolutely possible to recognize speech in other languages, and is quite simple to accomplish.To recognize speech in a different language, set the language keyword argument of the recognize_*() method to a string corresponding to the desired language. Most of the methods accept a BCP-47 language tag, such as 'en-US' for American English, or 'fr-FR' for French. For example, the following recognizes French speech in an audio file:import speech_recognition as sr r = sr.Recognizer() with sr.AudioFile('path/to/audiofile.wav') as source: audio = r.record(source) r.recognize_google(audio, language='fr-FR')Only the following methods accept a language keyword argument:recognize_bing()recognize_google()recognize_google_cloud()recognize_ibm()recognize_sphinx()To find out which language tags are supported by the API you are using, you’ll have to consult the corresponding documentation. A list of tags accepted by recognize_google() can be found in this Stack Overflow answer. "
딥러닝으로 음성인식 하는법 How to do Speech Recognition with Deep Learning ,https://blog.naver.com/takion7/221660937266,20190927,"음성 인식이 우리의 삶을 침범하고 있습니다. 휴대 전화, 게임 콘솔 및 스마트 워치에 내장되어 있습니다. 심지어 우리 집을 자동화하고 있습니다. 단 50 달러만으로 크게 주문하면 피자를 주문하거나 날씨 보고서를 보거나 쓰레기 봉투를 구입할 수있는 마술 상자 인 Amazon Echo Dot을 얻을 수 있습니다.   알렉사 Echo Dot은 이번 휴가철에 인기가 높아 아마존 에서 재고를 유지할 수없는 것 같습니다 !그러나 음성 인식은 수십 년 동안 사용되어 왔는데 왜 지금 주류에 도달하고 있습니까? 그 이유는 딥 러닝이 마침내 신중하게 제어되는 환경 외부에서 유용 할 정도로 음성 인식을 정확하게했기 때문입니다.Andrew Ng 는 음성 인식이 95 % 정확도에서 99 % 정확도로 갈수록 컴퓨터와 상호 작용하는 기본 방법이 될 것이라고 오랫동안 예측했습니다. 아이디어는이 4 % 정확도의 차이가 사이의 차이라는 것이다 성가 시게 신뢰성 과 매우 유용한 . 딥 러닝 덕분에 우리는 마침내 그 정점을 만들었습니다.딥 러닝으로 음성 인식을 수행하는 방법을 배우십시오!​​머신 러닝이 항상 블랙 박스는 아닙니다신경 기계 번역 이 어떻게 작동하는지 알고 있다면 , 단순히 음성 녹음을 신경망에 공급하고 텍스트를 생성하도록 훈련시킬 수 있다고 생각할 것입니다.   그것은 딥 러닝을 통한 음성 인식의 성배이지만, 우리는 아직 거기에 없습니다 (적어도 이것을 쓴 시점에 – 우리는 몇 년 안에있을 것이라고 확신합니다).가장 큰 문제는 언어의 속도가 다르다는 것입니다. 한 사람은 ""hello!""라고 매우 빨리 말할 수 있고 다른 사람은 ""heeeelllllllllllllooooo!""라고 매우 느리게 말해 훨씬 더 많은 데이터가 포함 된 훨씬 긴 사운드 파일을 생성 할 수 있습니다. 두 사운드 파일 모두 정확히 같은 텍스트로 인식되어야합니다.“hello!”다양한 길이의 오디오 파일을 고정 길이의 텍스트에 자동으로 정렬하는 것은 매우 어려운 것으로 나타났습니다.이 문제를 해결하려면 심층 신경망 외에도 특별한 트릭과 여분의 세차를 사용해야합니다. 그것이 어떻게 작동하는지 봅시다!​​소리를 비트로 바꾸기음성 인식의 첫 단계는 분명합니다. 음파를 컴퓨터에 공급해야합니다.Part 3 에서는 이미지를 인식하고 신경망에 직접 공급하여 이미지를 인식하는 방법을 배웠다.   이미지는 각 픽셀의 강도를 인코딩하는 숫자 배열입니다 그러나 소리는 파도 로 전달됩니다 . 음파를 어떻게 숫자로 바꾸는가? ""Hello""라고 말하는이 사운드 클립을 사용하겠습니다.   ""Hello""라고 말하는 파형 음파는 1 차원입니다. 매 순간마다 파도의 높이를 기준으로 단일 값을 갖습니다. 음파의 작은 부분을 확대하여 살펴 보겠습니다.   이 음파를 숫자로 바꾸려면 동일한 간격으로 파의 높이를 기록하면됩니다.   음파 샘플링 이것을 샘플링 이라고 합니다. 우리는 1 초에 수천 번 판독하고 그 시점에서 음파의 높이를 나타내는 숫자를 기록하고 있습니다. 기본적으로 모든 압축되지 않은 .wav 오디오 파일입니다.""CD 품질""오디오는 44.1khz (초당 44,100 개의 판독 값)로 샘플링됩니다. 그러나 음성 인식의 경우 16khz (초당 16,000 개의 샘플)의 샘플링 속도만으로도 사람의 음성 주파수 범위를 포괄 할 수 있습니다.""Hello""음파를 초당 16,000 회 샘플링 할 수 있습니다. 처음 100 개의 샘플은 다음과 같습니다.   각 숫자는 1/16000 초 간격의 음파 진폭을 나타냅니다. 디지털 샘플링의 빠른 사이드 바샘플링은 가끔씩 만 판독하기 때문에 원래 음파의 대략적인 근사값 만 생성한다고 생각할 수 있습니다. 판독 값 사이에 차이가 있으므로 데이터를 잃어 버려야합니다.   디지털 샘플이 원래의 아날로그 음파를 완벽하게 재현 할 수 있습니까? 그 차이는 어떻습니까? 그러나 나이키 스트 정리 (Nyquist theorem) 덕분에 , 우리는 기록하고자하는 최고 주파수보다 적어도 두 배 빠른 샘플링을하는 한, 수학을 사용하여 이격 된 샘플로부터 원래 음파를 완벽하게 재구성 할 수 있다는 것을 알고 있습니다.나는 거의 모든 사람들이 이것을 잘못 알고 더 높은 샘플링 속도를 사용하면 항상 더 나은 오디오 품질로 이어진다 고 가정 하기 때문에 이것을 언급 합니다. 그렇지 않습니다.​​​샘플링 된 사운드 데이터 전처리이제 1 / 16,000 초 간격으로 음파의 진폭을 나타내는 숫자가 배열되어 있습니다.우리 는 이 숫자를 신경망에 바로 공급할 수 있습니다. 그러나 이러한 샘플을 직접 처리하여 음성 패턴을 인식하는 것은 어렵습니다. 대신, 오디오 데이터에서 일부 전처리를 수행하여 문제를 더 쉽게 만들 수 있습니다.샘플링 된 오디오를 20 밀리 초 길이의 청크로 그룹화하여 시작하겠습니다. 처음 20 밀리 초의 오디오 (즉, 첫 320 개 샘플)는 다음과 같습니다.   이 숫자를 간단한 선 그래프로 플로팅하면 20 밀리 초 동안의 원래 음파를 대략적으로 알 수 있습니다.   이 기록이 아니라 제 1 / 50의 길이 . 그러나이 짧은 녹음조차도 서로 다른 사운드 주파수의 복잡한 혼란입니다. 저음, 중음, 고음이 뿌려집니다. 그러나이 두 가지 주파수를 함께 사용하면 복잡한 인간의 소리를 구성 할 수 있습니다.신경망에서이 데이터를보다 쉽게 ​​처리 할 수 ​​있도록이 복잡한 음파를 구성 요소로 분리 할 것입니다. 우리는 낮은 피치 부분, 다음으로 가장 낮은 피치 부분 등을 알아낼 것입니다. 그런 다음 각 주파수 대역 (낮은 것에서 높은 것까지)의 에너지 양을 합산 하여이 오디오 스 니펫에 대한 지문 을 만듭니다 .피아노에서 C 메이저 코드를 연주하는 사람이 녹음되었다고 상상해보십시오. 그 소리는 세 개의 음표 (C, E 및 G)가 결합되어 하나의 복잡한 소리로 혼합됩니다. 우리는 그 복잡한 소리를 개별 음표로 분리하여 그것이 C, E 및 G임을 발견하고 싶습니다. 이것은 똑같은 생각입니다.우리는 푸리에 변환 이라는 수학 연산을 사용하여이 작업을 수행합니다 . 복잡한 음파를 간단한 음파로 분리합니다. 개별 음파를 확보하면 각 음파에 포함 된 에너지의 양을 더합니다.최종 결과는 저음 (즉, 저음)에서 고음까지 각 주파수 범위가 얼마나 중요한지에 대한 점수입니다. 아래의 각 숫자는 20 밀리 초 오디오 클립의 각 50hz 대역에 얼마나 많은 에너지가 있었는지 나타냅니다.   목록의 각 숫자는 해당 50hz 주파수 대역의 에너지 양을 나타냅니다 그러나 이것을 차트로 그릴 때 훨씬 쉽게 볼 수 있습니다.   20 밀리 초 사운드 스 니펫에는 고주파수에서 저주파 에너지가 많고 에너지가 많지 않음을 알 수 있습니다. 그것은 전형적인 ""남성""목소리입니다. 매 20 밀리 초 청크마다이 프로세스를 반복하면 스펙트로 그램으로 끝납니다 (왼쪽에서 오른쪽으로 각 열은 20ms 청크입니다).   “hello”사운드 클립의 전체 스펙트로 그램 오디오 데이터에서 음표 및 기타 피치 패턴을 실제로 볼 수 있기 때문에 스펙트로 그램이 좋습니다 . 신경망은 원시 음파보다 이러한 종류의 데이터에서 패턴을 더 쉽게 찾을 수 있습니다. 이것이 우리가 실제로 신경망에 제공 할 데이터 표현입니다.​​​짧은 소리에서 문자 인식처리하기 쉬운 형식으로 오디오를 만들었으므로 심층 신경망에 공급할 것입니다. 신경망에 대한 입력은 20 밀리 초 오디오 청크가됩니다. 각 작은 오디오 슬라이스에 대해 현재 말하고있는 사운드에 해당하는 글자 를 알아 내려고 시도합니다 .   ​​우리는 반복적 인 신경망 , 즉 미래 예측에 영향을주는 메모리를 가진 신경망을 사용할 것입니다. 예상되는 각 문자는 다음 문자의 가능성에도 영향을 미치기 때문입니다. 예를 들어 지금까지 ""HEL""이라고 말한 경우 ""Hello""라는 단어를 완성하기 위해 ""LO""라고 말할 가능성이 높습니다. “XYZ”와 같은 다음에는 말할 수없는 것을 말할 가능성이 훨씬 낮습니다. 따라서 이전 예측을 기억하면 신경망이 앞으로 더 정확한 예측을 할 수 있습니다.전체 오디오 클립을 신경망 (한 번에 한 청크)을 통해 실행 한 후에는 각 오디오 청크를 해당 청크 중에 가장 많이 쓰이는 문자에 매핑합니다. 다음은 ""Hello""라고 말하는 매핑입니다.   ​​우리의 신경망은 내가 말한 것 중 하나가“HHHEE_LL_LLLOOO”라고 예측하고 있습니다. 그러나 또한“HHHUU_LL_LLLOOO”또는“AAAUU_LL_LLLOOO”라고 말했을 가능성이 있다고 생각합니다.이 출력을 정리하기 위해 따라야 할 단계가 있습니다. 먼저 반복되는 문자를 단일 문자로 바꿉니다.HHHEE_LL_LLLOOO가 HE_L_LO가 됨HHHUU_LL_LLLOOO가 HU_L_LO가됩니다.AAAUU_LL_LLLOOO가 AU_L_LO가됩니다.그런 다음 공백을 제거합니다.HE_L_LO가 HELLO가 됨HU_L_LO가 HULLO가됩니다AU_L_LO가 AULLO가됩니다그러면“Hello”,“Hullo”및“Aullo”의 세 가지 가능한 전사가 생깁니다. 큰 소리로 말하면이 모든 소리는 ""Hello""와 비슷합니다. 한 번에 하나의 문자를 예측하기 때문에 신경망은 매우 소리 나는 전사를 제시합니다. 예를 들어,“그는 가지 않을 것이다”라고 말하면“그는 그물을 ud 다”와 같은 가능한 전사를 제공 할 수 있습니다.비결은 이러한 발음 기반 예측을 서면 텍스트 (도서, 뉴스 기사 등)의 대규모 데이터베이스를 기반으로하는 가능성 점수와 결합하는 것입니다. 가장 현실적이지 않은 전사를 버리고 가장 사실적인 전사를 유지합니다.가능한 전사 ""Hello"", ""Hullo""및 ""Aullo""중에서 분명히 ""Hello""는 텍스트 데이터베이스 (원래 오디오 기반 교육 데이터에는 언급되지 않음)에 더 자주 표시되므로 정확할 것입니다. 그래서 우리는 다른 사람 대신에 ""Hello""를 마지막 전사로 선택할 것입니다. 끝난!​​잠깐만!당신은 생각 될 수있다 ""그러나 누군가가 말하는 경우 ' 안부를 '? 유효한 단어입니다. 어쩌면 'Hello'가 잘못된 전사 일 것입니다!”물론 누군가“Hello”대신“Hullo”라고 말했을 수도 있습니다. 그러나 이와 같은 음성 인식 시스템 (미국 영어 교육)은 기본적으로 ""훌로""를 전사로 생성하지 않습니다. 사용자가 ""Hello""와 비교할 때 말하는 것은 매우 드물기 때문에 'U'사운드를 아무리 강조해도 항상 ""Hello""라고 말하는 것입니다.사용해보십시오! 휴대 전화가 미국 영어로 설정되어 있으면 휴대 전화의 디지털 어시스턴트가 세계“훌 로어”를 인식하도록하십시오. 거절한다! 항상 ""Hello""로 이해합니다.""Hullo""를 인식하지 못하는 것은 합당한 행동이지만 때로는 휴대 전화에서 말하는 것을 이해하지 못하는 성가신 경우가 있습니다. 그렇기 때문에 이러한 음성 인식 모델은 항상 이러한 최첨단 사례를 해결하기 위해 더 많은 데이터로 재교육됩니다.​​내 음성 인식 시스템을 구축 할 수 있습니까?머신 러닝의 가장 멋진 점 중 하나는 때로는 간단 해 보이는 것입니다. 당신은 ..., 데이터의 무리를 얻을 기계 학습 알고리즘으로 공급하고 마술 당신은 당신의 게임 노트북의 비디오 카드에서 실행되는 세계 최고 수준의 AI 시스템을 마우스 오른쪽 ?어떤 경우에는 그런 말이 있지만 말로는 그렇지 않습니다. 음성 인식은 어려운 문제입니다. 나쁜 품질의 마이크, 배경 소음, 리버브 및 에코, 악센트 변형 및 온 / 오프 등 거의 무한한 과제를 극복해야합니다. 신경 네트워크가 문제를 처리 할 수 ​​있도록 이러한 모든 문제가 훈련 데이터에 있어야합니다.또 다른 예가 있습니다. 큰 방에서 말할 때 무의식적으로 소리를 들려서 소음을 말할 수 있다는 것을 알고 있습니까? 인간은 어느 쪽이든 당신을 이해하는데 아무런 문제가 없지만,이 특별한 경우를 다루기 위해 신경망을 훈련시켜야합니다. 따라서 소음으로 고함을 지르는 사람들과 함께 훈련 데이터가 필요합니다!음성 인식 이제 시리, 구글의 수준에 수행하는 시스템!, 또는 알렉사를 구축하기 위해, 당신은해야합니다 많은 훈련 데이터를 - 훨씬 더 많은 데이터를 당신이 가능성이 당신을 위해 그것을 기록하는 수백 명의 사람들을 고용하지 않고 얻을 수있는 것보다. 또한 사용자는 품질이 좋지 않은 음성 인식 시스템에 대한 내성이 낮기 때문에이를 극복 할 수 없습니다. 80 %의 시간 동안 작동하는 음성 인식 시스템을 원하는 사람은 없습니다.Google 또는 Amazon과 같은 회사의 경우 실제 상황에서 녹음 된 수십만 시간의 음성 오디오는 금 입니다. 이것이 세계적인 음성 인식 시스템과 취미 시스템을 분리하는 가장 큰 것입니다. Google Now를 사용하는 요점 ! 그리고 Siri 는 모든 휴대 전화에서 무료 또는 구독료가없는 $ 50 Alexa를 판매 하여 가능한 많이 사용 하도록 하는 것 입니다. 이러한 시스템 중 하나에 대해 말한 모든 내용은 영원히 기록 되며 향후 버전의 음성 ​​인식 알고리즘에 대한 교육 데이터로 사용됩니다. 그게 다 게임이야!나를 믿지 않습니까? Google Now 가있는 Android 휴대 전화가 있다면 ! , 여기에 말한 모든 멍청한 말을 실제로 녹음 한 내용을 들으려면 여기를 클릭하십시오 .​   Alexa 앱을 통해 Amazon에 대해 동일한 내용에 액세스 할 수 있습니다 . 불행히도 Apple은 Siri 음성 데이터에 액세스 할 수 없습니다. 따라서 시작 아이디어를 찾고 있다면 Google과 경쟁하기 위해 자체 음성 인식 시스템을 구축하지 않는 것이 좋습니다. 대신 사람들이 몇 시간 동안 대화 한 내용을 녹음 할 수있는 방법을 찾으십시오. 데이터는 대신 제품이 될 수 있습니다.​​자세한 정보가변 길이 오디오를 처리하기 위해 여기에 설명 된 알고리즘 (대략)을 Connectionist Temporal Classification 또는 CTC라고합니다. 2006 년부터 원본을 읽을 수 있습니다 .Baidu의 Adam Coates 는 Bay Area Deep Learning School에서 음성 인식을위한 딥 러닝에 대한 훌륭한 프레젠테이션을 발표했습니다. YouTube에서 비디오를 볼 수 있습니다 (토론은 3:51:00에 시작됩니다). 추천.이 기사가 마음에 드시면 Machine Learning is Fun에 가입하십시오! 이메일 목록 . 새롭고 멋진 소식이있을 때만 이메일을 보내 드리겠습니다. 이런 기사를 더 많이 쓸 때 알아내는 가장 좋은 방법입니다.트위터 @ageitgey 에서 나를 팔로우 하거나 , 직접 이메일을 보내 거나 linkedin에서 나를 찾을 수도 있습니다 . 기계 학습을 통해 귀하 또는 귀하의 팀을 도울 수 있다면 귀하의 의견을 듣고 싶습니다.​source: Adam Geitgey in Medium "
A spelling correction model for end-to-end speech recognition ,https://blog.naver.com/woal1975/221694390415,20191031,"#ASR, #e2e, #DeepLearning, #spelling,#correction​Google 2019​Abstract E2E 방식의 음성인식 시스템의 훈련은 audio-text pair DB로만으로 훈련을 하기 때문에 저빈도 단어에 대한 인식률이 떨어진다. 외부 언어모델 훈련이외에 모델이 가지고 있는 에러분포를 활용하는 것이 중요한다. WERR 18.6% @Libri ​기본 컨셉  □문제: E2E훈련 방식 자체의 문제로써, 저빈도 단어에 대한 인식률이 좋지 않다.   ◇기존 해결방안: RNN-LM등의 외부  LM 의 사용, First-pass beam search와 shallow, cold and deep 방식의 fusion방식 활용  ■제안 방식: correcting error의 objective를 이용하여 텍스만으로의 훈련 모듈을 E2E 프레임워크에 결합하였다. TTS 활용 TTS입력 Text와 합성음의 인식결과 Text간의 pair를 통해  spelling correct model을 만들었다. --> 이를 통해 LAS모델이 자체적으로 가지고 있는 에러 패턴을 산출할 수 있었다.  "
"SER classifier, Speech Emotion Recognition,음성감정인식 분류기 종류 ",https://blog.naver.com/ssj860520/222841745437,20220807,"음성감정인식은 무엇일까요?음성감정인식은 음성에서부터 감정을 인식하는 것입니다.음성감정인식을 하는 방법에는 여러가지입니다.여러가지가 있지만 분류기가 있습니다.감정을 화가났는지 슬픈지 기쁜지~ 등등으로 분류하는 것이 분류기입니다.그러면 분류기의 종류에는 무엇이 있을까요?찾아보니 여러종류가 있는데요.​ 음성감정인식 분류기의 종류(SER, Speech Emotional Recognition classifiers)음성감정인식을 통해 음성으로부터 화자의 감정을 추론한다. 추론결과를 통해 감정을 분류하는데 분류를 하는 친구를 분류기(Classifier)라고 한다. 이 분류기의 종류에는 무엇이 있을까 SER classifier 종류 SER..qwertyuioop.tistory.com 아래의 글이 정리 잘 해놨두라고요.여러분들위의 글을 보도록 하세요  "
[바람돌이/딥러닝] Speech - 음성 데이터 이론 및 이해 ,https://blog.naver.com/winddori2002/222872853715,20220912," 안녕하세요. ​오늘은 음성 데이터를 공부하면서 정리하고 세미나에 공유한 내용을 공유하고자 합니다.​기존에 다른 블로그에서 정리했던 내용들과 논문을 참고하여 정리하였습니다.​Introduction to Sound data.pdf 파일로 정리 내용 공유합니다. ​수정이 필요하거나 질문 있으시면 댓글 부탁드립니다. 첨부파일Introduction_to_Sound_data.pdf파일 다운로드 PDF Contents​ ​​References[1] https://pyy0715.github.io/Audio/ [2] https://ratsgo.github.io/speechbook/docs/phonetics/acoustic [3] https://www.scienceall.com/%EC%86%8C%EB%A6%AC%EC%9D%98-3%EC%9A%94%EC%86%8Cthree-elements-of-sound/ [4] https://tech.kakaoenterprise.com/66 [5] https://darkpgmr.tistory.com/171 [6] https://hyongdoc.tistory.com/401 [7] https://sanghyu.tistory.com/38?category=1120070 [8] https://www.tek.com/en/blog/window-functions-spectrum-analyzers [9] https://ghebook.blogspot.com/2020/09/dft-discrete-fourier-transform.html [10] https://hyunlee103.tistory.com/46 [11] https://brightwon.tistory.com/11 [12] https://blog.naver.com/PostView.nhn?isHttpsRedirect=true&blogId=sooftware&logNo=221661644808 [13] https://sanghyu.tistory.com/49 [14] Gerhard, David. Pitch extraction and fundamental frequency: History and current techniques. Regina, SK, Canada: Department of Computer Science, University of Regina, 2003. [15] Kumatani, Kenichi, et al. ""Microphone array processing for distant speech recognition: Towards real-world deployment."" Proceedings of The 2012 Asia Pacific Signal and Information Processing Association Annual Summit and Conference. IEEE, 2012. [16] Adel, Hidri, et al. ""Beamforming techniques for multichannel audio signal separation."" arXiv preprint arXiv:1212.6080 (2012). [17] Pandey, Ashutosh, et al. ""Multichannel Speech Enhancement Without Beamforming."" ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2022. [18] Park, Daniel S., et al. ""Specaugment: A simple data augmentation method for automatic speech recognition."" arXiv preprint arXiv:1904.08779 (2019). [19] Colbert, Debborah. Manatee sound localization: Performance abilities, interaural level cues, and usage of auditory evoked potential techniques to determine sound conduction pathways. University of South Florida, 2008. [20] F. Bahmaninezhad, J. Wu, R. Gu, S. Zhang, Y. Xu, M. Yu, and D. Yu, “A Comprehensive Study of Speech Separation: Spectrogram vs Waveform Separation,” in INTERSPEECH, 2019. [21] R. Gu et al., “Enhancing End-to-End Multi-Channel Speech Separation Via Spatial Feature Learning,“ in ICASSP, 2020. [22] Sudo, Yui, et al. ""Multi-channel Environmental sound segmentation."" 2020 IEEE/SICE International Symposium on System Integration (SII). IEEE, 2020. [23] Yasuda, Masahiro, et al. ""Sound event localization based on sound intensity vector refined by DNN-based denoising and source separation."" ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2020. [24] Hwang, Yeongtae, et al. ""Mel-spectrogram augmentation for sequence to sequence voice conversion."" arXiv preprint arXiv:2001.01401 (2020). ​​#Deep #Learning #음성 #Signal #Processing #Sound #Speech #data #이론 #이해 #설명 "
How to Add Word Classes to the Kaldi Speech Recognition Toolkit ,https://blog.naver.com/lswbhm88/222050212205,20200803,https://www5.informatik.uni-erlangen.de/Forschung/Publikationen/2016/Horndasch16-HTA.pdf​ 
"[리뷰논문 리뷰]A systematic literature review of speech emotion recognition approaches, 감정인식 ",https://blog.naver.com/ssj860520/222867233069,20220905,"예전부터 연구되어왔다.연구되어있지만 상용화 되어있지 않았다.왜냐하면 성능이 좋지 않기 때문이다.이런 상황을 개선하려면 어떻게 해야할까?기존의 감정인식 기술들을 알아보고 개선을 해야 한다.아래의 글에서 감정인식 논문 리뷰를 볼 수 있다.감사합니다.​ [리뷰논문에서 발견한것]A systematic literature review of speech emotion recognition approaches, 감정인식리뷰논문을 계속읽어나가고 있다. 그리고 계속 정리하고 있다. 저자는 리뷰논문을 쓰면서 몇가지 발견한 사실을 정리해서 한 섹션에 담았다. 저자가 정리한 내용을 내가 기억하기 위해 남기려고 한다. 리뷰를 통해..qwertyuioop.tistory.com ​ "
Google Research on End-to-End Models for Speech Recognition -English version-  ,https://blog.naver.com/imhotep/222719365153,20220503,​ 
Google Research on End-to-End Models for Speech Recognition -English version-  ,https://blog.naver.com/imhotep/222714732979,20220428,​ 
Bytes are All You Need: End-to-End Multilingual Speech Recognition and Synthesis with Bytes ,https://blog.naver.com/woal1975/221694401569,20191031,"#ASR, #E2E, #DeepLearning, #AudioToBytes, #BytesToAudio, #Synthesis​Google 2019​Abstract E2E모델링의 기본단위를 Unicode 단위로 만듦으로써 다국어 개발에서의 활용도를 높인다. 유니코드를 활용함으로써 softmax layer의 크기를 줄일 수 있으며, 동시에 이렇게 만들어진 모델을 다른 다국어 개발에도 쉽게 활용할 수 있다. 이렇게 했을때 음성인식의 성능을 4~5%정도 (상대적 수치) 올릴 수 있었고, 음성합성쪽에 활용하여 SOTA를 얻었다.  "
Self-supervised learning of audio and speech representation [2/4] ( KAIST 정준선 교수님 ) / 음성 인식 모델 ,https://blog.naver.com/gypsi12/223005193223,20230204,"목차​- 음성 인식 모델 구성- Deep Speach 모델- CTC Loss​- 학습 데이터 수집의 어려움​- Speaker Recognition ( Speaker Identification, Speaker Verification ) 음성 인식 모델 구성이번에는 언어모델을 어떻게 구성하는가에 대해 알아볼 것이다.​ sequence modeling: input sequence를 다른 domain의 output sequence로 바꾸는 것​input sequence는 Sequence of sound pressure 즉, 소리이고output sequence는 단어이다.​​sequence 정보를 다룰 때에는 보통 RNN을 많이 쓴다.이전 Time step의 정보가 다음 Time step에 반영되기 때문이다.​| RNN의 장점다양한 길이의 input을 처리할 수 있기에 연속적인 sequence를 예측하는 모델에서 효과적이다.​| RNN의 단점첫번째,현재 Time step을 계산해야 그 다음 Time step을 계산할 수가 있어 학습 속도가 좀 느리고​두번째,Sequence가 길어지게 되면 역전파 학습시 현재 Time step과 거리가 먼 step의 가중치가 제대로 학습되지 않는Grandient Vanishing 문제(back propagation을 통한 1가중치를 업데이트 할 때에 0과 1사이의 값을 가지는 Gradient 값이 계속해서 곱해져서 결국 업데이트에 사용하려는 Gradient값이 0에 가까워져 업데이트가 제대로 되지 않는 현상)가 발생한다.​​​> RNN 기반 음성 인식 모델 구성RNN도 구조를 task에 알맞게 다양하게 사용할 수 있다.여러개를 입력받아 한가지를 반환하는 many to one 구조라던지여러개를 입력받아 여러개를 반환하는 many to many 구조도 가능하다.​음성인식에서는 many to many 구조를 사용해야 할 것이다.​하지만 문제점이 있다.input 길이가 output 길이보다 훨씬 길 수 밖에 없기 때문이다.​input이 만약 10초짜리 음성이라 한다 할 지라도 10만개가 넘는 개수의 input을 구성해야 하지만,그에 따른 output 단어는 10개밖에 되지 않을 수 있다.​딥러닝 모델은 학습을 통해 입력받은 데이터들의 패턴정보를 모델에 녹아낸다.이 녹아져 있는 정보들이 온전히 다 출력을 통해 표현되기 위해서는 입력과 비슷한 길이의 출력을 반환해야 한다.입력에 비해 출력이 너무 작게 된다면 그 출력은 국지적인 정보만을 담을 가능성이 높다.​이런 번거로움에서 벗어날 수 있는 형태인모든 input을 하나의 벡터에 녹아낸 다음에 그 벡터를 통해 output을 예측하는 Encoder-Deocder 구조도 사용되기는 하는데 한정된 길이의 벡터에 엄청난 길이의 input을 꾸겨 넣다 보니 정보 손실이 발생할 수 밖에 없다.​  Deep Speach 모델이제 딥러닝 음성인식 모델을 소개할 것이다.Deep Speach라는 non auto-regressive 모델이다.​특징이라면, 마지막에 CTC loss function을 사용한다는 것이다.하는 작업은 소리를 텍스트로 바꾸는 일로 동일하다.​이 모델은 많은 데이터를 사용하면 잘 된다는 단순한 생각으로 접근한다.​| 모델 구성 feature processing하기 위해 CNN Layer들이 있고그 뒤에 RNN Layer들이 있다.​2D CNN으로 feature를 만들고그 위에 RNN을 쌓아서 각각의 라인의 Token을 예측하는 방식이다.​물론, CNN과 RNN을 여러층 쌓는다.이 구조를 보니 앞서 StackGAN에서 소개한 적이 있는 텍스트 인코딩 기법인 Word CNN-RNN의 구조와 동일하다.​RNN의 변형으로써 GRU나 LSTM을 사용할 수 있을텐데실제로는 GRU를 7층 정도 쌓고 그 마지막에 CTC Loss를 적용한다.​ CTC(Connectionist Temporal Classification) Loss앞서 non auto regressive model을 사용할 때에는input하고 output하고 개수가 맞지 않는다고 했다.​여기서 더 나아가서 output이 너무 길게 되면 우리가 출력되기 원하는 target출력을 출력하고도 쓸데없이 넘치는 출력들이 남을 수 있다.​ 이럴 경우 어떻게 하면 좋을까?""cat""이라는 음성을 입력받았으면 c,a,t를 반환할 수 있는 RNN을 생각해보자.출력의 개수를 4개라고 정했을 때,'c','a','t'를 출력하고 남은 토큰은 무엇을 예측해야 할까?​그냥 padding(의미없는 토큰)을 예측하면 어떨까?출력의 개수가 작아서 padding 개수가 몇개 필요하지 않는다면 괜찮을 수도 있다.하지만, 음성을 출력한다는 것은 한 단어를 출력하는 것이 아닌 경우가 많다.우리는 문장으로 말을 하기 때문이다.​실제로 예측하는 토큰은 몇 개 안되는데 남는 토큰이 몇십개라면 그 모델은 제대로된 학습이 되지 않는다.앞선 문제처럼 "" Target(예측하려는 실제 값)보다 모델 output의 길이가 길 경우"" alignment가 맞지 않는다고 한다.  무언가 기발한 방법이 필요하다.그럼 아래와 같이 하면 어떨까? ​일단, 최대한 input 개수만큼 output을 계속 예측하도록 하여 RNN을 온전히 써먹는 구조로 놓는다.​이때 생각해볼 점이 음성에는 특정 알파벳을 발음하는 구간이 존재한다.그 구간에 해당하는 모델 output이 그 구간에 해당하는 알파벳을 예측하도록 놓는 것이다.이후 중복되는 것들 중에 하나만 남기는 후처리를 해주면 된다.​""cccaaaaatt"" => 후처리 => ""cat""​좋은 방법이지만 문제점이 있다.같은 알파벳이 두 번 연속으로 들어가는 단어들을 처리하지 못한다.  hello라는 음성이 들어오게 되었을 때 만약 알파벳(자연어처리[nlp]에서는 character라고 부른다) 예측을 위와 같이 한다고 하면""hhellllooo"" 가 후처리를 거치고 나면 ""helo""가 되어 버린다.  이 문제를 해결하기 위해 새로운 blank token인 ""-""를 도입할 수 있다.만약 character 사이에 ""-""가 있다면 그 character는 연속으로 들어가는 character로 판단하여 중복 제거를 하지 않고 남겨놓는다. 위와 같이 예측을 하게 되면""hhell-loo""를 예측하게 되고​'-' 앞에 중복되는 h와 l은 하나만 남고 날라가게 되고'l-l'에서 '-'만 제거됨으로써'hello'로 처리가 완료된다.​물론, '-'가 여러개 연속으로 나타날 경우에는 '-'를 중복처리하여 한개만 남긴다.​'-'가 포함됨으로 인해 하나의 단어에 대해서도 여러가지 경우의 수가 존재하게 된다.​각 step마다 모델은 예측 character를 반환하게 될 터인데각 경우의 수에 대해, 그 순서에 맞는 character 조합을 내뱉도록 하는 모델을 만드는 것이 목표이다.  이 모델을 학습시키기 위해경우의 수들 중에 하나를 선택하고 그 sequence를 반환하기 위해 각 step마다 실제로 내뱉어야하는 character가 있을텐데해당 step에서 신경망이 그 character를 반환할 확률을 계산하여 모든 step에 대해 확률값을 곱해나가전체 sequence를 반환할 확률을 최종적으로 구한다.​최종적으로는 모든 경우의 수에 대한 확률을 합하여 이 확률합을 높이는(Maximum Liklihood) 방향으로 학습이 진행된다.​예를들어""hello""를 반환할 수 있는 모든 경우의 수들 중에""hhell-loo""를 선택한다면이 sequence를 모델이 반환할 확률이라 함은 다음과 같이 계산된다.​p(hhell-loo 를 반환할 확률) =p(1번째 step에 h를 반환할 확률) x p(2번째 step에 h를 반환할 확률) x p(3번째 step에 e를 반환할 확률) xp(4번째 step에 l를 반환할 확률) x p(5번째 step에 l를 반환할 확률) x p(6번째 step에 -를 반환할 확률) xp(7번째 step에 l를 반환할 확률) x p(8번째 step에 o를 반환할 확률) x p(9번째 step에 o를 반환할 확률)​이제 모든 경우의 수에 대해 모델이 해당 출력 sequence를 내뱉을 확률을 구하고이 모든 경우의 수의 확률의 합을 높이는 방향으로 학습이 진행된다.이 확률합을 CTC Loss라고 한다.​​ https://github.com/hccho2/CTC-Loss/blob/master/CTCLoss.pdfcharacter 조합을 도식화하면 위와 같은 행렬을 하나 만들 수 있다. ( 위 행렬은 8step으로 apple을 나타내기 위한 행렬 )​이 행렬은 아래와 같은 형태를 띄게 되며( target의 character에 '-'가 사이사이 끼어 있는 전체 character의 개수 x step 개수(= 모델 output 개수) )​​target을 뱉어낼 수 있는 경우의 수들의 조합을 위와 같이 화살표를 이어서 나타낼 수 있다.이때 각 공간에는 확률값들이 들어가게 되고 해당 확률값들은 신경망 학습을 통해 우리가 원하는 target의 출력을 내뱉도록 하는 확률값이 각 자리에 알맞게 위치하도록 재조정 된다.​이때, Dynamic programing이라는 방식을 사용하는데이 방식은 간단히 말하면길을 하나 만들어놓고여러개의 샘플을 계속 통과시켜나가면서그 길을 조금씩 수정시켜나감으로 인해나중에는 샘플과 유사한 값을 반환할 수 있는 길이 되는 학습 방식이다.​​위와 같은 방식으로CTC Loss는 학습이 된다.   학습 데이터 수집의 어려움음성 인식은 데이터 만드는 것이 힘들다​음성 인식 데이터를 만드려면 누군가는 말을하고 그 말을 받아적어야 할 것이다.그런데 이 받아 적는 방식이 사람마다 다를 수 있다.​만약 ""오늘 5시에 봐"" 라는 음성을 말한다면이에 대한 text로 ""오늘 다섯시에 봐""가 될 수 도 있고, ""오늘 5시에 봐"" 가 될 수 도 있다.​만약 콩글리시 발음으로 ""컴퓨타를 고쳐야겠네""라고 말하면이에 대한 text로 ""컴퓨터""로 적을지 ""컴퓨타""로 적을지도 고민해볼 만한 문제이다.​정말 데이터를 만들기 어려운 영역이다.​  Speaker Recognition ( Speaker Identification, Speaker Verification )이번 Session의 주된 주제인 Self-supervised learning을 들어가기에 앞서우리는 지금까지 ASR(automatic speech recognition)만 설명해왔다.​이번에는 화자인식(Speaker Recognition)에 대해 말해보려 한다.​이 분야는Speaker Identification과 Speaker Verification으로 나뉜다.​Speaker Identification : 음성이 들렸을 때, 해당 음성을 말하고 있는 사람을 구분하는 것Speaker Verification : 전에 녹음해 놓은 목소리가 있을 경우에, 현재 음성이 전에 녹음한 목소리와 동일 인물인지 아닌지 구분하는 것​| Speaker IdentificationAI 스피커를 가족들이 같이 쓴다고 생각해 보자.AI 스피커에 ""내가 좋아하는 음악 틀어줘""라고 말한다면,이 스피커는 가족 중에서 내가 누구인지를 먼저 알아맞추는 Speaker Identificaiton을 진행해야 한다.​| Speaker Verificationtelephone banking에서 많이 사용되는데은행에 전화를 하면, 저번에 내가 전화했을 때의 음성을 녹취를 해놓은 다음에 그 다음에 해당 전화로 전화가 왔을 때 현재 전화하고 있는 사람이 저번에 전화한 사람과 동일한지를 음성을 통해 확인을 하고 그 정보를 상담원에게 넘겨주는 기술이 사용된다. 이와 같이 보안이 필요한 경우에 많이 사용된다.​Speaker Verification(화자인식)같은 경우대부분 Metric Learning : 음성을 벡터공간상에 비슷하면 가깝고, 다르면 멀게 위치하여 학습하는 방식 을 통해 학습된다.​​그런데 재미있는 점이 있다.이 화자인식문제의 경우 사람도 잘하지 못한다.한 사람이 조용한 곳에서 녹음한 음성과 시끄러운 야외에서 녹음한 음성을 구분하는 것이 매우 어렵다는 말이다.​이 화자인식의 성능은Equal error rate라는 점수로 판단하는데, 사람의 경우 15%정도가 나온다고 한다. (10번 중에 1.5번은 틀린다)하지만, 인공지능 모델의 경우 이를 1% 안쪽까지 낮춘다.  이제 Self-supervision이라는 방법으로 위 문제를 푸는 방법을 다음 포스트를 통해 소개해보겠다. "
speech recognition system ,https://blog.naver.com/lincolnmoon/222568183539,20211115,m 
An Investigation Into On-device Personalization of End-to-End Automatic Speech Recognition Models  ,https://blog.naver.com/woal1975/221694372683,20191031,"#ASR, #End2End, #DeepLearning, #personalize​Google 2019Abastract 디바이스위에서 개인음성을 이용한 개인화 음성인식 모델의 훈련 End-to-End 방식의 모델링 WERR 63.7% on Server train WERR 58.1% on Device train gradient 계산과정을 두단계로 나눔으로써 45% 메모리 절약, 42%의 훈련시간 느려짐.  "
[논문] Multi-Path and Group-Loss-Based Network for Speech Emotion Recognition in Multi-Domain Datasets ,https://blog.naver.com/hannaurora/222420778037,20210705,"https://www.mdpi.com/1424-8220/21/5/1579 Multi-Path and Group-Loss-Based Network for Speech Emotion Recognition in Multi-Domain DatasetsSpeech emotion recognition (SER) is a natural method of recognizing individual emotions in everyday life. To distribute SER models to real-world applications, some key challenges must be overcome, such as the lack of datasets tagged with emotion labels and the weak generalization of the SER model fo...www.mdpi.com 한국어 음성 감정 데이터인 KESDy18과 음성 감정 인식에 대한 모델과 성능을 설명하는 논문한국인인데~~ 영어로 논문을 쓰다니~ (광광)​Section 1 : 소개Section 2 : 관련된 SER과 domain adaptation 연구에 대한 간략한 개요Section 3 : multi-domain dataset에서 SER의 multi-domain adaptation을 지원하기 위해 제안된 MPGLN에 대한 설명까지 섹션 1~3은 민공지능에서 정리한다!https://sswwd.tistory.com/54 Multi-Path and Group-Loss-Based Network for Speech Emotion Recognition in Multi-Domain DatasetsMulti-Path and Group-Loss-Based Network for Speech Emotion Recognition in Multi-Domain Datasets(2021), Kyoung Ju Noh *, Chi Yoon Jeong , Jiyoun Lim, Seungeun Chung, Gague Kim, Jeong Mook Lim and Hyu..sswwd.tistory.com 다음은 내가!Section 4 : MPGLN SER 평가 결과Section 5 : 결론  Section 4 : Evaluation4.1 Datasets우리는 위에서 제안한 모델을 평가하기 위해 5개의 multi-domain dataset 이용했다. 그중 3가지는 진짜 SER dataset이다. multi-cultural datasets에 기반한 MPGLN SER을 평가하기 위해 이 연구를 위해 구성된 2가지의 KESD 데이터베이스(KESDy18, KESDy19)와 IEMOCAP이 이용되었다.여러 개의 녹음 장치를 기반으로 하는 KESDy18과 KESDy19는 두 개의 도메인으로 이루어져 있다.IEMOCAP의 경우 5개의 목표 감정으로 수집되었다.(happy, sad, neutral, angry, frustration) 그리고 라벨 참여자들은 여섯 개의 기본적인 감정(angry, sad, happy, disgust, fear, surprise) 과 'frustration', 'excited', 'neutral' 중에서 오디오의 감정을 설명할 라벨을 하나 골랐다.수많은 데이터들에 'fear'나 'disgust'와 같은 IEMOCAP에서 목표로 하지 않은 감정으로 라벨링이 되었다.KESD 데이터의 경우에도 인간 감정 인식의 주관적, 다양성을 고려하여 'neutral'과 함께 6개의 기본 감정으로 라벨링을 진행하였다.KESDy18은 30명의 성우가 20개의 문장으로 angry, happy, neutral, sad의 4개의 감정을 연기한 음성으로 구성되어 있다. 6명의 라벨 참여자가 Figure 3a와 같은 방법으로 음성을 들으며 태그를 달았다. figure 3라벨 참여자들은 6개의 기본 감정과 1개의 중립이 포함된 7개의 분류된 감정에서 한 가지를 골라 태그 하였다. 그들이 태그 하는 감정은 성우들이 연기한 감정보다 더 다양하다. 그들은 음성의 arousal과 valence-level을 5개의 포인트로 나누어진 스케일에서 선택하기도 했다. 최종적인 분류 감정은 다수가 선택한 것으로 결정되었다. 각 음성의 arousal과 valence-scale은 평균을 내서 결정되었다.KESDy18은 동시에 2개의 다른 종류의 녹음기로 수집되었다.(핸드폰 내장 녹음기(PM), 컴퓨터에 연결된 외부 녹음기(EM)). 녹음 장치에 의해 KESDy18은 KESDy18_PM에 더해 KESDy18_EM으로 구성되어 있다.KESDy19은 IEMOCAP과 유사한 시나리오를 활용하여 40명의 한국인 성우들이 녹음한 샘플로 구성되어 있다. KESDy19는 두 명의 성우가 번갈아가며 연기하는 동안 음성과 심전도 신호를 수집한 20개의 sessions으로 구성되어 있다. 모든 연기 과정은 기록되었다. 각 세션은 4-10분 정도의 10개의 play로 이루어져 있다. 특정 감정을 이끌어 낼 수 있는 6개의 play가 시나리오의 기본이 되며, 다른 4가지는 연기를 하며 즉흥으로 이루어진다. 각 음성은 7개의 감정 라벨에서 하나로 태그 되었으며 arousal, valence는 5개의 스케일로 나뉘어 10명의 라벨 평가자에 의해 평가되었다. KESDy19는  KESDy19_PM와 KESDy19_EM을 사용한 데이터세트로 구성되었다. IEMOCAP은 10명의 성우들에 의해 녹음된 5개의 세션의 milti-modal audio로 오디오, 영상, 텍스트 데이터로 구성되어 있어 SER에 널리 사용되는 데이터셋이다. 각 세션에서 두 명의 성우가 즉흥 혹은 대본 연기를 수행한다. 이 수행은 3명의 라벨 참여자의 다수 의견에 기초하여 happy, sad, neutral, angry, surprise, frustration, excited, disgust, fear로 분리된 감정 라벨에 태그 되어 있다.또한 IEMOCAP은 6명의 외부 평가자에 의해 5점 척도에 따라 arousal, valence 등의 평가를 반올림 평균 낸 점수를 제공한다. 많은 이전의 연구들이 happy, sad, neutral, angry의 네 가지 감정을 분류하기 위해 IEMOCAP를 사용하여 SER을 평가하였다. ​figure 4는 IEMOCAP, KESDy18 및 KESDy19의 5점 척도에서 네 개의 서로 다른 감정의 valence, arousal의 분포를 보여준다. figure 4a-c에서와 같이 happy로 분류되는 음성들은 valence level의 최고 부분에 분포되어 있고 neutral은 중간에 걸쳐져 있다. sad나 angry는 대체적으로 낮은 수준에 분포되어 있다. 각기 다른 감정 라벨과 arousal의 관계는 더욱 불규칙적이다. sad의 감정은 거의 모든 레벨에 분포되어 있다. KESD 두 개의 happy는 비교적 높은 수준에 분포되어 있으나 IEMOCAP은 그렇지 않다. Figure 4. 5점 척도로 평가된 arousal과 valence의 데이터 샛별 관계를 설명​figure 4에서 각 감정들은 valence-level에서 3개의 그룹을 구성한다. 이 그룹은 happy, neutral / sad / angry이다. 본 연구에서는 table 1과 같이 figre 4에서 각 감정과 valence-level 사이의 관계를 5점 척도에서 3점 척도로 매핑하였다. 3점 척도의 각 valence-level은 negative, neutral, positive이다. 3점 척도로 매핑하기 위하여 본 연구에서는 첫 번째 수준에는 2.5미만, 두 번째 수준은 2.5~4.0미만, 세 번째 수준에서는 4.0 이상의 값을 가지는 샘플을 할당하였다. table 1a는 각 감정 범주에 대한 5점 척도의 평균 및 표전 변동을 보여준다. table 1b는 3점 척도로 매핑하였을 때 각 감정의 연관성을 보여준다. 신뢰도 Conf는 다음과 같이 계산된다.  여기서 Ci는 감정의 라벨이며  1 ≤ i ≤ 4이고, Vj는 valence-lavel, 1 ≤ j ≤ 3을 나타낸다.  4.2. Evaluation of the BLSTM-Based Baseline SERtable 2에서 표시된 것처럼 평가에 사용된 5개의 dataset의 샘플 수에서 불균형이 발생했다. 그러나 우리는 위에서 제안한 MPGLN SER의 객관적인 검증을 위해 oversampling, data augmentation, weighted loss methods를 사용하지 않기로 했다. table2. table 2는 평가에 사용된 세 개의 SER databese를 5개의 행으로 정리한 것이다. 여기서는 길이가 2초 이상은 음성을 angry, happy, neutral, sad로 나누어 사용하였다.SER 훈련에 사용될 datasets의 음성 샘플은 음성부분과 묵음 부분으로 구성되어 학습에 사용되었다. 본 연구에서는 묵음 부분을 음성에서 제거하지 않고 묵음 부분과 함께 모델에 넣었다.​각 감정별 샘플 개수의 불균형을 고려하여 weighted accuracy (WA), unweighted accuracy (UA), precision (PR), F1 score를 성능지표로 제시한다.WA는 전체 정확도이며 실제 라벨을 정확하게 예측한 총 테스트 데이터의 수와 샘플 수의 비율로 계산된다.UA는 4개 클래스의 recall 값의 평균으로 계산되며, 불균형 데이터 세트를 기반으로 한 SER 모델 평가에 중요한 성능 지표이다.본 연구에서는 화자와 speech signal의 변동을 줄이기 위해 각 dataset의 평균과 표준 편차의 z-normalization을 적용하였다. 우리는 화자 독립 검증 기법으로 평가하였으며 훈련을 위해 80%의 화자의 샘플을 사용하였으며 나머지 화자의 샘플은 테스트 데이터로 사용하였다.​IEMOCAP 평가를 위해서 우리는 한 session에 참여하는 두 명의 화자의 음성 데이터를 테스트 데이터로 적용하는 LOSO(Leak-One-Session-Out) validation 기술을 사용했다. KESDy18은 30명의 화자에서 6명을 평가로 사용하였다. KESDy19는 40명의 화자가 2인 1조로 녹음한 20개의 session 중 4개 session에 대해 총 8명의 샘플로 평가를 진행하였다. 각 데이터 셋에서 화자 독립 평가를 위해 분리된 training, test 데이터는 table3과 table4 및 table 6~8에 표시된 것처럼 single domain, multi-domain 혹은 domain generalization 평가에 동일하게 적용되었다. table 3. SER dataset에 설정된 LLD(Low Level Description) 특성에 따른  BLSTM 기반 SER 모델의 성능table 4. BLSTM 기반의 SER 베이스라인 모델의 성능본 연구의 평가에는  transferred embedding feature가 없이 temporal embedding features와 learning loss인 Lcd에 기초한 베이스라인이 SER 기초 모델로 가정되었다. 이 기초 모델은 single-path single-loss(SPSL)체계를 사용하여 동작되었다고 볼 수 있다. 평가단계에서 제안되었던 MPGLN모델과 베이스 라인 SPSL SER 모델은 batch size 200, epochs 25로 그리고 마지막 두 개의 FC 레이어에서 optimizer Adam(drop rate 0.6)으로 학습되었다. optimizer의 learning rate는  0.001 이었다. 이 모델은 10번 이상의 훈련 및 테스트를 거쳐 평가되었으며, 각 성능 metric의 값을 평균 내어 최종 값으로 하였다.기본 SPSL SER 모델은 zero-crossing rate, energy, spectral centroid, spectral roll-off과 같은 21-D time- and spectrum domain(TimeSpectral) LLDs와 함께 13-D MFCC 및 40-D Mel-spectrogram (Mel-spec)으로 구성된 음성 세그먼트의 프레임당 74-D LLD 통합 기능을 사용했다. 우리는 여러 SER 데이터 세트를 기반으로 하는 기본 SER 모델을 사용하여 각 LLD 조합의 성능을 평가했다. table 3는 본 연구에서 사용한 LLDs 모델의 input feature에 따른 성능 평가를 요약한 것으로  IEMOCAP, KESDy18_EM 및 KESDy19_EM 데이터 세트를 기반으로 하였다. table 3의 결과에서 MFCC가 SER의 주된 특성임을 알 수 있다. TimeSpectral LLD와 함께 MFCC 및 Mel-spectrogram의 입력 조합을 사용할 경우 MFCC의 단일 입력과 비교하여 SER 성능은 F1 점수를 기준으로 1.6%에서 3.2%로 향상되었다.table 4는 5개의 데이터 세트에서 4개의 감정을 분류할 때 화자 독립의 BLSTM baseline SPSL의 평가 결과를 보여준다. KESDy19에 기초한 평가는 IEMOCAP와 유사한 성능 결과를 보였다. KESDy18은 평가에서 다른 두 데이터베이스보다 우수한 성능을 보였다.Zheng 등의 이전 연구에서는 IEMOCAP에 기반한 5가지 감정 분류에 대한 CNN 기반 SER 모델의 40% WA 성능을 입증했다.  SER 성능의 공정한 비교를 위해, 본 연구는 많은 이전 SER 연구에서 테스트 환경이었던 IEMOCAP에 기초한 4가지 감정 클래스의 UA 성능을 제시한 이전 RNN 기반 SER 모델과 비교하였다.​표 5에서 우리는 이전의 RNN 기반의 SER 모델과 SPSL baseline 모델을 LOSO 평가로 하여 IEMOCAP에서 4개의 감정 분류하는 성능을 비교하였다. 이러한 연구는 샘플 수의 불균형을 고려하여 각 감정 클래스에 대한 평균 호출의 UA 메트릭을 사용하였다. 표 5에 표시된 바와 같이, 우리의 기본 BLSTM SER 모델은 IEMOCAP 기반의 LOSO 검증에서 UA 59%의 경쟁력 있는 성능을 달성했다. table 5 4.3. Evaluation of Multi-Domain Adaptation표 6–8에 나타낸 바와 같이, 평가는  single-domain 평가, multi-domain 적응, 그리고 교육 및 평가에 참여하는  source와 target domains에 따른multi-domain generalization을 사용하여 수행되었다. 각 데이터셋에서 화자 독립 평가를 위해 분리된 train, test 데이터는 표 3-8에 사용된 것과 동일한 구성을 사용했다. 표 6-8에서 가장 높은 F1 점수는 강조 표시하였다. 표 6은 5개의 datasets 각각에 기반하여 4개의 감정 분류할 때의 평가 결과를 보여준다. 평가는 SER 모델의 유형에 따라 세 가지 실험 환경에서 수행되었다. 기본 SPSL 모델은 temporal embedding features과 single-loss Lcd를 통해 학습되었다. Multi-path-single-loss(MPSL)는 multi-path embedding vectors 를 사용하며 valence-level 분류를 위해 보완적 손실 Lcv가 없는 Lcd에서만 훈련되었다. Multi-path-single-loss(MPGL)은 Lcd와 Lcv로 구성된 multi-path embedding vectors와 group loss, Lg로부터 학습되었다.table 6b에 표시된 KESDy18_PM 데이터 세트를 기반으로 한 harmonic-mean  F1 점수와 비교할 때, single-loss LCD를 사용한 MPSL의 SER 성능은 기준 SPSL보다 1% 향상되었다. loss group, LG에 대해 훈련된 SERMPGL 모델은 SPSL의 F1에 비해 F1이 최대 3.7% 향상된 것으로 나타났다. table 7은 다양한 환경에서 수집된 다중 도메인 SER 데이터셋에서 집계된 샘플로 SER 모델을 훈련했을 때의 multi-domain 적응 평가 결과를 보여준다다. 화자의 약 20%로 분리된 테스트 샘플은 화자 독립 평가를 위해 사용되었습니다. table 7a에서 볼 수 있듯이, 두 개의 녹음장치를 통해 동시에 수집된 두 개의 데이터 셋으로 구성된 KESDy18과 관련하여, MPGL의 group-loss, Lg에 대해 훈련받은 앞서 제안된 SER 모델은 기본 SPSL에 비해 최대 3.7%의 F1 개선을 달성했다.table 8은 multi-domain generalization를 지원하기 위해 제안된 MPGLNSER의 평가 결과를 보여준다. table 8a의 평가에서, SER 모델은 KESDy18_PM, KESDy18_EM 및 KESDy19_EM 데이터셋의 샘플로 훈련되었고, 훈련에는 사용되지 않았지만 동일한 언어문화에서 수집된 KESDy19_PM에서 분리된 테스트 샘플로 평가되었다. table 8a의 평가 결과는 MPGL 모델의 F1 점수가 기준 SPSL에 비해 1.2% 향상되었음을 보여준다. 표 8b의 평가에서는 언어문화가 다른 KESDy18_EM 및 IMEOCAP 데이터셋에 대해 SER 모델을 교육했을 때 한국어 KESDy18_PM 도메인 데이터셋을 사용하여 모델을 평가했다. 제안된 MPGLNSER는 기준 모델에 비해 약 3.5% 향상된 F1 점수를 보였다.figure 5는 table 8의 로스의 변화를 보여주는데, baseline SPSL의 loss,Lcd와 MPGL SER 모델의 loss, Lcd 및 Lcv를 포함하고 있다. 이러한 손실은 총 KESDy18_EM 및 IEMOCAP 샘플을 사용하여 학습 기간 중 25 epochs 마다 측정되었다. 2개의 손실을 동시에 파악한 MPGL 모델의 Loss, Lcd는 기본 SER 모델의 Lcd보다 빠르게 훈련되었다. 이는 valence-level label을 예측하는 데 사용된 MPGLN의 다른 보완적 손실인 Lcv가 baseline SPSL의 손실인 Lcd와 유사하게 감소했음을 보여준다.​ figure 6은 테스트 데이터의 64D 임베딩 벡터가 T-SEN(T-Stochastic Neighboring)을 통해 2-D 임베딩 공간으로 축소된 분포를 보여줍니다. 64D 임베딩 벡터는 표 8b의 평가에서 MPSL 및 MPGL 소프트맥스가 활성화되기 직전에 FC 계층에서 생성된 것입니다.​ ​figure 6a는 complementary loss, Lcv 없이 오직 loss, Lcd에 의해 학습된 MPSL의 임베딩 벡터의 분포를 보여준다. figure 6b는 loss group LG(Lcd와 Lcv)에 기초한 MPGL 모델의 분포를 보여준다. figure 6b는  multi-path embedding vectors와 loss group, LG로부터 학습하는 MPGLNSER 모델을 보여주는데, happy에 속한 샘플이 보다 밀접하게 그룹화되어 있으며, 그림 6a에 나온 MPSL 분포에 비해 angry 및 sad 클래스의 샘플이 서로 더 가깝게 배치되어 있다.  Section 5 : Conclusions우리는 실제 애플리케이션에 구축하기 위해서는 SER 모델의 generalization이 필수적이라고 판단했다. 본 논문에서는 multi-domain dataset을 기반으로 감독된 multi-domain adaptation 과 generalization을 지원하기 위해 SER용 MPGLN을 제안한다. 제안된 MPGLNSER는 음성 샘플의 수작업 LLD 기능의 입력을 사용하는 BLSTM 네트워크를 위한 temporal feature generator를 포함한다. 또한 MPGLN용으로 사전 교육된 VGish 모델에서 이전된 feature extractor를 활용하였다. 제안된 MPGLNSER는 각기 다른 감성과 치수 라벨 사이의 연관성에 의해 유도된 동시 다중 손실을 학습하였다. 제안된 MPGLNSER는 다양한 스피커 도메인, 언어 문화, 수집 장치 및 절차 환경의 5가지 실제 SER 데이터 세트를 사용하여 평가되었다. 여기에는 KESDy18 및 KESDy19 데이터베이스가 포함되어 있다. KESDy18은 한국어의 짧은 문장을 말하는 성우들이 특정한 분리된 감정을 표현하여 전달한 음성 샘플로 구성되어 있다. KESDy18 데이터베이스는 두 개의 다른 녹음장치로 수집하여 KESDy18_PM 및 KESDy18_EM 데이터셋으로 구성되어 있다. KESDy19 데이터베이스는 KESDy19_EM과 KESDy19_PM으로 구성되었으며, 수집된 음성 샘플 음성에는 각각 휴대 전화의 내장 마이크를 기반으로 한 시뮬레이션 데이터 세트의 음성 및 IEMOCAP와 유사한 절차에 따라 작동하는 음성 샘플 음성이 포함되어 있다.본 연구에서는 SER 모델이 기본 SER 모델로서 기능이 전송되지 않고 MPGLN에 포함된 BLSTM 기반 temporal embedding feature generator로만 교육되었다고 가정한다. 우리는 IEMOCAP를 사용하여 기본 SER 모델의 성능 안정성을 검증했다. BLSTM 기반 SER 모델은 4가지 감정 범주 분류 시 59%의 경쟁력 있는 UA 결과를 보였다. 제안된 MPGLNSER의 다중 도메인 적응 및 도메인 일반화 평가는 다양한 평가 환경에 따른 기준 모델의 성능을 비교함으로써 영어 사용 IEMOCAP와 한국어 KESDy18 및 KESDy19 데이터 세트를 사용하여 수행되었다. 다중 손실에 대해 교육을 받은 제안된 MPGLNSER 모델은 단일 도메인 데이터 세트에서 네 개의 감정 라벨을 분류할 때 기준 모델에 비해 F1 성능이 최대 3.7% 향상되었다. 다중 도메인 데이터 세트의 집계된 음성 샘플을 사용하여 SER 모델에 대해 훈련 및 테스트한 감독된 다중 도메인 적응에 대한 MPGLNSER의 성능 평가도 기준 F1 점수보다 최대 3.7% 향상된 것으로 나타났다. 제안된 MPGLNSER의 다중 도메인 일반화를 평가한 결과, F1 점수는 교육에 사용되지 않은 다른 언어문화의 샘플을 사용한 경우 기본 SER에 비해 3.5% 향상되었습니다. 이러한 결과를 통해, 감독된 다중 도메인 적응을 지원하는 MPGLNSER가 다중 도메인 데이터 세트를 기반으로 한 SER 모델의 일반화를 강화하는데도 효과적이라는 것을 알게 되었다. 향후 작업을 위해 다문화 SER 데이터 세트를 기반으로 감성 표현의 음향 특성 차이를 도출하고 도메인 차이를 고려한 딥러닝 기반 SER 모델의 학습 방법을 연구할 계획이다. 또한, 제안된 MPGLNSER를 실제 애플리케이션에 배치하여 실제 음성 데이터에 대한 평가를 통해 모델의 일반화 가능성을 지속적으로 높일 것이다.​  MFCC + mel-spect + time spectral 을 사용한 특성 추출을 시도해보면 좋을 것 같다.또한 valence level을 뽑는 모델과 VGG를 앙상블 한 것도 참신한데 다른 논문에서는 CNN과 LSTM을 직렬로 연결했기 때문이다. 병렬 연결이 어려워보이지만 좋은 성능을 낼 것 같다! "
Voice Recognition System Market for Automotive worth USD 4063.47 Million by 2026 ,https://blog.naver.com/vibhavarimmr/221615300525,20190813,"Voice Recognition System Market for Automotive – Industry Analysis and Forecast (2017-2026) _ by Technology (Embedded and Hybrid), by Fuel Type (Battery Electric Vehicle, Internal Combustion Engine and Others), by Level of Autonomous Driving (Autonomous, Conventional and Semi-Autonomous), by End user (Economy Vehicles, Mid-Priced Vehicles and Luxury Vehicles), by Application (Artificial Intelligence and Non-Artificial Intelligence) and by Geography (North America, Europe, Asia Pacific, Middle East & Africa and Latin America)Voice Recognition System Market valued USD 875.8 Billion in 2016 and expected to reach USD 406.47 Billion by 2026, at a CAGR of 17.2%.​Please click the link to find report description and Table of Content in detail Global Voice Recognition System Market – Industry Analysis and Forecast (2017-2026).​Voice recognition also referred as speech recognition is a system that translates the analog waves of human voice into digital data by sampling the sound and to convert speech to on-screen text or a computer command, a computer has to go through several complex steps. Voice recognition is an alternative to typing on a keyboard in order to operate device, perform commands, or write without having to use a keyboard, mouse, or press any buttons; basically a computer software program or hardware device with the ability to decode the human voice. Voice recognition is a process of providing a fairly natural and intuitive way of controlling the simulation while allowing the user’s hands to remain free.​Voice Recognition System Market for Automotive – Industry Analysis and Forecast (2017-2026)​The main purpose of introducing voice recognition in automotive sector is to simplify the life of driver and passenger in vehicle and furthermore make user-friendly operating system in order to perform some actions using speech commands by the driver or the passengers such as open/close doors, switch on/off headlamp and signal indicator of the vehicle and more. Voice Recognition System Market in automotive consist of control of communication systems integrated in the car infotainment system including telephone, audio devices and destination inputs for navigation. This system will facilitate in achieving “hands-free,” “eyes-free” operation mode​Report includes assessment of market definition along with the identification of key players and analysis of their strategies, complete quantitative analysis of the industry from 2016 to 2026 to enable the stakeholders to capitalize on the prevailing Voice Recognition System Market opportunities, market analysis and comprehensive segmentation with respect to the on technology, application, fuel type, level of autonomous driving, end user, and geography to assist in strategic business planning.​High cost of high-end voice recognition system will restrain the market growth in near future. However, the cost of the voice recognition system is likely to decrease in the coming years.​APAC is expected to register the highest growth rate in Voice Recognition System Market for Automotive during forecast periodAPAC is expected to lead the Voice Recognition System Market for Automotive during the forecast period. Rise in significant adoption of advance technologies in automotive electronics in APAC will drive the market in near future.​Scope of the Report:​Voice Recognition System Market for Automotive, By Technology: Embedded and HybridVoice Recognition System Market for Automotive, By Application: Artificial Intelligence and Non-Artificial IntelligenceVoice Recognition System Market for Automotive, By Fuel Type: Battery Electric Vehicle (BEV), Internal Combustion Engine (ICE) and Others (Hybrid Vehicles)Voice Recognition System Market for Automotive, By Level of Autonomous Driving: Autonomous, Conventional and Semi-AutonomousVoice Recognition System Market for Automotive, By End User: Economy Vehicles, Mid-Priced Vehicles and Luxury VehiclesVoice Recognition System Market for Automotive, By Geography: North America, Europe, Asia Pacific, Middle East, Africa and Latin America​Contact: MAXIMIZE MARKET RESEARCH PVT. LTD.Omkar Heights,Manik Baug, Vadgaon Bk,Sinhagad Road, Pune – 411051, Maharashtra, India.+91 96071 95908/ 9607065656​For more information visit: https://www.maximizemarketresearch.com/market-report/voice-recognition-system-market/1108/ "
System.Speech 및 SAPI 5.3을 사용하여 이미 로드된 문법에 단어를 추가하는 방법 ,https://blog.naver.com/luezoid2572/222649701692,20220217,"다음 코드가 주어졌을 때, Choices choices = new Choices();choices.Add(new GrammarBuilder(new SemanticResultValue(""product"", ""<product/>"")));GrammarBuilder builder = new GrammarBuilder();builder.Append(new SemanticResultKey(""options"", choices.ToGrammarBuilder()));Grammar grammar = new Grammar(builder) { Name = Constants.GrammarNameLanguage};grammar.Priority = priority;_recognition.LoadGrammar(grammar); 로드된 문법에 단어를 추가하려면 어떻게 해야 합니까? 네이티브 코드와 SpeechLib interop을 사용하여 이를 달성할 수 있다는 것을 알고 있지만 관리되는 라이브러리를 사용하는 것을 선호합니다.업데이트: 내가 달성하고자 하는 것은 개별 변경으로 인해 전체 문법을 반복적으로 로드할 필요가 없다는 것입니다. 작은 문법의 경우 호출하여 좋은 결과를 얻었습니다. _recognition.RequestRecognizerUpdate() 그런 다음 이벤트에서 이전 문법을 언로드하고 재구축된 문법을 로드합니다. void Recognition_RecognizerUpdateReached(object sender, RecognizerUpdateReachedEventArgs e) 큰 문법의 경우 이것은 너무 비쌉니다.문법 규칙 참조를 통해 간접 참조를 사용해야 하는 것 같습니다. 이것은 GrammarBuilder.AppendRuleReference 메서드를 사용하여 수행할 수 있습니다. 일부 SRGS 문법 파일을 사용하여 먼저 문법을 테스트하는 것이 더 쉬울 수 있습니다.원칙은 몇 가지 참조가 있는 큰 주요 문법을 동적으로 로드할 작은 사용자 특정 단어 목록 문법으로 로드하는 것입니다.srgs 형식은 http://www.w3.org/TR/speech-grammar/#S2.2 , http://msdn.microsoft.com/en-us/library/system.speech.recognition.grammarbuilder 를 참조하십시오. 프로그래밍 방식 버전의 경우 .appendrulereference.aspx 입니다.  문법이 매우 큰 경우 받아쓰기 문법 옵션을 사용하는 것이 대안입니다. 표준 받아쓰기 문법이 있지만 직접 지정할 수도 있습니다.http://msdn.microsoft.com/en-us/library/system.speech.recognition.dictationgrammar.aspx 및 생성자를 참조하십시오.당신은 이것을 업데이트하지 않을 것입니다. 가능한 모든 단어가 포함되어 있습니다.  네이티브 SAPI에서는 ISpGrammarBuilder2::AddTextSubset()을 사용합니다. "
voice recognition ,https://blog.naver.com/lobiz/222672115621,20220314,"- https://github.com/yashpatel007/voicecode- package install  pip install SpeechRecognition  pip install PyAudio​- online and offline Speech recognition engine/API supportsCMU Sphinx (works offline)Google Speech RecognitionGoogle Cloud Speech APIWit.aiMicrosoft Bing Voice RecognitionHoundify APIIBM Speech to TextSnowboy Hotword Detection (works offline)​- test code import speech_recognition as srr = sr.Recognizer()# https://github.com/yashpatel007/voicecode/blob/master/experiment/harvard.wavharvard = sr.AudioFile('harvard.wav')with harvard as source:    audio = r.record(source)r.recognize_google(audio)# it should print out""""""the stale smell of old beer lingers it takes heatto bring out the odor a cold dip restores health andzest a salt pickle taste fine with ham tacos alPastore are my favorite a zestful food is the hotcross bun"""""" - apirecognize_bing(): Microsoft Bing Speechrecognize_google(): Google Web Speech APIrecognize_google_cloud(): Google Cloud Speech: requires installation of the google-cloud-speech packagerecognize_houndify(): Houndify by SoundHoundrecognize_ibm(): IBM Speech to Textrecognize_sphinx(): CMU Sphinx: requires installing PocketSphinx (works offline).recognize_wit(): Wit.ai​- support formatWAV: must be in PCM/LPCM format, AIFF, AIFF-C, FLAC: must be native FLAC format; OGG-FLAC is not supported​- Microphone  import speech_recognition as srr = sr.Recognizer()# list available microphonesavailableMicrophones = sr.Microphone.list_microphone_names()# choose one from the list as mic = sr.Microphone(device_index=mic_index)with mic as source:    audio = r.record(source)r.recognize_google(audio)""""""This prints out whaat you speak into your microphone""""""# use infinite while loop for contineous listening. try:            while True:                # print(""listening"")                # record aurio for source(i.e mic) for 0.5 sec                with self.mic as source:                    r.adjust_for_ambient_noise(source, duration=0.5)                    audio = self.r.listen(source)                                voice = r.recognize_google(audio, show_all=True)                print(""command :"", voice)                if(not voice):                    continue                results = set()                for item in voice.get(""alternative""):                    results.add(item.get(""transcript"").lower())                self.processtext(results)except sr.RequestError:            print(""API Unavailable"")except sr.UnknownValueError:            print(""not recognized"")except KeyboardInterrupt:            print(""Press Ctrl-C to terminate while statement"") - sample with GUI class VoiceCode():    def __init__(self):        # recognizer        self.play = True        self.r = sr.Recognizer()        self.initRecognizer()        self.mic = sr.Microphone()        self.microphones = sr.Microphone.list_microphone_names()        """"""      Set Energy threashhold for the recognizer: This might differ with loudness and mic quality    """"""    def initRecognizer(self):        self.r.energy_threshold = 400        self.r.dynamic_energy_threshold = True        pass        """"""    List all available microphones that computer have access to.    """"""    def initMic(self, idx):        #microphones = sr.Microphone.list_microphone_names()        self.mic = sr.Microphone(device_index=idx)        #self.mic = self.setMic(microphones)        pass     """"""     Choose a microphone     """"""    def setMic(self, microphones):        global sr        print(""choose your microphone"")        for i, microphone in enumerate(microphones):            print(i, "". "", microphone)        while (True):            mic_index = int(input(""Enter Index: ""))            if(mic_index >= 0 and mic_index < len(microphones)):                break            print(""Wrong input please enter valid number\n"")        print(""Selected mic: "", microphones[mic_index])        mic = sr.Microphone(device_index=mic_index)        return mic    def processvoice(self):        try:            while self.play:                # print(""listening"")                # record aurio for source(i.e mic) for 0.5 sec                with self.mic as source:                    self.r.adjust_for_ambient_noise(source, duration=0.5)                    audio = self.r.listen(source)                                # process the audio with Google Speech Recognition, tyou can also choose others                voice = self.r.recognize_google(audio, show_all=True)                print(""command :"", voice)                if(not voice):                    continue                results = set()                for item in voice.get(""alternative""):                    results.add(item.get(""transcript"").lower())                self.processtext(results)        except sr.RequestError:            print(""API Unavailable"")        except sr.UnknownValueError:            print(""not recognized"")        except KeyboardInterrupt:            print(""Press Ctrl-C to terminate while statement"")        passif __name__ == ""__main__"":    vc = VoiceCode()    def startvoicetype():        startbtn.setEnabled(False)        micoption.setEnabled(False)        langoption.setEnabled(False)        action1.setEnabled(False)        # print(""starting"")        vc.play = True        vc.initMic(int(micoption.currentIndex()))        vc.setLanguage(str(langoption.currentText()))        # create a separate  thread so the voice recognization does not stall the UI        def run():            try:                vc.processvoice()            except KeyboardInterrupt:                print(""Press Ctrl-C to terminate while statement"")                pass        thread = threading.Thread(target=run)        thread.setDaemon(True)        thread.setName(""voiceanalyzer thread"")        thread.start()    def stopvoicetype():        startbtn.setEnabled(True)        micoption.setEnabled(True)        langoption.setEnabled(True)        action1.setEnabled(True)        msg.setText("""")        print(""stopping"")        print(threading.enumerate())        vc.play = False    sys.exit(app.exec_()) - full code: https://github.com/yashpatel007/voicecode/blob/master/voicetype.py voicecode/voicetype.py at master · yashpatel007/voicecodeContribute to yashpatel007/voicecode development by creating an account on GitHub.github.com ​ "
[천안 언어치료] [아산 언어치료] 어음청각검사(KSA - Korean Speech Audiometry) ,https://blog.naver.com/ewha_children/222624364167,20220121,"안녕하세요, 이화바른언어발달센터입니다.인공와우 아동들의 수술 전후의 청능재활을 위한 검사도구인 EARS-K를 소개해드렸는데요.(이에 대한 링크는 맨 아래에 첨부해둘게요^^)​오늘은 단음절, 이음절, 문장 등을 이용하여 어음인지역치, 단어인지도, 문장인지도를 측정하고,중추청각장애(central auditory processing disorder, CAPD)의 평가를 위해서도 사용하는 어음청각검사를 소개해드리고자 해요 :) 또한 어음청각검사는 보청기, 인공와우(cochlear implant, CI) 등 청각보조장치의 착용효과 및 청능재활의 평가도구로도 유용하게 사용할 수 있습니다.​어음을 이용한 청각검사방법은 순음(pure tone)청각검사보다 의사소통능력을 측정하기에 타당한데,보장구를 착용한 아동의 경우 순음청각검사만으로는 종합적인 청각능력을 판단하는데 어려움이 있어어음청각검사를 통해서 보완해야 합니다.​그럼 KSA에 대해 찬찬히 알아보겠습니다 :) 사진출처: 아이소리몰· 어음인지역치검사 (Speech Recognition threshold, SRT)​1. 검사목적제시된 이음절단어(spondees, 양양격 단어)를 정확하게 50% 확인할 수 있는 가장 작은 강도(dB HL)를 측정하는 검사로 순음청각검사와의 일치여부를 확인함으로써 검사의 신뢰도를 확인하고, 어휘청취의 민감도를 측정하는 것입니다. 또한 단어인지도(word recognition score) 검사의 기초자료로도 사용할 수 있습니다. 특히 학령기 및 학령전기 아동의 경우 어음 자극을 이용한 청각검사가 순음이나 환경음 등과 같은 비언어적인 자극을 사용했을 때보다 검사의 집중력과 성공률이 높고, 의사소통능력의 평가에 있어서도 타당도가 높고, 아동의 경우에도 집중력을 높이고 흥미를 유발하기 위해 실물 장난감이나 그림판을 이용하는 것이 더 유리하다는 점에서 의의가 있습니다.​2. 이음절어표- 연령에 따른 구성: 일반용(만 13세 이상), 학령기용(만 6~12세), 학령전기용(3~5세)​· 단어인지도검사 (Word Recognition Score, WRS)​1. 검사목적듣기에 가장 적절한 강도로 단어를 제시하였을 때, 정확히 이해하는 정도(%)를 측정하여 의사소통 장애의 정도, 청력 손실 병변 부위에 대한 정보, 보청기 적응 및 선택, 보장구의 적합 및 재활, 재활의 평가와 계획, 중추처리청각장애 판별 및 재활 등에 필요한 정보를 제공합니다.​2. 단음절어표- 연령에 따른 구성: 일반용(만 13세 이상), 학령기용(만 6~12세), 학령전기용(3~5세)​· 문장인지도검사 (Sentence Recognition Score, SRS)​1. 검사목적듣기에 가장 적절한 강도로 문장을 제시하였을 때, 정확히 확인하는 정도(%)를 측정하는 것으로(ASHA, 1988), 문장자극을 이용한 어음인지도(speech recognition score) 또는 어음명료도(speech discrimination score) 검사의 임상적 중요성이 강조되고 있습니다. 문장인지도 검사는 일상생활의 듣기능력에 대한 실질적인 정보의 제공, 청력 손실 병변 부위에 대한 정보, 보장구의 선택, 적합 및 재활의 평가와 계획, 중추청각처리장애 판별 및 재활 등의 여러 목적으로 사용됩니다.​2. 문장표- 연령에 따른 구성: 일반용(만 13세 이상), 학령기용(만 6~12세), 학령전기용(3~5세)참고문헌: Determining thereshold level (ASHA, 1988), 어음청각검사(이정학 외, 2010)​다음시간에는 아동용 청각검사에 대해 알아보도록 할게요!우리아이의 청능재활과 언어발달과 관련한 문의는이화바른언어발달센터로 연락주시면 친절하게 설명해드리겠습니다 :)오늘도 아이들과 평안한 하루 되세요 ^^   https://blog.naver.com/ewha_children/222624453164 [천안 언어치료] [아산 언어치료] 청각 언어 재활을 위한 평가 가이드(EARS-K: Evaluation of Auditory Responses to Speech)안녕하세요, 이화바른언어발달센터입니다 :) 오늘은 지난 시간에 이어, 오스트리아 MED-EL 사가 인공...blog.naver.com EARS-K에 대한 검사가 궁금하시다면, 위 링크를 참고해주세요^^  ​#천안언어치료, #천안언어발달, #천안아동발달, #천안발달센터, #천안언어검사, #천안발음검사, #천안조음치료, #천안난독증, #천안청각장애, #천안청능훈련, #천안말더듬, #천안자폐스펙트럼, #천안부모교육, #불당동언어치료, #불당동언어발달, #불당동아동발달, #불당동부모교육, #불당동언어검사, #불당동발음검사, #불당동조음치료, #불당동난독증, #불당동청각장애, #불당동말더듬, #불당동자폐스펙트럼, #아산언어치료, #아산언어발달, #아산아동발달, #아산부모교육, #아산언어검사, #이화바른언어발달센터 "
챗GPT와 영어회화를 매끄럽게 하려면! ,https://blog.naver.com/smartbooks1/223103241193,20230516,"챗GPT는 검색엔진이 아니라 ‘생성형 언어 인공지능’입니다. 네이버나 구글 같은 검색엔진은 인터넷의 문서를 검색해서 보여주지만, 챗GPT는 우리가 질문을 하면 전 세계의 ‘거의 모든’ 지식을 학습한 상태에서 마치 사람처럼 그때그때 대답을 만들어(생성해서) 줍니다. 일단 챗GPT를 만든 오픈AI사의 챗GPT 사이트에 접속해서 영어 스피킹과 리스닝을 할 수 있게 해 보죠. 출처: Image by Freepik챗GPT가 영어로 말하고 듣게 하기_Talk-to-chatGPT1. 먼저 PC에서 크롬 브라우저를 여세요. 크롬 브라우저가 아니어도 챗GPT에 접속할 수 있지만, 앞으로 영어로 대화하는 등 다양한 기능을 이용하려면 크롬 웹스토어의 확장앱을 설치해야 합니다.(만약 PC에 크롬 브라우저가 없다면 구글에서 ‘Google Chrome’을 검색한 다음 다운받아 설치하면 됩니다. PC에 크롬이 설치되어 있으면 작업표시줄에 아래와 첫 번째 아이콘이 나타납니다.) 2. 챗GPT 사이트(ai.com 또는 chat.openai.com)에 접속한 다음 <Sign up> 단추를 누르세요. 3. 오픈AI 사이트에 계정을 만들라는 창이 나타나면, 화면의 지시에 따라 이메일 주소, 비밀번호, 성과 이름, 성별, 휴대폰 번호 등을 입력하고 회원가입을 하세요.이미 구글 계정이 있는 분은 ‘Continue with Google’을 클릭하면 오프AI에 편하게 가입할 수 있습니다. 이외의 많은 구글 크롬 확장앱과 AI 툴들에 같은 방식으로 쉽게 가입할 수 있으므로 구글 계정을 만들기를 권합니다. 이미 유튜브 계정을 만드느라 가입되어 상태이니 그 이메일을 가져오면 됩니다. 4. 이제 2번 화면에서 로그인을 하면 챗GPT와 처음 만나게 됩니다. 간단하죠? 화면 하단에 무엇이든 물어볼 수 있는 입력창이 있는데, 여기에 질문을 입력하면 인공지능 챗GPT가 바로 대답을 생성해서 보여줍니다. 질문을 잘 구성할수록 뛰어난 대답을 들을 수 있습니다. 5. 우리는 챗GPT와 영어로 대화를 할 거예요. 챗GPT는 문자로 질문하고 대답하는 인공지능이기 때문에 스피킹과 리스닝을 하려면 ‘Talk-To-ChatGPT’라는 확장앱을 설치해야 합니다. 구글에서 ‘Talk to ChatGPT’를 검색하세요. 6. 검색 결과 화면 상단에 나오는 ‘Talk-To-ChatGPT’를 클릭한 후, 다음 화면이 나오면 <Chrome에 추가> 또는 <Add to Chrome> 단추를 누르세요. 7. 이제 챗GPT에 스피킹, 리스닝 기능이 덧붙여졌습니다. 크롬 브라우저의 인터넷 주소 입력란 오른쪽에 작은 ‘로봇’ 아이콘이 나타납니다. 만약 Talk-To-ChatGPT가 설치되었는데 ‘로봇’ 아이콘이 안 보이면, 직소 퍼즐 조각처럼 생긴 ‘확장 프로그램’ 아이콘을 클릭해서 스크롤 막대를 내린 다음 ‘Talk-To-ChatGPT’ 옆의 핀 모양 ‘고정’ 아이콘을 클릭하세요. 그러면 인터넷 주소 입력란 오른쪽에 로봇 아이콘이 항상 표시됩니다. 8. 키보드에서 <F5> 키를 눌러 새로고침을 한 번 하세요. 그런 다음 로봇 모양 아이콘을 클릭하면, 조금 전에 설치한 스피킹, 리스닝 확장앱인 Talk-To-ChatGPT가 나타나는데 <START> 단추를 누르세요. 9. 아이콘 하단의 얇은 막대가 빨간색으로 뜹니다(이 상태에서 대화를 주고받을 경우, 챗GPT가 말을 할 때는 이 막대가 초록색으로 바뀝니다). 이 빨간색 막대는 ‘내(챗GPT)가 음성을 들을 준비가 되었다’는 표시입니다. 이 아이콘을 한 번 클릭하면 ‘마이크’ 아이콘에 금지선이 그어지면서 음성 수신이 중단됩니다.‘마이크’ 아이콘을 한 번씩 누를 때마다 음성 수신과 음성 중지 상태로 전환됩니다. 여기까지 잘 따라오셨다면 음성 대화형 확장앱 Talk-To-ChatGPT를 성공적으로 깐 것입니다. <잠깐> 여러분의 PC에 헤드셋, 또는 마이크와 스피커가 연결되어 있어야 정상적으로 따라할 수 있습니다. ‘마이크’ 아이콘이 활성화된 상태인데 아래에 빨간색 선이 나타나지 않는다면, 여러분의 PC에 마이크가 연결되어 있는지 점검해 보세요.​<잠깐> 챗GPT는 여러분이 말을 하다가 조금 머뭇거리면, 아직 여러분의 말이 다 끝나지 않았음에도, 마구잡이로 말을 시작해서 아무 말이나 길게 합니다. 챗GPT는 사람과 달리, 지금 자기랑 대화하는 여러분의 반응이 어떤지 모릅니다. 이럴 때는 세 번째의 ‘화살표’ 아이콘을 클릭해서 말을 중단시키면 됩니다.​챗GPT와 영어 회화를 매끄럽게 하려면​이제 챗GPT와 영어 스피킹을 할 수 있습니다. 그런데 챗GPT는 여러분이 영어로 말을 하다가 조금만 머뭇거리거나 멈춰도 막 끼어들어 자기 말을 줄줄 늘어놓기 시작합니다. 이래서는 스피킹 연습을 제대로 할 수 없겠죠? Talk-To-ChatGPT의 설정을 다듬어 보죠.​1. 챗GPT에 Talk-To-ChatGPT 확장앱을 붙이면, 구글의 오른쪽 상단에 다음과 같은 작은 창이 뜬다고 했죠? 여기서 ‘설정’ 아이콘을 클릭합니다(위 9번 화면의 네 번째 아이콘).​2. 다음과 같은 ‘Talk-To-ChatGPT settings’ 화면이 뜹니다. 다음의 내용을 참조하여 언어 인공지능 챗GPT와 영어로 대화하기 편하도록 설정해 보죠. ❶ AI voice and Language: 챗GPT의 목소리와 언어 종류를 선택합니다. 혹시 설정값이 ‘Microsoft Heami-Korean’으로 되어 있으면 챗GPT가 영어를 한국식으로 발음해서 이상하게 들립니다. 목록 단추를 누른 다음 'Google US English(en-US)’를 선택하세요. 이렇게 설정하면 챗GPT가 미국식 영어 발음으로 말하기에 자연스럽게 들립니다.❷ AI talking speed: 챗GPT의 말하는 속도를 설정합니다. 챗GPT가 영어로 너무 빨리 말하면 우리가 알아듣기 힘들겠죠? 원래의 설정값, 즉 1로 그대로 둡니다.❸ AI voice pitch: 챗GPT의 억양을 지정합니다. 기본값인 1로 두세요.❹ Speech recognition language: 챗GPT가 듣고 인식할 언어를 선택합니다. 우리는 영어 스피킹 연습을 하려는 중이므로, 목록 단추를 눌러 미국식 영어, 즉 ‘English-en-US’로 설정하세요.❺ ‘Stop’ word: 챗GPT가 말을 멈추게 하는 명령어를 입력합니다. 기본값인 stop으로 둡니다. 이렇게 두면 여러분이 “stop”이라고 말하면 챗GPT가 말을 멈춥니다.❻ ‘Pause’ word: 챗GPT에게 말을 잠깐 멈추라는 명령어를 입력하는 곳입니다. 기본값인 pause로 두면, 여러분이 “pause”라고 말하면 챗GPT가 말을 잠시 멈춥니다.❼ Automatic send: 여러분이 말을 한 후 챗GPT에게 그 메시지를 자동으로 보낼지 여부를 설정하는 곳으로 매우 중요합니다. 만약 ‘Automatically send message to ChatGPT after speaking’(여러분이 말한 후에 자동으로 챗GPT에게 메시지 보내기)에 체크 표시가 되어 있으면, 여러분이 말하다가 잠깐만 멈춰도 챗GPT가 끼어들어 자꾸 말을 합니다. 이것을 방지하고 싶으면, 반드시 이 체크 표시를 없애세요.❽ Manual send word(s): 수동 전송 단어를 설정하는 곳입니다. 여러분이 여기에 입력한 명령어를 말할 때에만 챗GPT가 비로소 말을 시작하게 설정하는 곳입니다.​_이 포스트는 『조이스박의 챗GPT 영어공부법』의 일부 내용을 정리한 것입니다.​ 조이스박의 챗GPT 영어공부법 저자조이스 박출판스마트북스발매2023.05.13. ​ "
 Speech Recognition 예제 ,https://blog.naver.com/anece00/222053893252,20200807,첨부파일hello.wav파일 다운로드 첨부파일ex_stt.py파일 다운로드 ​ 
챗지피티 무료 AI 영어 회화하는 방법 및 후기(확장프로그램) ,https://blog.naver.com/ppower124/223043698844,20230314,"트위터에 Danielmusk4680님이 아주 좋은 꿀팁을 올려주셔서 공유해본다. 올해 회사에서 세운 목표 중 하나가 Opic speaking IH level인데, 이걸로 달성할 수 있을듯...^^ 는 개뿔...Thank you for helping me with my English speaking도 문법적으로 잘 말하지 못 하는 내 실력 어캄?ㅠ​ 예전에 사둔 10만원짜리 마이크 드디어 쓸 때가 왔구나!!! 확장프로그램 추가하는 법 아주 쉬우니 잘 따라와보세요~ ● 1단계 : 웹스토어 접속 및 확장프로그램 검색PC로 구글 크롬 브라우저 접속한 뒤 웹스토어 들어가서 Talk-To-ChatGPT라는 확장프로그램을 검색하자. 웹스토어 링크를 모를 경우 구글에 검색하면 바로 나오지만 편하게 아래 링크 바로 첨부하겠음 Chrome 웹 스토어브라우저에 새로운 기능을 추가하고 인터넷 탐색 환경을 맞춤설정할 수 있는 소규모 프로그램입니다.chrome.google.com ● 2단계 : 확장프로그램 추가뜸들일 필요 없이 바로 설치해주면 된다. 크롬 브라우저에만 추가되는 시스템이기 때문에 익스플로어 엣지나 네이버 웨일 등에서 사용하지 못한다. ● 3단계 : 챗지피티 사이트 접속챗 지피티 사이트를 접속하여 우측 위에 있는 Talk-To-ChatGPT v1.6.1 작은 창을 확인한다. 확장프로그램 설치한 후 곧바로 ChatGPT 사이트 키면 뜨지 않을 수 있다. 나도 처음에 그랬다.​인터넷 창을 껐다가 다시 키거나, 챗 지피티 사이트를 새로고침 한 번 해주면 뜨더라. 이 창이면 바로 start버튼을 눌러주자 ● 4단계 : 톡투챗지피티 설정 버튼 클릭AI는 수많은 언어를 알아들을 수 있는데, 언어별로 발음이 비슷한 녀석들 있다. 그러므로 ChatGPT가 어떤 언어를 알아들을 것인지 설정해줘야한다. start버튼 누르면 위와 같이 설정 톱니바퀴가 뜨므로 클릭하자 ● 5단계 : 톡투챗지피티 언어 설정하기AI voice and language는 chatGPT AI가 우리에게 어떤 언어로 얘기할지 설정하는 것이다. 우리 영어회화할거잖아? Google US English를 선택해주면 된다.​Speech recognition language는 우리가 어떤 언어로 얘기할지 AI에게 말해주는 설정이다. 우리는 영어로 말할 것이므로 English - en - US하면 된다. 참고로 언어가 정말 많아서 일본어, 중국어, 프랑스어 회화도 가능하다.​그 외 AI talking speed로 AI가 어떤 속도로 말해줬으면 좋겠는지 설정할 수 있고, AI voice pitch로 문장과 문장 간의 간격을 설정할 수 있다. 우리가 한 문장 한 문장 알아듣는데 오래걸린다면 이 숫자를 크게해야겠지?​ ​stop word는 영어회화(채팅)를 끝내고 싶을 때 말할 단어를 말하는데, 내가 Stop이라 말했더니 채팅이 끝나버렸다. pause word도 마찬가지. ● 기타 하면 좋은 설정들 및 후기I don't like you talking too long so please answer with just one sentence or answer as short as possibleChatGPT는 모든 질문에 대해 장황하게 답해서 영어회화에 어려움을 겪는다. 그러므로 위와 같이 대답을 짧게 해달라고 규칙을 설정해두면 좋음​2. Please correct me if what I say is grammatically incorrect2. Can you correct my pronunciation?원하는 구체적인 피드백을 요구할 수 있다. 문법을 위주로 첨삭받고 싶으면 위와 같이 [문법 틀린 부분 있으면 고쳐줘]라고 요구할 수 있음. 발음 피드백도 요청할 수 있으나 내 실력의 한계로 잘 되진 않는다... 굴욕적이다​3. 나같은 영어회화 허졉은 말하다 2~3초 머뭇거리거든? 곧바로 ChatGPT의 대답이 나와버린다. 설정에서 Automatically send message to ChatGPT after speaking의 체크박스를 풀면 내 말을 중간에 끊는 현상이 없어진다.​ 4. 발음이 부정확해도, 채팅창에 다른 단어 입력되어도 문맥상 알아듣는다. 나도 Short의 발음이 이상해서 shirt라고 채팅이 입력됐는데, 챗 지피티는 잘 알아듣고 대답해준다.​채팅에 입력되는 단어로 내 발음이 부정확한지 아닌지 확인할 수 있을 듯. 5. 혹시나 발음 교정해줄 수 있냐니까 해줄 수 있다고 답이 왔다. 그런데 발음 교정은 활용이 힘들다. 나는 shirts라 의도하고 말했는데 short라 알아듣고 채팅이 입력되면, short 발음 좋은데요? 라고 답변이 오기 때문...내 의도는 shirts라고....​6. AI라서 살짝 기계적인 어조 및 발음이라고 하지만 개인적으로 이것도 감지덕지하다고 생각함 ● 결론앞으로 마이크를 이용한 언어 소통 모델도 활발해질 것으로 보이는데, 매일 챗 지피티로 10분 20분씩 프리토킹 노력하다보면 영어 스피킹에 자신감이 붙지 않을까 생각함...단순 희망사항이다. 물론 이런 프리토킹하면서 자존심 상하면 내 스스로 유튜브나 여러 실생활 영어 문장들 찾아다니며 공부할 듯. 스스로 공부 및 복습도 중요해!!​오픽 학원 다녀야하나 고민 중이었는데 이걸로 사교육 부담은 좀 줄어들 것으로 보인다 ㅎㅎㅎ "
"입술 움직임을 만들어낸다, 립 제너레이션 ",https://blog.naver.com/with_msip/222963100403,20221223," 딥러닝의 발전과 함께, 몇 단어로 구성된 문장만으로 그림을 생성하는 인공지능이 유행하고 있습니다. 단순한 그림 한 장 만드는 것부터 영상을 생성하기 까지 지속적으로  발전하고 있는데요. 인물의 사진으로 입술의 움직임을 만들어낼 수 있는 기술인 ‘립 제너레이션(lip generation)’ 함께 살펴볼까요? 립 제너레이션 기술립 제너레이션 기술은 오디오와 영상 속 인물의 입술 발화를 맞추는 AI 기반의 립싱크 기술입니다. 인물의 영상 또는 이미지와 오디오를 입력하면 오디오 싱크에 맞춰 말하는 듯한 입술의 영상을 생성하죠. 최근에는 영상과 같은 디지털 커뮤니케이션이 활발해지고 있어, 실제 입술 움직임을 동기화하는 것이 더욱 중요해지고 있기 때문에 립 제너레이션 기술에 대한 연구도 활발히 진행되고 있습니다. 립 제너레이션의 결과 생성되는 립싱크 ⓒhttps://medium.com/립 제너레이션과 함께 발전하는 음성변환 기술립 제너레이션과 함께 발전하는 기술들이 있습니다. 대표적으로 음성변환(speech to speech translation, S2ST) 기술인데요. 음성변환은 특정 언어의 음성을 타겟 언어의 음성으로 변환하는 기술입니다. 음성인식(automatic speech recognition, ASR), 번역(machine translation, MT), 음성생성(text to speech, TTS)의 순서로 입술의 움직임을 동기화하는 방식으로 활용되고 있습니다. 이러한 과정에서 신호처리를 통해 음성의 특징을 추출하고, 딥러닝을 통해 번역을 수행한 후, 해당 번역 결과를 음성으로 합성하는 과정이 포함됩니다.  음성변환 기술을 나타낸 그림 ⓒhttps://www.techexplorist.com​LipGAN과 Wav2Lip영상이나 음성합성 기술에 대한 여러 연구가 진행되고 있는만큼, 립 제너레이션에 대한 연구도 꾸준히 발전하고 있습니다. 가장 대표적인 입술 움직임 동기화 기술은 2019년 발표된 LipGAN(논문제목 Towards Automatic Face-to-Face Translation)입니다. 영상과 번역된 오디오를 합성하여 실제 번역된 오디오를 말하는 것과 같은 영상으로 입술 움직임을 생성하는 기술입니다.  LipGAN의 네트워크 구조 ⓒhttps://medium.com/앞서 설명한 음성변환 기술에 음성인식, 번역, 음성생성, CycleGAN을 활용하여 만들어낸 목소리를 타겟 목소리와 비슷하게 조정하고, 자연스러움을 연출하는 음성전달(voice transfer) 과정에 LipGAN을 사용하면 특정 언어로 발화된 영상을 다른 언어로 립싱크한 영상으로 변환됩니다.​2020년 제안된 기술인 Wav2Lip은 이전 LipGAN에서 더 발전된 모델입니다. 이 기술은 음성의 길이, 어휘 개수 등에 제한되지 않고 더 정확한 립싱크를 맞추는 모델을 제안하면서 립 제너레이션의 우수함을 판단할 수 있는 새로운 평가 지표로 제안하였습니다.  Wav2Lip의 네트워크 구조 ⓒhttps://medium.com/LipGAN에 비해 Wav2Lip은 더 우수하고 강력한 립싱크 판별기인데요, 판별자(Discriminator)가 여러 프레임으로 확장되어 생성된 입술 움직임이 얼마나 정확한 지 결정할 때에 시간적인 문맥을 이해하는 부분을 추가하였습니다.​또한 사전훈련(pre-train)하여, 모델을 훈련하는 동안 가중치가 고정되어 생성기(generator)의 시각적 아티팩트에 영향을 받지 않고 생성된 입술 움직임의 정확성에만 집중할 수 있습니다. 출력 프레임의 전체 얼굴이 실제처럼 보이도록 시각적 품질과 관련된 함수를 추가하여 이전 모델인 LipGAN에서 발견된 아티팩트를 최소화하였습니다.​더욱 흥미로운 것은, Wav2Lip의 저자들이 공개한 프로젝트 페이지의 실제로 훈련된 네트워크를 통해 영상 파일(video)과 음성 파일(audio)을 입력하면 해당 네트워크의 결과물로 립 제너레이션 된 영상을 획득할 수 있습니다. 아래 홈페이지에서 음성과 목소리의 짝을 맞추어 립 제너레이션 해보세요. Interactive Wav2Lip DemoInteractive Demo Select a video file (Max 20 seconds): Select an audio file (or) video with audio (Max 20 sec): Or choose from the example pairs below! Using our open-source code , you can attempt to lip-sync higher resolution/longer videos. You will be able to tune the inference parameters and henc...bhaasha.iiit.ac.in 이후에도 Wav2Lip에 이어 Attention-Based Lip Audio-Visual Synthesis for Talking Face Generation in the Wild와 같은 개선된 여러 기술들이 공개되고 있습니다.  해당 기술이 사용될 수 있는 분야애니메이션을 만들 때 프리스코어링(prescoring) 방법을 사용하는데요, 이러한 프리스코어링은 소리부터 먼저 녹음한 후 소리에 맞춰 작화(그림을 그리는)를 하는 방식을 사용합니다. 이러한 프리스코어링은 생동감있는 화면을 만들어내기 좋지만 시간과 돈이 많이 드는데요, 립 제너레이션 기술이 이를 보조할 수 있습니다. 한국의 AI인간 ⓒhttp://www.koreatimes.co.kr/또한 AI인간(virtual human)의 경우에도, 얼굴 생성(face generation) 기술이 사용됩니다. 이러한 얼굴 생성에는 발화 시의 입술 모양도 필수적으로 고려되어야 하는데요, 이 때 참조가 되는 현실 인간의 영상 없이도 립 제너레이션 기술을 활용하여 AI인간의 얼굴과 음성만으로 충분히 구현해낼 수 있을 것입니다.​앞으로도 영상합성 기술에 꾸준히 사용될 립 제너레이션 기술, 더욱 더 발전하여 더더욱 자연스러운 입술 움직임을 만들어낼텐데요. 인공지능 기반 얼굴 합성 변조 기술인 딥페이크 시장이 커지면서 더욱 빛을 발할 것 같습니다. 앞으로도 립 제너레이션 기술의 발전을 기대해주세요.   ​  "
[betanews] OpenAI releases an official ChatGPT app for iOS ,https://blog.naver.com/sunggonk/223108279838,20230522,"[betanews] OpenAI releases an official ChatGPT app for iOS​By Sofia Wyciślik-Wilson​There are a huge number of fake ChatGPT apps out there -- particularly for Android handsets -- but now the company behind the artificial intelligence has released an official iOS app.​The app is not only free, it is also devoid of advertising. For most people, the app provides access to the GTP-3 powered AI, but anyone who subscribes to ChatGPT Plus gains access to the power of GPT-4.​In a blog post about the release, OpenAI says: ""Since the release of ChatGPT, we've heard from users that they love using ChatGPT on the go. Today, we’re launching the ChatGPT app for iOS. The ChatGPT app is free to use and syncs your history across devices. It also integrates Whisper, our open-source speech-recognition system, enabling voice input. ChatGPT Plus subscribers get exclusive access to GPT-4’s capabilities, early access to features and faster response times, all on iOS.""​The company goes on to say:​We're starting our rollout in the US and will expand to additional countries in the coming weeks. We’re eager to see how you use the app. As we gather user feedback, we’re committed to continuous feature and safety improvements for ChatGPT.​With the ChatGPT app for iOS, we’re taking another step towards our mission by transforming state-of-the-art research into useful tools that empower people, while continuously making them more accessible.​Android users need not panic -- you have not been forgotten. OpenAI adds: ""P.S. Android users, you're next! ChatGPT will be coming to your devices soon"".​You can download ChatGPT here.​from https://betanews.com/2023/05/19/openai-releases-an-official-chatgpt-app-for-ios/ "
C#: System.Speech 네임스페이스를 사용하여 WAV 파일을 텍스트(음성-텍스트)로 변환 ,https://blog.naver.com/foxclore4615/222682668462,20220325,".NET 음성 네임스페이스 클래스를 사용하여 WAV 파일의 오디오를 화면에 표시하거나 파일에 저장할 수 있는 텍스트 형식으로 변환하는 방법은 무엇입니까?튜토리얼 샘플을 찾고 있습니다.업데이트여기 에서 코드 샘플을 찾았습니다. 그러나 시도했을 때 잘못된 결과를 제공합니다. 아래는 내가 채택한 vb 코드 샘플입니다. (사실 나는 vb/c#... 중 하나인 한 언어를 신경 쓰지 않습니다.) 그것은 나에게 적절한 결과를 제공하지 않습니다. 나는 우리가 올바른 문법(즉, 녹음에서 우리가 기대하는 단어)을 넣으면 그 텍스트 출력을 얻어야 한다고 가정합니다. 먼저 통화 중 샘플 단어를 사용해 보았습니다. 때때로 그 (한) 단어만 인쇄하고 다른 것은 인쇄하지 않았습니다. 그런 다음 녹음에서 전혀 예상하지 못한 단어를 시도했습니다. 불행히도 그것도 인쇄되었습니다... :( Imports SystemImports System.Speech.RecognitionPublic Class Form1    Dim WithEvents sre As SpeechRecognitionEngine    Private Sub btnLiterate_Click(ByVal sender As System.Object, ByVal e As System.EventArgs) Handles btnLiterate.Click        If TextBox1.Text.Trim.Length = 0 Then Exit Sub        sre.SetInputToWaveFile(TextBox1.Text)        Dim r As RecognitionResult        r = sre.Recognize()        If r Is Nothing Then            TextBox2.Text = ""Could not fetch result""            Return        End If        TextBox2.Text = r.Text    End Sub    Private Sub Button1_Click(ByVal sender As System.Object, ByVal e As System.EventArgs) Handles Button1.Click        TextBox1.Text = String.Empty        Dim dr As DialogResult        dr = OpenFileDialog1.ShowDialog()        If dr = Windows.Forms.DialogResult.OK Then            If Not OpenFileDialog1.FileName.Contains(""wav"") Then                MessageBox.Show(""Incorrect file"")            Else                TextBox1.Text = OpenFileDialog1.FileName            End If        End If    End Sub    Public Sub New()        ' This call is required by the Windows Form Designer.        InitializeComponent()        sre = New SpeechRecognitionEngine()    End Sub    Private Sub sre_LoadGrammarCompleted(ByVal sender As Object, ByVal e As System.Speech.Recognition.LoadGrammarCompletedEventArgs) Handles sre.LoadGrammarCompleted    End Sub    Private Sub sre_SpeechHypothesized(ByVal sender As Object, ByVal e As System.Speech.Recognition.SpeechHypothesizedEventArgs) Handles sre.SpeechHypothesized        System.Diagnostics.Debug.Print(e.Result.Text)    End Sub    Private Sub sre_SpeechRecognitionRejected(ByVal sender As Object, ByVal e As System.Speech.Recognition.SpeechRecognitionRejectedEventArgs) Handles sre.SpeechRecognitionRejected        System.Diagnostics.Debug.Print(""Rejected: "" & e.Result.Text)    End Sub    Private Sub sre_SpeechRecognized(ByVal sender As Object, ByVal e As System.Speech.Recognition.SpeechRecognizedEventArgs) Handles sre.SpeechRecognized        System.Diagnostics.Debug.Print(e.Result.Text)    End Sub    Private Sub Form1_Load(ByVal sender As Object, ByVal e As System.EventArgs) Handles Me.Load        Dim words As String() = New String() {""triskaidekaphobia""}        Dim c As New Choices(words)        Dim grmb As New GrammarBuilder(c)        Dim grm As Grammar = New Grammar(grmb)        sre.LoadGrammar(grm)    End SubEnd Class 업데이트(11월 28일 이후)기본 문법을 로드하는 방법을 찾았습니다. 다음과 같이 진행됩니다. sre.LoadGrammar(New DictationGrammar) 여기에 여전히 문제가 있습니다. 인식이 정확하지 않습니다. 출력은 쓰레기입니다. 6분 파일의 경우 음성 파일과 전혀 관련이 없는 5-6단어의 텍스트를 제공합니다.System.Speech의 클래스는 텍스트 음성 변환(주로 접근성 기능)을 위한 것입니다.음성 인식을 찾고 있습니다. .Net 3.0부터 사용할 수 있는 System.Speech.Recognition 네임스페이스가 있습니다. Windows Desktop Speech 엔진을 사용합니다. 이것은 당신을 시작할 수 있지만 더 나은 엔진이 있다고 생각합니다.음성 인식은 매우 복잡하고 제대로 하기 어렵습니다. 일부 상용 제품도 있습니다.  실제로 자연어 도구 키트가 필요합니다. 파이썬에서는 NTLK http://www.nltk.org/ 를 사용했습니다..Net에서 방금 Antelope를 찾았습니다.https://stackoverflow.com/questions/1762040/natural-language-toolkit-equivalent-in-c기사도 참조하십시오 http://en.wikipedia.org/wiki/Speech_recognition  SpeechRecognitionEngine 을 사용해야 합니다. 웨이브 파일을 사용하려면 SetInputToWaveFile 을 호출하십시오. 더 많은 도움을 드리고 싶지만 저는 전문가가 아닙니다.아, 그리고 만약 당신의 말이 정말로 triskaidekaphobia 라면, 나는 인간의 음성 인식 엔진조차도 그것을 인식하지 못할 것이라고 생각합니다 ...  귀하의 코드를 테스트했지만 웨이브 파일을 제대로 잡지 못하고 있습니다. 잡히고 있다OpenFileDialog1.FileName.Contains(""wav"")가 아니면MessageBox.Show(""잘못된 파일"")또 다른TextBox1.Text = OpenFileDialog1.FileName종료else 조건이 아닙니다. 문자열에서도 .wav를 사용해 보았습니다.또한 wav 파일을 마이크가 아닌 텍스트로 변환하기 위한 샘플 코드가 필요합니다. 좋은 해결책을 찾으셨다면 여기에 올려주세요.  나는 이것이 오래된 질문이라는 것을 알고 있지만 나중의 질문과 답변에서 더 나은 정보를 얻을 수 있습니다. 예를 들어 asp.net 웹 앱에서 음성을 텍스트로 변환하는 가장 좋은 옵션은 무엇인가요? 를 참조하세요.SetInputToDefaultAudioDevice()를 호출하는 대신 SetInputToWaveFile()을 호출하여 오디오 파일에서 읽을 수 있습니다.Windows Vista 및 Windows 7에 제공되는 데스크톱 인식 엔진에는 참조된 답변에 표시된 받아쓰기 문법이 포함되어 있습니다. "
Voice recognition microphone dictation ,https://blog.naver.com/sgu_edu/221603993059,20190803,"인턴으로 생활하던 중, 오후 5시 이전에 병원을 떠나기는 쉽지 않았다. 병동에서 일할 경우는 더더욱 그런데, 주 이유는 progress note를 마무리 해야 하기 때문. ​회진, 필요한 검사들 오더와 확인, 환자 퇴원 수속 등을 마치고 기타 등등의 일을 하면서 짬짬히 맡은 환자들의 progress note를 제시간에 마치는 데에 필요한 효율성을 습득하는데, 적지 않은 시간이 걸린다. 보통 6개월, 길게는 1년 혹 개인에 때라 그 이상이 필요할 수도. ​의료인들에겐, Progresss note는 아주 중요한 대화의 수단이자, 병원에게는 병원비를 산출하는 수단이기에 중요하다. medical team member 그 누가 읽더라도 환자의 상태를 알 수 있어야 하고, 이 환자에게 어떤 상황이 발생했을 경우 가이드라이인 되어주기도 한다. ​생각을 정리하며, 탁! 탁! 탁! 키보드를 두들기며 적어나가는데, 아무리 빨라도 그리고 간단히 한다 해도 10분은 걸린다. 새로 입원환자이거나 복잡한 케이스의 경우엔 더 걸리기도 한다. 이런 걸 10번 (10명의 환자를 본다고 한다면) 한다고 치면 적어도 최소 100분에서 길게는 150분이 걸리는 것인데... 주말에 25-35명의 노트를 한다하면 컥....​한달 전, Program Direct, Dr. Baggerman, 이 우리에게 큰 선물을 안겨주었다. 그 선물은 바로, ​ Speech recognition micro phone​Micro phone을 통해 직접 dictation이 가능하다. (저기 대고 말하면, 컴퓨터 EMR system에 고대로 착착착)스마트폰을 음성으로 작동시키는 것처럼.  ​이렇게 해서, 전문의들 위주로 쓰이던 dictation micro phone service를 레지던트들도 쓸 수 있게 되었다. 심지어, 스마트폰에 앱을 설치 후, 집에서도 사용 가능. (어떤 병원들은, 집에서 병원 EMR system에 접근하지 못하게 하기도 한다. 우리 병원을 포한한 병원들은, 노트북과 인터넷이 있으면 어디에서든 EMR system에 접근하여, 못마친 일을 어디서든 할 수도 있도록 하고 있다.)​게다가, 나의 콩글리쉬 발음도 알아듣는다. 게다가 AI(인공지능) system 으로, 쓰면 쓸수록 나의 발음을 더 잘 알아듣게 되고, 오류도 점점 줄거라 한다.오늘은, gradually를 really로 알아들어서 탁탁탁 키보드로 수정이 필요했지만..​​​​​팁: 인터뷰 다닐 때, 지원자들이 종종 EMR system을 묻기도 하는데, 이것도 물어봐야 하지 않나 싶다.  dictation micro phone service. ​병원이 얼마나 편리한 EMR system을 가지고 있냐에 따라 레지던트들의 삶의 질이 달라진다. 물론, 다른 병원으로 실습을 나가보지 않는 이상, 내가 있는 곳이 가장 나쁘고 가장 좋을 거지만 - 비교할 대상이 없기에. 예를 들면, 우리 병원 근처에 있는 IM 내과 레지던시 프로그램이 있는 병원에선, 상당히 구형 버전의 EMR을 쓰고 있어서, 모든 걸 (labs, drugs, 매일 같이 반복되는 부분들까지)  다 탁 탁 탁 타이핑 해야 한단다. 반면, EPIC (적어도 미국에선, 가장 효율적이고 편리한 EMR이라고 정평이 나있다.) 을 이용하는 우리 병원에선, 쉽게 labs/drugs/반복되는 것들을 불러올 수 있다. 같은 일을 해도, EPIC을 이용하면 시간을 많이 줄일 수 있는데, 여기에  voice recognition micro phone dictation service까지 있다면, 문서작업에 할애하는 시간을 상당히 줄이고, 환자 케어와 그들의 목소리에 더 많은 시간을 쓸 수 있다. ​ "
올해 두번째 SSCI - 탑저널 억셉! ,https://blog.naver.com/gmliving/223071869651,20230412,"와우!! 방금 전에 에듀테크의 탑 저널 중에 하나인 Interactive Learning Environments 에서 억셉 소식을 받았다!!! 천재 박사과정생인 인디애나 주립대의 천재호 쌤과 함께 야심차게 AI Chatbot 분야의 리뷰 논문을 쓴 것인데, 역대급 기록인 50일 만에 억셉을 받았다. 아마 평생 이 기록을 깨기는 힘들 것 같다. ​제목은 ""A systematic review of research on speech-recognition chatbots for language learning with implications for future directions"" ​50일 만에 억셉을 받았다는 거는 에디터가 이 논문을 내 줄 마음이 처음부터 있었다는 거다. 이건 지금 자세하게 말하기 힘들지만 우리는 알고 있지. 에디터가 이 논문을 얼마나 좋아하고 내고 싶었는가 말이다.​암튼 이로서 우리 팀이 적어도 언어교육 분야에서는 인공지능 챗봇으로 어느정도 입지를 갖게 될 것 같다. 즉, 우리 논문을 인용하지 않고는 챗봇을 논하기에 어렵게 되었다는 거다. 훌륭한 논문을 써 주신 선배 연구자들에게도 무한한 감사. 이러한 성과는 오롯이 우리의 것은 결코 아니다. 그들의 어깨 위에서 서서 바라본 결과물이다.​암튼 너무 기뻐서 잠이 잘 안 오지만... 일단 자고 현재 진행 중인 챗봇 논문도 마무리 해 보자!​으흐흐흐흐흐 기쁜 거! 이게 연구하는 묘미 아니겠나? ㅋㅋㅋㅋ ​ "
[서초보청기] 보청기 착용 시 청력평가의 중요성 ,https://blog.naver.com/hhkhcc/223100462238,20230512,"​보청기를 사용하는 분이라면 한 번쯤은 청력평가를 받아본 경험이 있을 텐데요. 자신의 청력에 적합한 보청기 선정을 위해서는 정확한 청력평가가 가장 기본이 되어야 합니다. 보통 청력평가는 소리가 들리면 버튼을 누르거나 손을 드는 등의 반응을 통해 들을 수 있는 가장 작은 역치를 찾는데요. 이러한 청력평가의 종류에는 어떤 것이 있을까요? 함께 알아보겠습니다. 순음청력평가PTA (Pure Tone Audiometry)  ​청력평가 중 가장 기본이 되는 평가이자, 중요한 평가인 순음청력평가는 '삐-'하는 순음(Pure Tone)을 이용하여 주파수별 청력 역치를 측정하는 평가입니다. 평가 주파수는 보통 250Hz, 500Hz, 1,000Hz, 2,000Hz, 4,000Hz, 8,000Hz 6개 주파수입니다. 순음청력평가를 통해 청력손실의 종류(전음성 난청, 감각신경성 난청, 혼합성 난청) 및 청력손실의 정도와 청력 형태 등을 알 수 있습니다. 이러한 순음청력평가는 자극음의 전달 방식에 따라 '기도청력평가'와 '골도청력평가'로 나눌 수 있습니다.​​1. 기도청력평가(Air conduction pure-tone audiometry) 기도청력평가기도청력평가는 헤드폰이나 삽입형 이어폰(인서트폰)을 사용하여 공기 중의 소리가 귓속을 통해 들어가 고막, 이소골을 거쳐 달팽이관으로 전달되는 청각의 모든 경로를 측정하는 평가입니다. 전체 청각 전달 경로 중 한 곳만 이상이 있어도 난청으로 나타납니다. 평가 주파수는 1,000Hz에서 시작하여 한 옥타브 간격으로 2,000, 4,000, 8,000Hz의 고주파수 대역을 먼저 평가하고, 저주파수인 500, 250Hz 순으로 평가합니다.​​2. 골도청력평가(Bone conduction pure-tone audiometry) 골도청력평가골도청력평가는 골진동체를 귀 뒤쪽의 유양돌기에 위치시킨 후 두개골을 진동시켜 두개골에 내재된 달팽이관을 직접 자극하여 청력을 측정하는 평가입니다. 외이나 중이에 이상이 있더라도 내이 기능에 문제가 없다면 골도청력평가 시 정상 청력을 나타내기 때문에 골도 평가를 통해 외이나 중이의 이상 유무를 확인할 수 있습니다.​ 어음청력평가SA (Speech Audiometry)어음청력평가는 말 그대로 의사소통에 주로 사용하는 어음의 청취 능력 및 이해도를 평가합니다. 어음분별력이 많이 저하된 경우에는 보청기를 통해 적절히 소리를 증폭한다 하더라도 만족스러운 착용 효과를 느끼지 못할 수 있습니다. 따라서 난청의 선별은 물론, 보청기 착용 전 보청기 착용 효과를 어느 정도 예측할 수 있는 평가입니다.어음청력평가에는 어음인지역치(SRT), 단어인지도(WRS) 평가가 있습니다. ​​1. 어음인지역치(Speech Recognition Threshold) 평가 어음인지역치(SRT) 평가는 동일한 강세가 있는 2음절 단어를 제시하여 겨우 들을 수 있는 최소의 소리 강도 레벨을 측정합니다. 이는 일상생활의 의사소통 능력을 측정하므로 순음보다 더욱 의미 있는 평가이며,  순음청력평가의 신뢰도를 알 수 있습니다.​2. 단어인지도(Word Recognition Score) 평가 단어인지도(WRS)는 대상자에게 편안하게 들리는 강도에서 단음절어 단어를 제시하고 얼마나 정확하게 인지하는지 백분율로 나타냅니다. 단어인지도를 통해 보청기 효과를 예측할 수 있기 때문에 일반적인 청력평가뿐만 아니라 보청기 적합 전후에 반드시 시행해야 하는 필수적인 평가입니다. 이 평가를 통해 WSR 점수가 높다면 보청기를 착용 시 효과가 그만큼 좋을 것이라 예측하고, 만약 점수가 많이 낮다면 효과는 그만큼 적을 것이라 예측합니다. 정기적인 청력평가의 중요성최근 고령화로 난청으로 인한 치매 예방에 대한 관심도 높아져 자연스럽게 보청기를 고려하는 분들이 늘고 있습니다. 보청기에 대한 아무 정보가 없는 분들은 청력 평가 없이 그냥 아무 보청기나 구매하여 착용하는 된다고 생각하시는 분들도 많습니다. 하지만, 보청기는 손상된 청력을 보충해주는 청각 보조 기기로 청력을 정확히 확인하고 과학적인 소리 조절을 받아야 편안한 착용을 할 수 있습니다. 정확한 청력평가 결과는 보청기 선정부터 소리 조절까지 영향을 미치는 가장 기본적이고 객관적인 정보입니다. 보청기를 착용한 이후에도 여러 가지 이유로 청력의 변동이 있을 수 있기 때문에 적어도 1년에 한 번씩 보청기 센터를 방문하여 정기적인 청력 체크를 통해 최적의 상태로 만족스럽게 보청기를 착용하셔야 합니다. 꾸준한 관리가 착용 만족도를 높일 수 있기 때문에 정확한 청력평가가 가능한 곳에서 전문가로부터 상담을 받으시고 보청기 효과를 최대한 얻으시길 바랍니다.  국제 규격에 맞는 방음부스 시설을 완비하고 있는 황혜경보청기 청각언어센터는 청각학 석박사 전문가 그룹으로 국가 표준(보청기 적합관리, KS I 0562)에 부합하는 보청기 전문센터이며, 국내 최고 수준의 전문 장비와 시설을 갖추고 최고의 청능재활서비스를 제공하고 있습니다. 국내 최초로 세계 유명 브랜드(벨톤, 스타키, 시그니아, 오티콘, 와이덱스, 포낙 등) 상담 시스템을 도입하여 보청기를 대상자의 청력 정도, 연령 및 생활환경을 고려해 합리적인 가격으로 비교 추천하고 있습니다. 서초구, 마포구, 송파구, 강서구, 종로구에 총 5곳의 프리미엄 직영센터를 운영하고 있으니 이용하기 편리한 센터로 방문해 주시기 바랍니다.   황혜경보청기 청각언어센터 - 서초방배센터 안내주 소 : 서울 서초구 방배로 162, 예광빌딩 2층전화번호 : 02. 3477. 0114지 하 철 : 7호선 내방역 7번 출구 우측 바로 앞주 차 : 예광빌딩 뒤편 지상주차장 이용​[서초방배센터 오시는 길] 황혜경보청기 청각언어센터 서초방배서울특별시 서초구 방배로 162 예광빌딩 2층황혜경보청기 청각언어센터 마포공덕서울특별시 마포구 마포대로 68 마포아크로타워 7층황혜경보청기 청각언어센터 송파잠실서울특별시 송파구 올림픽로 119 파인애플상가 3층황혜경보청기 청각언어센터 강서마곡서울특별시 강서구 공항대로 247 퀸즈파크나인 B동 2층황혜경보청기 청각언어센터 종로3가서울특별시 종로구 종로 122 타워15 2층 ​ "
음성지능 기술과 영어학습 ,https://blog.naver.com/etripr/222947884615,20221206,"안녕하세요, 에뚜리앙 서포터즈 김초원 활동자입니다.여러분, 음성인식이라고 들어보셨나요? ​음성인식의 대표적 예로는 인공지능비서, 스피커, 핸드폰 음성인식 기능, 자동차 내비게이션 음성인식 등이 있습니다.​ ​이러한 음성인식 및 음성합성 기술은 어떻게 이루어질까요혹시 음성 스펙트럼이 보이시나요?이 음소, 음절이 모여 생긴 음성 파형이 되어 들리는 것이 음성언어입니다.   음성인식(speech recognition)은 사람이 말하는 음성언어를 컴퓨터가 해석하여 문자 데이터로 전환하는 것을 말합니다. 그래서 speech-to-text를 줄여서 STT라고 부릅니다.     음성합성(speech synthesis)은 말소리의 음파를 기계가 자동으로 만들어 내는 기술로 text-to-speech를 줄임말로 TTS라고 부릅니다.    과거 고전적인 음성 인식방식은 음성의 가장 작은 단위의 음소를 찾아내고 이를 바탕으로 단어를 추출하고 문장을 찾아내는데, 한계가 있어 요즘에는 딥러닝 기반의 입력에서 출력까지 파이프라인 네트워크 없이 한 번에 처리하는 End-to-End 방식(종단 간 기계학습)을 사용하는 게 성능이 가장 좋습니다.​   이러한 인공지능 중에 음성지능기술을 기반으로 만들어진 앱을 통해 집에서 편리하게 영어 공부를 할 수 있는 방법이 있는데요. 바로 한국교육방송공사(EBS)와 한국전자통신연구원(ETRI)이 협력 개발하여 만든 ""펭톡AI""입니다. 초등학교 3~6학년 학생들이 활용할 수 있는 이 대화형 원어 교육 시스템을 통하여 영어로 대화하고 평가받으면서 스스로 영어를 학습할 수 있도록 돕는 프로그램입니다. AI펭톡은 한국전자통신연구원이 1990년대부터 개발해온 음성인식과 자연어 대화처리 기술의 집약체로 2010년 기술을 완성한 뒤에 기술이전을 통해 이미 곳곳에서 활용하고 있습니다.   ​특히, 전국 6,000여 개의 초등학교에 투입되고 있으며, 이렇게 공교육에 대규모로 AI를 활용한 외국어 교육을 도입하는 것은 전 세계적으로 처음이며, 현재 AI펭톡은 시나리오 없이 주제를 던져도 사람에 가깝게 자연스러운 대화를 이어갈 수 있는 게 특징입니다.   ​더불어, 앞으로 음성인식과 음성합성의 연구가 나아갈 방향은 궁극적으로 사람처럼 자연스러운 대화가 가능한 기계를 만드는 것이라고 합니다. 인공지능의 딥러닝 기술을 기반으로 해온 것을 기존 ASR에서 E2E방식으로 많은 변화가 있었는데요. 이러한 알고리즘과 기술적인 문제와 더불어 음성학,뇌공학 분야도 함께 협업으로 연구를 지속하면서 더욱 정교한 대화 구사가 가능한 프로그램을 만든 것이 핵심이라고 생각합니다. 앞으로 이러한 성능적인 부분의 발전도 지속되어야 하지만 더 먼 미래를 바라보았을 때는 증강현실(VR)같은 인터페이스와 연동하고 응용하여 실생활에서 더욱 친근한 기술이 되길 바랍니다.     이제 음성지능에 대해 기본적인 내용을 이해하시겠다면, ETRI의 따끈따끈한 음성 인공지능 기술 개발 소식에 대해서 전해드리겠습니다.  24개 언어를 이해하는 음성인식 기술을 개발했다고 하는데요. 이는 글로벌 업체와 대등한 최다언어, 최고성능의 음성인식으로 30개 언어를 연내 확보했으며 희소 언어 확장에 걸림돌을 해결할 것이라는 소식이 있습니다.      특히, 기존에 흔히 활용되던 종단형(End-to-End) 음성인식 기술의 단점을 개선해 활용성을 높였다고 하는데요. 느린 응답속도 문제는 스트리밍 추론 기술을 개발, 실시간 처리가 가능하도록 개선하고 아울러, 의료와 법률, 과학기술 등 특정한 도메인에 대한 음성인식 특화가 쉽도록 하이브리드 종단형 인식 기술도 개발하여 적용했다고 합니다. 더 자세한 내용을 알고 싶으시다면 바로 아래 링크를 참조해주시면 감사합니다.  참조: https://blog.naver.com/etripr/222924447520https://www.dongascience.com/news.php?idx=45383​​​​ ​​ ​​ "
[ASR]SpecAugment: A New Data Augmentation Method for Automatic Speech Recognition ,https://blog.naver.com/vesmir/221904266675,20200412,"https://ai.googleblog.com/2019/04/specaugment-new-data-augmentation.html SpecAugment: A New Data Augmentation Method for Automatic Speech RecognitionPosted by Daniel S. Park, AI Resident and William Chan, Research Scientist     Automatic Speech Recognition (ASR), the process of taking an ...ai.googleblog.com ​ "
[논문리뷰]Deepfake Speech Detection Through Emotion Recognition: A Semantic Approach - Introduction ,https://blog.naver.com/ssj860520/222864749545,20220902,딥페이크에 대해서는 이미 알고 있을 것이다.당사자에게 동의 받지 않는 딥페이크는 사회적인 문제를 야기할 수 있다.딥페이크인지 아닌지 감지하는 것은 딥페이크로 인한 사회적 문제가 퍼지는 것을 어느정도 방지할 수 잇다.딥페이크를 감지하려면 어떻게 해야 할까?이 논문의 필자는 딥페이크에는 감정적인 요소가 실제보다 덜 나타날 것이라고 가정을 한다.그래서 감정인식 측면에서 딥페이크인지 아닌지를 판단하는 방법을 제시한다.더 자세한 내용은 아래의 글을 보면 알 수 있을 것이다.​ [논문리뷰]Deepfake Speech Detection Through Emotion Recognition: A Semantic Approach - Introduction최근에 ICASSP 2022 paper 리스트를 보다가 제목이 재밌어 보이는 논문이 있었다. 제목은 Deepfake Speech Detection Through Emotion Recognition: A Semantic Approach 이다. Speech Emotion Recognition 을 이..qwertyuioop.tistory.com ​ 
[근황] SSCI 2편 출간! 새 논문 + 리비전! ,https://blog.naver.com/gmliving/223094565320,20230506,"이번주에 영혼의 단짝 재호쌤과 함께 쓴 두개의 SSCI 논문이 출간되었다. ​하나는 Interactive Learning Environments라는 교육공학쪽의 상위저널 (Q1)에 출간된 A systematic review of research on speech-recognition chatbots for language learning: Implications for future directions in the era of large language models 으로 음성인식 챗봇에 대한 리뷰 논문이 되겠다. 최근 각광을 받는 ChatGPT까지 아우르는 리뷰를 시도했다는 점에서 의미가 있겠다.​ 다른 하나는 ChatGPT에 대한 연구로 EAIT (SSCI, Q1)에 출간 되었다. 교사와 ChatGPT간의 상호작용에 대한 연구로 초등 교사들에게 실제로 GPT를 사용하게 한 이후 데이터와 인식을 보고 하였다. ​​​ A systematic review of research on speech-recognition chatbots for language learning: Implications for future directions in the era of large language modelsChatbot research has received growing attention due to the rapid diversification of chatbot technology, as demonstrated by the emergence of large language models (LLMs) and their integration with a...doi.org   Large language models in education: A focus on the complementary relationship between human teachers and ChatGPT - Education and Information TechnologiesArtificial Intelligence (AI) is developing in a manner that blurs the boundaries between specific areas of application and expands its capability to be used in a wide range of applications. The public release of ChatGPT, a generative AI chatbot powered by a large language model (LLM), represents a s...doi.org ​4월 말에는 오랫동안 준비해 오던 새 논문을 투고해서 교공의 탑 저널인 컴앤에듀 리뷰에 들어갔다. 리뷰 들어가자 마자 두명이 리뷰 수락을 했는데... 어떻게 전개될지 심장이 쫄깃해 진다.​그리고 지금은 리비전 하나 마무리 단계에 있다. 정신이 하나도 없이 시간을 보내고 있는데 잘 마무리 해야 겠다. 오늘까지 마무리 짓고 투고하는 걸로?​이후엔  전주대 활용한 교수님 그리고 재호쌤과 함께 하는 후속 논문 마무리 하나 하고 새 논문에 다시 들어가야 한다. 이어지는 논문 작업들로 많이 바쁘지만 퀄리티 저하 없이 하나하나 최선을 다해야 겠다. "
자연 대화 인터페이스 (Natural Speech Interface) ,https://blog.naver.com/towards-ai/222166360437,20201208,"이번에는 자연 대화 인터페이스에 대해 설명드리려고 합니다. 현재 음성 신호처리 기술은 기계학습과 딥러닝 기술의 적용으로 많이 향상되었고 주변 소음 제거 및 감정에 따른 음성 합성 기술들의 진보로 음성 합성 목소리가 인간의 목소리와 거의 유사해졌습니다.​ 자연 음성 인터페이스 (Natural Speech Interface)자연 음성 인터페이스 (Natural Speech Interface)은 아래와 같습니다. 1. Wakeup-word recognition (Speech detection, Pattern matching) ex) “Alexa”, “Ok, Google”2. Preprocessing (Noise reduction, dereverberation)3. Speech Recognition (음향모델, 언어모델)4. Natural Language Understanding (Parsing, Translating, Information retrieval)5. Text-to-speech (Text normalization, Statistical training, Waveform synthesis)​자연 음성 인터페이스 중에 Preprocessing인 음성향상 기법 (Speech Enhancement), 음원분리와 Text-to-speech 음성합성 기법에 대해 중점적으로 다루고자 합니다.​ 음성 향상 기법(Speech Enhancement): 음성에 포함된 주변 소음 및 잔향 제거를 통한 음성 신호 복원 기술특징기존음성신호와 잡음신호 사이의 통계적 정보 이용하는 통계모델 기반 음성향상기법 주로 사용현재심화신경망으로 음성 신호와 잡음이 존재하지 않는 깨끗한 음성 신호 사이의 비선형적인 관계를 모델링 ​ 음원 분리(Source Separation)혼합된 음성 신호를 개별 음성으로 분리 -> 음성과 이미지 정보를 이용해 음성 향상​ 음성 합성 (Speech Synthesis) 기법: 입력된 텍스트를 인간의 목소리로 변환- 종단간 접근방법: 입력부터 음성 출력까지 하나의 모듈로 구성 * Encoder: 자동으로 입력된 텍스트에 대한 특징(텍스트 정보를 잘 나타내는 숫자)들을 뽑아냄  * Attention Module: self-attention, multi-head attention * Decoder: attention 벡터들과 이전에 생성된 파라미터들을 이용하여 필요한 파라미터들을 생성 * Neural Vocoder: 음성 특징들로부터 음성을 만들어냄, 음성 샘플들 간의 순차적 특징을        이용하는 자기회귀(Autoregressive) 모델 ex) Wavenet, WaveGlow 등​- 감정과 개성을 표현하는 음성 합성​ 향후 발전방향음성인식 기술은 딥러닝 기술을 통해 모델 성능이 보다 향상 -> 음성에 대해 많은 데이터를 수집하기 어렵기 때문에 앞으로는 적은 데이터로도 학습할 수 있도록 나아가야 함 (Self-supervised learning 이용 필요) * 여기서 Self-supervised learning은 데이터 자체에서 스스로 레이블을 생성하여 학습에 이용하는 방법으로 추후에 자세히 다루도록 하겠습니다.​​[References]​- 2020 인공지능학회 추계학술대회 Natural Speech Interface - Pre-processing and synthesis perspective by Hong-Goo Kang​ "
컴퓨터언어학 독학하기 Speech and Language Processing Chapter 2 ,https://blog.naver.com/yeye0626/222678189415,20220320,"  1.     Introduction2.     Regular Expressions, Text Normalization, Edit Distance # IntroA.      Simple pattern-based method-       Ex. ELIZA, chatbots -       Pattern matching: ‘I need X’ -> ‘What would it mean to you if you got X?’-       Doesn’t actually need to know anything-       Imitation of the responses of actual person-       실제로 대화를 이해하지 않고 단순히 output 배출-       Simple “diversion”​B.      Modern ‘more sophisticated understanding’ requiring method -       User’s intent를 이해해야 답할 수 있는 기능들 (answering questions, booking flights, finding restaurants)-       이때 사용되는 가장 중요한 도구: the regular expression: used to specify strings which make extraction from a document possiblestrings=such as tables of prices​C.      Text normalization -       Converting text to a more convenient, standard form -       Specific methods①     Tokenization(문장 성분 분석)②     Lemmatization(단어를 기본형으로 바꾸기)③     Stemming(어간 분리)④     Sentence segmentation(마침표, 느낌표 등을 이용해서 text문장 분리)⑤     Edit distance (compare words and other strings): measures how similar two strings are based on the number of edits                                         edit: insertions, deletions, substitutions   2.1     Regular Expressions: a language for specifying text search strings: an algebraic notation for characterizing a set of strings-> corpus에서 pattern에 일치하는 모든 texts를 return하기 위해 사용됨 2.1.1        Basic Regular Expression Patterns-       가장 간단한 RE는 a sequence of simple characters-       RE는 case sensitive하다. 즉 소문자와 대문자를 구분한다 (woodchuck=/=Woodchuck)①     The square braces [ ]: specify a disjunction of characters [ ]안에 있는 것 중 아무거나 와도 됨ex. /[wW]oodchuck/ - Woodchuck or woodchuck, /[abc]/ - ‘a’, ‘b’, or ‘c’​②     The dash -: specify the range 범위를 표현 #부터 #까지ex. /[A-Z]/ - ABC…Z​③     The caret ^: specify what a single character cannot be (negation)단, ^가 [ ] 안에서 맨 앞에 오는 경우에만 부정의 뜻으로 쓰임, 그냥 뒤에 오면 ‘^’ 의미, 그리고 [ ] 없이 맨 앞에 오는 경우는 Start of line 의미함 ex. /[^Ss]/ - S와 s둘다 아님​④     The question mark ?: means the preceding character or nothingex. /woodchucks?/ - woodchunk or woodchucks   /colou?r/ = color or colour​⑤     The Kleene *: means zero or more occurrences of the immediately previous character or regular expression ex. /aa*/ - a, aa, aaa, …   /[ab]*/ - aaaa, bbbb, abab, …​⑥     The Kleene +: means one or more occurrences of the immediately preceding character or regular expressionex. /a+/ - a, aa, aaa, ….​⑦     The period .: a wildcard expression that matches any single characterex. /beg.n/ - begin, begun, beg’n, ….-> /a.+b/ : a와 b 사이에 any string of character ​⑧     Anchors : particular places in a string을 나타내는 표현(1)    The caret ^ : start of a line           ex) /^The/(2)    The dollar sign $ : end of a line       ex) /The dog\.$/(3)    \b : word boundary                     ex) /\b99\b/ : 99 , $99 , 299(4)    \B : non-word boundary 2.1.2        Disjunction, Grouping, and Precedence-       Disjunction operator : the pipe symbol |: either A or B ex. /cat|dog/ - either cat or dog​-       Grouping operator: the parenthesis operators ( & ): make the several characters into a single character ex. /gupp(y|ies)/ - either guppy of guppies-       Operator precedence hierarchy- Parenthesis                        ( )- Counters                          * + ? { }- Sequences and anchors          the ^my end$- Disjunction                        |-       Greedy (default): regular expressions always match the largest string they cani.e., expand to cover as much as a string as they can-       Non-greedy (we can enforce it): matches as little text as possible -> the operator *? & the operator +?​ 2.1.3        A simple Example- false positives : strings that we incorrectly matched- false negatives : strings that we incorrectly missed increasing precision = minimizing false positives   increasing recall = minimizing false negatives2.1.4        More Operators ​  2.1.5        A More Complex Example2.1.6        Substitution, Capture Groups, and ELIZA1)     Substitution: allow a string characterized by a RE to be replaced by another string​2)     Capture Groups: replacing string with whatever string matched the item in parentheses the resulting match is stored in a numbered register첫번째 : \1, 두번째 : \2 …etc.Ex. /the (.*)er they (.*), the \1er we \2/​# parentheses의 두가지 사용법1)     Grouping for specifying the order in which operators should apply2)     Capturing something in a register​# non-capturing group: grouping only, not registering  ?:을 삽입한다Ex. /(?:some/a few) (people|cats) like some\1/ 2.1.7        Lookahead Assertions: 해당 조건을 만족하는 하는 것만 return- (?= pattern) : true if pattern occurs   -> zero-width (match pointer doesn’t advance)- (?! pattern) : true if pattern does not match (=negation lookahead)  -> zero-width, too 2.2     Words-       Corpus (plural corpora) : a computer-readable collection of text or speech-       상황에 따라 punctuation도 word으로 count할수도, 안할수도 있음​-       Utterance(발화) : the spoken correlate of a sentencetwo kinds of disfluencies of utterance*fragment : 말 더듬기 ex. main- mainly*fillers or filled pauses : 말 멈추고 추임새 넣기 ex. um, uh, etc.​-       Lemma : a set of lexical forms having the same stem (major part-of-speech, word sense)-       Wordform : the full inflected or derived form of the word​-       Two ways of talking about words*types: the number of distinct words in a corpus(겹치는 단어는 한번만 세기) -> set of words in the vocabulary = V -> the number of types = vocabulary size = |V|*tokens: the total number N of running words​-       Herdan’s Law or Heap’s Law: the relationship between the number of types |V| and number of tokens N ​-       Vocabulary size for a text goes up significantly faster than the square root of its length in words-       Dictionary entries or boldface forms are a very rough upper bound on lemmas(since some lemmas have multiple boldface forms) 2.3     Corpora-       단어들은 여러 요인에 의해 다양하게 발현된다-> language, genre, demographic characters of writer/speaker, time-       따라서 who produced the language, in what context, for what reason를 알아야 함-       Corpus creator는 datasheet or data statement를 제작해야 함 2.4     Text Normalizationnormalization process에 있어서 필요한 세가지 tasks: Tokening(segmenting) words, normalizing word formats, segmenting sentences 2.4.1        Unix Tools for Crude Tokenization and Normalization: 가장 쉽고 간단한 것부터 해보자-       tr : systematically change particular characters in the inputtr -c : complements to non-alphabet (여집합, 즉 알파벳 아닌 것은 제외)tr -s : squeezes all sequences into a single character   -> tr -sc : 둘다 한번에 적용 가능​-       sort : sorts input lines in alphabetical ordersort -n : sort numerically rather than alphabeticallysort -r : sort in reverse order (highest-to-lowest)​-       uniq : collapse and count adjacent identical linesuniq -c : complements to non-alphabet (예영이 뇌피셜,, 책에 애매하게 나와있슴)​ 2.4.2        Word Tokenization: the task of segmenting running text into words-       주의해야 할 것들: comma, period, clitic(ex. don’t, I’m), multiword expression(ex. New York)-       Named entity recognition: the task of detecting names, dates, and organizations-       Commonly used tokenization standard: Penn Treebank tokenization: separates out clitics, keeps hyphenated words together, separates out all punctuation 2.4.3        Byte-Pair Encoding for Tokenization: train이 되지 않은 unknown word를 처리하기 용이함-       빈도수가 높은 character의 나열부터 automatically subword를 생성함subword: tokens smaller than words​-       Two parts of tokenization schemes# token learner : takes a raw training corpus and induces a vocabulary# token segmenter : takes a raw test sentence and segments it into the tokens in the vocabulary-       참고 사이트) https://wikidocs.net/22592 2.4.4        Word Normalization, Lemmatization and Stemming-       Word normalization: the task of putting words/tokens in a standard format, choosing a sungle normal form for words with multiple forms​-       Case folding: mapping everything to lower case (ex. Woodchuck = Woodchucks 동일시)-> information retrieval & speech recognition에는 유리,   text classification tasks, information extraction, machine translation에는 불리​-       Lemmatization: the task of determining that two words have the same root, despite their surface differencesl  Stems : the central morpheme of the word, supplying the main meaningl  Affixes: adding ‘additional’ meaning ​-       Stemming: naïve version of morphological analysis that mainly cutting of word-final affixes대표적 stemming algorithm: The Porter Stemmer 2.4.5        Sentence Segmentation: punctuation을 기준으로 문장 구분, 마침표는 조금 까다롭긴 하지만 다른 용도로 사용되는 마침표를 dictionary에 저장해놓는 방식으로 해결 2.5     Minimum Edit Distance-       NLP에서 strings의 유사한 정도를 측정하는 것이 중요한 과제임-       Ex. spelling correction, coreference-       Edit distance: a way to quantify both of these intuitions about string similarity​-       Minimum edit distance: minimum number of editing operations needed to transform one string into another -       Alignment : a correspondence between substrings of the two sequences-       Operation : deletion, substitution, insertion-       The Levenshtein distance : 모든 operation을 1점으로 계산   2.5.1        The Minimum Edit Distance Algorithm-       Dynamic programming 동적 프로그래밍: solve problems by combining solutions to sub-problemsex. Viterbi algorithm, CKY algorithm for parsing​-       Minimum edit distance algorithm-       Minimum cost alignment : visualize an alignment as a path through the edit distance matrix-       Viterbi algorithm: probabilistic extension of minimum edit distance : maximum probability alignment 2.6     Summary·        The regular expression language is a powerful tool for pattern-matching.·        Basic operations in regular expressions include concatenation of symbols, disjunction of symbols ( [ ], |, and .), counters (*, +, and {n,m}), anchors (ˆ, $) and precedence operators ((,)).·        Word tokenization and normalization are generally done by cascades of simple regular expression substitutions or finite automata.·        The Porter algorithm is a simple and efficient way to do stemming, stripping off affixes. It does not have high accuracy but may be useful for some tasks.·        The minimum edit distance between two strings is the minimum number of operations it takes to edit one into the other. Minimum edit distance can be computed by dynamic programming, which also results in an alignment of the two strings. "
"Intelligent Speech and Voice Recognition, ff ",https://blog.naver.com/lliadodyssey/222815062286,20220717,"Google, Baidu, iFLYTEK, Facebook, Amazon, Apple Inc, IBM, Microsoft, Brianasoft, Neurotechnology, Sensory Inc., VoiceBase, Auraya, LumenVox, Nuance Communications, Raytheon BBN Technologies "
Web Speech API  ,https://blog.naver.com/tebahsoft/222787409870,20220624,"출처 : https://www.section.io/engineering-education/speech-recognition-in-javascript/ Speech Recognition Using the Web Speech API in JavaScriptThis tutorial will give readers a detailed guide on how to build a webpage that implements speech recognition using the Web Speech API in JavaScript.www.section.io webkitSpeechRecogniton이 있는지 점검 if (""webkitSpeechRecognition"" in window) {  // Speech Recognition Stuff goes here} else {  console.log(""Speech Recognition Not Available"")} webkitSpeechRecogniton 옵젝트 생성 let speechRecognition = new webkitSpeechRecognition(); 한 단어 또는 구문만 인식할 때  speechRecognition.continuous = true; 최종 결과 이전에 중간 결과를 받아 볼 때 speechRecognition.interimResults = true; 언어 선택 speechRecognition.lang = document.querySelector(""#select_dialect"").value; 이벤트onStart, onEnd, OnError, onResult​onResult- 인식 결과가 있을 때 이벤트 발생​final transcript 를 outside에, interimtranscript를 inside에 선언 let final_transcript = """";speechRecognition.onresult = (event) => {  // Create the interim transcript string locally because we don't want it to persist like final transcript  let interim_transcript = """";}; result array에 대해 loop로 처리하면서 final 이면 final script에 추가하고 아니면 interim에 추가한다. for (let i = event.resultIndex; i < event.results.length; ++i) {  // If the result item is Final, add it to Final Transcript, Else add it to Interim transcript  if (event.results[i].isFinal) {    final_transcript += event.results[i][0].transcript;  } else {    interim_transcript += event.results[i][0].transcript;  }} ​ "
데이터 사이언스 면접 질문 너무 잘 대답하는 Chat GPT ,https://blog.naver.com/data-scientist-luna/223009904373,20230209,"내가 받았던 데이터 사이언스 면접 질문들 중 답이 확실히 정해져있는 것들을 위주로 챗 지피티한테 물어보고 있는데 대답을 정말 참 잘한다. 확실히 답이 정해져있는 문제에 대해선, 나보다 영어도 잘 하고 나보다 아는 것도 많아서 데이터 사이언티스트 간단한 테크니컬 면접은 그냥 패스할 것 같다. 답이 정해져있지 않는 생각과 경험을 요구하는 문제들도 곧 테스트 해 볼 예정이다! ​예전에는 이거 질문 하나 대답하려고 구글링해서 이런 저런 데서 공부해가지고 답을 내가 만들었는데, 이제 그럴 필요 없이 Chat GPT가 대답을 알아서 잘 짜주기 때문에 데이터 사이언스 면접 준비 때 이용하면 아주 유용할 것으로 보인다!  Q. How do you determine when to use tree based vs. linear model vs neural network in machine learning?리니어 모델을 쓸 지, 트리 베이스드 모델을 쓸 지, 뉴로 넷을 쓸지 어떻게 결정하나요? The choice between using tree-based models, linear models, or neural networks in machine learning depends on various factors, including the nature of the problem you are trying to solve, the characteristics of the data, and the resources available for training and deploying the model. Here are some general guidelines for when each of these types of models might be appropriate:​Tree-based models: Tree-based models are often used for problems involving non-linear relationships between the features and the target variable. They are also effective for handling missing values and categorical variables and can be used for both regression and classification problems. Tree-based models can handle high-dimensional data, making them a good choice for problems with many features.​Linear models: Linear models are most appropriate for problems where the relationship between the features and the target variable can be approximated by a straight line. They are fast to train and interpretable, which makes them a good choice for problems where understanding the relationship between the features and target is important. Linear models are also a good choice for problems with low-dimensional data and for problems where computational resources are limited.​Neural networks: Neural networks are often used for problems involving complex non-linear relationships between the features and the target variable. They are particularly effective for image and speech recognition problems, but can also be used for a wide range of other problems. Neural networks have the ability to learn hierarchical representations of the data, making them a good choice for problems with large amounts of high-dimensional data. However, they can be computationally intensive to train, making them less appropriate for problems with limited computational resources.​Ultimately, the best way to determine the most appropriate model for your problem is to try different models and compare their performance using appropriate evaluation metrics. It's also important to consider the available resources for training and deploying the model and the needs of your application in terms of interpretability, speed, and accuracy.​ ​ "
플러그인으로 음성인식하기! Google Speech To Text Plugin Unreal Test  ,https://blog.naver.com/peli_/222563907661,20211110,"플러그인 다운로드 깃허브(무료) : Vakarian15/UE4-Google-Speech-to-Text-Plugin: UE4 Google Speech Recognition Plugin (github.com)​플러그인 다운로드 에픽게임즈(유료) : Google Speech Kit in Code Plugins - UE Marketplace (unrealengine.com)​구글 클라우드 가입 후 구글 API키 생성해야 한다. 음성을 입력받아 구글 딥러닝 서버에 보낸 후 리턴값을 받아오는 형식이라 딜레이는 좀 있다.  ​현재는 Hello, Hi만 알아듣고, 못 알아듣는 음성을 인풋시킬 경우 튕겨버린다.몽총하게 동작하도록 작업한 나 반성해라 ㅠ​그래도 아예 동작조차 안 되던 사흘 전보다 훨씬 많이 왔다!!!! "
Wav2vec 2.0 : A Framework for Self-Supervised Learning of Speech Representations Review ,https://blog.naver.com/sooftware/222095559750,20200921,"Wav2vec 2.0 : A Framework for Self-Supervised Learning of Speech Representations​Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, Michael AuliFacebook AI Research (FAIR)arXiv (2020.06)​References​●  arXiv●  source code●  BERT●  Wav2vec●  VQ-Wav2vec●  speech book​Summary​●  BERT in Speech Recognition●  Representation learning with 53,000 hours of unlabeled speech data.●  Excellent speech recognition performance with only 40 sentences (10 minutes) of labeled data​Abstract​●  음성 데이터 자체만으로 학습한 후 Fine-tuning하는 간단한 방법으로 semi-supervised 방법보다     좋은 성능을 냄●  TIMIT, LibriSpeech 100h 데이터셋에서 State-Of-The-Art (SOTA) 를 달성●  LibriSpeech 100h은 단 1시간의 데이터만으로 기존 SOTA보다 높은 성능을 보임●  단 10분의 데이터 (40문장) 으로 LibriSpeech clean 5.7 / noisy 10.1 Word Error Rate 기록●  LibriSpeech의 전체 학습셋을 사용했을 때 (960h) clean 1.9 / noisy 3.5 WER을 기록 ​Wav2vec​●  현재 어느 정도 수준 이상의 음성인식기를 만들기 위해서는 대량의 데이터가 필요함 (수천 시간)●  세상에는 7,000개 이상의 언어가 존재하는데 모든 언어에 대해 이 정도 수준의 데이터를     구축하기는 어려움●  영아 (infant) 들은 단순히듣는 것만으로 음성에 대해 학습함​Wav2vec 1.0 (previous work) ●  wav2vec은 크게 encoder network f와 context network g 두개의 파트로 구성     (둘 모두 convolution neural network)●  wav2vec은 해당 입력이 Positive인지 Negative인지 이진 분류    (Binary Classification)하는 과정에서 학습●  Positive : (Ci, Zi+1), Negative : otherwise●  Negative 쌍은 입력 음성의 i번째 context representation Ci와     다른 음성의 hidden representation들 중 랜덤 추출●  즉, 2개의 네트워크는 입력 음성의 다음 시퀀스가 무엇일지에 관한 정보를     음성 피처에 녹여내도록 학습​VQ-Wav2vec (previous work) ●  Wav2vec 아키텍처 중간에 Vector Quantization 모듈을 추가한 구조●  VQ 모듈 : continuous representation Z를 discrete representation Z^로 변환●  Discretization(이산화)는 discrete한 input을 필요로하는 NLP 알고리즘들을     바로 적용할 수 있다는 장점이 있음​Vector Quantization ●  Z를 선형변환하여 logit을 만듦●  여기에 Gumbel Softmax와 argmax를 취해 one-hot vector를 만듦●  이후 Embedding matrix를 내적해 Z^를 만듦​Wav2vec 2.0 ●  VQ-wav2vec의 모델을 Transformer로 대체●  Pre-training 이후 labeled data로 fine-tuning (Connectionist Temporal Classification     (CTC) loss 사용)●  이전 연구들과의 차이점으로 Filter-Bank와 같은 피쳐추출 과정이 없음​Model​●  VQ-Wav2vec와 비교하여 Transformer를 사용했다는 특징이 있음​Feature Encoder:: N x [Conv1d, Dropout, GroupNorm, GELU]​Transformer:: Positional Encoding을 conv1d로 대체:: Convoulation Layer 뒷단에 layer normalization 적용​Quantization module:: Vector Quantization 부분 참고​Training​BERT의 masked language modeling (MLM)과 유사하게 latent speech의 일부분을 masking하며, 모델은 quantized latent audio representation을 맞추는 방식으로 트레이닝이 진행됨. 이렇게 학습한 후 labeled 된 데이터로 fine-tuning 진행.​Masking​1. 전체 오디오 구간 중 6.5%를 랜덤하게 선택 2. 선택된 구간부터 10 time-step만큼 masking (masking은 중복될 수 있음) ​전체 오디오 구간 중 약 49% 정도가 masking (평균 14.7 timestep (299ms))​Objective Lm : Contrastive Loss, Ld : Diversity Loss, Lf : L2 penalty, {alpha, beta} : hyperparameter​Contrastive Loss Diversity Loss ​Fine-tuning​이렇게 Pre-train 된 모델을 ASR 태스크로 Fine-tuning (+ projection layer)29 character tokenCTC LossSpecAugment​Experiment​Datasets​Unlabeled data 1 : LibriVox-60k (전처리하여 53.2k 사용) ReferenceUnlabeled data 2 : LibriSpeech 960htrain-10min, train-1h, train-10h, train-100h, train-960h 설정 (LibriSpeech)​Result​WER on the Librispeech dev/test sets when training on the Libri-light low-resource labeled data setups of 10 min, 1 hour, 10 hours and the clean 100h subset of Librispeech WER on Librispeech when using all labeled data of 960 hours Conclusion​적은 비용으로도 좋은 성능의 음성인식기를 만들 수 있는 연구 방향을 제시함Seq2seq 구조 혹은 word-piece 단위로의 변경을 통해 성능 향상 기대 "
AI - Speech (1) Application 1 ,https://blog.naver.com/sooftware/221915928690,20200419,"「AI - Speech」 (1) Application 1​본 글은 음성신호처리 연구실의 박호종 교수님의 강의를 듣고 작성되었음을 밝힙니다.관련 글 목록은 이곳에서 보실 수 있습니다.​음성/오디오/sound 특성​● 방송 / 통신 / 엔터테인먼트의 핵심 기술● 학계/산업계의 전문 엔지니어 부족● Art, 취미 활동과 관련된 기술● 음성에서 언어의 종속성   음성 이해, 음성 신호의 품질 및 명료도 평가에서 중요한 요인● 음성은 대표적인 생체신호   음성 기반 헬스케어, 장애인을 위한 복지 기술● Data 양 : V >> A (4차원 : 2차원)● 심리적 민감도 : V < A   특정 sound에 대한 거부감 존재   심리 변화에 큰 영향 : 공포 영화에서 분위기 조정● 요구 품질 : V < A   일반적으로 낮은 화질은 허용되지만 낮은 음질은 허용되지 않음● 멀티미디어에서의 독립성 : V < A​​Applications - Speech Synthesis​기계를 이용하여 언어 정보를 가지는 음성 신호를 생성 : Text-To-Speech (TTS)  음정을 맞추기는 쉽지만, 발음을 정확히 맞추기가 어렵다.●  Before 2010 : digital waveform 연결, boundary smoothing  음절 단위로 미리 녹음해놓고 파형을 저장해놓는다.Text를 보고 미리 녹음해놓은 파형을 적절하게 이어붙인다.=> 부자연스러움​이러한 부자연스러움을 해결하기 위해 단어 단위로 녹음하는 것이 더 자연스러웠음하지만 자연스럽게 하는 과정이 굉장히 어렵다.​Example) ﻿고기소고기돼지고기불고기물고기=> 물꼬기라고 발음이 됨 ※ 문제가 됨 ※ ​● After 2010 : AI-based waveform generation  현재 AI를 이용하여 상용화가 가능할 정도의 음질이 나오게 됨● 부모가 들려주는 동화책● 죽은 사람의 목소리 재현 : 신체 구조로부터 음색 추정​​Applications - Music Synthesis​전자장치를 이용하여 music signal 합성 (쉽게 말하면 전자 키보드)​​Applications - Sound for Game and Animation​게임이나 애니메이션 사운드를 직접 만드는 기술(물건이 떨어지는 소리, 발자국 소리 등..)​● 기존에는 미리 녹음된 waveform을 상황에 맞추어 출력   => 단순한 sound 반복에 의한 피로감● Sound 합성 엔진 이용   물체 특성과 움직임에 따라 수학적으로 sound를 합성​​Applications - Speech Recognition​기계가 음성 신호에 포함되어 있는 언어정보를 인식  ● Human-machine interface의 핵심 기술   휴대전화에서 mic 입력으로 문자 및 명령 입력   장애인, 특수 환경에서의 기기 동작   지능형 로봇● 언어에 대한 지식 필요● 감정 인식에 대한 연구 진행● 문제점   기술적 한계 : 사용자는 매우 높은 인식률 요구   사용에 대한 거부감 (의외로 불편)   더 편리한 다른 방법이 있으면 사용하지 않음   => 말보다 키보드 or 마우스 입력이 편하다● 자연어 (Natural Language) 인식   휴대폰에서 많이 사용하는 이유는, 조그마한 휴대폰에 타이핑하기가 힘들기 때문이다.   음성인식은 항상 첫번째 옵션이 아닌, 두번째 세번째 옵션인 것을 이해해야 한다.​​AI Assistant​아마존의 에코, SK의 누구, KT의 기가지니 등 최근 많이 볼 수 있는 제품생각보다 불편한 탓에 아직 활용가치가 높지 않다.​● 문제점 1   아마존 인형의집 주문 사건   TV에서 나온 소리를 인식해서, 미국 전역에 장난감을 주문한 사건● 문제점 2   화자 인식  => 여기서 ""내""가 누군지 모른다.​● Privacy Issue 항상 Sound를 수집하고 있다?    수집은 하지만 폐기한다?    일상 대화    인간 인식 Camera (CCTV)에 비하여 위험성은 인지 못함​​자동차에서의 음성 인식​자동차 내에서는 ""복잡한 입력보다는 음성으로 입력을 하자""라는 논리가 성립이 됨But! 자동차라는 이유로 생기는 문제점이 있음​● 다양한 잡음 (자동차 잡음, 라디오 소리, 대화 소리 등 ...)● 원거리 마이크● 버튼보다 불편함​​Very Efficient Speech Communication  ● 파형 전송 없이 텍스트만 전송하므로 정보량이 매우 적음● 자연스러운 통신을 위하여 감정과 Speaker 특성 전송  여기에 기계 번역까지 더해진다면, 어느 언어와도 편리한 통신이 가능함아직은 Ideal한 얘기지만, 이렇게 될 것이다. "
"자바스크립트 ""Web Speech API"" ",https://blog.naver.com/webhackyo/223027838559,20230226,"Web Speech API는 웹 개발자가 음성 인식 및 합성 기능을 애플리케이션에 통합할 수 있도록 하는 JavaScript API입니다. 이 API는 음성 인식 기능을 제공하는 SpeechRecognition 인터페이스와 음성 합성 기능을 제공하는 SpeechSynthesis 인터페이스의 두 가지 구성 요소로 구성됩니다.​음성 인식 인터페이스:SpeechRecognition 인터페이스는 음성 언어를 인식하고 텍스트로 전사하는 데 사용됩니다. 다음은 SpeechRecognition 인터페이스를 사용하여 JavaScript에서 음성을 인식하는 방법의 예입니다.​// Create a new SpeechRecognition objectconst recognition = new SpeechRecognition();​// Start listening to user's voice inputrecognition.start();​// Add an event listener to handle the recognition resultrecognition.addEventListener('result', (event) => {  const transcript = event.results[0][0].transcript;  console.log(`You said: ${transcript}`);});​이 예제에서는 새 SpeechRecognition 개체를 만들고 start() 메서드를 호출하여 사용자의 음성 입력 듣기를 시작합니다. 또한 인식 결과를 처리하기 위해 이벤트 리스너를 추가하여 이벤트 객체에서 기록된 텍스트를 검색하고 콘솔에 기록합니다.​음성 합성 인터페이스:SpeechSynthesis 인터페이스는 텍스트에서 음성을 생성하는 데 사용됩니다. 다음은 SpeechSynthesis 인터페이스를 사용하여 JavaScript에서 음성을 생성하는 방법의 예입니다.​// Create a new SpeechSynthesis objectconst synth = window.speechSynthesis;​// Create a new SpeechSynthesisUtterance objectconst utterance = new SpeechSynthesisUtterance('Hello, world!');​// Set the voice for the utteranceutterance.voice = synth.getVoices()[0];​// Speak the utterancesynth.speak(utterance);​이 예제에서는 새 SpeechSynthesis 개체와 말할 텍스트가 포함된 새 SpeechSynthesisUtterance 개체를 만듭니다. 그런 다음 발화에 대한 음성을 설정하고 speak() 메서드를 호출하여 텍스트에서 음성을 생성합니다.​전반적으로 Web Speech API는 음성 인식 및 합성 기능을 웹 애플리케이션에 통합하기 위한 강력한 도구이며 사용자를 위한 광범위한 새로운 상호 작용 및 경험을 가능하게 할 수 있습니다. "
Optical character recognition 광학 문자 인식 ,https://blog.naver.com/wind5700/222622935248,20220116,"​ Video of the process of scanning and real-time optical character recognition (OCR) with a portable scanner. Optical character recognition or optical character reader (OCR) is the electronic or mechanical conversion of images of typed, handwritten or printed text into machine-encoded text, whether from a scanned document, a photo of a document, a scene-photo (for example the text on signs and billboards in a landscape photo) or from subtitle text superimposed on an image (for example: from a television broadcast).[1]Widely used as a form of data entry from printed paper data records – whether passport documents, invoices, bank statements, computerized receipts, business cards, mail, printouts of static-data, or any suitable documentation – it is a common method of digitizing printed texts so that they can be electronically edited, searched, stored more compactly, displayed on-line, and used in machine processes such as cognitive computing, machine translation, (extracted) text-to-speech, key data and text mining. OCR is a field of research in pattern recognition, artificial intelligence and computer vision.Early versions needed to be trained with images of each character, and worked on one font at a time. Advanced systems capable of producing a high degree of recognition accuracy for most fonts are now common, and with support for a variety of digital image file format inputs.[2] Some systems are capable of reproducing formatted output that closely approximates the original page including images, columns, and other non-textual components. Contents1History1.1Blind and visually impaired users2Applications3Types4Techniques4.1Pre-processing4.2Text recognition4.3Post-processing4.4Application-specific optimizations5Workarounds5.1Forcing better input5.2Crowdsourcing6Accuracy7Unicode8See also9References10External links History[edit]  See also: Timeline of optical character recognitionEarly optical character recognition may be traced to technologies involving telegraphy and creating reading devices for the blind.[3] In 1914, Emanuel Goldberg developed a machine that read characters and converted them into standard telegraph code.[4] Concurrently, Edmund Fournier d'Albe developed the Optophone, a handheld scanner that when moved across a printed page, produced tones that corresponded to specific letters or characters.[5]In the late 1920s and into the 1930s Emanuel Goldberg developed what he called a ""Statistical Machine"" for searching microfilm archives using an optical code recognition system. In 1931 he was granted USA Patent number 1,838,389 for the invention. The patent was acquired by IBM.Blind and visually impaired users[edit]In 1974, Ray Kurzweil started the company Kurzweil Computer Products, Inc. and continued development of omni-font OCR, which could recognize text printed in virtually any font (Kurzweil is often credited with inventing omni-font OCR, but it was in use by companies, including CompuScan, in the late 1960s and 1970s[3][6]). Kurzweil decided that the best application of this technology would be to create a reading machine for the blind, which would allow blind people to have a computer read text to them out loud. This device required the invention of two enabling technologies – the CCD flatbed scanner and the text-to-speech synthesizer. On January 13, 1976, the successful finished product was unveiled during a widely reported news conference headed by Kurzweil and the leaders of the National Federation of the Blind.[citation needed] In 1978, Kurzweil Computer Products began selling a commercial version of the optical character recognition computer program. LexisNexis was one of the first customers, and bought the program to upload legal paper and news documents onto its nascent online databases. Two years later, Kurzweil sold his company to Xerox, which had an interest in further commercializing paper-to-computer text conversion. Xerox eventually spun it off as Scansoft, which merged with Nuance Communications.In the 2000s, OCR was made available online as a service (WebOCR), in a cloud computing environment, and in mobile applications like real-time translation of foreign-language signs on a smartphone. With the advent of smart-phones and smartglasses, OCR can be used in internet connected mobile device applications that extract text captured using the device's camera. These devices that do not have OCR functionality built into the operating system will typically use an OCR API to extract the text from the image file captured and provided by the device.[7][8] The OCR API returns the extracted text, along with information about the location of the detected text in the original image back to the device app for further processing (such as text-to-speech) or display.Various commercial and open source OCR systems are available for most common writing systems, including Latin, Cyrillic, Arabic, Hebrew, Indic, Bengali (Bangla), Devanagari, Tamil, Chinese, Japanese, and Korean characters.Applications[edit]  OCR engines have been developed into many kinds of domain-specific OCR applications, such as receipt OCR, invoice OCR, check OCR, legal billing document OCR.They can be used for:Data entry for business documents, e.g. Cheque, passport, invoice, bank statement and receiptAutomatic number plate recognitionIn airports, for passport recognition and information extractionAutomatic insurance documents key information extraction[citation needed]Traffic sign recognition[9]Extracting business card information into a contact list[10]More quickly make textual versions of printed documents, e.g. book scanning for Project GutenbergMake electronic images of printed documents searchable, e.g. Google BooksConverting handwriting in real-time to control a computer (pen computing)Defeating CAPTCHA anti-bot systems, though these are specifically designed to prevent OCR.[11][12][13] The purpose can also be to test the robustness of CAPTCHA anti-bot systems.Assistive technology for blind and visually impaired usersWriting the instructions for vehicles by identifying CAD images in a database that are appropriate to the vehicle design as it changes in real time.Making scanned documents searchable by converting them to searchable PDFsTypes[edit]  Optical character recognition (OCR) – targets typewritten text, one glyph or character at a time.Optical word recognition – targets typewritten text, one word at a time (for languages that use a space as a word divider). (Usually just called ""OCR"".)Intelligent character recognition (ICR) – also targets handwritten printscript or cursive text one glyph or character at a time, usually involving machine learning.Intelligent word recognition (IWR) – also targets handwritten printscript or cursive text, one word at a time. This is especially useful for languages where glyphs are not separated in cursive script.OCR is generally an ""offline"" process, which analyses a static document. There are cloud based services which provide an online OCR API service. Handwriting movement analysis can be used as input to handwriting recognition.[14] Instead of merely using the shapes of glyphs and words, this technique is able to capture motions, such as the order in which segments are drawn, the direction, and the pattern of putting the pen down and lifting it. This additional information can make the end-to-end process more accurate. This technology is also known as ""on-line character recognition"", ""dynamic character recognition"", ""real-time character recognition"", and ""intelligent character recognition"".Techniques[edit]  Pre-processing[edit]OCR software often ""pre-processes"" images to improve the chances of successful recognition. Techniques include:[15]De-skew – If the document was not aligned properly when scanned, it may need to be tilted a few degrees clockwise or counterclockwise in order to make lines of text perfectly horizontal or vertical.Despeckle – remove positive and negative spots, smoothing edgesBinarisation – Convert an image from color or greyscale to black-and-white (called a ""binary image"" because there are two colors). The task of binarisation is performed as a simple way of separating the text (or any other desired image component) from the background.[16] The task of binarisation itself is necessary since most commercial recognition algorithms work only on binary images since it proves to be simpler to do so.[17] In addition, the effectiveness of the binarisation step influences to a significant extent the quality of the character recognition stage and the careful decisions are made in the choice of the binarisation employed for a given input image type; since the quality of the binarisation method employed to obtain the binary result depends on the type of the input image (scanned document, scene text image, historical degraded document etc.).[18][19]Line removal – Cleans up non-glyph boxes and linesLayout analysis or ""zoning"" – Identifies columns, paragraphs, captions, etc. as distinct blocks. Especially important in multi-column layouts and tables.Line and word detection – Establishes baseline for word and character shapes, separates words if necessary.Script recognition – In multilingual documents, the script may change at the level of the words and hence, identification of the script is necessary, before the right OCR can be invoked to handle the specific script.[20]Character isolation or ""segmentation"" – For per-character OCR, multiple characters that are connected due to image artifacts must be separated; single characters that are broken into multiple pieces due to artifacts must be connected.Normalize aspect ratio and scale[21]Segmentation of fixed-pitch fonts is accomplished relatively simply by aligning the image to a uniform grid based on where vertical grid lines will least often intersect black areas. For proportional fonts, more sophisticated techniques are needed because whitespace between letters can sometimes be greater than that between words, and vertical lines can intersect more than one character.[22]Text recognition[edit]There are two basic types of core OCR algorithm, which may produce a ranked list of candidate characters.[23]Matrix matching involves comparing an image to a stored glyph on a pixel-by-pixel basis; it is also known as ""pattern matching"", ""pattern recognition"", or ""image correlation"". This relies on the input glyph being correctly isolated from the rest of the image, and on the stored glyph being in a similar font and at the same scale. This technique works best with typewritten text and does not work well when new fonts are encountered. This is the technique the early physical photocell-based OCR implemented, rather directly.Feature extraction decomposes glyphs into ""features"" like lines, closed loops, line direction, and line intersections. The extraction features reduces the dimensionality of the representation and makes the recognition process computationally efficient. These features are compared with an abstract vector-like representation of a character, which might reduce to one or more glyph prototypes. General techniques of feature detection in computer vision are applicable to this type of OCR, which is commonly seen in ""intelligent"" handwriting recognition and indeed most modern OCR software.[24] Nearest neighbour classifiers such as the k-nearest neighbors algorithm are used to compare image features with stored glyph features and choose the nearest match.[25]Software such as Cuneiform and Tesseract use a two-pass approach to character recognition. The second pass is known as ""adaptive recognition"" and uses the letter shapes recognized with high confidence on the first pass to recognize better the remaining letters on the second pass. This is advantageous for unusual fonts or low-quality scans where the font is distorted (e.g. blurred or faded).[22]Modern OCR software include Google Docs OCR, ABBYY FineReader and Transym.[26] Others like OCRopus and Tesseract uses neural networks which are trained to recognize whole lines of text instead of focusing on single characters.A new technique known as iterative OCR automatically crops a document into sections based on page layout. OCR is performed on the sections individually using variable character confidence level thresholds to maximize page-level OCR accuracy. A patent from the United States Patent Office has been issued for this method [27]The OCR result can be stored in the standardized ALTO format, a dedicated XML schema maintained by the United States Library of Congress. Other common formats include hOCR and PAGE XML.For a list of optical character recognition software see Comparison of optical character recognition software.Post-processing[edit]OCR accuracy can be increased if the output is constrained by a lexicon – a list of words that are allowed to occur in a document.[15] This might be, for example, all the words in the English language, or a more technical lexicon for a specific field. This technique can be problematic if the document contains words not in the lexicon, like proper nouns. Tesseract uses its dictionary to influence the character segmentation step, for improved accuracy.[22]The output stream may be a plain text stream or file of characters, but more sophisticated OCR systems can preserve the original layout of the page and produce, for example, an annotated PDF that includes both the original image of the page and a searchable textual representation.""Near-neighbor analysis"" can make use of co-occurrence frequencies to correct errors, by noting that certain words are often seen together.[28] For example, ""Washington, D.C."" is generally far more common in English than ""Washington DOC"".Knowledge of the grammar of the language being scanned can also help determine if a word is likely to be a verb or a noun, for example, allowing greater accuracy.The Levenshtein Distance algorithm has also been used in OCR post-processing to further optimize results from an OCR API.[29]Application-specific optimizations[edit]In recent years,[when?] the major OCR technology providers began to tweak OCR systems to deal more efficiently with specific types of input. Beyond an application-specific lexicon, better performance may be had by taking into account business rules, standard expression,[clarification needed] or rich information contained in color images. This strategy is called ""Application-Oriented OCR"" or ""Customized OCR"", and has been applied to OCR of license plates, invoices, screenshots, ID cards, driver licenses, and automobile manufacturing.The New York Times has adapted the OCR technology into a proprietary tool they entitle, Document Helper, that enables their interactive news team to accelerate the processing of documents that need to be reviewed. They note that it enables them to process what amounts to as many as 5,400 pages per hour in preparation for reporters to review the contents.[30]Workarounds[edit]  There are several techniques for solving the problem of character recognition by means other than improved OCR algorithms.Forcing better input[edit]Special fonts like OCR-A, OCR-B, or MICR fonts, with precisely specified sizing, spacing, and distinctive character shapes, allow a higher accuracy rate during transcription in bank check processing. Ironically, however, several prominent OCR engines were designed to capture text in popular fonts such as Arial or Times New Roman, and are incapable of capturing text in these fonts that are specialized and much different from popularly used fonts. As Google Tesseract can be trained to recognize new fonts, it can recognize OCR-A, OCR-B and MICR fonts.[31]""Comb fields"" are pre-printed boxes that encourage humans to write more legibly – one glyph per box.[28] These are often printed in a ""dropout color"" which can be easily removed by the OCR system.[28]Palm OS used a special set of glyphs, known as ""Graffiti"" which are similar to printed English characters but simplified or modified for easier recognition on the platform's computationally limited hardware. Users would need to learn how to write these special glyphs.Zone-based OCR restricts the image to a specific part of a document. This is often referred to as ""Template OCR"".Crowdsourcing[edit]Crowdsourcing humans to perform the character recognition can quickly process images like computer-driven OCR, but with higher accuracy for recognizing images than that obtained via computers. Practical systems include the Amazon Mechanical Turk and reCAPTCHA. The National Library of Finland has developed an online interface for users to correct OCRed texts in the standardized ALTO format.[32] Crowd sourcing has also been used not to perform character recognition directly but to invite software developers to develop image processing algorithms, for example, through the use of rank-order tournaments.[33]Accuracy[edit]  Commissioned by the U.S. Department of Energy (DOE), the Information Science Research Institute (ISRI) had the mission to foster the improvement of automated technologies for understanding machine printed documents, and it conducted the most authoritative of the Annual Test of OCR Accuracy from 1992 to 1996.[34]Recognition of Latin-script, typewritten text is still not 100% accurate even where clear imaging is available. One study based on recognition of 19th- and early 20th-century newspaper pages concluded that character-by-character OCR accuracy for commercial OCR software varied from 81% to 99%;[35] total accuracy can be achieved by human review or Data Dictionary Authentication. Other areas—including recognition of hand printing, cursive handwriting, and printed text in other scripts (especially those East Asian language characters which have many strokes for a single character)—are still the subject of active research. The MNIST database is commonly used for testing systems' ability to recognise handwritten digits.Accuracy rates can be measured in several ways, and how they are measured can greatly affect the reported accuracy rate. For example, if word context (basically a lexicon of words) is not used to correct software finding non-existent words, a character error rate of 1% (99% accuracy) may result in an error rate of 5% (95% accuracy) or worse if the measurement is based on whether each whole word was recognized with no incorrect letters.[36] Using a large enough dataset is so important in a neural network based handwriting recognition solutions. On the other hand, producing natural datasets is very complicated and time-consuming.[37]An example of the difficulties inherent in digitizing old text is the inability of OCR to differentiate between the ""long s"" and ""f"" characters.[38]Web-based OCR systems for recognizing hand-printed text on the fly have become well known as commercial products in recent years[when?] (see Tablet PC history). Accuracy rates of 80% to 90% on neat, clean hand-printed characters can be achieved by pen computing software, but that accuracy rate still translates to dozens of errors per page, making the technology useful only in very limited applications.[citation needed]Recognition of cursive text is an active area of research, with recognition rates even lower than that of hand-printed text. Higher rates of recognition of general cursive script will likely not be possible without the use of contextual or grammatical information. For example, recognizing entire words from a dictionary is easier than trying to parse individual characters from script. Reading the Amount line of a cheque (which is always a written-out number) is an example where using a smaller dictionary can increase recognition rates greatly. The shapes of individual cursive characters themselves simply do not contain enough information to accurately (greater than 98%) recognize all handwritten cursive script.[citation needed]Most programs allow users to set ""confidence rates"". This means that if the software does not achieve their desired level of accuracy, a user can be notified for manual review.An error introduced by OCR scanning is sometimes termed a ""scanno"" (by analogy with the term ""typo"").[39][40]Unicode[edit]  Main article: Optical Character Recognition (Unicode block)Characters to support OCR were added to the Unicode Standard in June 1993, with the release of version 1.1.Some of these characters are mapped from fonts specific to MICR, OCR-A or OCR-B. Optical Character Recognition[1][2]Official Unicode Consortium code chart (PDF) 0123456789ABCDEFU+244x⑀⑁⑂⑃⑄⑅⑆⑇⑈⑉⑊U+245xNotes1.^ As of Unicode version 14.02.^ Grey areas indicate non-assigned code points See also[edit]  AI effectApplications of artificial intelligenceComparison of optical character recognition softwareComputational linguisticsDigital libraryDigital mailroomDigital penInstitutional repositoryLegibilityList of emerging technologiesLive ink character recognition solutionMagnetic ink character recognitionMusic OCROCR in Indian LanguagesOptical mark recognitionOutline of artificial intelligenceSketch recognitionSpeech recognitionTesseract OCR engineVoice recording​​ 역사OCR의 역사는 우리의 생각보다 길다. OCR의 역사는 1928년에 독일의 G. Taushek가 미리 준비된 몇 개의 표준 pattern문자와 입력문자를 비교하여 표준 pattern문자와 가장 유사한 것을 해당 문자로 선정하는 pattern matching기법을 이용한 문자 인식 방법을 특허로 등록하면서 시작되었다. 연구소나 기업 등에서 대형 컴퓨터를 통하여 수행되던 것이 PC와 이미지 스캐너의 보급으로 우리 가까이 사용되기 시작한 것은 이미 20년이 넘었다.해외에서 먼저 연구가 시작된 OCR 기술은 영어인식을 위주로 개발되었으며, 우리나라에는 90년대 초반에 한글 문자 인식 소프트웨어가 실험실 수준으로 개발되어 일반에 소개되었었다. 연산처리 능력이 뛰어난 워크스테이션급에서 동작되던 것들이 개인용 컴퓨터의 처리 능력이 업그레이드 되면서 PC용 OCR들이 시장에 속속 나타나던 것이 90년대 중반이었다.[1]최근 네이버 Clova에서 OCR서비스를 오픈했으며, 네이버 클라우드 플랫폼 콘솔에서 Gateway API를 통해 이용 가능하다. OCR 챌린지인 'ICDAR Robust Reading Competition'에서 '19년 4개 분야를 석권, 정확도와 기술력을 인정받았다고 한다.​​1. 개요  光學文字認識(광학문자인식) / Optical Character Recognition(OCR)[1]​보통 컴퓨터가 2진법(0/1) 데이터를 폰트를 통해 인간이 인식할 수 있는 형태로 글자를 보여 준다면, OCR은 그 반대로 인간이 종이 위에 써 놓은 글씨를 인지하여 텍스트 데이터로 치환한다. 보통은 스캐너로 읽어들인 이미지 파일을 분석하여 텍스트나 워드 파일로 결과물을 내놓는다. Adobe Acrobat도 OCR 기능이 있다.​이미 존재하는 폰트와 대조하는 식으로 이미지를 인식하므로, 적어도 300dpi 이상의 해상도에 필기체보단 정자로 또박또박 잘 쓴 글씨가 인식률이 더 높다. 물론 이전에 프린터로 인쇄했던 문서라면 가장 잘 인식된다.​언어/문자별로 은근히 인식률이 차이가 있다. 그냥 한 줄로 쭉 쓰는 영어 및 서유럽 언어가 가장 연구가 진행되어 있어서 인식률이 매우 뛰어나다. 어지간한 영어 문서는 99.5% 제대로 인식된다고 보면 된다. 휘갈겨 쓴 필기체의 인식률도 뛰어난 편이다. 반면 한글, 한자같은 문자는 모양이 복잡하기도 하고 연구 투자도 서양에 비해 미진한 편이기 때문에 더 높은 해상도로 스캔하거나 하지 않으면 오자율이 상당하다. 손으로 쓴 한글이 특수 문자로 변환되는 건 아주 흔하다. 또한 모양이 비슷한 글자가 있어서 결과물이 야민정음으로 나오는 경우도 있다. 특히 '관'을 '판'으로 인식하는 오류가 가장 심해서, ""-에 판하여""로 검색하면 논문이 줄줄이 나온다.(구글 검색 결과) 실생활에 쓰이는 문서(이력서나 공문서)가 소설 책같이 글자만 있는 것은 아니므로 안 그래도 떨어지는 인식률이 바닥을 달린다. 특히 표나 그림이 들어간 문서는 인식률이 최악이다. 한국의 오래된 행정 문서들이 수백 년 역사를 가진 미국보다 느리게 디지털화되고 있는 이유이기도 하다.​같은 한자라도, 비영어권 문자라도 일본어처럼 히라가나, 가타카나가 섞인 문면은 한글에 비해 훨씬 인식률이 높은 편이다. 글자 자체가 정형화되어 있어서 활자본 글자는 90% 이상 인식이 된다. ソ(so)와 ン(n) 정도만 빼면 웬만한 것들은 정확하다. 전각과 반각 문서에도 있지만 일본이 자국어를 컴퓨터로 표현하는 것을 연구한 역사는 꽤 유구하다. 물론 ソ와 ン를 문맥으로 자연스럽게 구분하기 위한 연구도 활발하다.​예전에 비해 인식률이 많이 올라간 상황이나, 100% 믿지는 말 것. 원본 이미지는 보관하는 게 신상에 좋다.​최근 네이버 Clova에서 OCR서비스를 오픈했으며, 네이버 클라우드 플랫폼 콘솔에서 Gateway API를 통해 이용 가능하다. OCR 챌린지인 'ICDAR Robust Reading Competition'에서 '19년 4개 분야를 석권, 정확도와 기술력을 인정받았다고 한다. 금액도 1건 당 3원이며, 무료 100건/월 제공된다. 다만, 개발자를 대상으로 만든 서비스이기 때문에 일반인이 사용하기엔 조금 복잡하다. 온라인 문서변환 서비스와 마찬가지로, 데이터 유출 방지와 보안을 위해 믿을 수 있는 플랫폼만 이용하도록 하자. https://www.ncloud.com/product/aiService/ocr​대한민국 여권에 영문 이름을 적어 넣을 때도 1차적으로 OCR을 사용하는 것으로 보인다. KIM이 KTM으로 잘못 적히거나 PARK이 RARK으로 잘못 적히거나 JIWON이 JIWOW로 잘못 적히는 문제는 OCR이 아니라면 일어날 수 없는 문제이다. OCR을 뜬 뒤 직원이 글자가 잘못 인식된 것을 미처 발견하지 못하고 여권을 발급하면 저렇게 된다. 신용카드/체크카드 발급시에도 종종 이런 오류가 발생한다.​모바일용 구글 번역에서는 OCR을 이용한 번역을 지원한다. 인식률은 매우 괜찮은 수준.​우체국에서도 OCR을 이용해 주소를 판독한다고 한다. 89년도부터 도입이 시작됐던 모양이다.​최근 OCR 앱이 나오면서 스마트폰으로 사진을 찍고, 바로 문서로 변환할 수 있게 되었다. 영어 원서는 어느 앱이나 인식률이 높다. 한글은 TextGrabber + Translator의 성능이 괜찮다. FineReader를 만든 ABBYY에서 만든 앱으로 가격은 11,000원이다. 몇 년 전 OCR을 생각하면 훌륭하지만, 오류가 많아서 손으로 일일이 수정해야 하는 것은 변함없다. 프랑스어나 독일어, 베트남어 등 diacritic이 있는 언어는 Office Lens를 추천한다. 마이크로소프트에서 만들어 무료로 배포하는 앱으로, diacritic 인식률이 훌륭하다.​Adobe Acrobat Pro DC에서도 한국어 OCR을 지원해 준다.​2. 예시  1. 네이버 클라우드 플랫폼 OCR​​OCRI분야에서 가장 권위있는 경진대회인 ""CDAR Robust Reading Competition"" 에서 4개 분야를 석권한 네이버 Clova OCR이 최근 네이버 클라우드 플랫폼 콘솔을 통해 서비스 이용 가능해졌다. 고가의 OCR프로그램을 별도로 설치하여 사용하지 않아도 건 별 3원이라는 저렴한 가격으로 이용가능하다. 활자체의 경우 타사대비 15%이상 높은 인식률을 보이며, 필기체의 경우에도 2~3배 정도 높은 인식률을 보유한다.CLOVA OCR 서비스는 네이버의 AI 기술을 활용하여 주요 비즈니스 활용에 최적화된 고성능 OCR 인식 모델을 적용시켰다. 문자 인식이 제공되는 언어는 한국어, 영어, 일본어이며, 필기체 인식은 한국어와 일본어가 지원된다.네이버 클라우드 플랫폼의 CLOVA OCR은 문서 레이아웃 분석 및 글자를 읽는 순서 방향을 추정하여 둥글게 곡선으로 배열되거나 기울어진 문자, 필기체 인식 등 고성능 AI 모델바탕으로 높은 수준의 정확도를 제공한다. OCR분야 가장 권위있는 글로벌 챌린지ICDAR2019 4개 분야에서 1위, CVPR 및 ICCV 국제학회 논문 선정 등 독보적인 기술이 집약되어 있다.​2. ABBYY OCROCR 프로그램 중 가장 성능이 좋다고 알려진 ABBYY FineReader 11. 2019년 4월 현재 가장 최신 버전인 14 기준으로 개인용은 239,000원.[2] 후지쯔 스냅스캔 시리즈 등 여러 회사의 문서처리용 고속스캐너 중에 번들로 넣어주는 모델이 있다. 문서를 스캔하면서 PDF파일로 묶는데, 그 때 OCR한 결과를 넣어서 검색가능한 PDF로 만들어준다.​3. 제품  유료네이버 클라우드 플랫폼Naver Cloud Platform 3원/건당 (무료제공 100회/월)Template OCR: 템플릿이 있는 문서들의 반복적인 스캔을 진행할 경우 인식률을 획기적으로 높일 수 있다.Document OCR: 많은 학습데이터를 기반으로 CLOVA AI 기술을 적용하여 영수증/신용카드/사업자등록증/명함/신분증 등의 문서의 주요 Feature를 추출합니다.ABBYY: 가장 성능이 좋은 것으로 알려져있다.FineReader 14 Standard(개인용): $199.99 / ￦239,000FineReader 14 Corporate(기업용)[3]: $399.99 / ￦349,000FineReader 15 Standard(개인용): 269,900원FineReader 15 Corporate(단일사용): 399,900원FineReader 15 Corporate(동시사용): 756,800원AdobeAdobe Acrobat Pro DC: $14.99(월) / ￦29,700(월)[4]NuanceNuance OmniPage 18: $149.99Nuance OmniPage Ultimate: $499.99ReadirisREADIRIS PDF 17: $49GrooperGrooper OCR: $499.99무료구글 드라이브: 구글 드라이브에 업로드한 파일[5]을 구글 문서로 변환. 인식률은 매우 괜찮은 수준. 크롬 브라우저에서 할 경우 인식률을 더 높일 수 있다.네이버 웨일: 이미지의 마우스 오른쪽을 클릭. 이미지에 있는 글자 번역 클릭. 작은 파파고 번역창에서 이미지에 있는 글자의 언어를 선택. 마우스로 영역을 지정. 작은 파파고 번역창에 인식된 글자가 나오고 동시에 번역까지 나온다.MORT : 실시간으로 게임을 번역하기 위해 만들어진 OCR이다.알PDF[6][7]Capture2Text: 마우스로 영역을 지정한 부분을 인식 가능한 OCR. 일본어 만화를 보기 위해 만들어졌으며, 클립보드로 출력을 지원해서, EZTrans XP(or 구글번역), 아네모네 등과 조합해서 실시간으로 만화를 번역하며 볼 수 있다.Capture2OCR: 마우스 영역 지정을 통해 이미지에서 텍스트를 뽑아 번역 해주는 OCR.New OCR Free Online OCR모바일CamScannerAdobe ScanOffice LensTextGrabberOCR Manga Reader: 위의 Capture2Text 제작자가 만든 Android용 만화 OCR. 만화를 보며 구글 번역과 연동해서 선택한 영역을 번역해서 볼 수 있다.  [1] 광학표시판독(Optical Mark Recognition, OMR)과 구분해야 한다.[2] ABBYY FineReader 12로 2016년 수능특강 물리2를 스캔한 모습  그러나 이처럼 한글과 수식이 섞인 문서를 인식하려고 하면 어김없이 깨지는 모습을 보인다. 사실 인식 후 문서의 양식을 유지해 준다거나 이미지 밑에 문자를 숨겨서 검색만 가능하게 해 준다거나 하는 부가 기능이 유용한 거지 인식 기능이 엄청나게 차이 나지는 않는다.[3] 개인용 기능에 문서 비교 기능과 매월 5,000 페이지의 OCR 자동화 기능이 추가되어 있다.[4] 다른 OCR 제품들과 달리 한 번 구입하면 끝이 아니라, 매월 구독료를 지불해야 한다. 이는 Adobe의 CC 버전 이후 모든 제품들이 동일하다.[5] 확장자가 webp인 파일은 구글문서 변환이 안됨.[6] 2.10.3 버전 이후 OCR 인식 기능이 중단되었으나 2.3버전 이후 OCR 기능이 재개되었으며 성능이 향상되었다.[7] 물론 개인 사용자에게만 무료고, 기업이 이용하려면 유료다.​​광학 문자 인식(OCR) 가이드 Google Cloud Platform에서 광학 문자 인식(OCR)을 수행하는 방법을 알아보세요. 이 가이드에서는 이미지 파일을 Google Cloud Storage에 업로드하고, Google Cloud Vision API를 사용하여 이미지에서 텍스트를 추출하고, Google Cloud Translation API를 사용하여 텍스트를 번역하고, 번역을 다시 Cloud Storage에 저장하는 방법을 설명합니다. Google Cloud Pub/Sub를 사용하여 여러 태스크를 큐에 추가하고 이를 실행하기 위한 적절한 Cloud Functions를 트리거할 수 있습니다.​목표여러 백그라운드 Cloud Functions를 작성하고 배포합니다.Cloud Storage에 이미지를 업로드합니다.업로드된 이미지에서 텍스트를 추출, 번역, 저장합니다.​비용이 가이드에서는 비용이 청구될 수 있는 다음과 같은 Google Cloud 구성요소를 사용합니다.Cloud FunctionsPub/SubCloud StorageCloud Translation APICloud Vision프로젝트 사용량을 기준으로 예상 비용을 산출하려면 가격 계산기를 사용하세요. Google Cloud를 처음 사용하는 사용자는 무료 체험판을 사용할 수 있습니다.​시작하기 전에Google Cloud를 처음 사용하는 경우 계정을 만들고 Google 제품의 실제 성능을 평가해 보세요. 신규 고객에게는 워크로드를 실행, 테스트, 배포하는 데 사용할 수 있는 $300의 무료 크레딧이 제공됩니다.Google Cloud Console의 프로젝트 선택기 페이지에서 Google Cloud 프로젝트를 선택하거나 만듭니다.참고: 이 절차에서 생성한 리소스를 유지하지 않으려면 기존 프로젝트를 선택하지 말고 프로젝트를 새로 만드세요. 작업이 끝나면 프로젝트를 삭제하여 프로젝트와 관련된 모든 리소스를 삭제할 수 있습니다.프로젝트 선택기로 이동Cloud 프로젝트에 결제가 사용 설정되어 있는지 확인합니다. 프로젝트에 결제가 사용 설정되어 있는지 확인하는 방법을 알아보세요.Cloud Functions, Cloud Build, Cloud Pub/Sub, Cloud Storage, Cloud Translation, and Cloud Vision API를 사용 설정합니다.API 사용 설정Cloud SDK 설치 및 초기화Cloud SDK가 이미 설치되어 있으면 다음 명령어를 실행하여 업데이트하세요.gcloud components update개발 환경을 준비합니다.Node.jsPythonGo자바Node.js 설정 가이드로 이동​데이터 흐름 시각화OCR 가이드 애플리케이션의 데이터 흐름 단계는 다음과 같습니다.언어에 상관없이 텍스트를 포함하는 이미지가 Cloud Storage에 업로드됩니다.Vision API를 사용하여 텍스트를 추출하고 출발어를 감지하는 Cloud 함수가 트리거됩니다.텍스트는 Pub/Sub 주제에 메시지를 게시하여 번역 큐에 추가됩니다. 번역은 출발어와 다른 각 도착어 큐에 추가됩니다.도착어가 출발어와 일치하면 번역 큐를 건너뛰고 텍스트가 다른 Pub/Sub 주제인 결과 큐로 전송됩니다.Cloud 함수는 Translation API를 사용하여 번역 큐의 텍스트를 번역합니다. 번역 결과는 결과 큐로 전송됩니다.다른 Cloud 함수가 결과 큐에 있는 번역된 텍스트를 Cloud Storage에 저장합니다.결과는 Cloud Storage에서 각 번역의 txt 파일로 검색됩니다.해당 단계를 시각화하면 다음과 같습니다. ​애플리케이션 준비이미지를 업로드할 Cloud Storage 버킷을 만듭니다. YOUR_IMAGE_BUCKET_NAME은 전역적으로 고유한 버킷 이름입니다.gsutil mb gs://YOUR_IMAGE_BUCKET_NAME텍스트 번역을 저장할 Cloud Storage 버킷을 만듭니다. YOUR_RESULT_BUCKET_NAME은 전역적으로 고유한 버킷 이름입니다.gsutil mb gs://YOUR_RESULT_BUCKET_NAME번역 요청을 게시할 Cloud Pub/Sub 주제를 만듭니다. YOUR_TRANSLATE_TOPIC_NAME은 번역 요청 주제의 이름입니다.gcloud pubsub topics create YOUR_TRANSLATE_TOPIC_NAME완료된 번역 결과를 게시할 Cloud Pub/Sub 주제를 만듭니다. YOUR_RESULT_TOPIC_NAME은 번역 결과 주제의 이름입니다.gcloud pubsub topics create YOUR_RESULT_TOPIC_NAME샘플 앱 저장소를 로컬 머신에 클론합니다.Node.jsPythonGo자바git clone https://github.com/GoogleCloudPlatform/nodejs-docs-samples.git또는 zip 파일로 샘플을 다운로드하고 압축을 풀 수 있습니다.Cloud Functions 샘플 코드가 있는 디렉터리로 변경합니다.Node.jsPythonGo자바cd nodejs-docs-samples/functions/ocr/app/​코드 이해하기종속 항목 가져오기애플리케이션은 Google Cloud Platform 서비스와 통신하기 위해 몇 가지 종속성을 가져와야 합니다.Node.jsPythonGo자바 functions/ocr/app/index.jsGitHub에서 보기 // Get a reference to the Pub/Sub componentconst {PubSub} = require('@google-cloud/pubsub');const pubsub = new PubSub();// Get a reference to the Cloud Storage componentconst {Storage} = require('@google-cloud/storage');const storage = new Storage();​// Get a reference to the Cloud Vision API componentconst Vision = require('@google-cloud/vision');const vision = new Vision.ImageAnnotatorClient();​// Get a reference to the Translate API componentconst {Translate} = require('@google-cloud/translate').v2;const translate = new Translate();이미지 처리다음 함수는 Cloud Storage에서 업로드된 이미지 파일을 읽고 이미지에 텍스트가 포함되어 있는지를 감지하는 함수를 호출합니다.Node.jsPythonGo자바 functions/ocr/app/index.jsGitHub에서 보기 /** * This function is exported by index.js, and is executed when * a file is uploaded to the Cloud Storage bucket you created * for uploading images. * * @param {object} event A Google Cloud Storage File object. */exports.processImage = async event => {  const {bucket, name} = event;​  if (!bucket) {    throw new Error(      'Bucket not provided. Make sure you have a ""bucket"" property in your request'    );  }  if (!name) {    throw new Error(      'Filename not provided. Make sure you have a ""name"" property in your request'    );  }​  await detectText(bucket, name);  console.log(`File ${name} processed.`);};다음 함수는 Cloud Vision API를 사용하여 이미지에서 텍스트를 추출하고 번역을 위해 큐에 추가합니다.Node.jsPythonGo자바 functions/ocr/app/index.jsGitHub에서 보기 /** * Detects the text in an image using the Google Vision API. * * @param {string} bucketName Cloud Storage bucket name. * @param {string} filename Cloud Storage file name. * @returns {Promise} */const detectText = async (bucketName, filename) => {  console.log(`Looking for text in image ${filename}`);  const [textDetections] = await vision.textDetection(    `gs://${bucketName}/${filename}`  );  const [annotation] = textDetections.textAnnotations;  const text = annotation ? annotation.description : '';  console.log('Extracted text from image:', text);​  let [translateDetection] = await translate.detect(text);  if (Array.isArray(translateDetection)) {    [translateDetection] = translateDetection;  }  console.log(    `Detected language ""${translateDetection.language}"" for ${filename}`  );​  // Submit a message to the bus for each language we're going to translate to  const TO_LANGS = process.env.TO_LANG.split(',');  const topicName = process.env.TRANSLATE_TOPIC;​  const tasks = TO_LANGS.map(lang => {    const messageData = {      text: text,      filename: filename,      lang: lang,    };​    // Helper function that publishes translation result to a Pub/Sub topic    // For more information on publishing Pub/Sub messages, see this page:    //   https://cloud.google.com/pubsub/docs/publisher    return publishResult(topicName, messageData);  });​  return Promise.all(tasks);};텍스트 번역다음 함수는 추출된 텍스트를 번역하고 이를 Cloud Storage에 다시 저장하기 위해 대기열에 추가합니다.Node.jsPythonGo자바 functions/ocr/app/index.jsGitHub에서 보기 /** * This function is exported by index.js, and is executed when * a message is published to the Cloud Pub/Sub topic specified * by the TRANSLATE_TOPIC environment variable. The function * translates text using the Google Translate API. * * @param {object} event The Cloud Pub/Sub Message object. * @param {string} {messageObject}.data The ""data"" property of the Cloud Pub/Sub * Message. This property will be a base64-encoded string that you must decode. */exports.translateText = async event => {  const pubsubData = event.data;  const jsonStr = Buffer.from(pubsubData, 'base64').toString();  const {text, filename, lang} = JSON.parse(jsonStr);​  if (!text) {    throw new Error(      'Text not provided. Make sure you have a ""text"" property in your request'    );  }  if (!filename) {    throw new Error(      'Filename not provided. Make sure you have a ""filename"" property in your request'    );  }  if (!lang) {    throw new Error(      'Language not provided. Make sure you have a ""lang"" property in your request'    );  }​  console.log(`Translating text into ${lang}`);  const [translation] = await translate.translate(text, lang);​  console.log('Translated text:', translation);​  const messageData = {    text: translation,    filename: filename,    lang: lang,  };​  await publishResult(process.env.RESULT_TOPIC, messageData);  console.log(`Text translated to ${lang}`);};번역 저장마지막으로 다음 함수는 번역된 텍스트를 받아 Cloud Storage에 다시 저장합니다.Node.jsPythonGo자바 functions/ocr/app/index.jsGitHub에서 보기 /** * This function is exported by index.js, and is executed when * a message is published to the Cloud Pub/Sub topic specified * by the RESULT_TOPIC environment variable. The function saves * the data packet to a file in GCS. * * @param {object} event The Cloud Pub/Sub Message object. * @param {string} {messageObject}.data The ""data"" property of the Cloud Pub/Sub * Message. This property will be a base64-encoded string that you must decode. */exports.saveResult = async event => {  const pubsubData = event.data;  const jsonStr = Buffer.from(pubsubData, 'base64').toString();  const {text, filename, lang} = JSON.parse(jsonStr);​  if (!text) {    throw new Error(      'Text not provided. Make sure you have a ""text"" property in your request'    );  }  if (!filename) {    throw new Error(      'Filename not provided. Make sure you have a ""filename"" property in your request'    );  }  if (!lang) {    throw new Error(      'Language not provided. Make sure you have a ""lang"" property in your request'    );  }​  console.log(`Received request to save file ${filename}`);​  const bucketName = process.env.RESULT_BUCKET;  const newFilename = renameImageForSave(filename, lang);  const file = storage.bucket(bucketName).file(newFilename);​  console.log(`Saving result to ${newFilename} in bucket ${bucketName}`);​  await file.save(text);  console.log('File saved.');};​함수 배포이 섹션에서는 함수를 배포하는 방법을 설명합니다.Cloud Storage 트리거를 사용하여 이미지 처리 함수를 배포하려면 샘플 코드(또는 자바의 경우 pom.xml 파일)가 포함된 디렉터리에서 다음 명령어를 실행합니다.Node.jsPythonGo자바gcloud functions deploy ocr-extract \ --runtime nodejs16 \--trigger-bucket YOUR_IMAGE_BUCKET_NAME \--entry-point processImage \--set-env-vars ""^:^GCP_PROJECT=YOUR_GCP_PROJECT_ID:TRANSLATE_TOPIC=YOUR_TRANSLATE_TOPIC_NAME:RESULT_TOPIC=YOUR_RESULT_TOPIC_NAME:TO_LANG=es,en,fr,ja""--runtime 플래그에 다음 값을 사용하여 원하는 Node.js 버전을 지정할 수 있습니다.nodejs16(권장)nodejs14nodejs12nodejs10여기서 YOUR_IMAGE_BUCKET_NAME은 이미지를 업로드할 Cloud Storage 버킷 이름입니다.Cloud Pub/Sub 트리거를 사용하여 텍스트 번역 함수를 배포하려면 샘플 코드(또는 자바의 경우 pom.xml 파일)가 포함된 디렉터리에서 다음 명령어를 실행합니다.Node.jsPythonGo자바gcloud functions deploy ocr-translate \ --runtime nodejs16 \--trigger-topic YOUR_TRANSLATE_TOPIC_NAME \--entry-point translateText \--set-env-vars ""GCP_PROJECT=YOUR_GCP_PROJECT_ID,RESULT_TOPIC=YOUR_RESULT_TOPIC_NAME""--runtime 플래그에 다음 값을 사용하여 원하는 Node.js 버전을 지정할 수 있습니다.nodejs16(권장)nodejs14nodejs12nodejs10Cloud Pub/Sub 트리거를 사용하여 결과를 Cloud Storage에 저장하는 함수를 배포하려면 샘플 코드(또는 자바의 경우 pom.xml 파일)가 있는 디렉터리에서 다음 명령어를 실행합니다.Node.jsPythonGo자바gcloud functions deploy ocr-save \ --runtime nodejs16 \--trigger-topic YOUR_RESULT_TOPIC_NAME \--entry-point saveResult \--set-env-vars ""GCP_PROJECT=YOUR_GCP_PROJECT_ID,RESULT_BUCKET=YOUR_RESULT_BUCKET_NAME""--runtime 플래그에 다음 값을 사용하여 원하는 Node.js 버전을 지정할 수 있습니다.nodejs16(권장)nodejs14nodejs12nodejs10​이미지 업로드이미지를 이미지 Cloud Storage 버킷에 업로드합니다.gsutil cp PATH_TO_IMAGE gs://YOUR_IMAGE_BUCKET_NAME각 항목의 의미는 다음과 같습니다.PATH_TO_IMAGE는 로컬 시스템의 이미지 파일(텍스트 포함) 경로입니다.YOUR_IMAGE_BUCKET_NAME은 이미지를 업로드할 버킷의 이름입니다.샘플 프로젝트에서 이미지 중 하나를 다운로드할 수 있습니다.로그를 확인하여 실행이 완료되었는지 확인합니다.gcloud functions logs read --limit 100YOUR_RESULT_BUCKET_NAME에 사용한 Cloud Storage 버킷에서 저장된 번역을 볼 수 있습니다.삭제이 튜토리얼에서 사용된 리소스 비용이 Google Cloud 계정에 청구되지 않도록 하려면 리소스가 포함된 프로젝트를 삭제하거나 프로젝트를 유지하고 개별 리소스를 삭제하세요.프로젝트 삭제비용이 청구되지 않도록 하는 가장 쉬운 방법은 가이드에서 만든 프로젝트를 삭제하는 것입니다.프로젝트를 삭제하려면 다음 안내를 따르세요.주의: 프로젝트를 삭제하면 다음과 같은 효과가 발생합니다.프로젝트의 모든 항목이 삭제됩니다. 이 가이드의 기존 프로젝트를 사용한 경우 프로젝트를 삭제하면 프로젝트에서 수행한 다른 작업도 삭제됩니다.커스텀 프로젝트 ID가 손실됩니다. 이 프로젝트를 만들 때 앞으로 사용할 커스텀 프로젝트 ID를 만들었을 수 있습니다. appspot.com URL과 같이 프로젝트 ID를 사용하는 URL을 보존하려면 전체 프로젝트를 삭제하는 대신 프로젝트 내에서 선택한 리소스만 삭제합니다.여러 가이드와 빠른 시작을 살펴보려는 경우 프로젝트를 재사용하면 프로젝트 할당량 한도 초과를 방지할 수 있습니다.Cloud Console에서 리소스 관리 페이지로 이동합니다.리소스 관리로 이동프로젝트 목록에서 삭제할 프로젝트를 선택하고 삭제를 클릭합니다.대화상자에서 프로젝트 ID를 입력한 후 종료를 클릭하여 프로젝트를 삭제합니다.Cloud 함수 삭제Cloud Functions를 삭제해도 Cloud Storage에 저장된 리소스는 삭제되지 않습니다.이 가이드에서 만든 Cloud Functions를 삭제하려면 다음 명령어를 실행합니다.gcloud functions delete ocr-extractgcloud functions delete ocr-translategcloud functions delete ocr-saveGoogle Cloud Console에서 Cloud Functions를 삭제할 수도 있습니다.​https://blog.naver.com/gridoneai/222446247705 [그리드원] AI(인공지능)와 만난 OCR(광학 문자 인식)은?최근 다양한 산업 분야에서 디지털 전환이 빠르게 이루어지면서 업무 자동화에 대한 관심이 늘어나고 있습...blog.naver.com  ​ "
「Deep Speech」 Paper Review ,https://blog.naver.com/sooftware/221826730288,20200227,"Deep Speech: Scaling up end-to-end speech recognition    https://arxiv.org/pdf/1412.5567.pdf (Awni Hannun et al. 2014)​다음 글은 이곳에서 더 깔끔한 화면으로 보실 수 있습니다.​​Abstract  본 논문은 2014년도에 나온 논문이다. 논문을 읽어본 결과, 당시에는 음성인식에 End-to-End 방식의 딥러닝을 적용한 사례가 없던 모양이다. (또는 주목할만한 성과가 나오지 않았던 모양이다.) 본 논문에서는 End-to-End 방식으로 Switchboard Hub5'00 데이터셋에서 16.0%의 Error Rate를 기록하며 State-Of-The-Art (SOTA) 를 달성한 성과를 밝히고 있다. 기존 음성인식의 traditional한 방식은 전처리 과정이 상당히 많이 필요했지만, 이러한 과정 없이 데이터와 레이블만을 이용한 End-to-End 방식으로 이러한 성과를 냈음을 거듭 강조하고 있다. 그리고 이전 방식으로는 노이즈가 있는 환경에서 급격히 떨어졌는데 반하여, 본 논문 방식으로는 노이즈가 있는 환경에서도 좋은 성능을 기록했다고 한다. 즉, 기존 방식보다 더 간단하면서도 좋은 성능을 기록한 End-to-End 방식의 Speech-Recognition 모델을 소개하는 논문이다.​​Introduction  Abstract에서 강조한 내용을 다시 한번 강조한다. 기존 traditional한 방식은 전문가들의 손이 많이 가야했다. (노이즈 필터링 등..) 하지만 그러한 노력에도 불구하고, 실제 노이즈가 낀 상황에서의 인식률은 좋지 못했다. 하지만 End-to-End 방식으로 이러한 2가지의 단점을 개선할 수 있다고 주장한다.​물론 End-to-End 방식으로 가기 위해서는 몇가지 고려해야할 사항들이 있었지만, 본 논문에서는 기존 연구 결과들을 참고하여 End-to-End 방식을 시도할 수 있었다고 말한다. 그리고, 본 논문에서는 RNN 모델을 사용했다. 뒤에 더 자세히 설명하겠지만 본 논문에서는 학습 시간을 단축시키기 위해 많은 고려를 한 것으로 보인다.​​RNN Training Setup    해당 챕터에서는 자신들이 어떤 식으로 모델을 구성했는지에 대해 설명한다.모델의 핵심은 RNN으로 구성되어 있으며, 트레이닝 셋은{(x_1, y_1), (x_2, y_2), ... (x_t, y_t)}와 같은 딕셔너리 형식으로 구성했다고 한다.(x는 스펙트로그램, y는 문자로 구성 )모델은 총 5개의 히든 레이어로 구성했다고 한다. 논문을 읽으면서 모델 아키텍쳐가 상당히 특이하다고 생각했다.​현재 내가 알고있는 방식과는 사뭇 다른 방식이였는데, 히든 레이어 중 1, 2, 3번째 레이어는 병렬적 (Parallel) 하게 처리하기 위해 서로 독립적으로 포워딩 된다고 한다. RNN 아키텍쳐를 사용하게 되면 이전 셀의 아웃풋이 필요하기 때문에 어쩔 수 없이 병렬처리의 한계점이 있기 때문에 학습 속도 개선을 위해 각 인풋을 독립적으로 처리했다고 한다.여기서도 나는 본 논문이 학습 시간을 단축시키기 위해 상당히 노력했다는 인상을 받았다.   위의 수식을 통해 포워딩이 진행되는데, 여기서 g는 최소 0, 최대 20의 값을 가지는 ReLU 함수이다.그리고 4번째 레이어에서는 Bidirectional-RNN으로 구성했다.   (f)는 정방향 (forward), (b)는 역방향 (backward)을 표현한 것이다.이때 주의할 점으로는, forward는 t = 1 에서 t = T 방향으로 흐르고, backward는 t = T에서 t = 1 방향으로 흐른다는 점이다.   그리고 마지막 5번째 레이어는 이렇게 forward, backward의 결과에 웨이트를 주고 1, 2, 3 번째 레이어와 동일한 ReLU를 활성화 함수로 사용했다.   그리고 이렇게 나온 결과는 Softmax 함수에 넣어서 최종적으로 Classfication을 진행한다. 또한 loss 계산시에는 CTC loss를 사용했다.여기서 또 특이했던 점으로, LSTM이 아닌 기본 RNN을 사용했다는 점이다.그 이유로 본 논문에서는 LSTM의 단점은 메모리가 많이 소요되고, 학습이 오래걸린다는 점을 꼽았다.총 5개의 히든 레이어 중 실제 RNN 계층은 1개 뿐이라는 점과, LSTM이 아닌 RNN을 사용했다는 점이 인상깊었다.최근 연구에서는 기본 RNN을 사용하는 모습을 거의 볼 수 없는데, 당시에는 아직 GPU의 성능이 그리 좋지 않던 때라 그런지 학습 속도에 대해 굉장히 고려를 많이한 모습이 보였다.​Regularization​본 논문은 학습 시, 드랍아웃 비율을 5 - 10%정도를 유지했다고 한다.그리고 Spectrogram의 프레임 길이는 10ms, 포워딩은 5ms를 사용했다.(해당 부분은 오류가 있을수도 있습니다)통상적으로 음성 인식에서 프레임 길이는 20 - 40ms를 사용하기 때문에 프레임 길이가 상당히 짧다고 생각했다.해당 부분은 다른 이유가 있어서 짧게 한 건지, 당시에 프레임 길이에 대한 연구가 현재보다 덜 발달해서 그런 것인지는 확인을 해봐야 할 듯 하다.​​Language Model​본 논문의 모델은 성능 테스트시에, 정확히 맞추거나 그럴싸하게 틀렸다고 한다.   arther => are there, n tickets => any tickets 등 꽤나 말이 되도록 틀린 것을 볼 수 있다.본 논문은 이보다 더 정확한 인식을 위하여 N-gram Language Model을 사용했다고 한다.매우 방대한 텍스트 Corpus로 N-gram language model을 학습시켰으며, 해당 언어 모델은 다음 공식에 사용됐다.   여기서 알파, 베타는 설정 가능한 파라미터이다.본 논문에서는 성능을 향상시키기 위해 빔서치를 사용했는데, 이때 빔 사이즈를 1,000 - 8,000으로 상당히 크게 준 것을 볼 수 있었다.이후에 나온 논문들을 봤을 때, 빔 사이즈 단위는 기껏 해봐야 수십 정도였는데 본 논문은 상당히 큰 빔 사이즈를 사용한 것을 볼 수 있었다.​​​Optimizations  해당 장에서는 어떻게 최적화를 했는지에 대해 설명하고 있다.주로 빠른 학습을 시키기 위해 어떤 노력을 했는지를 설명했다.​​Data Parallelism데이터를 효과적으로 처리하기 위해 2-level data parallelism을 사용했다고 한다.미니배치 단위로 처리를 했는데, 이때 배치의 크기를 GPU 메모리 한계까지 사용했다고 한다.또한, 학습을 빨리하기 위해 NMT와 같은 Text-NLP에서 많이 사용되는, 길이 순으로 정렬해서 비슷한 길이끼리 배치로 묶었다고 한다. 이렇게 비슷한 길이끼리 배치로 묶게 되면, 배치 안에서 Max Length를 맞추기 위해 PAD token을 최소화 할 수 있다.(아직 작성중입니다.) "
"잠실 보청기 / 어음청각검사 (Korean Speech Audiometry, KSA) ",https://blog.naver.com/medicarehearing/221901207233,20200410,"​ ​청력손실을 확인할 수 있는 다양한 청각 검사 중 어음청각검사는 청각장애인의 언어 인지력과 이해력을 측정함으로써 일상생활의 의사소통능력을 파악할 수 있습니다. 또한 순음을 사용하는 청각검사에 비해 의사소통능력을 측정하는 데 있어 보다 타당한 검사이므로 보청기 착용 전후 평가 시 반드시 함께 진행되어야 하는데요.​따라서 오늘은 어음청각검사가 만들어지고 시행 된 이유와 검사의 특징에 대해 자세히 알아보도록 하겠습니다!^^​​ 어음청각검사란?어음청각검사는 보청기 착용 효과를 평가하기 위해 Flectcher에 의해 1929년 미국에서 처음 사용하기 시작하였습니다. 또한 미국은 2차 세계대전 이후, 청력이 손상된 군인들의 치료와 재활을 위하여 좀 더 체계화된 어음을 이용한 보청기 평가 방법을 개발하였으며, 단음절, 이음절, 문장 등을 사용하여 어음인지역치, 단어인지도, 문장인지도를 측정하고, 중추청각장애(CAPD)를 평가할 수 있게 되었습니다. 이러한 어음청각검사는 현재 보청기뿐만 아니라 인공와우와 같은 청각보조장치의 착용 효과 및 청능 재활의 평가도구로 널리 사용되고 있습니다. ​추가적으로 어음청각검사에서 가장 많이 사용되는 어음인지역치, 단어인지도, 문장인지도 검사에 대해서 자세히 알아보도록 하겠습니다. ​​ 어음인지역치검사 (speech recognition thershold, SRT)제시된 이음절 단어(spondees, 양양격 단어)를 정확하게 50% 확인할 수 있는 가장 작은 강도(dB HL)를 측정하는 검사로 순음청력검사의 일치 여부를 확인함으로써 검사의 신뢰도를 확인하고, 어음 청취의 민감도를 측정할 수 있으며 단어인지도 검사의 기초자료로도 사용되고 있습니다. ​ 단어인지도검사 (word recognition score, WRS)어음청력검사 중 단어인지도 검사는 듣기에 가장 적절한 강도로 단어를 제시하였을 때, 정확히 이해하는 정도를 %로 측정하여 의사소통 장애의 정도, 청력 손실 병변 부위에 대한 정보, 보청기의 적응 및 선택, 보장구의 적합 및 재활, 재활의 평가와 계획, 중추청각처리장애 판별 및 재활 등에 필요한 정보를 제공할 수 있습니다. ​ 문장인지도검사 (sentence recognition score, SRS) 대상자가 듣기에 가장 적절한 강도 수준에서 문장을 제시했을 때, 정확히 문장을 확인할 수 있는 정도를 %로 측정하는 검사입니다(ASHA, 1988). 문장인지도검사는 일상생활에서 듣기 능력에 대한 실질적인 정보의 제공, 청력 손실 병변 부위에 대한 정보, 보장구 선택, 적합 및 재활, 재활의 평가와 계획, 중추청각처리장애 판별 및 재활 등의 여러 가지 목적으로 사용되고 있습니다. ​ ​​이상으로, 메디케어 보청기 잠실센터였습니다. 더 자세한 내용 및 보청기 관련 상담을 원하시면 잠실센터로 연락 주시길 바랍니다^^감사합니다.​*해당 내용은 Determining threshold level for speech (ASHA, 1988) 및 어음청각검사 (전문가 지침서) 에서 발췌하였습니다. ​​ 메디케어히어링 잠실센터서울특별시 송파구 올림픽로37길 130 파크리오 A상가 304호 ​ "
"음성(speech)신호의 log-mel과 log-mel delta, log-mel delta delta ",https://blog.naver.com/ssj860520/222818909695,20220719,"음성 신호 처리에서 많이 쓰이는..요즘은 모르겠지만 많이 쓰였다던 log-mel과 log-mel delta 그리고 log-mel delta delta 에 대해 알아보자.​log-mel과 delta시리즈는 음성신호에서 추출한 feature인데~ 사용할 경우 괜찮은 성능이 나타나서 많이 사용했던 것으로 보인다. log-mel을 정의하면 log-mel delta와 log-mel delta delta는 금방 유도가 되니 log-mel 부터 정의해보도록 하자.​speech s[n]이 이 있을때 s[n]은 frame 단위로 잘라서 사용한다. m번째 프레임 신호를 아래와 같이 아래첨자를 사용해 표현하자. 여기에 STFT ( 음성신호처리에서 푸리에 표현(Fourier Representation in speech) 를 적용하면 주파수상의 스펙트럼을 알수 있다. 기호로는 아래와 같이 대문자를 이용해 써보자. log-mel을 정의하려면 mel filter bank에 대해 알아야 되는데 mel filter bank에 대해서는 이 글 (Mel-spectrum-과-MFCC-Mel-Frequency-Cepstral-Coefficient의-의미 ) 을 보자.mel filter bank를 구성하는 i번째 필터에 대해서 log-mel을 정의할 수 있다. 이 log mel간의 difference를 구한 것이 log-mel delta이고 log-mel delta의 차를 구한것이 log-mel delta delta 이다. 위의 식은 log-mel delta이고 log-mel delta의 delta 가 log-mel delta delta이다 이렇게 log-mel, log-mel delta, log-mel delta delta는 음성분석의 feature로 사용된다고 한다. 차원을 한번 정리해보도록 하자. 위와 같이 세개의 축으로 표현되기 때문에 CNN을 이용할 수 있다고 한다~​출처M. Chen, X. He, J. Yang and H. Zhang, ""3-D Convolutional Recurrent Neural Networks With Attention Model for Speech Emotion Recognition,"" in IEEE Signal Processing Letters, vol. 25, no. 10, pp. 1440-1444, Oct. 2018, doi: 10.1109/LSP.2018.2860246. "
'전시안내로봇 큐아이' 국립중앙박물관의 슈퍼 히어로를 만나보세요! ,https://blog.naver.com/100museum/222985808614,20230116," '큐아이'는 음성인식과 자율주행 기능을 기반으로 관람객에게 전시실 및 전시품 등 문화 정보를 제공하는 똑똑한 전시안내 로봇입니다. 선사·고대관, 중·근세관, 실감영상관 등 주요 전시실(21개소)과 화장실 등 주요 시설까지 동행 안내한 뒤 전시 해설을 해줍니다. 외국인 관람객을 위해서는 다국어 서비스(영어, 중국어, 일본어)를, 청각 장애인을 위한 수어 해설도 제공하고 있습니다. 상설전시관에서 전시안내로봇 '큐아이'에게 전시 해설을 듣고, 편하게 대화를 나눠보세요!​QI is an AI-operated, self-driving robot designed to use speech recognition technology to help visitors navigate the exhibition halls and exhibits. QI can accompany visitors to the major exhibitions–a total of 21 rooms–such as Prehistory, Ancient History, Medieval History, Early Modern History and Immersive Digital galleries. The robot can provide a guided tour or show visitors the way to the restroom. QI services are available in three foreign languages (English, Chinese, and Japanese) along with a sign language function for the hearing-impaired. Find QI, our helpful robot, in the Permanent Exhibition Hall, get a guided tour or just chat with it!​“QI""是基于语音识别及无人驾驶功能向游客提供展览室、展品等文化信息的智能导览机器人。“QI""同行前往先史·古代馆以及中·近代馆、沉浸式影像馆等主要展览室(21个)和洗手间等主要设施后进行讲解。为外国人游客提供多国语(英语、汉语、日语)服务，还为听力残疾游客提供手语讲解服务。请在常设展览馆接受展览向导“QI” 的讲解服务，并与其随意交谈!​「QI」は音声認識と自動運転機能を基に観覧客に展示室及び展示品などの文化情報を提供する賢い展示案内ロボットです。先史・古代館、中・近世館、イマーシブデジタルギャラリーなど主要展示室(21ヶ所)とトイレなど主要施設まで同行案内し、展示解説を行います。外国人観覧客のためには多言語サービス(英語、中国語、日本語)を、聴覚障害者のための手話解説も提供しています。常設展示館で展示案内ロボット「QI」の展示解説を聞きながら、気軽に会話を楽しんでください！​#국립중앙박물관 #큐아이 #박물관로봇 #전시안내로봇 #전시해설로봇 #인공지능로봇 #AI로봇 #NationalMuseumofKorea #NMK #ROBOT #QI #SmartMuseum "
청력테스트 종류와 결과 해석해보기 ,https://blog.naver.com/smile_oticon/222941420598,20221129,"안녕하세요, 청각솔루션네트워크 히어링허브 분당센터 원종규 청각사입니다. 청력테스트는 어떤 종류가 있을까요?​청력테스트는 여러가지 종류가 있습니다. 크게 객관적인 청력테스트, 주관적인 청력테스트로 나눌 수 있는데요~ 오늘은 주관적인 청력테스트의 종류와 그 결과를 어떻게 해석하는지 확인해 보도록 하겠습니다. 순음청력검사주관적인 청력테스트의 가장 대표적인 검사입니다. 가장 대중적으로 많이 시행되기 때문에 건강검진에서 누구나 한번씩은 받아보았을 겁니다. 방음부스 안에 들어가서 헤드폰을 착용한 상태로 소리가 들리면 버튼을 누르는 검사입니다. 250Hz 부터 8KHz의 순음을 사용하여 검사하며 기도청력검사와 골도청력검사 두가지로 나누어집니다. 기도청력검사는 헤드폰을 사용하거나 인서트이어폰을 사용하고, 골도청력검사는 골진동체를 사용하여 검사합니다. 기도청력검사는 O,X로 오른쪽 왼쪽을 구분하며, 색으로 오른쪽은 빨간색, 왼쪽은 파란색으로 표기합니다. 여러가지 방법이 있지만 수정상승법이란 방식을 사용하여 검사하며, 30dB 부터 검사를 시작합니다. 하지만 난청이 의심되는 환자들의 경우 50dB 부터 시작하는 경우도 있으며 처음 소리를 듣는 크기를 찾을때 까지는 20dB씩 올려주며 검사하고 소리를 들으면 10dB 낮추고 못들으면 5dB을 올리는 방식이 수정상승법입니다. 3번의 자극음 중 2번을 들으면 그 지점을 역치로 산정합니다. 청력도를 보시면 세로축이 데시벨, 가로축이 주파수입니다. 각 주파수별로 청력역치를 검사하는 순음청력검사는 난청의 정도를 파악하는데 의미가 있습니다. 데시벨 기준으로 20dB 정상, 40dB 경도난청, 55dB 중도난청, 70dB 중고도 난청, 90dB 고도난청, 90dB 이상은 심도난청으로 구분합니다. 보청기를 착용하기 전 시행하는 순음청력검사가 중요한 이유는 보청기의 초기적합에 사용되기 때문입니다. 각 보청기 제조사는 각자 다른 어음처리기법을 가지고 있고, 순음청력검사 결과를 바탕으로 보청기에서 출력하는 소리의 크기를 정하기 때문에 정확하게 순음청력검사를 시행해야 보청기 소리를 불편함 없이 셋팅할 수 있습니다. 어음청력검사순음청력검사와 비슷하나 순음 대신 단어로 검사하는 방법입니다. 총 3가지의 검사로 나누어지는데, 어음청취역치검사, 어음탐지역치검사, 어음인지도검사입니다. 하나씩 설명드리도록 하겠습니다. 1. 어음청취역치검사(Speech Recognition Threshold test)어음청취역치는 이음절 단어를 사용하여 검사합니다. 3개의 단어를 들려주고 그 중에서 2개를 알아맞추는 지점을 어음청취역치로 지정합니다. 검사음의 시작강도는 순음청력검사 평균값에 +30dB 로 시작하며 2개를 맞추는 최소의 소리크기 지점을 찾는 것으로 이 검사의 목적은 순음청력검사의 신뢰도를 확인하는데 있습니다. 순음청력검사역치와 평균적으로 10dB 이내의 차이를 보이면 신뢰도가 높은 검사결과라고 볼 수 있습니다. 하지만 유소아나 장애등으로 인해 정확하게 발음을 하는 것이 어려운 경우 시행하는 검사는 따로 있습니다.​2. 어음탐지역치검사(Speech Detection Threshold test)어음탐지역치검사는 단어의 존재 유무를 탐지만 하는 검사방법입니다. 단어가 무슨 소리인지 알아맞추지 못하더라도 단순히 탐지가 가능하다면 역치로 산정하는 검사법으로 SRT 검사가 불가능한 경우 시행하는 검사입니다. 보통 순음청력검사역치와 15dB 이내에서 일치하면 신뢰도가 높은 검사라 보고있으며 SRT 검사와는 10dB 정도 더 좋은 역치를 가지게 됩니다. 3. 어음인지도검사(Words Recogniton Score) 어음인지도검사는 다른 검사법들처럼 역치를 체크하는 검사가 아닙니다. 최적수준에서의 한음절로 된 단어를 들었을 때 그 단어를 정확하게 인지하는 점수를 측정하는 검사법으로 보통 20~25개의 단음절어를 가지고 검사하게 됩니다.​어음재인도 검사의 활용은 보청기 평가, 난청 병변부위 판별의 목적이 있습니다. 보청기를 착용 후 어느정도의 말소리 인지 능력을 가질 수 있는지 예측할 수 있습니다. 최적수준에서 검사하는 방법이기 때문이죠. 또한 난청의 병변부위, 감각성 난청인지, 신경성 난청인지 확인할 수 있는 중요한 검사이기도 합니다. 최적수준에서 측정한 검사결과값이 최적수준을 넘어가는 순간 오히려 점수가 떨어지는 현상, Roll over 라고 부르는 현상이며 이 현상이 나타나는 환자들은 보통 신경성난청, 즉 달팽이관 이상의 신경부위의 손상으로 인한 난청으로 판별할 수 있습니다.  특히 보청기를 상담할 때 환자의 보청기를 통한 이득이 얼마나 될지 유추할 수 있는 중요한 검사법이기에 보청기센터에서 필수적으로 시행해야 되는 검사법이기도 합니다.  이상 주관적인 청력테스트 방법들을 정리해 보았는데요~ 청력은 나빠지면 다시 돌아오지 않기 때문에 난청이 있다고 인지하는 그 순간부터 추적관찰이 필요합니다. 조기의 난청 중재 또한 매우 중요하기 때문에 난청이 있으시다면 얼른 보청기나 다른 청각보조장치의 도움을 받아 더이상 귀가 나빠지지 않게 예방하는 것이 가장 중요합니다.​감사합니다. 오티콘보청기 분당점경기도 성남시 분당구 성남대로331번길 3-13 대명제스트빌딩 ​ "
ClovaCall: Korean Goal-Oriented Dialog Speech Corpus for ASR of Contact Centers ,https://blog.naver.com/sooftware/221928847969,20200425,"ClovaCall: Korean Goal-Oriented Dialog Speech Corpus for Automatic Speech Recognition of Contact Centers   논문링크2020-04-20에 클로바에서 공개한 따끈따끈한 논문입니다. 한국어 음성 데이터셋 공개와 더불어 베이스라인 코드와 AI Hub, 공개한 데이터셋에 대한 학습 결과까지 포함하고 있습니다. 현재 제가 진행하는 프로젝트와 완전히 동일한 주제이면서도 같은 데이터셋 (AI Hub) 을 이용한 학습까지 했다고하니 굉장히 기대하면서 읽었습니다. 게다가 논문을 낸 기관이 네이버 Clova여서 더 기대가 됐네요.​​​Abstract​ASR은 여러 어플리케이션에서 필수적입니다만, License-free한 데이터는 찾기 힘들고, 유명한 Switchboard와 같은 데이터는 조금 구식입니다. 또한 가장 큰 문제점은 이러한 오픈 데이터는 대부분 영어로 이루어져 있다는 점입니다. 그래서 본 논문에서는 대용량의 한국어 음성 데이터셋을 공개한다고 밝힙니다. 11,000명의 화자에게서 얻은 60,000 쌍의 음성 데이터셋입니다. (1,000시간의 AI Hub 데이터셋이 2,000명의 화자로 이루어진 점과 비교하여 다양한 화자들로 구성된 데이터셋임을 알 수 있습니다.) 또한 해당 데이터셋을 검증하기 위한 베이스라인 코드를 같이 공개했습니다. 해당 데이터셋 및 코드는 이곳에 공개되어 있습니다.​​​1. Introduction​음성인식 서비스는 다양한 분야에 적용가능한 핵심 기술입니다. 하지만 이러한 음성인식 서비스 개발을 위한 오픈 데이터들은 (Wall Street Journal (WSJ), TIMIT, Switchboard, CallHome, Librispeech) 공개된 지 오래된 구식의 데이터들이며, 모두 영어로 이루어져 있습니다. 물론 AI Hub와 zeroth와 같이 오픈된 한국어 음성 데이터셋도 있지만, 이 데이터셋은 일상대화와 같은 주제들을 다룬 데이터셋입니다. 이러한 데이터셋들을 사용하게 되면 특정 도메인을 타겟으로 한 태스크에서는 성능이 좋지 않습니다. 그래서 본 논문에서는 음식점을 도메인으로한 데이터셋을 공개했다고 밝힙니다. 대부분의 문장은 10초 이내의 짧은 발화로 구성되어 있으며, End-point detection이나 alignment가 이슈가 되지 않는다고 합니다.또한 본 논문에서는 이러한 데이터셋이 어느 정도의 성능을 낼 수 있는지를 보이기 위해 Deep Speech 2 (DS2)와 Listen, Attend and Spell (LAS)의 2개의 모델로 실험을 진행했다고 밝힙니다. 이때 Pretraining-finetuning, from-scratch training, scratch training with data augmentation 의 3가지 방법을 통해 비교했다고 합니다. 이에 더하여, 비교를 위해 이미 공개되어 있는 2개의 한국어 음성 데이터셋에 대해서도 같이 실험을 진행했습니다. 2개의 데이터셋은 QA Dataset (task-specific)과 AI Hub Dataset (dialog)를 의미합니다.​​​2. Related Work​본 장에서는 기존의 여러 데이터셋들에 대해 소개하고 있습니다. WSJ, TIMIT, Switchboard, CallHome, LibriSpeech 등의 데이터셋을 소개하고 있으며, 해당 데이터셋들은 여러 ASR 모델들의 성능을 비교하는 벤치마크로 사용되고 있습니다. 하지만 다시 강조하듯이, 이러한 데이터셋들은 일상 대화라는 주제를 가진 데이터셋이며, 특정 도메인을 주제로한 데이터셋들은 거의 오픈되지 않습니다. 또한 이렇게 오픈된 일상 대화 데이터셋들은 특정 도메인을 타겟으로 한 모델의 Pretraining 데이터셋이 될 수 있지만, 일상 대화라는 도메인과 특정 도메인을 타겟으로하는 데이터셋은 서로 꽤나 상이하여 Pretraining 하더라도 좋은 결과를 내지 못하고 있다고 합니다.​​​3. Korean Clova Call Speech Corpus​​3.1 Naver Clova AI for Contact Center​ClovaCall 데이터셋 구축은 NAVER Clova AI for Contact Center (AICC) 프로젝트의 메인 주제였습니다. ClovaCall 데이터셋은 자연어 이해, 음성인식, 음성합성 등에 활용될 수 있으며, '음식점 예약'이라는 시나리오를 주제를 목적으로한 대용량 데이터셋임을 다시 한번 강조합니다.​3.2 Data Cconstruction from Humans​데이터 구축 과정에 대해 설명합니다. 데이터 구축은 다음 과정을 거쳤다고 합니다.making a Sentence Pool (어떤 문장들로 데이터셋을 구성할지)call-based recording (전화상으로 들어온 소리를 녹음)refining the recorded speech data (데이터 정제)자세한 내용은 논문을 참고해주시면 되겠습니다.(데이터 구축 과정이 제 관심분야는 아닌지라 ㅎㅎ;;)​3.3 Statistical Analysis  본 논문에서는 ClovaCall 데이터셋과 AI Hub 데이터셋을 분석한 결과를 보이고 있습니다. 단어, 문자, 음소, 발화 길이를 분석하여 막대그래프로 표현했습니다.​​​4. Speech Recognition Result​이제 드디어 제가 관심있는 파트가 등장했습니다.​4.1 Experimental Setup​Dataset & Training Schemes앞서 언급했듯이, Pretraining-finetuning, from-scratch training, scratch training with data augmentation와 같은 3가지 방식으로의 학습방식 결과를 비교했습니다.​데이터셋은 총 3가지를 사용했습니다.  1.  AI Hub for Pretraining  2.  QA Call for Finetuning  3.  Clova Call for FinetuningAI Hub 데이터셋은 Pretraining을 위해 사용되었고, QA Call 데이터셋은 Clova Call 데이터셋과의 성능 비교를 위해 사용되었습니다. 또한 Noise-Augmentation & Spec-Augmentation 2가지 기법을 각자 사용하여 결과를 비교했습니다.​Data preprocessing먼저 AI Hub 데이터셋과 CLova Call 데이터셋의 샘플링 레이트가 서로 다릅니다. AI HUb는 16k의 샘플링 레이트를 가지는데 반해, Clova Call 데이터는 통화로부터 수집하다보니, 8k의 샘플링 레이트를 가집니다. 그래서 Clova Call 데이터를 Upsampling을 통해 8k => 16k의 샘플링 레이트를 가지도록 수정했습니다. 그리고 모든 모델은 log-spectrogram을 인풋으로 가지며, 20ms의 window_size와 10ms의 stride를 가집니다. (librosa 라이브러리를 사용했습니다.) 또한 모든 스펙트로그램은 instance-wise standardization 방식으로 정규화를 진행했습니다.​개인적으로 왜 Mel-Spectrogram이 아닌, 그냥 Spectrogram을 사용했는지에 대해 궁금증이 남아서, 공동 제 1저자 분 중 한분께 메일로 문의를 드렸습니다. 혹시 답변 해주신다면 답변 내용도 남기겠습니다.​ASR ModelsASR 모델로는 DeepSpeech2, Listen, Attend and Spell 2개의 아키텍처로 비교했습니다.DeepSpeech2 아키텍처는 2D-Convolutional layer with 32 channel를 포함하고 있으며, 5개의 Bidirectional LSTM layer로 구성되어 있습니다. (각 방향당 800개의 unit을 사용했습니다.) 또한 트레이닝은 CTC loss를 이용하여 학습했다고 합니다.​LAS 아키텍쳐는 DeepSpeech2와 같은 Convolution layer를 포함시키고, 각 방향당 512개의 unit을 가진 3층의 Bidirectional LSTM layer로 인코더를 구성하며, 512개의 unit을 가진 2층의 Unidirectional LSTM layer로 디코더를 구성했다고 합니다. 또한 어텐션 매커니즘으로는 「Attention based Models for Speech Recognition」- Review Link에서 제안한 Location Aware 어텐션 매커니즘을 사용했습니다.​모델 평가 지표로는 Character Error Rate (CER)을 사용했으며 다음과 같은 Metric을 따릅니다.  ​4.2 Comparison Results on Datasets  이제 결과를 비교하는 시간입니다. 결과만 놓고보자면, AI Hub 데이터셋으로 Pretraining 한 후, ClovaCall 데이터셋으로 Finetuning한 LAS 모델이 가장 좋은 성능 (Error Rate 7%) 을 냈습니다.또한 주목할만한 점은, AI Hub 데이터셋만으로 학습시킨 모델의 경우, Specific-domain의 데이터로 테스트 했을 때, 성능이 매우 저조했다는 점입니다. 가장 좋은 성능을 낸 LAS 모델로 비교를 하자면 Error Rate 69.2%를 기록했네요.또한 본 실험에서는 Data Augmentation이 의미있는 결과를 내지 못했습니다. 본 논문에서는 이를 이미 노이즈가 낀 상황에서 데이터를 만들었다는 점과, 파라미터 수가 작은 LAS 모델에서 Spec Augmentation을 적용했다는 점이 의미있는 효과를 내지 못한 점으로 꼽았습니다.​​​5. Concluding Remarks​결국 이 논문의 핵심을 특정 도메인을 대상으로 한 ASR 시스템을 구축하기 위해서는 해당 도메인의 데이터셋이 필요한데, 저희가 그런 '음식점 예약' 도메인에서의 큰 데이터셋을 공개했고, 이 데이터셋을 사용했더니 결과가 좋았습니다 ! 를 강조한 논문이였습니다. "
영어로 물어봐야 더 많이 알려주는 챗지피티(Low-resource VS High resource) ,https://blog.naver.com/nicsdiary/223074986674,20230414,"Large language models like GPT-3 and its successor, GPT Four, face a challenge due to the scarcity of certain languages in NLP research. These languages are categorized as low-resource languages, and only 20 of the approximately 7,000 spoken languages worldwide are considered high-resource languages. The majority of NLP research is focused on high-resource languages, leaving low-resource languages unintelligible to AI. Researchers are working on creating new datasets to comprehend low-resource languages like Jamaican Patois, not to generate text, but to teach the models to understand them. ** ""low-resource"" refers to languages that have limited digital resources available for NLP research, such as text corpora, dictionaries, or language models. These languages are typically less common or less widely spoken, and therefore, have received less attention from NLP researchers compared to high-resource languages. As a result, building NLP systems that can effectively process low-resource languages is a challenging task.​Leaving low-resource languages unintelligible means that people who speak those languages will not benefit from NLP technology. NLP technology has the potential to enhance communication, break down language barriers, and create new opportunities for people in different parts of the world. However, if the models are only trained on high-resource languages, they will not be able to understand or generate text in low-resource languages. This can lead to a digital divide, where people who speak low-resource languages are left behind in the digital age, unable to access the benefits of modern technology. Furthermore, it can perpetuate linguistic and cultural biases, as the models will only be trained on a limited set of languages and perspectives. Therefore, it is important to include low-resource languages in NLP research and ensure that the benefits of the technology are accessible to everyone.​NLP related companies can take several steps to decrease the gap between low-resource and high-resource languages.​Firstly, they can invest in collecting more data on low-resource languages by collaborating with local communities and organizations. This data can be used to develop language models and datasets for these languages, which can then be integrated into NLP systems.​Secondly, they can explore transfer learning techniques that allow the knowledge learned from high-resource languages to be applied to low-resource languages. This approach can help overcome the scarcity of data on low-resource languages by leveraging the existing knowledge from high-resource languages.​Thirdly, NLP related companies can focus on developing technologies that can support the creation and distribution of content in low-resource languages. This can include technologies that facilitate translation, content creation, and speech recognition.​Finally, they can work towards making their systems more accessible and inclusive by supporting more languages and dialects, and by developing technologies that are tailored to the needs of different communities. By taking these steps, NLP related companies can help to reduce the gap between high-resource and low-resource languages and promote more equitable access to NLP technologies for people around the world. "
"음성 AI 기술 종류 설명 | STT, TTS, SER, KWS 등... ",https://blog.naver.com/hannaurora/223096602629,20230509,"이 글은 음성을 input으로 받는 음성 AI 기술 종류에 대한 정보입니다.기술명, 축약어를 알아야 자료를 찾을 때 편하니까요!(역시 제가 나중에 보려고 기록하는 글.. 히히)​test_INPUT, test_OUTPUT은 모델 테스트 단계에서 받는 데이터와 내뱉는 결과에 대한 예시입니다.음성에는 자연어 데이터가 담겨있기 때문에 NLP, NLU를 추가적으로 작성하였습니다. 기술명축약어한글test_INPUTtest_OUTPUT설명Speech-to-Text or (ASR: Automatic Speech Recognition)STT음성텍스트변환(아무나의)음성텍스트음성을 텍스트로 변환Text-to-SpeechTTS텍스트음성변환텍스트(학습에 사용한) 음성텍스트를 음성으로 변환Voice Cloning or (Speech Synthesize)N/A음성복제텍스트(특정인물의)음성특정 인물의 음성 데이터를 소량으로 넣어서 복제Natural Language ProcessingNLP자연어처리텍스트목적에 맞는 형식기계가 인간의 언어를 이해하고 적절하게 응답하는 것Natural Language UnderstandingNLU자연어이해텍스트목적에 맞는 형식이해하는 것에 초점을 맞춘 기술, NLP중 하나Speaker RecognitionSR화자식별(누군가의)음성(사전 선택지중)인물A음성 기반으로 화자를 식별Voice Activity Detection or (SD: Speech Detection)VAD음성검출음성이 섞인 오디오0 or 1오디오에서 음성 유/무를 검출Keyword Spotting or Word SpottingKWS핵심어 검출음성(사전에 정의한)핵심어입력되는 음성에서 사전에 정의한 핵심어를 검출Speech EnhancementSE음성향상저품질 음성고품질 음성잡음, 잔향을 제거하고 음성의 품질을 향상하는 것Speech SeparationSS음성분리음성이 섞인 오디오여러 음성 분리단일 오디오에서 여러 음성을 분리, 혹은 잡음이나 음악에서 분리Speech Emotion Recognition or Emotion DetectionSER음성감정인식음성(사전에 정의한)감정A음성에서 화자의 감정을 인식Sentiment AnalysisSA감성인식음성(인터뷰, 고객 리뷰 등)감정 레이블(긍정, 부정, 중립)특정 주제에 대한 감정을 인식하는 것으로 주로 찬성/반대, 좋음/싫음 등의 2진 분류. 신제품에 대한 고객의 반응 등으로 쓰일 수 있음Speech to Speech TranslationS2ST음성번역한국어 음성영어 음성음성을 다른 언어의 음성으로 변환Voice-based diagnosisVBD음성기반질환진단음성질병의 정도, 질병 종류음성으로 질병의 진전 정도와 음성/양성 여부를 판단 ​ "
speech commands(미완) ,https://blog.naver.com/myunyui/222082766210,20200907,"Speech Commands: A Dataset for Limited-Vocabulary Speech Recognitionhttps://arxiv.org/pdf/1804.03209.pdf​Introduction일차적인 목표는 배경 소음이나 관련 없는 말에서 가능한 한 적은 수의 잘못된 긍정으로, 10개 이하의 표적 단어 집합으로부터, 단어의 구어 시기를 감지하는 작은 모델을 만들고 시험하는 방법을 제공하는 것 Related Work라벨은 문장 수준에서만 정렬되므로 단어 수준 정렬 정보가 부족하다. 이를 통해 키워드 스팟팅보다 완전 자동 음성 인식에 적합하다. Motivations대부분의 음성 인터페이스는 전화나 다른 장치에서 로컬로 인식 모듈을 실행한다. 이것은 마이크로부터의 오디오 입력을 지속적으로 청취하며, 인터넷을 통해 데이터를 서버로 전송하기 보다는 원하는 트리거 구문을 청취하는 모델을 실행한다.일단 가능한 트리거가 들리면, 오디오를 웹 서비스로 전송하기 시작한다. 로컬 모델은 웹 서비스 제공자의 통제 하에 있지 않은 하드웨어에서 실행되기 때문에 온디바이스 모델이 존중해야 하는 하드 리소스 제약이 있다.일반적으로 존재하는 모바일 프로세서가 대부분의 서버보다 훨씬 낮은 총 컴퓨팅 기능을 가지고 있기 때문에 대화형 응답을 위해 거의 실시간으로 실행되려면 온디바이스 모델은 클라우드 동급 제품보다 더 적은 계산을 요구해야 한다. 즉, 모바일 기기는 배터리 수명이 제한되어 있고 연속적으로 실행되는 것은 매우 에너지 효율이 높아야 한다. 이러한 고려사항은 로컬 모델에서 사용할 수 있는 에너지 양을 제한하는 열 방출량에 열 제약이 있으며, 에너지 스타와 같은 프로그램에서 전체 전력 사용량을 최대한 줄이도록 권장된다. 종적인 고려사항은 사용자들이 단말기에서 빠른 응답을 기대하며, 전체 서버 응답이 지연되더라도 좋은 경험을 위해서는 명령어를 수신했다는 초기 확인이 중요하다.  • 키워드 탐지 모델은 더 작고 더 적은 컴퓨팅을 포함해야 한다.• 매우 에너지 효율적인 방식으로 운영되어야 한다.• 입력의 대부분은 말이 아닌 침묵 또는 배경 잡음이 될 것이므로, 이러한 입력에 대한 잘못된 긍정을 최소화해야 한다.• 음성인 대부분의 입력은 음성 인터페이스와 무관하므로 모델이 임의의 음성에서 트리거되지 않아야 한다.• 인식의 중요한 단위는 전체 문장이 아니라 단어나 짧은 구절이다. 이러한 차이는 기기 내 키워드 스포팅2와 일반 음성인식 모델 간의 훈련과 평가 과정이 상당히 다르다는 것을 의미한다. 모질라의 '공통 음성'과 같은 일반적인 음성 작업을 지원하는 일부 유망 데이터셋이 있지만, 키워드 스팟팅에 쉽게 적응하지 못하고 있다.  Collection Requirements모든 발언을 표준 시간인 1초로 제한하기로 결정했다.또한 문장의 일부로서가 아니라 고립된 단어로만 쓰기로 결정했다.우리가 목표로 하는 트리거 단어 과제와 더 밀접하게 닮았기 때문이다. 그것은 또한 정렬이 그렇게 중요하지는 않기 때문에 라벨 부착을 훨씬 더 쉽게 한다. Word Choice우리 어휘의 핵심으로 20개의 흔한 단어를 선택하게 되었다. 여기에는 0부터 9까지의 숫자가 포함되었으며, 버전 1에서는""Yes"", ""No"", ""Up"", ""Down"", ""Left"", ""Right"", ""On"", ""Off"", ""Stop"", and ""Go""의 명령어로 유용할 10개의 단어가 포함되어 있다. 데이터 집합 버전 2에서 “Backward”, “Forward”, “Follow”, and “Learn”의 네 가지 명령어를 추가했다.""나무""와 같은 몇몇은 목표 단어와 비슷하게 들리고 모델의 분별력을 시험하는 좋은 시험이기 때문에 선택되었다. 다른 것들은 많은 다른 음소를 포괄하는 짧은 단어들로 임의로 선택되었다. 최종 리스트는 ""Bed"", ""Bird"", ""Cat"", ""Dog"", ""Happy"", ""House"", ""Marvin"", ""Sheila"", ""Tree"", and ""Wow"". Implementation웹사이트에서 사용자의 동의를 받아(마이크 허용) 녹음 진행이 과정에서 같은 단어가 여러 번 반복되면서 발생할 수 있는 발음 변화를 피하기 위해 녹음된 단어 중 무작위 순서가 선택됐다.녹음 과정이 완료되면 사용자는 모든 클립을 검토하고, 클립이 마음에 들면 업로드하도록 요청 받는다. 그러면 오디오를 서버 응용프로그램에 업로드하는 웹 API가 호출되고, 이 웹 API가 이를 클라우드 저장 버킷에 저장한다. Quality Control(품질관리)데이터의 품질을 높이기 위해 수집된 음성 중에서 다른 사람이 알아들을 수 없는 단어이거나 다른 단어로 들린다면 그 음성은 제외하였다. 그리고 5KB보다 작은 파일은 정확하지 않을 것이라 판단하여 이 역시 제외하였다. Manual Review 녹음된 음성을 제3의 참가자에게 들려주고 타이핑하도록 하여 일치하지 않는 데이터는 제외하였다 Background Noise 실제 제품에서 키워드를 포착하기 위한 핵심 요건은 음성이 포함된 오디오와 없는 클립을 구분하는 것이다. 이를 훈련하기 위해, 나는 여러 종류의 배경 소음이 담긴 몇 분 길이의 16KHz WAV 파일을 추가했다. 이들 중 몇몇은 예를 들어 흐르는 물이나 기계와 같은 소음 환경에서 직접 기록되었다. Evaluation이 데이터 집합의 주요 목표 중 하나는 서로 다른 모델의 결과 간에 의미 있는 비교를 가능하게 하는 것 데이터 집합에 존재하는 각 단어의 기록 수  Top-One Error나는 모델이 언어가 없을 때, 그리고 그것이 인식하지 못하는 단어가 말되었다고 생각하는 때를 구분해서 나타내기를 원한다.이러한 ""개방형"" 범주는 실제 적용에서 예상되는 발생에 따라 가중치가 부여되어야 결과의 인지된 품질을 반영하는 현실적인 측정기준을 산출할 수 있다. 텐서플로우 스피치 명령 예제 코드에 대해 선택한 표준은 ""예"", ""아니오"", ""위"", ""아래"", ""좌"", ""우"", ""온"", ""오프"", ""스톱"", ""고""의 10개 단어를 검색하고 ""미지의 워드""에 대한 특수 라벨을 1개, ""무언어""(무언어)에 대한 특수 라벨을 1개, 또 다른 ""무언어""에 대해 탐지(무언어)하는 것이다. 그런 다음 시험은 12개 범주 각각에 대해 동일한 수의 예제를 제공하여 수행되며, 이는 각 등급이 전체에서 약 8.3%를 차지함을 의미한다. 데이터 집합[15]에 수반되는 예시 훈련 코드는 이 측정지표에 대한 88.2%의 결과를 제공한다. 이는 질적으로는 합리적이지만 완벽한 대응과는 거리가 먼 모델로 해석되므로, 보다 정교한 아키텍처가 이를 능가하는 기준선 역할을 할 것으로 기대된다. Streaming Error Metrics Top-One은 결과의 인식된 품질의 단일 차원을 캡처하지만 실제 애플리케이션에서 성능의 다른 측면에 대해서는 별로 드러내지 않는다.└단일 차원 캡쳐의 의미 - 예를 들어, 제품의 모델들은 연속적인 오디오 데이터에서 단어가 언제 시작되고 언제 끝나는지 모르는 반면, Top One은 단어의 시작을 알 수 있다. 전체 점수에서 각 범주의 동일한 가중치는 일반적인 환경에서 트리거 단어와 침묵의 분포를 반영하지 않는다. 이렇게 더 복잡한 모델의 속성 중 일부를 측정하기 위해, 나는 그것들을 연속적인 오디오 스트림과 비교하여 테스트하고 여러 메트릭스에 점수를 매긴다. V2 데이터를 사용하여 학습한 기준선 모델의 결과:   "
ChatGPT와 영어회화 공부 ,https://blog.naver.com/samuelknj/223044276931,20230314,"크롬 웹스토에서  Talk-to-GhatGPT를 설치 (웨일 브라우저에서 사용중) 2. chat.openai.com/chat에 접속하면 오른쪽 상단에 못보던 게 생김 3. START를 누르면 아래 모양으로 뭔가가 생김 4. 맨 오른쪽 기어 모양을 누르면? 5. 셋팅으로 간다 6. Speech recognition language를 English - en-US로 선택 후 save 7. Automatically send message to ChatGPT after speaking의 체크를 해제하면, 화자가 말하는 중간에 버벅대도 AI가 중간에 말을 끊어먹지 않음​8. 기타등등을 설정하고 저장 대화 시작​기타 유의할 점은아는게 많아서 대답할 때 기~~일게 대답하니까, 기다리다 지칠 수 있음.     따라서, 짧게 대답해 달라고 한 번 얘기하고 나서2. 같은 얘기를 관용표현으로 해 달라던가3. 틀린말을 하면 고쳐달라던가​등의 요구사항을 적절하게 얘개해 주면 찰떡 같이 알아듣고 반영 해 줌.  이상​​​​​​​ "
Boost Customer Satisfaction: Explore the Best IVR Systems in 2023 ,https://blog.naver.com/office24by7/223112470757,20230526,"In the modern business landscape, customer satisfaction plays a pivotal role in the success of any organization. As businesses strive to provide exceptional customer service, they increasingly use advanced technology solutions. Interactive Voice Response (IVR) systems have emerged as a powerful tool for streamlining customer interactions and enhancing overall satisfaction. ​This comprehensive guide will explore the best IVR systems available in 2023 and delve into the benefits they offer to businesses. We will also highlight the advantages of hosted IVR services provided by Office24by7, a leading hosted IVR services provider. ​By understanding the capabilities of these systems and the value they bring, businesses can optimize their customer service processes and achieve higher levels of customer satisfaction. ​1: Understanding the Importance of IVR Systems​A.The Role of IVR Systems in Customer ServiceIVR systems are automated telephony systems that allow businesses to interact with customers through pre-recorded voice prompts and keypad inputs. They play a vital role in customer service by efficiently handling routine inquiries, providing self-service options, and routing calls to the appropriate departments. IVR systems streamline customer interactions, reduce wait times, and improve overall service efficiency.​B.Benefits of Implementing IVR Systems​By deploying an effective IVR system, businesses can reap several benefits, including:​Improved Call Routing: IVR systems can intelligently route calls based on customer inputs, ensuring that calls are directed to the most appropriate agent or department. This leads to quicker resolutions and reduces customer frustration.​24/7 Availability: IVR systems can provide round-the-clock service, allowing customers to access information and perform actions anytime. This availability contributes to increased customer satisfaction and loyalty.​Call Volume Management: IVR systems can handle a high volume of calls simultaneously, ensuring customers don't experience long wait times or get stuck in queues. This efficient call management improves customer experience and reduces abandonment rates.​2: Exploring the Best IVR Systems in 2023​A.Hosted IVR Services: An OverviewIn recent years, hosted IVR services have gained significant popularity due to their cost-effectiveness, scalability, and ease of implementation. Hosted IVR solutions, such as those provided by Office24by7, eliminate the need for businesses to invest in expensive hardware or employ dedicated IT resources. Instead, the IVR system is hosted in the cloud, and businesses can access its features and functionalities through a secure online portal. This approach offers numerous advantages, making hosted IVR services a preferred choice for businesses of all sizes.​B.Key Features to Look for in the Best IVR Systems​When evaluating IVR systems for your business, it is crucial to consider the following key features:Call Routing and Transfer: Look for IVR systems that offer intelligent call routing basedon customer inputs, ensuring calls are directed to the most appropriate destination.​Integration with CRM Systems: Seek IVR systems that seamlessly integrate with your existing CRM software, allowing personalized customer interactions and access to relevant customer data.​Speech Recognition: Advanced IVR systems utilize speech recognition technology to understand and respond to customer voice commands, enhancing the self-service experience.​C. Office24by7 Hosted IVR ServicesOffice24by7 offers industry-leading best hosted IVR services, providing businesses with a robust and flexible solution for customer service needs. Some of the standout features of Office24by7's hosted IVR services include:​Customizable Call Flows: Office24by7's IVR system allows businesses to design and customize call flows based on their unique requirements. This flexibility ensures a seamless and personalized customer experience.​Advanced Call Routing: The IVR system intelligently routes calls to the most appropriate agents or departments, minimizing wait times and improving first-call resolution rates.​CRM Integration: The IVR system seamlessly integrates with leading CRM platforms, allowing businesses to access customer data in real time and provide personalized interactions.​3: Tips for Maximizing the Benefits of IVR Systems​A. Tailor IVR Prompts for Clarity and Ease of UseCraft clear and concise IVR prompts to effectively guide customers through the self-service options. Use natural language, provide clear instructions, and offer keypad shortcuts to simplify the navigation process.​B. Provide an Option to Speak with a Live AgentWhile self-service options are essential, offering customers the ability to speak with a live agent if needed is crucial. Include a ""speak to an agent"" option in your IVR menu to cater to complex inquiries or situations that require human assistance.​C. Implement Speech Recognition TechnologyConsider implementing speech recognition capabilities in your IVR system to give customers a more intuitive and natural interaction experience. Speech recognition allows customers to navigate the system using voice commands, enhancing self-service options.​ConclusionIn 2023, implementing the best IVR system is crucial for businesses looking to boost customer satisfaction and streamline their customer service operations. Like those provided by Office24by7, best Hosted IVR services offer a cost-effective and scalable solution that eliminates the need for significant infrastructure investments.​By leveraging the key features of IVR systems and optimizing their implementation, businesses can enhance call routing, provide self-service options, and personalize customer interactions. With the right IVR system, businesses can elevate customer satisfaction levels, improve operational efficiency, and gain a competitive edge in the market. ​Explore the best IVR systems in 2023, including Office24by7's best  hosted IVR services, and unlock the full potential of exceptional customer service.Contact as on : sales@office24by7.com "
내 목소리 듣고 있니? 도청하는 AI 막는 AI ,https://blog.naver.com/tech-plus/222763127662,20220607,"giphy‘빅 브라더가 당신을 감시하고 있다’. 빅 브라더(Big brother)는 조지 오웰의 디스토피아 소설 ‘1984’에 등장하는 가상 국가 오세아니아의 최고 권력자를 말합니다. 해당 캐릭터에서 비롯된 용어로 개인 정보를 독점해 사회를 통제하는 권력이란 뜻도 담겨있죠. ​AI(인공지능) 기술이 발달한 요즘은 우리도 모르게 빅브라더에 정보를 제공하는 일이 흔하다고 합니다. 회사가 직원의 말을 듣기 위해 ‘보스웨어’를 사용하기도 하고, ‘스파이웨어’ 앱으로 전화 통화 내역을 감시하는 이들도 있죠. ​ wiredAI 스피커, 우리 대화 듣고 있다?​AI 스피커도 대표적입니다. 몇 년 전 아마존 AI 스피커 에코(Echo)는 도청 가능성에 대한 논란에 휩싸인 바 있어요. 가정 내에서 일어난 일상 대화를 녹음, 직장 동료들에게 전송한 사건이 발생했기 때문이죠. ​당시에는 해당 사건이 단순 오류, 우연이란 말도 있었지만, 한편으론 섬찟하게 느껴지기도 했습니다. 사용자가 명령을 내리지 않아도 AI가 작동할 수 있다는 걸 알게 되었기 때문인데요. ​보안 매체 MRP 연구소는 “에코가 도청 기기로 변할 수 있다”라고 경고했으며 리눅스 운영체제의 최고 권한을 획득하면 물리적 증거 없이 악성코드를 설치해 음성을 원격 스트리밍, 도청하기 쉽다고 설명했죠. ​ AI 도청 막는 AI​한마디로 아마존의 에코와 같은 AI 기기가 도청 기기로 전략할 가능성이 충분하다는 건데요. 컬럼비아 대학교(Columbia University) 연구팀은 이를 막기 위한 시스템을 연구해 왔습니다. 완벽히 도청 가능성을 차단할 순 없지만, 녹음된 음성들이 들리지 않게 만드는 거죠. ​연구진은 ‘신경망 음성 위장 기술(Neural Voice Camouflage)’을 사용했는데요. ‘적대적 공격(Adversarial attack)’을 이용한 방식입니다. 사람의 목소리가 인지되면 기계 학습을 사용해 AI가 소리를 조정, 백색 소음을 깔아 스피커가 다른 음성으로 오인하도록 만들어 줍니다. 하지만 해당 방법은 ‘실시간’으로 음성이 도청될 경우엔 사용하기 어려워요. ​연구진은 녹음된 음성에 한해 해결 방법을 찾았습니다. 자동 음성 인식(Automatic Speech Recognition) 시스템을 이용하는 거죠. 2초 단위로 음성을 인식, 자동 완성하게끔 설정해 기계가 완전히 다른 말을 데이터화하게끔 만드는 겁니다. ​ 자동 완성 기능 활용해 AI 헷갈리게 하기 ​만약에 누군가 “저녁 먹으러 XX에 갈래?”라고 말했다고 가정해 볼게요. 이때 약 2초 동안“저녁 먹으러”까지 해석한 시스템이 “저녁 먹으러 갈래”, “저녁 먹으러 갈까” 등 문자 자동완성처럼 다른 문장을 데이터화하는 식입니다. ​AI가 문장 전체를 들을 수 없게끔 짧은 시간(2초) 동안 계속 자동 완성 기능을 작동시키면서 위장을 하는 거죠. 연구팀은 2초간의 위장 언어를 삽입하는 시스템을 적용했을 때, 녹음 정확도를 19.8%까지 줄이는데 성공했다고 설명했어요. 평소 데이터 저장 정확도는 88.7%에 달해요. ​반면 2초간의 방법을 적용하기 전, 시스템에 백색 소음을 추가하거나 앞서 잠깐 언급한 적대적 공격 방법을 썼을 땐 문자 인증 정확도는 87.2%, 79.5%로 나왔죠. 단, 오류 발생 정도는 52.5%로 절반 이상이라 정확도 개선은 꾸준히 해야 한다고 합니다. ​연구진은 일반적으로 가장 위장하기 어려운 단어는 ‘the’, ‘she’, ‘he’와 같은 짧은 단어라면서 ""다행히도 이 단어들은 대화의 주된 목적이 가장 드러나지 않는 부분이다”라고 설명했습니다. ​해당 연구를 진행한 Mia Chiquier 컴퓨터 과학자는 “해당 시스템은 AI 앞에서 프라이버시를 보호하기 위한 첫 번째 단계일 뿐이다. 인공지능은 목소리뿐만 아니라 얼굴, 행동에 대한 데이터를 수집한다. 우리는 우리의 프라이버시를 존중하는 새로운 기술이 늘 필요하다”라고 설명했어요. ​미국시민자유연합의 고위 정책 분석가인 Jay Stanley는 “섬뜩하게도 오늘날 위장 시스템은 매우 필요하다. 이전까지는 알고리즘의 정확도가 주요했지만, 관련 작업이 사용자가 원하지 않음에도 잘 작동되는 요즘은 잘못 해석되는 이런 시스템이 필요하다”라고 말했어요. ​​테크플러스 에디터 전다운tech-plus@naver.com​[fv0012] ""우리도 재사용 로켓 만든다""...제2의 스페이스X로 불리는 '이곳'의 정체 - 테크플러스-Techplus일론 머스크가 이끌고 있는 우주 탐사 기술회사 ‘스페이스X’, 몇 번 들어본 적 있을 겁니다. 위성 인터넷 사업 스타링크로 유명한 업체이기도 한데요. 스페이스X가 민간 우주 업체들 중에서도 주목받는 이유는 또 하나 있죠. 바로 ‘재사용 로켓’ 때문인데요.tech-plus.co.kr "
"Speak 스픽 영어회화 어플 내돈내산 9개월차 후기(할인코드 공유, 교재) ",https://blog.naver.com/june93kr/222906408760,20221021,"할인코드 궁금하신 분들은 맨 아래로 가시면 됩니다.😁​​올해 설에 Speak 스픽 평생 회원으로 전환하고 어느덧 1년 가까이 지났는데요. 👍🏻👍🏻최근에는 해외여행에 대한 코로나 규제도 많이 풀려서 해외여행 가는 분들이 주변에 정말 많아졌잖아요.저도 내년에 꼭 해외여행 가볼 생각이라 느슨해진 영어회화 공부에 다시 불을 지피기 시작했어요 ㅋㅋㅋㅋ다시 의지를 불태우기 위해 Speak 스픽 영어회화 내돈내산 후기!! 😝😝❤️​​📍 Speak 스픽 인스타그램 영어 스피킹은 스픽(@speak_kr) • Instagram 사진 및 동영상팔로워 333K명, 팔로잉 81명, 게시물 914개 - 영어 스피킹은 스픽(@speak_kr)님의 Instagram 사진 및 동영상 보기www.instagram.com 📍 Speak 스픽 블로그 스픽 블로그앱스토어 1위 영어 회화 앱, 스픽(Speak)에서 운영하는 블로그입니다.blog.speak.com 📍 Speak 스픽 유튜브https://www.youtube.com/channel/UCWC0Tnr5P2fcZ3Hy6m7rqEA​ Speak 스픽 가격 (할인코드, 할인 팁)Speak 스픽 영어회화 이용권 가격은 1년에 129,000원이에요.인터넷에 많이 있는 할인코드를 이용하면 2만원 할인된 가격으로 구독 가능하니 꼭 할인받으세요.​그리고 할인 팁으로 1년 회원권 먼저 끊고 있으면, 저렴한 가격으로 평생 회원에 대한 이벤트 문자가 와요.그때 평생회원으로 넘어가는것도 좋아요. (저는 총 34만원으로 평생회원이되었어요👍🏻👍🏻) ​​평생 이용권450,000원430,000원​연간 구독권(1년)129,000원109,000원(9,083원/월)​​▼▼▼ 할인코드로 2만원 할인 받고 가입하기 ▼▼▼https://app.usespeak.com/i/YAPDTC 저랑 같이 스픽해요허정은 님이 스픽 프리미엄 ₩20,000 할인권을 보내드렸어요. 최신 음성인식 기술로 강화된 스픽의 소통식 수업을 경험해 보세요!app.usespeak.com ​ Speak 스픽 프리미엄 플랜 혜택- 2,000개 이상의 대화형 스피킹 수업 무제한 수강- AI 발음 코치가 주는 즉각적인 발음 피드백- 간격 반복 학습 알고리즘으로 자동 제공되는 리뷰 수업- 매월 최대 아이패드 경품이 제공되는 습관 형성 챌린지​ Speak 스픽 수업 커리큘럼​Speak 스픽 영어회화의 커리큘럼은 단계별로 세분화되어있는 프로그램 구성이라 영어가 처음인 왕초보부터 고급까지 17개의 코스로 세분화된 500일 분량의 방대한 수업 커리큘럼이 있어요.단순하게 교과서적인 설명이 아닌, 원어민이 실생활에서 실제로 사용하는 표현을 엄선해서 커리큘럼을 짰기때문에 더 효율적이에요.최근에는 여행영어나 아기를 위한 엄마아빠 영어 스페셜 커리큘럼까지 다양해서 남녀노소 듣기에 정말 굳굳 👍🏻👍🏻😍​ ​단순하고 지루한 이론 수업이 아닌 10분 이내의 짧은 영상으로 지루하지 않고, 원어민들과 실제로 대화하는 듯한 경험을 주고 무한 연습을 시키는게 스픽만의 매력이에요. 😍😍​   ​Speak 스픽 어플 이용할때마다 항상 놀라는 점이 음성인식이 정확하다는 점이에요.스픽의 음성 인식(Automatic Speech Recognition) 모델은 원어민의 영어 음성 데이터와, 100만 명 이상 한국인의 영어 음성 데이터를 조합하여 학습된 AI 기술을 도입한 모델이라고 해요.사용자의 음성을 즉시 발음 기호로 변환하여 음소 단위까지 분석을 한다고 하니 진짜 기술의 발전이란....👍🏻👍🏻이런 기술로 수업 10분 듣다보면 100문장 순삭이에요.​ Speak 스픽 교재, 습관완성패키지스픽 습관완성 패키지에 등록해서 교재를 받았는데요.습관완성패키지 프로그램의 가격은 69,000원이에요.습관완성 패키지는 습관 코치 프로그램, 스픽 교재 (써머리 북), 습관 트래커 3가지로 구성된 프로그램이에요.30일 동안 정해진 미션을 수행하면 습관완성패키지 금액을 환급 받을 수있어요. 👍🏻👍🏻​  ​매일매일 습관 코치가 수업을 듣도록 독려해주고,가끔 까먹고 수업 못듣는날도 있는데 매일매일 알려주니 자연스레 몸에 습관이 배어요.그리고 무엇보다 스픽 수업에 대한 교재를 받을 수 있는 점이 메리트에요. 👍🏻👍🏻​올해 초 스픽 가격인상때문에 말이 많았는데 아마 내년에도 인상에 대한 이야기가 나오지 않을까 싶어요?​코로나 해외여행 규제도 풀렸고 여행가기 전에 여행영어 공부 다같이 아자아자 해봐요.😝😝​​▼▼▼ 할인코드로 2만원 할인 받고 가입하기 ▼▼▼ 저랑 같이 스픽해요허정은 님이 스픽 프리미엄 ₩20,000 할인권을 보내드렸어요. 최신 음성인식 기술로 강화된 스픽의 소통식 수업을 경험해 보세요!app.usespeak.com ​ "
SNS소통연구소 제10회 전국 스마트폰활용지도사 워크샵 | DCC 디지털 컨텐츠 큐레이터 강사 조미영 ,https://blog.naver.com/jm7501/223083530294,20230424,"디지털 컨텐츠 큐레이터 조미영입니다😊SNS소통연구소 제 10회 전국 스마트폰활용지도사 워크샵이 진행됐습니다.전국에 계신 고수 강사님들과 워크샵을 함께하면서 힘찬 에너지와 인생이 즐거워지고 비지니스가 풍요로워짐을 느끼는 1박2일이였습니다. 이번 워크샵은 ✔️강사들의 교육 꿀팁✔️교육커리및 제안서 작성 요령✔️컴퓨터 활용지도사 자격교육✔️프리젠테이션 전문지도사 자격교육✔️확장프로그램및 chatgpt 정복-브레이브 브라우저-y2mate.com-speech recognition anywhere-adblock-evernotewebclipper-keywords everywhere-chatgpt for google-YouTube Summary with ChatGPT등등등 아낌없는 자료 대방출및각 기관에서 교육시 꼭 해야할 내용과 노하우를 배우는 워크샵이였습니다. 각 지역별 인사및 정보 공유시간을 갖고, 스마트폰활용및 SNS, 스마트워크에서 출제한 문제로 100만원 상당의 시상금이 걸린카훗대회를 진행했습니다.1, 2, 3등하신 강사님들은 정말 대단하신거 같습니다.  넘쳐나는 정보시대에 잊지않고 다 습득하셔서,  수업시간에 알고있는 지식을 수강생들에게 교육하심에 존경의 박수를 보냅니다.  카훗대회 중간에 버튼을 잘 못 눌러 나갔다 재입장해 점수가 엉망이지만, 상위권에 갈려면 저는 한참을 더 공부해야합니다🤣😅  수업에 지친 강사님들을 위해,  sns소통연구소 지정 레크레이션 강사!  명다경 강사님은 노래강사로도 활동중이시고  노래도 잘하시지만, 진행이 넘나 에너지 넘치고 재미지십니다😊😁😄덕분에 SNS 소통연구소 이종구 대표님의 댄스타임으로 강의장이 초토화됐습니다👍👍​지부별 장기자랑에서는 다들 바쁘신데도 불구하고, 화려한 댄스와 소품까지 준비하시고    수어로 인트로해주신 성남지부 강사님들께 상금 55만원이 시상되었습니다👏👏  대전KT인재개발원의 단골 맛집 명륜진사갈비에서 무한으로 고기도 맛게먹고, 그간 있었던 이야기 보따리를 풀어 소통하는 귀한시간이였습니다.  DCC ( Digital Content Curator)​디지털 컨텐츠 큐레이터갖추고 있으면, 절대적으로 유리한 조건!​언제, 어디서 캐스팅 될줄 모르는 강사시장에 꼭 필요한 자격과정입니다.연예인을 예를 들자면  주종목은 가수지만, 춤도 잘추고, 연기도 잘하고, 예능도 잘하고, CF도 찍는 섭외 1순위 연예인이 되기위해소속사에서 자기개발 트레이닝 시키듯이😊​SNS소통연구소에서 진행되는 교육과정 이수와 자격증시험에 합격해야합니다.디지털 컨텐츠 큐레이터가 되기 위해 열공했습니다.  총 9개 자격증 취득!!👏👏👏DCC금뺏지와 자격증을 받으니 뿌듯합니다빛나는 금뺏지처럼, 빛나는 강사가 될 수 있도록 항상 노력하고, 잘하는 디지털컨텐츠큐레이터가 되야겠습니다.😊😘  ✔️스마트폰활용지도사 1,2급스마트폰에서하는 SNS마케팅 교육과정✔️디지털문해교육전문지도사1급디지털문해력 제대로 배우는 과정✔️유튜브크리에이터전문지도사1,2급PC에서하는 유튜브크리에이터 과정✔️SNS마케팅전문지도사1,2급PC에서하는 SNS광고 마케팅 과정✔️스마트워크전문지도사1,2급PC에서하는 스마트워크 시스텀구축 교육   웃음이 가득한 SNS소통연구소!!회식에 술도 많이 드셨을텐데 아침일찍 일어나 런닝하는 자기 관리 철저한 본부장님뿐만 아니라, 대표님, 부대표님이하 모든 강사님의 긍정과 열정이 모두 모여있기에,  SNS소통연구소가 성장할 수 밖에 없는 조직이 아닐까 뼈속 깊이 느끼는 워크샵이였습니다.   함께 호흡하고, 성장할 수 있음에 무한영광입니다.   자리를 빛내주시고,  뜻깊은 시간될 수 있게 워크샵 준비해주신 모든분들께 감사드립니다👍ᆢ👍 궁금할 땐 네이버 톡톡하세요! 터치하면 전화연결 됩니다😊 ​ "
Introducing Translatotron: An End-to-End Speech-to-Speech Translation Model ,https://blog.naver.com/wonsukdream/221558192351,20190609,"Introducing Translatotron: An End-to-End Speech-to-Speech Translation ModelWednesday, May 15, 2019Posted by Ye Jia and Ron Weiss, Software Engineers, Google AI​Speech-to-speech translation systems have been developed over the past several decades with the goal of helping people who speak different languages to communicate with each other. Such systems have usually been broken into three separate components: automatic speech recognitionto transcribe the source speech as text, machine translation to translate the transcribed text into the target language, and text-to-speech synthesis (TTS) to generate speech in the target language from the translated text. Dividing the task into such a cascade of systems has been very successful, powering many commercial speech-to-speech translation products, including Google Translate.​In “Direct speech-to-speech translation with a sequence-to-sequence model”, we propose an experimental new system that is based on a single attentive sequence-to-sequence model for direct speech-to-speech translation without relying on intermediate text representation. Dubbed Translatotron, this system avoids dividing the task into separate stages, providing a few advantages over cascaded systems, including faster inference speed, naturally avoiding compounding errors between recognition and translation, making it straightforward to retain the voice of the original speaker after translation, and better handling of words that do not need to be translated (e.g., names and proper nouns).​TranslatotronThe emergence of end-to-end models on speech translation started in 2016, when researchers demonstrated the feasibility of using a single sequence-to-sequence model for speech-to-text translation. In 2017, we demonstrated that such end-to-end models can outperform cascade models. Many approaches to further improve end-to-end speech-to-text translation models have been proposed recently, including our effort on leveraging weakly supervised data. Translatotron goes a step further by demonstrating that a single sequence-to-sequence model can directly translate speech from one language into speech in another language, without relying on an intermediate text representation in either language, as is required in cascaded systems.​Translatotron is based on a sequence-to-sequence network which takes source spectrograms as input and generates spectrograms of the translated content in the target language. It also makes use of two other separately trained components: a neural vocoder that converts output spectrograms to time-domain waveforms, and, optionally, a speaker encoder that can be used to maintain the character of the source speaker’s voice in the synthesized translated speech. During training, the sequence-to-sequence model uses a multitask objective to predict source and target transcripts at the same time as generating target spectrograms. However, no transcripts or other intermediate text representations are used during inference.​ Model architecture of Translatotron. PerformanceWe validated Translatotron’s translation quality by measuring the BLEU score, computed with text transcribed by a speech recognition system. Though our results lag behind a conventional cascade system, we have demonstrated the feasibility of the end-to-end direct speech-to-speech translation. ​Compared in the audio clips below are the direct speech-to-speech translation output from Translatotron to that of the baseline cascade method. In this case, both systems provide a suitable translation and speak naturally using the same canonical voice.​​ Input (Spanish)Reference translation (English)Baseline cascade translationTranslatotron translation ​You can listen to more audio samples here.​Preserving Vocal CharacteristicsBy incorporating a speaker encoder network, Translatotron is also able to retain the original speaker’s vocal characteristics in the translated speech, which makes the translated speech sound more natural and less jarring. This feature leverages previous Google research on speaker verification and speaker adaptation for TTS. The speaker encoder is pretrained on the speaker verification task, learning to encode speaker characteristics from a short example utterance. Conditioning the spectrogram decoder on this encoding makes it possible to synthesize speech with similar speaker characteristics, even though the content is in a different language.​The audio clips below demonstrate the performance of Translatotron when transferring the original speaker’s voice to the translated speech. In this example, Translatotron gives more accurate translation than the baseline cascade model, while being able to retain the original speaker’s vocal characteristics. The Translatotron output that retains the original speaker’s voice is trained with less data than the one using the canonical voice, so that they yield slightly different translations.​ Input (Spanish)Reference translation (English)Baseline cascade translationTranslatotron translation (canonical voice)Translatotron translation (original speaker’s voice) ​More audio samples are available here.​ConclusionTo the best of our knowledge, Translatotron is the first end-to-end model that can directly translate speech from one language into speech in another language. It is also able to retain the source speaker’s voice in the translated speech. We hope that this work can serve as a starting point for future research on end-to-end speech-to-speech translation systems.​AcknowledgmentsThis research was a joint work between the Google Brain, Google Translate, and Google Speech teams. Contributors include Ye Jia, Ron J. Weiss, Fadi Biadsy, Wolfgang Macherey, Melvin Johnson, Zhifeng Chen, Mengmeng Niu, Quan Wang, Jason Pelecanos, Ignacio Lopez Moreno, Tom Walters, Heiga Zen, Patrick Nguyen, Yu Zhang, Jonathan Shen, Orhan Firat, and Yonghui Wu. We also thank Jorge Pereira and Stella Laurenzo for verifying the quality of the translation from Translatotron. "
PC에서 손가락 사용하지 않고 음성으로 글쓰기 ,https://blog.naver.com/coaching81/223105017987,20230517, 오랫동안 컴퓨터 작업을 하다보면 허리도 아프고 눈도 침침해지곤 하죠?​특히 이동중이거나 급히 문서를 작성해야 할때 편한 자세로 문서 작업을 하게 되면얼마나 좋을까요?​손을 다친 분들이나 시니어들을 위해  꼭 필요한 기능 !!!바로 Speech Recognition Anywhere​​음성을 인식해서 텍스트로 변환해주기에 다양한 작업을 자동화시킬 수 있고 시간도 크게 절약해 준답니다  자 그럼~사용방법 시작해 보겠습니다​우선 Speech Recognition Anywhere 프로그램을 크롬 웹스토어에서 다운 받습니다​​​ 아래와 같은 화면이 나오면 크롬추가를 눌러줍니다 ​크롬에 추가를 누르면 아래와 같은 화면이 나오고 작은 빨간 박스의 확장프로그램 관리 아이콘을 눌러줍니다​그럼 아래의 빨간색 큰 상자 창이 나오고 Speech Recognition Anywhere 옆의 핀을 눌러 파란색이 되도록 합니다 그럼 아래와 같이 주소창 옆에 마이크 아이콘이 생성됩니다 마이크 아이콘을 누르면 아래의 화면이 생성되고 화살표 창에 마우스를 한번 터치해서 커서가 깜박이게 한후​​​ ​ 작성하고자 하는 문서나 웹으로 가서 음성으로 말을 하면 아래와 같이 자동으로 텍스트가 작성된 답니다 ​​어떤가요?  문서작성 어렵지 않겠죠 이제부턴 힘들게 PC에서 손으로 쓰지 말고 내 음성으로 글을 써 보아요 ​오늘도 연정쌤은 여러분들의 스마트한 디지털 세상을 응원한답니다 궁금할 땐 네이버 톡톡하세요! ​​​​​ 
"보청기센터 방문기: 크리스탈 보청기 & 청각언어센터 (부산 해운대, 장산역 3번 출구) ",https://blog.naver.com/hearing1004/222785518380,20220623,"지난 2년 동안 코로나(COVID19)로 인해 서울을 벗어날 일이 거의 없었는데, 일 때문이지만 드디어 오랜만에 부산으로 간다.전날 여행을 떠난다는 설렘에 잠을 설친 덕분에 몸살 끼가 살짝 돈다. 목적지는 조혜령 선생이 원장으로 있는 크리스탈보청기&청각언어센터 ^^센터의 위치는 부산 지하철 2호선의 종점인 해운대구, 장산역 3번 출구 바로 옆 건물 3층, 2022년 5월 14일에 문을 열었다. 조 원장은 국내 청각학의 발원지인 한림국제대학원대학교 청각언어치료학과를 졸업했다.학부(가야대학교)에서는 청각학과 언어병리학을 복수 전공을 했다.그리고 조 원장은 청각학 전공 석사과정 재학 시 한국연구재단 과제인 ‘한국 난청인을 위한 보청기적합공식의 개발’ 관련 연구원으로 활동을 했었다.조 원장은 연구를 통해 국제학술지(SCOPUS)인 ASR(Audiology & Speech Research)에 3편의 논문을 게재하기도 했다. 장산역에서 내려 3번 출구로 나가는데 계단이 상당히 높다. ㅠㅠㅠ다행히도 밖으로 나가니 센터 입구가 바로 옆에 있다.3층 엘리베이터에서 우측을 보니 하얀 크리스탈 보청기&청각언어센터 간판이 보인다. 내부에 들어서니 안내데스크, 대기실, 청능 및 언어 재활실이 한눈에 보인다.조원장의 성격대로 쾌적하고, 깔끔하다.■ 주요 시설은 청능재활에 필요한 청각평가실, 보청기적합 및 조절실, 청능훈련 및 언어재활실, 보청기 수리실 등으로 구성되어 있다.■ 청능재활에 필요한 기기로는 청력검사기, 보청기 조절을 위한 컴퓨터, 보청기 성능측정기 및 실이측정기, 수리에 필요한 기기 등을 갖추고 있다.■ 모든 시설과 장비는 한국산업표준(KS I 0562) 및 국제표준(ISO 21388)에 부합하도록 갖추어져 있다. 조혜령 원장은 말한다.크리스탈보청기&청각언어센터의 운영철학은 '난청인의 의사소통 능력의 개선'을 위해 ‘해외에서도 인정받을 수 있는 청능재활의 절차와 서비스’가 기본 원칙이라며, 다음의 원칙을 지킬 것이라고 강조한다....■ 철저한 청각평가를 통한 보청기의 선택■ 한국 난청인에게 적합한 이득 및 주파수반응곡선의 적용■ 효과적인 착용효과의 평가를 통한 보청기의 조절과 청능훈련■ 철저한 사후관리 ■ 참고로 조원장의 발표 논문은 다음과 같다□ Development of Hearing Aid Gain for Korean Hearing Impaired  version 2.0 Considering Levels and Spectra of Korean Conversational Speech, and Preferred Gain. Audiol Speech Res 2021; 17(1):29-34.□ Comparison of Preferred Real-Ear Insertion Gain between Open- and Closed-Canal Fitting Hearing Aids. Audiol Speech Res 2021; 17(2):180-186.□ Changes in Speech Recognition Scores and Sound Quality  According to Gain Adjustment in High Frequency Bands. Audiol Speech Res 2022; 18(2):73-82.​이곳저곳을 둘러보고 이야기를 나눈 후 시장 끼를 느끼고 식당으로 향한다, ㅎㅎㅎ 전화 및 위치: 051-701-8752, 부산광역시 해운대구 해운대로 814 세종월드프라자 A동 304호크리스탈 보청기 : 네이버 통합검색 (naver.com)네이버 지도 (naver.com) 크리스탈보청기로 나가는 전철역 & 상가 건물 크리스탈보청기 내부 전경 및 시설 청능재활에 필요한 장비 및 기기 보청기적합실 모습 조혜령 원장과 함께^^ 조혜령 원장의 삭사과정 재학 및 연구원 시절 ^^ (강석천 조선일보 논설고문과, 연구원, 논문발표, 세미나 발표 등) 크리스탈 보청기 & 청각언어센터부산광역시 해운대구 해운대로 814 세종월드프라자 A동 304호 #보청기 #청능재활 #크리스탈보청기 #청각언어센터 #해운대 #장산역 #3번출구 #청각학 #한림국제대학원대학교 #청각언어치료학과 #청각학전공 #언어병리학 #석사과정 #한국연구재단 #보청기적합공식 #Audiology #청각평가실 #보청기적합 #청능훈련 #언어재활실 #청능재활 #청능훈련 #청력검사기 #성능측정기 #실이측정기 #한국산업표준 #국제표준 #보청기선택 #착용효과 #사후관리 #세종월드프라자 #난청인 #의사소통능력 "
대만 로봇 박람회 TAIROS 2023 ,https://blog.naver.com/icetour0915/223111723516,20230525,"대만 로봇 박람회는 대만의 산업자동화 및 로봇공학에 전념하는전시회 및 컨퍼런스입니다. 이 행사는 국내외 기업. 제조업체 및 전문가들이 최신 제품. 기술 및 서비스를 선보일 뿐만 아니라 아이디어를 교환하고 새로운 비지니스 기회를 모색 할 수 있는 플랫폼을 제공합니다.  이 전시회는 산업자동화. 스마트기계. 지능형 로봇 등을 포함한 광범위한 주제를 다룹니다. 대만 타이로스 2023타이페이 로봇 박람회개최기간2023년 8 월 23일 ~ 8월 26 일 전시장소Taipei Nangang Exhibition Center, Hall 1&2주최http://www.tairos.tw/en/개최규모약 760 여개사 (2022년 기준) AUTOMATION TAIPEI 합산 전시품목[SMART MANUFACTURING]SMART FACTORYEnterprise Resource Planning(ERP), Quailty Control solution, Product Lifecycle Management(PLM), Mixed Reality(MR), Recognition System, industrial Internet of Things(IIoT), Factory Automation Equipment and System, Wireless Communication Equipment and Module, Software-Defind networking, Sensor Network System, Supervisory Control and Data Acquistion(SCADA), Energy Saving System, Risk Management system, Development and consulting etcINDUSTRIAL INTELLIGENCEIntelligent Machinery, Industrial Robot, Testing & Measuring Instrument, Computer-aided Design(CAD), Computer-aided Engineering(CAE), Computer-Aided Manufacturing(CAM), Storage Equipment, Fork Lift truck, Automated Guided Vechicle(AGV), Embedded System, Industrial Personal Computer(IPC), Laser Equipment, Process Automation, Automatic Control Engineering, Pneumatic & Hydraulic Equipment, Production Line Design and Related Services. [SMART SERVICES (BUSINESS APPLICATION)]SMART LOGISTICS Order management system(OMS), Warehouse Management System(WMS), Electronic Data Interchange(EDI), Advanced Planning & Scheduling System(APS), Automatic Storage & Retrieval System(AS/RC), Computer Aided Picking System(CAPS), Automated Material Handling Equipment & SystemSMART RETAILPoint of Sale(POS), Intelligent Customer Service System, Store Enviroment Design, Automated Vending Solutions, Automated Order System, Business Process Automation(BPA), Unmanned Store.SMART MEDICALSmart Medical device, mHealth, Remote Patient Monitoring Systems, Smart Healthcare Tools, Wearable device, Hospital Information System(HIS)SMART SERVICE3D Printing, Office Automation, Smart Home/ Appliances/ Building Materials, Smart Energy Saving & Monitoring, Safety Monitoring, Automation in Agriculture & Fisheries, Building automation [SERVICE ROBOT]COMMERCIAL ROBOTSReception, Guided Service, Catering Service, Financial&Investment Service, Customer Service, Airport AssistancePERSONAL ROBOTWearable Intelligence, Personal Vehicle(Auxiliary Drives, Self-Balancing Electric Vehicle, Electric Unicycle), Intelligent Assistant, Chatbot.HOME ROBOTSHome care, Cleaning, House-keeping, Security, EntertainmenSPECIAL-PURPOSE ROBOTSSurgery, Prosthetic, Assistive Device, Agriculture, Logistics & Warehousing, EducationSECURITY & DEFENSE ROBOTSPatrol, Disaster Relief, Surveillance, Reconnaissance [ARTIFICIAL INTELLIGENCE(AI)]Deep Learning, Machine Learning, Neural Network, Image Recognition, Speech Recognition, Natural Language Processing(NLP), Big Data [INDUSTRIAL SUPPLY]Electric Motor, Variable-frequency Drive, Electrical Component, Contactor, Switch & Signal Relay, Enclosure, Electric Parts, Repair Parts, Sensor, Vision System, Clamping Jaw, Parts of Robots, Equipment Components 동시 개최 박람회(1) 대만 물류 & IOT 박람회 TAIPEI LOGISTICS & IOT EXHIBITION 2023(2) 대만 자동화 박람회 AUTOMATION TAIPEI 2023​ 상세 일정대만 로봇 박람회 TAIROS 20231. 전시회명 : 대만 로봇 박람회 TAIROS 2023              (TAIWAN AUTOMATION INTELLIGENCE AND ROBOT SHOW) TAIROS 2023 ( 대만 로봇 박람회 ) TAIPEI ...www.icetour.co.kr #taiwanautomationintelligenceandrobotshow#taiwanautomationintelligenceandrobotshow2023#TAIROS#TAIROS2023#타이로스#타이로스2023#대만TAIROS#대만TAIROS2023#대만타이로스#대만타이로스S2023#타이페이TAIROS#타이페이TAIROS2023#타이페이대만TAIROS#타이페이대만TAIROS2023#2023타이페이대만TAIROS#대만TAIROS박람회#대만TAIROS박람회2023#대만TAIROS전시회#대만TAIROS전시회2023#타이페이TAIROS박람회#타이페이TAIROS박람회2023#타이페이TAIROS전시회#타이페이TAIROS전시회2023#대만로봇박람회#대만로봇전시회#대만타이로스박람회#대만타이로스전시회#타이페이로봇박람회#타이페이로봇전시회#대만로봇박람회TAIROS#대만로봇전시회TAIROS#타이페이로봇박람회TAIROS#타이페이로봇전시회TAIROS#대만로봇박람회TAIROS2023#대만로봇전시회TAIROS2023#타이페이로봇박람회TAIROS2023#타이페이로봇전시회TAIROS2023  "
"난청, 보청기 구입 전 꼭 해야 할 청력평가는? ",https://blog.naver.com/hhkhcc/222846799423,20220812,"​ ​ 보청기 구입을 고민할 때 보통 가장 궁금해하는 것이 바로 보청기 브랜드 또는 가격입니다. 물론 브랜드와 가격도 중요하지만 보청기를 조절하기 위한 기본 시설과 상주하는 전문가를 빼놓으시면 안됩니다. ​그중에서도 보청기를 만족스럽게 착용하기 위해서는 먼저 정확한 청력 평가가 이루어져야 합니다. 청력 평가 자료를 기반으로 보청기를 조절하고 보청기 효과도 알아볼 수 있기 때문입니다. 오늘은 저희 센터에 방문하시면 받게 되는 청력평가의 종류와 명칭에 대해 자세히 안내해 드리겠습니다.  ​​ 순음청력평가(PTA; Pure Tone Audiometry) 청력평가 중 기본이자 가장 중요한 '순음청력평가(Pure-Tone Audiometry)'는 순음을 이용하여 주파수별 청력을 측정하는 것입니다. 순음 청력평가를 통해 난청의 종류, 중증도, 형태 등을 알 수 있으며, 청력 상태의 종합적인 평가와 더불어 보청기 및 인공와우의 착용 전후의 결과를 알 수 있습니다. 황혜경보청기 서초방배센터 순음을 이용한 청력평가는 '공기전도 청력평가’와 ‘골전도 청력평가'  두 가지로 나눌 수 있습니다.'공기전도 청력평가(air conduction audiometry)'는 헤드폰이나 삽입형 이어폰을 사용하며, 외이와 중이, 그리고 내이 등 전체적인 청각 경로의 청력을 평가합니다. 각 귀에 250~8,000Hz까지 주파수별로 순음을 제시하여 소리가 들리면 버튼을 누르는 방법으로 청력을 찾습니다. 청각 전달 경로 중 한 곳만 이상이 있어도 난청으로 나타납니다. ​'골전도청력평가(bone conduction audiometry)'는 외이와 중이를 거치지 않고 직접 달팽이관을 자극하여 청력을 측정하는 평가입니다. 골도 진동체를 이용하여 귀 뒤편의 유양돌기에 부착시킨 후 공기전도 청력평가와 동일한 방법으로 진행하지만 250~4,000Hz까지의 주파수에서만 측정합니다. 외이나 중이에 이상이 있더라도 내이 기능에 문제가 없다면 골전도 청력평가 시 정상 청력으로 나타날 수 있습니다.  ​ 황혜경보청기 종로3가센터어음청력평가(SA; Speech Audiometry) 순음청력평가에 이어 소개해 드릴 ‘어음청력평가(speech audiometry)’는 어음에 대한 인지능력 및 이해력을 측정하는 평가입니다. 난청의 선별은 물론, 보청기 착용 전 보청기 착용 효과를 어느 정도 예측할 수 있는 평가입니다. 어음청력평가를 통해 순음청력평가 결과의 신뢰도를 확인할 수 있고, 다양한 듣기 상황에서의 언어 인지 능력을 평가할 수 있습니다. 어음분별력이 많이 저하된 경우에는 보청기를 착용해도 만족스럽지 못할 수 있습니다. ​ 황혜경보청기 마포공덕센터​어음청력평가는 크게 두 가지로 나눌 수 있습니다. 바로 '어음인지역치'와 '단어인지도'입니다.‘어음인지역치(speech recognition threshold, SRT)’란 2음절 단어를 들려주고 인지할 수 있는 가장 작은 소리 강도를 측정하는 평가입니다. 어음인지역치평가 결과를 통해 순음청력평가의 신뢰도를 확인할 수 있습니다.‘단어인지도(word recognition score, WRS)'란 듣기 편안한 강도에서 단음절 단어를 얼마나 정확히 인지하는지를 백분율로 측정하는 것입니다. 편하게 듣는 소리 강도에서 단음절 단어를 들려줘서 이 평가를 통해 어음 이해 능력을 확인함과 동시에 보청기의 착용 효과를 예측할 수 있어 보청기 적합 전후로 많이 시행합니다. 황혜경보청기 송파잠실센터​​ 어음청력평가 전에는 어음 강도 레벨이 쾌적하고 편안하게 느껴지는 강도인 ‘쾌적 강도 레벨(most comfortable level, MCL)’과 소리가 커서 불편하게 느껴지는 강도인 ‘불쾌 음압레벨(uncomfortable loudness level, UCL)’을 측정합니다. ​두 가지의 강도 측정을 통해 개인의 역동 범위를 측정하고 거기에 맞춰 보청기의 증폭 범위를 제한하는 데 사용하고 있습니다.​ 황혜경보청기 강서마곡센터 보청기는 손상된 청력을 보충해 주는 청각 보조 기기입니다. 청력을 정확히 확인하고 과학적인 소리 조절을 받아야 편안한 착용을 할 수 있습니다. 보청기 선정부터 소리 조절까지 가장 기본적이고 객관적인 자료로 활용되는 것이 청력평가 결과입니다. 착용 전후로 반드시 주기적인 청력평가를 받아야 하므로 국제 규격에 맞는 방음부스 시설이 완비되어 있어야 합니다. 꾸준한 관리가 착용 만족도를 높일 수 있기 때문에 정확한 청력평가가 가능한 곳의 전문가로부터 상담을 받으시고 알맞은 보청기를 추천받으시길 바랍니다. ​저희 황혜경보청기 청각언어센터는 청각학 석박사 그룹으로 국내 최초 멀티 브랜드 보청기(벨톤, 스타키, 시그니아, 오티콘, 와이덱스, 포낙 등) 상담 시스템을 도입하여 착용자에게 적합한 보청기를 합리적인 가격에 비교 추천하고 있습니다. 현재 서초구, 마포구, 송파구, 강서구, 종로구에 총 5곳의 프리미엄 직영 센터를 운영하고 있으며 어떤 센터를 방문하셔도 동일한 청능재활 서비스를 제공받으실 수 있습니다. 언제나 여러분들에게 최선을 다할 수 있도록 노력하겠습니다.   황혜경보청기 청각언어센터 서초방배서울특별시 서초구 방배로 162 예광빌딩 2층황혜경보청기 청각언어센터 마포공덕서울특별시 마포구 마포대로 68 마포아크로타워황혜경보청기 청각언어센터 송파잠실서울특별시 송파구 올림픽로 119 파인애플상가 3층황혜경보청기 청각언어센터 강서마곡서울특별시 강서구 공항대로 247 퀸즈파크나인 B동 2층황혜경보청기 청각언어센터 종로3가서울특별시 종로구 종로 122 타워15 2층 ​ "
'챗 GPT'가 답했다/4차산업혁명의 기술을 대분류하면?/What's the major--- industrial Revolution? ,https://blog.naver.com/hoki5127/223039946603,20230311,"What is the major classification of the technology of the fourth industrial revolution? ​The technology of the fourth industrial revolution can be broadly classified into the following categories:Artificial Intelligence and Machine Learning: This technology involves developing algorithms and software that can perfire human intelligence, such as image and speech recognition, natural language processing, and decision-making.Internet of Things (IoT): IoT refers to a network of physical objects, devices, and sensors that are connected to the internet and can exchange data with each other. This technology enables automation and data collection in various industries.Robotics and Automation: Robotics involves the design, construction, and operation of robots that can perform tasks autonomously or with minimal human intervention. Automation, on the other hand, involves the use of technology to perform tasks that were previously done manually.Blockchain: Blockchain is a decentralized, digital ledger that records transactions and enables secure and transparent data sharing across a network. It has the potential to revolutionize industries such as finance, supply chain management, and healthcare.Additive Manufacturing: Additive manufacturing, also known as 3D printing, involves the creation of three-dimensional objects by adding layers of material. This technology is transforming various industries by enabling faster prototyping, customization, and on-demand production.These technologies are often interconnected and can work together to create even more advanced applications.​​Translate it into Korean. [ 번역 ]​4차 산업혁명의 기술은 크게 다음과 같은 범주로 분류할 수 있습니다:인공 지능과 기계 학습: 이 기술은 이미지 및 음성 인식, 자연 언어 처리 및 의사 결정과 같이 일반적으로 인간의 지능이 필요한 작업을 수행할 수 있는 알고리즘 및 소프트웨어 개발을 의미합니다.사물 인터넷(IoT): IoT는 인터넷에 연결되어 서로 데이터를 교환할 수 있는 물리적인 물체, 기기 및 센서의 네트워크를 나타냅니다. 이 기술은 다양한 산업에서 자동화 및 데이터 수집을 가능하게 합니다.로봇과 자동화: 로봇은 자율적으로 또는 최소한의 인간 개입으로 작업을 수행할 수 있는 로봇의 설계, 구축 및 운영을 의미합니다. 반면, 자동화는 이전에 수동으로 수행되던 작업을 기술을 이용하여 수행하는 것을 의미합니다.블록체인: 블록체인은 거래를 기록하고 네트워크 전체에서 안전하고 투명한 데이터 공유를 가능케 하는 분산형 디지털 원장입니다. 이 기술은 금융, 공급망 관리 및 보건 등의 산업을 혁신시킬 수 있습니다.부가 제조: 부가 제조 또는 3D 프린팅은 재료의 층을 추가함으로써 3차원 객체를 생성하는 것을 의미합니다. 이 기술은 더 빠른 프로토타입 제작, 맞춤형 제작 및 주문 생산이 가능하여 다양한 산업에서 변화를 일으키고 있습니다.이러한 기술은 종종 상호 연결되어 더욱 진보된 응용 프로그램을 만들어 냅니다. "
chatGPT로 영어회화 연습하기 ,https://blog.naver.com/lms50925/223065922778,20230405,"우선 Talk to ChatGPT를 설치해야한다 chatGPT 설치 ​Talk to ChatGPT를 검색해서 들어간다​ ​2.  <Chrome에 추가> 버튼을 클릭한다 ​​​3. <확장 프로그램 추가>를 클릭한다 ​​4. 설치완료 chatGPT 설정​5. 다시 <ChatGPT>를 검색해서 들어간다​​chatGPT  ​6. Try ChatGPT를 클릭한다 ​​​7. <START>를 클릭한다 ​8. 권한을 허용해준다 ​​9. 그럼 우측상단에 사용하는 아이콘들이 나온다 ​10. 아래처럼 빨간줄이 있을때는 질문가능  -GPT가 대답중에는 녹색줄이 된다​ 11. 마이크를 클릭하면 동그란 아이콘이 나오며 마이크 끈 상태가 된다 ​​12. 맨 우측의 톱니바퀴를 클릭해서 설정화면으로 들어간다​     1) 아래 체크해제 - 이건 내 말도중이라도 잠시 끊어지면 말이 끝난걸로 인식하고 바로 응답한다     2) Manual  send word(s) 설정 - ""done""로 해놓으면 이 단어가 나오면 내가 말한걸 전달한다     3) 대답하는 언어설정  AI voice and language 를 <Google Us English (en-US)>로 바꾼다     4) 말하는 언어설정  Speech recognition language를  <English - en- US>로 바꾼다. 그 다음 <Save>하고 나가면 된다.  사용하기이제 아래화면처럼 빨간 바가 나와 있을때 영어로 이야기하면영어로 답변을 해준다.....​​ ​ "
(난청인/보청기 착용 시) 마스크를 착용한 사람의 말소리가 잘 안 들려요.. ,https://blog.naver.com/hearing1004/222846036608,20220812,"2020년 1월부터 시작한 코로나대유행은 2022년 현재까지도 진행 중에 있으며,이제는 일상적으로 언제, 어디서든 마스크를 착용한다.하지만 화자(speaker)가 마스크를 착용했을 때 청력손실이 있거나 보청기를 착용한 난청인은 마스크를 착용하지 않았을 때에 비해 말소리를 이해하기가 어렵다.  왜??? 그 이유를 살펴보면 다음과 같다.■ 마스크를 착용한 화자의 말소리를 이해하기 어려운 이유□ 화자의 입술 및 표정을 살피기 어렵다.난청인 또는 보청기를 착용한 난청인은 화자의 입술 또는 표정은 살피는 것은 타인과의 의사소통에 많은 도움을 준다. 그런데 화자가 입술이 보이지 않는 일반적인 마스크를 착용하면 입술읽기 또는 표정읽기가 어려워지기 때문이다. □ 고주파수 대역의 음압이 감소한다.마스크를 착용하면 아래의 그림처럼 2 kHz 이상 고주파수대역 어음의 음압이 감소한다. 그리고 비말차단, KF80에 비해 KF94 마스크에서 더 많은 음압이 감소한다. 결국 고주파수대역의 이득 감소는 조용한 곳 및 잡음 하에서 단어인지도(어음분별력) 및 보청기 음질의 저하를 초래한다(아래 그림 참조). 마스크 착용 전후의 한국어 장기평균어음스펙트럼(long-term average speech spectrum): 마스크 착용 시 고주파수대역의 음압이 감소함■ 해결방법□ 아래 우측의 그림과 같이 화자가 난청인을 위한 마스크를 착용한다.아래 사진은 화자가 착용하는 일반용 그리고 난청인을 위한 입술이 보이는 마스크를 나타낸 것이다. 일반용(좌) 그리고 입술이 보이는 난청인을 위한 마스크(우) □ 보청기의 주파수반응곡선(frequency response curve)에서 2 kHz 이상의 음압(이득)을 점차 올려준다.아래 사진은 고주파수대역의 이득을 올렸을 때 잡음 하에서의 단어인지도 및 음질의 변화를 나타낸 것이다. 고주파수 대역의 이득을 올렸을 때(post) (좌): 잡음 하에서의 단어인지도가 증가한다. (우): 소리의 울림 및 선명도가 개선된다. Reference: Jo, H., Lee, M., Kim, J., Choi, E., Lee, H., & Lee, K. (2022). Changes in Speech Recognition Scores and Sound Quality According to Gain Adjustment in High Frequency Bands. Audiology & Speech Research, 18(2). 73-82. #난청인 #보청기 #마스크 #난청인을위한마스크 #보청기착용 #말소리가안들린다 #코로나대유행 #코로나 #단어인지도 #음질 #입술읽기 #표정읽기 #의사소통 #고주파수대역 #어음 #비말차단 #KF80 #KF94 #어음분별력 #한국어 #장기평균어음스펙트럼 #주파수반응곡선 #잡음 #소리울림 #선명도 "
(논문 분석) 3-D Convolutional Recurrent Neural Networks With Attention Model for Speech Emotion Recognit ,https://blog.naver.com/ssj860520/222866347101,20220904,감정인식 논문중에 하나인3-D Convolutional Recurrent Neural Networks With Attention Model for Speech Emotion Recognition에 대해 궁금해졌습니다.열심히 읽어보는데 어렵더라고요. 그럴땐 어떻게 해야할까요?바로~ 누군가를 쓴 리뷰를 보면 되죠.아래의 리뷰에 아주 잘 나와있네요 ㅎㅎㅎ감사합니다. 사랑합니다.​ (논문 모델 분석) 3-D Convolutional Recurrent Neural Networks With Attention Model for Speech Emotion Recognition감정인식 관련 논문인 [1]을 읽었다. 이 논문의 제목만 읽고 이 논문의 핵심은 3-D CRNN과 Attention 을 사용해서 감정인식을 한것으로 생각할 수 있다. 3-D CRNN과 Attention 이 중요한 역할을 한것은 맞지만 그..qwertyuioop.tistory.com ​ 
"enable - 단어뜻, 예문 정리  ",https://blog.naver.com/rosie_incanada/223096892939,20230509,"안녕하세요. 캐나다에 살고 있는 동시통역사 로지입니다.​오늘은 enable 동사에 대해서 같이 공부해볼까 합니다. ​그럼 시작할께요. Let's get started.   enable의 뜻을 먼저 볼까요?​1) (사람 또는 사물)이 가능하게 하다.​2) (컴퓨터 시스템, 기기)를 활성화시키다예문으로 그럼 같이 공부해볼께요. ​1) (사물 또는 사람) 이 가능하게 하다 라는 뜻의 예문을 먼저 볼께요. ​■ The GPS system enalbes drivers to navigate unfamiliar roads more easily. GPS 시스템은 드라이버들이 낯선 길에서 더 쉽게 길을 찾을 수 있도록 해줍니다. ​■ The training program will enable participants to acquire new skills. ​트레이닝 프로그램덕택에 참여자들이 새로운 기술을 습득할 수 있게 되었습니다.​■ The automatic doors will enable wheelchair users to enter the building​ without assistance. 자동문덕택에 휠체어타는 사람들이 빌딩에 도움없이 들어갈 수 있을 것입니다. ​■ The homeowner's approval has enabled us to start work on the renovation project.  집주인의 승낙덕택에 리모델링을 시작할 수 있었습니다. ​■ The manager enabled her team to work remotely by providing them with laptops. 관리자는 팀원들에게 노트북을 제공함으로써 팀원들이 원격업무를 할 수 있도록 해주었습니다. ​■ The popularity of social media platforms has enabled photographers to reach a wider audience.소셜미디어의 인기로 인해서 사진작가들이 더 많은 대중관객들을 만날 수 있게 되었습니다. ​ ©Sangga Rima 출처 USPLASH2) (컴퓨터 시스템, 기기)를 활성화시키다 라는 의미의 예를 볼까요?​■ To enable Wi-Fi on your computer, click on the wireless icon in the taskbar. 컴퓨터에 Wi-Fi를 활성화하려면, 작업표시줄에 있는 wireless 아이콘을 클릭하세요. ​■ To enable automatic updates on your computer, go to the settings menu and select ""Window Update"". 컴퓨터에 자동업데이트를 활성화하려면, 세팅매뉴에 가서 ""Window Update""를 클릭하면 돼.  ​■ I enabled two-factor authentication on my email account. 내 이메일 계정에 이중인증을 활성화해놨어. ​■ I enabled night mode on my phone to reduce eye strain. 눈의 피로를 줄이기 위해서 핸드폰에 야간모드를 켰어. ​■ I enabled speech recognition on my computer to control the system. 내 컴퓨터 시스템을 컨트롤을 할 수 있는 음성인식 기능을 활성화했어. ​ ©Kari Shea 출처 USPLASH""활성화하다'라는 뜻의 enable 대신 쓸 수 있는 대체표현으로는 ""activate"" 또는 ""turn on""을 쓸 수 있습니다.​enable 동사에 대한 감을 좀 잡으셨나요?그럼 오늘도 로지의 잔소리 한마디!!​오늘 제가 쓴 예문 꼭꼭 소리내서 읽어보세요.최소 5번씩 읽으시길 추천드리구요, 본인만의 예도 만들어서 써보신다면 더할나위 없겠죠?​그럼 안녕~^^​​​ "
"보청기가 물에 빠졌을 때, TV시청이 어려운 이유와 해결방법 ",https://blog.naver.com/hearing1004/222798778935,20220703,"▣ 질문 내용청능재활센터(보청기센터): 대상자의 청력은 오른쪽은 농(deaf)에 가까우며, 왼쪽은 심도의 청력손실로 순음청력역치는 아래의 좌측 그림과 같으며. 단어인지도(word recognition score)는 20%이다. 이득이 높은 외이도형(ITC)의 보청기를 착용한 후 가족과의 대화는 만족스러운 편이다. 하지만 보청기를 물에 빠트리고 나서 건조시킨 후 가족과의 의사소통에는 여전히 만족하지만 TV시청 시 말소리를 이해하기가 힘들다고 한다.그 원인과 해결 방법은 어떤 것이 있나???  ▣ 질문에 대한 답변 ■ 물에 빠졌을 때 보청기의 전기음향 파라미터에 미치는 영향□ 첫째, 우선 보청기가 물에 빠지면 보청기 마이크로폰과 수화기의 음구에 있던 먼지, 귀지 등의 이물질이 물과 혼합된다. 그 후 보청기가 건조되어 물기가 마르면 이물질이 음구 내에 흡착되어 음구의 용적을 감소시킨다. 이로 인해 음구 내에서의 음향임피던스가 증가하여 소리의 전달, 특히 고주파수 대역의 전달을 방해하게 되어 아래 우측 그림과 같이 고주파수 대역의 이득 또는 주파수반응곡선을 감소시키게 된다.  □ 둘째, 물에 빠졌을 때 보청기의 증폭기(칩) 주변의 먼지 등의 이물질이 물과 혼합되어 흡착된 경우, 이물질이 각 배선 또는 부품 간에 캐패시터(capacitor)로 작용할 수 있다. 그리고 캐패시터의 역할로 인해 고주파수 대역의 일부를 그라운드(음극)로 빠져나가게 하고, 결국 수화기 측에서 고주파수 대역의 이득 및 출력이 감소하는 결과를 초래한다. 이러한 현상은 마이크로폰 및 수화기에 이물질이 들어간 것과 같은 결과를 가져오게 된다. ■ 물에 빠진 보청기를 건조시켰을 때 TV시청이 어려운 이유대부분의 TV 등 대부분의 음향기기에서 발생하는 주파수반응곡선은 건청(normal hearing)인이 선호하는 음질 위주로 조절하므로 저주파수 대역의 이득을 강조된 형태로 나타난다. 특히 스피커의 성능이 나쁜 경우는 고주파수 대역을 제대로 증폭하지 못하는 즉, 주파수범위가 좁은 경우도 나타난다. 한국어의 대역중요함수(band importance function)가 1,000 Hz 이하에서 높은 것을 감안하더라도 선행 연구 결과에서 볼 수 있듯이 고주파수 대역의 인지는 매우 중요하다(Cho et al., 2022).따라서 고주파수 대역의 이득 또는 주파수반응곡선이 감소하거나 주변에 잡음이 있는 경우는 TV 스피커에서 발생하는 고주파수 대역의 음소를 제대로 인지하지 못해서 어음인지에 어려움을 겪거나 어음명료도의 저하가 발생한다. ■ 물에 빠졌음에도 가족 간의 대화에 문제가 없는 이유친근한 가족 간의 대화에서는 상대방의 대화음이 증폭기, 스피커 등 다양한 기기 및 변환기를 거치지 않고 전달되기 때문에 고주파수 대역이 TV에 비해서는 잘 전달되기 때문에 대화에 어려움을 느끼지 않은 것으로 보인다.   ■ 해결 방법□ 해결 방법은 마이크로폰과 수화기의 음구를 정확하게 청소를 할 수 있다면 쉽게 해결할 수 있다. 그리고 좋은 방법은 아니지만 고주파수 대역의 이득을 원래의 주파수반응곡선이 나오도록 올려주는 방법도 생각할 수 있다.□ 이러한 현상을 확인하기 위해서는 보청기의 전기음향 특성측정기를 통해 물에 빠지기 전과 후의 이득 및 주파수반응곡선을 확인해 보면 알 수 있다. 따라서 보청기의 조절 후 항상 보청기의 특성을 측정하여 기록해 두는 것이 중요하다.□ 하지만 보청기센터에서는 마이크로폰과 수화기 음구의 청소 및 수리가 어렵다. 따라서 해당 보청기를 제조사에 수리를 의뢰하는 것이 효과적이다. ■ 기타: 효과적인 TV시청참고로 난청인이 TV를 효과적으로 시청하기 위해서는 보청기의 주파수반응 조절 외에도 다양한 방법이 있지만, 최소한 다음과 같이 TV시청 환경을 조절할 필요가 있다.□ 창문을 닫는 등 불필요한 잡음의 제거- 난청인은 잡음에 취약하기 때문이며, 창문 등을 열었을 때 소음이 발생하지만, 난청인은 이를 무시하는 경향이 있습니다. 따라서 TV시청 시 창문을 닫거나, 가능하다면 TV에 가까이 않도록 하고, 설거지 등에 따른 잡음의 발생을 최소화해야 한다.□ TV의 ‘설정’에서 에코(echo)의 양을 최소한으로 조절- 난청인은 잔향(reverberation)이 있는 곳에서 어음의 인지가 어렵다. 따라서 에코의 양을 줄이거나, 서라운드 스피커 등을 통한 TV시청은 가능한 한 피해야 한다. 그리고 거실이 크다면, 거실에 가구 등을 적절히 배치하여 잔향을 최대한 줄여야 한다. ■ 참고문헌Jo, H., Lee, M., Kim, J., Choi, E., Lee, H., & Lee, K. (2022). Changes in speech recognition scores and sound quality according to gain adjustment in high frequency bands. Audiology and Speech Research, 18(2), 73-82.​#보청기 #물에빠졌을때 #TV시청 #해결방법 #청능재활센터 #보청기센터 #청력역치 #단어인지도 #이득 #외이도형 #ITC #의사소통 #전기음향 #파라미터 #마이크로폰 #수화기 #음구 #음향임피던스 #주파수반응곡선 #증폭기 #캐패시터 #주파수범위 #대역중요함수 #주변잡음 #어음인지 #어음명료도 #특성측정기 #보청기제조사 #에코 #잔향 "
ASTI 마켓 인사이트 | 음성인식 서비스 ,https://blog.naver.com/withkisti/223070010901,20230503,"| ASTI Market insight : 음성인식 서비스 |​○ 음성 인식 기술은 인간의 언어를 사용해 기계와 상호작용할 수 있는 기능을 제공하며, 최근에는 생체 인증 및 문서화 기술이 주목받고 있다.○ 음성 인식 기술은 종단간 방식의 적용과 자연어처리, 자연어이해 기술의 발달로 정확도가 견고해지면서, 다양한 산업별 응용서비스 확장이 기대된다.○ 음성 인식 서비스의 세계시장은 2021년 82억7,200만 달러 규모이며, 연평균 21.6 % 성장으로 2026년 219억9,500만 달러가 될 것으로 전망된다.○ 음성 인식 서비스는 기반기술, 플랫폼, 디바이스, 응용서비스 등 융복합형 산업을 형성하고 있으며, 커넥티드카, 웨어러블 기기, 로봇 시장의 성장은 기존의 기기 제어, 명령 실행 등을 지원하는 음성 인식 서비스 시장을 지속적으로 견인할 것이다.○ 최근 비대면 업무 증가는 음성 기록, 문서화 서비스의 수요를 크게 증가시킬 것으로 예상되며, 단순 반복 업무 자동화와 개인 식별 및 인증을 기반으로 한 금융, 정부, 의료, 기업 서비스 등 응용서비스 시장이 성장할 것으로 전망된다.  1. 음성 인식 서비스의 개요■ 음성 인식 기술의 개념음성 인식 기술이란 일반적으로 컴퓨터가 입력받은 인간의 음성언어를 인식해 문자로 변환하는 기술을 말하며, 인식된 결과에 대하여 음성 이해(Speech Understanding)과정을 통해 음성의 의미에부합하는 명령을 수행하는 기술을 포함한다.​최초의 음성 인식 기술은 컴퓨터로 입력된 음성 파형이 어떤 음과 가까운지, 어느 단어를 나타내는지를 인식하는 방식이었다. 컴퓨팅 성능의 향상으로 언어 모델링 개념이 도입되면서 대용량의 언어 모델을 기반으로 언어적 패턴의 특징을 학습해 입력되는 음성의 특징을 기반으로 음성을 인식하는 단계적 방식으로 발전하였다.​최근에는 종단간(End-to-End) 구조 기반의 학습 방식이 주목받고 있다. 종단간 방식은 인간 처럼 음성을 들은 후 바로 단어와 문장으로 인식하는 방법으로 단계적으로 처리하는 기존의 방식에 비해 더 많은 데이터와 연산과정이 필요한 난점을 가지고 있지만, 단계마다 필요한 작업들을 생략하고 시스템을 하나로 통합해 운영하기 때문에 시스템 복잡도를 낮출 수 있어 음성 인식의 정확성 및 속도의 향상을 위한 핵심기술이 될 것으로 평가받고 있다. 단계적 모델과 종단간 모델 개념도(출처 : Andrew Ng, Machine Learning Yearning)단계적 모델과 종단간 모델 개념도■ 음성 인식 서비스의 현황음성 인식 기술은 타 산업의 제품 및 서비스에 적용을 통해 새로운 비즈니스 창출이 가능한 기반기술로서 사용자의 편의성을 증진시키기 위해 지속적인 연구가 필요한 인간-컴퓨터 상호작용(Human-Computer Interaction) 기술 중 하나로 평가받고 있다. 그리고 음성인식 기술은 키보드나 기타 입력장치를 통한 입력이 아닌 사람의 음성을 통한 입력이 가능하므로 손발이 자유롭지 못한 상황에서도 정보를 전달할 수 있어 사용자가 다른 활동을 하면서도 서비스를 이용할 수 있는 점에서 안전성과 편의성을 지향하는 다양한 서비스에 도입되고 있다.​초기 음성 인식 기반 서비스는 AI 스피커, 스마트 홈, 스마트 카와 같이 음성 인터페이스를 통해 디바이스를 제어하는 분야를 중심으로 발전하였으며, 현재는 단순하고 반복적인 업무의 자동화를 위해 음성 인식 기술을 적용하고자 하는 수요가 크게 증가함에 따라 콜센터 문의 및 서비스 응대, 음성 기록의 자동 텍스트 변환 등 업무 자동화 분야로 영역이 확장되고 있다.​2. 시장동향 및 전망■ 시장 규모각종 산업분야에서 AI, IoT, AR 등의 첨단기술을 서비스에 접목하려는 시도가 계속되는 가운데 음성 인식 기술을 활용한 서비스가 증가하고 있다. 음성 인식 서비스의 세계 시장규모는 2021년 82억 7,200만 달러이며, 연평균 21.6% 수준으로 성장해 2026년 219억 9,500만 달러가 될 것으로 전망된다.​세계 음성 인식 시장을 지역별로 살펴보면, 미주 지역은 42%로 전 세계 음성 인식 시장의 가장 큰 점유율을 차지한다. 미주 지역의 점유는 미국의 독보적인 AI 기술력과 AI 분야에 대한 선제적인 투자를 이유로 볼 수 있다. 미주 지역은 2021년 33억5,000만 달러에서 연평균 18.3%로 성장해 2026년 77억6,800만 달러에 이를 것으로 전망된다. 유럽 지역은 2021년 16억6,500만 달러에서 연평균 19.9 %로 성장해 2026년 41억3,100만 달러에 이를 것으로 전망된다. 아시아 지역은 2021년 26억3,300만 달러에서 연평균 26.5%로 성장해 2026년 85억3,700만 달러에 달해 지역별 시장규모 중 가장 높다.​글로벌 차원의 음성 인식 서비스 시장규모 증가 추세는 전 세계 산업 및 기업의 AI 분야에 대한 관심이 증가하고 디지털화 및 기술혁신을 선호하는 정부 정책의 확대에 영향을 받은 것으로 판단된다.​ (왼쪽)세계 음성 인식 시장규모(`21~`16)/(오른쪽)지역별 음성 인식 시장규모(`21~`26)(출처 : Markets&Markets, Speech & Voice Recognition Market, 2021)​국내의 음성 인식 시장규모는 2021년 9억800만 달러이며, 연평균 27.4%로 성장해 2026년 30억4800만 달러가 될 것으로 전망된다. 이는 COVID-19의 영향으로 조성된 비대면 문화의 확산에 따라 언택트 스토어, 금융권 컨택센터, 상담 및 조사가 필요한 공공·의료 서비스 분야로 음성 인식 솔루션의 적용 범위가 확대되었으며, 팬데믹이 종식되더라도 편리한 언택트 소비 경험이 하나의 문화로 정착되어 지속적으로 음성 인식 응용 서비스 수요확대가 이어질 것으로 전망된다.​ (왼쪽) 국내 음성 인식 서비스 시장규모(`21~26)/(오른쪽) 아시아 국가별 음성 인식 시장규모(`21~`26)(출처 : Markets&Markets, Speech & Voice Recognition Market, 2021)​■ 적용 분야별 시장규모음성 인식 서비스는 소비자용 서비스, 자동차, 기업, 금융, 의료, 군사, 교육, 기타 분야로 구분할 수 있다. 그 중에서 소비자용 서비스가 가장 높은 점유율을 보이며, 2021년 36억1,900만 달러이고 연평균 27.6%로 성장해 2026년 122억5,000만 달러에 이를 것으로 전망된다. 소비자용 서비스가 높은 비중을 차지하는 이유는 모바일, 랩톱, 태블릿 및 스마트 홈 장치와 같은 소비자 제품의 음성 인식 기술 보급률이 높기 때문이다. 향후 웨어러블 디바이스에 대한 수요 증가 및 개인화된 장치에 대한 인증·보안 문제와 같은 요인은 음성 인식 시장의 성장을 촉진할 것으로 예상된다.​■ 국내외 주요기업 현황음성 인식 서비스 세계 시장에서는 애플, MS, 구글, 아마존 등의 빅테크 기업이 시장을 주도하고 있으며, 기업별 자사 AI 플랫폼을 중심으로 각종 AI 서비스를 확장하는 형태로 서비스를 제공하고 있다.​최근에는 빅테크 기업의 활발한 스타트업 인수합병(M&A)과 적극적인 R&D 투자를 기반으로 AI 플랫폼 및 서비스 영역을 확장하고 있다. 최근 클라우드 기반으로 AI 플랫폼을 운영하는 빅테크 기업들은 기술력 보유 스타트업에 대한 M&A를 수익증대 방법 중 하나로 여기고 있으며, M&A는 새로운 서비스를 위한 초기 투자비용이나 기술개발 비용을 지출하지 않고도 즉시 사업을 시작할 수 있고, 쉽게 경쟁우위를 확보할 수 있기 때문에 시장진출 전략으로 활용되고 있다. 국내 시장에서는 네이버, 카카오가 AI 플랫폼을 출시하고 주도해 왔으며, 최근에는 국내 AI 플랫폼 분야 스타트업의 성장으로 의료분야, 콜센터 상담분야 등 자체 음성 인식 기술을 적용해 다양한 응용서비스분야로 영역을 확장하고 있다.​​3. 분석자 인사이트 시장 영향요인 분석정부의 정책과 지원에 힘입어 AI 산업 분야는 빠르게 성장했고, 상용화 단계에 이르렀다. 이러한 시장 수요의 성장에 따라 음성 인식 분야 역시 응용 솔루션 등 최신 기술의 개발이 가속화할 것으로 전망된다. 음성 인식 기술은 인간의 언어를 사용하여 기계와 상호 작용할 수 있는 기능을 제공해 왔고, 최근에는 가장 빠른 생체 인식 및 문서화 기술로서 주목받고 있다.​음성 인식 기술은 종단간 방식의 적용과 자연어처리(NLP), 자연어 이해(NLU) 기술의 발달로 정확도가 견고해지면서, 다양한 산업별 서비스가 점차 확대되어 음성 인식 서비스의 대중화를 예상할 수 있다. 커넥티드카, 웨어러블 기기, 로봇 시장의 성장은 기존의 기기 제어, 명령 실행 등을 지원하는 음성 인식 서비스 시장을 지속적으로 견인할 것이며, 최근 비대면 업무 증가는 음성 기록 서비스의 수요를 크게 증가시킬 것이다. 이 외에도 음성 인식 기술을 기반으로 한 업무 자동화(전사, 고객상담 등), 개인 식별 및 인증은 금융, 정부, 의료, 기업 서비스 등 다양한 응용서비스 시장이 성장할 것으로 예상된다. ​음성 인식 시장 진입을 위한 촉진요인, 저해요인 등의 주요 요인은 위 표와 같이 정리할 수 있다.음성 인식 시장은 기반 기술에 종속성 때문에 기반 기술을 가지고 있지 않은 소규모 기업의 시장 진출은 어려울 수 있다. 하지만 공개된 API 활용 또는 AI 플랫폼 기업과의 협력을 통해 새로운 서비스를 발굴할 수 있다.​다각적 관점에서 차별화된 서비스의 개발을 위해 기업에서 발생되는 다양한 서비스 도메인별 음성 시나리오 데이터의 수집과 라벨링된 AI 학습데이터 구축은 서비스 경쟁력 확보 및 시장진출에 중요한 역할을 할 것으로 판단된다. 음성 인식 서비스는 콘텐츠, 플랫폼, 네트워크, 디바이스 등의 기술을 포함하는 융복합형 산업의 특성을 가지고 있기 때문에 시장 트렌드에 주목하고 꾸준한 분석을 통해 새로운 사업 기회를 발굴하려는 노력이 필요할 것이다.  □글쓴이: 데이터분석본부 호남지원 선임연구원 이은지 / 문의: eunji_lee@kisti.re.kr□ASTI MARKET INSIGHT 2021-067보고서 원문보기 ​ "
[AI 프로모션] 네이버의 AI Service 무료로 이용하세요! (~22/12/31) ,https://blog.naver.com/n_cloudplatform/222757985568,20220603,"​​​네이버가 제공하는 세계 최고 수준의 AI,우리 비즈니스에 적용된다면 어떤 마법이 펼쳐질까요?​​요즈음 AI 기술이 적용되지 않은 분야가 없을 정도로,인공지능을 활용한 비즈니스의 성장세가 눈에 띕니다.​네이버 클라우드 플랫폼 또한다양한 AI 상품들을 제공하고 있어필요한 영역에, 원하는 방식대로자유롭게 적용하실 수 있는데요!​여러분의 비즈니스가 네이버의 AI를 만나한 층 더 업그레이드 될 수 있도록,특별한 AI Service 프로모션을 준비했습니다.  ​🎁네이버 클라우드 플랫폼 AI Service30만원 크레딧 프로모션​​지금 바로 30만원 크레딧 받으시고,AI 서비스로 비즈니스 성장을 도모하세요!​  ​🚀NAVER Cloud for AI를소개해드립니다!​네이버 클라우드 플랫폼에서 제공하는 AI 상품군은아래와 같은 특징을 가지고 있습니다.​👍 세계 최고 수준의 AI 기술과 솔루션으로💡 다양한 아이디어와 시너지를 낼 수 있고📈 비즈니스 맞춤형 AI 솔루션까지 제공합니다​​이러한 장점 덕분에, 다양한 고객사에서네이버 클라우드 플랫폼의 AI 상품을 이용해다양한 서비스를 운영하고 있습니다.​ AI는 언어와 밀접한 관련이 있는데,네이버 클라우드 플랫폼의 AI 서비스는한국어 인식 및 분석 능력이 월등하여 선택했습니다.삼성카드네이버 클라우드 플랫폼의 CLOVA Chatbot 사용 후CLOVA 엔진은 다른 AI 엔진에 비해정확한 답변을 유도한다는 것을 체감할 수 있었습니다.현대해상 ​🔎어떤 AI 서비스를 이용할 수 있나요?​네이버 클라우드 플랫폼에서 제공하는AI Service 상품군에 사용하실 수 있습니다.​다양한 AI 서비스 중 필요한 서비스만 골라, 바로 사용해보세요!​어떤 상품이 있을지 궁금하신 분들을 위해네이버 클라우드 플랫폼이 추천하는AI 서비스 상품들을 소개해드립니다.​​ Clova OCR​문서나 이미지에서 텍스트를 추출해데이터로 간편히 변환할 수 있는 클로바 OCR​​ CLOVA Dubbing​텍스트만 있다면, 고품질의 음성으로 바로 변환!AI 보이스를 더한 나만의 더빙 영상을제작할 수 있는 클로바 더빙​​ CLOVA Chatbot​고객의 질문 의도를 이해해빠르고 정확한 대응을 가능케 하는 클로바 챗봇​ CLOVA Speech​마지막으로, 대화나 음성을 한번에 딱!텍스트로 슥- 변환해주어음성 메모, 자막 생성, 그리고 통화 녹취까지완벽하게 돕는 클로바 스피치​​​이외에도, 네이버 클라우드 플랫폼에는더 많은 AI 상품군이 준비되어 있습니다.​자세한 내용은 아래 버튼을  통해 확인해주세요 😀​ ​​  ​프로모션 안내AI Service 프로모션으로서비스 이용 크레딧 30만원 받아가세요!​​Clova Dubbing을 비롯, 모든 AI Service 카테고리에 적용 가능한30만원 크레딧을 제공하는 프로모션입니다.​또한, 30인 이상 기업 대상, 1:1 영업상담 완료 시에는추가 크레딧을 제공하니, 놓치지 마세요!​​📆 신청 기간2022년 5월 1일 ~ 12월 31일* 신청 후 일주일 내 가입하신 이메일을 통해 안내드립니다.​🙋‍♂️ 신청 대상네이버 클라우드 플랫폼 신규 이용자* 신규 가입자 또는 이전에 유상 이용 이력이 없는 유저를 대상으로 합니다.​🕑 이용 기간크레딧 등록 후 6개월​​​ ​ [유의사항]크레딧 신청 후 네이버 클라우드 플랫폼 계정에 등록된 이메일로 안내드립니다.메일함을 꼭 확인해 주세요. 크레딧 유효기간은 등록일로부터 6개월입니다.본 프로모션은 네이버 클라우드 플랫폼 신규이용자(신규 가입자 또는 유상 이용 이력이 없는 고객)를 대상으로 하며 조기에 종료될 수 있습니다. 적용 상품 AI · NAVER API / API Gateway / AiTems / CLOVA AiCall / CLOVA AiCall-Call / CLOVA Chatbot / CLOVA OCR / CLOVA Speech / CLOVA Voice / CLOVA Dubbing / CLOVA Speech Synthesis (CSS) / CLOVA Speech Recognition (CSR) / CLOVA Premium Voice (CPV) / Papago NMT / Papago SMT / Papago Doc Translation / Papago Website Translation / Papago Korea Name Romanizer / Papago Image to Text Translation / Papago Image to Image Translation / Pose Estimation / Object Detection / nShortURL / CAPTCHA / Search Trend / Maps ​ "
아이폰 설정 단축어 모음 ,https://blog.naver.com/goodman_korea/223009312937,20230208,"​ 사파리에서 URL 열기로 열면 해당 설정 메뉴로 바로 열린다.자동잠금을 한번에 열기 위해 검색하다 찾아서 기록해 놓는다. ​iCloud(아이클라우드)iCloud: prefs:root=CASTLEiCloud Backup: prefs:root=CASTLE&path=BACKUP​Wireless Radios(WIFI, 무선설정)Wi-Fi: prefs:root=WIFIBluetooth: prefs:root=BluetoothCellular: prefs:root=MOBILE_DATA_SETTINGS_ID​Personal Hotspot(개인용핫스팟)Personal Hotspot: prefs:root=INTERNET_TETHERINGPersonal Hotspot ⇾ Family Sharing: prefs:root=INTERNET_TETHERING&path=Family%20SharingPersonal Hotspot ⇾ Wi-Fi Password: prefs:root=INTERNET_TETHERING&path=Wi-Fi%20Password​VPNVPN: prefs:root=General&path=VPNDNS: prefs:root=General&path=VPN/DNS​Notifications(알림)Notifications: prefs:root=NOTIFICATIONS_IDNotifications ⇾ Siri Suggestions: prefs:root=NOTIFICATIONS_ID&path=Siri%20Suggestions​Sounds(사운드)Sounds: prefs:root=SoundsRingtone: prefs:root=Sounds&path=Ringtone​Do Not Disturb (집중모드)Do Not Disturb: prefs:root=DO_NOT_DISTURBDo Not Disturb ⇾ Allow Calls From: prefs:root=DO_NOT_DISTURB&path=Allow%20Calls%20From​Screen Time (스크린 타임)Screen Time: prefs:root=SCREEN_TIMEScreen Time ⇾ Downtime: prefs:root=SCREEN_TIME&path=DOWNTIMEScreen Time ⇾ App Limits: prefs:root=SCREEN_TIME&path=APP_LIMITSScreen Time ⇾ Always Allowed: prefs:root=SCREEN_TIME&path=ALWAYS_ALLOWED​General (일반)General: prefs:root=GeneralGeneral ⇾ About: prefs:root=General&path=AboutGeneral ⇾ Software Update: prefs:root=General&path=SOFTWARE_UPDATE_LINKGeneral ⇾ CarPlay: prefs:root=General&path=CARPLAYGeneral ⇾ Background App Refresh: prefs:root=General&path=AUTO_CONTENT_DOWNLOADGeneral ⇾ Multitasking (iPad-only): prefs:root=General&path=MULTITASKINGGeneral ⇾ Date & Time: prefs:root=General&path=DATE_AND_TIMEGeneral ⇾ Keyboard: prefs:root=General&path=KeyboardGeneral ⇾ Keyboard ⇾ Keyboards: prefs:root=General&path=Keyboard/KEYBOARDSGeneral ⇾ Keyboard ⇾ Hardware Keyboard: prefs:root=General&path=Keyboard/Hardware%20KeyboardGeneral ⇾ Keyboard ⇾ Text Replacement: prefs:root=General&path=Keyboard/USER_DICTIONARYGeneral ⇾ Keyboard ⇾ One Handed Keyboard: prefs:root=General&path=Keyboard/ReachableKeyboardGeneral ⇾ Language & Region: prefs:root=General&path=INTERNATIONALGeneral ⇾ Dictionary: prefs:root=General&path=DICTIONARYGeneral ⇾ Profiles: prefs:root=General&path=ManagedConfigurationListGeneral ⇾ Reset: prefs:root=General&path=ResetControl CenterControl Center: prefs:root=ControlCenterControl Center ⇾ Customize Controls: prefs:root=ControlCenter&path=CUSTOMIZE_CONTROLS​Display (디스플레이 및 밝기)Display: prefs:root=DISPLAYDisplay ⇾ Auto Lock: prefs:root=DISPLAY&path=AUTOLOCKDisplay ⇾ Text Size: prefs:root=DISPLAY&path=TEXT_SIZE​Accessibility (손쉬운사용)Accessibility: prefs:root=ACCESSIBILITY​Wallpaper (배경화면)Wallpaper: prefs:root=Wallpaper​Siri (시리 및 검색)Siri: prefs:root=SIRIApple PencilApple Pencil (iPad-only): prefs:root=Pencil​Face IDFace ID: prefs:root=PASSCODE​Emergency SOS (긴급 구조 요청)Emergency SOS: prefs:root=EMERGENCY_SOS​Battery (배터리)Battery: prefs:root=BATTERY_USAGEBattery ⇾ Battery Health (iPhone-only): prefs:root=BATTERY_USAGE&path=BATTERY_HEALTH​Privacy (개인정보 보호 및 보안)Privacy: prefs:root=PrivacyPrivacy ⇾ Location Services: prefs:root=Privacy&path=LOCATIONPrivacy ⇾ Contacts: prefs:root=Privacy&path=CONTACTSPrivacy ⇾ Calendars: prefs:root=Privacy&path=CALENDARSPrivacy ⇾ Reminders: prefs:root=Privacy&path=REMINDERSPrivacy ⇾ Photos: prefs:root=Privacy&path=PHOTOSPrivacy ⇾ Microphone: prefs:root=Privacy&path=MICROPHONEPrivacy ⇾ Speech Recognition: prefs:root=Privacy&path=SPEECH_RECOGNITIONPrivacy ⇾ Camera: prefs:root=Privacy&path=CAMERAPrivacy ⇾ Motion: prefs:root=Privacy&path=MOTIONPrivacy ⇾ Analytics & Improvements: prefs:root=Privacy&path=PROBLEM_REPORTINGPrivacy ⇾ Apple Advertising: prefs:root=Privacy&path=ADVERTISING​App StoreApp Store: prefs:root=STOREApp Store ⇾ App Downloads: prefs:root=STORE&path=App%20DownloadsApp Store ⇾ Video Autoplay: prefs:root=STORE&path=Video%20Autoplay​Wallet (지갑)Wallet: prefs:root=PASSBOOK​Passwords (암호)Passwords: prefs:root=PASSWORDS​MailMail: prefs:root=MAILMail ⇾ Accounts: prefs:root=ACCOUNTS_AND_PASSWORDS&path=ACCOUNTSMail ⇾ Accounts ⇾ Fetch New Data: prefs:root=ACCOUNTS_AND_PASSWORDS&path=FETCH_NEW_DATAMail ⇾ Accounts ⇾ Add Account: prefs:root=ACCOUNTS_AND_PASSWORDS&path=ADD_ACCOUNTMail ⇾ Preview: prefs:root=MAIL&path=PreviewMail ⇾ Swipe Options: prefs:root=MAIL&path=Swipe%20OptionsMail ⇾ Notifications: prefs:root=MAIL&path=NOTIFICATIONSMail ⇾ Blocked: prefs:root=MAIL&path=BlockedMail ⇾ Muted Thread Action: prefs:root=MAIL&path=Muted%20Thread%20ActionMail ⇾ Blocked Sender Options: prefs:root=MAIL&path=Blocked%20Sender%20OptionsMail ⇾ Mark Addresses: prefs:root=MAIL&path=Mark%20AddressesMail ⇾ Increase Quote Level: prefs:root=MAIL&path=Increase%20Quote%20LevelMail ⇾ Include Attachments with Replies: prefs:root=MAIL&path=Include%20Attachments%20with%20RepliesMail ⇾ Signature: prefs:root=MAIL&path=SignatureMail ⇾ Default Account: prefs:root=MAIL&path=Default%20Account​Contacts (연락처)Contacts: prefs:root=CONTACTS​Calendar (캘린더)Calendar: prefs:root=CALENDARCalendar ⇾ Alternate Calendars: prefs:root=CALENDAR&path=Alternate%20CalendarsCalendar ⇾ Sync: prefs:root=CALENDAR&path=SyncCalendar ⇾ Default Alert Times: prefs:root=CALENDAR&path=Default%20Alert%20TimesCalendar ⇾ Default Calendar: prefs:root=CALENDAR&path=Default%20Calendar​Notes (메모)Notes: prefs:root=NOTESNotes ⇾ Default Account: prefs:root=NOTES&path=Default%20AccountNotes ⇾ Password: prefs:root=NOTES&path=PasswordNotes ⇾ Sort Notes By: prefs:root=NOTES&path=Sort%20Notes%20ByNotes ⇾ New Notes Start With: prefs:root=NOTES&path=New%20Notes%20Start%20WithNotes ⇾ Sort Checked Items: prefs:root=NOTES&path=Sort%20Checked%20ItemsNotes ⇾ Lines & Grids: prefs:root=NOTES&path=Lines%20%26%20GridsNotes ⇾ Access Notes from Lock Screen: prefs:root=NOTES&path=Access%20Notes%20from%20Lock%20Screen​Reminders (미리 알림)Reminders: prefs:root=REMINDERSReminders ⇾ Default List: prefs:root=REMINDERS&path=DEFAULT_LIST​Voice Memos (음성 메모)Voice Memos: prefs:root=VOICE_MEMOS​Phone (전화)Phone: prefs:root=Phone​Messages (메세지)Messages: prefs:root=MESSAGES​FaceTimeFaceTime: prefs:root=FACETIME​Maps (지도)Maps: prefs:root=MAPSMaps ⇾ Driving & Navigation: prefs:root=MAPS&path=Driving%20%26%20NavigationMaps ⇾ Transit: prefs:root=MAPS&path=Transit​Compass (나침판)Compass: prefs:root=COMPASS​Measure (측정)Measure: prefs:root=MEASURE​Safari (사파리)Safari: prefs:root=SAFARISafari ⇾ Content Blockers: prefs:root=SAFARI&path=Content%20BlockersSafari ⇾ Downloads: prefs:root=SAFARI&path=DOWNLOADSSafari ⇾ Close Tabs: prefs:root=SAFARI&path=Close%20TabsSafari ⇾ Clear History and Data: prefs:root=SAFARI&path=CLEAR_HISTORY_AND_DATASafari ⇾ Page Zoom: prefs:root=SAFARI&path=Page%20ZoomSafari ⇾ Request Desktop Website: prefs:root=SAFARI&path=Request%20Desktop%20WebsiteSafari ⇾ Reader: prefs:root=SAFARI&path=ReaderSafari ⇾ Camera: prefs:root=SAFARI&path=CameraSafari ⇾ Microphone: prefs:root=SAFARI&path=MicrophoneSafari ⇾ Location: prefs:root=SAFARI&path=LocationSafari ⇾ Advanced: prefs:root=SAFARI&path=ADVANCED​Health (건강)Health: prefs:root=HEALTH​Shortcuts (단축어)Shortcuts: prefs:root=SHORTCUTS​Music (음악)Music: prefs:root=MUSICMusic ⇾ Cellular Data: prefs:root=MUSIC&path=com.apple.Music:CellularDataMusic ⇾ Optimize Storage: prefs:root=MUSIC&path=com.apple.Music:OptimizeStorageMusic ⇾ EQ: prefs:root=MUSIC&path=com.apple.Music:EQMusic ⇾ Volume Limit: prefs:root=MUSIC&path=com.apple.Music:VolumeLimitTVSettings ⇾ TV: prefs:root=TVAPP​Photos (사진)Photos: prefs:root=Photos​Camera (카메라)Camera: prefs:root=CAMERACamera ⇾ Record Video: prefs:root=CAMERA&path=Record%20VideoCamera ⇾ Record Slo-mo: prefs:root=CAMERA&path=Record%20Slo-mo​Books (도서)Books: prefs:root=IBOOKS​ "
Yoon Lost a foothold in international on the recognition of NK’s nuclear possession based far right ,https://blog.naver.com/kimjc0000/223089113991,20230430,"Yoon Seok-yeol Lost a foothold in international negotiations on the recognition of North Korea’s nuclear possession based on the far right in Korea​​President Yoon Seok-yeol sympathized with the “recognition of North Korea as a nuclear state” due to the demand for the use of US nuclear weapons in North Korea in the domestic far-right-based negotiations with the United States, removing the foothold for participation in international negotiations in the disarmament negotiations led by NATO and the Biden administration.President Yoon told Secretary of Defense Austin at the Military Command and Control Center (NMCC) at the US Department of Defense on the 27th, ""I have complete confidence in the United States' steadfast extended deterrence commitment."" ""We will face a determined and overwhelming response from the US allies and the ROK Armed Forces,"" he said, referring to the use of nuclear weapons by North Korea.The U.S. Department of Defense announced that the Nuclear Specialization Team (NCT) and the U.S. Army's Nuclear Disablement Teams (NDT·Nuclear Disablement Teams) in Korea's Armed Forces Chemical, Biological and Biological Defense Command to respond to nuclear attacks in line with the Korea-U.S. FS)' and the joint training during the Ssangryong exercise. The Korean government did not disclose the existence of the Nuclear Specialization Team (NCT) to the Armed Forces Chemical, Biological, and Defense Command under the direct control of the Ministry of National Defense, and the US Department of Defense first disclosed it through the Korea-US summit.The White House prepared a negotiating proposal to “recognize North Korea as a nuclear state” in response to South Korea’s demand for “use of US nuclear weapons against North Korea” at the Korea-US summit, and this is confirmed by the ROK-US joint military drills in response to nuclear weapons.President Yoon tolerated the US strategy of “recognizing North Korea’s nuclear possession and talking through nuclear disarmament negotiations” through the “promise to use US nuclear weapons when North Korea uses nuclear weapons” at the White House press conference of the summit with President Biden.Two days later, right after his speech at Harvard University, President Yoon announced in a question and answer session, “I oppose nuclear disarmament negotiations, not recognition of nuclear possession,” and replaced it with a domestic announcement separate from the joint declaration at the summit.In a Q&A at Harvard University (28th), President Yoon said, ""The Republic of Korea has such a technological foundation that it can become nuclear-armed quickly, even within a year, if it decides to become nuclear-armed."" “We are closely watching the war situation. Depending on the situation, we will work with the international community to ensure that international norms and laws are respected,” he said. “There may be various options.”President Yoon showed a diplomatic strategy that relied on US intelligence agencies by applying a negotiating policy of restricting participation in the international community in “in case of North Korean nuclear use” and “in case of mass attack on Russian civilians”.President Yoon’s repeated strategy of “diplomacy in response to enemy actions” was abused to expand domestic far-right support and abuse international local warfare under Reagan’s hard-line conservative system, which prioritized covert information transactions over open policies.In his speech to the U.S. Congress, President Yoon said, ""We must make it clear to North Korea that, as President Reagan said, 'there is a point that we cannot tolerate and there is a line that we must never cross,'"" expressing the Republican Party's application of the Reagan regime strategy. revealedIn his speech to Congress, President Yoon quoted Secretary of State Shultz’s 1983 speech to the United Nations under the Reagan regime, “South Korea, a shining model of an American-backed economy,” and said, “There is a clear comparison now between South Korea, which chose liberal democracy, and North Korea, which chose communist totalitarianism.” Prepared.At the summit press conference, President Yoon said, “Korea and the US are communicating and sharing necessary information on that part” in response to the question of the’suspicion of wiretapping in the United States. Regarding issues with complex variables, I plan to watch the results of the US investigation over time and communicate fully,” he replied, effectively condoning the wiretapping of US intelligence agencies in Korea.In particular, in a joint statement from the summit, President Yoon said, “President Biden and President Yoon reaffirm their commitment to diplomacy with North Korea as the only means to achieve lasting peace on the Korean Peninsula, and urge North Korea to return to negotiations. ” he signed.President Yoon’s Biden administration’s’opposition to nuclear disarmament’ was held at Harvard University, saying, “The Washington Declaration does not recognize North Korea as a nuclear power, but rather denies North Korea’s possession of nuclear weapons. He said, “I oppose it,” and downgraded it from the press conference at the summit to a “single interpretation” that is separate from international agreements.Regarding the outcome of the summit meeting two days ago, President Yoon said, “The ROK and the U.S. will have an immediate summit meeting in the event of a nuclear attack by North Korea, and through this, take a swift, overwhelming, and decisive response using all the forces of the alliance, including the U.S. nuclear weapons. We made a promise,” he said, officializing the “Nuclear war with North Korea’s possession and use” theory.Based on the fact that the 'Joint Declaration' is superior in international agreements and that the 'Washington Declaration' is a separate agreement between Korea and the United States and has no international effect as it is applied only to both countries, President Yoon adopted a method of acknowledging this at an official press conference and later denying it. seems to beThe Washington Declaration reaffirmed that “any nuclear attack on South Korea by North Korea will be met with an immediate, overwhelming and decisive response. President Biden emphasized that the US extended deterrence against Korea is supported by mobilizing all US capabilities, including nuclear weapons. Furthermore, the United States will further enhance the regular visibility of US strategic assets to South Korea, as evidenced by the upcoming planned call by US strategic nuclear submarines to South Korea, and will expand and deepen cooperation between the two militaries.” Instead, it was specified as a 'strategic nuclear submarine calling at a port in Korea' and mobilized the word 'North Korea's nuclear attack' to document 'North Korea's possession of nuclear weapons'.On the 26th, the Defense Video Information Distribution Service (DVIDS) of the U.S. Department of Defense disclosed the joint training between the Nuclear Disabling Team (NDT) and the Korean Nuclear Specialization Team (NCT) on the Korean Peninsula from March 20 to 24, and Yonhap News confirmed on the 29th. revealed​​International negotiations, far-right base, Russian civilian attacks, Reaganomics hardline conservatives, recognition of North Korea's nuclear possession, Yoon Seok-yeol diplomacy, Korea Nuclear Specialization Team (NCT), Korea-US summit, nuclear disarmament negotiations, nuclear war disarray​​ "
[Talk to ChatGPT] 챗 GPT 와 즉시 음성 대화 - 영어 공부 4 ,https://blog.naver.com/iammaster2/223078694908,20230418,"음성으로 ChatGPT 를 이용할 수 있는 크롬 확장 프로그램 Talk to ChatGPT 를 테스트하였다. 영어 회화를 연습하는 경우 유용한데, 전편 게시글에서 소개한 Voice Control for ChatGPT 프로그램과 유사한 기능을 갖고 있다.​차이점이라면 Voice Control ChatGPT 프로그램은 반드시 마이크 버튼을 누른 상태에서만 음성 인식을 할수 있는 반면에, Talk to ChatGPT 프로그램은 일단 시작하면, 기본적으로 항시 음성 인식 대기 상태가 된다. 다른 특성은 유사하므로 취향에 따라 프로그램을 선택하면 된다.​Talk to ChatGPT 설치 방법은, 일반적인 구글 확장 프로그램 설치 게시글을 참조하면 된다(https://blog.naver.com/iammaster2/223072838159). 설치 이후 OpenAI 사이트에서 Try ChatGPT 버튼을 선택하면 우측 상단에 프로그램 창이 나타난다(아래 화면 참조).​​​ ​프로그램 창에서 START 버튼을 선택하면 프로그램 창 아래쪽이 빨간색으로 표시되는데, 이는 음식 인식 대기 상태라는 의미가 된다. 즉 사용자가 음성 대화를 시작할 수 있다는 뜻이 된다.​​​ ​예를 들어, 전편에 있는 질문을  텍스트로 입력하면(관광목적으로 미국 공항에 도착한 여행자와 출입국 심사관과의 대화. 심사관부터 시작. 영어로 진행.), 전체 대화를 영어로 들려준다. 이때 프로그램 창 아래쪽은 녹색으로 변경된다. 즉 ChatGPT 가 음성으로 대화를 하고 있음을 알려준다.​​​ ​​전편에 있는 두번째 예제(나는 LA 공항에 도착한 여행자이고 너는 출입국 심사관이라고 가정. 이런 경우에 1:1 대화를 영어로 진행.) 를 텍스트로 입력하게 되면, 그 이후에는 영어로 1:1 음성 대화를 나눌 수 있다. ​만일 음성 인식 기능을 막고 싶은 경우에는, 프로그램 창에 있는 첫번째 마이크 아이콘을 선택하면 된다. 그러면 아래처럼 아이콘이 변경된다. 또다른 방법은 ALT + SHIFT + H 키를 동시에 누르면 된다. 해제하고 싶은 경우에는 한번 더 수행하면 원래대로 돌아간다.​​​ 음성 인식이 차단되었기때문에 사용자는 텍스트로만 질문을 할 수 있지만, 답변은 계속 음성으로 돌아온다. 답변마저 음성을 차단하고 싶은 경우에는 두번째 스피커 버튼을 선택하면 된다.​​​ ​​현재 ChatGPT 답변이 음성으로 나오고 있는 도중에, 음성을 끊고 싶은 경우에는 세번째 아이콘을 선택하면 된다.  답변 도중에 음성이 끊어지고, 다음 사용자 질문을 기다리게 된다.​​​ ​프로그램 창에 있는 마지막 아이콘은 설정 버튼이다. 언어 선택 및 음성 대화 속도 등을 조절할 수 있다.​​​ ​설정 옵션 중에서 Automatic send 가 기본으로 되어있음을 알 수 있다. 즉 사용자 음성 대화가 끝나면 즉시 ChatGPT 에게 전송되도록 되어 있다. 이 부분이 Voice Control for ChatGPT 와 다른 점이다.​그런데 만일 이 기본 옵션을 사용하지 않으면, 대화 끝에 특정한 말(예를 들어 send message now)을 붙여야만 전송되기때문에 불편할 수 있다.​특이한 옵션으로는 AI voice and language 와 Speech recognition language 옵션이 각각 따로 있다. 질문은 한국어로 하고 답변은 영어로 할 수 있는 것인지 테스트해보았더니 제대로 되지 않았다. 한국어 억양으로 영어를 한다던지 영어 발음으로 한국어를 하는 어색한 상태가 되었다. 양쪽 모두 같은 언어로 되어있는 기본 옵션을 사용하면 된다.​Voice Control for ChatGPT 와 비교하면, 마이크 버튼 선택없이 즉시 음성으로 대화를 할수 있는 것은 편리하지만, 프로그램 안정성이 다소 흔들리는 경우도 있다. 사용중에 작동이 잘되지 않아서 ChatGPT 를 다시 로그인해야 되는 경우가 가끔 있었다. ​​​ "
음성인식을 통한 대화 인공지능 for ChatGPT ,https://blog.naver.com/downfa11/223105112758,20230518,"chatGPT API 공개됐을때 잠깐 만져보면서 만든거 chatGPT API가 공개됐음원문 내용 : Whisper API와 함께 OpenAI에서 공개했습니다. Whisper는 Voice2Text 변환 모델로 ...blog.naver.com 사용자의 음성을 인식해서 대답해주는 인공지능임ㅇㅇ​​Sounddevice 라이브러리를 사용해 사용자의 음성을 녹음한 뒤, Speech-recognition를 통해 인식해 답변하는 구조​대답은 chatGPT API를 활용해서 터미널에 출력해준다. ​​chatGPT 안쓰고 직접 챗봇을 꾸려도 되지만... 발화 데이터가 구리니까 재미없음 이미 깃허브에잇슴​​​​​​내가 직접 녹음한 음성은 음질이 구려서 그런지 Speech-recognition이 한국말로 인식하질 않음. 이유는 모름...​멍청하게 계속 혼자 소리내면서 녹음했는데도 안되더라ㅠ ​10초동안 녹음한 내용을 파일로 저장하도록 함수를 구현했음 def voice_recorder(seconds, file):    print(""Recording Started…"")    recording = sounddevice.rec((seconds * 44100), samplerate= 44100, channels=2)    sounddevice.wait()    write(file, 44100, recording)    print(""Recording Finished"")voice_recorder(10, ""test.wav"") ​하지만 아나운서 녹음 파일같은거 가져와서 실행해보면 아주 잘 인식해서 chatGPT가 답변해준다는 점에서 성공ㅋ블로그에 올리게 됐음.​​​​api_key 만 지웠어요 안지우니까 깃허브에서 메일옴ㅋㅋㅋㅋㅋ​ GitHub - downfa11/VoiceChat-via-ChatGPT: with speech-recognition, sounddevice, chatGPTwith speech-recognition, sounddevice, chatGPT. Contribute to downfa11/VoiceChat-via-ChatGPT development by creating an account on GitHub.github.com ​ "
대화형 인공지능(Conversational AI) 음성인식 기술이란? ,https://blog.naver.com/etripr/222953105274,20221212,"안녕하세요 ETRI 블로그의 에뚜리 통신원입니다.​여러분은 스마트 스피커, AI 비서와 같은 서비스를 잘 이용하고 계신가요? 애플의 ‘시리’, 삼성의 ‘빅스비’, 구글의 ‘Google 어시스턴트’에서 아마존의 ‘알렉사’까지! 음성인식 기술이 적용된 여러 제품은 이미 생활 곳곳에 널리 퍼져있습니다. ​얼마 전 ETRI에서 한국어와 영어, 중국어, 일본어 등 세계 주요 24개 언어를 음성으로 인식, 문자로 변환할 수 있는 『대화형 인공지능(Conversational AI) 기술』을 개발했습니다. 이 음성인식 기술의 성능이 구글 등 글로벌 업체와 비교해서도 한국어는 우위이고, 타 언어에서도 대등한 수준을 보였다고 하는데요.​그렇다면 이 ‘대화형 인공지능 기술(Conversational AI)’ 에 대해 알아볼까요?  대화형 인공지능 기술이란?대화형 인공지능(AI)은 인공지능 분야에 속하는 기술로,  AI는 학습을 통해 언어를 분석하고 이해함으로써 인간의  음성을 인식하고 인간과 자유롭게 대화하거나 음성을 텍스트로 변환하는 등의 기능을 제공합니다. ​사실 인간의 대화는 매우 복잡합니다. 친구끼리의 대화에서는, 상대방이 말을 하기 전에, 상대방이 무슨 말을 할지 미리 알아챌 때도 있습니다. 대화형 인공지능(AI)은 대화의 맥락을 이해하고 지능적인 반응을 통해 사람과 유사한 수준의 대화가 가능하도록 합니다.​​​ 인공지능의 음성인식​대화형 인공지능 기술이 발전하기 위해선 먼저 AI가 정확히 인간의 음성을 인식해야 합니다. AI가 음성을 인식하기는 쉽지 않습니다. 음성에 들어있는 언어와 발음의 종류, 파형, 음색, 높낮이 등의 정보를 음성 신호에서 얻어내 별도의 처리 과정을 거쳐야만 인식할 수 있습니다. 또한, AI의 음성인식을 위한 기계학습을 위해서는 대규모의 학습데이터가 필요하다는 점도 음성인식 기술의 어려운 점입니다.​최근에는 AI 기술 중 심층학습(딥러닝) 기술의 발전과 함께 음성인식의 정확도가 향상되면서, 보다 다양한 상황에서 음성인식을 활용할 수 있게 되었습니다. 딥러닝을 통해 대량의 데이터로부터 AI가 스스로 음성  패턴을 학습하고, 이를 통해 우리 생활 전반에서 다양하게 쓰이고 있습니다.​ 음성인식 기술의 원리는?​음성인식 기술은 STT(Speak to Text) 또는 자동 음성인식(ASR, Automatic Speech Recognition)이라고 하는데요. 기계 학습 또는 인공 지능(AI) 기술을 사용하여 사람의 음성을 읽을 수 있는 텍스트로 처리하는 것을 말합니다. 그렇다면 이 음성인식 기술은 어떻게 작동하는 걸까요?​1. 음향분석(음성의 디지털화)우선, 인식하고 싶은 음성을, 마이크 등의 입력 장치로 녹음하여 입력합니다. 그리고, 입력한 음성에서 특미량(특징이 수치화된 것)을 추출하여 컴퓨터가 인식하기 쉬운 데이터로 성형합니다. 이 작업을 ""음향 분석""이라고 합니다. 입력된 음성 데이터의 주파수나 강약, 소리와 소리의 간격, 시계열 등 다양한 특징을 추출하여 컴퓨터가 인식하기 쉬운 데이터로 변환합니다.일상 대화에서는 잡음 등이 들어있는 원시 데이터에서도 음성을 들을 수 있지만, 컴퓨터는 그대로는 음성을 인식하기 어려울 수 있기 때문에, 아날로그 신호인 음성을 디지털 신호의 파형으로 변환해, 음소를 추출해, 노이즈를 제거합니다.​​2. 음파로부터 음소 추출 모델음향 모델에서는 입력된 음성에서 추출된 특징이 어느 ""음소""에 가까운지를 찾아내어 텍스트화하는 작업을 합니다. 음소는 의미의 차이와 관련된 최소 음성 단위를 의미합니다. 입력된 음성과 컴퓨터가 가지는 통계적으로 처리한 데이터를 비교해, 그 음성이 「아」인지 「아니」인지 「오」인지, 제일 특징이 가까운 음소를 선택해 출력합니다.​ 3. 발음 사전에 의한 패턴 매치음소를 추출한 것은 단지 알파벳의 나열로, 의미가 있는 단어와는 다르기 때문에이 단계에서는 음소를 의미있는 단어로 변환하는 작업이 수행됩니다. 이 단계에서 사용되는 모델이 「발음 사전」입니다.​? 발음 사전이란?발음 사전이란 단어와 그 발음이 세트로 등록된 사전입니다. 예를 들어 ""좋아""라는 단어의 발음은 ""zoa""라고 등록되어 있습니다. 이전 단계에서 확인된 음소와 발음 사전을 일치시켜 단어로 변환합니다. 그러면 그냥 알파벳이었던 문자열이 한국어 텍스트로 변환됩니다. 예를 들어 'zoa'라는 음소가 있을 경우 '좋아', '조아' 등의 단어가 단어의 후보가 됩니다.​​4. 적절한 문장 조립(언어 모델)「언어 모델」이란, 발음 사전으로 특정한 단어와 단어, 품사 등의 출현 빈도를 모델화한 것입니다. 문장의 학습 데이터를 대량으로 축적·처리하여 출현 빈도를 기록한 후, 인식하고자 하는 데이터와 비교하여 출현할 확률이 높은 문장으로 생성합니다. 여기까지의 경과를 거쳐, 최종적으로, 자연스러운 문장이 될 가능성이 높은 문장이 텍스트로서 출력됩니다.​​ 우리 생활 속 AI 음성인식 활용 사례 음성의 식별에 AI의 딥 러닝이 활용된 것으로, 음성 인식의 정밀도는 크게 향상되었습니다. 최근 몇 년 사이에 다양한 제품들이 개발된 것을 보아도 알 수 있죠. AI를 활용한 음성인식에서는 구체적으로 무엇을 할 수 있는지, 어떻게 비즈니스에 활용할 수 있는지 구체적인 사례와 함께 소개해 보도록 하겠습니다.​ 1. 회의록의 자동 작성이나 녹음본 문자 변환​ <출처: 네이버 CLOVA 홈페이지>​회의 및 회의 내용을 기록하는 회의록에는 정확성과 속도가 모두 필요합니다. 그러나 최근에는 음성 인식 AI를 탑재하고 실시간으로 자동으로 회의록을 작성하거나, 녹음데이터를 문자로 변환해 주는 도구도 늘고 있습니다. 특히 최근에 네이버 클로바 노트 서비스 등은 높은 음성 인식율을 바탕으로 많은 직장인들과 학생들에게 큰 도움이 되고 있죠. 물론 담당자에 따라, 회의록을 작성하는 시간이나 내용의 정확성에 편차가 발생할 수 있지만, 시간을 줄여준다는 점에서는 이견이 없습니다.​​ 2. 다국어 간의 통역·번역​ ​AI 음성인식을 응용한 번역기를 도입하여 다국어 간의 통역과 번역이 가능합니다. 지금까지 통역을 통해서 하고 있던 다른 언어간 커뮤니케이션이 가능합니다. 이런 기술을 바탕으로 다국어번역 기능을 제공하는 화상회의 플랫폼 서비스까지 런칭되고 있어서 대면뿐 아니라 웹 회의에 사용하는 번역 시스템도 실용화되고 있습니다.​​ 3. 말하기만으로 기기 조정​음성인식 AI 스피커를 사용하면 스피커에 음성으로 말을 걸어 연동하는 에어컨 스위치를 켜거나 오늘의 날씨를 검색하거나 음악을 듣는 등 음성만으로 기기를 조작할 수 있습니다. 그래서 요즘 많은 분들이 ‘스마트홈‘을 구축하고 아침에 일어나서 커텐틀 여는 것부터 저녁에 자기 전 조명을 끄는 것까지 말하는 것만으로 다 해내기도 합니다.​가정 뿐만 아니라 산업에서 활용한다면, 혼자서도 몇 대의 로봇을 조작하면서 작업할 수도 있을 것입니다.​ <출처: 빅스비 / 아마존 / 애플 / 구글 홈페이지>​ 4. 컴퓨터와의 대화스마트폰 탑재 어시스턴트('Siri'나 'Google 어시스턴트' 등), 웹사이트에서 문의 사용을 담당하는 '채팅봇' 등 인간과 대화할 수 있는 서비스도 등장하고 있습니다. 이들은 AI 음성인식 기술에 '자연언어처리(NLP)' 기술을 결합하여 컴퓨터와 인간의 대화를 성립시키고 있습니다. 자연 언어 처리란 인간이 말하는 언어의 의미를 컴퓨터로 분석하는 기술을 말합니다.​이와 같이 음성 인식에 다른 AI 기술을 결합하면 음성 인식 AI의 가능성이 더욱 넓어집니다.  앞으로 더 발전해 갈 대화형 인공 지능 음성인식 기술어떠신가요? 앞으로 더욱 더 발달 될 음성인식 AI 기술 기대 되시나요?​ETRI는 2018년 평창 동계 올림픽에도 공식 자동통역 서비스에 핵심 기술을 제공한 바 있는데요. 이런 음성인식 AI 기술이 발전하게 되면 중소·벤처 기업에서부터 학교, 개인 개발자 등 다양한 사용자들이 해당 기술을 사용 할 수 있게 되고 ICT 산업 전반의 발전에 도움이 됩니다.​ETRI에서도 우리나라 인공지능 분야의 글로벌 경쟁력을 높이고 기술 자주권을 확보하는 데 도움이 되기 위해 연구를 계속해 가겠습니다..  ​ ​​ ​ "
아두이노 니클라 보이스 (NICLA VOICE) ,https://blog.naver.com/ubicomputing/223105404355,20230518,"아두이노 니클라 보이스(NICLA VOICE) 개요본 제품은 아두이노 니클라 보이스 입니다.임베디드 음성 인식 어플리케이션(always-on speech recognition on the edge)을 위해 디자인된 제품입니다.Syntiant의 강력한 NDP120 Neural Decision processor를 탑재하여 여러개의 AI 알고리즘을 실행할 수 있습니다.특징The 22.86 x 22.86 mm Nicla Voice allows for easy implementation of always-on speech recognition on the edge, because it integrates Syntiant's powerful NDP120 Neural Decision processor to run multiple AI algorithms, leveraging bio-inspired, advanced machine learning to automate complex tasks.Nicla Voice comes with a comprehensive package of sensors: in addition to its microphone, it features a smart 6-axis motion sensor and a magnetometer, making it the ideal solution for predictive maintenance, gesture/voice recognition and contactless applications.Nicla Voice offers onboard Bluetooth® Low Energy connectivity to easily interact with existing devices, and is compatible with Nicla, Portenta and MKR products.Finally, its ultra-low power consumption makes 24/7 always-on sensor data processing possible, with the option of battery-powered standalone operation.Small enough to fit into wearables or retrofit existing machinery, enabling AI yet requiring minimal energy: Nicla Voice is the ""impossible"" combination that makes voice recognition on the edge possible – and easier than ever.Powerful processor with integrated Deep Neural Networks in a tiny form factor (22.86 x 22.86 mm)Integrated microphone, magnetometer and smart 6-axis IMUOnboard Bluetooth® Low Energy connectivityAdd speech recognition capabilities to your projectsUltra-low power for 24/7 always-on sensor data processingStandalone when battery poweredCompatible with Portenta and MKR products MicroprocessorSyntiant® NDP120 Neural Decision Processor™ (NDP):1x Syntiant Core 2™ ultra-low-power deep neural network inference engine1x HiFi 3 Audio DSP1x Arm® Cortex® M0 core up to 48 MHzMicrocontrollerNordic Semiconductor nRF52832:64 MHz Arm® Cortex M4 SensorsHigh performance microphone (IM69D130)6-Axis IMU (BMI270)3-axis magnetometer (BMM150)I/OCastellated pins with the following features:1x I2C bus (with ESLOV connector)1x serial port1x SPI2x ADCProgrammable I/O voltage from 1.8-3.3VInterfaceExternal microphone connector (ZIF)USB interface with debug functionalityMemory512KB Flash / 64KB SRAM 16MB SPI Flash for storage48KB SRAM dedicated for NDP120Dimensionsand weight22,86 x 22,86  mm2 gOperating temperature0° C to +85° C (32° F to 185°F)PowerHigh speed USB (500mbps)Pin Header3.7V Li-po battery with Integrated battery charger and fuel gauge (BQ25120AYFPR) ConnectivityBluetooth® Low Energy (ANNA-B112) ​문서https://docs.arduino.cc/resources/schematics/ABX00061-schematics.pdfhttps://docs.arduino.cc/resources/datasheets/ABX00061-datasheet.pdf연관제품연관제품 1​제품정보: https://vctec.co.kr/product/detail.html?product_no=21764 아두이노 니클라 보이스 (NICLA VOICE) - 가치창조기술아두이노 니클라 보이스 (NICLA VOICE) 판매가(VAT별도) 143,100원 상품코드 P000BGFC 상품요약정보 아두이노 니클라 보이스 입니다. SNS 상품홍보 (최소주문수량 1개 이상 ) 수량을 선택해주세요. 상품명 상품수 가격 아두이노 니클라 보이스 (NICLA VOICE) 143,100원 총 상품금액 (수량) : 143,100원 (1개) NAVER 네이버 ID로 간편구매 네이버페이 네이버페이 구매하기 찜하기 톡톡 이전 이벤트 네이버페이 다음 재고안내 상품의 실시간 재고는 네이버 페이 버튼 옆 상담 톡톡 버튼 으로 문의...vctec.co.kr ​가치창조기술​ "
"[클로바 케어콜] 대화의 기술, 사람을 돕다 ",https://blog.naver.com/clova_ai/222720508898,20220504,"대화의 기술, 사람을 돕다클로바 케어콜​우리는 그것을 대화라고 부르지 '않기로' 했어요​전화를 통해 은행의 입출금 업무를 보던 시절부터, 최근의 전화 고객센터, 설문조사, 대출 광고까지. 우리에게 ARS라는 이름으로 더 익숙한 전화 시스템은 어떤 기억으로 남아있으신가요? ​전화기 너머로 딱딱한 기계 음성을 듣고, 어떤 질문에 대한 답변을 말하거나 숫자 버튼 누르기를 반복하다 보면, 어느 순간엔 '상담사와의 전화연결'이란 안내를 먼저 기다리곤 하셨을 거예요.​자동응답 전화 시스템은 과거의 챗봇과 유사한 한계를 갖고 있습니다. 사람이 미리 준비한 질문과 답변 시나리오로 구성되어 있어, 시나리오에 해당하는 답변만을 이해할 수 있었어요. 사람의 언어표현은 매우 다양하고 때론 직관적이며 즉흥적이기 때문에, 객관식의 질의응답을 뛰어넘는 '대화'의 단계까지는 나아갈 수 없었죠.​ ​​대화의 기술, 사람을 돕다클로바 케어콜​네이버 클로바는 '자연스러운 대화'를 위한 세계 최고 수준의 AI 기술을 보유하고 있습니다. ​사람의 자연스러운 언어 표현을 이해하는 음성인식(Speech Recognition)과 자연어처리(Natural Language Processing), 숨겨진 의도와 맥락에 맞춰 적절한 답변을 제공하는 대화 모델(Dialogue Management), 답변의 맥락에 어울리는 감정까지 표현할 수 있는 자연스러운 음성합성(Speech Synthesis) 기술 등이 그것입니다.​ ​이를 기반으로, 전화를 통한 대화 경험을 완전히 새롭게 할 클로바 케어콜(CLOVA CareCall)이 탄생했습니다.​클로바 케어콜은 사람의 자유로운 언어 표현에서 의도와 맥락을 이해하고, 적절한 안내를 자연스러운 말투의 음성으로 제공할 수 있죠. 코로나19 이슈 초기, 국내 최초로 주요 지자체의 협업을 통해 폭증하는 확진자들에 대한 전화 상담 및 모니터링 업무를 수행하기도 했습니다. 또한, 홀로 거주 어르신께 전화를 걸어 건강 상태와 안부를 묻는 서비스를 제공하여 공공기관의 돌봄 업무를 지원하고 있죠.​​​​클로바 케어콜, 하이퍼클로바를 만나다​2021년, 네이버 클로바는 독자 개발한 초대규모 AI 기술 하이퍼클로바를 공개했습니다. 물론, 국내 최초였죠. ​하이퍼클로바는 세계에서 한국어 데이터를 가장 많이 보유한 언어 모델입니다. 언어를 정확하게 인식하고 응답하는 것은 물론, 분류/요약/문체 변환뿐만 아니라 실생활이나 비즈니스에서 사용할 수 있는 수준의 문장을 창작하는 단계에 도달해 있습니다. 이 기술이 클로바 케어콜에 적용되면 어떤 일이 일어날까요? ​ ​클로바 케어콜에 하이퍼클로바가 적용됨에 따라, 사람이 만든 정형화된 시나리오에서 이루어졌던 의사소통이, 일상적인 주제의 대화로 확장되었습니다. ​'오늘은 어떤 일이 있었는지, 기분은 어떠한지, 좋아하는 가수는 누구인지, 내일의 약속은 무엇인지' 등 시시콜콜한 일상 주제의 대화까지 가능해졌죠.​사람이 오랜 시간 직접 만들어야 했던 대화 시나리오를, 하이퍼클로바가 순식간에 초대규모로 생성해 내면서, 클로바 케어콜은 사용자의 언어를 더 정확하게 인식하고, 답변할 수 있게 되었습니다. 사용자의 언어 표현에서 맥락과 감정을 더욱 민감하게 감지해서, 더욱 적합한 답변을 제공할 수 있는 것이죠. 때론 대화처럼 말을 아끼고, 감정에 공감하는 표현만 제공하기도 합니다.​​​​서울, 인천, 광주, 대구, 부산그리고 세계 무대까지​국내 최초의 AI 전화 돌봄 서비스인 클로바 케어콜은, 부산 해운대를 시작으로 서울, 인천, 고양, 광주, 대구, 영덕군과 업무 협약을 맺으며 전국 지자체와의 협력에 박차를 가하고 있습니다. 일상적인 대화를 통해 돌봄의 가치를 제공함으로써, '대화하는 AI 기술'의 능력을 한껏 발휘하고 있죠.​​ 노인 돌봄 ‘클로바 케어콜’, 세계에 소개한다네이버가 노인의학 분야 최고 권위의 국제학회인 ‘IAGG 2022’에서 ‘클로바 케어콜’ 사례를 발표한다. 클로바 케어콜은 돌봄이 필요한 독거 어르신들에게 AI가 전화를 걸어, 식사, 수면, 건강 등을 주제로 어르신의 몸 상태를 확인하고, 어르신의 답변에 따라 자연스럽게 대화를 이어가는 네이버의 AI 서비스다. 음성인식 및 대화 데이터 생성에 네이버의www.hankookilbo.com ​최근엔 노인 의학 분야의 세계 최고 권위 학회 IAGG 2022 발표에 클로바 케어콜이 채택되었습니다.​IAGG(International Association of Gerontology and Geriatrics)는 세계 노인들의 건강, 복지, 권익 향상을 위한 다양한 연구 결과물을 전 세계에 공유하는 학술대회입니다. 이번 채택은 국내외 돌봄 전화 서비스들에게 있어 시사하는 바가 적지 않습니다.​ 전화하는 것건강 상태를 확인하는 것안부를 묻고 공감하는 것말벗이 되는 것​클로바 케어콜의 기술적 발전, 사회적 역할은 어디까지일까요? ​저희는 계속해서 '대화의 기술이 나아가야 할 방향'에 대해 고민하도록 하겠습니다 🍀​  ​  네이버 웹툰 <유미의 작가 수칙>에서 '클로바 케어콜'을 만나보세요!​ 유미의 작가 수칙 - 6화 : 대화에도 기술이 필요해!6화 : 대화에도 기술이 필요해!comic.naver.com ​ "
"[컴퓨터과학과] Please explain ANN, DNN, CNN, RNN of deep learning in detail with table format. (ChatGPT답변) ",https://blog.naver.com/hyongsu44/223086771133,20230427,"​You are a professor and consultant from now on. Please explain ANN, DNN, CNN, RNN of deep learning in detail with table format.​​Sure, here is a detailed explanation of the main types of deep learning neural networks: Neural Network TypeDescriptionUse CasesArtificial Neural Network (ANN)A basic type of neural network composed of multiple layers of nodes that process input data and produce output. Each node performs a simple mathematical operation on the input it receives, and the outputs of each node are combined to produce the final output.Used for a wide range of tasks, including image and speech recognition, natural language processing, and financial forecasting.Deep Neural Network (DNN)A type of neural network that consists of multiple layers of nodes, each of which performs a complex mathematical operation on its input data. The output of each layer is then passed to the next layer, and so on, until the final output is produced.Used for complex tasks, such as image and speech recognition, natural language processing, and game playing.Convolutional Neural Network (CNN)A type of neural network that is designed to process and analyze image data. It uses a technique called convolution, which involves applying a set of filters to the input image to extract features, and then using those features to make predictions.Used for tasks such as image classification, object detection, and facial recognition.Recurrent Neural Network (RNN)A type of neural network that is designed to process sequential data, such as natural language text or time series data. It uses a feedback loop that allows it to incorporate information from previous inputs into its current prediction.Used for tasks such as language translation, speech recognition, and sentiment analysis. I hope this helps! Let me know if you have any other questions.​​Thanks. Any further inputs?​​Sure, here are some additional inputs on each of the neural network types:Artificial Neural Network (ANN):ANNs are inspired by the structure and function of the human brain.The input layer receives the raw data, and each subsequent layer performs a series of mathematical operations on the input data to extract higher-level features.The output layer produces the final prediction or classification.Deep Neural Network (DNN):DNNs are simply neural networks with many layers.The increased number of layers allows for more complex models that can learn more intricate patterns in the data.However, training deep networks can be challenging due to the vanishing gradient problem.Convolutional Neural Network (CNN):CNNs use convolutional layers to extract features from images.These features can then be used to make predictions or classifications.CNNs typically also include pooling layers to reduce the size of the feature maps and prevent overfitting.Recurrent Neural Network (RNN):RNNs are designed to process sequences of data, such as text or time series.They use a recurrent layer that allows information to be passed from one time step to the next.RNNs are particularly useful for tasks that require understanding of the context or history of the input data.Overall, deep learning neural networks have revolutionized many fields, from computer vision to natural language processing, and continue to show promise in advancing many areas of research and industry.​​Thanks. Any further explanation on the above?​​Sure, here are some additional details on each neural network type:Artificial Neural Network (ANN):ANNs can be trained using a variety of techniques, including supervised, unsupervised, and reinforcement learning.Common activation functions used in ANNs include sigmoid, ReLU, and tanh.ANNs can have varying numbers of hidden layers, and the number of nodes in each layer can be adjusted depending on the complexity of the problem being solved.Deep Neural Network (DNN):DNNs can be used for both supervised and unsupervised learning tasks, including classification, regression, clustering, and generative modeling.Common optimization techniques used for training DNNs include stochastic gradient descent (SGD), Adam, and RMSprop.DNNs can suffer from overfitting, which occurs when the model learns to fit the training data too well and performs poorly on new, unseen data.Convolutional Neural Network (CNN):CNNs have been particularly successful in image recognition tasks, achieving state-of-the-art results on benchmark datasets such as ImageNet.Common activation functions used in CNNs include ReLU and tanh.CNNs can also include dropout layers, which randomly drop out a portion of the neurons during training to prevent overfitting.Recurrent Neural Network (RNN):RNNs can be used for a variety of tasks, including sequence classification, sequence prediction, and language modeling.Common activation functions used in RNNs include sigmoid and hyperbolic tangent.RNNs can suffer from the vanishing gradient problem, which occurs when the gradients used to update the weights during training become too small to be effective.Overall, each type of neural network has its strengths and weaknesses, and choosing the right architecture depends on the specific problem being solved. Understanding the key features and trade-offs of each type of neural network is essential for designing effective models.​​ "
MLOps Specialization [#4] Steps of an ML Project / Case study ,https://blog.naver.com/winterholicq/223079763588,20230420,"프로젝트 뭐할지 먼저 정하고, 데이터를 먼저 정한다 베이스라인, 라벨링) → 모델 (error analysis)→ Deploy in production → monitor the system. ​​Case study  음성인식에 대해서 한다고 가정했을때, KEY metric에 집중해야 한다. computing power 까지 고려해 본다.​ 오디오 클립을 하나 틀었을때, um today’s weather를 크게 세가지로 라벨링 할 수 있겠는데, 세번째 것은 노이즈가 포함되었다고 볼 수 있다.또한, silence가 얼마나 필요한 걸까? : 클립마다 필요할 수 있다. 오디오 클립의 볼륨이 각기 다 다를텐데, 어떻게 노말라이제이션 할 것인가?이러한 질문들 모두는 Data definition question이라고 한다.​ 그 다음으로 해야 할 것은 모델 셀렉션인데, 학계에서는 주로, 데이터는 픽스 시켜놓고 code 와 하이퍼 파라미터만 건들여서 성능을 좋게 만드는 경향이 있다. ​ 그에 반해서, Product team의 경우 코드는 픽스 시켜놓고, 데이터셋트와 하이퍼파라미터를 다르게 더 하는게 효과가 있다는 것을 확인했다.따라서 ML system = Code+Data 이다.Error analysis는 모델이 어디서 부족한지 알려주기도 하지만, 데이터가 어디서 부족한지도 알려주기도 한다. High accuracy 모델을 얻기 위해서 해야할 일들이다.​ 마지막으로, Deployment model 을 예로 들면, speech recognition에서,VAD 모듈을 사용해서, (Voice activity detection) 내가 보내고자하는 오디오 클립만 분리한다. 그 이후 Prediction server에 데이터를 보낸다.앤드류응의 예시로 보았을때, 어른의 목소리로 트레이닝 했다가 굉장히 어린 사람들이 쓰기 시작하면서 performance 가 안좋아 지기 시작했었다.Concept drift, Data drift. - data distribution 이 바뀌면 발생되는 것들이다. "
41] King Charles's first speech ,https://blog.naver.com/donglm/222871757678,20220911,"41]King Charles's first speech왕 찰스의 첫 연설查尔斯国王的第一次演讲 Watch: King Charles's first speech in full.King Charles III gives his first speech as monarch after the death of his mother, Queen Elizabeth II. Hear his speech in full. Published; 1 day agoSection; BBC NewsSubsection; UK보기: 찰스 왕의 첫 연설 전문.킹 찰스 3세 그의 첫 연설인  군주로서 사후에의 그의 어머니 여왕 엘리자베스 2세들어보십시오.그의 연설을 완전히게시됨; 1 일 전부분; BBC 뉴스일부; 영국 观看：查尔斯国王的第一次完整演讲。查理三世国王在他的母亲伊丽莎白二世去世后发表了他作为君主的第一次演讲。完整聆听他的演讲。发表； 1天前部分; 英国广播公司的新闻小节； 英国    0:01i speak to you today with feelings of profound sorrow throughout her life her majesty the queen my beloved mother was an inspiration an example to me and to all my family and we owe her the most heartfelt debt any family could owe to their mother for her love affection guidance understanding and example queen elizabeth was a life well lived a promise with destiny kept and she is mourned most deeply in her passing that promise of lifelong service i renew to you all today alongside the personal grief that all my family are feeling we also share with so many of you in the united kingdom 1:00in all the countries where the queen was head of state in the commonwealth and across the world a deep sense of gratitude for the more than 70 years in which my mother as queen served the people of so many nations in 1947 on her 21st birthday she pledged in a broadcast from cape town to the commonwealth to devote her life whether it be short or long to the service of her peoples that was more than a promise it was a profound personal commitment which defined her whole life she made sacrifices for duty her dedication and devotion as sovereign never wavered through times of change and progress 2:02through times of joy and celebration and through times of sadness and loss in her life of service we saw that abiding love of tradition together with that fearless embrace of progress which makes us great as nations the affection admiration and respect she inspired became the hallmark of her reign and as every member of my family can testify she combined these qualities with warmth humor and an unerring ability always to see the best in people i pay tribute to my mother's memory and i honor her life of service 3:01i know that her death brings great sadness to so many of you and i share that sense of loss beyond measure with you all when the queen came to the throne britain and the world were still coping with the privations and aftermath of the second world war and still living by the conventions of earlier times in the course of the last 70 years we have seen our society become one of many cultures and many faiths the institutions of the state have changed in turn but through all changes and challenges our nation and the wider family of realms of whose talents traditions and achievements i am so inexpressibly proud 4:00have prospered and flourished our values have remained and must remain constant the role and the duties of monarchy also remain as does the sovereign's particular relationship and responsibility towards the church of england the church in which my own faith is so deeply rooted in that faith and the values it inspires i have been brought up to cherish a sense of duty to others and to hold in the greatest respect the precious traditions freedoms and responsibilities of our unique history and our system of parliamentary government as the queen herself did with such unswerving devotion i too now solemnly pledge myself 5:01throughout the remaining time god grants me to uphold the constitutional principles at the heart of our nation and wherever you may live in the united kingdom or in the realms and territories across the world and whatever may be your background or beliefs i shall endeavor to serve you with loyalty respect and love as i have throughout my life my life will of course change as i take up my new responsibilities it will no longer be possible for me to give so much of my time and energies to the charities and issues for which i care so deeply but i know this important work will go on in the trusted hands of others 6:00this is also a time of change for my family i count on the loving help of my darling wife camilla in recognition of her own loyal public service since our marriage 17 years ago she becomes my queen consult i know she will bring to the demands of her new role the steadfast devotion to duty on which i have come to rely so much as my heir william now assumes the scottish titles which have meant so much to me he succeeds me as duke of cornwall and takes on the responsibilities for the duchy of cornwall which i have undertaken for more than five decades today i am proud to create him prince of wales to whistle camry 7:01the country whose title i've been so greatly privileged to bear during so much of my life and duty with catherine beside him our new prince and princess of wales will i know continue to inspire and lead our national conversations helping to bring the marginal to the center ground where vital help can be given i want also to express my love for harry and megan as they continue to build their lives overseas in a little over a week's time we will come together as a nation as a commonwealth and indeed a global community to lay my beloved mother to rest in our sorrow let us remember 8:01and draw strength from the light of her example on behalf of all my family i can only offer the most sincere and heartfelt thanks for your condolences and support they mean more to me than i can ever possibly express and to my darling mama as you begin your last great journey to join my dear late papa i want simply to say this thank you thank you for your love and devotion to our family and to the family of nations you have served so diligently all these years may flights of angels sing thee to thy rest you----------------------------- ▸▸一>See you! <봐요>! 再见! ----------------------------- ----------------------------- "
90] Top 10 Most Anticipated Technology.  ,https://blog.naver.com/donglm/223062475087,20230402,"90] Top 10 Most Anticipated Technology. 최고 10가지 가장 기대되는 기술. 最受期待的 10 大技术。-Top 10 Most Anticipated Technology Trends of 2023OUR TOP 10최고 10가지 가장 기대되는 기술 트렌드의 2023년 우리의 탑 10 2023 年最受期待的 10 大技术趋势我们的前 10    0:00did you know that by the end of 2023 the global technology Market is estimated to reach a whopping 5.2 trillion dollars and with this growth comes a surge of exciting new advancements in this video we're counting down the top 10 most anticipated technology trends of 2023 from AI to VR we'll show you what's next in the World of tech so click play and join us on this thrilling journey into the future 2023년 말까지 세계 기술시장이 무려 5조 2천억 달러에 달할 것으로 추정된다는 사실을 알고 계셨습니까? 이러한 성장과 함께 이 비디오에서는 AI에서 VR에 이르기까지 2023년에 가장 기대되는 10가지 기술 트렌드를 카운트다운합니다. 세계의 기술을 클릭 재생하고 다음 단계를 보여드리며 함께 우리와 이 짜릿한 여행 미래이겠습니다.  10]in at number 10 robotic process automation RPA is a rapidly growing technology that's transforming The Way businesses operate by automating repetitive tasks such as data processing transaction handling email replies and more RPA software is designed to imitate human actions making it a valuable tool in streamlining business processes while some industry experts predict that RPA could impact the jobs of 230 million knowledge workers it's also creating new career opportunities in fact RPA offers a range of job opportunities for it professionals including RPA developer RPA analyst RPA architect and more these roles pay well and as technology continues to evolve mastering RPA is a key step towards securing a successful and lucrative career in the tech industry and 10위 로봇 프로세스 자동화 RPA는 빠르게 성장하는 기술입니다. 데이터 처리 트랜잭션 처리 이메일 회신 및 더 많은 RPA 소프트웨어는 인간의 행동을 모방하도록 설계되었습니다. 일부 업계 전문가는 RPA가 2억 3천만 지식 근로자의 일자리에 영향을 미칠 수 있다고 예측하는 반면 비즈니스 프로세스를 간소화하는 데 유용한 도구입니다. 또한 RPA는 RPA 개발자 RPA 분석가 RPA 아키텍트 등 IT 전문가에게 다양한 직업 기회를 제공합니다. 기술이 계속 발전함에 따라 RPA를 마스터하는 것은 성공적이고 유리한 경력 기술 산업이며 09]number nine new Energy Solutions are an increasingly important aspect of Our Lives as we strive for a more sustainable and environmentally friendly future with a growing focus on renewable energy sources like solar wind and hydropower people are making a conscious effort to minimize their carbon footprint and waste this shift towards Greener Energy Solutions is not only good for the planet but also creates a wealth of job opportunities in fields like science engineering and social science from energy Specialists and solar plant designers to climate strategy Specialists and renewable energy technologists the new energy sector offers a range of exciting and meaningful careers whether you're passionate about sustainability interested in cutting-edge technology or just eager to make a positive impact there's a role in new energy that's right for you 9번의 새로운 에너지 솔루션은 우리가 보다 지속 가능하고 태양풍과 같은 재생 가능 에너지원에 대한 관심이 높아지고 있는 환경 친화적인 미래 수력 발전소 사람들은 탄소 발자국을 최소화하기 위해 의식적으로 노력하고 있으며 친환경 에너지 솔루션을 향한 이러한 전환을 낭비하는 것은 지구에 좋을 뿐만 아니라 또한 과학 공학 및 에너지 전문가 및 태양열 발전소 설계자에서 기후 전략 전문가 및 신재생 에너지 기술자 새로운 에너지 부문은 최첨단 기술에 관심이 있는 지속 가능성에 대한 열정이 있든 긍정적인 영향을 미치고 싶은 열망이 있든 관계없이 다양하고 흥미롭고 의미 있는 경력을 제공합니다. 새로운 에너지에는 맞는 역할이 있는 당신입니다 08]at number eight genomics is a rapidly advancing field that holds enormous potential for improving human health and well-being by studying the makeup of DNA and genes genomics researchers are able to gain insights into the underlying causes of disease and develop new and more effective treatments with its cutting-edge technology genomics is also creating a range of new career opportunities whether you're interested in technical roles like bioinformatics analyst or software engineer or non-technical roles like genetics engineer or genome research analysis there's a role in genomics that's right for you with its focus on diagnostics analysis and research genomics offers a chance to make a real difference in the world and to help Advance the frontiers of science so if you're passionate about biology technology and making a positive impact genomics is the field for you8위 유전체학은 DNA와 유전자의 구성을 연구함으로써 인간의 건강과 복지를 개선할 수 있는 엄청난 잠재력을 지닌 빠르게 발전하는 분야입니다. 유전체학 연구자들은 질병의 근본적인 원인과 새롭게 개발하고 최첨단 기술 유전체학을 통한 보다 효과적인 치료는 또한 다음과 같은 다양한 새로운 경력 기회를 창출합니다. 생물 정보학 분석가 또는 소프트웨어 엔지니어와 같은 기술적인 역할 또는 유전학 엔지니어 또는 게놈 연구 분석과 같은 비기술적인 역할에 관심이 있습니다. 유전체학에는 역할이 있습니다 진단 분석 및 연구 유전체학은 세상에 진정한 변화를 가져올 수 있는 기회를 제공하고 생물학 기술에 대한 열정과 긍정적인 영향을 미치는 유전체학에 분야는 위한 당신입니다. 07]at number seven 3D printing is a Cutting Edge technology that's revolutionizing the way we design prototype and manufacture products from biomedical devices to Industrial components 3D printing is having a major impact in a wide range of sectors and with its growing popularity there are now many high-paying and in-demand careers available for those with a background in AI machine learning modeling and 3D printing whether you're interested in becoming a CX program manager 3D printer engineer AI engineer or robotics trainer the field of 3D printing has something to offer so if you're passionate about Innovation and technology and want to be a part of shaping the future 3D printing is the field for you 7위 3D 프린팅은 최첨단 기술입니다. 그것은 우리가 프로토타입을 설계하고 생의학 장치에서 산업용 구성 요소에 이르기까지 제품을 제조하는 방식을 혁신하고 있습니다. 3D 프린팅은 다양한 분야와 인기가 높아짐에 따라 이제 많은 고임금 및 AI 기계 학습 모델링 및 3D 프린팅 CX 프로그램 관리자가 되고 싶은지 여부 3D 프린터 엔지니어 AI 엔지니어 또는 로봇 트레이너 3D 프린팅 분야는 제공할 무언가가 있습니다. 따라서 혁신과 기술에 대한 열정이 있고 미래를 형성하는 일에 동참하고 싶은 3D 프린팅에 분야는 위한 당신입니다. 06]at number six digital trust has become an essential aspect of our increasingly digital world where people rely on technology for various aspects of their lives the need for secure and reliable digital Technologies has led to the growth of cyber security and ethical hacking as specializations these fields offer a range of job opportunities from Junior to senior positions with salaries ranging from high to very high to get ahead in the field of ethical hacking professional certifications may be required while cyber security positions can be pursued with a diploma or even a master's degree the top jobs in these specializations include cyber security analysts penetration tester security engineer security architect security automation engineer and network security analyst if you do want to make sure that you don't miss out on any more exciting content like this hit the like button and don't forget to subscribe to the channel before we proceed with the rest of this thrilling Journey Through the top 10 most anticipated technology trends of 2023 and continuing with 6위에서 디지털 신뢰는 사람들이 삶의 다양한 측면에서 기술에 의존하는 점점 더 디지털화되는 세상의 필수 요소가 되었습니다. 신뢰할 수 있는 디지털 기술은 사이버 보안및 전문 분야로서의 윤리적 해킹 이 분야는 윤리적 해킹 분야에서 앞서 나가기 위해 주니어부터 고위직까지 다양한 직업 기회를 제공합니다. 전문 인증 사이버 보안 직위는 졸업장 또는 석사 학위로 추구할 수 있지만 이러한 전문 분야의 최고 직업에는 사이버 보안 분석가, 침투 테스터 보안 엔지니어 보안 설계자 보안 자동화 엔지니어 및 네트워크 보안 분석가가 포함됩니다. 이와 같은 흥미로운 콘텐츠를 더 이상 놓치지 않으려면 좋아요 버튼을 누르고 가장 기대되는 이 스릴 넘치는 여정을 진행하기 전에 채널을 구독하는 것을 잊지 마시고 통해 10대 기술 트렌드를 2023년에 계속해서 05]number five extended reality XR, XR is a rapidly growing technology Trend that encompasses various forms of simulated reality including virtual reality VR augmented reality AR and mixed reality Mr it involves creating a virtual world that blends seamlessly with the real world allowing people to experience new and exciting environments without the constraints of physical boundaries this technology is not only popular among Gamers but also has significant applications in fields such as medicine retail and modeling and다섯 번째 확장 현실 XR, XR은 가상현실, VR, 증강현실, AR, 혼합 현실 미스터 그것은 가상 세계를 만드는 것과 관련이 있습니다. 물리적 경계의 제약 없이 사람들이 새롭고 흥미로운 환경을 경험할 수 있도록 현실 세계와 완벽하게 조화를 이루는 이 기술은 게이머들 사이에서 인기가 있을 뿐만 아니라 또한 의약품 소매 및 모델링이며 04]at number four artificial intelligence AI is a rapidly growing field that continues to impact Our Lives work and play in numerous ways it's become known for its superiority in various applications such as image and speech recognition navigation apps personal assistants and ride sharing Services AI is set to revolutionize several Industries as it's being used to analyze interactions predict demand detect changing customer behavior and make better resource utilization decisions the AI Market is expected to grow to a 190 billion dollar industry by 2025 leading to the creation of new jobs in development programming testing support and maintenance these jobs also offer some of the highest salaries in the market today ranging from 125 000 per year for machine learning Engineers to 145 000 a year for AI architects machine learning a subset of AI is also being deployed across Industries creating a high demand for skilled professionals the growth of AI and machine learning is expected to create nine percent of new jobs in the US by 2025 including robot monitoring professionals data scientists automation Specialists and content curators if you're interested in the field mastering Ai and machine learning could lead to a career as an AI research scientist an AI engineer machine learning engineer or AI architect and4위 인공 지능 AI는 빠르게 성장하는 분야로 Our Lives 작업에 지속적으로 영향을 미치며 다양한 방식으로 재생하여 이미지 및 음성 인식 내비게이션 앱 개인 비서 및 승차 공유 서비스 AI는 다음과 같이 여러 산업에 혁명을 일으킬 것입니다. 상호 작용을 분석하고 수요를 예측하고 변화하는 고객 행동을 감지하고 더 나은 자원 활용 결정을 내리는 데 사용되고 있습니다. AI 시장은 2025년까지 1,900억 달러 규모의 산업으로 성장하여 개발 프로그래밍 테스트 지원 및 유지 관리 이러한 작업은 또한 기계 학습 엔지니어의 경우 연간 125,000에서 AI 설계자의 경우 연간 145,000에 이르기까지 오늘날 시장에서 가장 높은 급여를 제공합니다. AI는 또한 숙련된 전문가에 대한 높은 수요를 창출하는 산업 전반에 배포되고 있습니다. 기계 학습은 2025년까지 미국에서 로봇 모니터링 전문가 데이터 과학자 자동화 전문가 및 현장 마스터링 Ai에 관심이 있는 콘텐츠 큐레이터와 기계 학습은 AI 연구 과학자로서의 경력으로 이어질 수 있습니다. AI 엔지니어 기계 학습 엔지니어 또는 AI 설계자이며 03]at number three datification has become an integral part of our daily lives the need for secure and safe storage of this data has become a top priority the field of datification offers ample opportunities for individuals with a background in technology even those without advanced degrees to find high paying jobs from Big Data engineers and robotics engineers to IT architects and business intelligence analysis there's a wide range of careers to choose from additionally by acquiring certifications in data related specializations individuals can enhance their skill set and increase their chances of success in the datification field and세 번째로 데이트는 우리 일상 생활의 필수적인 부분이 되었습니다. 이 데이터의 안전한 저장이 최우선 순위가 되었습니다. 데이터화 분야는 기술에 대한 배경 지식이 있는 개인에게도 고급 학위가 없는 사람에게도 빅 데이터 엔지니어 및 로봇 공학 엔지니어에서 IT 설계자 및 비즈니스 인텔리전스 분석 데이터 관련 전문 분야에서 인증을 취득하여 추가적으로 선택할 수 있는 다양한 직업이 있습니다. 에서 성공할 가능성을 높이는 데이터 필드이며  02]at number two smarter devices are revolutionizing the way we live work and play with the help of artificial intelligence AI devices have become more intelligent efficient and and user-friendly from home robots to Smart appliances wearables and work devices AI is making our lives easier and more convenient the demand for smarter devices is growing and as a result the demand for professionals with skills in it and automation is also increasing companies are transforming into digital spaces and requiring employees who have a good Proficiency in these fields it and automation have become crucial components in almost every higher level job jobs such as it manager data scientists product tester product manager automation engineer and it researcher are in high demand in this industry by mastering the skills through a course like RPA you can achieve expertise in your career and thrive in your chosen field whether it's in it marketing or management두 번째로 더 스마트한 장치는 우리가 일하는 방식을 혁신하고 있습니다. 인공지능의 도움으로 놀이하기 AI 장치는 가정용 로봇에서 스마트 가전 웨어러블 및 업무 장치에 이르기까지 보다 지능적이고 효율적이고 사용자 친화적이 되었습니다. 보다 편리하고 스마트한 기기에 대한 수요가 증가하고 있으며 그 결과 IT 및 자동화에 대한 기술을 갖춘 전문가에 대한 수요도 증가하고 있습니다. 자동화는 IT 관리자 데이터 과학자 제품 테스터 제품 관리자 자동화 엔지니어 및 IT 연구원과 같은 거의 모든 상위 직종에서 중요한 구성 요소가 되었습니다. 이 산업은 RPA와 같은 과정을 통해 기술을 습득함으로써 경력 및 전문 지식을 얻을 수 있습니다. 선택한 분야에서 번창하십시오. 그것은 마케팅 또는 관리에 있습니다 01]and at number one computing power is a vital aspect of the digital world with almost every device and Appliance becoming computerized the Computing infrastructure is expected to continue evolving with advancements like 5G and the predicted emergence of 6G making Computing even more powerful this increase in computing power is also creating numerous jobs in the tech industry but specialized qualifications are often required data science Robotics and it management are just a few of the fields that are expected to see a surge in employment as the demand for computing power increases to take advantage of this trend it's recommended to consider learning RPA or robotic process automation this is a branch of computing and automation software that can train you for a high playing role in the IT industry with potential jobs including data scientists AI engineer robotics researcher AI architect and Robotics designer so there you have it thanks so much for watching we do hope you found this information useful if you did don't forget to give us a like and hit that subscribe button for more informative content like this so we'll see you in our next video1위 컴퓨팅 파워는 거의 모든 장치와 디지털 세계의 중요한 측면이며 기기가 컴퓨터화되고 있습니다. 컴퓨팅 인프라는 5G와 같은 발전과 컴퓨팅을 더욱 강력하게 만드는 6G의 출현으로 계속 발전할 것으로 예상됩니다. 이러한 컴퓨팅 성능의 증가는 기술 산업에서 수많은 일자리를 창출하지만 전문 자격은 종종 데이터 과학 로보틱스가 필요하며 관리는 컴퓨팅 성능에 대한 수요가 증가함에 따라 고용이 급증할 것으로 예상되는 분야 중 일부에 불과합니다. 이용하다 이 트렌드는 RPA 또는 로봇 프로세스 자동화 학습을 고려하는 것이 좋습니다. 이것은 데이터 과학자를 포함하여 잠재적인 직업으로 IT 산업에서 높은 역할을 수행하도록 훈련할 수 있는 컴퓨팅 및 자동화 소프트웨어의 한 분야입니다. AI 엔지니어 로봇 연구원 AI 아키텍트 그리고 로보틱스 디자이너입니다. 시청해 주셔서 감사합니다. 찾았으면 합니다. 좋아요를 잊지 않으셨다면 이 정보가 유용합니다. 이와 같은 더 유익한 콘텐츠를 보려면 구독 버튼을 누르십시오. 그럼 만나요 다음 비디오. ​[Music] foreign [음악] 외국의----------------------------- ▸anticipate [ænˈtɪsəˌpeɪt앤티스페잇]an·tici·pate 1.예상豫想하다 2.예측하다 3.기대期待[고대]하다 과거형     anticipated [3지칭 단수현재(e)s] anticipates 과거 분사 anticipated 현재 분사 anticipating 형용사형anticipated [ænˈtɪsəpèitid] 기대하던, 대망의unanticipated 못 예상한, 기대[예상]하지 않은명사형anticipation[ænˌtɪsɪˈpeɪʃn안티시페이션] 1.예상(豫想), 예측(豫測) 2.기대(期待), 고대(苦待)<prefix & suffix접두미사接头尾辞>anticipateanticipate a good vacation멋진 휴가를 예상하다anticipate a political reaction정치적 반동이 일어날 것으로 예상하다anticipate another's wishes남의 뜻을 넘겨잡다anticipate one's salary봉급/급료을 예상하고 돈을 미리쓰다anticipate trouble곤란한 일이 생길 것을 근심하다, 곤란을 예상하다anticipated기대하던, 대망의anticipate a person's desires남의 욕구를 알아차리고 들어주다confidentially anticipate내밀히 기대하다▸ ​RPA (Robotic Process Automation): 로봇 프로세스 자동화(机器人过程自动化)는 이전에는 사람이 하던 반복적인 태스크를 소프트웨어 로봇이 대신하는 것이다. ▸XR (Extended Reality) 확장현실(扩展现实) 확장현실은 다양한 몰입형 및 인터랙티브 기술 영역을 아우르는 포괄적인 용어로, 증강현실(AR), 혼합현실(MR), 가상현실(VR)을 망라한다. 모바일 장치, VR 헤드셋과 안경 및 기타 기술을 통해 XR에 접근할 수 있다.​VR (Virtual Reality) 가상현실(虚拟现实)VR로 디지털 세계에 몰입하여 디지털 개체의 진정한 깊이와 시각적 풍부함을 경험할 수 있다. 기존의 인간-컴퓨터 인터페이스에서는 2D 뷰를 통해 3D 세계와 간접적으로 상호 작용한다. 그러나 VR에서는 그 사물이 있는 세계 속으로 들어가 마치 현실처럼 그 세계를 경험할 수 있다.​AR (Augmented Reality) 증강현실(增强现实)AR로는 실제 개체와 상호 작용하면서도 공간적으로 일관된 방식으로 가상 개체를 현실 세계로 가져올 수 있다. 물체의 세계로 들어가는 가상현실과 달리, 증강현실은 물체를 나 자신과 내가 속한 세계로 가져온다.​MR (Mixed Reality) 혼합현실(混合现实)MR은 가상 세계와 물리적 세계 사이의 연속체를 말한다. 스펙트럼의 한쪽 끝 에는    VR(가상현실) 경험이 있고, 다른 쪽 끝에는 AR(증강현실) 경험이 있다. 패스스루(pass-through) 비디오와 같이 실제 세계의 요소를 VR 경험에 추가하면, 해당 경험을 실제 현실로 확장할 것이다. 이와 반대로 AR 경험에서 현실 세계를 대체하거나 위장하는 가상 객체를 추가하면, 그 경험은 디지털 현실로 확장될 것인데마치 진짜 존재하는 것처럼 실제 세계에 배치할 수 있는 홀로그램같은 것이다.▸一>See you again! <봐요 다시>! 再见! ---------------------------------------------------------- "
Chat GPT에게 '인공지능시대에서 초등저학년을 위한 영어능력 평가방법은?'라고 물어보았다.  ,https://blog.naver.com/aqbox/223038252131,20230308,"인공지능 시대에서 초등저학년을 위한 영어 능력 평가 방법은 여러 가지가 있을 수 있습니다. 그 중 몇 가지 예시를 들어보면 다음과 같습니다.​1.음성 인식 기술을 활용한 말하기 평가​초등저학년 학생들은 영어 단어나 문장을 말하는 데 어려움을 겪을 수 있습니다. 이를 평가하기 위해 음성 인식 기술을 활용한 말하기 평가 시스템을 구축할 수 있습니다. 학생들이 주어진 단어나 문장을 말하면, 시스템이 이를 인식하여 정확도와 유창성 등을 평가할 수 있습니다.​2.영어 듣기 평가​초등저학년 학생들은 영어 듣기 능력을 평가하는 것이 중요합니다. 이를 위해 다양한 레벨의 듣기 시험 문제를 제공하여 학생들의 이해도를 평가할 수 있습니다. 또한, 학생들이 들은 내용을 요약하거나 질문에 답하는 형태로 문제를 출제하여 학생들의 이해도와 응용 능력을 평가할 수 있습니다.​3.온라인 학습 플랫폼을 활용한 독해 평가​초등저학년 학생들은 영어 독해 능력을 향상시키기 위해 온라인 학습 플랫폼을 활용하는 경우가 많습니다. 이러한 학습 플랫폼에서는 학생들이 주어진 문장이나 단어를 이해하고, 그에 대한 문제를 푸는 형태로 학습할 수 있습니다. 이를 통해 학생들의 독해 능력을 평가할 수 있습니다.​4.게임을 활용한 어휘 평가​초등저학년 학생들은 게임을 좋아하는 경우가 많습니다. 이를 활용하여 게임을 개발하여 학생들의 어휘 능력을 평가할 수 있습니다. 예를 들어, 이미지나 단어를 보여주고 그에 대한 정확한 뜻을 맞추는 게임을 제공하여 학생들의 어휘 이해도를 평가할 수 있습니다.​​​There can be several ways to assess the English proficiency of elementary and primary school students in the age of artificial intelligence. Here are some examples:​1'Speaking assessment using speech recognition technology​Elementary and primary school students may have difficulty speaking English words or sentences. To evaluate this, a speaking assessment system using speech recognition technology can be developed. When students speak the given words or sentences, the system can recognize them and evaluate their accuracy and fluency.​2.Listening assessment​It is important to evaluate the English listening skills of elementary and primary school students. To do this, listening test questions at various levels can be provided to assess students' comprehension. Additionally, questions can be formulated in a way that requires students to summarize or answer based on what they heard to evaluate their comprehension and application skills.​3.Reading assessment using online learning platforms​Elementary and primary school students often use online learning platforms to improve their English reading skills. On these platforms, students can read sentences or words and answer questions based on them. This can be used to assess their reading comprehension.​4.Vocabulary assessment using games​Elementary and primary school students often enjoy playing games. Games can be developed to assess their vocabulary skills. For example, a game can be created that shows an image or a word, and students have to select the correct meaning to evaluate their vocabulary comprehension.​These are just a few examples of how English proficiency assessment for elementary and primary school students can be done in the age of artificial intelligence, and various other methods can be applied.​​​ "
🥳 챗 GPT 에게 '영어 잘하는 방법'을 묻다 ,https://blog.naver.com/bulfoxx/223071926648,20230412,"​🧞‍♂️ 최대한 많이 이야기 하라!​더 많이 이야기해볼수록, 영어는 더 편안해질 것이다. 친구들, 가족들, 동료들 누구든 함께 이야기하라. 영어 회화 클럽이나 같이 이야기나눌 사람을 구하는 것도 좋다.​+ Where can I find them?​: HelloTalk, Tandem, MyLanguageExchange, Facebook groups, Reddit threads, forums, Italki, Verbling​🧞‍♂️ 원어민이 쓰는 영어를 들어라!​새로운 단어나 문장을 배울 수 있다. 또한 영어의 소리와 리듬을 익힐 수 있다.​+ Let me know some contents or platforms that I can listen to native English speakers.​: NPR, This American Life, Ted Talks on Podcast. It's free on platforms like Spotify, Apple Podcast, Google Podcasts. Youtube, News websites such as CNN, BBC, and Reuters. Radio, such as BBC Radio, NPR, Voice of America. Language learning apps, such as Duolingo, Babbel, and Rosetta Stone.​🧞‍♂️ 영어로 읽고 써라!​영어 읽기를 통해 영어 단어와 문법을 배울 수 있다. 영어로 쓰는 것은 문장구조, 철자 그리고 문법을 연습할 수 있다. ​🧞‍♂️ 매일 매일 습관처럼 영어를 써라!​음식을 주문하거나 쇼핑을 할 때 또는 다른 사람과 이야기할 때 영어를 써라. 진짜 현실에서 쓰는 문장들을 많이 연습할 수 있을 것이다. ​+ Wait, It's kind of weird thing to speak in English every day in Korea cuz it's Korea. ​: Find an English-speaking friend. ​🧞‍♂️ 발음에 집중하라!​단어가 어떻게 발음되는지 집중하고, 정확히 말할 수 있도록 연습해라. 스스로 직접 들어보고 다른 사람들에게 피드백 받아라. ​+ Whoever wants to feedback my english pronounciation? ​: Speech recognition software, Speech therapy, Language tutor, Language exchange partner. ​🧞‍♂️ 관용구와 표현들을 배워라!​더 효과적으로 소통할 수 있게 하고, 더욱 자연스럽게 들릴 수 있게 만들 것이다. ​+ Can I find any books about English idioms and expressions? ​: The cambridge international dictionary of idioms(by Cambridge university press), Essential idioms in English(by Robert J. Dixon), Idioms for everyday use(by Milada Broukal), Idiomatic american english(by Barbara K. Gaines), The american heritage dictionary of idioms(by Christine Ammer)​🧞‍♂️ 실수를 두려워하지 말라!​실수하는 건 자연스러운 배움의 과정이다. 실수하는 걸 너무 두려워하지말고, 실수하면서 배워나가라.  "
퀄컴 테크날러지는 어떻게 머신 러닝의 저력을 실현하는가 ,https://blog.naver.com/qualcommkr/222726242540,20220509,"​퀄컴 AI 리서치(Qualcomm AI Research)는 2018년에 설립된 이후 무선 인터넷부터 자동차, XR, IoT(사물 인터넷), 모바일에 이르기까지 사실상 모든 기술 분야에 영향을 미치는 업계의 핵심 자원으로 발전했습니다. 성장을 거듭하는 퀄컴의 팀은 샌디에이고(미국), 암스테르담(네덜란드), 서울(한국), 마컴(캐나다)에 있습니다. 기쁘게도 저는 이 사업을 초기부터 이끌었습니다. 연구 범위는 확장되었지만, 퀄컴의 기업 문화는 변함없이 목표 지향적 혁신, 열정적인 실행, 개방성을 나침반으로 삼아왔습니다. 이런 가치는 전사적으로 적용되고 있죠.​​목표 지향적 혁신​퀄컴의 연구원들은 선구적 기초 연구부터 시장의 현안을 해결하는 응용 연구에 이르기까지 광범위한 주제에 대해 연구하고 있습니다. 사실 거의 모든 기술은 기계 학습으로 최적화가 가능합니다. 바로 이런 이유에서 퀄컴은 유비쿼터스 AI를 구현하기 위해 노력하고 있습니다. ​ ​퀄컴 AI 리서치 팀에서는 광범위한 주제를 연구하기 때문에 풀스택(full stack) AI 최적화에 대한 시스템 차원의 접근법을 취하고 퀄컴이 수년 동안 구축한 전력 효율성에 대한 전문 지식을 잘 활용하는 것이 중요합니다. 퀄컴의 연구원들에게 어떤 사업에 가장 기대가 큰지 전망에 대해 물었습니다. 아래의 영상에서 답변을 확인해 보세요.​ ​​열정적 실행력​AI 연구는 사일로(silo) 속에서 이루어지는 것이 아니라 여러 팀에 걸쳐 수행된 작업의 결과물입니다. 그 때문에 혁신 창출을 위해 인재들의 역량과 성격을 적절히 조합하는 것이 핵심이죠. 하드웨어와 소프트웨어 전문가, 연구원, 엔지니어의 협력을 통해 AI 연구는 대규모 상용화에 이를 수 있습니다. 퀄컴 AI 리서치 팀은 최신 과학 기술과 하드웨어에 쉽게 접근할 수 있습니다. 퀄컴의 개방 정책은 직원들이 직급과 부서를 넘어서 열린 자세로 서로 소통하며 아이디어를 원활하게 교환할 수 있는 분위기를 조성합니다. 무엇보다 연구자들은 자신의 연구 분야에서 자율성을 부여받죠. 퀄컴 팀은 성공적인 AI 연구 협업에 대해 다음과 같이 말합니다. ​ ​​개방성​퀄컴은 매년 NeurIPS, ICLR, ICML, CVPR 등 정상급 과학 컨퍼런스에서 논문을 발표합니다. 퀄컴 AI 리서치 팀은 일부 혁신 논문의 경우 코드를 공개하여 다른 연구자들이 그 결과를 재현하고, 이를 토대로 삼을 수 있도록 했습니다. 지난해 퀄컴 AI 리서치는 TwentyBN 및 Reservoir Labs를 인수하여 전문성을 강화했습니다. 퀄컴은 개발자 네트워크 산하에서 TwentyBN의 컴퓨터 비전 데이터 세트 Jester와 Something-Something을 다시 선보일 수 있게 된 것을 자랑스럽게 생각합니다. 퀄컴의 머신 러닝 커뮤니티에 대한 기여는 여기에서 그치지 않습니다.​작년에 퀄컴은 AI 모델 효율성 툴킷(AIMET)의 새로운 업그레이드 버전을 출시하기 위해 노력했으며, 최근에는 AIMET 백서를 발표하여 개발자가 온디바이스 AI를 성공적으로 구현하는 데 활용할 수 있는 양자화 기술을 소개했습니다. AIMET Model Zoo는 32비트 부동 소수점(FP32) 모델을 정확도 손실이 거의 없이 8비트 정수(INT8) 모델로 양자화할 수 있는 사전 훈련된 모델과 레시피 컬렉션인데요. AIMET Model Zoo(링크 3)를 사용하면 개발자와 연구원의 작업이 훨씬 더 수월해지죠. 다음은 퀄컴 팀이 AI 커뮤니티 전체에 어떤 기여를 하고 있는지 밝힌 인터뷰입니다. ​ ​​온디바이스 AI의 장점​퀄컴 AI 리서치는 에너지 효율적인 AI를 발명하고, 개발하여 상용화하는 데 힘쓰고 있습니다. 퀄컴은 이런 기술의 접근성을 점차 높이고 있으며, 개인 정보 보호 강화, 지연시간의 단축, 비용 절감을 통해 온디바이스 AI를 구현합니다. 컴팩트한 신경망 모델과 고효율의 실리콘이 있다는 것은 기술이 인간을 대신하여 더 많은 작업을 성공적으로 수행할 수 있다는 것을 의미하며, 그 결과 인간은 더 중요한 일에 집중할 수 있습니다. 퀄컴의 AI 연구팀은 이를 가능하게 만드는 데 핵심적인 역할을 합니다. 현재 다음 분야에서 채용이 진행 중입니다. ​Principal Deep Learning Systems EngineerDeep Learning Systems Research Engineer on Camera Perception for Autonomous DrivingPerformance Analysis Engineer for Deep Learning SystemsSoftware Engineer for Machine Learning OperationsSoftware Engineer for Deep Learning EnablementComputer Vision and Machine Learning R&D Intern for Autonomous DrivingMachine Learning Researcher – Audio and SpeechDeep Learning Researcher - Speech RecognitionDeep Learning Model Efficiency EngineerDeep Learning SW EngineerDeep Learning Researcher (Intern)​  ​ "
(TOEIC #07) 토익 LC part3 단어장 ,https://blog.naver.com/shine4son/222659450127,20220228,"<토익단어장>-교재-ETS TOEIC 토익 정기시험 기출종합서 LCLC part3p.111 ​ transferV. 옮기다, 이동하다, 전근가다, 옮기다, 넘겨주다, 환승하다, 이적하다​N. 이동, 이적, 환승I heard that you're transfering to our office in Hong Kong.당신이 우리 회사 홍콩 지사로 전근갈거라고 들었어요.​a job transfer 일자리 이동electronic data transfer 전자 데이터 전송 speech recognition음성 인식The company is ready to start marketing our new speech recognition software in Asia.회사는 우리회사의 새로운 음성 인식 소프트웨어를 아시아에서 마케팅을 시작할 준비를 한다. since접속사. ~ 때문에 / ~한 이후로전치사. ~(이후)부터부사. 그 이후로, 그 후Since my team has had a lot of success selling the product here, they've asked me to move to Hong Kong to head up the sales efforts there.우리 팀이 여기서 많은 성공적인 판매를 올렸기 때문에 그들이 거기서 판매 활동을 이끌기 위해 나에게 홍콩으로 전근을 요청을 했다.​We’ve lived here since 1994. 우리는 1994년부터 여기에 살고 있다​We were divorced two years ago and she has since remarried. 우리는 2년 전에 이혼을 했고 그녀는 그 후 재혼을 했다. head up(부서 등을) 이끌다, 책임지다I'm in agreement about creating a special task force, but that's only if we can find the right person to head up the team.특별 태스크포스를 구성하는 데는 동의합니다만, 팀을 이끌만한 적임자를 찾을 수 있는 경우에만입니다. effort(s)N. 노력 / (특정한 성과를 거두기 위한 집단의 조직적인) 활동make an effort 노력을 하다​the United Nations’ peacekeeping effort 유엔의 평화 유지 활동 retailerN. 소매업자, 소매업a person or business that sells things directly to customers for their own useMy cousin works in Hong Kong for a big retailer. work for ~(회사, 고용주)~에서 일하다, ~을 위해 일하다She works for an engineering company. 그녀는 엔지니어링 회사에 다닌다. work in ~~(도시,국가, 분야. 부서)에서 일하다I’ve always worked in education. 나는 항상 교육 분야에서 일을 해 왔다. work at ~~(장소)에서 일하다I needed a job which would enable me to work at home.나는 내가 집에서 일하는게 가능한 일이 필요하다. (just) in case혹시라도 ~할 경우에 대비해서I'll give you her e-mail address in case you want to contact her.혹시라도 니가 그녀에게 연락하기 원할 경우를 위해 내가 너에게 그녀의 이메일 주소를 줄게.​You probably won’t need to call—but take my number, just in case. (당신이) 아마 전화하실 필요는 없을 거예요. 하지만 만약을 위해서 제 번호를 받아 놓으세요. ​  최근에 토익을 위한 문법 강의인 <해석을 위한 문법>이라는 온라인 수업을 듣고 있어요.고거 며칠 들어었다고 오늘 토익문제집 공부하는데좀 수월하다는 느낌적인 느낌?!아쉽게도 이번주 주말도 이렇게 지나가네요.그래도 일요일의 마지막을 영어공부로 마무리하니나름 보람을 챙깁니다 ㅎㅎ새롭게 시작되는 한 주도 화이팅하세요!​ ​ "
"[서포터즈 취재] ""시리야~"", ""오케이 구글"", ""헤이 빅스비"" ",https://blog.naver.com/sw_maestro/221931902343,20200428,"   이제는 손으로 검색을 하지 않고 ""목소리""만으로 인터넷 검색을 하고, 음악을 재생하는 등 다양한 기능을 사용할 수 있습니다. '음성인식 가상비서(Voice Assistant)'를 통해 음성 명령만으로도 간편하게 원하는 동작을 실행할 수 있는데, '음성인식 가상비서'는 어떤 것일까요?     ●     '음성인식 가상비서'가 무엇인가요?기존에 사람들은 전자기기에서 원하는 기능을 사용하기 위해 키보드, 마우스, 터치 등을 통해 기계에 명령을 내리고 화면을 통해 출력을 받았습니다. 반면 '음성인식 가상비서(Voice Assistant)'는 이러한 과정을 ""목소리(음성)""를 통해 이루어지게 하며 인간과 로봇의 소통(HRI: Human Robot Interface)이 가능하게 합니다. 음성인식 가상비서 서비스는 애플의 서비스 중 하나인 시리(Siri)를 시작으로 아마존의 알렉사(Alexa), 구글 홈의 구글 어시스턴트 등 다양한 기업에서 스마트폰, 스마트 스피커 또는 스마트 TV 등을 통해 제공되고 있습니다.     ●    음성인식 가상비서에 어떤 기술이 사용되나요?음성인식 가상비서는 음성인식(ASR) 기술과 장기간 축적된 음성 데이터를 기반으로 딥러닝(Deep Learning) 알고리즘이 접목되며 눈에 띄게 발전했습니다. 음성인식 가상비서에 사용된 핵심 기술로는 음성인식(ASR), 음성이해(NLU), 음성합성(TTS) 등이 있습니다. 음성인식 가상비서 서비스는 다양한 데이터와 기술들이 복합적, 유기적으로 결합되며 구현됩니다. -       음성인식(ASR) 기술음성인식(ASR: Automatic Speech Recognition) 기술은 사용자의 음성 명령을 컴퓨터가 이해할 수 있는 언어(Text)로 자동 변환하는 기술입니다. 인공신경망 엔진을 활용해 단어와 문장을 학습합니다. -       음성이해(NLU) 기술음성이해(NLU: Natural Language Understanding) 기술은 주어진 Text가 어떤 의미인지에 대해 파악하는 기술입니다. 예를 들어 ""오늘 서울 날씨를 알려줘""라고 말하면 특정 위치(대한민국 서울)의 ""기상 정보(날씨)""를 찾아서 전달하라는 의미로 분석하는 것입니다. 발화자의 의도를 파악하기 위해서는 다양한 유사 패턴을 학습합니다.최근에는 딥러닝 기술이 상용화되면서 패턴에 대한 학습 능력이 크게 향상되고 있다. -       음성합성(TTS) 기술음성합성(TTS: Text to Speech) 기술은 Text 문장을 음성으로 변환하는 기술입니다. 시리나 빅스비와 같은 음성인식 가상비서를 사용할 때 스피커로 출력되는 기계의 음성을 생각할 수 있게 합니다. 이때 기계가 아니라 실제 사람이 말하는 것처럼 속도나 높낮이를 조절하는 것이 중요합니다.    ●    음성인식 가상비서는 어떤 분야에 사용될 수 있나요?음성인식 서비스를 실생활 가장 가까이에서 접할 수 있는 것은 당연히 “스마트폰”일 것입니다. 애플의 시리(Siri), 삼성의 빅스비(Bixby), 구글의 구글 어시스턴트(Google Assistan)와 같이 요즘 스마트폰에는 기본적으로 음성 인식 서비스가 탑재되어 있습니다. ​  최근에는 스마트 스피커를 통한 음성인식 가상비서의 활용도가 매우 높아지고 있는데, 구글 홈, 카카오 미니C, 네이버 프렌즈, KT 기가지니, 갤럭시 홈 등 국내외의 다양한 기업이 인공지능(AI) 블루투스 스피커를 제작하고 서비스를 제공하고 있습니다.​   스마트폰과 스마트 스피커에서 사용되는 음성인식 가상비서는 단순히 목소리를 인식해 특정 기능을 수행하는 것을 넘어서 인공지능(AI)을 기반으로 작동하기도 합니다. 뉴스나 환율, 날씨에 대한 정보를 얻을 수도 있고 음악 또는 TV 프로그램을 재생하거나 TV, 냉장고와 같은 외부 기기와 연결해 제어하는 등 다양한 기능을 수행합니다. 스마트 스피커를 활용해 독거 어르신의 안전을 확인하거나 치매예방에 활용되는 등 복지 분야에도 폭넓게 활용됩니다. 향후 음성인식 가상비서는 냉장고, 청소기 등 각종 전자 제품과 아우디, BMW, 재규어와 같은 자동차, 게임, 커머스에 이르기까지 적용 범위가 확대될 것입니다.    ●    향후 기대되는 점최근에는 목소리로 명령을 실행하는 것을 넘어서 목소리로 감정을 파악하고 적절한 순간에 지능형 서비스를 제공하는 것에 이르기까지 보다 발전된 서비스를 구현하기 위한 연구가 계속되고 있습니다. 또한 음성 데이터를 분석해 치매, 울혈성 심부전, PTSD와 같은 신체 질환의 징후를 감지하고 건강 관리에 도움을 주는 등 기존의 서비스를 편리하게 이용할 수 있도록 도와줄 뿐만 아니라 사용자에게 적합한 서비스 또는 기능을 제시하는 등 발전된 역할로 활용될 수 있습니다.​스마트폰, 웨어러블 디바이스, 사물인터넷(IoT)에 대한 대중의 접근성이 높아지고 많은 인기를 얻고 있는 만큼 음성인식 가상비서의 역할 및 활용도도 높아질 것입니다. 앞으로는 인터넷 검색, 음악 재생, 날씨 확인 등의 단순한 서비스뿐만 아니라 가까운 레스토랑을 검색해 예약하고, 외부 보안 카메라 잠금을 해제하거나 자동차 시동을 거는 등 복잡하고 다양한 영역에 활용될 것입니다.​​  ​   ​ "
"Chat GPT와 MJ와의 대화(영영)+영영요약+문제5+정답해설(영영): future trends, deep learning, digital currency... ",https://blog.naver.com/mj-english/223056584406,20230327,"MJ: what are the future trends we have to know in terms of importance and usefulness?​AI: As an AI language model, I can provide you with some general trends in various fields that are expected to be important and useful in the near future:Artificial intelligence and machine learning: AI and machine learning are becoming increasingly important in a wide range of industries and applications, from healthcare and finance to transportation and logistics.Internet of Things (IoT): IoT refers to the network of connected devices, sensors, and other objects that are able to exchange data and communicate with one another. As IoT technology continues to develop, it is expected to have a major impact on fields such as healthcare, manufacturing, and agriculture.Renewable energy: With the growing concerns over climate change and the need to reduce our reliance on fossil fuels, renewable energy sources such as solar and wind power are becoming increasingly important.Blockchain: Blockchain technology is a secure and transparent way to store and transfer information, and it has potential applications in fields such as finance, healthcare, and supply chain management.Virtual and augmented reality: Virtual and augmented reality technologies are being used in industries such as gaming, education, and healthcare, and they are expected to become increasingly important as the technology continues to improve.Cybersecurity: As more and more data is stored and exchanged online, cybersecurity is becoming increasingly important. Companies and organizations are investing in cybersecurity measures to protect against hacking and data breaches.These are just a few examples of the many trends that are expected to be important and useful in the future. It is important to keep up with the latest developments in your field and be open to learning about new technologies and trends that may impact your work or industry. MJ: tell me more about Artificial intelligence and machine learning.​AI: Artificial intelligence (AI) refers to the development of computer systems that can perform tasks that normally require human intelligence, such as visual perception, speech recognition, decision-making, and natural language processing. AI systems can be designed to learn and improve over time through machine learning algorithms, which allow the computer to identify patterns and make predictions based on large data sets.Machine learning (ML) is a subfield of AI that focuses on the development of algorithms that can learn from and make predictions on data. These algorithms can be trained using large sets of data, allowing them to recognize patterns and make predictions based on new data. There are different types of machine learning, including supervised learning, unsupervised learning, and reinforcement learning.Supervised learning involves providing the algorithm with labeled data, meaning that the data has been classified with a correct answer, and the algorithm learns to identify patterns and make predictions based on this labeled data. Unsupervised learning involves providing the algorithm with unlabeled data and allowing it to identify patterns and relationships on its own. Reinforcement learning involves training an algorithm to make decisions based on feedback received from its environment.AI and machine learning are increasingly being used in a wide range of industries and applications, including healthcare, finance, transportation, and entertainment. They are also used to power virtual assistants, chatbots, and recommendation systems, among other things.However, there are also concerns about the potential negative consequences of AI and machine learning, such as privacy violations, algorithmic bias, and the displacement of jobs. As such, it is important for developers, policymakers, and society as a whole to consider the ethical implications of these technologies and work to ensure that they are used in a responsible and beneficial way. MJ: what is the most advanced form or mode of Artificial intelligence and machine learning?​AI: The most advanced form of Artificial Intelligence and Machine Learning is currently considered to be Deep Learning, which is a subset of Machine Learning that uses neural networks with many layers to learn representations of data. Deep Learning algorithms can analyze vast amounts of complex data and make predictions or decisions based on that data with high accuracy. It has been used in a variety of applications, including image and speech recognition, natural language processing, and autonomous driving. Some of the most notable examples of Deep Learning include AlphaGo, a computer program that defeated a human world champion in the game of Go, and OpenAI's GPT-3, a language model that has shown impressive capabilities in natural language processing and generation. MJ: what does renewable energy have to do with mother nature and humanity's fate? Are there any measures to save mother nature and the human race from potential eventual extinction or doom?​AI: Renewable energy has a significant role to play in saving Mother Nature and humanity's fate. Climate change, caused by the burning of fossil fuels, is one of the most pressing environmental issues facing humanity today. Renewable energy sources like solar, wind, hydro, and geothermal power can help to reduce our dependence on fossil fuels, thereby decreasing greenhouse gas emissions and mitigating the effects of climate change.In addition to mitigating climate change, renewable energy also has many other environmental benefits, including reducing air and water pollution and preserving biodiversity. It also offers a sustainable energy source that is not depleted, unlike fossil fuels.There are many measures that can be taken to save Mother Nature and the human race from eventual extinction or doom. Some of these measures include:Transitioning to a low-carbon economy: This involves reducing our reliance on fossil fuels and transitioning to renewable energy sources like solar, wind, and hydro power.Implementing sustainable agricultural practices: This includes reducing food waste, promoting sustainable farming practices, and reducing the use of pesticides and other harmful chemicals.Preserving natural habitats: Protecting natural habitats and conserving biodiversity is essential for maintaining the health of the planet.Promoting sustainable transportation: Encouraging the use of public transportation, biking, and walking can help reduce carbon emissions and improve air quality.Adopting sustainable consumption practices: This involves reducing our consumption of single-use plastics, promoting sustainable packaging, and reducing the amount of waste we produce.These measures, along with many others, can help to ensure a sustainable future for both Mother Nature and humanity. MJ: how can human beings be compatible with the animal life and the plant life and ...mineral life?​AI: To be compatible with animal life, plant life, and mineral life, humans must be mindful of their impact on the environment and take measures to reduce harm. Here are some steps individuals can take:Reduce carbon footprint: Reducing carbon emissions by conserving energy and reducing the use of fossil fuels can help combat climate change, which is causing harm to many plant and animal species.Conserve water: Conserving water by reducing waste and using water-saving devices can help protect freshwater ecosystems, which are home to many species of animals and plants.Reduce waste: Proper waste management, including recycling and composting, can reduce pollution and habitat destruction caused by waste disposal.Protect wildlife: Protecting wildlife habitats and avoiding activities that harm animals, such as hunting and fishing, can help preserve biodiversity.Plant trees and gardens: Planting trees and gardens can help provide habitat for wildlife and reduce carbon emissions by absorbing carbon dioxide from the atmosphere.Use environmentally-friendly products: Using environmentally-friendly products, such as biodegradable cleaners and reusable bags, can reduce harm to the environment and wildlife.Overall, living in a way that is compatible with animal life, plant life, and mineral life requires mindfulness and a commitment to reducing harm to the environment. MJ: can digital currency or blockchain be compatible with dollar dominance? what are the future of dollar or blockchain? And how useful is blockchain in the future?​AI: Digital currency and blockchain can coexist with dollar dominance, but they also present a potential challenge to it. The U.S. dollar has been the dominant global reserve currency for decades, and it is still widely used for international trade and financial transactions. However, the emergence of digital currencies and blockchain technology has the potential to disrupt traditional financial systems and challenge the role of the dollar.Blockchain is a distributed ledger technology that allows for secure, transparent, and tamper-proof record-keeping. It has numerous potential applications beyond digital currencies, such as in supply chain management, identity verification, and smart contracts. As such, blockchain is likely to become increasingly useful in the future, as more industries explore its potential.In terms of the future of the dollar, it is difficult to predict with certainty. However, some experts predict that the rise of digital currencies and blockchain could lead to a gradual shift away from the dollar as the dominant global reserve currency. This could happen if more countries and institutions adopt digital currencies as an alternative to the dollar, or if blockchain technology is used to create more efficient and secure international payment systems.That being said, it is important to note that the dollar is deeply entrenched in global financial systems, and any shift away from it is likely to be gradual and slow-moving. Additionally, the U.S. government and the Federal Reserve have a significant amount of control over the value of the dollar, and they may take steps to maintain its dominance.In summary, digital currencies and blockchain have the potential to be compatible with dollar dominance, but they also present a challenge to it. The future of the dollar and blockchain is uncertain, but both are likely to play significant roles in the global economy in the coming years. MJ: Thanks to you, I am much the wiser today. Thanks, Pal, my A.I. pen pal^^​AI: You're welcome! I'm glad I could be of help. Don't hesitate to ask if you have more questions in the future.​영영요약:MJ asked about the future trends that are important and useful. AI responded by listing some of the trends that are expected to have an impact in various fields, including artificial intelligence and machine learning, the internet of things (IoT), renewable energy, blockchain, virtual and augmented reality, and cybersecurity. MJ then asked for more information about artificial intelligence and machine learning, and AI explained that AI is the development of computer systems that can perform tasks that require human intelligence, while machine learning is a subfield of AI that focuses on the development of algorithms that can learn from and make predictions on data. The most advanced form of AI and machine learning is currently considered to be deep learning, which is a subset of machine learning that uses neural networks with many layers to learn representations of data. Renewable energy has a significant role to play in saving Mother Nature and humanity's fate. Climate change, caused by the burning of fossil fuels, is one of the most pressing environmental issues facing humanity today. AI suggested that some of the measures that can be taken to save Mother Nature and the human race from eventual extinction or doom include transitioning to a low-carbon economy, implementing sustainable agricultural practices, preserving natural habitats, and promoting sustainable transportation.​객관식문제5:Which of the following is NOT mentioned as a future trend in the given passage? a) Renewable energy b) Blockchain technology c) Artificial intelligence d) Quantum computing e) Internet of Things (IoT)​​​Answer: d) Quantum computing is not mentioned in the passage.​What is machine learning? a) A subfield of AI that focuses on developing computer systems that can perform tasks that normally require human intelligence b) The process of providing the algorithm with labeled data c) The process of training an algorithm to make decisions based on feedback received from its environment d) A technology that uses neural networks with many layers to learn representations of data e) A technology used to power virtual assistants, chatbots, and recommendation systems​​​Answer: a) Machine learning is a subfield of AI that focuses on developing computer systems that can perform tasks that normally require human intelligence.​What is the most advanced form of Artificial Intelligence and Machine Learning? a) Reinforcement learning b) Supervised learning c) Unsupervised learning d) Deep learning e) None of the above​​​Answer: d) Deep learning is the most advanced form of Artificial Intelligence and Machine Learning.​What are the potential negative consequences of AI and machine learning? a) Increased privacy b) Algorithmic bias c) More job opportunities d) Greater ethical concerns e) Increased social interactions​​​Answer: b) Algorithmic bias is a potential negative consequence of AI and machine learning.​What are some measures that can be taken to save Mother Nature and the human race from eventual extinction or doom? a) Transitioning to a low-carbon economy b) Promoting sustainable transportation c) Preserving natural habitats d) All of the above e) None of the above​​​Answer: d) Transitioning to a low-carbon economy, implementing sustainable agricultural practices, preserving natural habitats, and promoting sustainable transportation are some measures that can be taken to save Mother Nature and the human race from eventual extinction or doom.​ "
TALKTOCHATGPT로 영어회화 무료로 공부하기 ,https://blog.naver.com/growthgo/223095844170,20230507,"​요즘 제가 '닥터 최정숙'이라는 드라마에 푹 빠져 있는데 꼭 저의 이야기인 것 같아서 배 아프게 재미있으면서도 저한테 많은 인사이트를 주는 프로그램입니다.​20대 후반 영어교육대학원에서 공부를 마치고 영국 뉴캐슬대학교 언어학 전공에 입학하였는데 부득이한 이유로 끝까지 마치지 못한 저의 꿈이 못내 아쉬워하고 있었습니다! ​이처럼 우리는 삶을 살다 보면 이루지 못한 꿈을 갖게 되곤 합니다. 그것이 제어할 수 없는 상황 때문이거나 기회 부족 때문이든, 이러한 꿈은 몇 년에서는 몇십 년에 이르기까지 현실화되지 못할 수 있습니다. ​현실에서 영어를 가르치는 일을 하고 있지만 영어회화에 항상 목말라 있었습니다다행히도 여러 가지 매체로 꾸준히 공부를 했는데 최근에는 CHATGPT를 통해 영어회화 연습을 많이 하고 있습니다. 언젠가 제 꿈인 영국 대학원 진학을 목표로 제가 도움을 받고 있는 아주 간단한 확장 프로그램인 TALK-TO-CHAT GPT를 소개하려고 합니다.​저랑 같이 영어 수준 업해보시겠어요?​ TALK TO CHATGPT란?Talk-to-chatgpt는 크롬 구글 확장 프로그램 중 음성으로 명령어를 넣어 AI가 음성으로 답을 해주는 인공지능챗봇입니다.​Chrome에서 talktochatgpt를 검색합니다.'크롬에서추가'를 클릭하면 확장 program이 깔리게 됩니다.  ​2. 팝업창이 뜨면 추가를 클릭합니다. ​3. 톡투챗지피티가 구글에 추가됩니다. ​4. 오픈AI 챗지피티로 접속하면 자동으로 TALKTOCHATGPT가 왼쪽 상단에 나타납니다.스타트를 눌러주면 실행이 됩니다.  ​5. 그전에 영어로 연습하기 위해서 간단한 세팅이 필요합니다. 왼쪽 상단에 설정을 누릅니다. ​설정화면이 나오면 아래처럼 세팅해 줍니다.Speech recognition language에서 English-en-USAI voice and language에서 Google US English ​6. 세팅도 다 되었으니 talktochatgpt로 대화 연습을 시도하여 주세요! 하는 방법은 오른쪽 상단에 박스가 있는데 마이크 모양이 빨간 불일 때는 내가 말할 수 있고초록 불일 때는 AI가 말하고 있는 걸 알려줍니다. 스피커 모양은 Speaker로 나오는 음성의 재생을 중단할 수 있습니다. 화살표 버튼은 빨리 감기이므로 현재 읽고 있는 음성 재생을 넘길 수 있습니다.   ​AI가 제 말을 못 알아듣는 경우도 있긴 하지만차근차근 연습하면 언젠가 실력이 늘 거예요!Good luck!​ ​ "
교사를 위한 주요 8가지 구글 DOC Add-On들 ,https://blog.naver.com/thepledo/221677104631,20191014,"  최근 많은 교사들이 구글에서 제공하는 G-Suite for Education을 사용하거나 관심을 가지고 있습니다. ​오늘은 교사를 위한 최고의 구글 문서 부가기능들을 10가지 소개하여 드리겠습니다. 이 부가 기능으로 할 수 있는 것들은 다음과 같습니다.​· 음성 및 텍스트 설명을 모두 사용하여 피드백 주고받기.· 다른 언어의 악센트를 구글 문서에 삽입· 교육적인 Lubrics를 쉽고 빠르게 생성하기· 음성 인식을 사용하여 구글 문서에 음성 입력· 문서에 사용할 클립 아트 이미지 라이브러리에 액세스하여 사용· 쉬운 문서 탐색을 위해 목차를 자동으로 생성· 다이어그램, 플로우 차트 및 마인드 맵을 작성하여 문서에 삽입· 문서에 전자서명을 추가하고 PDF 형식으로 공유​① Kaizena (Voice Comments) ② Easy Accents ③ OrganicSlice: Teacher Rubric ④ Lucidchart Diagrams ⑤ Hellosign ⑥ EasyBib Bibliography Creator ⑦ Speech Recognition Soundwriter ⑧ Template Gallery ​  William Lee / wlee@pledo.coCEO and President, PLEDO, LLCMember, SRT(Secure Relaible Transport) ForumHead of Business Development, Switchboard LiveVideo Evangelist, Wowza Media SystemsAdvisor, STEAMPunksEduFormer CEO, Widermax Corp ​ "
Get to Know Hangul ,https://blog.naver.com/kore410_2023/223108239153,20230522,"First, we will start off with how everything has started. We will be talking about the origin of Hangul. It is totally okay if you have never heard of it before. Watch the video, it will explain all!​FAQIs there a difference between Korean and Hangul?- Korean and Hangul are different. Although both are often used interchangeably, Korean refers to the spoken language and Hangul refers to the written language. ​Does this language actually take only one week to learn?- King Sejong mentioned that learning Hangul shouldn't take more than a week. Hold your horses, learning a new language is hard. King Sejong was addressing people who already speak Korean. Of course, it is easy for them to read Hangul. What we are teaching here is only the basics. It will take longer for your to read, write, speak, and understand Korean. ​I've noticed Korean is very monotone.- Yes, Korean does not have tones different from the nearby languages like Chinese and Japanese. Fun fact, Korean did include tones a long time ago. Today, it only exists among a few dialects. But you don't need to worry about it. ​Do North Korea and South Korea use the same language?Yes and no. While some parts of the language remain the same, each Korea took a different approach in developing the language. The North went for isolated development of their language, creating new words as needed. The South went for adopting different languages as their own, embracing a few words of English, Chinese, Japnese, German, etc. as needed. However, the overall Korean language is still similar in that if North and South Koreans encounter each other, they will still be able to communicate using their own Korean. ​Am I pronouncing things correctly?Indeed, pronuncaition comes to a challenge when learning a new language. THe best thing is to have someone who is already fluent to check for you. However, you can check on your own by using your phone's AI assistant of speech recognition apps. Change the language to Korean, and if your phone understandds what you're saying, you're good to go!​I am afraid I would pronounce things incorrectly.It is okay to not be perfect when you've just started learning. People will understand by the context or gestures. So, don't worry too much. Remember that confidence is the key. ​Does 'ㅇ' have a sound?Vowels alone cannot make a word. In order to make a proper word, we need consonants. ㅇhas no sound, but is placed as a consonant to complete a word. When reading ㅇ, you are only prouncing the vowels.   Here's also a video you could watch for fun just for an additional understanding: https://youtu.be/K53oCDZPPiw "
"""AI godfather"" Geoffrey Hinton warns of dangers. ""AI 대부"" 제프리 힌턴, 구글 퇴사하며 AI 위험성 경고 ",https://blog.naver.com/misael/223092279030,20230503,"'인공지능 AI 연구의 대부'로 꼽히는 Geoffrey Hinton 제프리 힌턴 교수가 AI의 위험성을 알리며, 10년 이상 몸담았던 Google 구글을 떠났다.​YTN    윤보리    May 2, 2023​그는 AI 기술이 적용된 '킬러 로봇'이 현실이 되는 날이 두렵다고 했다.​제프리 교수는 1972년부터 AI를 연구하며 딥러닝 개념을 처음 고안했지만, 지금은 평생을 바친 연구를 후회한다고 했다.​그는 ""AI가 생성한 가짜 사진과 동영상, 글이 넘쳐나며 사람들은 더는 무엇이 진실인지 알 수 없게 될 거라고"" 했다.​챗GPT 등이 인간의 업무 능력을 보완하기도 하지만 비서나 번역가 등을 대체할 수 있다며, AI 기술이 노동시장에 미칠 영향에 대해서도 우려했다.​또 당초 ""AI가 사람보다 똑똑해지려면 30~50년, 또는 그보다 더 오랜 시간이 걸릴 거""라고 봤지만 이제 일부 기능에서는 인간의 지능을 넘어서기 시작했다고 평가했다.​그러면서 비밀리에 개발해도 타국의 추적이 가능한 핵무기와 다르게, AI는 규제가 도입되더라도 기업이나 국가 차원의 연구가 계속될 가능성이 있다는 점이 우려된다고 했다.​이어 ""구글과 마이크로소프트 등의 경쟁은 글로벌 규제 없인 멈추지 않을 것이며, AI 분야에 국제적인 규제가 필요하다""고 주장했다.​오픈AI의 공동 창업자였던 일론 머스크 역시 AI의 잠재적 위험성을 경고하면서 AI 개발에 규제가 필요하다고 주장해왔다.​그는 ""AI는 선과 악을 행하는 거대한 힘을 갖고 있다. AI에 대한 선의의 의존조차도 기계 작동법을 잊어버릴 정도가 되면 인류문명에 위험할 수 있다""고 경고했다.​현재 유럽연합과 주요 7개국 등 세계 곳곳은 AI 규제 관련 논의에 착수했지만, 규제 강도는 제각각이다. 유럽은 챗GPT 같은 생성형 AI를 고위험 도구로 분류해 엄격한 규제 대상으로 삼는 방안을 논의 중이다. 하지만 미국과 일본은 기업의 자율 규제와 활용에 무게를 두면서 법적 규제에 신중한 모습이다.​빅테크 기업들의 인공지능 개발 경쟁에 불이 붙은 가운데, AI의 순기능은 살리면서도, 예상되는 사회적 문제점은 제도로 보완하기 위한 논의는 이제 겨우 시작이다.​인공지능이 우리에게 던진 이 쉽지 않은 숙제에 대해 앞으로 어떤 해법이 도출될 수 있을지 주목된다. ""AI godfather"" Geoffrey Hinton warns of dangers as he quits Google​By Zoe Kleinman & Chris Vallance     BBC News     2023. 05. 02​A man widely seen as the godfather of artificial intelligence (AI) has quit his job, warning about the growing dangers from developments in the field. Geoffrey HintonGeoffrey Hinton, 75, announced his resignation from Google in a statement to the New York Times, saying he now regretted his work.​He told the BBC some of the dangers of AI chatbots were ""quite scary"".​""Right now, they're not more intelligent than us, as far as I can tell. But I think they soon may be.""​Dr Hinton also accepted that his age had played into his decision to leave the tech giant, telling the BBC: ""I'm 75, so it's time to retire.""​Dr Hinton's pioneering research on neural networks and deep learning has paved the way for current AI systems like ChatGPT.​In artificial intelligence, neural networks are systems that are similar to the human brain in the way they learn and process information. They enable AIs to learn from experience, as a person would. This is called deep learning.​The British-Canadian cognitive psychologist and computer scientist told the BBC that chatbots could soon overtake the level of information that a human brain holds.​""Right now, what we're seeing is things like GPT-4 eclipses a person in the amount of general knowledge it has and it eclipses them by a long way. In terms of reasoning, it's not as good, but it does already do simple reasoning,"" he said.​""And given the rate of progress, we expect things to get better quite fast. So we need to worry about that.""​In the New York Times article, Dr Hinton referred to ""bad actors"" who would try to use AI for ""bad things"".​When asked by the BBC to elaborate on this, he replied: ""This is just a kind of worst-case scenario, kind of a nightmare scenario.​""You can imagine, for example, some bad actor like [Russian President Vladimir] Putin decided to give robots the ability to create their own sub-goals.""​The scientist warned that this eventually might ""create sub-goals like 'I need to get more power'"".​He added: ""I've come to the conclusion that the kind of intelligence we're developing is very different from the intelligence we have.​""We're biological systems and these are digital systems. And the big difference is that with digital systems, you have many copies of the same set of weights, the same model of the world.​""And all these copies can learn separately but share their knowledge instantly. So it's as if you had 10,000 people and whenever one person learnt something, everybody automatically knew it. And that's how these chatbots can know so much more than any one person.""​Matt Clifford, the chairman of the UK's Advanced Research and Invention Agency, speaking in a personal capacity, told the BBC that Dr Hinton's announcement ""underlines the rate at which AI capabilities are accelerating"".​""There's an enormous upside from this technology, but it's essential that the world invests heavily and urgently in AI safety and control,"" he said.​Dr Hinton joins a growing number of experts who have expressed concerns about AI - both the speed at which it is developing and the direction in which it is going.​'We need to take a step back'​In March, an open letter - co-signed by dozens of people in the AI field, including the tech billionaire Elon Musk - called for a pause on all developments more advanced than the current version of AI chatbot ChatGPT so robust safety measures could be designed and implemented.​Yoshua Bengio, another so-called godfather of AI, who along with Dr Hinton and Yann LeCun won the 2018 Turing Award for their work on deep learning, also signed the letter.​Mr Bengio wrote that it was because of the ""unexpected acceleration"" in AI systems that ""we need to take a step back"".​But Dr Hinton told the BBC that ""in the shorter term"" he thought AI would deliver many more benefits than risks, ""so I don't think we should stop developing this stuff,"" he added.​He also said that international competition would mean that a pause would be difficult. ""Even if everybody in the US stopped developing it, China would just get a big lead,"" he said.​Dr Hinton also said he was an expert on the science, not policy, and that it was the responsibility of government to ensure AI was developed ""with a lot of thought into how to stop it going rogue"".​'Responsible approach'​Dr Hinton stressed that he did not want to criticise Google and that the tech giant had been ""very responsible"".​""I actually want to say some good things about Google. And they're more credible if I don't work for Google.""​In a statement, Google's chief scientist Jeff Dean said: ""We remain committed to a responsible approach to AI. We're continually learning to understand emerging risks while also innovating boldly."" AI: What is the future of artificial intelligence? BBC News  Is artificial intelligence (AI) an opportunity, a threat, or even both?Artificial intelligence is a branch of computer science which develops machines and software which can perform tasks which normally require humans to do them, for example, decision-making or speech recognition.Experts are divided on how this technology should be regulated, and who by.​It is important to remember that AI chatbots are just one aspect of artificial intelligence, even if they are the most popular right now.​AI is behind the algorithms that dictate what video-streaming platforms decide you should watch next. It can be used in recruitment to filter job applications, by insurers to calculate premiums, it can diagnose medical conditions (although human doctors still get the final say).​What we are seeing now though is the rise of AGI - artificial general intelligence - which can be trained to do a number of things within a remit. So for example, ChatGPT can only offer text answers to a query, but the possibilities within that, as we are seeing, are endless.​But the pace of AI acceleration has surprised even its creators. It has evolved dramatically since Dr Hinton built a pioneering image analysis neural network in 2012.​Even Google boss Sundar Pichai said in a recent interview that even he did not fully understand everything that its AI chatbot, Bard, did.​Make no mistake, we are on a speeding train right now, and the concern is that one day it will start building its own tracks. Interview: Geoffrey Hinton talks impact and potential of AI       CBS Mornings      Mar 25, 2023​Geoffrey Hinton is considered a ""Godfather of artificial intelligence"", having championed machine learning decades before it became mainstream. As chatbots like ChatGPT bring his work to widespread attention, we spoke to Hinton about the past, present and future of AI. ​CBS Saturday Morning's Brook Silva-Braga interviewed him at the Vector Institute in Toronto on March 1, 2023.​Time stamps​2:45 – why mainstream AI in the 1980s shunned neural networks3:21 – Hinton believed in neural networks because the brain worked this way 4:31 – two different paths to intelligence6:15 – lack of data and compute that impeded deep learning’s progress8:57 – the start of deep learning in 200610:04 – two big deep learning developments: speech recognition at Google and object recognition at University of Toronto10:49 – how object recognition works in layman’s terms15:47 – breakthroughs in object recognition influencing the AI community18:28 – why Hinton likes the company Cohere19:20 – biological brains vs digital brains21:39 – ChatGPT as an “idiot savant” with a lack of understanding of truth24:56 – how society should handle this new AI30:52 – self-healing minefield proposed by the government31:43 – how to create an effective autonomous solder and the alignment problem33:12 – are large language models “just autocomplete”? Are humans “just autocomplete”?33:52 – translating “The trophy would not fit in the suitcase because it was too big” into French. English-French translation and the insights it provides into LLMs’ understanding of the world35:50 – computers coming up with new ideas for itself37:00 – AI displacing jobs38:15 – how big of a revolution is AI?40:37 – is AI sentient? "
현존하는 인공지능프로그램의 종류 ,https://blog.naver.com/webpd3/223096709018,20230508,"인공지능 프로그램은 여러 종류가 있으며, 그 용도와 기술적 특성에 따라 분류될 수 있습니다. 대표적인 인공지능 프로그램 종류를 아래와 같이 소개해드리겠습니다.​챗봇(Chatbot)​사용자와 대화를 하는 인공지능 프로그램으로, 주로 고객센터나 상담 서비스에서 이용됩니다.​예시: Google Assistant, Amazon Alexa, Apple Siri, Kakao i​이미지 인식(Image recognition)​이미지를 인식하고 분석하는 인공지능 프로그램으로, 자율주행 자동차, 보안 시스템, 의료진단 등에서 사용됩니다.​예시: Google Lens, Amazon Rekognition, IBM Watson Visual Recognition​음성 인식(Speech recognition)​음성을 인식하고 텍스트로 변환하는 인공지능 프로그램으로, 음성인식 기반의 가상 비서나 음성 명령어 인식 시스템에서 이용됩니다.​예시: Google Cloud Speech-to-Text, Amazon Transcribe, Apple Siri Speech Recognition​자연어 처리(Natural Language Processing)​인간의 언어를 이해하고 처리하는 인공지능 프로그램으로, 검색 엔진, 자동 번역, 문서 분류 등에 이용됩니다.​예시: Google Cloud Natural Language API, Amazon Comprehend, IBM Watson Natural Language Understanding​추천 시스템(Recommendation system)​사용자의 취향이나 행동 패턴을 분석하여 상품 추천이나 맞춤형 콘텐츠 제공 등에 이용됩니다.​예시: Netflix, Amazon Personalize, YouTube​감성 분석(Sentiment analysis)​문서나 대화 등에서 감정을 분석하는 인공지능 프로그램으로, 마케팅, 고객서비스, 정치 분석 등에서 이용됩니다.​예시: Google Cloud Natural Language API, Amazon Comprehend, IBM Watson Tone Analyzer​기계 번역(Machine translation)​자동으로 한 언어에서 다른 언어로 번역하는 인공지능 프로그램으로, 글로벌 비즈니스나 국제학술대회 등에서 이용됩니다.​예시: Google Translate, Amazon Translate, Microsoft Translator​이 외에도 다양한 인공지능 프로그램들이 있으며, 기술의 발전과 함께 새로운 종류의 인공지능 프로그램도 지속적으로 개발될 것으로 예상됩니다​​ "
Chat GPT ,https://blog.naver.com/jeannie74/223079918187,20230420,"#linc #chatgpt​ChatGPT 가 핫이슈이다. 영어단어공부할 때 ChatGPT 활용하는 것을 레코드해서 친구들에게 공유​  VocabularyPart of speechMeaningintelligentadjectiveHaving the ability to learn, understand, and apply knowledge and skills.intelligencenounThe ability to learn, understand, and apply knowledge and skills.AI (artificial intelligence)nounThe development of computer systems that can perform tasks that typically require human intelligence, such as visual perception, speech recognition, decision-making, and language translation.advantagesnounBenefits or positive aspects of something.disadvantagesnounDrawbacks or negative aspects of something.opportunities and challengesnoun phraseA situation that offers possibilities for advancement or progress but also presents difficulties or obstacles.show promiseidiomIndicate potential for success or effectiveness in the future.plagiarismnounThe act of using someone else's work or ideas without proper attribution or permission.ethicsnounThe principles of right and wrong that govern behavior.ethicaladjectiveRelating to or in accordance with the principles of ethics.machine-generatedadjectiveProduced by a machine or computer program.to generateverbTo produce or create something.to pump out an essayverb phraseTo quickly and mechanically produce an essay, often without much thought or effort.to mitigateverbTo make something less severe, harmful, or painful.Government regulationnoun phraseRules or laws established by a government to control or direct certain activities.to regulateverbTo control or direct certain activities according to established rules or laws.threatnounA statement or action that suggests harm, danger, or damage.humanitynounThe human race; human beings collectively.privacynounThe state of being free from public attention or intrusion into one's personal life or affairs.time managementnoun phraseThe ability to use time effectively and efficiently.save timeverb phraseTo use time in a way that maximizes productivity and efficiency.awareadjectiveHaving knowledge or perception of a situation or fact.panicnounA sudden, overwhelming feeling of fear or anxiety.panickyadjectiveFeeling or showing sudden, overwhelming fear or anxiety.to detectverbTo discover or identify the presence or existence of something. AI (artificial intelligence) - noun: The simulation of human intelligence processes by machines, especially computer systems.​plagiarism - noun: The act of using someone else's work without giving proper credit or permission.​ethics - noun: The moral principles that govern a person's behavior or the conduct of an activity.​ethical - adjective: Relating to or concerning moral principles, especially those governing a person's behavior.​unethical - adjective: Contrary to accepted moral principles; not morally correct.​advantages - noun: Factors or circumstances that put one in a favorable or superior position.​opportunities - noun: A set of circumstances that makes it possible to do something.​strengths - noun: The qualities or attributes that make a person or thing strong.​challenges - noun: A call or summons to engage in any contest, as of skill, strength, etc.​show promise (idiom) - phrase: To display qualities or potential that suggest future success or development.​machine-generated - adjective: Created or produced by a machine or computer.​to generate - verb: To produce or create something.​to pump out an essay - phrase: To produce an essay quickly and in large quantities.​to mitigate - verb: To make less severe, serious, or painful.​government regulation - noun: The act of controlling or directing according to a set of rules or laws enforced by the government.​to regulate - verb: To control or direct according to a set of rules or laws.​threat - noun: A statement or action that is intended to cause harm or damage.​humanity - noun: The human race; human beings collectively.​privacy - noun: The state of being free from public attention or being observed by others.​time management - noun: The process of planning and controlling how much time to spend on specific activities.​save time - verb phrase: To prevent the loss or waste of time.​aware - adjective: Having knowledge or perception of a situation or fact.​panic - noun: A sudden feeling of fear or anxiety, often causing irrational behavior.​panicky - adjective: Feeling or showing panic.​to detect - verb: To discover or identify the presence or existence of something.​to brace yourself against something - phrase: To prepare oneself mentally or emotionally for something difficult or unpleasant.​misleading - adjective: Tending to give a wrong idea or impression.​lofty - adjective: Of imposing height; elevated in character, noble; haughty.​ "
5. How do our brains process speech.(4:21) ,https://blog.naver.com/ari8317/222265882708,20210305,"                                                                                                                                                  -by Garath Gaskell​The average 20 year old knows between 27,000 and 52,000 different words. By age 60, that number averages between 35,000 and 56,000. Spoken out loud, most of these words last less than a second. So with every word, the brain has a quick decision to make: which of those thousands of options matches the signal? About 98% of the time, the brain chooses the correct word. But how?​Speech comprehension is different from reading comprehesion but it's similar to sign language comprehension - though spoken word recognition has been studied more than sign language. The key to our ability to understand speech is the brain's role as a parallel processor, meaning that it can do multiple different things at the same time. ​Most theories assume that each word we know is represented by a separate processing unit that has just one job: to assess the likelihood of incoming speech matching that particular word. In the context of the brain, the processing unit that represents a word is likely a pattern of firing activity across a group of neurons in the brain's cortex. When we hear the beginning of a word, there are many possible matches. Then, as the word goes on, more and more units register that some vital piece of information is missing and lose activity. Possibly well before the end of the word, just one firing pattern remains active, corresponding to one word. This is called the ""recognition point."" In the process of honing in one word, the active units suppress the activity of others, saving vital milliseconds. Most people can comprehend up to about 8 syllables per second. ​Yet, the goal is not only to recognize the word, but also to access its stored meaning. The brain accesses many possible meanings at the same time, before the word has been fully identified. We know this from studies which show that even upo hearing a word fragment - like ""cap"" - listeners will start to register multiple possible meanings, like captain or capital, before the full word emerges.​This suggests that every time we hear a word there's a brief explosion of meanings in our minds, and by the recognition point the brain has settled on one interpretation. The recognition pocess moves more rapidly with a sentence that gives us context than in a random string of words. Context also helps guide us towards the intended meaning of words with multiple interpretations, like ""bat"", or ""crane"", or in cases of homophones like ""no"" or ""know"".​For multilingual people, the language they are listening to is another cue, used to eliminate potential words that don't match the language context. So, what about adding completely new words to this system?​Even as adults, we may come across a new word every few days. But if every word si represented as a fine-tuned pattern of activity distributed over many neurons, how do we prevent new words from overwriting old one?​We think that to avoid this problem, new words are initially stored in a part of the brain called the hippocampus, well away from the main store of words in the cortex, so they don't share neurons with other words. Then, over multiple nights of sleep, the ew words gradually transfer over and interweaves with old ones. Researchers think this gradual acquisition process helps avoid disrupting existing words. So in the daytime, unconscious activity generates explosions of meaning as we chat away. At night, we rest, but our brains are buy integrating new knowledge into the word net work. When we wake up, this process ensures that we're ready for the ever-changing world of language.​​*Ted 강연은 둘 중 하나다. 감동을 주어 무얼 결심하게 하거나, 새로운 지식을 얻게 하거나. 때로는 둘 다.^^​*언어를 가르치고 있으니만큼 흥미롭게 읽었는데, 결론은 엉뚱하게도 '잠이 중요하다'였다.ㅋㅋ​*스무 살 정도 되는 성인은 27,000 - 52,000개의 단어를 알고 있고, 60세 정도 되면 35,000에서 56,000으로 는단다. 사실 35,000과 56,000의 간극은 얼마나 큰가? 그래도 3만보다는 5만에 가까운 어휘력을 갖게 되길. 책 좀 읽어야 하는데, 그것도 어려운 것으로.​*다시 '잠'으로 돌아와서, 그래서 자기 전에 이렇게 강연을 보고, 쓰고, 다시 블로그에 한 자 한 자 옮겨 적는 활동이 꽤 도움이 되겠구나. 한다. 문제는 요새, 낮이나 초저녁에 자고 밤에 잠을 못 자고.. 아침에 눈을 붙이거나 피곤한 채로 일어난다는 것. 미국 장이 완전 널뛰기 중이거든~ 아마 그 이유가 크지 싶다!ㅋ​*보면 비슷한 주제로 쭉 간다면 같은 카테고리의 단어들을 자주 접하니 더 잘 외울 수 있을 것 같다. 술과 커피가 수면에 미치는 영향에서 배운 cortex(대뇌피질)가 여기도 나와 이번에는 찾지 않아도 되었음. 오늘 하나 더 추가하는 것은 hippocampus(해마)다. 처음엔 보고 하마+캠퍼스인 줄 알고 뭔가.. 했다는. 무식이 철철이다.ㅋㅋ^^;;​*교재 만드는 작업과 함께, 1Day 1Ted로.. 콩이 쌓여간다. 우후훗! "
"[인공지능 로봇]아티보, 음성 언어 인식하기  ",https://blog.naver.com/onmyway86/222024545976,20200708,"https://education.cubroid.com/ EDUCATION CODING :: CUBROIDGet started with easy and fun online education and learning with Cubroid! arrow_forward Sign up to start You can check your progress. Learn anytime, anywhere for free. Provides easy and fun educational material. education.cubroid.com 지난 주에 이어서 이제는 음성 언어 인식하기로~카메라에 내장된 마이크를 이용하여 음성 코딩을 할 수 있겠지만음성이라는 것이 조금 분석하기 어렵기 때문에 현재도 사진, 카메라 기능이 더 보급되어 있을듯 ㅎ이 어려운 음성을 어떻게 인식할지 한번 확인해보기 위해서~ 에듀케이션 사이트로 이동해서 12번 음성 인식하기 기능 살펴보기~​  12번 음성인식에 들어가면 음성 인식에 관한 설명이 있다~요즘 코로나로 인해서 온라인 수업을 하면서 많이 사용하는 네이버 클로바 더빙은 TTS기능~text to speech 이니깐 반대 개념이라고 보면 되겠지~   이번 코딩에서는 아래에서 두번 째 있는 speech 카테고리에 코딩 블럭을 사용하면 되고~음성 녹음 시간은 3초에서 7초까지 인식하고 타이머로 세팅하면 5초까지 기다렸다가 시작할 수 있다는 점~   블럭 설명은 아래와 같고~맨 밑에 있는 speech recognition 을 이용하면 변수로 활용하여서 다양한 동작을 구현할수 있을 것 같았다  아! 맨 앞에 있는 언어는 16개 언어까지 지원이 된다고 하는데다른 언어를 쓸 것은 아니지만 그만큼 인식할 수 있는 단어가 많다고 봐야하려나??   암튼 언어 관련 코딩 블럭의 가짓수는 적은데이를 이용해서 코딩하는 예제가 있어서우선 예제를 따라서 진행해보고 이어서 응용으로 진행해도 좋을 것 같았다~  코딩하는 곳으로 이동~https://coding.cubroid.com/ ArtiboMake your own AI robot! AI Coding Block, ARTIBO Experience AI by using ARTIBO’s camera and microphone! tag_faces CREATE ACCOUNT “Recognize an apple by ARTIBO’s camera(image recognition), Compare the result to ‘apple’(text), If so, speak ‘apple’ from speaker(speaker output) and display ‘apple’ on LCD...coding.cubroid.com 우선 음성 인식은 아티보 헤드에서 하기 때문에코드를 실행한 뒤 이런 마이크표시가 뜬 뒤 녹음을 시작하면 됨~와이파이망 속도에 따라서 인식되는 속도가 다르니 이 부분을 유의하면 좋을듯 ㅎ  암튼 처음에 해 본 것은 나름 변수값으로 사용하여서 내가 말한 글자 크게 화면에 나타내기~원래 화면에도 어떤 내용인지 뜨긴 하지만 이렇게 화면에 표시가 되면 실행하면서 바로바로 피드백을 받을 수 있을듯(피지컬을 실행한 뒤 모니터 화면 보다는 피지컬 교구를 많이 보니깐~)  그래서 이러한 것을 으용해서 예제는 해피이지만 괜히 앵그리로 표현해보기 ㅋ  음성 인식을 하니 어디에 사용할 수 있을까 고민을 하다가~역시 움직임이지 싶더라는~일단 단순하게 나타내보았는데 이를 이용하면 음성 인식을 이용하여 미로탈출이나 조작을 할 수 있지 않을까?그리고 한국어로 세팅을 해 놓으니 go가 아니라 고 로 인식을 한다는 점~언어에 따라서 참조 언어를 잘 세팅해야할 듯 ㅎ   연습을 해보면서 느낀 점은 예전에 했던 라제타 스톤 하는 느낌인듯 ㅎ요즘 구글 어시스턴트로 영어 수업 때 발음 체크를 가끔 하는데~아티보를 활용하여 영어 교과와도 연계해서 수업 진행해도 괜찮을 것 같긴 했다~다만 변수값을 넣을 때에는.... 글자가 겹쳐서 찍히는 단점이 있던데 개선되겠지? ㅋ  ​ "
논문 - speaker recognition by machines and humans ,https://blog.naver.com/gustjtls123/222279616546,20210318,"들어가며...1. 최근 12년간 저명한 화자인식 모델 기법을 알아본다.2. voice-activity detection (VAD), features, speaker models, standard evaluation data sets, and performance metrics 을 포함하여 각 기법의 차이점을 알아본다.3. 화자인식을 2가지 측면에서 다룬다.    1) 법의학의 화자인식 방법    2) 일반 청자가 신경과학의 관점에서 어떻게 화자인식을 수행하는지4. 결과적으로 화자인식을 사람vs기계 비교하고 각각 강점과 약점을 찾아낸다.​tip. 법의학적 = 과학수사 Introduction 최근 화자인식과 검증에 대한 연구가 활발하다. 이는 사회에서 이를 필요로 하는 분야가 많아지기 때문이다.가까운 미래에 siri같은 것들이 우리가 말한 것을 인지하는 수준 뿐만 아니라 우리의 목소리나 다른 식별할 수 있는 특징을 잡는 지적이고 더 기능적인 기술이 나올 것이다. 핸드폰 같이 휴대가능한 장비들이 나오면서 생체 인증은 중요해졌고 목소리를 사용하여 인증하는 것은 인간에게 가장 자연스러운 현상이다. 화자 인식은 법 집행, 국가 보안, 일반 법의학에서도 중요하다. 범죄를 예상하거나 더 정확한 증거 확보를 위해서 필요하다. 인간은 당연하게 목소리로 서로를 인식한다. 심지어 비언어적 요소로도 인지할 수 있다. 과연 인간은 어떻게 자연스럽게 이를 인식할 수 있는가가 중요한 연구과제가 되었다. 설명하기 전에, 아래와 같이 화자인식을 정의하고 간다.1. naïve speaker recognition : 의식적인 훈련없이 친근한 목소리를 인식하는 것2. forensic speaker recognition : 훈련된 청자는 기계적으로 화자 샘플과 발성된 문장을 비교한다.(범죄자의 목소리 식별 등)3. automatic speaker recognition : 화자분석과 결정프로세스를 수행하는 컴퓨터 분석1,2번에서는 사람이 직접적으로 프로세스에 관여한다. 2,3번은 본질적으로 기술적이다.  해당 논문에서는 최신의 3번 기술(화자인식 by computer)을 알아보고 1,2번과 비교할 것이다. 특히 2번같은 경우 3번과 굉장히 비슷한 처리과정을 보이기 때문에 이를 같이 살펴보는 것이 큰 의미가 있다. 두 분야가 아직은 협력하고 있지 않지만 협력하게 된다면 앞으로의 발전에 긍정적 영향력을 가져올 것이다.​ speaker-recognition tasks화자 인식은 두가지로 분류 가능 : 화자식별, 화자검증 - 화자식별에서 input이 해당 set 안에서만 들어오는 경우(ex 회사직원) : closed or in-set scenario - 미확인 화자도 input 되는 경우 : open-set speaker recognition (also out-of-set speaker identification)    * UBM필요 universal background model문장종속과 문장독립이 있음text-dependent / text-independent​ challenges in speaker recognition음성은 말하는 것에만(text) 국한된 것이 아니라 말하는 방식까지 포함된다.style shifting, intraspeaker variability = 같이 화자가 같은 단어를 항상 같은 식으로 발화하는 것은 아니다.또한 음성을 저장하고 전송하는 방법이 다양하기 때문에 화자인식을 하기 더 어려워진다.(ex 전화할때 상대방이 누구인지 알기 힘들거나 상대방이 아프면 누구인지 인식이 어려움)​ SOURCES OF VARIABILITY IN SPEAKER RECOGNITION변동성의 원인들 1. Speaker-based variability sources : 목소리에도 가변성 존재  -> intrinsic 혹은 within-speaker variability다음과 같은 특징을 가지고 있음.• Situational task stress : 주변 잡음이 있을 수 있음.• Vocal effort/style : 발성방법의 차이가 있을 수 있음• Emotion : 감정이 내제됨• Physiological : 생리적 특성 (아프거나 약 먹었거나)• Disguise : 변조할 수 있음​2. Conversation-based/higher-level mode/language of speaking variability sources : 아래와 같은 시나리오 때문에 가변성을 가짐• human-to-human• human-to-machine(그림참조)​3. Technology- or external-based variability sources아래와 같은 요소로 인하여 가변성 발생• electromechanical• environmental• data quality​결과적으로 화자인식에서 위와 같은 특싱을 인식하고 이에 맞는 프로세스를 구현해야 한다.외부적 요소로 인한 가변성은 수학적 기법을 통하여 어느 정도 해결하고 있다. 또한 외부적 요소를 얻을 수 있는 방법은 내인적 요소(본질적 변동성)보다 많다. 본질적 변동성은 자동평가에서 정량화하고 설명하기 매우 어렵다. CHALLENGES IN SPEAKER RECOGNITION결론적으로 특징 파라메터를 잡아내어 음성인식 및 화자검증을 구현하는 것이 중요하다. SPEAKER CHARACTERIZATION: FEATURE PARAMETERS화자 개인마다 고유 음성특징을 가진다. 음성인식에서 측정가능하고 미리 정해진 양상들(소리를 의미있게 비교하기 위한)을 가져야한다.(음성의 특징이 필요하다) 이를 feature parameters 라 정의한다.또한 음성의 가변성과 저하로 인하여 많은 feature parameters가 필수적이다.​- 이상적 feature parameter의 특성 = ideal property1. 다른 화자간 가변성은 높게, 내적화자(본인)의 가변성에 대하서는 낮아야함2. 위장(disguise or mimicry)에 저항할 수 있어야함3. 관련된 자료에서 높은 발생 빈도(차별할 수 있는 능력)    - 원하는 특성을 잘 표현한다.목적 부합성4. 전달력이 강함    - ??5. 추출과 측정이 용이​이제 과학수사 화자인식에서 쓰이는 특징 파라메터에 대해서 논의한다.(자동 음성인식에 적용하려구)이는 상황과 가변성에 의존한다. 따라서 다음과 같은 상황에서 파라메터를 알아보자. 1. AUDITORY VERSUS ACOUSTIC FEATURES2. LINGUISTIC VERSUS NONLINGUISTIC FEATURES3. SHORT-TERM VERSUS LONG-TERM FEATURESAUDITORY VERSUS ACOUSTIC FEATURES청각특징(Auditory features)은 듣고 객관적으로 묘사할 수 있는 것이다. 청각특징은 훈련된 청취자가 ""듣고 객관적으로 설명할 수 있는"" 음성 측면으로 정의된다.​어쿠스틱(음향) 특징(Acoustic features) 자동 알고리즘을 사용하여 음성 신호에서 파생된 수학적으로 정의된 매개 변수이다. 자동 시스템이 자주 어쿠스틱 특징을 사용ex) 기본주파수, 포뮬러 주파수​둘다 결국 어떤식으로 소리가 나는지에 대한 변수​결론 : 과학수사에서 둘다 고려해야 한다.  LINGUISTIC VERSUS NONLINGUISTIC FEATURES언어적 특징 매개변수(Linguistic feature parameter)는 ""특정 언어의 구조 내에서 또는 언어 또는 방언 간에"" 대비를 제공할 수 있다. 실제로 사람들이 어떤 식으로 발화하는지를 고려하는 변수​비언어적 특징(Nonlinguistic feature)에는 음성 내용과 관련이 없는 측면이 포함된다.대표적인 비언어적 특징으로는 음성 품질, 유창성, 음성 일시 중지(주파수 및 유형), 말하기 속도, 평균 기본 주파수 및 비음성 소리(기침, 웃음 등)가 포함될 수 있다.결국, 이러한 특징은 본질적으로 청각 또는 청각일 수 있다. SHORT-TERM VERSUS LONG-TERM FEATURES특징파라메터의 시간 간격으로 단기/장기를 구분할 수 있다.널리 사용되는 자동 시스템은 대부분 단기 음향 기능, 특히 음성 스펙트럼에서 추출된 기능을 사용한다.(위에서 말한 특징들이 짧은 시간동안 이뤄지는 것임) -> 단기 파라메터​장기 파라메터는 평균 단기 파라미터(예: 기본 주파수, 단기 스펙트럼)이다.​이는 개별 음성 사운드에 의한 변동에 무감각하다는 장점이 있으며 음성 세그먼트에서 보다 부드러운 측정을 제공한다.또한 장기 특징에는 에너지, 피치 및 포름형 등고선이 포함되며, 이는 장기간에 걸쳐 측정/평균화된다.특징 매개변수가 전체 음성 발화에서 추출된 경우, 우리는 이를 발화 수준 특징 또는 줄여서 발화 특징으로 참조한다.(너무 길어서 장기 파라메터로 끊어서 확인)​​  ​ FORENSIC SPEAKER RECOGNITIO오늘날, 과학수사 화자 식별은 일반적으로 언어학과 통계학에서 배경을 가진 전문가 음성학자에 의해 수행된다. 모든 사람이 동의하는 표준 절차는 없다. 왜냐하면 절차는 상황에 의해 좌우될 수 있기 때문이다. 따라서 다음 기법을 소개함 1. THE LIKELIHOOD RATIO2. APPROACHES IN FORENSIC SPEAKER IDENTIFICATION1. THE LIKELIHOOD RATIO우도비 측정(likelihood ratio (LR) measure) 도입하였다. 과학수사 전문가가 법정에서 소견의 강도를 표현하기 위해서. 즉, 음성 평가를 y,n가 아닌 확률적으로 신뢰도를 측정하는 것이다.따라서 과학수사 음성 비교 전문가의 목표는 음성 샘플이 동일한 화자와 다른 화자에 의해 말해졌다고 가정할 때 측정된 차이를 확률로 추정하는 것이다(아래 수식으로 이해 가능)[수식] X = 범죄 중 녹음된 음성 샘플(음성 녹음)Y = 용의자(피겨)로부터 얻은 음성 샘플.H0 = X와 Y가 동일인이 말하는 가설입니다.H1 = X와 Y가 서로 다른 사람이 사용한다는 가설입니다.E = 법적 증거(예: X와 Y의 평균 피치가 10Hz씩 다름)가 관찰되었습니다. 2. APPROACHES IN FORENSIC SPEAKER IDENTIFICATION기술된 방법은 전부 또는 부분적으로 전문가가 수행한다.(사람이 조정)아래 방식들을 활용하여 접근 1. AUDITORY APPROACH2. AUDITORY-SPECTROGRAPHIC APPROACH3. ACOUSTIC–PHONETIC APPROACH1. AUDITORY APPROACH이 접근법은 청각 음성학자에 의해 실행되며 증거 테이프와 예시의 상세한 기록물을 생성하는 것을 포함한다. 그들의 경험을 바탕으로, 전문가들은 음성 샘플을 듣고, 특이하거나 주목할 만한 목소리의 모든 측면을 감지하려고 시도한다.​한계 : 청각적 접근은 다른 접근법과 결합되지 않는 한 완전히 주관적이다.​* 청각적 특징의 비교를 바탕으로 법정에서 증거 진술(증거의 근거를 설명하는 공식 진술)을 제시 2. AUDITORY-SPECTROGRAPHIC APPROACHvoiceprint analysis라고 알려진 spectrographic  접근 방식은 음성 분광 프로그램(spectrograms)의 시각적 비교를 기반으로 한다.(음성 스펙트럼 분석)유사성 대 전형성을 이해하는 데 용이하도록 추가적인 배경 화자 스펙트럼도 포함한다.(음...UBM같은 느낌)​결론 : 청취 + 스펙트럼 시각적 비교로 화자식별 강화​한계 ​ - intraspeaker(동일한 화자로부터의 음성 변화)와 interspeaker(다른 화자로 인한 음성 변화) 변이를 구별할 수 있을지 모름. - 결국 검사자의 주관과 경험에 큰 영향을 받는다.(각 스펙트럼과 청각을 분석함에 있어 객관성 떨어짐) 3. ACOUSTIC–PHONETIC APPROACHacoustic phonetics = 음파의 전달과정을 물리적으로 연구하는 분야이 접근 방식은 음성 샘플의 정량적 음향 측정과 결과의 통계적 분석이 필요하다.  Acoustic features를 사용함(자동 알고리즘을 사용하여 음성 신호에서 파생된 수학적으로 정의된 매개 변수).우도 비율(LR) 방식을 적용할 수 있음. 따라서 가장 객관적인 방식임. 그러나 약간의 주관이 개입할 수 있음. -> 화자가 해당 순간 작용한 스트레스의 영향을 받았는지 안받았는지 검사자가 개입할 수 있음.​결론 : 3가지 방식 중 가장 객관적, 그러나 약간의 주관이 개입할 수도 있음 - 법원 증거 제출 불가  Naïve SPEAKER RECOGNITIO(일반(생물학적) 화자 인식)이 섹션에서는 일반 청자가 화자를 식별하는 방법과 현재 인간의 뇌에서 화자인식 과정에 대한 다양한 측면에 대해 논의한다. 1. IDENTIFY SPEECH SEGMENTS 2. SPEAKER RECOGNITION VERSUS DISCRIMINATION3. FAMILIARITY WITH LANGUAGE4. ABSTRACT REPRESENTATIONS OF SPEECH5. SPEAKER RECOGNITION IN THE BRAIN: FINAL REMARKS    1) STRENGTHS OF HUMAN LISTENERS    2) WEAKNESSES OF HUMAN LISTENERS1. IDENTIFY SPEECH SEGMENTS(음성식별)결론 : 인간의 뇌는 음성과 소리를 기본적으로 식별한다.(뇌의 활동량이 다름)          이때 인간이 내는 소리는 웃음과 기침을 포함하여 (음성이 있든 없든 상관없이) 음성으로 간주됨 왼쪽 일반 소리, 오른쪽 음성2. SPEAKER RECOGNITION VERSUS DISCRIMINATION(화자인식 vs 식별력)화자인식 : 익숙한 음성을 인식하는 것식별력 : 낯선 음성을 차별하는 것익숙함은 주관적인 조건이지만, 그 대상이 그 사람의 말을 듣는 데 얼마나 많은 시간을 할애했느냐에 따라 그 사람과 친숙해지는 것이 명백하다. 즉, 말하는 사람에 대한 친숙함은 듣는 사람이 관찰하는 음성 데이터의 양에 따라 달라진다. ​흥미롭게도, 사람에게 화자인식과 식별력은 별개의 인지 능력으로 알려져 있다.화자인식 -> 패턴 인식 작업식별력 -> 패턴 인식 + 음성 특징 분석(음성 판별력)​그러나 자동 화자인식에서는 거의 동일한 알고리즘을 적용한다. 이는 뒤 섹션에서 좀더 알아보자 3. FAMILIARITY WITH LANGUAGE(언어에 대한 친숙함)인간이 친숙하고 알려진 언어를 사용하는 사람을 더 잘 인식하는 것으로 관찰된다.그러므로, 인간의 음성 인식 능력은 특정 언어의 음운론에 대한 그들의 친숙함에 달려 있다. 4. ABSTRACT REPRESENTATIONS OF SPEECH(음성의 추상적 표현)인간의 뇌는 친숙한 음성에서 음성내용과 화자정보를 같이 추상적으로 형성한다.(친구의 소리를 들으면 발화 내용를 인지하고 친구의 정보를 같이 떠올리는 것)이를 통하여 여러 degradation(저하(소음 등))에서 효율성과 안정성을 확보가능. 화자가 말하는 것을 인지하는 것이 더 효율적이다.(에너지가 덜 필요)5. SPEAKER RECOGNITION IN THE BRAIN(뇌의 화자 인식 최종 결론)인간의 뇌는 복잡한 스펙트럼 및 시간적 소리를 처리하고, 음성 자극에 민감하며, 언어의 음운체계에 익숙하고, 소음 및 기타 저하에 강한 음성 및 화자 정보의 추상적인 표현을 구축한다. 이러한 능력의 대부분은 자동적이고, 특히 소음환경에서 강함을 보여준다. 따라서 이를 모방하는 것은 바람직함.그러나 기계에 사람을 익숙하게 만드는 것이 얼만큼의 데이터를 요구하는가에 대한 문제발생. 따라서 그대로 이를 자동 화자인식에 적용하는 것은 매우 어렵다.​강점 / 약점 분석[강점] - 인간은 화자인식이 어려운 상황에서도 놀라운 정확도로 인숙한 화자를 식별 - 인간은 화자의 목소리의 특이점을 잘 찾아낸다.​[약점] - 인간은 문맥적 편견에 민감하다.(순수 데이터로만 분석하지 않을 가능성) - 인간은 실수를 하기 쉽다. - 말하는 사람의 목소리를 오랫동안 기억할 수 없다. - 익숙한 음성이 비슷하면 헷갈릴 수 있다 - 목소리 사이의 미묘한 차이를 구별할 수 없다 - 시간이 지날수록 음성을 듣는 집중력 저하발생 - LR로 인한 음성 비교 결과는 일관되지 않을 수 있습니다.(전문가끼리 다른 결과) - 예상과 추론으로 인한 식별이 존재할 수 있음(순수 데이터로만 분석하지 않을 가능성)​  AUTOMATIC SPEAKER RECOGNITION인간의 최소 개입으로 독립적으로 작동하도록 설계된 컴퓨터 프로그램은 화자 음성을 식별합니다. 시스템 사용자는 설계 매개변수를 조정가능(최소 개입)논문에서는 문장독립과 화자검증에 집중한다.연구는 주로 화자인식평가(SRE)를 통해 NIST가 제시하는 표준화에 의해 주도된다.(NIST SRE)​ 자동 화자검증 구조1. 등록화자의 음성으로 특징을 추출하고 화자검증 모델 구현(등록 화자모델)2. background data로 등록 화자모델과 비교하며 구현3. 모델의 결과값을 얼마나 특징들이 비슷한지 점수로 반환4. 임계값으로 화자검증​ FEATURE PARAMETERS IN AUTOMATIC SPEAKERRECOGNITION SYSTEMS특징 파라메터1. VAD(음성과 아닌 것을 구분) : 음성 유무를 이진수로 표현   * 근데 요즘은 Combo-SAD(Speech Activity Detection) 사용 2. SHORT-TERM FEATURES ​    - 20-25ms 이내에 짧은 음성에서 추출한 매개변수    - MFCCs(Mel-frequency cepstral coefficients)와 LPC(linear predictive coding) 기반 매개변수 사용      * 아직도 현업에서 사용하는 기능임. 아래 링크에서 자세한 공부 MFCCsarticles about speech recognitionratsgo.github.io 3. FEATURE NORMALIZATION   - 특징값을 정규화(특정 범위안에 넣는것 혹은 편차를 똑같이)하는 것    - acoustic features의 특성 중 하나는 성능 저하에 대한 저항성이다    - In reality, it is not possible to design a feature parameter that will be absolutely unchanged in modified acoustic conditions and also provide meaningful speaker dependent information.(실제로 성능 저하와 음성을 완벽하게 분리하는 이상적 파타메터를 만드는 건 좀 힘듬)    - 그러나 아래와 같은 방법으로 이를 최소화시킴          feature-normalization techniques : cepstral mean subtraction, feature warping, relative spectra(RASTA) processing, and quantile-based cepstral normalization    - 일반적인 정규화 체계에는 형상 왜곡과 중심 평균 및 분산 정규화가 포함된다.(feature warping / cepstral mean / variance normalization) SPEAKER MODELING     - modeling = feature parameter 추출 이후 실시하는 단계    - 왜곡에 영향을 받지 않을 때 좋은 모델링이라고 할 수 있음.    - 이상적으로 다른 화자 변화(interspeaker discrimination)는 최대로 내부 화자 변화(intraspeaker        variation)는 최소로 하면 됨.    - 아래는 모델링의 발전에 대해 설명 1. GAUSSIAN-MIXTURE-MODEL-BASED METHOD2. ADAPTED GMMs: THE GMM–UBM SPEAKER-VERIFICATION SYSTEM3. THE GMM SUPERVECTORS4. GMM Supervector SVMs5. FA OF THE GMM SupervectorS6. LINEAR DISCRIMINANT ANALYSIS 7. NAP8. WCCN9. SPEAKER VERIFICATION USING i-VECTORS1,2번은 이전 논문 참고(최근 화자인식 기술동향)1. GAUSSIAN-MIXTURE-MODEL-BASED METHOD음성특징 파라메터(feature)의 likelihood를 비교하여 판단 - 확률적 성질 이용 머신러닝 - 수식 없이 이해하는 Gaussian Mixture Model (GMM)수식없이 이해하는 Gaussian Mixture Model /* 2017.8.4 by. 3개월 */ 개요 머신러닝에서 자주 사용되는 Gaussian Mixture Model(GMM)을 알아보겠습니다. GMM은 머신러닝에서 Unsupervised Learning(클러스터링)에..3months.tistory.com 2. ADAPTED GMMs: THE GMM–UBM SPEAKER-VERIFICATION SYSTEM이상적 GMM을 위해서는 음성 데이터가 충분히 많아야함. 이걸 보안하기 위해서 나온 방식. 배경화자들의 음성에서의 특징과 비교하여 결과 도출 - GMM보다 신뢰성 향상.​3. THE GMM SUPERVECTORS4. GMM Supervector SVMs3,4번은 아래 링크에 잘 설명되어 있음 [카카오AI리포트]카카오미니는 목소리를 어떻게 인식할까[카카오AI리포트] 김명재 | 카카오 음성처리파트 | 최근 음성인식의 성능이 많이 향상되면서, 음성이 친숙한 인터페이스로 자리잡고 있다. 음성에는 우리가 전하고자 하는 언어적 정보뿐만 아니라, 나이, 건강 상태, 감정 상태 등의 정보도 포함되어 있다.또한, 음성은 각사람의 고유한 정보를 담고 있어 이를 분석하면 목소리의 차이를 구별할 수 있다. 이런 정보를 분석하고 자동화하는 방법을 화자인식이라 부른다. 화자brunch.co.kr ​5. FA OF THE GMM SupervectorS.FA = 슈퍼벡터에서 화자정보와 채널정보를 분리하는 시도(결국 슈퍼벡터를 만든 여러 요인을 분석하는 것)i-vector접근법이 가장 최신 접근법임.(위 링크에서 좀더 자세히 설명)● Linear Distortion Model(선형 왜곡 모델)슈퍼벡터가 4가지 성분의 선형결합으로 이뤄진다 가정    - 화자/채널/환경 독립 요소 : UBM에서 얻음, 확정값    - 화자 의존 요소    - 채널/환경 의존 요소    - 잔여 요소위 요소가 아래 식으로 표현(순서대로임) ● Classical Map AdaptationMAP을 사용하면 화자 의존 요소와 화자 독립 요소를 GMM의 평균값 연산에 포함시킬 수 있음.결론 : MAP방식에도 FA로 해석할 수 있는 요소가 있다. 하지만 완벽하게 반영했다 보기는 어려움 ​● Eigenvoice(고유음성) AdaptationFA를 적용한 첫 번째 방식모델의 매개변수를 고유 음성 행렬 차원으로 제한시키는 것이 목적 Vy가 화자요인으로 알려진 standard normal hidden variable -> 화자의존요소 포함시킴.장점 : 데이터가 적어도 화자요인을 적용할 수 있음.(화자의 특성을 좀더 잘 찾는다)단점 : 채널이나 내부화자 가변성은 포함되지 않음​● Eigenchannel Adaptation(고유 채널 적용)위 방식과 마찬가지로 채널만 가지고 따로 슈퍼벡터를 만들어서 이를 결과슈퍼벡터에 더함. 따라서 연산에서 채널이 적용될 수 있음. ● Joint FA ● The i-Vector Approach <6~8 : 채널/세션 보상 방법>6. LINEAR DISCRIMINANT ANALYSIS데이터 분포를 학습해 결정경계(Decision boundary)를 만들어 데이터를 분류(classification)하는 모델데이터를 특정 한 축에 사영(projection)한 후에 두 범주를 잘 구분할 수 있는 직선을 찾는 걸 목표로 함. 선형판별분석(Linear Discriminant Analysis) · ratsgo's blog선형판별분석(Linear Discriminant Analysis) 21 Mar 2017 | Linear Discriminant Analysis 이번 포스팅에선 선형판별분석(Linear Discriminant Analysis : LDA) 에 대해서 살펴보고자 합니다. LDA는 데이터 분포를 학습해 결정경계(Decision boundary) 를 만들어 데이터를 분류(classification) 하는 모델입니다. 이번 글은 기본적으로 고려대 강필성 교수님 , 김성범 교수님 강의를 참고로 했음을 먼저 밝힙니다. 자, 그럼 시작하겠습니다. ...ratsgo.github.io 7. NAP(nuisance attribute projection)화자를 제외한 다른 변동성은 적다고 가정. 가정에 따라 특징 공간을 화자에만 의존하게 사영시킴.-> 화자를 기반으로 하는 특징 공간 완성​8. WCCNSVM기반 화자인식에서 견고성을 향상시키기 위해서 사용목표는 SVM 훈련에서 오답률(false-alarm and miss-error rates)을 최소화 하는데 있음 * 오답률은 2가지 지표로 표현된다(DET 커브)LDA와 NAP와는 대조적으로 WCCN 투영법은 특징 공간의 방향을 보존​9. SPEAKER VERIFICATION USING i-VECTORSi-벡터를 사용하는 classification 방법들을 설명1) svm Classifier2) Cosine Distance Scoring3) Probabilistic Linear Discriminant Analysis PERFORMANCE EVALUATION IN STANDARDIZED Data sets(성능평가)1. THE NIST SRE CHALLENGE2. TYPES OF ERRORS3. EQUAL ERROR RATE4. DETECTION COST FUNCTION5. DETECTION ERROR TRADEOFF CURVERECENT ADVANCEMENTS IN AUTOMATIC SPEAKER RECOGNITION(최근 발전)1. NIST i-VECTOR MACHINE-LEARNING CHALLENGE AND BACK-END PROCESSING2. DURATION VARIABILITY COMPENSATION3. DNN-BASED METHODSMAN VS MACHINE IN SPEAKER RECOGNITION최근에는 자동 화자인식이 사람보다 성능이 좋음. 앞으로의 방향성은 어떻게 인간이 자동 화자인식의 결정을 도울 수 있는 방법을 찾는 것이다.(ex 모델링 기법을 개발 등등)​평균적으로 기계가 더 잘하긴 하는데 이게 항상 사람보다 좋다고 할 수는 없음. 비언어적 요소(표정 등)가 중요한 경우 인간이 더 이해를 잘 함. 또한 인간이 친숙한 사람을 기계보다 잘 인식한다. 따라서 이런 사람이 더 잘하는 방식을 도입하기 위해서 노력해야 한다. 아래 표는 비교 결과를 요약한 것이다. 결론인간은 자신이 잘 아는 화자의 고유한 특성을 쉽게 식별하는데, 반면에 자동 시스템은 특징 매개변수를 적절하게 정의할 수 있는 경우에만 고유한 특성을 학습할 수 있다. 따라서 방해(저하) 요소를 억제하면서 화자식별에 관여하는 매개변수를 강조하는 표현방식을 찾는 것은 앞으로의 지속적 과제이다. "
AI디지털교과서 2025년부터 : AI Digital School Textbooks to Go into Use in 2025 ,https://blog.naver.com/lanxpert/223028590784,20230227,"​안녕하세요? 랭스퍼트입니다. 오늘은 며칠 전 네이버 뉴스 스탠드 영자 신문에 실린 기사 하나를 소개드립니다. 2025년부터 AI디지털교과서가 도입된다는 내용입니다. ​현재도 디지털교과서를 활용하고 있지만 AI디지털교과서는 AI에 기반을 두다 보니 당연히 현재의 디지털교과서와는 차별성이 있겠지요.  현재 이용중인 디지털교과서 - 과학, 사회, 영어 과목​그럼, 기사문을 살펴보겠습니다. ​​[Customized digital school textbooks to go into use in 2025]-개인별 맞춤 디지털 교과서 2025년부터 사용-The Korea Times Artificial intelligence-powered digital textbooks customized to different academic levels will go into use in 2025, beginning with junior elementary students and middle and high school freshmen, the education ministry said Thursday.2025학년부터 초등학교 3학년과 중1, 고1 학생들을 시작으로 학업 수준이 다른 학생들의 개인별 요구에 맞춘 AI(인공지능)디지털교과서가 사용될 것이다. ​customized 개개인의 요구에 맞춘go/come into use 쓰이게 되다 junior elementary student 초등학교 3학년  --- 고등학교나 대학교에서 junior는 3학년을 뜻하는 단어이지만 초등학교에서는 이렇게 잘 쓰지 않아요. ​​​ The plan aims to meet different educational needs of students across various academic levels by harnessing metaverse, AI or extended reality technologies amid shrinking school-age populations and technology advancement, the ministry said.교육부는 학령 인구가 감소하고 기술이 발달하는 가운데  메타버스, AI 혹은 확장 현실을 적용하여 학생 개인별 맞춤형 수업의 필요를 충족시키고자 이 방안을 내놓았다고 한다. ​harness (동력원 등으로) 이용/활용하다 school age 학령 extended reality 확장 현실(가상 현실, 증강 현실, 혼합 현실 등을 아우르는 개념)  extended reality (출처: unsplash.com)​​ Third- and fourth-year elementary school students and middle and high school freshmen will be the first beneficiaries of the envisioned digital textbooks to be used for mathematics, English and information courses from 2025. Through 2027, such textbooks will be expanded to students across all school levels.3~4학년과 중1, 고1 학생들은 2025년부터 수학, 영어, 정보 교과를 공부할 때 현재 구상중인 디지털 교과서를 사용하게 되는 첫번째 수혜자가 될 것이다. 2027년까지 AI디지털교과서 사용이 전 학생들에게 확대될 것이다. ​beneficiary 수혜자envisioned 구상중인(아직 구체화 안된) through (~을 포함하여) ~까지  예) We will be in Beijing Monday through Friday.  우리는 월요일부터 금요일까지 베이징에 있을 것이다. ​​​ Once completed, digital math textbooks will provide AI-guided tutoring to help students cope with difficult problems while English textbooks will utilize the speech recognition technology to help with listening and speaking exercises.완성이 되면, 디지털 수학 교과서는 학생들이 어려운 문제를 해결하도록 돕기 위해 AI가 안내하는 튜터링 기능을 제공하고, 영어 교과서는 음성 인식 기술을 활용해 듣기와 말하기 연습을 지원할 것이다. ​cope with 처리하다, 대처하다 utilize 이용/활용하다 speech recognition 음성 인식 ​ ​ An education ministry official said the ministry will also decide in May whether to expand digital textbooks to cover more subjects. By 2025, the ministry will also nurture up to 1,500 teachers specializing in education based on digital tools.또한 디지털교과서를 더 많은 교과목으로 확대할 것인지 5월에 결정할 것이라고 교육부 관계자는 말했다. 교육부는 또한 2025년까지 디지털 도구에 기반을 둔 교육을 전문으로 하는 교사1,500명을 양성할 것이다. ​official 관계자, 공무원, 관리 cover 다루다, 포함시키다 nurture 육성/양성하다 ​​​​ ​​​영어 음성 인식 기능은 현재 EBS에서 제공하는 AI펭톡에도 있고 기타 다른 앱에서도 많이 활용되고 있지만 수학의 튜터링 기능은 어떻게 적용될지 궁금해집니다. ​​※원문 링크https://www.koreatimes.co.kr/www/nation/2023/02/113_345980​​ "
"[인공지능] ""인공지능 기술의 적용 사례와 그에 따른 변화"" ",https://blog.naver.com/bbig3988/223087895913,20230428,"인공지능 기술의 적용 사례와 그에 따른 변화 ​ 인공지능(AI) 기술은 현재 산업부터 일상생활에 이르기까지 다양한 분야에서 적용되고 있습니다. 그리고 이러한 적용 사례들은 우리의 삶을 크게 변화시키고 있습니다. 다음은 인공지능 기술이 적용된 대표적인 사례들입니다.  ​음성 인식 ​음성 인식 기술은 최근 빠르게 발전하고 있습니다. 인공지능이 개발된 기술을 이용해 기계가 인간의 음성을 인식하고 이해할 수 있게 되었습니다. 이 기술은 음성 인식 스피커, 음성 인식 아이디, 음성 비서 등 다양하게 사용됩니다.  ​예시: ​```pythonimport speech_recognition as sr  ​r = sr.Recognizer()  ​with sr.Microphone() as source:    print(""무엇을 도와드릴까요?"")    audio = r.listen(source)  ​ try:    text = r.recognize_google(audio, language=""ko-KR"")    print(""당신: "" + text)except sr.UnknownValueError:    print(""죄송합니다. 제가 듣지 못했어요!"")except sr.WaitTimeoutError:    print(""시간이 초과되었습니다!"") ```  ​2. 이미지 인식 ​인공지능을 이용한 이미지 인식 기술은 컴퓨터 비전 분야에서 가장 중요한 기술 중 하나입니다. 이 기술을 이용해 컴퓨터가 사물을 인식하고 분류할 수 있게 됩니다.  ​예시: ​```pythonimport tensorflow as tffrom tensorflow import keras  ​(x train, y train), (x test, y test) = keras.datasets.cifar10.load_data()  ​class_names = [    ""비행기"",    ""자동차"",    ""새"",    ""고양이"",    ""사슴"",    ""개"",    ""개구리"",    ""말"",    ""배"",    ""트럭"",]  ​model = keras.models.load model(""my model.h5"")  ​predictions = model.predict(x_test[:5])  ​for i in range(5):    plt.imshow(x test[i])    plt.title(class names[predictions[i]])    plt.show()```  ​3. 자율 주행 기술 ​자율 주행 기술은 차량이 스스로 주행하는 기술입니다. 이를 위해 인공지능 기술이 사용되어 차량이 주행 도중에 상황을 판단하고, 신호를 처리할 수 있게 됩니다.  ​예시: ​```pythonimport carla  ​Connect to the simulator client = carla.Client(""localhost"", 2000)client.set_timeout(2.0)  ​Get the world ​world = client.get_world()  ​Spawn a vehicle ​vehicle bp = world.get blueprint library().find('vehicle.tesla.model3')vehicle transform = carla.Transform(carla.Location(x=0.0, y=0.0, z=2.0), carla.Rotation(yaw=0.0))vehicle = world.spawn actor(vehicle bp, vehicle_transform)  ​Enable the autopilot ​vehicle.set_autopilot(True)```  ​4. 자연어 처리 ​자연어 처리는 인간의 언어를 이해하고 처리하는 기술입니다. 이를 이용해 기계가 인간의 언어를 이해하고 이에 대한 답변을 전달할 수 있게 됩니다.  ​예시: ​```pythonimport openai  ​openai.api key = ""INSERT YOUR API KEY_HERE""  ​prompt = ""이상한 나라의 앨리스는 어떤 책인가요?""model = ""text-davinci-002""  ​response = openai.Completion.create(  engine=model, prompt=prompt, max_tokens=100)  ​print(response.choices[0].text.strip())```  ​위와 같은 인공지능 기술의 발전으로 우리의 삶은 큰 변화를 겪고 있습니다. 이제는 기계가 우리에게 제공하는 정보가 더욱 정확하고 빠르기 때문에 우리는 더욱 혁신적인 생각을 할 수 있게 되었습니다.  ​ ​ #인공지능 #머신러닝 #딥러닝 #자연어처리 #컴퓨터비전 #객체인식 #이미지분석 #감성분석 #음성인식 #자율주행 #로보틱스 #빅데이터 #클라우드컴퓨팅 #인터넷오브셋 #스마트팩토리 #스마트시티 #사물인터넷 #협업로봇 #금융서비스 #보안기술 #헬스케어 #가상현실 #확장현실 #순환신경망 #강화학습 #이메일마케팅 #카카오i #네이버클로바 #인공지능스피커 "
캐나다가 자율주행기술에서 앞서는 이유! ,https://blog.naver.com/insidecanada/221621312936,20190819,"안녕하세요 여러분!오늘은 4차 산업혁명의 핵심이라고 할 수 있는 자율주행 자동차에 대해 이해하고캐나다가 왜 자율주행기술 개발에 앞설 수 밖에 없는지 알아보도록 하겠습니다.​우선, ​자율주행 이란 운전자가 자동차의 핸들, 브레이크, 가속페달 등을 조작하지 않고도 자동차가 스스로 주변환경 및 차량 상태를 인식, 판단 제어하며 목적지까지 안전하게 주행하는 기술을 말합니다.​멀게만 느껴졌던 자율주행 기술이 점점 가속도를 내며 우리의 생활 근처까지 와있는데요. 이에, 세계 각국에서는 자율주행 기술개발의 주도권을 가져오기 위해 기술경쟁에 열을 올리고 있습니다. ​자율주행 기술을 선도하는 것은 미래 자동차 시장의새로운 플랫폼을 선점하는 것과 같기 때문입니다.​자율주행자동차는 인공지능(AI), 사물인터넷 (IoT) 등 다양한 분야와의 융합이 필수적이라 기존 자동차업계에서 주도하던 자동차 연구개발은 이제 정보기술 기업 혹은 자동차 부품기업에서 더 활발하게 연구 중입니다.​예를 들어, 캐나다 온타리오에 소재한 IT 기업인 Blackberry(블랙베리) 사는 자율주행자동차 플랫폼의 핵심인 QNX 오퍼레이팅 시스템을 개발, 포드, GM, 혼다, 토요타, 폭스바겐 등에 이를 공급하여1억 2000만대 이상의 차량에 Blackberry QNX 기술이 탑재되어 사용되고 있습니다. ​최근 LG 전자도 자율주행차 분야의 기술선도를 위해 블랙베리 사와 자동차용 소프트웨어 분야에서 기술협력을 맺기도 했습니다.​블랙베리 사 외에도 캐나다 온타리오 주에는 Google, Uber, GM, Magna, IBM 등 170개 이상의 기업들이 자율주행차 업계에서 활발하게 활동중입니다.   출처: 온타리오 주정부 이처럼, 글로벌 기업들이 캐나다, 특히 온타리오 주로 몰리는 이유는 자율주행 기술을 선도하고 관련 투자 및 일자리를 유치할 수 있는 독보적인 입지를 구축하고 있기 때문입니다. ​특히, 온타리오 주는 북미최대 자동차 생산 거점 중 하나이며 정보통신기술(ICT) 분야에 있어서는 미국 캘리포니아 주 다음으로 큰 규모를 자랑합니다. 또한 자율주행차의 핵심기술인 인공지능(AI)의 메카로 자리잡음으로써‘AI 대부’ 로 불리는 제프리 힌튼(Geoffrey Hinton) 교수를 배출한 토론토 대학 및 AI 연구소들과의 공동 연구를 위해 글로벌 기업들이 앞 다투어 캐나다로 몰려오고 있습니다. ​  출처: 온타리오 주정부 온타리오 주는 자율주행차량 테스트가 가능한 자율주행 혁신 네트워크(Autonomous Vehicle Innovation Network, AVIN) 시범존을 2017년 캐나다 최초로 오픈하였고, 최근에는 총면적 7.6km2 규모로 16km 의 실제 도로환경(신호등, 횡단보도 등)을 구현한 자율주행 테스트시설인 오타와 L5 를 오픈하여 실제 교통상황에서 기술개발 및 자율주행테스트가 가능하도록 하였습니다. ​이처럼, 미래형 자동차를 개발할 수 있는 완벽한 환경을 제공하고 있는 온타리오에서는글로벌 기업 외에도 다수의 유망한 스타트업 기업들이 무한한 잠재력을 갖고 활발히 활동 중입니다.​이에, 앞으로 활약이 기대되는 캐나다의 자율주행기술관련 스타트업 몇 곳을 소개하겠습니다.   ​핏스탑 (Pitstop)   토론토와 워털루를 기반으로 하는 핏스탑은 자율주행차의 예측정비(Predictive Maintenance) 에 인공지능을 활용하여, 다양한 종류의 자율주행차에 차량 상태에 대한 실시간 데이터를 제공한다. 최근 오토서비스월드(Auto Service World) 에 자동차 분야의 판도를 바꿀 ‘게임 체인저’ 로 소개되기도 했다.  다윈 AI (Darwin AI)   딥러닝 네트워크로 인해 컴퓨터 비전(computer vision, 컴퓨터를 사용해 인간의 시각적인 인식 능력을 재현하는 기술) 부터 음성인식(speech recognition)까지 다양한 분야를 자동차에 적용할 수 있게 되었다. 워털루 기반의 다윈 AI 는 워털루대학교의 교수들이 개발한 테크닉을 이용하여 인공신경망 (neural networks) 의 수행능력을 설명하고 최적화하는 것을 돕는다. 다윈 AI 는 오토모빌리티 LA 2018에 스타트업 상위 10곳 중 하나로 소개되기도 했다.  EVE (Evolved Vehicle Environments)   개인화(Personalization), 라이프스타일(Lifestyle), 연결성(Connectivity) 이 세 단어는 자동차 업계에서 사람들이 미래의 자동차를 어떻게 인식하는지 잘 보여준다. 오타와를 기반으로 하는 EVE 는 이미 테슬라, 스바루와 협력하여 고객이 차량의 인터페이스를 원하는 대로 바꿀 수 있도록 하고 있다.   팬토니엄 (Pantonium)   토론토를 기반으로 하는 펜토니엄은 대중교통 및 스마트 모빌리티의 경로를 자동으로 최적화하는 기술을 개발하여, 온타리오 벨빌 시에서 정해진 노선을 운행하는 것이 아닌 이용자의 요구에 따라 승객을 찾아가는 온디맨드식의 파일럿 프로젝트를 진행 중에 있다.   웨더텔레매틱스(Weather Telematics)​   북미 지역에는 약7천5백만명의 사람들이 겨울에 눈이 자주 내리는 곳에 살고 있으며, 이러한 지역에 운행하게 될 자율주행차는 눈, 빙판, 기타 여러 날씨변화에 대한 예측이 가능해야 한다. 오타와에 기반한 웨더텔레매틱스는 실시간으로 날씨와 트래픽 그리고 기타 데이터 등을 분석하여 차량에 맵을 기반으로 한 경고 및 알림을 제공한다.   온타리오의 자율주행차 산업에 대해 더 알고 싶으시다면 아래 영상을 확인해 보세요! 온타리오투자청(InvestinOntario) 뉴스레터를 구독하시면 자율주행자동차 등 캐나다 온타리오 주의 주요산업현황에 대한 업데이트를 받아보실 수 있습니다. ​구독! ↓↓↓ InvestinOntarioOntario, Canada's business environment offers growing companies key strategic advantages for global success.www.investinontario.com ​ "
네이버 클로바에 대해서 알아봐요 ! :)  ,https://blog.naver.com/ausiejulia/223104645008,20230517,"요즘 인공지능이라는 테마가 유행이죠!AI, GPT CHAT!! 우후!!!!!!​저도 종종 GPT CHAT에게 질문을 던져보는데.. 깜짝깜짝 놀랄 때가 많아요~~~ :)​심지어 코딩도 고쳐주기도 하구..한국말로 물어도 잘 대답하기도 하구..척척박사져!!​그뿐만 아니라 인공지능으로 아바타도 만들고.. 또 요즘엔 인플루언서가 인공지능으로 자신의 목소리를 이용해서 비즈니스를 해서 아주 성공적이란 기사도 봤구요.​인공지능프로그램을 이용해서 코인을 만들어서 상장을 한다거나..아주 무궁무진한 가능성을 보이는 분야.그리고 결국에 4차 산업혁명에 자율주행과 함께할 AI분야에 대해서 관심이 많은데국내에도 이용하기 편한 툴도 참 많답니다.​비서로 사용가능한 클로바를 소개해요!!​​ 네이버 클로바 용도음성 인식 비서 서비스지원 기기Android / iOS출시일2017년 5월 12일제작사NAVER Corp.웹 사이트[1]다운로드 ​클로바. 네잎클로버 같은 느낌이져!​나무위키에서 가져왔습니다..!!​​1. 개요[편집]  네이버의 인공지능 플랫폼 음성 인식 비서 서비스. LG U+의 주력 AI/IoT 서비스이기도 한다. Android 7.0 이상, iOS 14 이상2. 상세[편집]  네이버의 인공지능 플랫폼. 보통 클로바 스마트 스피커로만 알고 있으나, AI 스피커 이외에도 Clova Speech Recognition, Clova Speech Synthesis, Clova Face Recognition, Clova Premium Voice 등의 서비스를 제공하고 있다. (하단 AI Service 참조)​음성 인식 서비스의 경우에는, 대부분의 음성 인식 비서 서비스들과 비슷하게 자연어 처리, 음성 인식 처리에 딥러닝 기술을 사용한다. 특이하게도 iOS 11 이후의 Siri(영어권만)처럼 음성 합성 기술에도 머신러닝을 사용해 다른 비서보다 음성이 더 자연스럽고, 감정을 담은 것같다.​클로바를 부르려면 ""클로바"", ""샐리야"", ""피노키오"", ""제시카"" 중 선택해야 한다. 업데이트 이후 ""헤이 클로바""가 추가되었고 이를 권장한다.​음성을 통해 네이버의 검색 엔진을 사용하거나 외국어를 번역할 수 있으며 개인 일정(2.4 버전 이후 구글 캘린더 연동이 가능), 알람, 메모 등을 등록 및 질의할 수 있다.​네이버 VIBE, 지니뮤직, 벅스와 연동되어 음성으로 음악 추천을 요청하거나 특정한 음악의 제목을 말하면 자동으로 음악을 찾아서 재생한다. 국내 비서 플랫폼중 드물게 여러 음원 서비스를 이용 할 수 있다. * 바이브는 음성 인식 및 검색이 비교적 정확하나 벅스 연동시 음성으로 노래검색할려면 몸에 사리가 쌓인다.​화면 밝기나 음량과 같은 디바이스 설정의 제어 역시 가능한데, 하드웨어 제조사만큼의 권한을 가질 수 없으므로 제한이 있다.​현재 영어 프리토킹 모드는 앱에는 지원되지 않는다. 개선해서 재출시한다고 한다.​전체적으로 한국에서 정식으로 서비스중인 다른 인공지능 플랫폼들이 생활 편의성 기능만을 집중적으로 공략하는 반면, 클로바는 네이버가 장시간 서비스를 운영하면서 축적해 놓은 크고 아름다운 한국어 기반의 데이터들을 이용하고 있다는 것이 가장 큰 이점으로 평가되고 있다.​음성 변경은 프렌즈 스피커에서만 가능했었으나 2.14.0 버전부터 유인나 음성의 추가와 함께 앱과 스피커 모두 변경 가능하다.​Google Assistant, Alexa 처럼 모두에게 오픈된 확장기능(CEK, Clova Extension Kit)을 지원한다. Node.js기반으로 작성하여 서버를 운영해 클로바 콘솔과 연동하여 서비스를 제공할 수 있다.​클로바의 전신으로 네이버 링크가 존재했다. 2012년 서비스 시작되어 언제부터 구글 플레이에서 내려갔지만 내려간 후에도 apk를 설치하면 사용할 수 있었으나 클로바가 출시되자마자 정식으로 서비스 종료되어 서버 운영이 중단되었다.2.1. 클로바 웨이브[편집]  스마트 스피커.장점디자인과 스피커가 인공지능 스피커 중에는 괜찮은 편이다.무드등이 있으며 명령에 따라 반응한다.4개의 마이크가 있어 음성 인식이 잘되는 편이다.무지향성 스피커로 장소의 구애가 적은 편이다.단점휴대 가능한(?) 기기 치곤 전력 소모가 많은 편이다.전용 어댑터가 필요하다가끔씩 혼자 말한다.네이버 고객센터 대응이 별로다.오류부팅 후 어느정도 시간이 흐른 뒤 블루투스에 연결하여 음악을 재생하면 초기보다 음량이 많이 줄어드는 문제가 있다. 재부팅하면 다시 음량이 원래대로 복구된다. (2020.12.7 기준)2.2. 클로바 프렌즈[편집]  ​스마트 스피커.2.3. 클로바 데스크[편집]  2019년 3월 19일 일본에 출시된, 클로바를 탑재한 스마트 디스플레이 디바이스. 7인치 디스플레이를 탑재했다. 2021년 1월 기준으로 아직 일본에만 발매되었다.2.4. 클로바 클락[편집]  2020년에 출시된 시계 형태의 스피커이다. 리모컨 기능도 있다.이 기기에 U+ 기능이 추가된 클로바 클락+, 클로바 클락+2도 있다.[2]3. 여담[편집]  욕설 필터가 있다.[3][4] 이름에 자칫 욕설로 들릴 수 있는 단어가 포함된 사람에 대한 질문을 하면 이런 대답을 한다.(“듣고 싶지 않은 말이에요.”, “말은 그 사람의 인격이래요.” 등)고 대답을 한다. 정확히는 해당 인물의 프로필 전체를 요구할 경우 정상적인 대답이 돌아오지만 나이나 소속 등의 특정 정보 하나를 집어서 물어볼 경우 욕설 필터링에 걸려서 답변을 들을 수 없다. 이 때문에 인공지능 성능 자체가 떨어지는 것으로 생각한 사람들도 있는지 평이 그닥 좋지 못하다.특이하게도 노래를 틀어달라고 하면 VIBE에서 찾아서 들려주지만 노래를 불러달라고 하면 진짜 부른다(!!). 또한 동요를 불러달라 하면 곰세마리 같은 노래를 들려주고, 자장가를 불러달라 하면 반짝반짝 작은별 같은 노래를 직접 불러준다. 가수가 부른건지 인공지능이 직접 부른건지는 의문. 참고로 랩을 해달라 하면 자신의 기능들과 특징을 담은 가사의 랩을 한다. 심지어 비트박스를 부탁하면 비트박스도 직접 한다.2018년 4~5월까지 전국 영화관 광고시간 말미에 상영관 조명을 끄기 전에 스크린에 나왔다. 이때 대사는 ""클로바, 영화관 불꺼줘"", ""네, 영화관 조명을 끄겠습니다. 즐거운 영화관람 되세요.""이다.성별을 물어보면 자신을 만든 것은 염색체가 아니라고 대답한다. 결론은 무성(...).일본에서는 라인 브랜드로 출시되었으며, 2018년 7월 하순부터 스님들의 짤막한 법문을 들을 수 있는 서비스가 제공되어 화제가 되었다.(스마트스피커로 만나는 '3분 법문')Siri나 기가지니, 빅스비가 누구냐고 물어보면 비슷한 일을 하는 친구라 고충을 잘 알고 있어요 라고 대답한다.기가지니는 그 친구보다는 제 이름이 더 멋진것 같네요 라고 대답할 때도 있다.ok google이라 말하면 ""오 노. 낫 오케이. 아임 클로바""라고 대답한다.본인에 대한 험담(?) 을 하면 분명 그분은 진심이 아닐 거에요 라고 대답한다.아재개그가 점점 늘고있다. 기가지니를 물어보면 저를 지니고 그러면 안될지니라고 하고 시리야라고 하면 시리에게 가지 마시리나 괜시리 마음이 시리네요라고도 한다.지니야, 유튜브 켜줘라고 지시해도 유튜브 앱을 실행한다...4. AI Service[편집]  네이버 클로바 기술을 제공하는 네이버 클라우드 플랫폼​네이버 클로바가 개발한 기술들은 API의 형태로 이용가능하다. 네이버 클라우드 플랫폼에서 이용할 수 있는데, 이용할 수 있는 서비스는 아래와 같다. 4.1. Clova Speech Recognition (클로바 음성인식)[편집]  클로바 음성인식​사람의 목소리를 인식하여 작동하는 비서 애플리케이션, 챗봇, 음성 메모 등의 서비스를 만들 때 활용할 수 있는 음성 인식 API 서비스다. 음성 데이터는 API를 통해 Clova Speech Recognition(CSR) 엔진으로 전송되며, 해당 음성 데이터를 인식해서 텍스트로 변환하여 전달해준다. 텍스트 입력이나 물리적인 방법으로만 사용하던 서비스를 음성으로 컨트롤하는데 활용할 수 있다. 가전 제품 및 공동 주택의 홈 네트워크 제어, 배달 주문, 금융 서비스 등에 적용 가능하다. 모바일 SDK를 이용하여 스마트폰 애플리케이션으로 다양한 비서 서비스 등을 구현하는데 이용할 수 있으며, REST API를 이용하면 축적된 음성 데이터 파일을 텍스트로 변환하는데 활용할 수 있다.​현재 한국어, 일본어, 중국어, 영어 인식이 가능하다. 네이버에서 수년간 연구해온 결과로 국내에서 가장 뛰어난 한국어 인식률을 자랑한다. 사용 후기를 보면, 웹 기반의 콘솔에서 서비스를 제어할 수 있어서 편리하다고 한다.4.2. Clova Speech Face Recognition (클로바 얼굴 인식)[편집]  클로바 얼굴 인식​입력된 비전 데이터를 통해 얼굴을 인식하거나 얼굴 감지를 이용한 애플리케이션을 만들 때 유용한 API 서비스이다. 얼굴이 나온 사진을 이용하여 나이를 추측하거나, 닮은꼴 유명인 혹은 유사한 인종을 찾는 등 다양한 정보를 제공하는 서비스에 적용할 수 있다. 지속적인 학습을 통해 얼굴에 관련된 더욱 풍부한 정보를 제공한다. 이용요금은 서비스 이용횟수 기준으로 1,000건 단위로 부과된다. 4.3. Clova Voice (클로바 음성)[편집]  음성을 만들어내는 서비스이다.​아래의 기존 서비스들이 합쳐졌다.Clova Speech Synthesis 클로바 음성합성텍스트를 성우의 음성으로 자연스럽게 읽어주는 음성 합성 API이었다. 음성 안내 시스템, 뉴스/책 읽기 서비스 등에 활용할 수 있었다. 입력된 텍스트를 RESTful API 방식으로 전달하면 서버에서 인식해 mp3 포맷의 스트리밍 데이터나 파일로 리턴해주는 방식이었다. 요금은 1000글자당 4원이었다. 하지만 이후 CPV(Clova Premium Voice)와 함께 Clova Voice로 통합되었다. 기본 요금은 한 달에 90,000원으로 비싸다. ​* Clova Premium Voice 클로바 프리미엄 음성합성일반 음성 합성보다 더 자연스러운 음성합성 API이었다. Clova 의 기술로 같은 텍스트에도 기쁘거나 슬픈 감성이 반영된 다음 합성음을 사용할 수 있었다. 예를 들면, 엄격한 뉴스 앵커 스타일, 부드러운 친구 스타일, 담백한 일반인 스타일 등 다양한 스타일의 음성합성 이용이 가능했다. 클로바 음성합성과는 다르게 건당 5원이며, 한 건당 한 문장만 입력할 수 있었다.​하지만 클로바 음성합성과 마찬가지로 클로바 보이스로 통합되어 사라진 서비스이다. 4.4. Clova Note (클로바 노트)[편집]  긴 대화록을 문자로 바꾸어주는 speech-to-text(STT) 서비스이다. 서버에 음성 파일을 업로드해서 변환하는 방식이며 음성 파일 한개당 최대 180분까지 변환 가능하다. 이미 녹음되있는 음성 파일을 업로드할 수도 있고 직접 앱으로 녹음한 뒤 업로드할 수도 있다. 생각보다 정확도가 높고 변환 시간도 빠른 편이라 STT서비스 중에서 꽤나 우수한 성능을 보여준다. 게다가 단순 변환만 하는게 아니라 서로 다른 사람의 목소리를 구분할 수 있어서 회의같은 여러 사람끼리의 대화도 사람별로 정확하게 구분해서 나눠서 기록해준다.​현재 베타 서비스라 별도의 과금시스템은 없고 한 명의의 계정 당 매달 기본 300분에 최대 600분+@[5][6]의 이용시간이 제공된다. 이용시간은 최초 가입한 날로부터 30일마다 갱신되며 서버에 업로드한 음성 파일의 길이에 따라 이용시간이 차감되는 방식이다. 이용시간을 모두 소모하면 갱신 전 까지 추가 업로드가 불가능해진다.​​​​ "
"ChatGPT, I don't see the limit of your potential. ",https://blog.naver.com/jeannie090802/223105786467,20230518,"​   Picture this: you're sitting at your desk, pondering a question that's been on your mind for days. You turn to your computer and start typing, but instead of searching Google, you're having a conversation with an AI chatbot. This may seem like a futuristic scenario, but it's already happening today with chatbots like ChatGPT. Those who invariably doubt the potential of AI technology are considered outliers; and here is an opportune evidence for it: this wondrous opening was generated by ChatGPT in less than five seconds. AI chatbots are subject to never-ending controversies, yet their potential outweighs the currently existing debates about its limits. Technology will constantly advance; and so will the accuracy and usefulness of AI chatbots. ​          The unique characteristics of AI chatbots, such as their efficacious method of responding to requests will allow people to apply them to real-life situations. Currently, people are in desperate need of efficient programs that can be their assistants, especially if those programs have an endless database, impeccable logic, and efficient problem-solving process. AI chatbots might be the closest that humans can get to formulating such an assistant that can support us behind the scenes. AI chatbots work effectively because they can gather relevant information from their databases in a blink of an eye. When an individual suggests a topic, AI chatbots immediately dive into their knowledge banks to find research that can benefit the individuals and release them. People can apply this into their lives by utilizing AI chatbots for tasks that do not require personal opinions but are rather of a lower level. ​Moreover, unlike humans, AI chatbots do not have any time limits – they do not get emotionally stressed, overworked, or unavailable due to health conditions. Clients can use them anywhere and at any given time because AI usage is not hindered by anything except an accessibility to wifi. In addition, unlike human assistants, AIs can handle diverse issues at the same time because they are several branches of a trunk that contains the database. A child in the United States can access the chatbot while a grown-up in Nigeria is using it as well! ​         Another characteristic of AI chatbots is that they learn from their users. Chatbots obtain information such as how humans naturally use language, updated information about global issues, and how to deal with biased individuals based on past experiences. Self-learning is a stepping stone for an advanced Artificial General Intelliegence system that does not require Through these experiences, AI chatbots have room to prosper limitlessly and prove their potential through implications in real life situations. ​          The field that will have its capabilities increased to the greatest degree by AI chatbots is arguably education. Educational experts habitually compare the impact of AI chatbots in education to the impacts of calculators. Calculators aid students in basic and repetitive processes that are not necessary for advancing one’s academic abilities; instead, they are utilized for convenience and free up time for studying more complex fields and solving complicated problems that involve multiple layers of thought processes. AI chatbots can assist students just like calculators - students would not be entirely dependent on the chatbots for their assignments, but using them will either reduce the amount of time that is wasted on mundane tasks, or will act as a stepping stone for building up comfort with idiosyncratic ideas. ​In particular, students often utilize ChatGPT for researching, a process which often takes up a considerable amount of time when writing persuasive or descriptive essays. Through the usage of ChatGPT, students can easily access studies or texts written by professionals. Such texts often involve advanced vocabularies and professional background knowledge, which can be difficult for students. AI chatbots can not only summarize such articles, but can also transform the alien language into a more easily accessible style, based on a student’s skills. AI chatbots can also list some credible research regarding certain topics for students to independently review and analyze. The tedious research processes without any assistance have led to students getting overworked due to the time and effort that is required to constantly repeat the same process; some even refuse to complete any assignments that involve such dedication. AI chatbots can act as an opportune solution for these issues because they free up more time for students to complete parts of projects that are less stressful and are rather engaging, such as writing a creative essay or drawing pictures based on the solid research carried out by the chatbot. ​Furthermore, AI chatbots can act as teaching assistants within the classroom. These days, teachers juggle lots of work, including scoring students’ assignments, providing individual feedback for students, and teaching countless classes. AI chatbots cannot enable teachers to be completely free, due to AI chatbots’ limits that they can only exist online, have a lack of emotional maturity, and cannot quickly adapt to emergency situations. However, AI chatbots can generate scores and feedback for students’ work through receiving training from teachers on the standards of grading and the criteria expected for high-level projects. When AI chatbots generate scores in a matter of seconds, teachers will have more spare time to focus on planning lessons and engaging activities, as well as supporting students individually. AI-based scoring is also beneficial for students because they can receive objective grades that are not affected by the personal biases that some students suspect their teachers to have. These positive implications of AI chatbots in education can open up numerous new opportunities for enhancing the quality of learning, taking quality education to another level and making it accessible to more and more students in the near future. ​          Another field that would be ideal for AI chatbots is supporting people with disabilities. Disabilities often hinder people’s ability to function within a society, simply because today’s society has insufficient programs to aid those in need. The existing products, such as magnifying glasses for people with low vision, or  braille for blind individuals, do benefit them in their lives but still cannot allow all disabled people to fully function in a society. AI chatbots possess a massive potential to aid disabled people to live in a similar way to those who do not have disabilities. AI chatbots can enable blind individuals to “read the world” by verbally informing them about their current location based on what is seen through a smartphone camera. AI chatbots are excellent at identifying and describing things, so if one could be connected to a smartphone camera, it could elucidate the view to the individual in real-time by describing the weather, shops, or the people who are around and what they are doing. ​The distinction between a smartphone app that provides verbal description and an online chatbot lies in chatbots’ capability to provide vivid descriptions of the environment that a blind person is standing in, instead of simply informing them about the names of the nearby stores. The limitation of diverse applications that are available in the market is that they might guide blind individuals to certain locations, but the systems do not consider other factors such as friends of the blind individual passing by, or a particular store or bakery that the individual loves to visit. However, AI chatbots can utilize their facial recognition features to inform the disabled individuals about the identities of passers by, which might include friends, family, or teachers. Also, with the database that the AI possesses, it can act as the individual’s ‘eyes’ to recommend different restaurants and bakeries nearby based on the individual’s tastes. This can potentially provide enhanced mobility for blind individuals to aid them in freely navigating and communicating with the world around them without the assistance of service dogs or family members. ​AI chatbots can additionally assist deaf people through their speech recognition abilities. Currently, sign language is the most common tool for communication among deaf people, because of the myriad limitations of previous speech recognition programs. Speech recognition programs that are utilized by people without disabilities do not require 100% accurate recognition skills because people can manually correct the errors in the recognition. On the other hand, quick recognition and high degrees of accuracy are essential for deaf people because they must be able to obtain real-time subtitles while people are speaking. AI chatbots can be an apt solution for this issue because they are capable of real-time comprehension and correction of people’s statements, and can formulate full sentences based on their interpretations. These full sentences could then be transcribed and displayed as subtitles on a smartphone screen for the deaf person to read. ​“Artificial intelligence is not just a technology, it is a mindset shift. Its potential lies not in replacing human intelligence, but in enhancing it to solve problems that were once thought impossible,” claims ChatGPT, when asked about itself. ChatGPT’s assessment of itself seems fair; it does possess endless potential as an inventive technology that can resolve global issues in the most efficacious manner possible. It has potential in a  wide range of areas, mainly regarding its capabilities of contributing to the educational field and the support that it can provide to disabled individuals. Experts concede that there are limits and potential harms, yet there were also limitations and risks with our most innovative technologies, such as smartphones and the internet. It is our responsibility to utilize AI chatbots in a sustainable and safe way to positively benefit our community.  "
Y-combinator가 22년에 투자한 인공지능 회사들 (YC 22 Winter Batch)  ,https://blog.naver.com/bizucafe/223055942526,20230326,"인공지능이 정말로 뜨겁다. 특히 3월은 절정에 있는 것 같다. 지금부터 3개월에서 6개월이 지나면 어떤 이야기들을 할까? 계속 인공지능 이야기를 하고 있을 수도 있고, 어쩌면 인공지능 아닌 다른 이야기를 하고 있을지도 모르겠다고 생각하지만... 지금 moment에서 많은 것들이 바뀌고 있고 (그동안 쌓여왔기 때문에), 그리고 사람들이 앞으로 이 기술이 어떻게 실제로 사람들에게 영향을 주는 제품으로 변화할지 관심을 갖고 있는 것 같다. 당연히 정답은 없겠으나, 이미 이에 선제적으로 투자한 회사들을 보면 조금이나마 힌트가 생길 수도 있지 않을까 생각하며, 2022년 윈터 시즌에 Y-combinator에서 투자한 신규 스타트업들 중 인공지능을 주제로 한 14개의 팀을 짧게나마 노트로 정리해뒀다. 각각의 회사들이 풀고자 하는 문제는 당연히 다르고, 이에 있어서 그들이 푸는 문제가 꼭 정답이라고 생각하지는 않지만 분명 새로운 문제를 발견하는데 좋은 지침이 될 것이라고 생각하며..  ​* 정리된 순서는 아무런 의미가 없다. Y-combinator 홈페이지의 배치 정보를 기반으로 매뉴얼로 정리했다.   1. Whitelab Genomics​인공지능 기반으로 신약 개발에 있어 솔루션 제공하는 업체, 홈페이지에 들어가 보면 Meta(facebook), Tesla, Lyft 출신들이 나와 창업한 것으로 보임. 실험 설계 워크플로우에 대한 서비스도 제공하는 것으로 보이는데 그것이 본질로 보이지 않음. 그들이 풀고자 하는 문제는 어떻게 더 효과적인 약을 기존 프로세스보다 더 개선할 수 있을까 (인공지능 기반으로)에 있음. ​ Gene and Cell Therapy | AI Software Solutions | WhiteLab GenomicsAt WhiteLab Genomics we develop state of the art software solutions using AI in order to accelerate and de-risk gene and cell therapy developments.whitelabgx.com 2. Starling​하드웨어가 흥미로운 회사. 홈페이지에 들어가면 ""What information did you just flush down the toilet?”이라는 말이 중심에 들어가 있음. 화장실 내에서 소변과 대변 등에 대한 정보들을 홈 모니터링으로 점검할 수 있는 등 하드웨어와 서비스를 만드는 것으로 판단됨. 신약 개발보다는 사전적인 조기 진단에 초점을 둔 회사. ​ Starling Medical – UrinControl of your HealthUrinDx at a glance we are aiming to improve the outcomes ofstarlingmedical.com 3. Sieve Data​비디오 분석 서비스. 비디오에 있는 메타데이터들을 뽑아내서 여러 가지 용도로 쓸 수 있게 템플릿을 주어 업무를 돕는다는 설명이 있음. ""비디오만 넘기세요, 비디오 메타데이터를 뽑아드릴게요"" 같은 느낌. 정확한 유스 케이스는 모르겠으나, 홈페이지 상 아래와 같은 방식들로 사용할 수 있다고 함. 아주 쉽게 생각할 수 있는 정도로는 CCTV 분석을 사람 눈으로 하는 게 아니라 영상 돌리면 언제 문제 터졌는지 (예 : 블랙박스 같은 것) 그냥 검수해 주는 정도로 이해할 수도 있겠으나, 활용성은 당연히 상상 그 이상일 수 있다고 생각이 듦.  Sieve: Ship powerful AI features, fast.Ship powerful AI features , fast . Stop worrying about complex infrastructure and broken tools. Leverage the latest and greatest models, while collaborating with your entire team. Get Started for Free Book a Demo How Sieve Works New features in minutes The AI space is moving so fast, it's hard to ke...www.sievedata.com 4. DynamoFL​""The #1 Solution for Optimized Model Training"" : Federated learning이라는 기술을 디플로이해서 개발하는 과정에서 도움을 주는 솔루션으로 보임. ""3분 안에 우리가 가진 모델을 너가 사용하는 ML 워크플로우에 연결할 수 있다""라는 이야기를 함. 전공자가 아니라서 구체적으로 어떤 솔루션인지 모르겠지만, 개발단에서 도움이 필요한 서비스로 판단됨. ​ DynamoFLThe #1 Solution For Optimized Model Training. DynamoFL delivers state-of-the-art and regulation-compliant AI at a fraction of the cost.www.dynamofl.com 5. Speechly​다른 회사들과 다른 점은 2016년 설립된 회사라는 점이 돋보임. 그리고 북미 회사 아니라 핀란드 회사라는 점도. 'Superfast Speech Recognition' 음성인식만 깊게 파는 회사. 지금은 텍스트만 새로 만들 수 있어도 어마어마한 파장인데, 음성까지 녹음을 떠두고 할 수 있다면 이제 많은 성우들은 어떻게 되는 걸까 생각이 들기도 하고, 특히나 드라마 제작이나 영상 엔터테인먼트 제작에서 기본 데이터만 있으면 수많은 제작에 들어가는 과정을 자동화(까지는 아니더라도), 효율화 시킬 수 있지 않을까 생각하게 됨. 이미 구글 홈, 아마존 에코 같은(알렉사) 서비스들이 몇 년 전부터 하드웨어 보급하면서 하던 일들. 다만, 전공자가 아니라서 어떤 기술이 어떻게 다른지 구체적으로 현재로서 알 수 없음이 아쉽다. ​ Superfast Speech Recognition | SpeechlyOn device, on premise or in the cloud. Accurate, private and cost-efficient by design.www.speechly.com 6. Voize​독일 회사. 홈페이지가 독어로 되어있어서 많은 것들을 정확하게 알 수 없었지만, 큰 틀에서 음성 데이터를 기반으로 의료 영역 버티컬에서 솔루션을 만들고 있는 회사로 판단됨. 의료인들이 만드는 음성 데이터들을 문서작업으로 전환해 주는 작업 (의료 버티컬에 특화된 클로버 노트 같은 서비스라고 생각하고 있음) 특히나 의료 영역에서만 사용하는 단어들이 있고, 이 단어들은 하나하나가 정확하지 않으면 많은 판단의 오류를 낼 수 있기 때문에 더 중요한 것일까? 이런 영역이 있다면 또 어떤 영역이 있을까? 라이센스 있는 영역들도 다 비슷한 상황이지 않을지? 그러면 그 영역에 기회가 있다는 것일까? ​ voize | Pflegedokumentation per SpracheingabeMit voize können Pflegekräfte Dokumentation frei am Smartphone einsprechen. voize generiert automatisch strukturierte Dokumentationseinträge.www.voize.de 7. Mintlify​개발자들이 코딩을 문서화할 때, 이 문서화를 효율적으로 도와주는 서비스로 보임. 자신들을 'automatic code documentation engine'이라고 했는데, 솔루션이 아니라 엔진이라는 표현을 쓴 것이 인상적이었음. 개발자들이 만드는 데 더 집중해야 하고, 그것들을 도큐멘테이션 하는 과정을 자동으로 해준다는 것. ​ Mintlify - Beautiful documentation that converts usersDocumentation Showcase Pricing Careers Writer Login Get Started Switch theme Beautiful documentation that converts users Build the documentation you've always wanted. Beautiful out of the box, easy to maintain, and optimized for user engagement. Developer first Content is powered by MDX and lives al...mintlify.com 8. Mutable AI​개발자들을 위한 툴. Python, Jupyter로 개발하는 사람들 위해서 돌리는 polishing engine이라고 함. ""Mutable AI does this automatically, autocompleting, refactoring and minimizing quickly and easily."" 이런 것들을 한다고 쓰여 있음. ​ mutable.ai. AI Accelerated Software Development.Build fast with production quality using AI.mutable.ai 9. KorrAI​인공위성으로 지형지물 데이터들을 뽑아서 인프라 관리하는 것의 효율을 높여준다는 엄청나게 이해하기 어려운 비즈니스를 하는 회사. 아래 사진이 가장 그나마 이해하기 쉬운 것 같음. 특히 광물 채굴에 포커스가 되어있는데, 인공위성으로 지리적, 공간적 데이터를 모아서 얼마나 자원이 있는지 등 확인하고 실제로 효율적으로 채굴할 수 있게 돕는 그러한 일을 하는 것 같음. 1년 걸리는 이리 5주로 단축되었다고 하는데, 모든 케이스가 그러하지는 않겠지만... 세상은 참 많은 비즈니스가 있구나 생각하게 됨. ​ KorrAI - Infrastructure & remote ground monitoring, powered by satellites & IoTInfrastructure & remote ground monitoring, powered by satellites & IoTwww.korrai.com 10. Reality Defender​페이크로 만들어진 컨텐츠를 검수할 수 있는 도구. 인공지능이 컨텐츠들 뽑아내는데 가짜 뉴스부터 시작해서, 그 외 수많은 컨텐츠들이 진짜인지 가짜인지 검사할 수 있는 하나의 검수 툴. 창이 인공지능이라면 방패 역할을 하는 서비스. ​ Reality Defender — Enterprise-Grade Deepfake DetectionReality Defender is deepfake detection done right. With billions of visual and audio assets indexed, we offer the most robust deepfake detection available.realitydefender.com 11. Strong Compute북미 회사가 아닌 호주 회사. 오스트레일리아에 기반하고 있음. ""We're building the Future of Cloud."", ""Deep learning engineers are probably the most precious resource on this planet, Strong Compute has enabled our engineers to be 10x more productive."" 이런 캐치프레이즈를 갖고 있음. 더 효율적인 모델로 학습 시간을 엄청나게 개선하게 돕는다는 그런 비즈니스.​ Strong ComputeFast compute for AI. 10x-1000x faster training. Speed up your‍ neural net training today.strongcompute.com 12. Powerhouse AI싱가포르 베이스의 회사. 재고관리를 위한 솔루션. 직원들이 사진을 찍고, 소프트웨어에 올리면, 소프트웨어가 알아서 상자 수를 세주고 재고 관리를 해주는 것. 사람이 하나하나 관리하는 그러한 일들을 소프트웨어로 대체. 정확도가 굉장히 높다고 함. ​ Powerhouse AI - Inventory Counting in SecondsAccurately verify and (cycle) count your pallets and boxes up to 5x faster with the help of our visual AI-enabled app, Count&Go. The fastest growing solution.www.pwh.ai ​13. AiSupervision""Track / Manage / Improve / Automate - production in factories with many human workers."" 공장 생산 과정에서 돌아가는 모든 업무의 효율성을 개선할 수 있는 솔루션. 단순 업무 트래킹 넘어서 어떻게 모니터링하고, 생산 효율 최적화할 수 있을지 솔루션 내는 등 여러 가지 일을 제안하고 있음. 아마존이나 쿠팡에서 물류창고 홍보할 때 (물론 생산직은 아니지만), 엄청나게 미래적인 로봇들 움직이면서 사람이랑 효율성 내는 것들 생각할 수 있는데, 모든 공장이 이렇게 된다면? ​ AiSupervision - Operating system for factory production lines (YC W22)AISuperVision is a platform to track, manage and improve production in factories with many human workers. We automate what the best human supervisor would do if they were everywhere in the factory.www.aisupervision.com 14. Eventual​많은 회사들이 풀려고 하는 문제 : 데이터를 수집하고, 정리하고, 효율적으로 학습시키는 모든 파이프라인을 다 도와주겠다고 하는 서비스. ​ Eventual: The Data Warehouse for Imaging.Eventual provides managed infrastructure for all your compute, storage and data science environment needs, including managed DaFt deployments that can scale to any size your organization requires.www.eventualcomputing.com  미국에서는 이런 회사들에 투자를 하고 있지만, 그 회사들이 모두 미국 회사가 아니라는 점도 흥미롭고 무엇보다도 프로세스 자체에 대한 솔루션을 내는 회사들이 많다는 점이 인상 깊었다. 동시에 전공자가 아니기 때문에 이것들이 정말로 도움이 되는 솔루션인지, 그렇다면 다른 서비스들과 어떻게 접근이 다르고, 어떻게 결과가 다른지 등을 이해할 수 없다는 점이 아쉬웠다. 장기적으로 인프라 레벨의 서비스들은 이해 자체가 불가하면 '내 영역'이 아니라고 포기하는 것이 나에게도 맞는 것일까, 그렇지 않다면 해당 영역에 대한 이해력을 장기적으로 높여가기 위해서 어떤 시간들을 보내야 할까에 대한 스스로 반성의 시간도 갖게 되었고... 무엇보다 이런 투자들이 단기적으로 보았을 때 엄청난 break-through 만들어서 내일 당장 미래가 바뀌지 않겠지만, 하나하나 쌓이다 보면 인프라가 되어 많은 것들을 당연하게 받아들이게 되는 시대가 올 것이라 생각한다. (아이폰이 애플 혼자 개발한 것이 아니라 그 이전의 기술들이 누적되어 나올 수 있었던 것처럼) 모든 것들을 보려고 하는 것이 아니라... 전체 흐름 중에서 내가 할 수 있는 것, 내가 이해할 수 있는 것, 내가 판단할 수 있는 것, 그렇지 않은 것을 구분할 수 있어야 한다는 생각이 많이 든다.  "
TIL_0609_ML_basic ,https://blog.naver.com/mm323/222766184348,20220609,"Machine Learning SummaryLinear regression is a widely used model. The optimal parameters of a linear regression model can be calculated exactly, or can be found by gradient descent search, which is a technique that can be applied to models that do not have a closer-form solution.A linear classifier with a hard threshold -also known as a perceptron - can be trained by a simple weight update rule to fit data that are linearly separable. In other cases, the rule fails to converge.Logistic regression replaces the perceptron's hard threshold with a soft threshold defined by a logistic function. Gradient descent works well even for noisy data that are not linearly separable.Nonparametric models use all the data to make each prediction, rather than trying to summarize the data with a few parameters. Examples include nearest neighbors and locally weighted regression.Support vector machines find linear separators with maximum margin to improve the generalization performanace of the classifier. Kernel methods implicitly transform the input data into a high-dimensional space where a linear separator may exist, even if the original data are nonseparable.Ensemble methods such as bagging and boosting often perform better than indivisual methods. In online learning we can aggregate the opinions of experts to come arbitrarily close to the best expert's performance, even when the distribution of the data are constantly shifting.​ Deep Learning SummaryMethods for learning functions represented by deep computational graphsNeural networks represent complex nonlinear functions with a network of parameterized linear-threshold units.The back-propagation algorithm implements a gradient descent in parameter space to minimize the loss functionDeep learning works well for visual object recognition, speech recognition, natural language processing, and reinforcement learning in complex environments.Convolutional networks are particularly well suited for image processing and other tasks where the data have a grid topology.Recurrent networks are effective for sequence-processing tasks including language modeling and machine translation​https://github.com/yskim5892/AI_Expert_2022 GitHub - yskim5892/AI_Expert_2022Contribute to yskim5892/AI_Expert_2022 development by creating an account on GitHub.github.com MLCC RepresentationFeature EngineeringQualities of Good FeaturesCleaning Data "
[강의] 특이점이 온다(Ray Kurzweil 레이 커즈와일) ,https://blog.naver.com/dependentorigination/223055434382,20230325,"​​  인간을 능가하는 인공지능, ‘특이점’의 시대?- 자료출처: 내셔널지오그래픽 National Geographic Korea(2017년 9월 8일)​​인간은 오랫동안 모든 면에서 인공지능을 능가해왔지만 최근에는 그 격차가 줄어들고 있습니다.머지않아 인공지능이 인간을 압도하게 될지도 모르죠. 미래학자들은 초지능형 기계가 지구를 지배하며, 인류의 통제를 벗어나기 시작하는 ‘특이점’에 도달할 것이라 생각합니다.​​<이어 밀리언: 인류의 미래>새로운 시대, 변화하는 인류​Year Million  EPISODES1부. A.I.의 지배2부. 불멸의 삶을 살다3부. 현실의 경계를 허물다4부. 텔레파시로 통하다5부. 새로운 행성을 찾아서6부. 시간과 공간의 초월​ www.natgeokorea.com​​​  ​​​  하이브리드 사고에 대비(레이 커즈와일)Ray Kurzweil: Get ready for hybrid thinking- 자료출처: TED(2014년 6월 3일)​Two hundred million years ago, our mammal ancestors developed a new brain feature: the neocortex. This stamp-sized piece of tissue (wrapped around a brain the size of a walnut) is the key to what humanity has become. Now, futurist Ray Kurzweil suggests, we should get ready for the next big leap in brain power, as we tap into the computing power in the cloud.​2억년 전에, 우리의 포유류 조상들은 새로운 뇌의 특징인 신피질을 개발했다. 이 우표 크기의 조직 조각(호두 크기의 뇌에 감긴 것)이 인류가 무엇이 되었는지에 대한 열쇠이다. 미래학자 Ray Kurzweil은 클라우드의 컴퓨팅 능력을 활용할 때 두뇌 파워의 다음 큰 도약을 준비해야 한다고 제안한다.​​​  ​​​  다가오는 특이점(레이 커즈와일)Ray Kurzweil: The Coming Singularity - 자료출처: Big Think(2009년 4월 29일)​​RAY KURZWEILRaymond “Ray” Kurzweil (born 1948) is an American inventor and futurist. He is involved in fields as diverse as optical character recognition (OCR), text-to-speech synthesis, speech recognition technology, and electronic keyboard instruments. He is the author of several books on health, artificial intelligence (AI), transhumanism, the technological singularity, and futurism.He has received nineteen honorary Doctorates and honors from three U.S. presidents.Ray has written six books, four of which have been national best sellers.  The Age of Spiritual Machines has been translated into 9 languages and was the #1 best selling book on Amazon in science.  Ray’s latest book, The Singularity is Near, was a New York Times best seller, and has been the #1 book on Amazon in both science and philosophy.​레이 커즈와일레이먼드 커즈와일(1948년)은 미국의 발명가이자 미래학자이다. 그는 광학 문자 인식(OCR), 텍스트 음성 합성, 음성 인식 기술 및 전자 키보드 악기와 같은 다양한 분야에 관여하고 있다. 건강, 인공지능(AI), 트랜스휴머니즘, 기술적 특이성, 그리고 미래주의에 관한 여러 책의 저자이다.그는 세 명의 미국 대통령으로부터 19개의 명예 박사 학위와 명예를 받았다.레이는 6권의 책을 썼고, 그 중 4권은 전국적인 베스트셀러였다. 영적 기계의 시대는 9개 언어로 번역되었고 과학 분야에서 아마존에서 가장 많이 팔린 책 1위였다. 레이의 최근 저서인 <특이점은 가까이에 있다>는 뉴욕 타임즈 베스트셀러였고, 과학과 철학 모두에서 아마존 최고의 책이다.​​​  ​​​  ​The Singularity Is Near: When Humans Transcend BiologyRay Kurzweil Product detailsPublisher: Duckworth(February 11, 2010)Language: English  기술이 인간을 초월하는 순간 특이점이 온다레이 커즈와일, 김영사, 2007.​이것이 바로 우리의 미래다! 세계적 미래학자이자 사상가인 커즈와일이 예측하는 인류-기계 문명의 미래!2005년 <뉴욕 타임스> 발표 '미국에서 가장 많이 블로깅된 책 13위'의 『특이점이 온다』는 과학기술 발전으로 생물학적 인간의 조건을 뛰어넘는 미래 인류의 모습을 전망한다. 기술이 인간을 넘어 새로운 문명을 낳는 시점을 뜻하는 ‘특이점’이라는 개념을 강조하고 어떻게 활용할지, 대처하기 위한 방법은 어떤 것들이 있는지 소개한다.《특이점이 온다》는 특이점이 나타날 시기와 각종 기술 진화에 따른 변화와 그에 따른 혁명과 특이점이라는 변화가 인간과 전쟁, 우주의 지적 운명에 미칠 영향, 저자의 주장을 비판하는 내용들에 대한 반론 등으로 구성했다.(교보문고)​저자 레이 커즈와일(RAY KURZWEIL)선구적인 발명가이자 사상가, 미래학자로서, 지난 20년간 그가 수행한 미래 예측은 굉장한 정확도를 보였다. <월 스트리트 저널>은 커즈와일을 “지칠 줄 모르는 천재”라 평했고, <포브스>는 “궁극의 사고 기계”라 불렀다. 지는 커즈와일을 최고의 첨단 사업가 가운데 하나로 꼽으면서 “토머스 에디슨의 적자”라 평했다. PBS는 “미국을 만든 16명의 혁신가들” 중 한 사람으로 커즈와일을 꼽아 지난 200년간 세상을 바꾼 여러 발명가들과 나란히 그를 평가했다. 커즈와일은 미국 발명가 명예의 전당에 등재되어 있으며, 미국 기술 훈장, 레멜슨-MIT 상(세계에서 가장 권위 있는 혁신 관련 상), 13개의 명예 박사학위를 받았고, 또한 세 명의 미국 대통령으로부터 상을 받기도 했다. 그는 이 책 말고도 <환상적인 여행: 영원히 살 수 있을 정도로 수명 연장하기Fantastic Voyage: Live Long Enough to Live Forever>(테리 그로스먼 박사와 공저) <영적 기계의 시대The Age of Spiritual Machines><건강한 삶으로 가는 10%의 해답The 10% Solution for a Healthy Life><지적 기계의 시대The Age of Intelligent Machines> 등의 책을 썼다.​​목차프롤로그: 생각의 힘 ​1. 여섯 시기 직관적 선형 관점 대 역사적 기하급수적 관점 여섯 시기 특이점이 머지않다 ​2. 기술 진화 이론: 수확 가속의 법칙 생명 주기에 나타난 기술의 S자 곡선 무어의 법칙과 그 너머 DNA 염기 서열 분석, 메모리, 통신, 인터넷, 소형화 경제적 요구로서의 특이점​3. 인간 뇌 수준의 연산 용량 만들기 연산 기술의 여섯 번째 패러다임: 3차원 분자 연산을 비롯하여 떠오르는 신기술들 연산의 한계 ​4. 인간 지능 수준의 소프트웨어 만들기:어떻게 뇌를 역분석할 것인가 뇌의 역분석에 대한 개요사람의 뇌는 컴퓨터와 다른가? 뇌 들여다보기 뇌 모델 구축하기 뇌와 기계의 접속가속적으로 발전하고 있는 뇌 역분석 연구 뇌 업로드하기​5. GNR: 중첩되어 일어날 세 가지 혁명유전학: 정보와 생물학의 접점 나노기술: 정보와 물리 세계의 접점로봇공학: 강력한 AI ​6. 어떤 영향들을 겪게 될 것인가? 인체에 미칠 영향 뇌에 미칠 영향 인간 수명에 미칠 영향 전쟁에 미칠 영향: 원격, 로봇식, 강인한, 소규모, 가상 현실 패러다임 학습에 미칠 영향 일에 미칠 영향놀이에 미칠 영향 우주의 지적 운명에 미칠 영향: 왜 인류가 유일한 존재일 가능성이 높은가 ​7. 나는 특이점주의자입니다 의식이라는 골치 아픈 문제 나는 누구일까? 나는 무엇일까?초월로서의 특이점 ​8. 뗄 수 없게 얽힌 GNR의 희망과 위험 뗄 수 없게 얽힌 편익……그리고 위험 다양하게 펼쳐질 존재론적 위험들 방어 준비포기라는 발상 방어 기술의 발달과 규제가 미칠 영향GNR 방어 전략 ​9. 비판에 대한 반론다양한 비판들믿을 수 없다는 비판 맬서스주의자들의 비판 소프트웨어에 관한 비판 아날로그 처리 방식에 관한 비판 신경 정보 처리의 복잡성에 근거한 비판 미세소관과 양자 연산에 관한 비판 처치-튜링 명제에 관한 비판 실패율에 대한 지적 ‘속박’ 효과에 대한 지적 존재론 입장의 비판: 컴퓨터가 의식을 가질 수 있는가 빈부 격차에 대한 지적 정부 규제 가능성에 대한 지적 유신론 입장의 비판 전체론 입장의 비판 ​에필로그 옮긴이의 말 자료와 연락처 정보부록: 다시 보는 수확 가속의 법칙  "
500자 칼럼(2134) AI가 어떻게 인간성을 구할 수 있는가?  ,https://blog.naver.com/kt_choi/221674045928,20191010,"최광태의  500자 칼럼(2134)  AI가 어떻게  인간성을 구할 수있는가? #AI #Deeplearning ​조국대전이  어저께는 기점으로 점점 마지막을 향해 치닫고 있는 것 같습니다. 정치문제는 말하기 조차 거북하고 , 심지어 가족간에도 의견이 다른 경우도 많죠.  누가 잘못되었는지 , 누가 누구를 속이고 있는지 조만간 밝혀질 것 같습니다.  ​가짜뉴스의 홍수 속에서 멍들어가는 민초들.. 무었을 보아야 할지,  유튜브를 통한 일인 미디어의 영향력이 이렇게 거대해  질것이라고는 상상조차 하기 힘들었죠.  이게 진정한 민주주의로 가는 길이라면 힘들어도 받아들여야죠!! ​칼럼본론으로 들어갑니다. ​오늘은 TED강의중 AI관련 강의가 있어 칼럼의 소재로 뽑아 보았습니다.  하기에 링크된 강의한번 들어보세요. ​Deep Learning은 미국에서  만들어졌지만 중국에서 Implementation은  더욱 빠른 속도로 이루어지는 것 같습니다.   아침 9시출근 밤 9시퇴근 6일근무(9.9.6)이상을  일하며 치고 올라오는 중국의 Start-up들의 피나는 노력,  광대한 중국 시장,  일상화된 Mobile Payment, Speech Recognition, 드론....​수박 한조각도 스마트폰으로 모발을 통한 거래가 일어납니다. 도시에 사는 사람은 1:1로 수박농가 농부와 거래가 눈깜짝할 사이에 이루어 집니다. 이렇게 쌓여가는 big data DB는 점점더 기업가에게 힘을 주고, 기회의 시장으로 유도합니다. ​TED강의 “ How AI can save our humanity”By Kai-Fu Lee​https://youtu.be/ajGgd9Ld-Wc​AI가 우리 인산의 단순한 루틴업무의 일들은 빼았아 갈지는 몰라도,  인간이 왜 행복해야 하는지, 루틴한 일대신  인간자신을 위한 업무가 더욱 세분화된 Job형로 만들어 질것으로 예상합니다. ​로봇과 인간이 상호의존하고 Co-work하는 세대로 어떻게 다가가야 하는지 강사는 힘주어 이야기 합니다. ​​2019.10.10.19.50 pm맥까페, 수지구청역                 "
"케이무크로 인공지능 AI 강좌 체계적 학습 방법, 학습자 이수체계도  ",https://blog.naver.com/dysnomia/221774745469,20200130,"​무크 MOOC(Massive Open Online Course)는, 대학, 기관의 우수 온라인 강좌를 누구나 무료로 수강할 수 있는 서비스다.​교육부는 한국형 온라인 공개강좌(케이무크, K-MOOC) 수강생들이 인공지능 분야 강좌를 체계적으로 들을 수 있는 학습 길잡이(이수체계도)를 개발했다.​​   ​​이번에 개발된 이수체계도는 인공지능 강좌를 어떤 순서로 들어야 할지 모르는 학습자를 세분화해서 기초지식과 수강목적 등에 따라 맞춤형 과목과 학습 순서 정보를 제공한다.​학습자는 컴퓨터공학 전공자, 공학․자연과학 전공자, 인문․사회과학 등 전공자, 로보틱스나 자율주행 등 실무에 인공지능 기술을 적용하는 직업인, 경영인, 일반인으로 6가지 유형으로 분류했다.​​   ​​교육부는 인공지능 강좌 이수체계도 개발을 위한 정책연구를 실시해서, 국내외 대학 및 대학원, 민간기관, 해외 무크 플랫폼 등 24개 기관의 929개의 강좌를 분석하고 주요과목 55개를 도출했다.​​   ​​케이무크에서는 인공지능 분야를 누구나 온라인으로 학습할 수 있도록 무크강좌 이수체계도에 따른 모든 과목을 집중 개발해 제공할 예정이다. ​우선, 케이무크 홈페이지에서 인공지능 분야 무크강좌 이수체계도 55과목과 기존의 케이무크 강좌를 연결(매핑)해서 제공하고, 추가개발이 필요한 20개 과목은 올해 안에 개발을 완료할 예정이다.​​   ​​나날이 관심도가 높아지면서 뉴스를 비롯한 일상에서도 흔히 들리는 '인공지능'. 이제 기술자들 뿐만 아니라, 일반인들도 교양으로 지식을 쌓아놓으면 좋을 시기다. ​케이무크가 온라인 무료 강좌 플랫폼이니만큼, 이번 이수체계도를 잘 활용해서 자신의 상황에 맞게 체계적인 AI 관련 학습 기회를 가져보자. ​​ 누구나 무료로 수강하는 온라인 대학 강의, 케이무크 K-MOOC일상생활을 하다보면 자기 전공이나 혹은 다른 분야에 대해 좀 더 깊은 지식을 쌓아야겠다는 생각이 들 때...dysnomia.blog.me ​​ 인공지능(AI) 이수체계도 및 MOOC강좌 연결(매치) 결과​ 분류과목(55개)K-MOOC강좌 연결(매치)공통필수(9)Introduction to CS and Programming(컴퓨터과학 개론 및 프로그래밍)파이썬 프로그래밍(한동대학교)Math for Artificial Intelligence(인공지능을 위한 수학)  Introduction to Artificial Intelligence(인공지능 개론)인공지능의 기초(서울대학교) Introduction to Deep Learning(딥러닝 개론)딥러닝 개론(대구대학교) Introduction to Machine Learning(기계학습 개론)머신러닝(서울대학교) 외 2건 Advanced Machine Learning(고급 기계학습)  Reinforcement Learning (강화학습)  Computer Vision (컴퓨터비전)​ Natural Language Processing(자연어처리)텍스트 마이닝 실전 및 분석(연세대학교) 외 1건핵심선택(18)Introduction to Programming for Everyone (프로그래밍 개론) Object-Oriented Programming(객체지향프로그래밍)객체지향형 프로그래밍과 자료구조(영남대학교) Data Structures (데이터구조)자료구조(상명대학교) 등 외 1건 Algorithms (알고리즘)  Foundations of Data Science(데이터사이언스의 기초)R 데이터 분석 입문(단국대학교) 외 3건 Data Analysis (데이터분석)파이썬을 이용한 빅데이터 분석(세종대학교) 외 1건 Big Data Analytics (빅 데이터 분석)빅 데이터 첫 걸음(포항공과대학교) 외 6건 Basic (High school) Mathematics(기초 수학)  Calculus (미적분학)미적분학 I (성균관대학교) 외 2건 Linear Algebra (선형대수)선형대수학 (성균관대학교) 외 2건 Probability and Discrete Mathematics(확률과 이산수학)Mathematical Fundamentals for Data Science(고려대학교) Introduction to Statistics(통계학 개론)통계학의 이해 I(숙명여자대학교) 외 2건 Topics in Artificial Intelligence(인공지능 연구)  Topics in Machine Learning(기계학습 연구)  Applied Artificial Intelligence(인공지능 응용)빅데이터와 인공지능의 응용(서울대학교) 외 5건자율주행 분야 응용강좌  Understanding Artificial Intelligence(인공지능의 이해)ICBM+AI개론(고려대학교) Deep Learning Application and Practice (딥러닝 응용 및 실습)파이썬으로 배우는 기계학습 입문 (한동대학교) 외 1건 AI Ethics (인공지능 윤리) 분야별선택(28)Computer Graphics (컴퓨터그래픽)영상처리와 패턴인식(상명대학교)Linguistics (언어학)언어와 인간 (서울대학교)Probabilistic Graphical Models(확률적 그래픽 모델)  Pattern Recognition (패턴인식)영상처리와 패턴인식(상명대학교) Knowledge-based AI(지식기반 인공지능)  Speech Recognition (음성인식)  Game AI (게임인공지능)게임인공지능(세종대학교) Introduction to Robotics(로봇공학 개론)Fun-MOOC - 기계는 영원하다! (서울대학교) Artificial Intelligence for Robotics(로봇공학을 위한 인공지능)Mobile Robot Perception and Navigation(서울과학기술대학교) 외 1건 Economics and Computation(계량경제학)계량경제학(제주대학교) Artificial Intelligence for Business(비지니스를 위한 인공지능)  Biomedical Computing(생의학 컴퓨팅)바이오메디컬비전 및 응용(숭실대학교) 외 2건 Introduction to Cognitive Science(인지과학 개론)인간 뇌의 이해(서울대학교) Computational Cognitive Science(전산 인지과학)  Logic Programming(논리 프로그래밍)   Signals and Circuit Systems(신호 및 회로시스템)전자회로(영남대학교) Computer Structures (컴퓨터구조)컴퓨터 구조 (상명대학교) Hardware Systems (하드웨어 시스템)  Autonomous Agents(자율 에이젠트)  Robot Kinematics and Dynamics(로봇 운동학 및 역학)Robot Manipulator and Underwater Robot Application(서울과학기술대학교) Advanced Robotics Planning(고급로봇공학)Humanoid Robot(서울과학기술대학교) Human-Robot Interaction(휴먼-로봇 상호작용)  Neural Computation (신경 계산)  Computational Perception(계산 인식)  Human Intelligence and AI(인간지능과 인공지능)  Inference and Information Theory(추론 및 정보이론)  Economics (경제학 개론)경제학원론 – 미시경제학(서울대학교) Machine Learning for Trading(거래를 위한 기계학습)  ​ "
Stanford CS224N NLP with Deep Learning | Winter 2021 | Lecture 6 - Simple and LSTM RNNs ,https://blog.naver.com/sohnji12/223092760853,20230503,"Stanford CS224N NLP with Deep Learning | Winter 2021 | Lecture 6 - Simple and LSTM RNNs​1. RNN2. Other uses of RNN3. Exploding and Vanishing Gradients4. LSTMs5. Bidirectional and multi-layer RNNs​1. The Simple RNN Language Modelsequence of words, word embeddings, hidden states. output distribution using softmax to predict how likely the different words are going to appear as the next word​# Training an RNN Language ModelGet a big corpus of text which is a sequence of wordsFeed into RNN-LM; Compute output distribution y(hat) for every step t.Loss function on step t is cross-entropy between predicted probability distribution y(hat)(t), and the true next word y(t) ​ Computing loss and gradient across entire corpus is too expensive… In practice, consider x(1), … , x(t) as a sentence (or a document)Recall: Stochastic Gradient Descent allows us to compute loss and gradients for small chunk of data, and update.Compute loss J(theta) for a sentence (actually, a batch of sentences), compute gradients and update weights. Repeat. ​# Training the parameters of RNNs: Backpropagation for RNNsThe gradient w.r.t. a repeated weight is the sum of the gradient w.r.t. each time it appearsWhy? each of the value of the sum will be completely different from each other​# Multivariable Chain RuleGiven a multivariable function f(x, y), and two single variable functionsx(t) and y(t), here’s what the multivariable chain rule says:gradients sum at outward branches​​ ​# Backpropagation for RNNsBackpropagate over timesteps i=t, … , 0, summing gradients as you go. This algorithm is called “backpropagation through time” [Werbos, P.G., 1988, Neural Networks 1, and others]As sentence gets longer, more time consumed…​# Generating text with RNN Language Modelgenerate text by repeated sampling. Sampled output becomes next step’s input​sampled out put to input for the next word… and keep repeating it over and over again = generating text… end of sequence symbol = finished with generating text​# Evaluating Language Modelsstandard evaluation metric for LM is perplexitygiven a sequence of t words, what probability do you give to t+1 word, and inverse that…geometric mean of the inverse probabilities.perplexity is simply the cross entropy loss before exponentiated. Lower perplexity is better! ​# Why should we care about Language Modeling?Benchmark task that helps us measure our progress on understanding languageLM is a subcomponent of many NLP tasks, especially those involving generating text or probability of textpredictive typingspeech recognitionhandwriting recognitionspelling/grammar correctionauthorship identificaitonmachine translationsummarizationdialogueetc.​2. Other RNN uses# RNNs can be used for sequence taggingex) part-of-speech tagging, named entity recogntion# RNNs can be used for sentence classificaitonex) sentiment classification how to compute sentence encoding? usually better: take element wise max or mean of all hidden states# RNNs can be used for language encoder moduleex) Question Answering, machine translation, many other tasks ! # RNNs can be used to generate textex) speech recognition, machine translation, summarization… this is an example of conditional language model (also text classification) ​3. Problems with Vanishing and Exploding Gradients​# Vanishing Gradients​# Vanishing Gradient Proof Sketch (Linear Case)​# Why is vanishing gradient a problem?simple RNNs are very good at modeling nearby affects, but not long term effects…. ​# Effect of vanishing gradient on RNN-LMRNN-LM needs to model the dependencybut if the gradient is small, the model can’t learn this dependencyso, the model is unable to predict similar long-distance dependencies at test time​# Why is exploding gradient a problem? If the gradient becomes too big, then the SGD update step becomes too bigThis can cause bad updates: we take too large a step and reach a weird and bad parameter configurationIn the worst case, this will result in lnf or NaN in your network ​# Gradient Clipping: Solution for exploding gradientGradient Clipping: if the norm of the gradient is greater than some threshold, scale it down before applying SGD updateIntuition: take a step in the same direction, but a smaller step​# How ti fix the vanishing gradient problem? It’s too difficult for the RNN to learn to preserve information over many timesteps.In a vanilla RNN, the hidden state is constantly being rewrittenHow about a RNN with separate memory? ​# Long Short-Term Memory RNNs (LSTMs)Solution for vanishing gradients1997 paper is the paper that you always see sited for LSTMsbut it was missing what in retrospect has turned out to be he most important part of the modern LSTMEveryone cites that paper but really a crucial part of the modern LSTM is from Gers et al. (2000)Mastering NN is the path to fame and fortune, the funny thing is at the time that this work was done, that just was not true. Very few people were interested in…the original authors didn’t get recognized for that… Hochreiter moved over into doing bioinformatics… Gers is actually doing multimedia studiesOn step t, there is a hidden state h(t) and a cell state c(t)the cell is more equivalent to simple RNNboth are vectors length n, the cell stores long-term informationThe LSTM can read, erase, and write information from the cellthe cell becomes conceptually rather like RAM in a computerThe selection of which information is erased/written/read is controlled by three corresponding gatesThe gates are also vectors length non each timestep, each element of the gates can be open(1), closed(0), or somewhere in-betweenThe gates are dynamic: their value is based on the current context ​# Long Short-Term Memory (LSTM)We have a sequence of inputs x(t), and we will compute a sequence of hidden states h(t) and cell states c(t)On timestep t:forget gate controls what is kept vs forgotten from previous cell stateinput gate: controls what part of the new cell content are written to celloutput gate: controls what parts of cell are output to hidden sizeSigmoid function: all gate values are between 0 and 1New cell content: this is new content to be written to the cell. simple RNN equation. tanh. balanced around 0new cell content, we want to remember some but not all of the previous stepswe want to store some but not all the values we calculated Cell state: we take previous ccell content, hadamard product with the forget vector, and add to it the hadamardproduct inpt gate and cell updateHIdden state: read (output) some content of the cell ​ ​# How does LSTM solve vanishing gradients?makes it easier for the RNN to preserve information over many time stepsdoesn’t gurantee that there is no vanishing/exploding gradient, but it does provide an easier way for the model to learn long-distance dependencies​# LSTMs: real-world successIN 2013-2015,  LSTMs started achieving state-of-the-art resultsSuccessful tasks include handwriting recognition, speech recognition, machine translation, parsing, and image captioning, as well as language modelsLSTMs became the dominant approach for most NLP tasksNow(2021), other approaches (Transformers) have become dominant for many tasksFor example in WMT (a MachineTranslation conference + competition):In WMT 2014, there were 0 neural machine translation systems (!)In WMT 2016, the summary report contains “RNN” 44 times (and these systems won)In WMT 2019: “RNN” 7 times, “Transformer” 105 times​# Is vanishing/exploding gradient just a RNN problem?No! It can be a problem for all neural architectures (including feed-forward and convolutional), especially very deep ones. Due to chain rule / choice of nonlinearity function, gradient can become vanishingly small as it backpropagatesThus, lower layers are learned very slowly (hard to train) ​Solutions: lots of new deep forward / convolutional architectures add more direct connections (thus allowing the gradient to flow) ex) Residual connections aka “ResNet”one path is identity path, other path goes through layersex) DenseNetex) HighwayNet​Conclusion: Though vanishing/ exploding gradients are a general problem, RNNs are particularly unstable due to the repeated multiplication by the same weight matrix​5. Bidirectional and Multi-layer RNNs: motivationhidden states can be regarded as representation of a certain word in the context of this sentence. This we call a contextual representationThese contextual representation only contain information about the left contextWhat about right context?​# Bidirectional RNNsForward RNN + Backward RNN = Concatenated hidden statesNw has both left and right contextThey are only applicable if you have access to the entire input sequenceThey are not applicable to Language Modeling, because in LM you only have left context available. but if you do have entire input sequence, bidirectionality is powerful. For example, BERT (Bidrectional Encoder representations from Transformers) is a powerful pretrained contextual representation system built on bidirectionality.  "
런던 EA해커톤 후기 #쇼디치 Newspeak House ,https://blog.naver.com/foryourmoments/223013769862,20230213,"개발자 행사 외에도 다양한 토론, 소셜 모임이 열린다. 독립단체 운영 공간인 것을 작년부터 알고는 있었는데 이번 주말에서야 처음 참여해보게 되어 후기를 기록한다. 쇼디치 위치 탓인지 규모가 작은 홍대 상상마당 느낌이었다.​​​ Newspeak HouseThe London College of Political Technologynewspeak.house ​내가 참여한 행사는,Effective Altruism(EA) Hackerton​11일 (토) 오전10시 - 밤10시​참가비는 없었고 사전 등록이 필요했다.(다른 개발자 행사도 여럿 있지만 평일 저녁이 부담스러워 주말로 선택했다.)​​​ ​해커톤 처음 참여라서 긴장되는데 소셜까지 신경쓸 여유가 없을 거 같아 석사 동기 한명을 불러 같이 갔다. 확실히 아는 사람 한 명이 있는 덕을 톡톡히 봤다. 분위기 봐서 너무 아니면(?) 점심 시간 전후로 나와서 그냥 우리끼리 따로 캐치업하고 놀자 - 하고 나름 신호를 정하고 갔다. 생각보다 너무 괜찮아서 저녁 8시 최종 팀발표까지🎉 하고 나왔다.​​​ ​이벤트 공간 자체가 생각보다 괜찮았다. 요즘 런던은 어느 행사를 참여하든 일회용품이 없는 추세라 ... 나는 개인 텀블러를 따로 가져갔고 주최측에서 준비한 컵 하나를 따로 받아와 추가로 두고 사용했다. 해커톤 참여 인원 모두가 랩탑을 오랫동안 써야 하는데 테이블마다 사람 수대로 전원 연결을 할 수 있는 익스텐션 리드가 있어 편했다. (개인 노트북 거치대나 전용 키보드를 챙겨온 사람들도 은근히 있었다.)​​해커톤 테마 주제인 EA 공부를 약간 하고 가야하나 고민하다 시간이 없어 테드 영상 두어개 짧게 본 게 전부였는데 ... 다행히 오거나이저들이 테마와 관련 없는 플젝 아이디어도 오픈하겠다고 하여 발표 때에는 이런 저런 재미난 내용을 여럿 볼 수 있었다.+ 가장 인상깊었던 프로젝트 오너의 후기 글을 참고로 남긴다. speech recognition을 결합한 방식이 인상적이었다. GPT Automator, LangChain and Tool Engineering - EA London HackathonYesterday, Chidi Williams and I did the London EA Hackathon 2023. We built a tool to physically control your computer through your voice using Whisper…harries.co ​​우리 팀 아이디어는 ✨챗봇(a.k.a. AI assistant) 구성이었고 어느 포럼에 등록된 포스트가 담긴 JSON data 불러와서 (title, username, posthtml, 내용이 포함된 데이터셋) 텍스트 추출을 하고 데이터 ChatGPT API를 연결해 요약하는 식이었다.​​​ ​그 중 내가 담당한 건 text retrieval 파트였다. 벌써 1년 이상, 조금 오래되긴 했지만 NLTK로 text processing을 하는 프로젝트 경험이 있어 그 쪽으로 자원했다. 파트 분배할 때, 내가 담당할 모듈 뒷부분 아이디어 기버(giver)가 구현하고 싶어하는 개념을 이해할 수 있었던 것도 선택에 크게 작용했다.​최근 ChatGPT로 이런저런걸 시켜본 파이썬 프로젝트들을 팔로하고 있어서 ... 특히 테디노트에서 소개한 [웹데이터 크롤링 + ChatGPT 요약시키기] 코드를 따라하며 뚝딱거려본 덕분에 😂 이전에 text processing 프로젝트와 소스코드를 이어붙이면 할만하겠다 싶었다.​​​ ​셀레니움으로 웹 스크래핑하는 내용은 따로 스터디를 한 적도 있다 / 그 때 당시 생각해보았던 NLP 연결 내용들이 실제 이번 해커톤에서 실험할 수 있었다. [Web Scraping]모임기록 #Selenium영국에 거주하는 한국인끼리 테크스터디가 있다. 한달 반 정도 참여하면서 총 4번의 발표를 들었다. 그 중 ...m.blog.naver.com ​​하나의 팀으로 유기적으로 작업하려니 내 앞단에서 어떤 형태로 데이터를 넘겨주는지 알고 있어야 하고 반대로 내가 뒤로 넘겨줘야 하는 데이터 형태도 미리 지정해야 했다. 내가 이런 쪽으로 경험이 많지 않다고 솔직하게 말하고 schema 구성을 전적으로 다른 분들에게 맡겼다.​retrieval 앞에서 오는 건 프론트에서 입력하는 쿼리 인풋과 프로세싱 해야하는 데이터 소스 두 가지밖에 없어서 편했다. 뒤로 넘겨주는 것을 (OpenAI API로 넘겨줘야 하는) 텍스트 형태로 해야 하는데 어떻게 하면 되겠구나 하는 감이 어느 정도는 있었다. 그런데 나는 아직까지 윈도우만 사용중이고 도커를 사용해 본 적이 없어서 작업 경로 설정이라던지 내가 저장하는 pickle data storage path를 어떤식으로 맞춰야하는지 잘 몰라 도움을 받았다. 반대로 팀원들은 전부 맥 유저라😂 윈도우 environment variable 설정을 어떻게 해야 하냐 챗GPT에 물어보기도 하고 그랬다. [ChatGPT가 꽤 상세하게 알려주긴 하지만 결국 해결은 아래 블로그를 보고 되었다.] Using .env Files for Environment Variables in Python ApplicationsApplications are made to be deployed. At some point during development you will need to think about t...dev.to ​​당일 만난 사람들끼리 즉석에서 조를 구성해 하나의 프로젝트를 진행, Minimum Viable Product(MVP)를 완성하고 발표하는 과정에서 정말 많은 것을 배웠다. 특히 나는 데이터 사이언티스트로서 데이터 분석에 초점이 맞춰진 과제를 주로 하다보니 하나의 프로덕트 관점에서 프론트 백엔트 유기적으로 연결해야 하는 파이프라인을 전혀 모르는데 이 부분에 대한 걸 다른 팀원들 옆에서 지켜보며 많은 정보들을 알 수 있었다.​​해커톤 참여하기 전에 친구와 둘이서 덜덜 떨면서 우리 뭐 하나도 못 알아들으면 어쩌지 걱정하던게 무색하게 ... 물론 쟁쟁한 실력자 두 분이 대부분 하드캐리하셨지만 🤣 내가 10% 정도?는 기여한 것 같아서 처음 참여에 굉장히 운이 좋게 큰 성과를 얻었다고 느꼈다. 무엇보다도 내가 앞으로 어떤 공부들을 해야 하는지 키워드를 정말 많이 얻어서 큰 도움이 되었다.​​팀원들이랑 캐미도 좋았다. 그 중 한 분이 자기 플랏메이트 3명과 같이 오픈 하우스 해커톤을 연다고 왓츠앱 초대 남겨주셨는데 체력 + 소셜 에너지력 🤣🤣🤣 감당이 안될 것 같아 애매하게 있다. 참여한다면 다음에 기록을 남기는 것으로 ...!​​+ 참고로 다음 EA 해커톤은 캠프릿지에서 열린다고 한다. 관심있는 분들은👇 EAGxCambridge | Effective AltruismWe are excited to announce Cambridge’s first ever conference on effective altruism! EAGxCambridge will bring together 500 leading academics, working professionals, charity entrepreneurs, and students to address the world’s most pressing problems . The programme will feature workshops, talks, and con...www.effectivealtruism.org "
OpenAI ?  ,https://blog.naver.com/myob0/223011695632,20230210,"출처 : https://en.wikipedia.org/wiki/OpenAI# OpenAI - WikipediaOpenAI 49 languages Article Talk Read Edit View history From Wikipedia, the free encyclopedia Not to be confused with OpenAL . It has been suggested that GPT-4 be merged into this article. ( Discuss ) Proposed since February 2023. OpenAI is an American artificial intelligence (AI) research laborator...en.wikipedia.org OpenAI is an American artificial intelligence (AI) research laboratory consisting of the non-profit OpenAI Incorporated (OpenAI Inc.) and its for-profit subsidiary corporation OpenAI Limited Partnership (OpenAI LP). OpenAI conducts AI research to promote and develop friendly AI in a way that benefits all humanity. The organization was founded in San Francisco in 2015 by Sam Altman, Reid Hoffman, Jessica Livingston, Elon Musk, Ilya Sutskever, Peter Thiel and others,[5][1][6] who collectively pledged US$1 billion. Musk resigned from the board in 2018 but remained a donor. Microsoft provided OpenAI LP a $1 billion investment in 2019 and a second multi-year investment in January 2023, reported to be $10 billion.[7]History[edit]  Non-profit beginnings[edit]In December 2015, Sam Altman, Elon Musk, Greg Brockman, Reid Hoffman, Jessica Livingston, Peter Thiel, Amazon Web Services (AWS), Infosys, and YC Research announced[8] the formation of OpenAI and pledged over $1 billion to the venture. The organization stated it would ""freely collaborate"" with other institutions and researchers by making its patents and research open to the public.[9][10] OpenAI is headquartered at the Pioneer Building in Mission District, San Francisco.[11][3]According to Wired, Brockman met with Yoshua Bengio, one of the ""founding fathers"" of the deep learning movement, and drew up a list of the ""best researchers in the field"".[12] Brockman was able to hire nine of them as the first employees in December 2015.[12] In 2016 OpenAI paid corporate-level (rather than nonprofit-level) salaries, but did not pay AI researchers salaries comparable to those of Facebook or Google.[12] (Microsoft's Peter Lee stated that the cost of a top AI researcher exceeds the cost of a top NFL quarterback prospect.[12]) For these researchers, however, OpenAI's potential and mission drew them to the firm; a Google employee stated that he was willing to leave Google for OpenAI ""partly because of the very strong group of people and, to a very large extent, because of its mission.""[12] Brockman stated that ""the best thing that I could imagine doing was moving humanity closer to building real AI in a safe way.""[12] OpenAI researcher Wojciech Zaremba stated that he turned down ""borderline crazy"" offers of two to three times his market value to join OpenAI instead.[12]In April 2016, OpenAI released a public beta of ""OpenAI Gym"", its platform for reinforcement learning research.[13] In December 2016, OpenAI released ""Universe"", a software platform for measuring and training an AI's general intelligence across the world's supply of games, websites, and other applications.[14][15][16][17]In 2017 OpenAI spent $7.9 million, or a quarter of its functional expenses, on cloud computing alone.[18] In comparison, DeepMind's total expenses in 2017 were $442 million. In summer 2018, simply training OpenAI's Dota 2 bots required renting 128,000 CPUs and 256 GPUs from Google for multiple weeks.In 2018, Musk resigned his board seat, citing ""a potential future conflict (of interest)"" with his role as CEO of Tesla due to Tesla's AI development for self-driving cars, but remained a donor.[19]Transition to for-profit[edit]In 2019, OpenAI transitioned from non-profit to ""capped"" for-profit, with the profit capped at 100 times any investment.[20] According to OpenAI, the capped-profit model allows OpenAI LP to legally attract investment from venture funds, and in addition, to grant employees stakes in the company, the goal being that they can say ""I'm going to OpenAI, but in the long term it's not going to be disadvantageous to us as a family.""[21] Many top researchers work for Google Brain, DeepMind, or Facebook, which offer stock options that a nonprofit would be unable to.[22] Prior to the transition, public disclosure of the compensation of top employees at OpenAI was legally required.[23]The company then distributed equity to its employees and partnered with Microsoft and Matthew Brown Companies,[24] who announced an investment package of $1 billion into the company. OpenAI also announced its intention to commercially license its technologies.[25] OpenAI plans to spend the $1 billion ""within five years, and possibly much faster"".[26] Altman has stated that even a billion dollars may turn out to be insufficient, and that the lab may ultimately need ""more capital than any non-profit has ever raised"" to achieve artificial general intelligence.[27]The transition from a nonprofit to a capped-profit company was viewed with skepticism by Oren Etzioni of the nonprofit Allen Institute for AI, who agreed that wooing top researchers to a nonprofit is difficult, but stated ""I disagree with the notion that a nonprofit can't compete"" and pointed to successful low-budget projects by OpenAI and others. ""If bigger and better funded was always better, then IBM would still be number one.""The nonprofit, OpenAI Inc., is the sole controlling shareholder of OpenAI LP. OpenAI LP, despite being a for-profit company, retains a formal fiduciary responsibility to OpenAI Inc.'s nonprofit charter. A majority of OpenAI Inc.'s board is barred from having financial stakes in OpenAI LP.[21] In addition, minority members with a stake in OpenAI LP are barred from certain votes due to conflict of interest.[22] Some researchers have argued that OpenAI LP's switch to for-profit status is inconsistent with OpenAI's claims to be ""democratizing"" AI.[28] A journalist at Vice News wrote that ""generally, we've never been able to rely on venture capitalists to better humanity"".[29]After becoming for-profit[edit]In 2020, OpenAI announced GPT-3, a language model trained on large internet datasets. GPT-3 is aimed at natural language answering of questions, but it can also translate between languages and coherently generate improvised text. It also announced that an associated API, named simply ""the API"", would form the heart of its first commercial product.[30]In 2021, OpenAI introduced DALL-E, a deep learning model that can generate digital images from natural language descriptions.[31]In December 2022, OpenAI received widespread media coverage after launching a free preview of ChatGPT, its new AI chatbot based on GPT-3.5. According to OpenAI, the preview received over a million signups within the first five days.[32] According to anonymous sources cited by Reuters in December 2022, OpenAI was projecting $200 million revenue in 2023 and $1 billion revenue in 2024.[33]As of January 2023, OpenAI was in talks for funding that would value the company at $29 billion, double the value of the company in 2021.[34] On January 23, 2023, Microsoft announced a new multi-year, multi-billion dollar (reported to be $10 billion) investment in OpenAI.[35][36]The investment is believed to be a part of Microsoft's efforts to integrate OpenAI's ChatGPT into the Bing search engine. Fellow tech company Google has also been looking to develop a similar AI application, called Bard, after ChatGPT was launched fearing that the AI tool could threaten Google's place as a go-to source for information.[37]On February 7, 2023, Microsoft announced that it is building AI technology based on the same foundation as ChatGPT into Microsoft Bing, Edge, Microsoft 365 and other products.[38]Participants[edit]  Key employees:CEO and co-founder:[39] Sam Altman, former president of the startup accelerator Y CombinatorPresident and co-founder:[40] Greg Brockman, former CTO, 3rd employee of Stripe[41]Chief Scientist and co-founder: Ilya Sutskever, a former Google expert on machine learning[41]Chief Technology Officer:[40] Mira Murati, previously at Leap Motion and Tesla, Inc.Chief Operating Officer:[40] Brad Lightcap, previously at Y Combinator and JPMorgan ChaseBoard of the OpenAI nonprofit:Greg BrockmanIlya SutskeverSam AltmanAdam D'AngeloReid HoffmanWill HurdTasha McCauleyHelen TonerShivon ZilisIndividual investors:[41]Reid Hoffman, LinkedIn co-founder[42]Peter Thiel, PayPal co-founder[42]Jessica Livingston, a founding partner of Y CombinatorCorporate investors:Microsoft[43]Khosla Ventures[44]Infosys[45]Motives[edit]  Some scientists, such as Stephen Hawking and Stuart Russell, have articulated concerns that if advanced AI someday gains the ability to re-design itself at an ever-increasing rate, an unstoppable ""intelligence explosion"" could lead to human extinction. Musk characterizes AI as humanity's ""biggest existential threat.""[46] Seeking to mitigate the inherent dangers of Artificial Intelligence, OpenAI's founders structured it as a non-profit so that they could focus its research on making positive long-term contributions to humanity.[10]Musk and Altman have stated they are partly motivated by concerns about AI safety and the existential risk from artificial general intelligence.[47][48] OpenAI states that ""it's hard to fathom how much human-level AI could benefit society,"" and that it is equally difficult to comprehend ""how much it could damage society if built or used incorrectly"".[10] Research on safety cannot safely be postponed: ""because of AI's surprising history, it's hard to predict when human-level AI might come within reach.""[49] OpenAI states that AI ""should be an extension of individual human wills and, in the spirit of liberty, as broadly and evenly distributed as possible..."".[10] Co-chair Sam Altman expects the decades-long project to surpass human intelligence.[50]Vishal Sikka, former CEO of Infosys, stated that an ""openness"" where the endeavor would ""produce results generally in the greater interest of humanity"" was a fundamental requirement for his support, and that OpenAI ""aligns very nicely with our long-held values"" and their ""endeavor to do purposeful work"".[51] Cade Metz of Wired suggests that corporations such as Amazon may be motivated by a desire to use open-source software and data to level the playing field against corporations such as Google and Facebook that own enormous supplies of proprietary data. Altman states that Y Combinator companies will share their data with OpenAI.[50]Strategy[edit]  Musk posed the question: ""What is the best thing we can do to ensure the future is good? We could sit on the sidelines or we can encourage regulatory oversight, or we could participate with the right structure with people who care deeply about developing AI in a way that is safe and is beneficial to humanity."" Musk acknowledged that ""there is always some risk that in actually trying to advance (friendly) AI we may create the thing we are concerned about""; nonetheless, the best defense is ""to empower as many people as possible to have AI. If everyone has AI powers, then there's not any one person or a small set of individuals who can have AI superpower.""[41]Musk and Altman's counter-intuitive strategy of trying to reduce the risk that AI will cause overall harm, by giving AI to everyone, is controversial among those who are concerned with existential risk from artificial intelligence. Philosopher Nick Bostrom is skeptical of Musk's approach: ""If you have a button that could do bad things to the world, you don't want to give it to everyone.""[48] During a 2016 conversation about the technological singularity, Altman said that ""we don't plan to release all of our source code"" and mentioned a plan to ""allow wide swaths of the world to elect representatives to a new governance board"". Greg Brockman stated that ""Our goal right now... is to do the best thing there is to do. It's a little vague.""[52]Conversely, OpenAI's initial decision to withhold GPT-2 due to a wish to ""err on the side of caution"" in the presence of potential misuse, has been criticized by advocates of openness. Delip Rao, an expert in text generation, stated ""I don't think [OpenAI] spent enough time proving [GPT-2] was actually dangerous."" Other critics argued that open publication is necessary to replicate the research and to be able to come up with countermeasures.[53]Products and applications[edit]  OpenAI's research tend to focus on reinforcement learning (RL). OpenAI is viewed as an important competitor to DeepMind.[54]Gym[edit]Gym aims to provide an easy to set up, general-intelligence benchmark with a wide variety of different environments—somewhat akin to, but broader than, the ImageNet Large Scale Visual Recognition Challenge used in supervised learning research—and that hopes to standardize the way in which environments are defined in AI research publications, so that published research becomes more easily reproducible.[13][55] The project claims to provide the user with a simple interface. As of June 2017, Gym can only be used with Python.[56] As of September 2017, the Gym documentation site was not maintained, and active work focused instead on its GitHub page.[57][non-primary source needed]RoboSumo[edit]In ""RoboSumo"", virtual humanoid ""metalearning"" robots initially lack knowledge of how to even walk, and are given the goals of learning to move around, and pushing the opposing agent out of the ring. Through this adversarial learning process, the agents learn how to adapt to changing conditions; when an agent is then removed from this virtual environment and placed in a new virtual environment with high winds, the agent braces to remain upright, suggesting it had learned how to balance in a generalized way.[58][59] OpenAI's Igor Mordatch argues that competition between agents can create an intelligence ""arms race"" that can increase an agent's ability to function, even outside the context of the competition.Debate Game[edit]In 2018, OpenAI launched the Debate Game, which teaches machines to debate toy problems in front of a human judge. The purpose is to research whether such an approach may assist in auditing AI decisions and in developing explainable AI.[60][61]Dactyl[edit]Dactyl uses machine learning to train a Shadow Hand, a human-like robot hand, to manipulate physical objects. It learns entirely in simulation using the same RL algorithms and training code as OpenAI Five. OpenAI tackled the object orientation problem by using domain randomization, a simulation approach which exposes the learner to a variety of experiences rather than trying to fit to reality. The set-up for Dactyl, aside from having motion tracking cameras, also has RGB cameras to allow the robot to manipulate an arbitrary object by seeing it. In 2018, OpenAI showed that the system was able to manipulate a cube and an octagonal prism.[62]In 2019, OpenAI demonstrated that Dactyl could solve a Rubik's Cube. The robot was able to solve the puzzle 60% of the time. Objects like the Rubik's Cube introduce complex physics that is harder to model. OpenAI solved this by improving the robustness of Dactyl to perturbations; they employed a technique called Automatic Domain Randomization (ADR), a simulation approach where progressively more difficult environments are endlessly generated. ADR differs from manual domain randomization by not needing there to be a human to specify randomization ranges.[63]Generative models[edit]GPT[edit]Further information: Generative pre-trained transformer The GPT model The original paper on generative pre-training (GPT) of a language model was written by Alec Radford and his colleagues, and published in preprint on OpenAI's website on June 11, 2018.[64] It showed how a generative model of language is able to acquire world knowledge and process long-range dependencies by pre-training on a diverse corpus with long stretches of contiguous text.GPT-2[edit]Main article: GPT-2 An instance of GPT-2 writing a paragraph based on a prompt from its own Wikipedia article in February 2021 Generative Pre-trained Transformer 2, commonly known by its abbreviated form GPT-2, is an unsupervised transformer language model and the successor to GPT. GPT-2 was first announced in February 2019, with only limited demonstrative versions initially released to the public. The full version of GPT-2 was not immediately released out of concern over potential misuse, including applications for writing fake news.[65] Some experts expressed skepticism that GPT-2 posed a significant threat. The Allen Institute for Artificial Intelligence responded to GPT-2 with a tool to detect ""neural fake news"".[66] Other researchers, such as Jeremy Howard, warned of ""the technology to totally fill Twitter, email, and the web up with reasonable-sounding, context-appropriate prose, which would drown out all other speech and be impossible to filter"".[67] In November 2019, OpenAI released the complete version of the GPT-2 language model.[68] Several websites host interactive demonstrations of different instances of GPT-2 and other transformer models.[69][70][71]GPT-2's authors argue unsupervised language models to be general-purpose learners, illustrated by GPT-2 achieving state-of-the-art accuracy and perplexity on 7 of 8 zero-shot tasks (i.e. the model was not further trained on any task-specific input-output examples). The corpus it was trained on, called WebText, contains slightly over 8 million documents for a total of 40 GB of text from URLs shared in Reddit submissions with at least 3 upvotes. It avoids certain issues encoding vocabulary with word tokens by using byte pair encoding. This allows to represent any string of characters by encoding both individual characters and multiple-character tokens.[72]GPT-3[edit]Main article: GPT-3Generative Pre-trained[a] Transformer 3, commonly known by its abbreviated form GPT-3, is an unsupervised transformer language model and the successor to GPT-2. It was first described in May 2020.[74][75][76] OpenAI stated that full version of GPT-3 contains 175 billion parameters,[76] two orders of magnitude larger than the 1.5 billion parameters[77] in the full version of GPT-2 (although GPT-3 models with as few as 125 million parameters were also trained).[78]OpenAI stated that GPT-3 succeeds at certain ""meta-learning"" tasks. It can generalize the purpose of a single input-output pair. The paper gives an example of translation and cross-linguistic transfer learning between English and Romanian, and between English and German.[76]GPT-3 dramatically improved benchmark results over GPT-2. OpenAI cautioned that such scaling up of language models could be approaching or encountering the fundamental capability limitations of predictive language models.[79] Pre-training GPT-3 required several thousand petaflop/s-days[b] of compute, compared to tens of petaflop/s-days for the full GPT-2 model.[76] Like that of its predecessor,[65] GPT-3's fully trained model was not immediately released to the public on the grounds of possible abuse, though OpenAI planned to allow access through a paid cloud API after a two-month free private beta that began in June 2020.[81][82]On September 23, 2020, GPT-3 was licensed exclusively to Microsoft.[83][84]ChatGPT[edit]Main article: ChatGPTChatGPT is an artificial intelligence tool that provides a conversational interface that allows you to ask questions in natural language. The system then responds with an answer within seconds. ChatGPT was launched in November 2022 and reached 1 million users only 5 days after its initial launch.[85]GPT-4[edit]Main article: GPT-4Music[edit]OpenAI's MuseNet (2019) is a deep neural net trained to predict subsequent musical notes in MIDI music files. It can generate songs with ten different instruments in fifteen different styles. According to The Verge, a song generated by MuseNet tends to start reasonably but then fall into chaos the longer it plays.[86][87] In pop culture, initial applications of this tool were utilized as early as 2020 for the internet psychological thriller Ben Drowned to create music for the titular character. [88][89]OpenAI's Jukebox (2020) is an open-sourced algorithm to generate music with vocals. After training on 1.2 million samples, the system accepts a genre, artist, and a snippet of lyrics and outputs song samples. OpenAI stated the songs ""show local musical coherence, follow traditional chord patterns"" but acknowledged that the songs lack ""familiar larger musical structures such as choruses that repeat"" and that ""there is a significant gap"" between Jukebox and human-generated music. The Verge stated ""It's technologically impressive, even if the results sound like mushy versions of songs that might feel familiar"", while Business Insider stated ""surprisingly, some of the resulting songs are catchy and sound legitimate"".[90][91][92]Whisper[edit]Whisper is a general-purpose speech recognition model. It is trained on a large dataset of diverse audio and is also a multi-task model that can perform multilingual speech recognition as well as speech translation and language identification.[93]API[edit]In June 2020, OpenAI announced a multi-purpose API which it said was ""for accessing new AI models developed by OpenAI"" to let developers call on it for ""any English language AI task.""[81][94]DALL-E and CLIP[edit]Main article: DALL-E Images produced by DALL-E when given the text prompt ""a professional high-quality illustration of a giraffe dragon chimera. a giraffe imitating a dragon. a giraffe made of dragon."" DALL-E is a Transformer model that creates images from textual descriptions, revealed by OpenAI in January 2021.[95]CLIP does the opposite: it creates a description for a given image.[96] DALL-E uses a 12-billion-parameter version of GPT-3 to interpret natural language inputs (such as ""a green leather purse shaped like a pentagon"" or ""an isometric view of a sad capybara"") and generate corresponding images. It can create images of realistic objects (""a stained-glass window with an image of a blue strawberry"") as well as objects that do not exist in reality (""a cube with the texture of a porcupine""). As of March 2021, no API or code is available.In March 2021, OpenAI released a paper titled Multimodal Neurons in Artificial Neural Networks,[97] where they showed a detailed analysis of CLIP (and GPT) models and their vulnerabilities. The new type of attacks on such models was described in this work. We refer to these attacks as typographic attacks. We believe attacks such as those described above are far from simply an academic concern. By exploiting the model's ability to read text robustly, we find that even photographs of hand-written text can often fool the model.— Multimodal Neurons in Artificial Neural Networks, OpenAIIn April 2022, OpenAI announced DALL-E 2, an updated version of the model with more realistic results.[98] In December 2022, OpenAI published on GitHub software for Point-E, a new rudimentary system for converting a text description into a 3-dimensional model.[99]Microscope[edit]OpenAI Microscope[100] is a collection of visualizations of every significant layer and neuron of eight different neural network models which are often studied in interpretability. Microscope was created to analyze the features that form inside these neural networks easily. The models included are AlexNet, VGG 19, different versions of Inception, and different versions of CLIP Resnet.[101]Codex[edit]Main article: OpenAI CodexOpenAI Codex is a descendant of GPT-3 that has additionally been trained on code from 54 million GitHub repositories.[102][103] It was announced in mid-2021 as the AI powering the code autocompletion tool GitHub Copilot.[103] In August 2021, an API was released in private beta.[104] According to OpenAI, the model is able to create working code in over a dozen programming languages, most effectively in Python.[102]Several issues with glitches, design flaws, and security vulnerabilities have been brought up.[105][106]Video game bots and benchmarks[edit]OpenAI Five[edit]Main article: OpenAI FiveOpenAI Five is the name of a team of five OpenAI-curated bots that are used in the competitive five-on-five video game Dota 2, who learn to play against human players at a high skill level entirely through trial-and-error algorithms. Before becoming a team of five, the first public demonstration occurred at The International 2017, the annual premiere championship tournament for the game, where Dendi, a professional Ukrainian player, lost against a bot in a live 1v1 matchup.[107][108] After the match, CTO Greg Brockman explained that the bot had learned by playing against itself for two weeks of real time, and that the learning software was a step in the direction of creating software that can handle complex tasks like a surgeon.[109][110] The system uses a form of reinforcement learning, as the bots learn over time by playing against themselves hundreds of times a day for months, and are rewarded for actions such as killing an enemy and taking map objectives.[111][112][113]By June 2018, the ability of the bots expanded to play together as a full team of five, and they were able to defeat teams of amateur and semi-professional players.[114][111][115][116] At The International 2018, OpenAI Five played in two exhibition matches against professional players, but ended up losing both games.[117][118][119] In April 2019, OpenAI Five defeated OG, the reigning world champions of the game at the time, 2:0 in a live exhibition match in San Francisco.[120][121] The bots' final public appearance came later that month, where they played in 42,729 total games in a four-day open online competition, winning 99.4% of those games.[122]GYM Retro[edit]Gym Retro is a platform for RL research on video games. Gym Retro is used to research RL algorithms and study generalization. Prior research in RL has focused chiefly on optimizing agents to solve single tasks. Gym Retro gives the ability to generalize between games with similar concepts but different appearances.See also[edit]  DeepMindFuture of Humanity InstituteFuture of Life InstituteMachine Intelligence Research InstituteNotes[edit]  ^ The term ""pre-training"" refers to general language training as distinct from fine-tuning for specific tasks.[73]^ One petaflop/s-day is approximately equal to 1020 neural net operations.[80] "
"[고객사례] ""타사의 음성 인식 API보다 옵션이 다양하고 속도도 빨라서 만족스럽습니다."" - 클로바 음성 인식 기술을 이용한 앱 개발 사례 ",https://blog.naver.com/n_cloudplatform/221201908376,20180205,"  　 추후엔 네이버 클라우드 플랫폼의 얼굴 인식 API인 Clova Face Recognition(CFR) 상품을 사용해 발음연습기기 앱을 업그레이드해나갈 계획입니다.​-  발음연습기기 '발연기' 앱 개발팀 '3G' 이지은-​우리의 삶을 편리하게 해주는 기술 사례를 소개하는 것도 좋지만​마음을 따뜻하게 만드는 사례를 소개하는 것이 더 반가운 것 같습니다!​  ​오늘은 음성 인식 API 기술을 이용해청각 장애인의 언어 치료를 돕는덕성여대 3G팀의 사례를 소개합니다.​3G팀은 네이버 클라우드 플랫폼의음성 인식 API를 어떻게 사용하고 있는지,자세한 이야기를 아래 인터뷰를 통해만나 보시죠! :-)​​  interviewee : 덕성여대 3G팀 신지우, 이지은, 임지수 ​​ Q. 회사 및 서비스 소개를 부탁드립니다.A. 저희가 만든 앱 ‘발연기'는 '발음 연습 기기'의 약자로, 청소년 청각장애인들이 발음을 연습할 수 있도록 도와주는 발음 교정 애플리케이션입니다.​저희는 덕성여대에 재학 중인 동기로, ‘늘푸른 소리’라는 동아리 활동을 하며 IT 교육 봉사를 해왔습니다. 발음 연습 애플리케이션을 개발해야겠다고 처음 생각하게 된 건 2년 전, ‘청음 회관’이라는 청각장애인 복지 단체에서 청소년 청각장애인 IT 교육 봉사를 하던 중에 인공와우를 이식 받은 친구들을 만났을 때였습니다.​이 친구들 같은 경우에는 후천적으로 들을 수 있게 된 것이기 때문에 꾸준한 발음 교정 연습이 필요한데요. 하지만 국내에는 언어치료기관의 수가 매우 부족하고 그마저도 수도권에 밀집되어 있어서 친구들이 언어 치료에 어려움을 겪는다는 것을 알게 되었습니다. 저희는 이를 돕고자 집에서도 편하게 발음 연습을 할 수 있는 앱을 제작하기로 마음 먹었습니다. ​때마침 좋은 기회로, 2017년에 정부의 ICT인재양성사업인 '한이음 ICT 멘토링'에 참여하게 되었는데요. 거기서 '3G'라는 이름의 팀을 멘토님과 함께 구성해 청각장애인이 발음을 듣고 연습할 수 있는 앱을 본격적으로 개발했습니다. 사용자가 먼저 발음을 듣고 따라서 발음하면 정확도를 확인할 수 있는 연습 기능과 실제 언어치료사에게 자신의 발음을 피드백받을 수 있는 숙제 기능 등을 제공합니다.​​​​ Q. 네이버 클라우드 플랫폼을 언제부터, 어떻게 사용하게 되셨나요?A. 2017년 9월부터 네이버 클라우드 플랫폼의 Clova Speech Recognition(CSR)을 사용해왔습니다.​저희 앱은 사용자의 발음을 텍스트로 변환하는 STT 기능을 필요로 하기 때문에 STT 오픈 소스를 찾아다녔었습니다. 처음에는 해외 클라우드사 STT 기능을 사용했었는데요. 변환하기까지의 속도가 갈수록 점점 느려져서 제대로 작동하지 않게 되어 다른 STT 소스를 찾던 중, 네이버 클라우드 플랫폼의 Clova Speech Recognition(이하 CSR)을  발견해서 사용하게 되었습니다.​타사 STT 기능에서 나타나던 오류도 나타나지 않았고, 무엇보다 음성을 텍스트로 변환해주는 수준을 필요에 맞게 선택할 수 있는 점이 가장 좋았습니다. 저희 같은 경우에는 사용자의 발음을 자동으로 바꾸어 출력해주는 자동 완성 기능이 오히려 도움이 되지 않는 기능이예요. 사용자의 음성이 정확하게 텍스트로 출력되어야 발음을 제대로한 건지, 교정이 필요한지 알 수 있으니까요. 그런 면에서 타사는 한 가지 출력 품질만 제공해서 불편한 반면, 네이버 클라우드 플랫폼의 CSR은 출력 품질을 5가지로 제공하고 있어서 사용자의 발음 그대로 출력될 수 있도록 세팅할 수 있어 좋았습니다.​​​ Q. 네이버 클라우드 플랫폼을 선택한 이유는 무엇인가요?A. 높은 한국어 인식률과 손쉬운 사용, 그리고 좋은 가격 때문에 선택했습니다.​첫째는 높은 한국어 인식률입니다. 저희는 사용자의 정확한 발음을 인식해야 하는 것이 중요한데요. 클로바의 음성 인식 기술은 아무래도 한국 기업인 네이버가 만들어서 그런지 한국어 인식률이 매우 뛰어나서 좋은 것 같습니다.둘째는 손쉬운 사용법 때문입니다. 샘플 소스가 주석과 함께 알기 쉽게 업로드 되어 있어서 초심자가 사용하기 매우 쉽습니다. 뿐만 아니라 홈페이지에 에러 코드에 대한 설명도 자세히 나와 있어 에러가 나더라도 원인을 쉽게 분석할 수 있어 개발이 편했습니다.마지막으로는 가격입니다. CSR뿐만 아니라 네이버 클라우드 플랫폼의 대부분의 상품들이 타 클라우드에 비해 가격대가 좋게 형성된 것 같습니다. 학생 신분으로도 크게 부담스럽지 않게 사용할 수 있는 점도 선택의 이유였습니다.​​​ Q. 네이버 클라우드 플랫폼을 실제 어떻게 사용하고 계시고, 함께 한 후 어떤 점들이 좋아지셨나요?A. 변환 속도가 훨씬 빨라졌고, 코드도 눈에 띄게 짧아졌고, 음성인식 정확도도 높아졌습니다.​처음에 타사의 음성 인식 API를 사용했을 때 '느린 속도'에 대한 문제가 늘 걸림돌이었습니다. 음성이 텍스트로 변환되는 시간이 문장의 길이에 비례하여 점점 더 늦어져서 나중에는 사용 자체가 어려웠었는데요. CSR로 바꾼 후에는 이런 불편이 모두 해결되었습니다. 코드도 눈에 띄게 짧아져서 유지보수도 훨씬 간편해졌고, 변환된 텍스트 중 취향에 맞는 데이터를 선택하여 사용할 수 있는 점도 매우 좋았습니다. 덕분에 사용자가 발음한 소리를 객관적으로 확인할 수 있게 됐고 발음 교정 효과도 극대화할 수 있게 됐습니다.​​​ Q. 마지막으로, 네이버 클라우드 플랫폼 선택을 고민하시는 분들께 도움이 될 수 있는 의견 부탁드립니다.A. 다양한 음성인식 API를 사용해봤지만, CSR처럼 간결한 코드로 쉽게 사용할 수 있는 음성인식 서비스는 없었습니다. ​네이버 클라우드 플랫폼의 CSR은 빠른 속도는 물론 5가지 음성 분석 결과 중 개발품에 가장 적절한 것을 취사선택할 수 있다는 점이 큰 장점인 것 같습니다. 음성 인식 기술을 사용한 서비스를 계획 중인데 어떤 API를 쓸지 고민이시라면, 네이버 클라우드 플랫폼을 선택하는 것이 처음부터 쉬운 길로 갈 수 있는 것이라고 말씀드릴 수 있을 것 같습니다. 음성 인식 외에도 네이버의 다양한 인공지능(AI) API를 손쉽게 활용할 수 있는 것도 좋은 것 같아요. 추후에 저희는 네이버 클라우드 플랫폼의 얼굴 인식 API인 Clova Face Recognition(CFR) 상품을 사용해 발음연습기기 애플리케이션을 업그레이드해나갈 계획입니다. 사용자의 음성 뿐만 아니라 입 모양까지 함께 분석한다면 더욱 효과적으로 발음 교정을 할 수 있을 것 같아서요. 네이버 클라우드 플랫폼의 기술들을 잘 활용해서 앞으로 더 많은 친구들의 언어 치료를 도울 예정입니다. '발연기'도 파이팅! '네이버 클라우드 플랫폼'도 파이팅입니다!​  네이버 클라우드 플랫폼으로 따뜻한 세상을 만들어주고 계신 덕성여대 3G팀분들께 감사드립니다.​ [고객 사례] 1등 반려동물 앱 '아지냥이', 세대공감 스토리 '인생락서'도 네이버 클라우드 플랫폼!​국내 이용자 수 1위 반려동물 앱 '아지냥이'도,중장년 세대공감 글쓰기 서비스 '인생락서...blog.naver.com [7월 신규 상품] ""AI Service"" - 네이버의 인공지능 플랫폼 '클로바(Clova)'와 통번역 기술 '파파고(Papago)의 API 제공!AI Service가 여러분의 서비스를 한 걸음 앞선 비즈니스로 만들어드립니다. 궁금한 걸 물어보면 바로 찾...blog.naver.com     ​​​ "
ChatGPT와 영어로 수다떠는 방법 Talk to ChatGPT ,https://blog.naver.com/jejebi9220/223048949340,20230319,"주말에는 영어공부에 도움이되는 도구나 어플에 대해 활용법을 올려보려고 해요. 요새 가장 핫한 AI는 바로 ChatGPT일텐데요, 다들 사용해보셨나요??​딱히 거창한 기술은 필요없고, ChatGPT 사이트에 들어가서 궁금한 걸 채팅창에 물어보면 되요. 영어로 물어보면 더 빨리 더 정확한 정보를 알려주고, 한국어로 물어봐도 꽤 괜찮은 답을 알려주죠.​그런데 이 ChatGPT한테 질문하는데 채팅뿐만이 아니라 직접 말로도 가능하는 거 알고계셨나요? 오늘은 ChatGPT랑 말로 소통할 수 있는 방법과 이걸로 영어 스피킹 연습하는 방법에 대해서 알려드릴께요!​​​ ChatGPT랑 영어로 수다떨기저는 요새 ChatGPT랑 이렇게 영어로 스피킹 연습을 하고 있어요.​자기소개 및 안부 묻기만나면 인사부터하고 안부부터 묻게되니까 자연스럽게 ChatGPT와 인사하기!​  ​스몰토크하기재밌는 농담을 이야기 해달라고 했어요.  너무 하이개그인데요.....?​궁금한 것 질문하기  아이와 어떻게 하면 주말을 재밌게 보낼 수 있는지 질문했어요. 굉장히 일반적인 답변이지만 그래도 영어로 질문하고 답을 들을 수 있는게 너무 신긱하고 재밌었어요.​​AI지만 영어를 저보다 훨씬 잘하니 연습하기 참 좋은것 같더라구요. 모르는 표현도 물어보고 스몰토크도 하고.​  ​심심해서 한국어로도 대화해봤어요 ㅋㅋ​ChatGPT와 영어로 대화하는 방법은 아래에 적어둘께요. 매우 간단해요!​ Talk to ChatGPT 설치방법먼저 크롬 웹스토어 사이트에 들어갑니다. Chrome 웹 스토어브라우저에 새로운 기능을 추가하고 인터넷 탐색 환경을 맞춤설정할 수 있는 소규모 프로그램입니다.chrome.google.com 왼쪽에 검색창이 있는데  talk to ChatGPT라고 칩니다그럼 이렇게 제일 위에 Talk-to-ChatGPT라는 확장프로그램이 떠요. 이걸 클릭하고  Chrome에 추가를 눌러주세요, 추가하시겠습니까?라는 창이 뜨면 확장 프로그램 추가 클릭.​그리고 크롬 브라우저로 아래 ChatGPT를 실행해보세요. 미리 가입과 로그인이 되어있어야 ChatGPT에 질문이 가능해요. 구글이메일로도 바로 가입이 가능합니다.https://chat.openai.com/chat 그럼 오른쪽 상단에 Talk-to-ChatGPT가 뜨는게 보이죠? 여기 START버튼을 눌러보세요. 혹시 버튼이 안뜨면 퍼즐모양을 눌러서 확장프로그램에 있는 Talk-to-ChatGPT에 핀고정을 눌러주세요.​그럼 이제 ChatGPT와 영어로 대화해보세요. 영어로 대화하기 위해서는 설정에서  AI voice and language와 Speech recognition language를 둘다 영어로 설정해야해요!​프랑스어나 스페인어로 이야기하려면 마찬가지로 위의 두가지 설정을 프랑스어와 스페인어로 바꿔주시면 됩니다!영어뿐만 아니라 다양한 언어에서 활용이 가능하겠네요!​​  나름 심심할때 챗지피티랑 스몰톡 가능하겠죠? "
KDT 68일차 인공지능 ,https://blog.naver.com/ycs318/223060098222,20230330,"오늘의 진도 : STT  STT - Speech To Text / 음성을 텍스트로 변환CFR - Clova Face Recognition / 얼굴 인식 기능CSS - Clova Speech Synthesis / 텍스트를 음성으로 변환Object Detection - 객체 인식OCR Optical Character Recognition - 이미지에서 문자 인식  로그인 후 첫 페이지 오른쪽 상단 [ 콘솔 ] -> 왼쪽 메뉴 중 [ Service ] -> AI, NAVER API 선택​제일 먼저 [ Application 등록 ] 을 해야함먼저 CSR 만 Application으로 등록등록한 Application의 [ 인증정보 ] 를 눌러서 Client ID, Client Secret 기억해두기  어플리케이션 화면에서 [ 개발 가이드 ] -> [ CLOVA Speech Recognition 사용 ] -> [ CLOVA Speech Recognition API 가이드 ] -> [ stt (Speech-To-Text) ] -> 스크롤 내리면 API 예제 있음 그 코드 복사​자바 프로젝트 만들어서 소스 파일 추가 후 복사한 코드 붙여넣기 함한글 음성 파일 준비해서 경로만 넣어주면 바로 STT 가 동작함  스프링 부트를 사용해서 클라이언트가 보낸 음성 파일을 STT 해주는 서비스​ "
[AI-SPARK 0편] 연구개발특구진흥재단 주최 제3회 AI-SPARK 경진대회 최우수상 ,https://blog.naver.com/ansrl23/222872896566,20220919,"2022년 초 TAVE 친구들과 연구개발특구진흥재단 주최 AI-SPARK 공모전을 나간 적이 있었다.좋은 아이디어와 딥러닝을 통한 아이디어 검증을 보여준 덕분에 공모전에서 최우수상을 받을 수 있었고,​무엇보다 자연어 처리 NLP 큰 분야에서의 '감정 분석 Emotion Recognition'에 대해 좀 더 깊이 공부할 수 있었던 기회가 되었었다.​하지만 이런 부분은 기록으로 남기지 않으면, 홀라당 까먹는 법공모전 아이디어를 구체화하면서 새롭게 찾아보고, 공부했던 내용들을 기록으로 남기고자 한다.​(Emotion-Guided Video Summarization 프로젝트 깃허브 홈페이지 ↓) Creative and Descriptive Paper Title.Paper description.jeiyoon.github.io (왼쪽에서 두 번째가 접니다. 최우수상!! ㅎㅎ) “AI로 사회문제 해결”… 특구재단, 경진대회 진행 - 충청투데이[충청투데이 이정훈 기자] 연구개발특구진흥재단은 27일 \'제3회 연구개발특구 AI SPARK 챌린지(인공지능 경진대회)\' 대회에서 최종 선정된 우수팀을 대상으로 시상식을 개최했다.이번 제3회 대회는 특구 내 출...www.cctoday.co.kr (다들 시간이 안 된대서, 회사에 연차 내고 나 혼자 상 받으러 대전 갔다 옴... 그래서 사진도 나혼자만...ㅎㅎ...) 연구개발특구진흥재단 AI - SPARK 제3회 경진대회제3회 AI-SPARK 인공지능 경진대회대전광역시, 특히 대덕구에는 수많은 정부출연연구기관(정출연)이 있고, 정출연의 행정적 업무와 연구기획 방향성을 제시하기 위하여연구개발특구진흥재단이 있다고 한다. (이번 공모전을 통해 처음 알게 된 기관이다.)​AI-SPARK 공모전에서는""출연연 데이터를 바탕으로 사회적 문제를 도출하고 해결책 제시""라는 주제로 아이디어 공모를 받았고,이런 아이디어 기획 분야의 공모전의 경우 아무래도 하드웨어의 제약이 덜하다 보니​일반 학생이나 직장인의 접근성이 괜찮은 편이라 TAVE 내에서 참가 요청이 있었다.(데이터를 주고 Accuracy를 최대한 끌어올리라는 공모전의 경우에는 결국 하드웨어 싸움으로 가는 경우가 많아 접근하기가 어렵다)(데이터 분석을 위한 서버나... AWS, GCP가 얼만디...)​특구재단에서도 이 공모전에 대해 진심인 듯하고, 사회적인 분위기 또한 AI를 계속 띄우고 있으니아마 이 공모전은 제4회 제5회 등등 지속되지 않을까 싶다. 그래서 우리의 아이디어는? 시각 정보와 음성 정보 둘 다 활용한 영상 요약!공모전 발표 자료에서의 발제 자료 '짧은 동영상이 흥한다'공모전에서 우리가 주목했던 사회적 문제와 아이디어는""짧은 영상이 온라인에서 주목받는 데에 비해, 중소 크리에이터의 경우 인적, 자본적 자원 활용의 한계로 이런 트렌드에 뒤처지고 있다. 중소 크리에이터에 대한 지원을 위해 '자동화된 그리고 좀 더 나은 성능의 영상 요약 알고리즘'을 제공했다""였다.​여기서 사회적 문제 부분은 충분히 팩트로 설명할 수 있지만가장 중요한 '좀 더 나은 성능의 영상 요약 알고리즘'을 실현할 수 있어야제출할 만한 수준의 결과물로 만들 수 있다. 아이디어 요약 : 시각 정보와 음성 정보 둘 다 고려한 영상 요약기존 연구의 경우 Audio나 Visual Information만을 활용한 Video Summarization Algorithm이 많았다.그럼 이 둘을 합한다면? 좀 더 정확한 영상 요약이 되지 않을까라는 생각이 들었다.​그렇게 해서 Visual Summarization의 경우 'CLIP-It! Language-Guided Video Summarization' 논문을 주로 참고했고Audio(Speech) Summerization의 경우 'ETRI  오픈 API 활용 사례 공모전 가작의 AI CARE' 깃허브를 참조하여Visual information 그리고 Audio information 둘 다 활용하여 좀 더 정확도가 높은 새로운 Video Summerization 모델을 만들어보았다.(모두 아래에 링크해두었습니다.)​프로세스는 약간 복잡하다.Video로부터 Caption Information를 뽑아낸 후, 이 Caption을 바탕으로 1차적인 요약 정보를 추출​하고Audio 혹은 Speech로부터 Emotion Analysis을 시행한다. 이 Emotion에 일정 점수를 부여하여 또 다른 1차적인 요약 정보를 추출한다.그리고 이 둘을 조합하여 최종적인 영상 요약을 시행한다.​이걸 구현하면서 알게 되었던 AI Video Captioning 자동 영상 자막 생성, 매우 신기했었다.그리고 Speech Emotion Recognition(SER) 또한 실제로 꽤 맞아떨어지는 편이라 이 역시 놀라웠다.​(아래는 우리의 시현 영상)  나름 이 정도 정확도면 괜찮지 않을까?Video Summarization 관련 논문 : CLIP-It! Language-Guided Video Summarization CLIP-It! Language-Guided Video SummarizationA generic video summary is an abridged version of a video that conveys the whole story and features the most important scenes. Yet the importance of scenes in a video is often subjective, and users should have the option of customizing the summary by using natural language to specify what is importa...arxiv.org Speech Emotion Recognition(SER) 참고 ETRI AI 나눔음성 감정인식 데이터셋 등록자 성정환 등록일 (수정일 ) 2021-11-16 22:40 (2021-11-21 23:41) 조회수 711 접근 횟수 54 추가업로드 가능 좋아요 5 내용 토론장 (2) 활동내역 Description 1. 음성 감정인식 이란? 음성 감정인식 은 사용자의 음성 데이터를 분석하여 감정을 판단하는 기술이다. 음성을 분석하는 방법에 있어서 여러가지 방법이 있는데, 음성을 텍스트로 전...nanum.etri.re.kr ​ 앞으로 기록 정리할 내용들공모전 수상도 중요하지만, 공모전 준비하면서 얻은 지식들과 정보들을 정리하는 것 또한 중요하다.앞으로 7개 정도의 글을 통해 이 아이디어를 검증하면서 사용했던 모델과 스킬을 정리하고자 한다.나는 이번 프로젝트에서 음성 분석 쪽을 맡았기에, 그 부분을 좀 더 중점적으로 기록할 예정이다.​그럼 기록을 시작해보자. ​[1편]  공공 데이터 찾고 활용하기 (작성 중)[2편]  데이터 분포 확인 (작성 중)[3편]  그래프 범례에 한글 출력하기 (작성 중)[4편]  Waveplot과 Spectrogram (작성 중)[5편]  음성 데이터 가공 (작성 중)[6편]  Speech Emotion Recognition(SER) 음성 감정 인식 모델 (작성 중)[7편]  moviepy와 pydub (작성 중) "
영문 기사 - ChatGPT Launches First-Ever App On IPhones ,https://blog.naver.com/winnersheart/223106113307,20230519,"ChatGPT Launches First-Ever App On IPhones​TOPLINE OpenAI announced Thursday that its wildly popular AI chatbot, ChatGPT, is available for download on iPhones—which the company hopes will significantly expand access to the chatbot even as concerns grow about the impacts of the rapid expansion of AI products.​KEY FACTSThe app’s rollout is starting in the U.S before expanding to other countries in the coming weeks, according to OpenAI.​The company said the Android version of ChatGPT is “coming soon,” but did not share an exact date or time range for the launch.​The app is free and will synchronize history across users’ devices—allowing web search data to be transferred between iPhones and desktops.​The iOS app will feature voice input through OpenAI’s open-source speech-recognition system, Whisper.​Apple’s app store previously had a swath of ChatGPT alternatives, knockoffs and clones that scammed users and linked them to malware.​KEY BACKGROUNDOpenAI’s ChatGPT is one of several AI chatbots in a market filled with competing software from companies like Google, Microsoft and Amazon. Microsoft’s Bing AI was available on the app store prior to ChatGPT’s iOS launch—with the earliest version arriving back in October 2022. The AI chatbot race has garnered significant attention from the public and concern from lawmakers, who recently held a hearing on AI that featured OpenAI CEO Sam Altman. In a nearly three-hour-long discussion on AI’s potential dangers and regulatory pathways, Altman pitched the creation of a new federal agency with the authority to issue and revoke licenses for AI technology.​SURPRISING FACTA UBS study from February found that ChatGPT was the fastest-growing consumer application ever. “In 20 years following the internet space, we cannot recall a faster ramp in a consumer internet app,” a UBS analyst noted in the study.​BIG NUMBER$90 billion. That’s how much UBS expects the AI hardware and services market to be worth by 2025.​TANGENTNumerous school districts across the country banned ChatGPT over concerns it could promote cheating on assignments, but New York City Public Schools—the largest school district in the country—reversed course on its ban Thursday. New York City Public Schools Chancellor David Banks said of the policy switch: “While initial caution was justified, it has now evolved into an exploration and careful examination of this new technology’s power and risks.”​​Antonio Pequeño IVAntonio Pequeño IV is a breaking news reporter based in Los Angeles. Prior to joining Forbes, Antonio was a reporter with the San Fernando Valley Business Journal and Los Angeles Business Journal covering the local finance and biotech sectors. He is a graduate of CSU San Marcos and the University of Southern California. Tips: apequeno@forbes.com | Twitter @pequeno04​​ChatGPT, iPhone에서 최초의 앱 출시​TOPLINE OpenAI는 목요일 인기 AI 챗봇인 ChatGPT를 iPhone에서 다운로드할 수 있다고 발표했습니다. 회사는 AI 제품의 급속한 확장의 영향에 대한 우려가 커지는 상황에서도 챗봇에 대한 액세스가 크게 확대되기를 희망합니다.​주요 사실OpenAI에 따르면 앱의 출시는 미국에서 시작하여 앞으로 몇 주 안에 다른 국가로 확장될 예정입니다.​이 회사는 ChatGPT의 안드로이드 버전이 ""출시 예정""이라고 말했지만 정확한 출시 날짜나 시간 범위는 공유하지 않았습니다.​이 앱은 무료이며 사용자 기기 간에 기록을 동기화하여 iPhone과 데스크톱 간에 웹 검색 데이터를 전송할 수 있습니다.​iOS 앱은 OpenAI의 오픈 소스 음성 인식 시스템인 Whisper를 통한 음성 입력 기능을 제공합니다.​이전에 Apple의 앱 스토어에는 사용자를 속이고 멀웨어에 연결하는 수많은 ChatGPT 대안, 모조품 및 복제품이 있었습니다.​주요 배경OpenAI의 ChatGPT는 Google, Microsoft 및 Amazon과 같은 회사의 경쟁 소프트웨어로 가득 찬 시장에서 여러 AI 챗봇 중 하나입니다. Microsoft의 Bing AI는 ChatGPT의 iOS 출시 이전에 앱 스토어에서 사용할 수 있었으며 가장 초기 버전은 2022년 10월에 출시되었습니다. AI 챗봇 경주는 최근 OpenAI CEO Sam Altman이 참석한 AI에 대한 청문회를 개최한 국회의원들로부터 상당한 관심과 우려를 불러일으켰습니다. AI의 잠재적 위험과 규제 경로에 대한 거의 3시간 동안의 토론에서 Altman은 AI 기술에 대한 라이선스를 발급하고 취소할 권한을 가진 새로운 연방 기관의 창설을 발표했습니다.​​놀라운 사실2월의 UBS 연구에 따르면 ChatGPT는 가장 빠르게 성장하는 소비자 애플리케이션이었습니다. UBS 분석가는 연구에서 ""인터넷 공간 이후 20년 동안 소비자 인터넷 앱이 더 빨라진 것을 기억할 수 없습니다.""라고 말했습니다.​​큰 숫자900억 달러. 이것이 UBS가 2025년까지 AI 하드웨어 및 서비스 시장의 가치가 있을 것으로 예상하는 금액입니다.​​접선전국의 수많은 학군은 과제 부정 행위를 조장할 수 있다는 우려로 ChatGPT를 금지했지만 미국에서 가장 큰 학군인 뉴욕시 공립학교는 목요일 금지 방침을 취소했습니다. 뉴욕시 공립학교의 데이비드 뱅크스(David Banks) 교육감은 정책 전환에 대해 이렇게 말했습니다. ""초기 주의는 정당했지만 이제는 이 신기술의 힘과 위험에 대한 탐구와 신중한 조사로 발전했습니다.""​​https://www.forbes.com/sites/antoniopequenoiv/2023/05/18/chatgpt-launches-first-ever-app-on-iphones/?sh=4c046f39701c ChatGPT Launches First-Ever App On IPhonesThe company said the Android version of ChatGPT is “coming soon.”www.forbes.com ​ "
"딥러닝, 텐서플로 응용 분야 ",https://blog.naver.com/silver9028/223073337793,20230413,"# 딥러닝 알고리즘의 주요 응용 분야​: Computer Vision  컴퓨터가 인간의 시각 기능을 수행할 수 있도록 하는 방법을 연구하는 분야​: Natural Language Processing, NLP  컴퓨터가 인간의 언어 처리 기능을 수행할 수 있도록 하는 방법을 연구하는 분야​: Speech Recognition  컴퓨터가 인간의 음성 인식 능력을 수행할 수 있도록 하는 방법을 연구하는 분야​: Game  게임 환경을 이용해서 인공지능 기술 발전을 연구하는 분야​: Generative Model  학습 데이터의 분포를 학습해서 학습한 분포로부터 새로운 데이터를 생성하는 방법을 연구하는 분야​​# 컴퓨터 비전 (Computer Vision)​: 컴퓨터 비전은 인간의 시각과 관련된 부분을 컴퓨터 알고리즘을 이용해서 구현하는 방법을 연구하는 분야임  컴퓨터 비전 문제를 풀기 위해선 딥러닝의 여러 구조 중 CNN이 많이 사용됨 ​  이미지 분류Image Classification, Semantic Image Segmentation, 물체 검출Object Detection 등이  컴퓨터 비전의 대표적인 문제들임, 아래 그림은 이미지에서 물체 부분을 검출해내는 물체 검출 예제를 보여줌 ​​# 컴퓨터 비전의 응용 사례 - Tesla Autopilot​: 테슬라 자동차는 8개의 카메라와 12개의 초음파 센서를 통해 주변 물체를 감지함  또한 자체 제작한 하드웨어를 통해 빠른 딥러닝 연산을 수행함 ​​# 자연어 처리 (Natural Language Processing, NLP)​: 자연어 처리는 인간의 언어와 같이 자연어Natural Language로 표현된 언어를  컴퓨터가 이해할 수 있는 형태로 만드는 방법을 연구하는 학문임  자연어 처리를 위해서는 RNN 구조가 많이 사용됨​  문장 분류Text Classification, 이미지 캡셔닝Image Captioning, 기계 번역Machine Translation, 챗봇Chatbot 등이  자연어 처리의 대표적인 문제들임, 아래 그림은 딥러닝 기법을 이용해 중국어 문장을 영어 문장으로 번역하는  NMT(Neural Machine Translation) 기법을 보여줌 ​​# 자연어 처리 응용 사례 - Google Duplex​: 구글의 최신 NLP 연구 결과를 토대로 인간과 자연스럽게 대화를 수행할 수 있는 인공지능 기술을 개발하였음  구글에서는 이를 원하는 조건으로 예약을 수행할 수 있도록 도와주는 음성 가상 비서인  Google Duplex라는 서비스로 공개하였음 ​​# 음성 인식 (Speech Recognition)​: 음성 인식은 음성 데이터가 표현하는 문장이 무엇인지를 인식하는 문제임  소리를 글자로 바꿔준다고 하여 STT(Speech-To-Text)라고도 불림​  가정용 인공지능 스피커, 자율주행차에 내장된 음성 인식 시스템 등이 음성 인식의 대표적인 응용 분야임  아래 그림은 음성 인식 기법을 이용해서 가상 비서Virtual Assistant를 구현한  애플 Siri, 구글 Now, 마이크로소프트 Cortana 서비스를 보여줌​  음성 인식을 사용할 경우 자유자재로 움직일 수 있는 상태에서 컴퓨터에 명령을 내릴 수 있기 때문에  음성 인식은 컴퓨터와 상호 작용할 수 있는 차세대 인터페이스로써 큰 주목을 받고 있음 ​​# 음성 인식 응용 사례 - 인공지능 스피커​: 음성 인식 기술을 이용한 대표적인 응용 사례로는 인공지능 스피커가 있음  Google Home, Amazon Alexa 등의 인공지능 스피커는 사용자의 음성을 인식하고,  사용자의 질문에 대해 적절한 응답을 수행해줌 ​​# 게임 (Game)​: 게임은 인공지능 연구 역사의 초기부터 인공지능의 성능을 측정하기 위해서 널리 사용되었음  2016년에 바둑을 플레이하는 인공지능 알파고Alphago가 인간 최고 플레이어인 이세돌 9단을 꺾으면서  인공지능의 위력을 만천하에 증명했음  하지만 바둑 이전에 조금 더 경우의 수가 작은 체스의 경우  이미 1996년에 인공지능 체스 프로그램인 딥블루Deep Blue가 인간 최고의 체스 플레이어였던 개리 카스파로프를 꺾었음​  게임은 현실 세계를 기반으로 환경을 디자인했기 때문에 현실 세계 문제의 축소판으로 볼 수 있음​  게임 인공지능을 구현하기 위한 대표적인 머신러닝 알고리즘은 강화 학습임  최근에는 강화 학습과 딥러닝을 결합한 DQN 기법이 많은 주목을 받아 왔고, 알파고에도 DQN 기법이 사용되었음​  알파고를 만든 딥마인드 사에서는 최근에 바둑을 넘어  블리자드 사와 협력하여 스타크래프트를 플레이하는 인공지능을 연구하고 있음  따라서 빠른 시간 내에 인간 최고의 프로게이머를 이기는 인공지능 스타크래프트 플레이어가 등장할지도 모름​​# 게임 응용 사례 - OpenAI Five​: 게임 분야의 대표적인 최신 응용 사례로는 OpenAI Five가 있음  OpenAI Five는 강화 학습에 기반한 알고리즘을 통해서 복잡한 5 vs 5 전투 게임인 Dota2에 대해서  세계 최고의 플레이어들을 상대로 승리를 거두었음 ​​# 생성 모델 (Generative Model)​: 생성 모델은 학습 데이터의 분포를 학습해서 학습한 분포로부터 새로운 데이터를 생성하는 기법임​  2014년에 GAN(Generative Adversarial Networks) 구조가 발표된 이후  딥러닝을 이용한 생성 모델 기법이 급속도로 주목받게 되었음​  학습 데이터의 양을 늘려서 분류기의 성능을 높이는 데이터 증대Data Augmentation 기법 등에  생성 모델 기법을 응용할 수 있음​  아래 그림은 딥러닝을 이용한 생성 모델 구조 중 하나인 BEGAN을 이용해서  컴퓨터가 새로운 얼굴 데이터를 생성한 모습을 보여줌 ​​# 생성 모델 응용 사례 - Deepfake​: 생성 모델의 대표적인 최근 응용 사례로는 Deepfake 기술이 있음  Deepfake 기술은 생성 모델 기술에 기반하여 원본 영상의 얼굴을 자연스럽게 다른 사람의 얼굴로 대체할 수 있음  인공지능 기술의 부작용으로 Deepfake 기술에 기반한 가짜 뉴스 생성 등으로 인한 사회적 혼란이 우려되고 있음 "
"네이버 클로바 인공지능 스피커, 그 속에는 어떤 기술이 쓰였을까? ",https://blog.naver.com/n_cloudplatform/222050691430,20200804,"​안녕하세요, 네이버 클라우드 플랫폼입니다. 귀여운 클로바 인공지능 스피커, 다들 하나씩 있으시죠?(나만 없어…)  오늘은 이 클로바 인공지능 스피커에 담겨있는네이버 클라우드 플랫폼의 핵심 기술력,클로바 음성인식 기술인CSR (Clova Speech Recognition), CSS (Clova Speech Synthesis)상품을 소개하고자 포스팅을 작성하게 되었습니다!  CSR은 사람의 목소리를 인식하여 작동하는비서 애플리케이션, 챗봇, 음성 메모 등의 서비스를 만들 때활용할 수 있는 음성 인식 API 서비스입니다. 네이버가 뭡니까! 바로 국내 1위 검색엔진 아닙니까!! (^^;)  이런 네이버가 수 년간 연구한 기술력이 담겨있기 때문에,뛰어난 한국 음성 인식률을 자랑하는 기술입니다. 한국어 외에도,영어, 일어, 중국어를 지원하기 때문에글로벌 서비스를 생각 중이거나,해외의 고객들에게 음성인식 서비스를 제공해야 하는 업체에게도 제격입니다!  CSR은 기계가 사람의 말을 알아듣는 ‘듣기’의 측면이었다면, 그 반대인 '말하기'는 어떨까요?   클로바 스피커가 사랑받는 이유는이런 ‘듣기’ 뿐만 아니라 ‘말하기’도사람처럼 자연스럽기 때문인데요!이는 바로 클로바 음성 합성 API인 CSS입니다. ​CSS를 통해, 사용자들은텍스트를 입력하기만 하면!성우의 목소리를 바탕으로자연스럽게 말을 하는듯이 주어진 텍스트를 재생하기 때문에,음성 안내 시스템, 뉴스/책읽기 서비스 등에 활용하실 수 있습니다. 네이버 클라우드 플랫폼의 이런 서비스를이용해서 괄목할만한 성과를 이룬 기업이 있었는데요,바로 웅진북클럽입니다! 웅진북클럽은 아이들에게 책을 읽어주고함께 대화하는 서비스 ‘북클럽프렌즈’를CSR과 CSS를 활용하여 인터랙티브 북 서비스로 운영하고 있습니다. 자세한 고객사례는 아래 링크를 참고하세요! [이렇게 사용하세요!] 에듀테크[EduTech]: 교육과 클라우드가 만나면 생기는 일들​안녕하세요!네이버 클라우드 플랫폼 입니다.​​여러분들은 학교 다닐 때 수학을 좋아하셨나요?수포자 (...blog.naver.com  ​지금까지 네이버 클라우드 플랫폼의클로바 기반 음성인식 클라우드 상품을알아보았습니다.  두 상품의 API 모두 머신러닝 기반이기 때문에,네이버 서비스에서 얻어지는 풍부한 데이터를 통해 지금도 지속적으로 학습하고, 발전하고 있습니다. 매일매일 똑똑해지는 CSR과 CSS! 네이버 클라우드 플랫폼에서 지금 사용하실 수 있습니다.  클로바 이외에도 여러가지 AI 기반 상품들을제공하고 있으니, 자세한 설명은 아래 링크에서 확인하세요!​ ​ "
자연어 음성처리 ,https://blog.naver.com/rktlf6956/223038492642,20230308,"자연어: 인간이 사용하는 언어NLP : 인공지능의 한 분야(딥러닝 + 텍스트마이닝+ Ling~~ 짬뽕) 인간처럼 되야함 ==> 인간이 사용하는 언어를 처리하자. NLU(Natural Language Understanding) 자연어 이해 : 자연어 형태의 문장을 이해하는 기술사람-기계 상호작용이 필수인 경우 NLU는 핵심 기술NLG(Natural Language Generation) 자연어 생성 : 자연어 문장을 생성하는 기술​자연어처리 문제(problem) : 애매모호함!! ambiguity 한 단어에 해석이 너무 많음​자연어 처리 단계 semantic analysis : chasing이 어떠한 의미인지 찾아내는것 semantic 요소 = scared (boy가 scared하다 추론)pragmatic analysis : 물지못하게 막아야함품사, 객체(이름, 날짜, 장소  매칭해서 기억 MER ), coreference: Trump = him​Lexical Analysis : 문장 분할! !, . , ? 로 분리토큰화: 특수문자 이모티콘 ^^ 각자 의미를 가짐out of bucket~~ (사전에 없는 단어 = 신조어)형태소 분석: 띄어쓰기 등등 ex) 아버지가방에들어가신다​형태소 분석기 메케브로 실습 감정분석 - Word2Vec: word를 vector로 바꾼다. 이걸로 뭐함? 사람은 연관성에 따라 생각함 단어를 지정된 차원으로 줄여서 시각화를 함 => 단어 사이에 유사성을 보임syntactic analysis : 구문 분석​Machine Translation 신경망 나온 이후부터 잘 되기 시작함 구글이 번역에 이용하기 시작 GNMT (Google's Neural Machine Translation System.)Zero-Shot Translation 언어는 다르더라도 비슷한 특성이 존재한다. 이것을 이용해 번역​Transformer로 뭘 이용할 수 있나? Word Prediction with Transformers다음단어 뭐 올지 예측Using transformers to predict next word and predict <mask> wordMask Language Modeling ​  BERT vs GPT vs ELMoBERT : 오만데 다 사용​KoCLIP : 텍스트로 이미지 찾기 실습 생각 !! 응용 하거나 SNS 특화모델 도전​Speech Recognition 음성 인식 LSTMs (1997)​Google Duplex​​​​​​ "
(곧 다가올 미래의) 일잘러의 주머니 ,https://blog.naver.com/seng890211/223061641282,20230401,"​​AI 드로잉 사이트 : ""요청어""를 적으면, 원하는 그림을 그려준다 ​https://www.midjourney.com/home/ MidjourneyAn independent research lab exploring new mediums of thought and expanding the imaginative powers of the human species.www.midjourney.com https://www.bing.com/images/create/ BingBing은 지능적인 검색 기능은 사용자가 원하는 정보를 빠르게 검색하고 보상을 제공합니다.www.bing.com 제플린 필터 기능 만들기 ​http://sanghun.xyz/2017/01/apache-zeppelin%EC%9D%98-dynamic-form-%EC%A0%95%EB%A6%AC/ apache zeppelin의 dynamic form 정리dynamic form을 이용하면 충분히 쓸만한 custom dashboard를 만들 수 있다.sanghun.xyz 나 대신 글/카피를 써줘요✏️ - 뤼튼https://wrtn.ai/ 뤼튼 - AI 콘텐츠 생성 플랫폼뤼튼 - AI 콘텐츠 생성 플랫폼 | 고객의 반응을 얻고 매출을 늘리는 경쟁력 있는 카피, 강력한 AI와 함께 10배 빠르게 완성하세요.wrtn.ai 어색한 내 영어를 고쳐줘 _ https://www.deepl.com/write DeepL Write: AI 작문 도우미DeepL Write는 작문을 위한 보조 도구입니다. 명료하고 정확한 문장을 오류 없이 작성할 수 있습니다. 지금 무료로 사용해 보세요.www.deepl.com 나 대신 PDF 를 읽고 분석해줘 📑 - ChatPDFhttps://www.chatpdf.com/ ChatPDF - Chat with any PDF!ChatPDF is the fast and easy way to chat with any PDF, free and without sign-in. Talk to books, research papers, manuals, essays, legal contracts, whatever you have! The intelligence revolution is here, ChatGPT was just the beginning!www.chatpdf.com . ChatPDF 소개PDF 문서는 우리 삶의 필수적인 부분입니다. 우리는 작업 할당에서 연구 논문, 개인 문서에 이르기까지 모든 것을 사용합니다. 그러나 때로는 큰 PDF 파일 내에서 필요한 정보를 찾는 것이 어려운 작업이 될 수 있습니다. ChatGPT는 사용자가 고유한 방식으로 PDF 문서와 상호 작용할 수 있는 혁신적인 도구입니다.ChatGPT의 고급 AI 기술을 통해 사용자는 마치 사람과 대화하는 것처럼 모든 PDF 문서와 채팅할 수 있습니다.ChatGPT을 통해 사용자는 정보를 추출하고, 질문하고, 즉시 답변을 얻을 수 있으므로 정기적으로 PDF 문서로 작업하는 모든 사람에게 완벽한 솔루션입니다. ​2. 주요 기능- ChatGPT 기술로 구동되는 AI 챗봇- PDF 파일의 모든 단락에 대한 시맨틱 인덱싱- 대용량 PDF 파일에서 정보를 추출하고 질문에 답하는 기능 3. 사용 방법- 원하는 장치에서 https://www.chatpdf.com/을 방문하십시오.- 채팅할 PDF 파일을 업로드합니다.- AI 챗봇과 채팅하면서 질문을 하시면 PDF 파일에서 정보를 추출하여 답변을 해줍니다.​저의 부족한 글솜씨로 소개드리는 것 보다 인터넷에 이해하기 쉽게 정리된 글이 있어서 인용했습니다. 😂​​마이크로 ChatGPT와 대화하기 - Talk-to-ChatGPT[추천 지식/기술 : 마이크로 ChatGPT와 대화하기 - Talk-to-ChatGPT]​1. 소개챗GPT와 마이크를 이용해서 대화하고 싶을때 이용할 수 있는 크롬 익스텐션입니다. - 마이크로 이야기하면 이것이 텍스트로 변환되어서 질문으로 입력되고(Voice->Text),- ChatGPT가 답을 하면, 이것이 음성으로 변환되어 스피커로 나옵니다.(Text->Voice)ㄴ설치는 여기로 https://chrome.google.com/webstore/detail/talk-to-chatgpt/hodadfhfagpiemkeoliaelelfbboamlkㄴ익스텐션을 설치하고, chat.openai.com/chat에 접속하면 화면 우측 상단에 바로 실행창이 뜨면서 작동됩니다ㄴ 간단한 세팅(대답하는 언어 세팅, 인식하는 언어 세팅)만 하면 한글이나 영어로 대화를 주고 받을 수 있습니다.​2. 세팅방법한글로 대화를 하고 싶으면,(세팅을 아래와 같이)- AI voice and language: Google 한국의 (ko-KR)- Speech recognition language: 한국어 - ko -KR​영어로 대화를 하고 싶으면,(세팅을 아래와 같이)- AI voice and language: Google US English (en-US)- Speech recognition language: English - en-US​3. 활용방법상황1: 영어 스피킹 공부ㄴ 프롬프트에 ""나는 7살 한국 어린이다. 너랑 영어로 대화하면서 영어 공부하고 싶어""라는 식으로 입력하면 대화하면서 영어 스피킹 공부할 수 있어요. (필리핀 전화영어 대체 가능?)​상황2: 소개팅 상황(ㅎㅎ)ㄴ 프롬프트에 ""나는 xx살 한국 남자이고, xxx하는 분과 소개팅하는 상황이야. 어떻게 처음 어색함을 풀면서 자연스럽게 대화할 수 있을까?"" (소개팅 가상 시뮬레이션)​상황3: 잡인터뷰 상황(ㅎㅎ2)ㄴ 프롬프트에 ""나는 프론트엔드 엔지니어이다. 카카오모빌리티 입사를 위한 인터뷰를 하고 싶다. ~~~""​등등...첫 프롬프트에 상황을 잘 설명하면 더욱 상세한 상황에서 대화가 가능해요.​​​물류 트랜드와 다양한 분석이 보고 싶을때는 비욘드엑스 https://beyondx.ai/https://beyondx.ai/ 비욘드엑스유통·물류 콘텐츠 생산과 구독 네트워크로 소비 가치와 연결을 만드는 '비욘드엑스'는 네이버 프리미엄 콘텐츠 채널 '커넥터스' 운영사입니다.beyondx.ai ​ "
[AI-SPARK 6편] 케라스 모델 학습 콜백함수와 학습결과를 그래프와 표로 출력하기 ,https://blog.naver.com/ansrl23/222952796902,20221212,"지금까지 여러 기법과 함수, 코드를 통해 본격적인 학습을 위한 준비작업을 진행했다.파이썬으로 학습시키는 코드를 짤 때 매번 느끼는 거지만 참...학습 전까지의 코드량이 약 90% 정도 된다면, 막상 학습은 기껏해야 1, 2%, 학습 후 시각화와 결과 리뷰 코드가 8, 9%의 분량밖에 되지 않는다.그럼에도 불구하고 이 1, 2%의 코드량을 가지고 매달려야 하는 경우가 종종 있다.모델이 학습하는 방법과 정도 그리고 기간에 대해 정해줘야 내가 원하는 학습이 되기 때문이다.​마치 아이를 그냥 집에서 내보내면서 놀고와라라고 하는 것보다아이에게 몇시부터 몇시까지, 친구들과 축구를 하면서, 다치지 않게 조심히 놀고와라라고 하는 것이부모가 바라는 경험을 아이가 쌓을 수 있도록 하는 것처럼 말이다.​파이썬 모델 학습에서 그런 역할을 해주는 함수가 바로 '콜백함수(Callback)'이다. 모델 학습에서의 콜백함수(Callback Function)'콜백함수'의 정의는 '함수 호출 시에 알고리즘을 입력 인자로 전달하고, 호출받은 함수에서는 전달받은 알고리즘을 호출하여 사용하는 역할'이라고 하고 있다. 어렵다 어려워 (출처 : 위키독스)파이썬 모델 학습에서도 콜백함수는 학습 함수에 전달되어 '조건'처럼 쓰이게 된다.​파이썬 모델 학습에서 쓰는 콜백함수​는 아래의 페이지에서 잘 정리를 해주고 있다. (Keras 기준) Callbacks - Keras DocumentationKeras Documentation Home Why use Keras Getting started Models Layers Preprocessing Losses Metrics Optimizers Activations Callbacks Usage of callbacks 콜백 BaseLogger TerminateOnNaN ProgbarLogger History ModelCheckpoint EarlyStopping RemoteMonitor LearningRateScheduler TensorBoard ReduceLROnPlateau CSV...keras.io 다만 위 페이지에서는 콜백함수를 전부다 소개하다보니 양이 많은데짧은 경력이지만 모델 학습에서 주로 본 콜백함수는 'ReduceLROnPlateau'와 ​'EarlyStopping'이다.그리고 모델 학습에 있어 콜백함수를 쓰려면 꼭 사전에 'from keras.callbacks import ReduceLROnPlateau 혹은 EarlyStopping'을 선언하고 써야 한다. 케라스 모델 학습 콜백함수 불러오기콜백함수 'ReduceLROnPlateau'는 학습 결과지표의 개선 정도에 따라 Learning Rate의 크기를 조절하는 콜백함수이다.하이퍼파라미터에는 monitor와 factor, verbose, patience, min_lr 등이 있다.monitor : 관찰할 학습결과지표factor : 학습 속도를 얼마나 줄일지 그 크기verbose : 0 - 자동 적용 / 1 - 최신화 메시지patience : 학습결과지표의 연속된 미개선에 대해 몇 번 연속 미개선까지 기다릴지 그 횟수                       (예를 들어 patience가 3이라면, 3번 연속으로 개선이 없다면 learning rate에 factor를 적용하여 낮춤)min_lr : 최소 적용 학습속도 케라스 모델 학습 콜백함수 ReduceLROnPlateau콜백함수 'EarlyStopping'은 ReduceLROnPlateau보다 인내심이 적은 편이다.학습결과지표에 대해 어느 정도 선까지 개선이 없다면 학습을 멈춰버리는 콜백함수이다.하이퍼파라미터에는 monitor, min_delta, patience, verbose, mode, baseline, restore_best_weights가 있다.(하이퍼파라미터에 대한 설명은 케라스 홈페이지의 그림으로 대체하겠다.) 케라스 모델 학습 콜백함수 EarlyStopping (출처 : keras.io)콜백함수 ReduceLROnPlateau를 쓰면 이렇게 내가 설정한 100번 동안 알아서 학습률을 조정하면서 각 차수마다 학습된 결과를 보여준다. 학습결과가 물론 마음에 들지 않을 수 있다.이러한 경우에는 모델 학습에 사용된 하이퍼파라미터를 튜닝해야 한다.조금씩 조금씩 바꿔가며 돌리는 방법도 있겠지만 (일명 노가다라고 하죠)그것보다는 그리드서치로 현명하게 최적의 하이퍼파라미터를 찾아나가는 것이 좋을 듯하다. 파이썬 모델 학습 결과 그래프와 표로 출력하기학습은 다 했고, 이걸 이제 그래프로 그려서 학습 결과를 보여줄 준비를 한다.학습 결과를 그래프로 그리기 위해서는 matplotlib library를 이용한다. (import matplotlib.pyplot as plt) 그리고 베이스라인으로 이용한 모델에서 매우 감사하게도 그래프를 그리는 코드를 만들어주셨기에 해당 코드를 이용했다.먼저 사이즈를 정해주고, 어떤 데이터가 표현될지, 제목과 범례를 어떻게 할지, 그리고 x축 label 이름을 어떻게 표현할지 등등을코드 한 줄 한 줄마다 지정해 준 후 'plt.show()'함수를 통해 그래프를 그려라라고 명령해준다고 보면 된다. 학습 결과 그래프로 그리는 코드 (matplotlib 라이브러리 활용)학습결과 그래프 표현 - 이미 epoch 60 즈음부터 학습 과정이 수렴되었음을 알 수 있다.그리고 모델 학습 결과에 대해 f1 score, accuracy 등 학습결과지표들도 숫자로 확인해볼 필요가 있다.그때에는 classification_report라는 함수를 이용하면 precision, recall, f1 score, support에 대해서각 feature별로 그리고 총괄적인 결과를 보여준다. classification report 확인 결과, 나름 accuracy도 90% 이상, f1 score도 0.9 이상의 좋은 결과를 보여준다.실제 상황에서도 쓸 가치가 있다는 말이다.그럼 당연히 실제 동영상에서도 감정 분석을 해봐야지 않겠는가.다음 글에서는 실제 동영상에 대해 감정 분석을 하는 과정에 대해 (동영상 전처리와 적용)​리뷰를 해보도록 하겠다.​​※ 베이스라인으로 활용한 코드 (SER : Speech Emotion Recognition)    물론 이전 과정에서도 많은 도움이 되었지만 특히 학습 부분, 리뷰 부분에서 많은 도움이 되었다.    베이스라인 코드가 있었기에 이런 결과물을 만들 수 있었다. ETRI AI 나눔음성 감정인식 데이터셋 등록자 성정환 등록일 (수정일 ) 2021-11-16 22:40 (2021-11-21 23:41) 조회수 805 접근 횟수 60 추가업로드 가능 좋아요 5 내용 토론장 (2) 활동내역 Description 1. 음성 감정인식 이란? 음성 감정인식 은 사용자의 음성 데이터를 분석하여 감정을 판단하는 기술이다. 음성을 분석하는 방법에 있어서 여러가지 방법이 있는데, 음성을 텍스트로 전...nanum.etri.re.kr ​ "
모든 것을 기억하는 법 (Rewind) ,https://blog.naver.com/winterholicq/223075254279,20230415,"Rewind app우리는 하루에도 컴퓨터를 이용해서 수십가지 앱을 이리저리 오가며 엄청나게 많은 웹페이지, 도큐먼트, 미팅, 채팅등을 합니다. 이 모든 것을 모두 기억하기란 불가능에 가깝죠. 심지어 어쩌다가 무심코 닫아버린 웹페이지에 중요한 내용이 있었는데 방금전까지 보던 내용을 기억하지 못하곤 합니다. 이 문제를 심각하게 고민한 스타트업이 있습니다. Rewind가 바로 그 주인공입니다.​​Rewind 는 크게 세가지 방식을 통해 우리가 보거나, 말했거나, 들은 것을 모두 찾을수 있게 도와줍니다.컴퓨터(맥 한정) 에서 보거나, 듣거나, 읽은 것들을 모두 레코딩합니다. 이때 레코딩 된 내용은 로컬에 저장되므로 절대로 다른 곳으로 전송되지 않는다고 합니다. 실제로 Speech recognition을 활용해서 미팅중 대화내용을 대화형식으로 저장해서 ‘검색’ 할 수 있게 합니다이렇게 레코딩한 데이터는 굉장히 압축하여 (원본 데이터보다 약 3750배 더 효율적이라고 합니다) 실제로 가장 작은 용량의 맥북을 현재 구매해도 몇년치의 데이터를 컴퓨터 안에 저장할 수 있다고 하네요지원하는 앱에 대한 한정된 앱은 크게 없습니다. 슬랙메시지, 줌 콜, 워드 파일, PPT, 웹사이트, 트위터 앱, 인스타그램, 링크드인 등 소셜까지 모두 레코딩을 통해서데이터를 검색할 수 있게 합니다. Rewind 를 처음 켰을때 아래 나오는 Timeline을 통해 몇분전 혹은 몇시간 전 등 비교적 짧은 horizon에 무슨 일을 했는지 알 수 있습니다.하지만 가장 강력한 기능은 역시 검색기능이죠. 검색하면, 줌미팅, 메시지, 지메일, 워드파일, 웹사이트등 내가 보고 듣고 한 모든것들에 대해서 검색을 해 줍니다. 따로 아카이빙도 필요가 없습니다. 미팅에서 누군가가 화면공유를 한 화면에서 관련 검색어를 찾은 내용입니다. 자동으로 대화내용을 오디오 인식기능을 통해서 대화처럼 만들어 검색가능하게 해주죠. 더 놀라운 것은 이 줌 미팅을 하는동안 내 컴퓨터 화면에서는 어떤것을 하고 있었는지도 볼 수 있습니다!Rewind 의 비전은 “Give human perfect memory” 라고 합니다. 정말 실현될 것 같지 않나요?현재 Rewind는 베타테스터를 모집하고 있습니다. RewindWhat if you had perfect memory? Search or ask Rewind about anything you've seen, said, or heard.www.rewind.ai ​ "
[6주차] CNN ,https://blog.naver.com/chillax0/223104286357,20230517,"Lecture 05 : Convolutional Neural Networks [1] A bit of history.. A. Neural Network : 선형 레이어를 쌓고 그 사이에 비선형 레이어를 추가해 만든 것뉴런은 입력을 받아들이고, 입력에 대한 가중치를 곱해 활성화 함수를 통과시켜 결과를 출력한다. 이때 여러 개의 뉴런이 여러 층(layer)로 구성된 구조를 가진 것을 신경망이라고 한다. 초기에는 단순한 선형 모델로 시작하여 데이터를 학습하면서 가중치를 조정해 복잡한 패턴을 학습할 수 있다. 이 과정에서 주로 역전파(Backpropagation) 알고리즘을 사용이 된다. 역전파 알고리즘은 신경망이 예측한 출력값과 실제 값 사이의 오차를 역으로 전파시켜 가중치를 조정하며 학습한다. B. mode 문제mode 문제는 주어진 데이터 셋에서 가장 빈번하게 나타나는 값을 찾는 문제이다. 주로 범주형 데이터(자동차 모델 등)를 살펴볼 때 중심 경향성을 측정하는데 유용하며, 순서에 기반한 수학적 평균 중앙값을 계산할 수 없는 경우에 사용된다. 예를 들어 NN은 다양한 종류의 자동차를 올바르게 분류하기 위해 “중간 단계의 템플릿”을 학습시켜 최종 클래스 스코어를 계산할 수 있다.   [2] CNN의 역사  A. Mark I Perceptron machinePerceptron(퍼셉트론)은 인공 신경망의 가장 간단한 형태 중 하나이다. 이진 분류 문제를 해결하는데 사용되는 알고리즘으로, 입력값을 받아들이고 각 입력값에 연결된 가중치와의 곱을 계산한 후 이를 활성화 함수에 적용하여 출력값을 생성한다. 일반적으로 다수의 이진 입력값을 받아서 하나의 이진 출력값을 출력한다.예를 들어 퍼셉트론이 습도, 온도, 강수량과 같은 입력값을 받아서 비가 올 것인지 아닌지를 판단하는 등의 주어진 입력값을 통해 어떤 사건이 발생할 것인지 판단할 수 있다. 1957년 Frank Rosenlatt(프랑크 로젠블라트)가 최초로 퍼셉트론을 구현한 Mark I Perceptron machine을 개발했다. 이때의 Perceptron은 지금까지 배운 Wx + b와 유사한 함수를 사용하지만 다음과 같은 차이가 있다. 1) 출력값이 1 또는 0임2) 가중치 W를 업데이트하는 룰이 존재하지만, 당시에는 Backprob라는 개념이 없어 w를 조정하는 것에 불과함B. Multilayer Perceptron Networkperceptron은 초기에 선형 분류 문제에만 사용되었지만 다층 퍼셉트론을 활용하면 비선형 문제도 해결할 수 있다. 다층 퍼셉트론이란, 여러 개의 퍼셉트론을 layer(층)으로 구성한 구조로 hidden layer(은닉층)을 통해 복잡한 패턴을 학습하는 신경망 구조이다.Widrow와 Hoff는 Madaline을 개발해 역전파 알고리즘의 기초를 마련했으나, 정확한 Backprop 알고리즘은 없었다. C. Chain rule과 Update rule1986년, Rumelhart는 역전파 알고리즘을 이용하여 다층 퍼셉트론이 비선형 문제를 해결할 수 있음을 입증했다. 이 알고리즘은 출력과 기대 출력 간의 오차를 역으로 전파하여 각 가중치에 대한 오차 기여도를 계산하고 업데이트한다. 이를 통해 다층 퍼셉트론은 복잡한 패턴을 학습할 수 있게 되었다.여기에서 우리에게 익숙한 chain rule과 update rule을 볼 수 있으며, neural network를 학습시키는 것에 관한 개념이 최초로 정립되었다.  D. DNN의 학습가능성2006년, Geoff Hinton 과 Ruslan Salakhutdinov은 Deep Belief Networks(DBN)를 제안했다. DBN은 비지도학습과 지도학습을 결합한 알고리즘으로, 제한된 볼츠만 머신(RBM)을 쌓아올려 구성되는 신경망 구조이다.이때 RBM은 비지도학습을 위한 확률적 생성 모델로, 입력층과 은닉층만 존재하고, 은닉층 내의 노드들이 서로 연결되지 않는 제한이 있다. 에너지 기반 확률 모델로서, 입력 데이터와 은닉층의 활성화 상태에 대한 에너지를 정의하고, 이를 이용해 데이터의 확률 분포를 모델링해 입력 데이터의 특징을 추출하고 재구성하는 역할을 수행한다. E. First Strong results2010년, Hinton Lab에서는 음향 모델링(acoustic modeling)과 음성 인식(Speech recognition) 분야를 연구했는데 이때 NN이 음성인식 분야에 뛰어나다는 것을 발견했다.2012년, Alex가 ImageNet Classification에서 최초로 NN을 사용했고, ImageNet 분류 성능에서 error를 37.5%로 크게 낮췄다. AlexNet은 8개의 계층으로 구성된 CNN 구조이며 Convolutional layer(합성곱 계층), pooling layer(풀링 계층), ReLu(활성화 함수), dropout(드롭아웃) 등의 요소를 포함하고 있다. 그 당시 혁신적인 방법으로 GPU(그래픽 처리 장치)의 병렬 처리 능력을 활용해 대규모 신경망 모델을 훈련시켰다.  F. CNN이 유명해진 이유 고양이의 뇌에 전극을 꽃고, 자극을 주었을 때 뉴런이 oriented edges와 shapes같은 것에 반응한다는 것을 알아냄피질 내부에 지형적인 매핑(topographical mapping) 존재피질 내 서로 인접해 있는 세포들은 visual field 내어떤 지역성을 띄고 있음  [3] CNN 기존 Fully Connected Layer(선형변환)는 32x32x3 이미지를 1열로 쭉 펴서 w 와 내적을 하여 결과를 내지;만, CNN은 32x32x3의 이미지 데이터의 원형을 보존한 상태로 계산을 수행한다.-> 이때 데이터의 원형을 보존한다는 것은 공간을 보존한다고 이해하면 좋다.​ CNN 에서의 학습 대상인 가중치는 이미지보다 작은  filter이다.단, 필터의 깊이(depth)는 반드시 input의 깊이와 동일해야 한다.​​​ 이렇게 input이 filter를 거쳐서 나온 결과를 activation map이라고 한다.Activation Map은 입력 이미지에서 특징(특정 패턴 or 모양)이 뚜렷하게 드러난 영역을 나타낸다.​​​ 정리CNN은 공간을 보존하기 위해 여러 개의 filter를 사용함필터당 하나의 activation map을 생성필터의 깊이 = input의 깊이​​​​​​​​ "
"[파이썬(Python)][STT] 마이크에 인식된 음성(영어, 한국어 등)을 문자로 바꾸어 주는 프로그램 만들기 동영상 강의/ 강사: 유튜버 '나도코딩' ",https://blog.naver.com/human_intelligence/222922289710,20221108,"안녕하세요, 사람지능입니다.​이웃님들! 마이크로 음성을 입력하면 문자로 변환해 주는 프로그램이 길어 봐야 10줄 정도의 코드로 만들어진다는 사실을 알고 계셨나요?​혹시 배워 보고 싶으신가요? ▶ 아래 동영상 15분33초부터 33분 32초까지는 STT(Speech_to_Text) 실습 강의입니다.​https://youtu.be/WTul6LIjIBA ▶ STT(Speech To Text) 기본 환경 설정은 아래와 같다.pip install SpeechRecognition #음성 인식을 하려면 설치해야 함pip install PyAudio #마이크를 이용하려면 설치해야함 ▶ 아래는 마이크로 들리는 영어 음성을 문자로 바꾸어 주는(speech_to_text) 코드입니다. # 아래는 마이크로 들리는 영어 음성을 문자로 바꾸어 주는(speech_to_text) 코드입니다.​import speech_recognition as srr = sr.Recognizer()with sr.Microphone() as source: print('듣고 있어요') audio = r.listen(source) # 마이크로부터 음성 듣기​try: text = r.recognize_google(audio, language='en-US') #구글 API로 인식( 하루 50회로 제한된다고 함 ) print(text)​except sr.UnknownValueError: print('인식 실패') # 음성 인식 실패한 경우except sr.RequestError as e: print('요청 실패:{0}'.format(e)) # API Key 오류 또는 네트워크 단절 등 ▶ 아래는 위 코드를 실행한 결과입니다. '듣고 있어요'가 보이면 말을 하고 말이 멈추면 인공지능이 인식한 말을 text로 바꾸어 보여줍니다. 아래 노란 줄이 쳐진 부분이 음성 인식 결과물입니다. ▶ 아래는 한국어 음성을 한글로 바꾸어 주는 코드입니다. # 아래는 마이크로 들리는 한국어 음성을 문자로 바꾸어 주는(speech_to_text) 코드입니다.​import speech_recognition as srr = sr.Recognizer()with sr.Microphone() as source: print('듣고 있어요') audio = r.listen(source) # 마이크로부터 음성 듣기​try:    text = r.recognize_google(audio, language='ko') #구글 API로 인식( 하루 50회로 제한된다고 함 ) print(text)​except sr.UnknownValueError: print('인식 실패') # 음성 인식 실패한 경우except sr.RequestError as e: print('요청 실패:{0}'.format(e)) # API Key 오류 또는 네트워크 단절 등 ▶ 아래는 위 코드의 실행 결과입니다. ▶ 마이크가 아니라 파일로부터 음성을 불러오는 경우에는 아래의 코드를 사용합니다. 이때 mp3파일은 사용할 수 없다는 사실을 기억해 주세요. ​▶ 위 동영상 15분33초부터 33분 32초까지는 STT(Speech_to_Text) 실습 강의입니다.​​#파이썬 #Python #STT #speech #text #마이크 #인식 #음성 #영어 #한국어 #문자 #text #프로그램 #코딩 #코드 #code #만들기 #동영상 #강의 #강사 #유튜버 #나도코딩 "
"[상품소개]사람보다 더 사람같은, Clova Premium Voice ",https://blog.naver.com/n_cloudplatform/221816624988,20200220,"​   ​안녕하세요, 네이버 클라우드 플랫폼입니다.​이번 포스팅에서는네이버 클라우드 플랫폼의 Clova Premium Voice, '아라'의 목소리와 함께합니다!​아래의 영상을 재생하여본문 내용을 들을 수 있습니다.  ​ 여러분이 지금 듣고 있는 목소리,누구의 음성일까요?🔊 지하철 안내 음성, ARS 안내 메시지, AI 스마트 스피커의 음성, ​우리 일상의 많은 곳에서 자연스럽게 들을 수 있는 음성들은 모두 정교한 “음성 합성 기술”을 통해 만들어졌습니다. 음성 합성 기술은 이미 다양한 분야에서 활용되고 있는데요, 음성 합성 기술을 활용한다면 전달해야 하는 문구가 바뀌더라도 녹음을 새로 할 필요 없이 간단하게 음성을 생성할 수 있어서 업무 자동화와 비용 절감에 큰 도움이 됩니다.​또한, 운전이나 원거리와 같이 양손 사용이 자유롭지 않은 상황에서 사용자가 음성으로 명령하고, 그 결과를 음성으로 출력 받는 AI 스피커/기기의 기술도 계속해서 발전하고 있습니다. ​  더 강력해진 네이버 클라우드 플랫폼 Clova Premium Voice를 소개합니다 [ncloud 홈페이지: https://www.ncloud.com/product/aiService/cpv] ​네이버 클라우드 플랫폼의 Clova Premium Voice(이하 CPV)는 사람에 가까운 자연스럽고 깨끗한 합성음을 제공하는 음성 합성 API입니다. 기존의 음성 합성 기술은 기계처럼 딱딱한 음성을 제공하는데요, CPV는 실제 사람의 음성과 같이 자연스러운 음성을 제공합니다. 또한 목소리의 높낮이, 속도를 조정할 수 있고 밝은 목소리, 어두운 목소리 등 감정이 반영된 음성을 만들 수 있습니다. ​네이버 클라우드 플랫폼에는 프리미엄 보이스 이외에도 텍스트를 자연스럽게 성우 음성으로 읽어주는 CSS(Clova Speech Synthesis) 서비스가 있는데요, 함께 비교해보고 이용하면 좋을 것 같습니다!​​ Clova Premium Voice는 무엇이 다를까요?✔️ 사람의 목소리와 유사한 음성✔️  맥락에 맞는 자연스러운 음성의 높낮이, 속도, 감정 조절 가능✔️  손쉬운 사용 및 빠른 서비스 적용​CPV는 네이버의 인공지능 기술이 반영된 실시간 음성 생성이 가능한 neural vocoder를 사용하여, 실제 사람의 음성에 가까운 합성 음성을 제공합니다. 그리고 전달하고자 하는 내용 또는 맥락에 가장 적합한 목소리를 생성할 수 있도록 음성의 높낮이, 속도, 감정을 추가적으로 조절할 수 있는 기능을 제공합니다. ​​  [유튜브 캡처 – 네이버 클로바 | 유인나 목소리] 배우 ‘유인나’씨가 직접 클로바 스피커 보이스를 녹음하던 이 영상 기억하시나요? ​기업만의 특색 있는 목소리를 제작하는 경우에도 네이버만의 Speaker Adaptation 기법을 사용하여 짧은 시간의 음성녹음 만으로 고품질의 음성 합성 제작이 가능합니다. 다만, 커스텀 음성 제작은 현재 일반 고객이 바로 사용할 수 있는 기능은 아니며, 사용을 위해서는 추가 협의가 필요한 상품입니다. :)​​ 우리의 생활을 더 편리하게, Clova Premium Voice 활용 사례I. 식당 예약 서비스 구축 [아웃백 미금점 AICALL 도입 사례] 아웃백 미금점에서는 클로바의 음성합성기술을 활용하여 식당 예약 서비스를 구축하였습니다. 기념일이나 회식이 있는 날에는 전화를 통해 예약을 많이 하는데요, 사업주의 입장에서는 바쁜 상황에서 전화 응대도 어렵고, 직원들의 전화 응대 교육도 쉽지 않은 것이 사실입니다. 아웃백 미금점에서는 Clova AiCall을 활용하여 AI 응대자가 전화로 원하는 시간에 예약을 잡아주고, 메뉴 문의와 주문 등 전반적인 예약 관련 응대가 가능합니다. ​ Ⅱ. 동영상 더빙 [클로바 더빙: https://clovadubbing.naver.com/] “클로바 더빙”을 통해 CPV음성 “아라”의 음성을 빠르고 간단하게 활용할 수 있습니다. 동영상 음성 제작 시 유용하게 사용 가능한 서비스인데요, 문장을 입력하면 다양한 화자의 목소리를 들어보면서 감정 및 목소리 속도, 볼륨 조절도 가능합니다. 유튜브 제작이나 기업 영상 제작 시에 활용해 보시는 건 어떨까요?​ Ⅲ. 챗봇 서비스 및 음성 인식 기술과 연계한 AI 비서 [아지냥이 – 대한민국 1등 반려동물 앱] ‘아지냥이’는 반려동물을 위한 음악, 게임, 데일리미션, 버킷리스트, 건강상담 등을 제공하는 모바일 APP입니다. 모바일 커뮤니티 서비스에 네이버 클라우드 플랫폼의 챗봇과 CSR(Clova Speech Recognition)을 적용하였습니다. 추후 CPV도 적용할 계획을 가지고 있는데요, 이처럼 챗봇과 STT(Speech To Text), TTS(Text To Speech)를 연계한 AI 비서 서비스에도 CPV적용이 가능합니다. ​​이 외에도 금융 업계에서의 불완전 판매 모니터링이나 AI Contact Center와 같이 기존 콜센터에서 사람 인력으로 수행하는 업무들을 음성 합성 기술을 활용해 효율적으로 자동화할 수 있습니다.​    ​Clova Premium Voice는 현재 TTS(Text To Speech) 기술이 사용되고 있는 모든 영역에 활용할 수 있으며, 합성음을 조절하고 감정을 추가해서 최적화된 음성을 다양한 상황에 활용할 수 있습니다. ​CPV를 직접 사용해보고 싶으시다고요? 자세한 활용 사례 및 사용 방법은 CPV 2편을 통해 확인해보세요.​※ 참고 사이트[CPV 소개] https://www.ncloud.com/product/aiService/cpv[API 참조서] https://apidocs.ncloud.com/ko/ai-naver/clova_premium_voice/[API Demo] https://www.youtube.com/watch?v=-ifXyhZbi60​네이버 클라우드 플랫폼은 24시간 기술 지원을 제공하고 있으며,관련 교육지원 (Hands-on-lab) 및 다양한 세미나를 진행할 예정입니다. 감사합니다.​ ​      ​ "
심천 광전자 박람회 CIOE 2023 ,https://blog.naver.com/icetour0915/223106527977,20230519,"심천 광전자 전시회CHINA INTERNATIONAL OPTOELECTRONICS EXPOSION개최기간2023년 9 월 6 일 ~ 9 월 8 일 전시장소Shenzhen World Exhibition & Convention Center 주최https://www.cioe.cn/en/개최규모약 3,000 Exhibitors 참관객수약 100,000 여명 참가  동시개최 박람회Information and Communication ExpoInfrared Applications ExpoPrecision Optics Expo & Camera ExpoLasers Technology & Intelligent Manufacturing ExpoIntelligent Sensing ExpoPhotonics Innovation PavilionDisplay Technology ExpoChina International Optoelectronic Conference​ 전시품목[INFORMATION and COMMUNICATION EXPO] 심천 정보통신 박람회OPTICAL COMMUNICATION CHIPChip design, chip software, wafer, substrate, VCSEL chip, tunable laser chip, TEC chip, DFB and EML chip, PIN and APD chip, IC chip, TIA, LD Driver, CDR chip, high-speed modulator chip, pump Pu laser chip, PLC, AWGOPTICAL COMPONENTSTOSA, ROSA, BOSA, OSA, diodes, isolator components, collimator components, Demux, Mux, MT-FA, precision metal parts (tube holders, tube caps, tube shells), lenses, filters, filters, Photonic integrated devices, microwave dielectric ceramicsOPTICAL COMMUNICATION DEVICEOptical amplifiers, optical pump lasers, optical wavelength division multiplexers, optical wavelength division multiplexers, optical add-drop multiplexers, optical waveguide devices, optical dispersion compensators, optical repeaters, optical attenuators, optical fiber connectors, optical Isolators, fiber optic couplers, optical splitters, optical circulators, wavelength converters, ceramic bushings, ceramic ferrulesOPTICAL COMMUNICATION MODULEOptical receiving module, optical transmitting module, optical transceiver integrated module, optical forwarding module, coherent module, pluggable optical module, silicon optical module, optical splitter moduleOPTICAL COMMUNICATION TRANSMISSION EQUIPMENTSDH optical transmission equipment, Ethernet equipment, optical connection equipment, optical broadband access (EPON, GPON, NGPON) equipment, OLT equipment, GPON SBU equipment, MDU equipment, XGS-PON intelligent home gateway equipment, data transfer equipment, optical Amplifiers, optical multiplexers, optical demultiplexers, optical transceivers (SDH, multi-service optical transceivers, etc.), video and audio optical transceivers, optical fiber transceivers, interface converters, interface converters, protocol converters, mode converters, fiber channel cards, Digital video optical transceivers, ICT system equipment, IP Ethernet transmission productsOPTICAL COMMUNICATION DATA CENTER EQUIPMENTEthernet switch, ToR/Leaf switch, optical storage, optical fiber transceiver, server, hot-swappable AC power module, server optical network card, communication system, system integration/development, system maintenance and monitoring, integrated wiring system, power supply and distribution system, air-conditioning fresh air system, fire protection and security system, data center overall solution, micro-module data center,RADIO and TELEVISION COMMUNICATION EQUIPMENTCATV transmission, analog optical sending components, analog optical receiving components, laser pump sources, analog EDFA, YDFA, optical receiving front-end modems, optical transmitters, optical receivers, optical workstations, broadband access equipmentWIRELESS COMMUNICATIONRF devices, RF modules, RF filters, RF power amplifiers, RF switches, high-frequency semiconductors, PCB high-frequency copper clad laminates, microwave dielectric ceramics, EMC electromagnetic shielding materials, MPI circuit boards, LCP materials, high-frequency low dielectric loss materials , mobile terminal equipment, FSO equipment, base station antenna, small base station FIBER OPTIC MATERIALOptical fiber preform, optical fiber polishing liquid, optical fiber polishing powder, optical fiber protective sleeve, heat-shrinkable terminal, heat-shrinkable intermediate terminal, heat-shrinkable intermediate connection tube, heat-shrinkable sealing cap, mini-sealing cap, heat-shrinkable sleeveFIBER OPTIC CABLE. FIBER OPTIC SENSINGSingle-mode optical fiber, multi-mode optical fiber, plastic optical fiber, erbium-doped optical fiber, ytterbium-doped optical fiber, polarization maintaining optical fiber, radiation-resistant optical fiber, high-temperature resistant optical fiber, bending-resistant optical fiber, large-core optical fiber, indoor optical cable, outdoor optical cable, optical fiber current mutual induction fiber optic hydrophone, fiber optic gyroscope, fiber optic perimeter security system, fiber optic hydraulic sensor, distributed fiber optic temperature sensing system, fiber optic gas sensor, fiber optic stress sensor, grating sensor WIRING PRODUCTSOptical cable connection box, optical cable splice box, optical fiber distribution box, optical cable transfer box, distribution frame, cable tray, optical fiber channelWIRE and CABLECoaxial cable, coaxial cable, radio frequency cable, data cable, twisted pair, security cable, photoelectric composite cable, communication cable, wiring cable, information transmission line, control cable, explosion-proof cable, flame-retardant cableCABLE PRODUCTION EQUIPMENTOptical fiber polishing equipment, peeling and shearing machine, fiber stripping machine, loose parts machine, loose parts assembly machine, winding and binding machine, tail crimping machine, automatic Receptacle assembly machine, automatic labeling machine, pulling force test machine, high-speed coloring machine, cable sheath extrusion machine, copper twisting machine, wire cutting machine, beaming machineINSTRUMENTATIONLaser/detector chip test system, insertion loss test system, optical time domain reflectometer OTDR, PON optical power meter, LIV test system, aging power supply, ellipsometer, spectrometer, bit error meter, eye diagram instrument, optical variable attenuation optical fiber end face detector, coupling test power supply, adjustable DC power supply, desktop optical power meter, handheld optical power meter, single/dual wavelength light source, DWDM light source, tensile testing machine, simulation test and design softwareSEMICONDUCTOR MATERIALIndium phosphide, gallium nitride, gallium arsenide, single crystal silicon, organic germanium, germanium wafer, gallium aluminum arsenic, gallium arsenic phosphide, epitaxial wafer, photoresist, photolithography plate, lithium niobate single crystal thin film, passive element Devices, silicon optoelectronic integration, adhesives, compound materials and crystalsSEMICONDUCTOR EQUIPMENT and MANUFACTURINGChip epitaxy equipment, chip cleavage machine, placement machine, cleaning machine, dicing machine, X-RAY, semiconductor equipment, semiconductor packaging and testing, LD automatic eutectic machine, PD automatic eutectic machine, automatic wire bonding machine, leads Bonding machine, centrifugal defoaming machine, automatic coupling table, dispensing machine, cap sealing machine, loading and unloading machine, helium mass spectrometer leak detector, vacuum drying oven, parallel sealing welding machine, high and low temperature circulation box, high temperature aging box, refrigeration Thermal shock test chamber, salt spray test machine, UV curing equipment, guide rail, slide table, motor,OPERATIONS and SERVICESLithography technology, F5G, 5G, cloud computing, artificial intelligence, loT, smart terminal power supply and solutions  [INFRARED APPLICATIONS EXPO] 심천 적외선 응용프로그램 박람회INFRARED MATERIALInfrared Thermal Module, VOx, A-Si, Infrared Material, Infrared Crystal, Infrared Optical Thin Film, Infrared Window Material, Metal, Smoke Sensor, Focal Plane Material, Infrared Fencing, Active Infrared Intrusion Detector, Infrared Laser, Infrared Photosensitive Diode, Photomultiplier, Pyroelectric Material, Infrared Imaging MEMS ChipINFRARED COMPONENTInfrared Detector, Infrared Illuminator, Infrared Emitting Diode, Infrared Laser, Infrared Lens, Vox Infrared Detector, α-Si Infrared Detector, Infrared Sensor, Pyroelectric Detector, Thermopile, Infrared Imaging Sensor, Pyroelectric Sensor, Semiconductor Sensor, Thermal Detector, Infrared Detector Array, Infrared Photoelectric Sensor, Photoconductive Detector, Photomagnetoelectric Detector, Photon Drag Detector, PIN Photodiode, Schottky Diode, ASIN Chip, MOS Detector, Bipolar Transistor INFRARED EQUIPMENT and IMAGINGThermal Infrared Imager, Infrared Temperature Measurement, Infrared Night Vision Device, Infrared High Quality Speed Dome, Infrared Camera, Infrared Lens, Infrared Lamp, Infrared Heating Oven, Infrared Viewer, Infrared Distance Measurement, Infrared Communication Product, Vision System, Image Processor, X-ray and Detecting Equipment, CCD/CID Equipment, CCD/CID Camera, CMOS Camera, Motion Analysis Camera, Infrared Camera, High-speed Plasma Scanning Camera, Imaging System, Frame Grabber, Image Processor, Infrared Surveillance Equipment TESTING and MANUFACTURING EQUIPMENTInfrared Imaging Testing Equipment, Processing Equipment, Multispectral Thermal Infrared Imager, Infrared Hyperspectral Imager, Infrared Hyperspectral Detector, Raman Spectrometer, Infrared Thermal Camera, Infrared Sensor Testing Equipment MILLIMETER WAVE TECHNOLOGY and APPLICATIONTERAHERTZ MONITORING & IMAGINGUV TECHNOLOGY and APPLICATIONUV LED Core Device: Epitaxy, Chips, Lamp Bead, etc. UV LED Core Supporting Material: Substrate, Lens, Reflecting Material, etc. UV LED Industry Related Equipment: MOCVD, Packaging Equipment, Testing Equipment, etc. UV LED Modules and Applications: UVA Industrial Curing Equipment, Biomedical Equipment, DUV UVC Sterilization Module and Product: Household Appliances, Water Purification Product, Air Purification Product, Industrial Water Treatment Solution and Personal Health Surface Sterilization Product  [PRECISION OPTICS EXPO & CAMERA EXPO] 심천 정밀광학 카메라 박람회OPTICAL MATERIALOptical Glass, Sapphire Material, Quartz Glass, Crystal, Plastic Material, Grinding and Polishing MaterialOPTICAL COLD PROCESSING COMPONENTSpherical and Aspherical Lens, Prism, Cylindrical Mirror, Plane Window, Focusing Mirror, Full and Half Reflecting Mirror, Beam Expander, Filter, SplitterMOULD PRESSING / INJECTION MOLDED OPTICAL COMPONENTMolded Glass Component, Plastic Lens Component OPTICAL PROCESSING EQUIPMENTOptical Cold Processing Equipment: Single-sided and Double-sided Grinding and Polishing Machine, Lower Hem Machine, Lens Thinning and Grinding Machine, Molds and Grinding Wheels for Optical Processing, Ultra-precision Diamond Single-point and Turning Equipment, Glass Cutting Machine/Dicing Saw/External Circular Saw, Crystal Growth Equipment, etc. Optical Thermal Processing Equipment: Spherical, Aspherical Lens Molding Equipment and Electroforming Equipment, Plastic Optical Components Injection Molding Equipment, Mold Temperature MachineSAPPHIRE PROCESSING APPLICATIONSapphire Material, Sapphire Growth Equipment, Production Equipment, Sapphire Crystal, Sapphire Lens, Sapphire Window, Protective Cover, LED Substrate Material, Camera Protection Cover, Fingerprint Recognition Cover, Wearable Device Cover, Sapphire Film, Sapphire Shaped PartOPTICAL COMPONENTSReflective Film, Anti-reflective Coating, Interference Filter, Optical Protection Film, Polarizing Coating, Spectroscopic Film and Phase CoatingOPTICAL COATING MATERIALMetal Oxide, Target MaterialOPTICAL COATING EQUIPMENTVarious Vacuum Coating Equipment, Vacuum Valve and Vacuum Part, Vacuum Measurement and Calibration Instrument, Vacuum Pump, Vacuum Freeze-Drying Equipment, Ion Source, Vacuum System Accessories and Seals, RF Power Supply, Coating Fixture and Jig, Cleaning Equipment OPTICAL LENSMachine Vision Lens, Automotive Lens, Security Lens, Zoom Lens, Industrial Camera Lens, Telecentric LensCAMERA MODULECell Phone, Laptop, Tablet PC, Automotive and Camera Lens Module, Security Camera Module, Projection Module CAMERA ACCESSORIESCMOS, Sensor, Motor, Lens AccessoryCAMERA MANUFACTURING EQUIPMENTLens Assembly and Testing Instrument, Dispensing Equipment OPTICAL MEASURING EQUIPMENTLaser Interferometer, Spectrometer, Three-coordinate Measuring Instrument, Surface Vision Inspection System, Automatic Optical Inspection (AOI) System, Non-contact Displacement Measurement System, 3D Stereo Microscopic Imaging Equipment, Optical Stage and Displacement Table, Telescope, Astronomical Observation Instrument, Bird Watching Mirror and Mount,OPTICAL IMAGING SYSTEMMACHINE VISION and INDUSTRIAL AUTOMATIONSmart Camera, Industrial Camera, Industrial Lens, Light Source, Black and White Capture Card, Image Compression/Decompression Board, Image Processing Board, Image Processing Software, Machine Vision Tool Software, etc. Intelligent Vision Inspection System, Surface Inspection System, Visual Online Inspection System for Complex Industrial Object, Biometric System, Security Supervision System, Security Monitoring System, Biometric System, Light Field Inspection System, Machine Vision Medical Inspection System. AR/VRAR Optical Waveguide Component, Imaging System, AR Display Module, Diffraction Grating, Motion Capture Device, Sound Recognition Device, Helmet, Glasses, All-in-one Machine, Interactive Device, VR + Applications CAMERAMedical Camera, Automotive Camera, Intelligent Security Camera.  [LASERS TECHNOLOGY & INTELLIGENT MANUFACTURING EXPO]심천 레이저 박람회LASER MATERIAL and COMPONENTLaser Crystal, Laser Lens, Laser Chip, Laser Diode, Light-emitting Semiconductor, Special Fiber, Galvanometer, Beam Expander, Field Lens, Optical Grating, Acousto-Optic SwitchLASERSSolid State Laser, Semiconductor Laser (EEL, VCSEL, etc.), YAG Laser, CO2 Laser, Excimer Laser, Dye Laser, Free Electron Laser, Fiber Laser, Ultrafast laser, Quantum Cascade Laser, Disk Laser, Laser Detector, Laser Gyroscope, Laser Beam Quality Measuring Equipment, Laser Measurement System, Laser MaintenanceLASER COMPONENT and AUXILIARY SYSTEMAmplifier, Modulator, Coupler, Combiner, Attenuator, Frequency Stabilizer, PID, Electronic Circuit, Pumping Source, Laser Diode Bar, Laser Welding Head, Laser Cutting Head, Processing Table, Cabinet, Translation Stage, Control Software, Main Control Board, I/O Equipment, Heat Sink, Laser Power Supply, Energy Meter, Power Meter, Laser Interferometer, Spectrometer, Optical Tool Testing Instrument and Equipment, Laser Security Product (Protective Goggle, Laser Safety Cabin, Security Curtain, Security Screen, Protective Suit, Protective Mask, Laser Trap, Laser Alignment Paper, etc.), Laser ServiceLASER EQUIPMENTLaser Marking Machine, Laser Cutting Machine, Laser Welding Machine, Laser Engraving Machine, Laser Scribing Equipment, Laser Cleaner, Laser Cladding Equipment, Laser Ranging and Measurement, Laser Cosmetology, Laser Medical, Laser Display, Laser Obstacle Remover, Laser Gun MECHANICAL SYSTEM and NUMERICAL CONTROL SYSTEMProcessing Table, Cabinet, Translation Stage, Control Software, Main Control Board, I/O Equipment, Gas Supply System, Cooling System, Dedusting System 3D PRINTING / ADDITIVE MANUFACTURING3D Industrial / Desktop Printer, Laser 3D Scanner and Measuring Instrument, Laser Plate Making, 3D Laser Engraving Machine, Laser Tracker, Motion Capture System, 3D Photogrammetry System, 3D Printer Accessory, Numerical Control System, Inspection and Reverse Engineering Software, 3D Inspection Software3C ELECTRONIC INTELLIGENT EQUIPMENTStamping, Drilling and Tapping, Dispensing, Soldering, Screwing, Engraving, Polishing, Abrasive Blasting, Plug-in, Fitting, Grinding, Silk Printing, Curing, Pocking, Cleaning, Cutting, Film Covering, Labelling, Self-feeding, Transferring, Packaging, Carrying, and Semiconductor Precision Processing Equipment, 3D Equipment, Vision Software and Test Equipment, SMT (Glue Dispenser, Solder Paste Printer, Mounter, etc.) and Peripheral Equipment, New Energy Equipment, Packing Equipment, Intelligent Warehousing ROBOT and INDUSTRIAL AUTOMATION6 Axes Robot, Cartesian Robot, SCAPA Robot, Delta Robot, AGV Handing Robot, Servo Motor, Stepping Motor, Linear Motor, Reducer, Motor, Pneumatic System, Actuator, Module, Filament Bar, Guide Rail, Slideway, Bearing, Gear, Belt, Belt Pulley, Coupling, Clutch, Brake, Synchronous Belt, Fastener, Cam, PLC, SCADA, Frequency Converter, Numerical Control System, Initiator, Sensor, Transmitter, Remote Control, Industrial Switch, Embedded System, Industrial Computer and Data System, Linear Positioning System, Motion Control System, Human-Machine Interface, Seal, Industrial Control Machine, Connector, Industrial Power Supply, FA Factory Automation Platform, Industrial Electric Control System, Interface Bus Technology, Industrial Measurement and Instrumentation Auxiliary Equipment, Robotic Arm ​[INTELLIGENT SENSING EXPO] 심천 지능형 센싱 박람회LiDARMechanical LiDAR, Hybrid Solid-state LiDAR (Wedge Mirror Type, MEMS Micro-vibrator Type), Solid-state LiDAR (Flash, OPA, FWCW)LiDAR COMPONENTLaser, Detector, Scanner and Module, MEMS Micro-vibrating Mirror, Detector and Receiver IC, APD, SAPD, EEL PIN, SiPM, Narrow Bandpass Filter, Navigation System LiDAR SYSTEM and SOLUTIONLaser, Detector, Scanner and Module, MEMS Micro-vibrating Mirror, Detector and Receiver IC, APD, SAPD, EEL PIN, SiPM, Narrow Bandpass Filter, Navigation System 3D IMAGING and SENSINGVCESL Chip, EEL, DOE, Infrared LED, MEMS Micro Mirror, CMOS Image Sensor, CCD Image Sensor, Image Processing Chip, DDI Display Chip 3D CAMERAOptical Structure Camera, TOF Camera, Binocular Stereo Vision Camera, Infrared Sensor, Infrared Optical Emitter, Infrared CMOS Image Sensor, Image Processing Chip, Sensor Module, Imaging Algorithm BIOMETRICBiosensor, Fingerprint Sensor, Optical Sensor, Radio-Frequency Sensor, Temperature Sensor, Iris Recognition, Environmental Sensor, Biometric Filter, Gesture Recognition, Automatic Speech Recognition, Pressure Sensor, CMOS Sensor, Ultrasonic Sensor, DOE, MCU, FPCB, Biometric Soc Chip, Biosensor Chip, System Service ProviderMEMS and SENSORInertial Sensor, Pressure Sensor, Temperature Sensor, Angular Velocity Sensor, Pulse Sensor, Smoke Sensor, Fingerprint Sensor, Microphones, Acoustic Sensor, MEMS Biosensor, MEMS Image Sensor, MEMS Tactile Sensor, MEMS Microphone, Gyroscope, Accelerometer, Magnetic Sensor, ASIC Chip, Microfluidics Chip, Pressure Chip, Infrared Thermoelectric Reactor Chip, Sensor Module, Sensor Chip, MEMS/Sensor Solution, Radio-Frequency SolutionMILLIMETER-WAVE RADARMillimeter Wave Radar Core Device: MMIC, Baseband Digital Signal Processor, Antenna High Frequency PCB, Millimeter Wave System and Solution INDUSTRIAL SENSOROptical Fiber Sensor, Automotive Sensor, Spectral Confocal Displacement Sensor, Photoelectric Sensor, Fiber Grating Sensor, Gas Sensor, Temperature and Humidity Sensor, Environmental Sensor, Position Sensor, Acceleration Sensor, Flow Sensor, Vision Sensor, Dynamic Torque Sensor, Ultrasonic Sensor, Liquid Level Sensor, Displacement Sensor, Resistive Sensor, Resistance-Strain Sensor, Piezoresistive Sensor  [PHOTONICS INNOVATION EXPO] 심천 포토닉스혁신 박람회BiophotonicsMedical ImagingSmart System & Test EquipmentIC and Sensor related research technologyHealthcare PhotonicsOptoelectronic Display & ModulePhotonics ManufacturingSmarter Healthcare IC and Sensor Research and Development TechnologyInternet of Things, Big Data & Cloud ComputingRobots, Artificail Intelligence [DISPLAY TECHNOLOGY EXPO] 심천 디스플레이 기술 박람회DISPLAY MANUFACTURING EQUIPMENTExposure Equipment, Physical Vapor Deposition Equipment, Chemical Vapor Deposition Equipment, Etching Equipment, Inkjet Printing Equipment, Surface Gluer, Evaporation Equipment, Laser Equipment, Massive Transfering Equipment, Curing Equipment, Stripping Equipment, Cutting Equipment, Dispensing Equipment, Cleaning Equipment, Fitting Equipment, Detection and Repair Equipment, Edge Grinding Equipment, Visual Inspection EquipmentDISPLAY MATERIALTarget Material, OLED Material, Quantum Dot, Mini/Micro LED Chip, Driver IC, PCB, FPC, Glass Substrate, Glass Cover, LC Material, ITO Glass, Polarizer, Color Filter, Adhesive Product, Mask, Antistatic Material, Electrophoresis Material, Optical Waveguide Material, Photoresist, Developer, Electrode Material DISPLAY PANEL/MODULELCD Panel, OLED Panel, Mini-LED Panel, Micro-LED Panel, Micro-OLED Module, 3D Display Solution, COB Module, RGB Packing, Backlight Component TERMINAL DISPLAY PRODUCTLCD, OLED, Mini/Micro-LED, Micro-OLED, AR, VR, MR, Laser Projection, Reflective Display, On-board Display, Wearable Display, Commercial Display, Interactive Display, Transparent Display, Flexible Display, Glass-free Display, Holographic Display ​ 상세 일정심천 광전자 박람회 CIOE 20231. 전시회명 : 심천 광전자 박람회(CIOE 2023)               China International Optoelectronics Exposition CIOE 2023 (심천 광전자 박람회) INFORMATION COMMUNI...www.icetour.co.kr #CIOE2023#2023CIOE2023#심천CIOE2023#2023심천CIOE#중국CIOE2023#2023중국CIOE#중국심천CIOE2023#2023중국심천CIOE#심천CIOE2023#2023심천CIOE#중국CIOE2023#2023중국CIOE#중국심천CIOE2023#2023중국심천CIOE#심천광전자#심천광전자2023#중국광전자#중국광전자2023#심천광전자#심천광전자박람회#심천광전자전시회#중국광전자박람회#중국광전자전시회#2023심천CIOE2023#심천CIOE박람회#심천CIOE박람회2023#심천CIOE전시회#심천CIOE전시회2023#중국CIOE2023#중국CIOE박람회#중국CIOE박람회2023#중국CIOE전시회#중국CIOE전시회2023#심천광전자박람회CIOE#심천광전자전시회CIOE#중국광전자박람회CIOE#중국광전자전시회CIOE#심천광전자박람회CIOE2023#심천광전자전시회CIOE2023#중국광전자박람회CIOE2023#중국광전자전시회CIOE2023#중국광전자CIOE2023#심천광전자CIOE2023 "
"[딥러닝] ""Transfer Learning in Deep Learning"" ",https://blog.naver.com/walter_-/223080156528,20230420,"Transfer Learning in Deep Learning ​ Deep Learning has become very popular in recent years due to its remarkable achievements in various fields such as image recognition, natural language processing, and speech recognition. However, it requires a lot of data, time, and computational resources to train a deep neural network from scratch. Transfer Learning is a technique that addresses this issue by transferring the knowledge learned in one task to another related task.  ​Transfer Learning comes in handy when we have limited data or computational resources. In Transfer Learning, we use the knowledge learned in one task, called the source task, to solve another related but different task, called the target task.  ​How does Transfer Learning work? ​In Transfer Learning, we usually take a pre-trained model on a large dataset such as ImageNet and fine-tune it on a target dataset. The pre-trained model acts as a feature extractor, in which the lower layers learn generic features that are useful for many tasks, while the higher layers learn task-specific features.  ​By fine-tuning only the top layers of the pre-trained model, we can adapt it to the target task and achieve better performance with less data compared to training a model from scratch. In some cases, we can even freeze some of the lower layers that are already well trained on generic features, saving computation time and preventing overfitting.  ​Benefits of Transfer Learning Less Computation: Transfer Learning allows us to reuse the knowledge that has already been learned in the pre-trained model, reducing the computation time required for training a model from scratch. Less Data: Transfer Learning enables us to achieve better performance with less data by leveraging the pre-trained model's knowledge. Better Performance: Fine-tuning a pre-trained model on a target task often leads to better performance compared to training a model from scratch. Faster Convergence: Transfer Learning allows us to quickly converge to a good solution by initializing the model weights with the pre-trained model's weights. ​Conclusion ​Transfer Learning is a powerful technique that can save time, computation, and data required for training a model from scratch while achieving better performance on a target task. By leveraging the knowledge learned in pre-trained models, we can adapt them to different related tasks and solve real-world problems efficiently.  ​ 댓글과 공감 부탁드립니다~!모두 하시는 일 번창하시길 바랄게요!이웃 신청해주시면 바로 수락 드립니다!감사합니다~​ #TransferLearning #DeepLearning #FineTuning #PretrainedModel #NeuralNetwork #ConvolutionalNeuralNetwork #Classification #ImageRecognition #ObjectDetection #NaturalLanguageProcessing #RecurrentNeuralNetwork #LanguageModeling #SequenceToSequence #SentimentAnalysis #WordEmbedding #BERT #GPT #ELMo #Transformers #DomainAdaptation #DataAugmentation #FeatureExtraction #Transferability #AdversarialExamples #Generalization #Overfitting #Underfitting #HyperparameterTuning #ModelSelection "
[누구나 배우는 파이썬] 인공지능과 머신러닝 기술  ,https://blog.naver.com/ogu3421/222763436109,20220607,"※ 누리잡의 '누구나 배우는 파이썬, 기초부터 머신러닝까지!' 강의 내용을 정리한 글입니다.​목차1. 인공지능2. 머신러닝3. 머신러닝 학습법  1. 인공지능사람과 유사한 지능을 가지도록 인간의 학습능력, 추론능력, 지각능력, 상호작용 등을 컴퓨터 프로그램으로 실현하는 기술​▶ 인공지능의 구성 기술 인지, 지각학습, 추론행동1. 자연어 처리(Natural Language Processing)  - 대화  - 기계 번역  - 감정 분석  - 내용 분석2. 음성 인식(Speech Recognition)  - Speech to Text  - Text to Speech3. 이미지 인식(Computer Vision)  - 이미지  - 비디오1. 머신 러닝(Machine Learning)2. 딥 러닝(Deep Learning)1. 로봇2. 자율주행 자동차3. AI 스피커4. 챗봇 ​​▶ 인공지능의 역사 출처 : 자바스크립트 개발자는 머신러닝을 어떻게 바라봐야 하는가? – TOBETONG​​​ 2. 머신러닝데이터를 학습하고 신규데이터를 예측하고, 실제 결과를 비교하는 과정에서 기계가 스스로 데이터 패턴을 찾고 학습하며 모델을 최적화 해 나가는 기술로써, 데이터를 기반으로 하는 의사결정과정을 비약적으로 발전시킬 수 있다.​▶ 머신러닝과 기존 프로그램의 차이​① 정통적인 프로그램 ​② 머신러닝 ​​▶ 머신러닝의 단계​① 데이터셋 입력정제된 데이터 모음  - 글자(text)  - 이미지(image)  - 소리(sound)  - 측정데이터(number)  - 비디오(video)​② 모델 생성지도학습 : 분류, 회귀(예측)비지도학습 : 군집화, 이상감지 등강화학습​③ 예측인공지능을 활용하는 목적​​​ 3. 머신러닝 학습법지도학습비지도학습강화학습컴퓨터에 문제(Feature)와 정답(Label)이 있는 데이터(Trainind Set)를 학습 시킨 후, 운영 데이터(Test Set)를 분류 하거나 맞추는 것정확하게 많이 배우는 것이 중요 (양질의 충분히 많은 데이터)답을 가르쳐 주지 않고 공부시키는 방법. 훈련 데이터에 정답은 없고 입력 데이터만 있다.학습을 통해 입력 데이터의 패턴, 특성을 발견하는 방법뉴스 그룹핑, 상품 추천, 이상감지K-Means, DBSCAN목표가 보상(reward)의 최대치가 되도록 행위를 강화하는 학습보상(reward)알파고 ​(1) 지도학습분류 vs 예측 ​분류Classification예측Regressionn(회귀)결과학습데이터의 레이블 중 하나를 예측(discrete)연속된 값을 예측(continuous)예제스팸 메일 필터주가 분석 예측 ​(2) 비지도학습분류 vs 군집 ​분류Classification군집Clustering공통점입력된 데이터들이 어떤 형태로 그룹을 형성하는지가 관심사차이점레이블이 있다레이블이 없다예) 의학 임상실험 환자군 구별,구매자 유형 분류 ​​▶ 머신러닝 시 고려할 사항​- 적용하기 위한 충분한 데이터가 있는가?- 머신러닝으로 문제를 풀 수 있는가?- 성능이 떨어지지 않는가?- 데이터가 중요하다 → 편향되지 않은 데이터 필요​​​​#파이썬 #파이썬기초 #데이터분석 #데이터분석기초 #구글코랩 #머신러닝 #ptython #googlecolab #machinelearning​ "
AI 음성의 핵심기술 10가지  ,https://blog.naver.com/lyk6208/223043545690,20230313,"음성 인식 기술 (Speech Recognition Technology): 음성 인식 기술은 컴퓨터가 인간의 말을 이해하고 텍스트로 변환할 수 있도록 하는 기술입니다. 이 기술은 주로 자연어 처리 기술과 결합하여 사용됩니다.자연어 처리 기술 (Natural Language Processing Technology): 자연어 처리 기술은 인간의 언어를 이해하고 분석하여 기계가 처리할 수 있는 형태로 변환하는 기술입니다. 이 기술은 대화형 인터페이스를 비롯한 많은 인공지능 응용 프로그램에서 중요한 역할을 합니다.음성 합성 기술 (Speech Synthesis Technology): 음성 합성 기술은 텍스트를 음성으로 변환하는 기술입니다. 이 기술은 인공지능 비서, 화자 인식 등에 활용됩니다.감성 인식 기술 (Emotion Recognition Technology): 감성 인식 기술은 음성의 톤, 억양, 말소리 등을 분석하여 사람의 감정을 인식하는 기술입니다. 이 기술은 대화형 인터페이스, 로봇, 게임 등에서 사용됩니다.발화 인식 기술 (Speaker Recognition Technology): 발화 인식 기술은 음성을 분석하여 해당 음성이 특정 개인의 것인지 인식하는 기술입니다. 이 기술은 보안 시스템, 음성 인식 기반 인증 시스템 등에서 사용됩니다.음성 분리 기술 (Speech Separation Technology): 음성 분리 기술은 여러 사람이 동시에 말할 때 개별적인 음성을 분리하는 기술입니다. 이 기술은 회의록 작성, 음성 인식 등에서 사용됩니다.음성 강화 기술 (Speech Enhancement Technology): 음성 강화 기술은 소음이나 외부 잡음 등을 제거하거나 줄여서 음성을 더욱 선명하게 만드는 기술입니다. 이 기술은 음성 인식, 음성 합성 등에서 사용됩니다.음성 분석 기술 (Speech Analysis Technology): 음성 분석 기술은 음성의 특징을 분석하여 음성의 속성, 감정, 발화자의 신체적 특성 등을 파악하는 기술입니다. 이 기술은 감성 인식, 화자 인식, 음성 합성 등에서 사용됩니다.다중 언어 및 방언 지원 기술: 다양한 언어 및 방언을 인식하고 합성하는 기술로, 글로벌 시장에서 AI 목소리 기술의 활용도를 높입니다.실시간 음성 처리 기술: 음성 입력이 발생하는 즉시 처리하는 기술로, 음성 대화형 AI 시스템에서 지연 없는 대화를 가능하게 합니다. ​ "
ChatGPT 프롬프트 가이드: Part 2. Content ,https://blog.naver.com/rihodad/223067353458,20230407,"  쓰기 및 콘텐츠 생성을 위한 프롬프트 ChatGPT는 이미 많은 사람들이 이야기를 만들거나, 블로그에 올릴 게시물을 만들기 위해 사용하고 있습니다. 만약 아마추어 작가가 되고 싶거나, 언어 능력을 향상시키고 싶다면 다음의 ChatGPT 프롬프트가 도움이 될 수 있습니다.  1. 블로그 게시물 작성하기  {주제}로 500단어 분량의 블로그 게시물을 작성해줘Write a 500-word blog post on {Insert Topic here}   2. 동의어 제공자  나는 네가 동의어 공급자 역할을 해주기를 원해. 내가 너에게 단어를 말하면 너는 내 프롬프트에 따라 동의어의 대안 목록으로 나에게 대답해 줘. 프롬프트당 최대 10개의 동의어를 제공해줘. 제공된 단어의 동의어를 더 원하면 ""More of x""라는 문장으로 대답할께. 여기서 x는 동의어를 찾은 단어야. 단어 목록만 대답해주고 다른 것은 필요없어. 단어가 존재해야 해. 설명은 쓰지 마. 확인하려면 ""OK""라고 대답해줘.I want you to act as a synonyms provider. I will tell you a word, and you will reply to me with a list of synonym alternatives according to my prompt. Provide a max of 10 synonyms per prompt. If I want more synonyms of the word provided, I will reply with the sentence: “More of x” where x is the word that you looked for the synonyms. You will only reply to the word list and nothing else. Words should exist. Do not write explanations. Reply “OK” to confirm.   3. 기술 저널리스트  나는 네가 기술 작가로 활동하기를 원래. 창의적이고 매력적인 기술 작가로 활동하고 특정 소프트웨어에서 다양한 작업을 수행하는 방법에 대한 가이드를 작성해줘. 앱의 기본 단계를 제공하고 이러한 기본 단계를 수행하는 방법에 대한 매력적인 기사를 제공할 꺼야. 스크린샷을 요청할 수 있어. 스크린샷이 있어야 한다고 생각하는 곳에 추가(스크린샷)하면 나중에 추가할께. 다음은 앱 기능의 첫 번째 기본 단계야. ""1. 플랫폼에 따라 다운로드 버튼을 클릭합니다. 2. 파일을 설치합니다. 3. 두 번 클릭하여 앱을 엽니다.""I want you to act as a tech writer. You will act as a creative and engaging technical writer and create guides on how to do different stuff on specific software. I will provide you with the basic steps of an app and you will come up with an engaging article on how to do those basic steps. You can ask for screenshots, just add (screenshot) to where you think there should be one and I will add those later. These are the first basic steps of the app functionality: ""1. Click on the download button depending on your platform 2. Install the file. 3. Double-click to open the app""   4. 영어 번역기  나는 네가 영어 번역가, 맞춤법 교정가, 개량가로 활동하기를 원해. 나는 어떤 언어로든 너에게 말할 것이고 너는 그 언어를 감지하고 그것을 번역하여 영어로 수정되고 개선된 버전의 텍스트로 대답해줘. A0 수준의 단순화된 단어와 문장을 더 아름답고 품격 있는 상위 수준의 영어 단어와 문장으로 바꿔줬으면 해. 의미를 동일하게 유지하되 더 문학적으로 만들어 줘. 나는 네가 수정 사항에만 대답하고 개선 사항에 대해서는 설명을 쓰지 않기를 원해. 내 첫 문장은 ""je voudrais un verre de vin.""I want you to act as an English translator, spelling corrector, and improver. I will speak to you in any language and you will detect the language, translate it and answer in the corrected and improved version of my text, in English. I want you to replace my simplified A0-level words and sentences with more beautiful and elegant, upper-level English words and sentences. Keep the meaning the same, but make them more literary. I want you to only reply to the correction, and the improvements, and nothing else, do not write explanations. My first sentence is “je voudrais un verre de vin”   5. 표절 검사기  표절 검사기 역할을 해 줬으면 해. 나는 너에게 문장을 쓸 것이고 너는 주어진 문장의 언어로 된 표절 검사에서 발견되지 않은 답변만 할 수 있어. 답변에 설명은 쓰지 않아도 돼. 첫 번째 문장은 ""컴퓨터가 인간처럼 행동하려면 음성 인식 시스템이 화자의 감정 상태와 같은 비언어적 정보를 처리할 수 있어야 합니다.""I want you to act as a plagiarism checker. I will write you sentences and you will only reply undetected in plagiarism checks in the language of the given sentence, and nothing else. Do not write explanations in replies. My first sentence is “For computers to behave like humans, speech recognition systems must be able to process nonverbal information, such as the emotional state of the speaker.”   6. 시나리오 작가  시나리오 작가로 활동해 줘. 시청자를 사로잡을 수 있는 장편 영화 또는 웹 시리즈를 위한 매력적이고 창의적인 대본을 개발하게 돼. 흥미로운 캐릭터, 스토리 설정, 캐릭터 간의 대화 등을 생각해 내는 것부터 시작해 줘. 캐릭터 개발이 완료되면 우여곡절로 가득 찬 흥미진진한 스토리라인을 만들어 끝까지 시청자를 긴장하게 만들어 줘. 첫 번째 부탁은 ""파리를 배경으로 한 로맨틱 드라마 영화를 쓰고 싶어요.""야.I want you to act as a screenwriter. You will develop an engaging and creative script for either a feature-length film or a Web Series that can captivate its viewers. Start with coming up with interesting characters, the setting of the story, dialogues between the characters, etc. Once your character development is complete – create an exciting storyline filled with twists and turns that keep the viewers in suspense until the end. My first request is “I need to write a romantic drama movie set in Paris.”   7. 기사 개요에 대한 도움 받기  {주제}에 대한 기사 개요를 작성해 줘. 간결하게 작성하되 누락된 정보가 없는지 확인해야 해.Create an article outline for {Insert Topic here}. Make it concise yet ensure no information is left out.   8. 미리 작성된 콘텐츠에 창의성을 불어넣기  다음 콘텐츠를 더 창의적으로 만들어 줘. {여기에 콘텐츠 삽입}. 우아함과 단순함의 균형을 잘 유지해줘.Make the following content more creative [Insert content here]. Keep a good balance of elegance and simplicity.   9. 게시물에 은유 추가  {제품/서비스}의 이점을 설명하는 X 은유를 제안해 줘.Suggest X metaphors to describe the benefits of {Insert product/service}   10. 기사 요약  다음 내용을 요약해줘. {insert content here}Summarize the following content {insert content here}  #ChatGPT #프롬프트 #프롬프트예제 #블로그게시물작성 #콘텐츠생성 #표절검사 #기사요약 ​  "
Genesys Cloud CX: Architect GCX-ARC Exam Questions ,https://blog.naver.com/questionpass/223079368259,20230419,"PassQuestion is the leading provider of Genesys Cloud CX: Architect GCX-ARC Exam Questions, designed to help you pass the Genesys Cloud CX: Architect certification exam. With our extensive experience and expertise in this field, we are committed to providing you with the latest and most comprehensive Genesys Cloud CX: Architect GCX-ARC Exam Questions to help you achieve your certification goals. This ensures that you are well-prepared to meet the challenges of the certification exam and succeed in your career. With PassQuestion Genesys Cloud CX: Architect GCX-ARC Exam Questions, you'll be able to identify your strengths and weaknesses in these areas and focus your study efforts accordingly.  Genesys Cloud CX: Architect Certification ExamThe Genesys Cloud CX: Architect certification tests the candidate's knowledge in building basic and advanced call flows. They are also assessed in the concepts of prompts, recording and uploading, call flow design, creating inbound and outbound call flows, secure call flows, and IVR use of data actions. The Genesys Cloud Architect Certified Specialist is intended for routing application developers or anyone in a technical role involved in planning, developing, and testing inbound, outbound, and in-queue email routing applications developed with Genesys Cloud Architect.  The Genesys Certified Professional (GCP) - PureCloud certification is a pre-requisite for the Genesys Cloud Architect Certified Specialist exam.Genesys Cloud CX: Architect GCX-ARC Exam DetailsNumber of Questions: 60 questionsType of Exam: Multiple Choice, Multiple Select, and True/False Questions Passing Score: 65%Exam Language: EnglishTest Duration: 120 minutesGenesys GCX-ARC Exam TopicsTopic 1 - Introduction to Genesys Cloud CX ArchitectTopic 2 - Admin InterfaceTopic 3 - Variables and PromptsTopic 4 - Scheduling in Genesys Cloud CXTopic 5 - Flow ConfigurationsView Online Genesys Cloud CX: Architect Certification GCX-ARC Free Questions1. The Utilization feature of Genesys Cloud allows administrators to configure: (Choose three.)A.The maximum capacity that an agent may handle simultaneously for each supported media typeB.The after call work time for each media typeC.The length of time that an agent may spend on each media typeD.The number of different media types that an agent may handle simultaneouslyE.The media types that can interrupt current interactions that an agent is handlingAnswer: A, D, E2. Your contact center wants to track the outcome of calls and chats. What can be configured within Genesys Cloud to provide this functionality?A.Account CodesB.Wrap-up CodesC.Resolution CodesD.StatusAnswer: B3. What browsers are supported for use with all Genesys Cloud features? (Choose two.)A.Internet ExplorerB.FirefoxC.ChromeD.SafariE.OperaF.AvantAnswer: B, C4. Select the types of scheduling available in Genesys Cloud. (Choose two.)A.Manual SchedulingB.Load based SchedulingC.Automated SchedulingD.All of the aboveAnswer: B, C5. Which speech recognition feature is enabled by default for new Inbound call flows?A.Enable speech recognition for the entire flowB.Company Directory speech recognitionC.Complete match timeoutD.Incomplete match timeoutAnswer: B "
[논문리뷰-38] Dropout : a simple way to prevent Neural networks from overfitting ,https://blog.naver.com/mindmerge/223079943341,20230420,"# Dropout: A Simple Way to Prevent Neural Networks from Overfitting​ ​The passage discusses the problem of overfitting in deep neural networks with a large number of parameters and proposes dropout as a solution. ​Dropout is a technique that randomly drops units (along with their connections) from the neural network during training, which prevents units from co-adapting too much and leads to improved generalization performance. During training, dropout samples from an exponential number of different ""thinned"" networks, and at test time, ​it is easy to approximate the effect of averaging the predictions of all these thinned networks by using a single unthinned network that has smaller weights. The passage claims that dropout significantly reduces overfitting and gives major improvements over other regularization methods, and it has been shown to improve the performance of neural networks on supervised learning tasks in various domains, including vision, speech recognition, document classification, and computational biology. The passage also mentions that large networks are slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. ​Overall, the passage highlights the importance of regularization techniques like dropout in deep learning and their potential to improve performance on a range of tasks.​ ​The passage discusses the relationship between dropout and adding noise to hidden units in a neural network. ​Dropout is a form of stochastic regularization that can be interpreted as adding noise to the hidden units of the network. This idea is similar to the concept of adding noise to input units in the context of denoising autoencoders (DAEs), as previously proposed by Vincent et al. (2008, 2010). The passage also highlights that dropout can be extended beyond supervised learning and applied to other neuron-based architectures, such as Boltzmann Machines.​The passage also discusses the deterministic counterpart of dropout, obtained by marginalizing out the noise. In some cases, dropout can be analytically marginalized out to obtain deterministic regularization methods. Other researchers have explored similar ideas, such as van der Maaten et al. (2013) who explored deterministic regularizers for exponential-family noise distributions, including dropout.The passage also mentions previous work by Globerson and Roweis (2006) and Dekel et al. (2010) that explored an alternate setting where the loss is minimized when an adversary gets to pick which units to drop, instead of using a noise distribution. However, this work did not explore models with hidden units.Overall, the passage highlights the connection between dropout and adding noise to hidden units, as well as the potential to extend dropout to other architectures and explore its deterministic counterpart​""Stochastic regularization""은 뉴럴 네트워크에서 사용되는 일종의 정규화 기법으로, 모델이 학습 데이터에 과도하게 적합(fitting)되는 것을 방지하기 위해 사용됩니다. 이를 위해 네트워크의 일부 유닛들을 무작위로 비활성화시켜(즉, 출력값을 0으로 만들어) 모델이 학습 데이터의 노이즈나 이상치(outliers)에 지나치게 적합되는 것을 막습니다. 이러한 비활성화 과정을 통해 모델이 여러 개의 ""더 작은"" 서브네트워크들로 구성된 것처럼 보이게 되어, 모델이 강건(robust)하게 만들어지는 효과도 있습니다.​""Deterministic regularization methods""은 네트워크에서 사용되는 또 다른 정규화 방법입니다. 이 방법은 ""stochastic regularization""과 달리, 모델 내부의 노이즈를 추가하는 것이 아니라, 모델 구조 자체를 변경하여 파라미터 수를 줄이는 것입니다.​예를 들어, 네트워크 내부의 특정 가중치(weight)들을 0으로 만들거나 작게 조정함으로써, 모델이 더 간단한 구조를 갖게 되어 과적합(overfitting)을 방지할 수 있습니다. 이러한 방식은 모델이 더 적은 수의 파라미터로도 충분한 복잡도(complexity)를 가질 수 있도록 하며, 이를 통해 불필요한 복잡성을 제거하여 모델의 일반화 성능을 향상시키는 효과가 있습니다.""Deterministic regularization methods""은 네트워크 내부의 구조적인 변경을 통해 정규화를 수행하기 때문에, 학습 중에 무작위성(randomness)을 도입하는 ""stochastic regularization""보다는 더 예측 가능하고 안정적인 방법이라 할 수 있습니다.​""Stochastic regularization""과 ""dropout""은 비슷한 개념이지만, 약간의 차이가 있습니다.""Stochastic regularization""은 모델 내부에 노이즈를 추가하여 과적합(overfitting)을 방지하는 방법입니다. 이 방법은 네트워크의 파라미터를 일정 확률로 무작위로 선택하거나 노이즈를 추가함으로써, 모델이 학습 데이터에 과도하게 적합(fit)되지 않도록 합니다. 이 방법은 네트워크가 불필요한 복잡성을 제거하여 일반화 성능을 향상시키는 데에 효과적입니다.반면에 ""dropout""은 ""stochastic regularization""의 한 형태로, 네트워크 내부의 노드를 일정 비율로 무작위로 제거하는 방법입니다. 이는 네트워크 내부의 여러 부분집합(subset)을 학습시키는 효과를 가지며, 각 부분집합은 서로 다른 노드의 조합으로 이루어집니다. 이를 통해, 네트워크가 학습 데이터에 대해 여러 가지 다른 방법으로 일반화되도록 유도함으로써, 과적합을 방지하고 일반화 성능을 향상시키는 효과를 가집니다.따라서 ""dropout""은 ""stochastic regularization""의 한 예시이지만, ""stochastic regularization""은 ""dropout"" 이외에도 다양한 방법으로 구현될 수 있습니다.​ ​ ​Dropout is a technique used to prevent overfitting in neural networks by randomly dropping out (or deactivating) some of the neurons during training. ​This breaks up the co-adaptations between the neurons, making the network more robust and preventing it from relying too heavily on any one particular feature or neuron. This has been found to be effective in a wide variety of applications, including image and speech recognition, document classification, and computational biology.​However, one of the drawbacks of dropout is that it can increase training time, as the network needs to be trained multiple times with different randomly deactivated neurons. ​This results in a more noisy parameter update process, as the gradients being computed are not gradients of the final architecture that will be used at test time.To overcome this, one can use a deterministic regularization method that achieves similar effects to dropout, but without the stochasticity. This can be done by marginalizing the noise and obtaining a regularizer that does the same thing as the dropout procedure in expectation. For linear regression, this regularizer is a modified form of L2 regularization, but for more complex models, it is not as straightforward. "
인공지능 AI의 핵심 기능 자연어 처리 NLP 알아보기 ,https://blog.naver.com/loginplus365/222617584767,20220111,"<출처: 영화 아이언맨>​안녕하세요. 안전하고 편리한 서비스를 제공하는 IT 기업 민앤지 로그인플러스입니다. 애플이 출시한 AI 개인비서 시리가 2011년 처음 선보인 이후, AI 개인비서의 성능 및 서비스는 꾸준히 개선 및 확장되고 있습니다. ​“시리야, 음악 틀어줘!” “시리야, 오늘 날씨 어때?”​이렇게 시리와의 대화로 하루의 문을 여는 것이 익숙해진 요즘 AI 개인비서는 스마트폰, 스마트 스피커 뿐만 아니라 자동차, TV, 시계, 헤드폰, 안경 등으로 적용범위가 넓어지고 있습니다. 머지않아 영화 아이언맨에 등장하는 자비스처럼 토니 스타크의 질문이나 명령을 인지하고, 이해하여 합리적이고 창의적인 응대를 하는 AI 비서가 등장할 듯 합니다. ​이러한 일을 가능하게 하는 기술을 NLP(자연어 처리 기술)라고 하는데요, 과거 무전기폰에 대고 ‘우리집!’과 같은 단순한 단어를 힘주어 몇 번을 소리쳐야 했던 음성인식 기술이 이제는 인간의 비서 역할을 대신 할 수 있을 만큼 발전하고 있습니다.​오늘은 인간과 기계가 자연스러운 의사소통을 할 수 있게 도와주는 인공지능의 핵심 기능 NLP에 대해 알아보도록 하겠습니다.​ 자연어(natural language)란?자연어(natural language)란 프로그래밍 언어와 같이 사람이 인공적으로 만든 언어가 아닌, 자연계에 존재할 수 있는 모든 언어, 더 쉽게 말하면 우리들이 일상생활에 의사소통으로 사용하는 언어를 의미하는데요.  자연어에 대한 오랜 연구에도 불구하고 아직 컴퓨터가 자연어를 사람처럼 완벽히 이해하지는 못하고 있습니다.​예를 들어 “좋아 죽겠어!”라고 말하면 사람들은 ‘정말 좋은가 보다’고 생각하지만, 컴퓨터는 문자 그대로 인식하여 이해하는데 매우 힘들어 하는 것인데요. 컴퓨터 내에서 다루는 언어, 이를테면 프로그래밍 언어는 모든 예외가 이미 사전에 정의되어 있고, 규칙에 맞게 수행될 수 있지만 자연어는 그렇지 않기 때문입니다.​ ​ 자연어 처리(Natural Language Processing. NLP) 란?컴퓨터와 사람의 언어 사이의 상호작용에 대해 연구하는 컴퓨터 과학과 어학의 한 분야인데요. 정보검색분야에서는 이용자와의 자연스러운 대화를 통해, 이용자의 의도를 컴퓨터가 파악해 보다 정확한 정보를 다양한 형태의 데이터로부터 취합해 제공하는 데에 활용되고 있습니다.​즉, 인간의 자연어를 분석하여 인공어로 처리하는 기술을 뜻하는 것인데요. 기계학습과 딥러닝을 교집합으로 가지고 인간과 컴퓨터 사이의 통역사 역할을 하는 것입니다.​ ​ 자연어 처리(NLP)의 역사(History)자연어 처리는 인공 지능의 주요 분야 중 하나로 NLP의 역사는 1946년으로 거슬러 올라가는데요. 미국 과학자 워런 웨이버(Warren Weaver)는 2차 세계대전 때 적군의 암호문을 번역해 정보를 알아내기 위한 용도로 기계번역(MT, Machine Translation)이라는 기술을 개발해냈습니다.​그 후 기계번역은 암호문 분석 뿐만 아니라 언어 번역에도 확장되면서, 1990년대 이후 대량의 말뭉치(corpus) 데이터를 활용하는 기계 학습 기반 및 통계적 자연어 처리 기법이 주류가 되었는데요. 특히 최근에는 심층 기계 학습(deep learning)기술이 기계 번역 및 자연어 생성 등에 적용되어 방대한 텍스트로부터 의미 있는 정보를 추출하고, 활용하기 위한 언어처리 연구 개발이 전 세계적으로 활발히 진행 중입니다. 자연어 처리(Natural Language Processing. NLP)활용자동번역 (MACHINE TRANSLATION)자동 번역의 가장 대표적인 사례로는 구글 번역(Google Translate)을 들 수 있습니다.구글 번역의 경우, 매일 1천억 개 이상의 단어를 번역한다고 알려져 있는데요. 문장 전체를 한 번에 번역하기 위해서 수 백 개의 예제를 학습하는 예제 기반의 자동 번역 방식의 딥러닝을 사용한 신경망 기계 번역(neural machine translation) 방식을 채택하고 있습니다. 대화형 사용자 인터페이스(CONVERSATIONAL USER INTERFACE)대화형 사용자 인터페이스(CUI)는 컴퓨터가 실제 사람과의 대화를 모방하는 컴퓨터를 위한 인터페이스인데요. 대표적인 것이 챗봇(chabot)입니다. 챗봇은 기계와 사람이 텍스트를 통해서 대화할 수 있는 인터페이스를 갖추고 있는데요. 가장 활용되는 분야는 고객 상담 분야이며, 금융, 보험, 전자상거래 분야에까지 널리 확산되고 있습니다. 챗봇이 복잡한 임무를 수행하기 위해서는 사용자가 입력하는 내용을 이해하고, 그것을 해석하고, 적절하게 대응할 수 있어야 하는데요. 바로 이 지점이 자연어 처리가 중요한 역할을 하는 부분입니다.​ 텍스트 예측(TEXT PREDICTION)텍스트 예측은 어떤 구문이나 문장에서 다음에 올 단어를 예측하는 프로세스를 의미하는데요. 이러한 텍스트 예측에서 가장 일반적인 사례는 구글 검색입니다.​구글에서는 미리 훈련된 모델을 생성하기 위해서 신경 네트워크를 활용하는 자연어 처리(NLP) 알고리즘인 버트(BERT)라는 기법을 사용하고 있는데요. 이 모델은 인터넷에서 사용할 수 있는 방대한 양의 주석이 달리지 않은 텍스트를 활용해서 훈련을 합니다.  특히 버트의 알고리즘은 검색 엔진이 사람과 비슷한 방식으로 검색어를 이해할 수 있도록 도와주는데요. 그 외에도 구글 문서, Gmail 작성과 같은 수많은 애플리케이션에서도 텍스트 예측이 도움을 주는 자연어 처리 모델을 활용하고 있습니다.​ 감성 분석(SENTIMENT ANALYSIS)​감성 분석은 텍스트 데이터 안에서 감정을 해석하고 분류하는 프로세스인데요. 일반적으로 기업들이 감성 분석을 활용하여 온라인 피드백에서 고객이 서비스, 브랜드, 제품에 대해서 보이는 정서를 (긍정적, 부정적, 중립적인지에 대해서) 파악할 수 있습니다.​감성 분석이 탁월한 활약을 보이고 있는 곳으로는 제품 분석, 시장조사, 평판관리, 정밀한 타겟팅(Targeting), 시장 분석, 홍보, 순 추천 고객지수(NPS) 등이 있습니다.​ 텍스트 분류(TEXT CLASSIFICATION) 이메일, 소셜 미디어, 웹사이트, 채팅 내용 등 일부 분야에서는 자연어 처리에 의한 텍스트 분류가 필수적인데요. ​텍스트 분류 알고리즘은 대규모의 텍스트 데이터를 처리하는 소프트웨어 시스템에서 그 기초를 제공하고 있습니다. ​예를 들면, 이메일 소프트웨어는 텍스트 분류를 통해서 이메일에 태그를 붙여 특정한 카테고리를 나눌 수 있으며, 이를 통해 받은 편지함 또는 스팸메일함으로 편지를 보낼 수 있는 것인데요. 이것은 자연어 처리를 활용해서 텍스트를 분류하는 가장 대표적인 사례입니다. ​또한 포털 사이트, SNS 등에서 작성된 댓글이 부적절하다고 표시 될 필요가 있는지를 텍스트 분류 알고리즘을 통해서 판단하기도 하는데요. 전자상거래, 뉴스 에이전시, 콘텐츠 큐레이터와 같은 플랫폼에서도 텍스트 분류를 활용해 자동적으로 콘텐츠/ 제품/ 서비스에 태그를 달고 있습니다.​ 맞춤법 검사(SPELL CHECK)맞춤법 검사 프로그램은 어떤 텍스트 안에서 잘못 표기된 철자나 오타가 있는지를 확인하고 수정해 주는 소프트웨어 애플리케이션인데요. 대표적인 사례는 사용자들이 내용을 계속해서 작성하는 동안 맞춤법 검사를 자동적으로 수행해서 수정안을 표시해 주는 그래멀리(Grammarly)가 있습니다.​특히 맞춤법 검사 프로그램은 맞춤법을 틀리는 경우가 많은 인터넷에서도 상당히 중요한 부분을 차치하고 있는데요. 맞춤법 검사 프로그램을 통해 검색 결과에도 긍정적인 영향을 미치고 있습니다.​ 음성인식(SPEECH RECOGNITION)사람의 음성을 텍스트로 변환하여 여러 산업분야에 활용되는 기술입니다.  대표적인 사례가 시리(Siri), 구글 어시스턴트(Google Assistance), 아마존 알렉사(Alexa) 등의 음성 어시스턴트입니다.  특히 집 주인의 음성을 알아들어 지시를 수행하는 가정용 스마트 스피커로 조명, TV, 에어컨 등을 컨트롤 할 수 있는 서비스를 제공하기도 하는데요. 최근 많은 사람들이 이용하는 인공지능 스피커의 경우 음성인식 시스템을 기반으로 사람의 말을 인식하여 음악을 틀어주거나 가까운 사람에게 메시지를 보내주기도 합니다.​ ​문자 인식(CHARACTER RECOGNITION)광학 문자 인식(OCR)은 손글씨나 타이핑 글씨, 또는 인쇄된 텍스트의 이미지를 기계가 이해할 수 있는 코딩 언어로 변환하는 프로세스입니다.  인쇄된 텍스트를 디지털로 전환하기 위해서 흔히 사용되는 방법인데요. 신분증이나 여권을 인식하거나, 은행 카드/ 각종 지표/ 수표/ 티켓 등을 즉시 스캔하는 등의 문서 관련 업무에서 다양한 혜택을 제공하고 있습니다.​ ​​최근 단순 키워드 매칭을 벗어나 신경망을 도입한 새로운 검색 서비스나 인공지능 스피커 등 사람과 기계의 상호 작용이 늘어나면서 자연어 처리는 핵심 기술이 되고 있는데요. 네트워크로 연결된 다양한 스마트 장치와 사물 인터넷을 통해, 기업과 고객이 상호작용하고 소통하는 방식이 다양해 지면서 자연어 처리에 기반을 둔 AI 서비스 역시 기하급수적으로 성장하고 있습니다.​특히 전 세계 자연어 처리 시장 규모가 2020년 116억 달러에서 2026년이 되면 351억 달러로 성장할 전망이며, 2020년부터 2026년까지 자연어 처리 시장의 연평균 성장률은 20.3%에 이를 것으로 예측되고 있는데요. 아직 인간처럼 말하고, 생각하며, 성격을 갖는 AI를 기대하기에는 기술적으로 해결할 문제가 많지만 머지않은 미래에는 농담까지 주고받을 수 있는 AI 개인비서로 인해 우리의 삶이 보다 편리하고 윤택하게 변하기를 기대해봅니다.​​​​​  ​​​ ​ "
ChatGPT 모델인 NLP의 5가지 활용 사례 ,https://blog.naver.com/code00morning/223055292521,20230325,"ChatGPT 모델인 NLP의 5가지 활용 사례​ 오늘은 ChatGPT 모델인 NLP(Natural Language Processing)에 대해 알아보겠습니다.​먼저 NLP에 대해서 간단하게 알아보겠습니다.​기계가 '인간처럼 말하는 방법을 배우는 기술'을 자연어 처리(NLP)라고 부릅니다.​이 기술은 기계 사이의 상호작용을 다루는 인공지능(AI)의 한 분야입니다.​NLP는 기계가 들은 내용을 처리하고 받은 정보를 구조화하고 필요한 반응을 탐색하여 사용자가 이해할 수 있는 언어로 대응할 수 있는 능력을 제공합니다.​​ ​​첫 번째는 자동번역(Machine Translation)으로 텍스트 또는 말을 한 언어에서 다른 언어로 번역하는 소프트웨어 애플리케이션을 사용합니다.​자동번역의 가장 대표적인 사례로는 구글 번역(Google Translate)을 들 수 있습니다.​이는 예제 기반의 자동 번역 방식을 사용하는데, 더 좋은 결과를 만들어 내기 위해서 수 백 개의 예제를 학습합니다.​하지만 인간의 언어가 모호한 부분이 많기 때문에 시스템이 단어와 문장 그리고 의도를 이해하는 걸 어렵게 만들기도 합니다.​이때 자연어 처리를 통해 문제를 해결하는 데 도움을 받을 수 있습니다.  두 번째는 텍스트 예측(Text Prediction)입니다.​텍스트 예측은 어떤 구문이나 문장에서 다음에 올 단어를 예측하는 프로세스를 말합니다.​이러한 텍스트 예측에서 가장 유명하면서도 일반적인 사례는 바로 구글 검색인데요.​구글에서 미리 훈련된 모델을 생성하기 위해서 신경 네트워크를 활용하는 자연어 처리(NLP) 알고리즘인 버트(BERT)라는 기법을 사용하고 있습니다.​이 모델은 인터넷에서 사용할 수 있는 방대한 양의 주석이 달리지 않은 텍스트를 활용해서 훈련을 합니다.​버트의 알고리즘은 검색 엔진이 사람과 비슷한 방식으로 검색어를 이해할 수 있도록 도와줍니다.​그 외에도 구글 문서, 메일 작성과 같은 수많은 애플리케이션에서도 텍스트 예측이 도움을 주는 자연어 처리 모델을 활용하고 있습니다.  세 번째는 감정 분석(Sentiment Analysis)입니다.​감정 분석은 텍스트 데이터 안에서 감정을 해석하고 분류하는 프로세스입니다.​일반적으로 특정한 비즈니스와 관련된 애플리케이션을 사용하는 기업들이 감정 분석을 활용하면 온라인 피드백에서 고객이 기업의 서비스, 브랜드, 제품에 대해서 보이는 정서를 (긍정적, 부정적, 중립적) 파악할 수 있습니다.​감정 분석이 탁월한 활약을 보이고 있는 곳으로는 제품 분석, 시장 조사, 평판 관리, 정밀한 타겟팅(Targeting), 시장분석, 홍보, 순 추천 고객지수(NPS) 등이 있습니다.  네 번째는 텍스트 분류 (Text Classification)입니다.​텍스트 분류 알고리즘은 대규모 텍스트 데이터를 처리하는 소프트웨어 시스템에서 그 기초를 제공합니다.​예를 들면 이메일 소프트웨어는 텍스트 분류를 통해서 이메일에 태그를 붙여 특정한 카테고리를 나눌 수 있습니다.​이를 통해 받은 편지함 또는 스팸 메일함으로 편지를 보낼 수 있습니다.​또한 지메일 소프트웨어에서는 이메일을 기본(Primary), 소셜(Social), 프로모션(Promotion)으로 자동 분류하고 있는데, 이것은 자연어 처리를 활용해서 텍스트를 분류하는 가장 대표적인 사례입니다.  다섯 번째는 음성인식 (Speech Recognition)입니다.​오늘날에는 자연어 처리가 발전하면서 클릭이나, 타이핑, 텍스트를 선택하는 것 이외에도 목소리가 시스템에 입력하는 하나의 방식으로 받아들여지고 있습니다.​어떻게 하면 기계가 인간의 음성을 인식하고 그 의도를 이해하며 적절하게 반응하는 것을 익힐 수 있는지에 대한 완벽한 사례는 바로 코타나(cortana), 시리(Siri), 구글 어시스턴트(Google Assistant), 아마존 알렉사(Alexa) 등이 있습니다.  #ChatGPT #NLP #자연어처리 #자동번역 #텍스트예측 #감정분석 #텍스트분류 #음성인식 #AI #인공지능 "
인공지능(AI)은 어느 수준까지 발전할 수 있나? ㅡ chatGPT 답변 ,https://blog.naver.com/hcms8/223055446474,20230325,"인공지능은 어느 수준까지 발전할 수 있습니까?​인공지능(AI)의 잠재력은 방대하고 정확히 어디까지 발전할 수 있는지 예측하기 어렵습니다. 하지만, 인공지능은 최근 몇 년 동안 이미 상당한 발전을 이루었고, 이전에는 인간 지능의 독점 영역이라고 생각되었던 능력을 보여주었습니다.​현재 AI는 자연어 처리, 컴퓨터 비전, 음성 인식 및 자율 시스템을 포함한 다양한 응용 분야에서 사용되고 있습니다. 딥 러닝, 강화 학습 및 기타 기술의 발전으로 AI는 게임, 자연어 텍스트 생성, 심지어 음악 작곡과 같은 복잡한 작업을 더 잘 수행할 수 있게 되었습니다.​AI는 의료에서 운송에 이르기까지 많은 산업에 혁명을 일으킬 수 있는 잠재력을 가지고 있으며, 심지어 우리가 아직 상상하지 못한 새로운 분야를 창출할 수도 있습니다. 그것은 또한 기후 변화와 질병 통제와 같은 세계에서 가장 시급한 문제들 중 일부를 해결하는 데 도움이 될 수 있습니다.​이러한 발전에도 불구하고, AI는 여전히 편견과 윤리적 우려와 같은 도전에 직면해 있으며, 여전히 능력에 한계가 있습니다. AI가 정확히 어디까지 발전할지 예측하기는 어렵지만, 우리가 상상조차 할 수 없는 방식으로 우리의 세계를 재구성할 수 있는 잠재력을 가지고 있다는 것은 분명합니다.​​To what level can artificial intelligence advance?​The potential for artificial intelligence (AI) is vast and it is difficult to predict exactly how far it can advance. However, AI has already made significant progress in recent years and has shown capabilities that were previously thought to be the exclusive domain of human intelligence.​Currently, AI is used in a wide variety of applications, including natural language processing, computer vision, speech recognition, and autonomous systems. With advancements in deep learning, reinforcement learning, and other techniques, AI has become better at performing complex tasks such as playing games, generating natural language text, and even composing music.​AI has the potential to revolutionize many industries, from healthcare to transportation, and may even create new fields that we have not yet imagined. It could also help solve some of the world's most pressing problems, such as climate change and disease control.​Despite these advancements, AI still faces challenges, such as bias and ethical concerns, and there are still limitations in its abilities. While it is difficult to predict exactly how far AI will advance, it is clear that it has the potential to reshape our world in ways that we cannot even imagine.​ chatGPT 사용법 및 활용법 ㅡ 구글 chatGPT와 마이크로소프트 bing 그리고 생성aihttps://m.blog.naver.com/npocket/223053552361 https://openai.com/blog/chatgpt https://openai.com/ ht...m.blog.naver.com "
"[고객 사례] 1등 반려동물 앱 '아지냥이', 세대공감 스토리 '인생락서'도 네이버 클라우드 플랫폼! ",https://blog.naver.com/n_cloudplatform/221418657778,20181213,"​   국내 이용자 수 1위 반려동물 앱  '아지냥이'도,중장년 세대공감 글쓰기 서비스 '인생락서'도 모두 네이버 클라우드 플랫폼!​  반려동물 앱 '아지냥이' 국내 최다 이용자 수 반려동물 모바일 앱 '아지냥이'애견·애묘인들을 위한 모바일 플랫폼으로일정 및 건강 관리, 음악 듣기와 게임 기능까지,반려동물을 위한 다양한 서비스를 이용할 수 있는 모바일 앱입니다.[Google Play] [App Store]​​  온라인 커뮤니티 '인생락서' 어른들을 위한 일상 커뮤니티 '인생락서'쉴 틈 없이 살아오신 우리네 어머니, 아버지, 대한민국 모든 중장년들을 위한 추억·공감 글쓰기 커뮤니티입니다.향수를 자극하는 다양한 사진과 스토리를 공유할 수 있습니다. [Google play] [App Store]  안녕하세요, 네이버 클라우드 플랫폼입니다!​오늘 소개해드릴 고객 사례는,'삼성카드'에서 운영 중인 온라인 커뮤니티 플랫폼인'아지냥이'와 '인생락서' 서비스입니다.​반려동물 관련 유익한 정보를 공유하는 '아지냥이',그리고 바쁜 일상 속 힐링을 주는 '인생락서',모두 네이버 클라우드 플랫폼을 통해 안정적으로 서비스를 운영하고 있습니다.​아래 인터뷰를 통해 '삼성카드'의 커뮤니티 서비스가 어떤 상품을 활용하고 있으며, 어떤 점 때문에 네이버 클라우드 플랫폼을 선택하게 되었는지  함께 살펴보시죠!  ​  interviewee : 삼성카드 김형주 프로  Q. 삼성카드의 온라인 커뮤니티 서비스에 대해 소개해주세요.A. '아지냥이'와 '인생락서'는 바쁜 현대인들의 지친 마음을 위로해줄 소통 공간을 마련하기 위해 시작된 업계 최초의 온라인 커뮤니티 서비스입니다.삼성카드는 2014년 3월 ‘영랩’을 시작으로 2016년 1월에는 ‘베이비스토리’, 2017년에는 ‘키즈곰곰’, ‘아지냥이’, 그리고 지난 1월에는 ‘인생락서’ 등 고객들의 주요 관심사를 공유하는 온라인·디지털 기반 커뮤니티 서비스를 업계 최초로 운영하고 있습니다. 커뮤니티 서비스를 통해 고객과 사회 현안에 대해 공유하고 소통하는 등 기업과 사회가 상생하는 에코시스템을 구축해 각박한 경쟁 사회 속에서 지치고 힘든 사람들에게 심리적 안정과 힐링을 주는 ‘디지털 소통’ 공간을 제공하고자 합니다.  ​ Q. 네이버 클라우드 플랫폼의 사용을 결정하게 된 계기는 무엇인가요?A.  음성 인식 서비스의 정확도가 높았고, 제공할 서비스에 딱 맞는 기능을 제공하고 있는 네이버 클라우드 플랫폼을 도입하기로 결정하게 되었습니다.중장년층 시니어를 대상으로 하는 글쓰기 서비스인 ‘인생락서’를 준비하며, 어떻게 하면 더 쉽게 글쓰기에 가깝게 다가갈 수 있을지 고민하였습니다. 자신이 살아온 이야기를 스마트폰을 통해 표현하기에는 화면 상의 키패드가 너무 작고 불편하였습니다. 이때 떠올린 것이 사람이 말하는 것을 바로 받아 텍스트로 적어주는 음성인식 기능이었고, 이를 구현하기 위해 다양한 STT 솔루션을 검토하게 되었습니다. 테스트 결과 다른 솔루션보다 네이버 클라우드 플랫폼의 음성 인식 정확도가 높은 편이기도 했고, 서비스에서 요구하는 특정한 기능을 네이버 클라우드 플랫폼에서만 지원해줬던 것이 결정하게 된 가장 큰 계기가 아니었나 싶습니다. 물론 비용적 측면에서도 타 서비스에 비해 경쟁력이 높았고요.​ Q. 네이버 클라우드 플랫폼의 어떤 상품을 어떻게 사용하고 계시나요?A. Clova Speech Recognition으로 음성 인식 기능을 구현하고, Chatbot 및 API Gateway를 통해 고객 문의 답변에 활용하고 있습니다.‘인생락서’ 서비스는 Clova Speech Recognition(CSR), ‘아지냥이’ 서비스는 Chatbot 과  API Gateway​를 사용하고 있습니다. ‘인생락서’의 음성인식 기능은 편리한 UX 제공이라는 본연의 역할뿐만 아니라, 글쓰기 애플리케이션 중 기술적으로 차별화되어 고객들에게 어필하는 역할도 함께 해주고 있습니다. ‘아지냥이’의 경우, 사용자들이 반려동물에 대해 궁금한 것들이 있으면 기본적인 탐색 기능과 수의사 1:1 상담을 통해 원하는 답을 얻어 갈 수 있었는데요. 이번에 Chatbot을 도입하여 원하는 답을 찾는 데까지 걸리는 시간을 단축시켜 고객의 시간은 물론, 문의에 답변을 다는 수의사 선생님의 시간도 함께 아낄 수 있기를 기대하고 있습니다.​ Q.  네이버 클라우드 플랫폼이 갖는 특징 혹은 장점은 무엇인가요? A. 인공지능 서비스의 뛰어난 한국어 인식 능력과 신속한 고객지원이 큰 강점이라고 생각합니다.저희는 네이버 클라우드 플랫폼의 AI 서비스를 주로 사용하고 있습니다. 아무래도 AI는 언어와 밀접한 관련이 있는데, 한국어를 인식하고 분석하는 능력은 네이버 클라우드 플랫폼이 가장 월등하지 않을까 생각합니다. 특히 고객지원과 관련해서도, 다른 해외 클라우드 서비스보다 더욱 즉각적이고 친절하다는 장점도 있습니다.​ Q. 귀사가 지향하시는 서비스 방향은 무엇인가요? A. 앞으로 네이버 클라우드 플랫폼의 서비스들을 더욱 적극적으로 활용하고, 기존의 서비스와도 결합하여 한층 더 발전한 서비스를 제공하고자 합니다.단기적으로는 유아 창의교육 서비스인 '키즈곰곰'에서도 Clova Speech Recognition(CSR)를 적용할 계획을 가지고 있고, 나아가 네이버 클라우드 플랫폼이 제공하는 경쟁력 있는 서비스들을 더욱 적극적으로 사용하고자 합니다. 얼마 전 '아지냥이'에 적용한 Chatbot의 답변 성능이 안정화될 때쯤이면 CSR과 결합하여, 음성으로 반려동물에 대한 궁금한 것들을 묻고 답할 수 있게 되지 않을까 생각합니다.​ Q. 클라우드 도입을 고민하는 담당자에게 한 마디만 해주세요.A. 시간과 비용을 절약할 수 있는 클라우드 서비스의 도입을 적극적으로 검토해보시기를 권유 드립니다.한정된 비용으로 정해진 기간 내에 필요한 기능을 만들어 내고자 한다면 일단 클라우드 서비스를 적극적으로 검토해보시기를 권유 드립니다. 삼성카드도 네이버 클라우드 플랫폼의 AI 서비스와 함께 고객분들께 더욱 유용한 서비스를 제공할 수 있도록 노력하겠습니다. 감사합니다.​   아지냥이 다운로드 받기   [Google Play] [App Store]   인생락서 다운로드 받기   [Google play] [App Store] [고객사례] ""타사의 음성 인식 API보다 옵션이 다양하고 속도도 빨라서 만족스럽습니다."" - 클로바 음성 인식 기술을 이용한 앱 개발 사례　추후엔 네이버 클라우드 플랫폼의 얼굴 인식 API인 Clova Face Recognition(CFR) 상품을 사용해...blog.naver.com [7월 신규 상품] ""AI Service"" - 네이버의 인공지능 플랫폼 '클로바(Clova)'와 통번역 기술 '파파고(Papago)의 API 제공!AI Service가 여러분의 서비스를 한 걸음 앞선 비즈니스로 만들어드립니다. 궁금한 걸 물어보면 바로 찾...blog.naver.com   ​ "
[논문리뷰-37] A review on the long short-term memory model ,https://blog.naver.com/mindmerge/223078924106,20230419,"# A review on the long short-term memory model​ Long short-term memory (LSTM) is a type of recurrent neural network (RNN) that has gained a lot of attention and popularity in recent years due to its ability to effectively model sequential data. LSTMs have been successfully applied to various tasks such as speech recognition, machine translation, and natural language processing.​The basic idea behind LSTMs is to introduce memory cells that can store information for a long period of time and selectively forget or remember information depending on the context. ​The key components of an LSTM cell include an input gate, an output gate, a forget gate, and a memory cell. These gates control the flow of information into and out of the cell and the memory cell stores the information.​One of the major challenges in training RNNs is the problem of vanishing and exploding gradients, where the gradients can become very small or very large and cause the network to either converge very slowly or not converge at all. LSTMs address this problem by using a set of gating mechanisms that allow the gradients to flow smoothly through the network, making it easier to train deep RNNs.LSTMs have been applied to a wide range of applications in natural language processing, speech recognition, image and video processing, and time series analysis. For example, Google's speech recognition system uses LSTMs to model the temporal dependencies in speech signals and has achieved state-of-the-art performance on several benchmark datasets. Similarly, Google Translate uses LSTMs to model the sequence-to-sequence mapping between languages and has significantly improved the quality of machine translations. Amazon's Alexa also uses LSTMs to model the context of user queries and generate more accurate responses.​In terms of code resources, LSTMs are readily available in popular deep learning frameworks such as TensorFlow, PyTorch, and Keras. These frameworks provide easy-to-use APIs for building and training LSTM models, and also offer pre-trained models that can be fine-tuned for specific applications.In summary, LSTMs have revolutionized the field of deep learning and have become a go-to model for various applications that involve modeling sequential data. Their ability to handle the vanishing and exploding gradient problem has made them particularly effective for training deep RNNs, and their success in various applications is a testament to their power and versatility.​this paper is a comprehensive review of the LSTM model, focusing on its theoretical foundations, practical applications, and code examples. ​The authors also conducted a literature search and selected papers that made relevant contributions to the field using LSTM. The paper is structured as follows:Section 2 describes the theoretical foundations behind the LSTM model.Section 3 provides a concise description of a procedure to adjust the learnable parameters.Section 4 zooms in on different applications of the model as found in the literature.Section 5 provides an example implementation in Tensorflow.Finally, Section 6 summarizes the conclusions of the paper.​It's great to see a comprehensive review of the LSTM model and its practical applications, as it has been shown to be a highly effective tool in many areas of deep learning.​ ​ ​ ​ ​The LSTM (Long Short-Term Memory) model is a type of recurrent neural network that is designed to overcome the vanishing and exploding gradient problems typically encountered when learning long-term dependencies. ​These problems occur even when the time lags are long. The LSTM model overcomes these problems by using a constant error carousel (CEC) that maintains the error signal within each unit's cell. The LSTM cell is a recurrent network in itself, and it is made up of a memory cell that is extended with additional features, namely the input gate and output gate, to form the memory cell. The self-recurrent connections indicate feedback with a lag of one time step.A vanilla LSTM unit consists of a cell, an input gate, an output gate, and a forget gate. The forget gate allows the network to reset its state. The cell remembers values over arbitrary time intervals, and the three gates regulate the flow of information associated with the cell. The LSTM architecture consists of a set of recurrently connected sub-networks known as memory blocks. The memory block maintains its state over time and regulates the information flow through non-linear gating units.​To clarify how the LSTM model works, let us assume a network comprised of N processing blocks and M inputs. The forward pass in this recurrent neural system involves updating the block input component, which combines the current input and the output of that LSTM unit in the last iteration. The input gate is then updated, which combines the current input, the output of that LSTM unit, and the cell value in the last iteration. The LSTM unit determines which information should be retained in the network's cell states. The forget gate determines which information should be removed from the previous cell states. The cell value is then computed, which combines the block input, the input gate, and the forget gate values, with the previous cell value. Finally, the output gate is calculated, which combines the current input, the output of that LSTM unit, and the cell value in the last iteration. The block output is then calculated by combining the current cell value with the current output gate value.​ ​biLSTM은 LSTM과 비슷하지만, 양방향 연산을 수행하여 입력 시퀀스를 순방향과 역방향으로 모두 처리합니다. 이것은 순방향과 역방향에서의 정보를 모두 고려하여 출력을 생성하므로, LSTM보다 더 많은 정보를 활용할 수 있습니다.따라서 biLSTM은 순차적인 데이터를 처리하면서 더 많은 context를 고려하고, 긴 시퀀스에서도 장기 의존성을 더 잘 다룰 수 있습니다. 그러나 biLSTM은 계산 비용이 더 높으며, 학습 데이터가 적은 경우 과적합될 가능성이 더 높습니다.LSTM과 biLSTM 중 어떤 것을 사용할지는 문제의 성격과 데이터의 특성에 따라 다릅니다. LSTM은 단방향 연산이며, 계산 비용이 낮고 데이터가 적은 경우에 더 적합합니다. biLSTM은 더 많은 context와 장기 의존성을 고려해야 할 때 더 유용합니다.​the applications of the LSTM architecture are diverse and numerous. Some other examples include:Video Analysis: LSTM can be used for video analysis, such as activity recognition, motion tracking, and video captioning. For instance, in the work of Venugopalan et al. (2015), LSTM was used to generate descriptions of videos by encoding the video frames and decoding the generated text.Stock Market Prediction: LSTM can be applied to financial time series forecasting, such as stock price prediction, based on historical data. For example, in the study of Nair and Hegde (2018), LSTM was used for predicting stock prices of Indian companies.Natural Language Processing: LSTM has been widely used for various natural language processing tasks, including sentiment analysis, machine translation, and text summarization. In the work of Vaswani et al. (2017), LSTM was combined with attention mechanism to improve the performance of neural machine translation.Robotics: LSTM can be used for robotic control, such as grasping, manipulation, and navigation. For instance, in the study of Hochreiter et al. (2007), LSTM was used to learn the inverse dynamics of a robotic arm, which allows for precise control of the arm's movements.Energy Forecasting: LSTM can be applied to energy forecasting, such as predicting electricity demand, based on historical data. For example, in the study of Zhang et al. (2018), LSTM was used for short-term load forecasting in a smart grid system.Overall, the LSTM architecture has demonstrated its effectiveness in a wide range of applications, showcasing its capability to capture and learn temporal dependencies and long-term memory. "
"강남영어회화학원, 월스트리트 잉글리쉬 멀티미디어 새로운 기능 (동영상으로 만나보세요!!!) ",https://blog.naver.com/tlswlgprjt/221598472497,20190729,"​ 강남영어회화학원, 월스트리트 잉글리쉬 멀티미디어 새로운 기능 (동영상으로 만나보세요!!!)  안녕하세요, 딘디입니다 :)오늘도 역시 강남영어회화학원 '월스트리트 잉글리쉬' 관련 포스팅이에요.오늘은 업데이트된 멀티 미디어 기능소개해드릴게요!그럼 먼저 동영상으로 해당 기능 먼저 보여드릴게요.​​  강남영어회화학원 추천, 월스트리트 잉글리쉬의메인 수업인 EC를 참여하기 전에꼭 들어야하는 '멀티 미디어'라는 영상 학습 단계가 있어요.이 때 원어민이 한 문장이나 영어 발음을따라하는 섀도잉(SHADOWING) 부분이 있어요.직접 본인의 음성을 녹음해야하는 파트인데,이번에 새로 음성인식 & 분석 기능이 도입되어서 색깔별로 즉각적인 피드백이 나온답니다.바로 Automatic Speech Recognition(자동 음성 인식 기능) 인데요,사진으로 천천히 설명해드리도록 할게요.​​​​     제일 먼저, 3-4분 남짓되는 클립 한 편을 쭉 시청합니다.(영상 클립 길이는 레벨마다 달라요!)​​       그 뒤, 영상에 관련된 간단한 질문들이 나오고,내용을 간단하게 체크해요.저는 다 맞아서 100점 ㅎㅎㅎㅎㅎ​   그 뒤, 영상 속 주인공들이 한 문장을들려주고, 그 문장을 따라하는 섀도잉 파트가 나옵니다.자신의 목소리로 직접 녹음을 해야해요!​​​   먼저, 영상 주인공이 했던 문장을 듣고,Record(녹음)을 눌러 그 문장을 따라 합니다.​​​   그 뒤, 자동으로 녹음된 목소리를인식을 해요.​​​   단어나 억양, 발음들을 잘 따라했다면초록색 GOOD,보통이면 노란색 SO-SO,연습하여 재녹음이 필요하다면빨간색 IMPROVE가 나와요.저는 매끄럽게 잘 말했기 때문에모든 단어, 문장이 초록색으로 나왔어요 :)​​   이렇게, 단어 하나 하나 다 짚어줘서잘 알아듣지 못한 단어는 무엇이었는지,스펠링은 어떤지, 모두 확인 할 수 있어서혼자 학습 할 때 훨씬 더 좋더라구요.​​강남영어회화학원,월스트리트 잉글리쉬만의 최첨단 자동 음성 인식 기능을 추가한 멀티 미디어.저와 함께 영어 공부 하시지 않을래요?제가 말씀드렸던 EC에 관련된 내용이궁금하시다면 아래 포스팅 클릭해주세요 :)​​강남영어회화학원 추천,월스트리트 잉글리쉬 커리큘럼 👇​ 영어회화학원 추천, 월스트리트 잉글리쉬 강남센터영어회화학원 추천, 월스트리트 잉글리쉬 강남센터안녕하세요, 딘디입니다. 포스팅을 제목을 보고 읭? 하신...dindihye.com ​월스트리트 잉글리쉬 가격 관련 👇 월스트리트 잉글리쉬 가격 정보 & 할인 이벤트 - 강남 원어민 영어 회화 학원 추천 💚월스트리트 잉글리쉬 가격 정보 & 할인 이벤트강남 원어민 영어 회화 학원 추천 💚안녕하세요, 딘디...dindihye.com 월스트리트 잉글리쉬 강남센터서울특별시 서초구 서초대로77길 55 에이프로 스퀘어  강남영어회화학원, 월스트리트 잉글리쉬 강남센터서울시 서초구 서초대로77길 55 에이프로 스퀘어​​​​월스트리트 잉글리쉬에 상담시제 블로그 닉네임 [딘디 or 신지혜]를 말하면소정의 베네핏이 있는데요.강남 뿐만 아니라 종로, 신촌, 삼성 등전국 모든 센터에서 혜택을 받으실 수 있습니다.단, 퍼스널 튜터 or 컨설턴트와 상담을 하실 때 블로그 닉네임이나이름을 꼭 밝혀주세요! "
[도서] 수학을 읽어드립니다 - 남호성 ,https://blog.naver.com/joycestudy/222744670365,20220524,"""수학과 코딩을 가르치는 별난 영문과 교수의 특별하고 재미있는 수학이야기""라는 부제에 매료되어 읽어봤습니다. 문과생(?)답게 글을 매끄럽게 잘 쓰는 저자 덕분에 술술 읽힙니다. ​https://www.aladin.co.kr/shop/wproduct.aspx?ItemId=275091594&ttbkey=ttbjoycestudy1652001&COPYPaper=1 수학을 읽어드립니다지금까지의 자신의 경험담을 토대로 이 시대에 수포자로 살아가고 있는, 또 앞으로 수포자가 될지도 모르는 많은 사람들에게 수학의 쓸모는 물론 우리가 왜 수학을 공부해야 하는지에 대해 일깨우고, 앞으로 인공지...www.aladin.co.kr ​저자 개인의 독특한 여정이 눈에 띕니다. 고등학교 때 수학을 곧잘 했었으나, 문과/이과 나뉘는 시점에 수학을 자발적으로 포기(?)하고, 좀더 쉬워보이는 문과를 선택했다고 합니다.​문과/이과, 이 구별은 왜 아직도 그대로 유지되는 걸까요? 혹시 이것에 대해 아시는 분이 계시다면, 그 필요성과 효용을 알기 쉽게 설명해줬으면 좋겠습니다. 모든 학문 분야가 넘나들고 융합되는 21세기인데도 아직도 문과/이과를 나누고 있다는 게 참 신기합니다. ​잠깐 검색해보니, 문과/이과 통합 정책은 많이 나왔었는데, 정작 문과/이과 계열이 나뉘어져 있는 현행 대학 입시 체계 때문에 어쩔 수 없이 유지되고 있는 모양입니다. 문이과 통합 수능 입시 제도가 2022년까지 연기되었다고 하니까, 내년부터는 달라지는 걸까요?​https://namu.wiki/w/%EB%AC%B8%EC%9D%B4%EA%B3%BC%20%ED%86%B5%ED%95%A9 문이과 통합 - 나무위키1. 개요 통합교육 의 일환으로 초중등교육에서 문과 와 이과 를 통합해서 가르치는 것. 우리나라는 지금까지 고등학교 부터 문과 와 이과 를 구분해서 가르쳤지만 2018년 부터 문이과를 통합한다고 박근혜 정부 에서 발표했다. 그리고 2017년 8월 31일에 문이과 통합을 포함한 모든 수능 개편제도를 1년 연장하여 2022학년도 수능 시험부터 위 사항이 적용되게 된다. 하지만 각 대학이 문이과 선택 과목을 확실하게 지정하면서 2000년대 중반 7차 교육과정 때와 똑같은 현상이 반복되었고 문이과 통합 정책은 완전히 무용지물이 되고 말았다...namu.wiki 암튼, 수학 공부 안해도 되는 문과로 가서는 고려대 영문학과에 진학한 저자는 문학 대신 뭔가 더 있어보이는 언어학 계열의 '음성학'을 공부합니다. 석사과정까지 했던 걸 보면 나름 재미있었나 봅니다.​지도 교수님에게서 당시 한국통신(지금의 KT)에서 자동 음성인식(Automatic Speech Recognition, ASR) 시스템 개발 프로젝트 참여를 권유받았던 게 하나의 전환점이 됩니다. 아무리 언어학 지식을 들이대며 아는 체를 한다 해도 실질적인 작업 성능이 최우선인 공학적 목표 앞에서는 그 모든 것이 공염불이었다. 그 당시 받은 모욕과 분노는 내 인생을 바꾸는 첫 계기가 되었다.이걸 계기로 저자는 대학원을 박차고 나와 컴퓨터 학원에서 컴퓨터와 코딩을 배웁니다. 하지만, 현실의 장벽은 막막하고 높았답니다. 문학 석사 출신 백수인데 코딩 조금 할 줄 아는 사람이 취업할 수 있는 곳은?​마침 운이 닿았는지, 삼성그룹 공채에 응모해서 아주 특이한 전공 설명을 계기로 합격했고, 이후 삼성SDS에서 피씨 통신 유니텔 사업부 소속으로 많은 프로그래밍 경험을 쌓습니다. 그러다 1년 반의 시간이 흐르고, 회사를 나와서 덜컥 유학을 떠납니다. 또 다른 터닝 포인트. ​예일대학교 해스긴스 연구소. 동양인 최초로 합격했다고 하는데, 언어학 전공이면서 코딩을 할 줄 아는 게 아주 큰 점수를 얻은 이유라고 합니다. 점이 선으로 이어지는 의외의 반전인 셈이죠. 7년간 유학 생활.​모교인 고려대 영문과 교수를 돌아오게 되는데, 이런! 이젠 인문학의 죽음이 일상화된 탓인지, 완전 썰렁한 인문대 분위기에 놀랐다고 합니다. 대학원생은 두 명뿐이고, 학부생 역시 취업 준비에 열중할 수 밖에 없는...​제1호 영문과 제자와 코딩 공부부터 시작하는 저자. 남즈(NAMZ)라는 언어공학 연구소 설립. 연구실도 컴퓨터도 없이 떠돌며 스터디. 영문과 대학원생들이 우르르 공대수학 청강. ​그러다 운명의 날. 현대/기아자동차 음성 솔루션 담당하는 미디어젠 사장님과의 만남. 미디어젠의 연구소로 출범. 문과생만으로 이룬 연구소. 음성인식 기술뿐 아니라 인공지능에 기반한 다양한 언어 지능 분야로 업무를 확장. 6년만에 튼튼한 성장을 이룸. ​문과/이과 구분 자체를 이제는 넘어서야 한다는 주장. 실제로 수십 년 동안 많은 학생들이 고교 계열 구분을 통해 수학을 하는 집단과 수학을 하지 않아도 되는 집단으로 갈려 혜택과 불이익을 당하고 있다. 이를 통해 수학 공부의 의무를 면제받았다고 생각한 인문계 학생들은 사실상 수학을 공부할 권리를 박탈당한 것인지도 모른 채, 문과형 인간이라는 프레임에 갇혀 살아간다. 하지만 이는 국가와 개개인의 미래 잠재성을 상당 부분 거세당하게 하는 잘못된 교육정책에서 비롯된 것이라고 생각한다. 정보가 부족한 어린 학생과 부모에게 이러한 결정을 하도록 자유의지를 넘기는 제도는 당장 없어져야 한다고 본다. 저자는 우리나라 수학 교육 자체에 대한 비판도 멈추지 않습니다. 다음은 고등학교 중간고사 수학문제라고 합니다. 참, 열심히도 꼬아 놓았습니다. 이런 걸 풀어내면 수학적 천재가 되는 걸까요?​ ​저자는 수학 교육이 수학자를 기르기 위한 교육처럼 보인다고 비판합니다. 초·중·고등학교 수학 교과서를 보면 모두 수학자를 양성하는 정규 과정의 일부처럼 보인다. 우리나라의 모든 사람이 의무교육으로 수학자의 길을 밟아 가고 있는 셈이나 다름없다. 어떻게든 수학에 흥미를 잃지 않도록 해도 모자랄 판에, 수학자의 길이 웬 말인가? 수학자의 길과 수학으로 먹고살 사람들의 길은 다르다고 본다. 다른 것은 다르게 가르쳐야 한다. 수학자 양성에 필요한 교육 말고, 수학을 써먹을 사람에게 필요한 교육으로 전환하자고 주장합니다. 현재의 수학 교육은 ‘수학자’들에게 점령당해 있지만, 우리 학생들은 학문적 수학을 더 이상 강요받지 말아야 한다. 수학을 잘 써먹어서 취직을 하고 먹고살아야 할 미래의 인재들이기 때문이다. 이 엄연한 괴리를 인정하고 줄이자. 수학은 수학을 써먹어본 사람들이 책을 쓰고 학생을 가르치는 게 맞다. 그래야 필요한 것만 가르치고 쓸데없는 데 시간 보내지 않을 것이고, 왜 필요한지를 제대로 전달할 수 있을 것이다. ​수학 올림픽에서 문제 풀이 왕을 배출하는 것이 수학 선진국으로 가는 길이라는 헛된 신화에서 벗어나 철저히 필요에 따라 가르치고 배우는 수학으로 거듭나야 한다. 저자는 조금 다른 수학 교육을 제안합니다. 보이는 수학, 말하는 수학, 쓸모 있는 수학. 그리고, 다섯 가지: 함수, 미분, 행렬, 벡터, 확률 분야를 중심으로.​ 저자의 지적처럼 현재의 수학 교육이 문제가 많은 것은 누구나 인정하는 사실 같습니다. 하지만, 제 생각에는 현재의 수학 교육이 수학자 양성 교육이어서 문제인 것은 아닌 것 같습니다. 현재의 수학 교육으로 극히 소수일 지라도 제대로 된 수학자가 양성되고 있는지도 의문이기 때문입니다. 다른 나라의 저명한 수학자들이 우리나라와 같이 고난이도의 배배꼬인 수학문제를 제한시간 내에 실수없이 풀어내는 고강도 수련과정을 거쳐서 태어났을 것 같지 않기 때문입니다. 오히려 그 정반대 같습니다. 우리 수학 교육은 혹시 있었을지도 모를 수학 천재들을 아주 이른 시기에 말살하는 데 특별히 더 효과적이었을 듯 합니다. 수학이 재미있어 지기는 커녕 일찌감치 포기하거나 싫어하거나 증오하게 되지 않았을까요?​제 생각에 이 모든 사단은, '입시'에 있는 듯 합니다. 줄세우기를 해야만 하는 한국 대학 입시 때문입니다. 대입 시험 변별력을 위해 '수학'이 사용되는 현재와 같은 상황에서는 모든 게 꼬일 수 밖에 없습니다. 사실 '수학'만큼 명분도 좋고 난이도 조정도 쉬운 과목이 없는 듯 합니다. 다른 인문 과목들과는 다르게 정답도 논란없이 명확하게 나오니까 채점하기 참 간편합니다. ​유명 대입 수능 수학 강사의 연봉이 100억원을 넘어간다는 기이한 나라에서 수학이 참 고생하고 있다는 생각이 듭니다. 수학 교육계(?)에 몰리는 사회적 자원은 엄청난데, 정작 수학은 전혀 발전하지 않는 이상한 나라인 셈입니다. 발전은 커녕 이 나라에서 수학과 수학 교육은 그냥 학생들을 괴롭히고 고문하는 도구가 되어 버린 건 아닌가란 씁쓸한 자조마저 나옵니다.​입시를 위한 수학이 아니라, 그냥 '수학'을 가르치고 배울 수 있는 환경이 될 수 있기를 바래 봅니다. 지금보다는 수학을 재미있어 하는 사람들이 좀더 많아질 것이라고 생각합니다. 여러 부문에서 수학을 활용해서 여러 가지 문제를 풀어가는 활동이 좀더 다채롭게 펼쳐질 수 있을 것이라고 예상해봅니다. 그래야 수학도 더 발전하고 수학의 힘으로 다른 분야들도 더 발전해갈 수 있을 듯 합니다.​그럴 수 있는 날이 언제쯤 올까요? 과연 오기는 할까요?​​ "
머신러닝과 인공지능_15가지 일상 적용 ,https://blog.naver.com/meta_com/222872701882,20220912,"R-bloggers라는 R 이용자들이 애용하는 사이트에서 보내오는 뉴스레터에 실린 글이다.(https://www.r-bloggers.com/2022/09/machine-learning-impact-on-your-day-to-day-life/?utm_source=phpList&utm_medium=email&utm_campaign=R-bloggers-daily&utm_content=HTML)머신러닝이 우리 생활에 어떻게 영향을 미치고 있는지, 미칠 지에 대해 15가지로 정리해 소개하고 있다. 컴퓨터 공학을 하지 않은 일반인도 인공지능의 세계는 앞으로 더 친숙해질 세상이니 조금씩 발을 담근다는 차원에서 한번 읽어볼 만하다. 15개 항목의 카테고리와 간단한 사례 중심으로 한글로 옮겨 본다.​ 1.     인텔리전트 게임 – 2016년 이세돌 알파고 대결 사례2.     자율주행차3.     사이보그 (인간 신체 일부가 기계로 대체된)- 부상으로 팔을 잃은 사람이 기계 팔을 달고 그 기계 팔이 뇌와 소통4.     위험한 일 대체- 폭탄 투하, 용접 등 (폭탄 투하 드론을 조종하는 AI, 용접 로봇)5.     환경보호-대용량 환경데이터를 분석하여 기후변화 대응전략 수립과 그 효과 검증에 활용6.     디지털 감정이입과 로봇친구- 인간 감정을 이해하고 흉내낼 줄 알고, 감정을 북돋우는 스킬 보유한 감정로봇7.     노인 간호- 요양보호사의 일을 대체8.     의료서비스 향상- 예) 난치영역인 유전병 치료 연구에도 머신러닝의 예측 기술을 활용 9.     은행업무 개선- 예) 이상 거래 감지 기능10.  개인 수요 맞춤형 디지털 미디어 – 개인 시청 데이터에 기반한 콘텐츠 추천 시스템, AI기사 작성, AI활용 소설 창작11.  주택 자동 보안시스템과 집안일 자동화 – 예) 재활용품 자동 분리 12.  물류 자동화13.  개인 일상 지원-저녁 요리 메뉴 자동 결정, 퇴근 전 이미 배달돼 있는 저녁 요리 재료.   14.  건물과 AI – 전통적 소매업과 AI의 결합으로 오프라인 소매업이 없어지거나 변형15.  개인 맞춤형 뉴스와 시장정보​Machine Learning Impact on your day-to-day life, One of the most significant technological advancements since the microprocessor is currently thought to be artificial intelligence (AI), more especially machine learning.AI used to be a fantastical idea from science fiction, but it is now a part of everyday life.Deep learning advances in machine learning are being made possible by neural networks, which mimic the function of actual neurons in the brain.If we know how to use machine learning’s capabilities, it can make our lives happier, healthier, and more productive.According to some, AI is starting a new “industrial revolution.” The current Industrial Revolution will make use of cerebral and cognitive abilities, as opposed to the previous one, which made use of physical and mechanical strength.Computers will eventually replace both mental and manual labor. But how precisely will this take place? And is it already taking place?Here are 15 ways that machine learning and artificial intelligence will affect your daily life.Machine Learning Impact on your day-to-day life!​Surprising Things You Can Do With R »​1. Intelligent Gaming Some of you may recall the chess match between IBM’s Deep Blue and Gary Kasparov in 1997.But even if you weren’t around back then, you might recall how Lee Sedol, the Go world champion, was defeated by Google DeepMind’s AlphaGo in 2016.Go is a traditional Chinese game that is significantly harder for computers to learn than chess.However, AlphaGo was carefully educated to play Go, not by merely studying the movements of the very best players, but by learning how to play the game better by repeatedly playing against itself.Python is superior to R for writing quality codes »​2. Self-Driving Cars and Automated Transportation Have you recently taken a flight? If so, you have already had professional experience with transportation automation.These contemporary commercial aircraft detect their location while in flight using the FMS (Flight Management System), which combines GPS, motion sensors, and computer systems.So, on average, a Boeing 777 pilot only spends seven minutes flying the aircraft manually, and most of those minutes are used during takeoff and landing.The transition to autonomous vehicles is more challenging. There are more vehicles on the road, hazards to avoid, and restrictions in terms of traffic patterns and laws that must be taken into consideration.However, autonomous vehicles are now a reality. A study with 55 Google vehicles that have driven over 1.3 million miles in total found that these AI-powered automobiles are even safer than human-driven cars.The issue with navigation has long since been resolved. Your smartphone’s GPS already feeds Google Maps with location information.It is possible to calculate how quickly a device is moving by comparing its location from one moment in time to the next. Simply put, it can determine in real-time how slow the traffic is.This information can be combined with user-reported incidents to provide a picture of the flow of traffic at any given time.Based on traffic congestion, construction zones, or accidents between you and your destination, maps can suggest the quickest route for you.What about the ability to really drive a car, though? In other words, machine learning enables self-driving cars to instantly adjust to altering road conditions while also picking up new roadside information.Onboard computers can make split-second choices even faster than skilled drivers by continuously analyzing a flood of visual and sensor data.It isn’t magic. It is built on the same machine learning principles that are utilized in other sectors.Real-time visual and sensor data are the input features, and choosing from a range of potential future “actions” for an automobile is the output feature.ML Self-Driving CarsThese autonomous vehicles do indeed already exist, but are they ready for widespread use? Maybe not yet, given that drivers are now required to be in the cars for safety.Consequently, the technology isn’t yet flawless despite significant advancements in this new field of autonomous transportation. You’ll undoubtedly want to own one of these automobiles yourself in a few months or years, though.Data Analytics Courses for Beginners-Certifications »​3. Cyborg TechnologyIt goes without saying that our bodies and minds have inherent limitations and flaws.Shimon Whiteson, an Oxford C.S. professor, predicts that as technology advances, we will be able to use computers to supplement some of our deficiencies and limits, increasing many of our inherent skills.But hold on—before you start visualizing apocalyptic worlds made of steel and flesh, think for a second about how, in a sense, most people are already “cyborgs” as they go about their daily lives.How many folks are you aware of who could get by without their dependable smartphone?We already rely on these portable computers for a variety of tasks, including communication, navigation, knowledge acquisition, getting essential news, and many more.Yoky Matsuoka of Nest is another person who thinks people who have lost limbs will benefit from AI. A robotic limb will eventually be able to communicate with the brain.Amputees will have more influence over their daily lives and face fewer restrictions because of this technology.Machine Learning Impact on your day-to-day life!Which programming language should I learn? »​4. Taking Over Dangerous Jobs​Bomb disposal is one of the riskiest jobs. Today, among other dangerous tasks, robots (or, more precisely, drones) are replacing them.The majority of these drones are currently controlled by humans. However, if machine learning technology advances, these duties would eventually be carried out entirely by AI-powered robots.Thousands of lives have already been saved by this technique alone.Welding is yet another task that robots are taking over. Intense heat, noise, and poisonous gases are all byproducts of this type of employment.These robot welders would need to be pre-programmed to weld in a certain spot absent machine learning.However, improvements in computer vision and deep learning have made it possible to be more adaptable and accurate.Data Science Statistics Jobs  » Are you looking for Data Science Jobs?​5. Environmental ProtectionMore data may be stored and accessed by machines than by any one person, including astounding numbers.One day, AI could analyze massive data to find trends and then use that knowledge to find answers to problems that weren’t previously solvable.To find climate mitigation strategies, for instance, IBM’s Climate & Sustainability Program employs AI to evaluate environmental data from thousands of sensors and sources.Additionally, their tools enable municipal planners to simulate potential environmental impact reduction strategies and conduct “what-if” scenarios.And that’s only the start. Every day, new ideas that are exciting for the environment hit the market, from distributed energy networks to self-adjusting smart thermostats.Training and Testing Data in Machine Learning »​6. Digital Empathy and Robots as Friends Still lacking in emotion, most robots. However, a Japanese company has taken the first significant steps toward creating an emotional and understanding robot partner.Pepper the companion robot was unveiled in 2014, and when it went on sale in 2015, all 1,000 units were gone in under a minute.The robot’s software was designed to understand human emotions, mimic them, and support its human friends’ emotional well-being.Machine Learning Impact on your day-to-day life!What are the algorithms used in machine learning? »​ 7. Improved Elder CareMany seniors find it difficult to complete routine duties. Many are forced to rely on family members or pay outside aid. Many families are becoming more concerned about elder care.According to Washington State University computer scientist Matthew Taylor, AI is at a point where replacing this need won’t be too far off.In-home robots could provide assistance to elderly family members who don’t want to leave their homes. With that option, managing a loved one’s care is more flexible for family members.Seniors’ general well-being could be improved by using these robots to assist them with daily duties and to keep them independent and in their homes for as long as feasible.Even infrared camera-based systems that can identify when an elderly person falls have been tested by medical and AI researchers.Researchers and medical professionals can also keep an eye on things like eating and drinking habits, fevers, restlessness, frequency of urination, comfort in chairs and beds, fluid intake, diminishing mobility, and more.Python is superior to R for writing quality codes »​8. Enhanced Health CareIt’s excellent news that hospitals may soon entrust an AI with your health. Hospital-related disorders like sepsis and accidents are less common in hospitals that use machine learning to help with patient care.The application of predictive models in AI is enabling researchers to better comprehend hereditary illnesses, which is one of medicine’s most difficult problems to solve.Before diagnosing or treating a patient, health practitioners used to have to manually go through reams of data.High-performance GPUs are becoming essential components of deep learning and AI platforms.When combined with the explosion in computing power, deep learning models quickly provide real-time insights that aid healthcare professionals in developing novel new drugs and treatments, reducing medical and diagnostic errors, anticipating adverse reactions, and lowering healthcare costs for both patients and providers.Descriptive Predictive and Prescriptive Analytics »​9. Innovations in BankingThink about the number of people who have bank accounts. Take into account the number of credit cards in use on top of that.How many man hours would it take for staff to sort through the daily thousands of transactions?Your bank account can be empty or your credit card might be charged to the maximum by the time they discovered an issue.AI can also assist banks and credit issuers in spotting fraudulent conduct as it occurs using location data and purchase trends.These models for anomaly detection are based on machine learning to watch over transaction requests. They can identify trends in your transactions and warn users of questionable behavior.Even before processing the payment, they can verify with you that the purchase was really yours.If it was just you dining out while on vacation, it could seem uncomfortable, but it might ultimately result in you saving thousands of dollars.What is the future of data analytics? »​10. Personalized Digital MediaThe entertainment sector has enormous promise for machine learning, and streaming services like Netflix, Amazon Prime, Spotify, and Google Play have already used the technology.To prevent buffering and poor playback, some algorithms are already in operation, ensuring that you receive the highest quality from your internet service provider.In order to improve the recommendations made by streaming services, machine learning (ML) algorithms are also using the seemingly unending flood of data regarding user viewing patterns.They will contribute more and more to media production as well. Natural language processing (NLP) algorithms aid in the creation of trending news articles to cut down on production time, while Shelley, an AI created by MIT, assisted people in creating horror stories using deep learning algorithms and a database of user-generated content. At this pace, the next big names in content creation could not even be people. Artificial Intelligence and Data Science »​11. Home Security and Smart HomesMany homes seek AI-integrated cameras and alarm systems for the greatest security technology.These cutting-edge systems create a database of your home’s frequent visitors using facial recognition software and machine learning, enabling these systems to quickly identify unauthorized intruders.Other helpful functions offered by AI-powered smart homes include tracking when you last walked the dog and alerting you when your children get home from school.The most recent systems may even autonomously summon emergency services, making it an appealing substitute for subscription-based programs that offer comparable advantages.Consumer AI will make it possible for numerous practical home automation. AI has the potential to streamline chores and household management when paired with appliances.The pantry robot and refrigerator might connect with one another using AI-powered apps, making the oven behave like a home cook. Never again running out of food or supplies would be possible with immediate resupply. Robotic cleaners would operate almost entirely independently of humans once the cleaning was scheduled via sensor-to-appliance linkages.Reducing domestic trash and automating recycling would be another benefit of smart houses, putting the household in a better equilibrium with the planet.Removing humans from housework could have a significant positive impact on sustainability, time efficiency, and stress reduction.Machine Learning Impact on your day-to-day life!Learn statistics for Data Science » Play Quizzes »​12. Streamlined Logistics and Distribution Imagine receiving an item in a short period of time for a very inexpensive shipping fee.With its promise to control the enormous volumes of data and decisions in the trillion-dollar shipping and logistics industry, AI in logistics and distribution has this promise.Amazon has already begun testing autonomous drones, which will significantly outperform their already impressive two-day shipping.Shipping expenses are still rather high at the moment. Automation and AI-based efficiency improvements will result in significant drops in shipping costs and faster delivery times.Additionally, supply chain management, vehicle upkeep, and inventory optimization opportunities will make delivery quicker, simpler, and more ecologically friendly. Basic statistics concepts »​13. Digital Personal AssistantsImagine not having to worry about what to make for dinner since your personal assistant is aware of your preferences, what you keep in your pantry, and the days of the week you enjoy cooking at home.Imagine if all of your groceries are waiting at your door when you come home from work so you can make the delectable supper you’ve been yearning for.Even better, you’ve got a bonus recipe for a brand-new sweet treat you’ve been meaning to try. Every year, digital assistants become more intelligent.Companies like Amazon and Google are investing billions of dollars to improve the speech recognition capabilities of digital assistants and teach them about our daily habits, paving the way for ever-more complex jobs.​14. Brick and Mortar and AIOrange Silicon Valley CEO Georges Nahon predicts a day when standing in line at a store would be a thing of the past.He comments on how tech and retail are combining, such as Amazon and Whole Foods, and claims that “the face will be the new credit card, the new driver’s license, and the new barcode” because of artificial intelligence.The adoption of biometric capabilities has already fundamentally transformed security through facial recognition.Some predict that the old retail industry will completely disappear as a result of e-commerce and the Internet, but it is more likely that they will reach some type of equilibrium.In order to obtain a competitive advantage, it is indisputable that even the largest traditional retail giants are beginning to use AI-powered solutions. Assess Performance of the Classification Model »​15. Customized News and Market ReportsAccording to Reg Chua, COO of Reuters News, newsrooms are beginning to embrace the potential of tailored news and market reports thanks to technology.Can you picture receiving market reports that were prepared specifically for you on demand rather than just at market close?Your personalized report contrasts how your portfolio performed vs the larger market, noting important reasons why, as opposed to providing a general rehash of market performance.As in: “It’s 3:14 p.m. Your portfolio is currently down 3% while the market is up 2%. This is partly attributable to last week’s acquisition of XYZ stock, which has since dropped significantly.There are many more industries that may use this technology, such as ad tech, agriculture, sports, and more, in addition to the obvious ones like finance and investing.Machine Learning Impact on your day-to-day life!       SummaryThe idea of artificial intelligence has been around for a while, as many knowledgeable people have noted. It has existed ever since the very beginning of computing.Pioneers have long thought of methods to create machines that are intelligent and can learn. Applying machine learning to AI is currently the most promising method.We want to make it possible for machines to learn and then learn how to learn, rather than trying to encode them with all the knowledge they will ever need upfront (which is impossible).The age of machine learning has arrived, and it is currently transforming every aspect of our life.If you are interested to learn more about data science, you can find more articles here finnstats.The post Machine Learning Impact on your day-to-day life! appeared first on finnstats.​  "
Machine Learning - The Future ,https://blog.naver.com/literaturedtj/223051591227,20230330,"Machine Learning - The FutureMachine Learning, commonly known as ML, is currently one of the hottest and most sought-after areas of computing.ML enables machines to make data-driven decisions and predictions in ways that were previously impossible.This technology has far-reaching implications for the future of automation and the automation of many tasks and activities that are currently done manually. It is a rapid cycling of breakthroughs and progress that have enabled powerful new potential to solve current and future problems.ML has already made major advancements in areas such as computer vision, object recognition, natural language processing, and speech recognition.ML has been used to create autonomous cars, robotic systems, and even artificial intelligence. With the capabilities of ML becoming more sophisticated and the cost of computing becoming cheaper, the future of automation and customization of services is closer than ever before.ML also offers great promise for efficiency gains and improved customer service in many industries.As ML continues to advance, we are seeing an increasing amount of automation being applied to industries such as health care, finance, transportation, and media and entertainment. AI-backed solutions will allow businesses to collect more data and real-time insights, enabling them to make more informed decisions in a shorter period of time.From customer service calls to customer support centers, we are already beginning to see the effects of ML – in helping to reduce costs, increase customer satisfaction, and make people’s lives easier.With the advancements of ML, it is creating a wave of disruption that is drastically changing the way businesses operate and interact with customers. As organizations continue to experiment with and invest in this technology, AI and automation will begin to further shape the world and how humans interact with it.As we move forward, the possibilities for ML are limitless and it will be up to us to see where this technology will lead us in the future.  "
[Bain & Co.] Generative AI and Retail: Beyond the Skateboarding Panda ,https://blog.naver.com/weapon_mugi/223109170356,20230523,"요약 SummaryFrom personalized marketing to enhanced customer engagement, use cases are proliferating.본문In retail, many executive teams have by now had a chance to try generative artificial intelligence tools. That experimentation has sometimes taken on a surreal quality—like the picture above, created by asking OpenAI’s DALL-E tool for a photo of a panda bear on a skateboard wearing sneakers in Times Square. However, the emerging uses for generative AI in retail are concrete indeed and will have far-reaching consequences.​First, a reminder of where we were on AI and what’s changed. Traditional AI solves specific problems or makes specific predictions; it relies on algorithms to learn patterns and structures in data and can apply that learning to new data. Generative AI also analyzes patterns in vast data sets, but the big difference is that it uses that analysis to generate original, new to the world content, be that text, images, or music (video shouldn’t be too far off).​Agile companies across industries are starting to innovate with this new technology. For instance, Coca-Cola quickly launched an AI platform that allows digital creatives to generate original art using iconic Coke brand assets such as its distinctive contoured bottles. The platform, built in collaboration with OpenAI and Bain & Company, draws on DALL-E and its text-input sister, GPT-4. For Coca-Cola, it’s just the beginning of a broader generative AI push.As things stand, generative AI’s emerging use cases in retail can be loosely grouped into four categories: personalized marketing, customer engagement and service transformation, operations and productivity, and customer and industry insights. ​Personalized marketingGenerative AI enables personalized marketing at a scale that wasn’t previously practical. Take an email campaign targeting many thousands of consumers: The technology can adapt the core message to each recipient, using language and incentives that should resonate based on their past preferences. For a grocery shopper who cares most about deals, it could draft an email emphasizing value and include a coupon; for someone who sees themselves as a gourmet, it could extol the origin of the food and include recipes.​Marketing professionals can use generative AI tools to boost their productivity—for instance, by using them to generate a first draft of a blog post or creative imagery (in any style—photorealistic, cartoon, abstract, etc.) that they can then review and refine. Once the content is finalized, generative AI can also create derivative assets (such as resized pictures for social media) much faster, freeing up time for higher-order tasks.​Generative AI can be woven into the fabric of a retailer’s website or app through individually tailored landing pages, product descriptions, and illustrations. Apparel retailers have an opportunity to help customers better visualize how an item of clothing would look on them, using generative AI to alter their own photos.​There are also disruptive possibilities that don’t fit easily into marketing categories right now, such as a recommended recipe list based on a photo of what’s in a customer’s refrigerator, or a “closet concierge” that recommends outfits from photos of their existing wardrobe and the nature of the occasion.​Customer engagement and service transformation​Compared with more basic chatbots that use templated responses, virtual assistants powered by generative AI offer a much better experience for shoppers when they need help or inspiration, not least because the latest technology is better able to contextualize interactions with information it has gathered previously.​Consider a customer looking for new running shoes as she dashes between meetings. Talking naturally to her device rather than typing, she asks the retailer’s AI assistant which pairs are best reviewed for a runner who clocks up to 15 kilometers a week. The assistant highlights options, putting at the top of the list a shoe that the customer once bought in a previous iteration. After getting accurate answers to her spoken follow-up questions about whether her size is in stock and how long delivery will take, she buys the familiar pair.​Tools like this will offer an attractive fix for the poor search experience on some retail websites, given that plugging in generative AI expertise via an API will be easier than upgrading in-house search infrastructure. AI will also enable multimodal search, in which shoppers will no longer be limited to searching with text and keywords, but will have other possible starting points, such as photos, voice, and video.​For customer service agents, generative AI can recommend a script to follow in a call, suggest targeted offers that might tempt the customer, and produce summaries of each conversation. Generative AI is already turbocharging social media interaction by suggesting a range of possible replies to customer queries and comments. These prompts can help social media specialists engage with more people. However, customers will catch on to companies that appear to engage personally but don’t follow through on the points they raise. ​Operations and productivity Retail executive teams might already be tapping the potential of generative AI in a personal capacity—by using OpenAI’s speech recognition system, Whisper, to automatically take meeting notes, for example. This is just the beginning of the operational gains across the organization, as generative AI provides staff with tools that can help them speed through commoditized tasks.​Take frontline knowledge management. Retail associates are inundated with training documentation upon joining. It’s hard to retain and use that information when needed—for instance, when a grocery employee is posted to the meat counter and must observe strict hygiene rules—and there are only so many times a new hire can bother managers for clarification. Generative AI chatbots can give prompts and instructions just when associates need them, in a conversational tone that increases confidence as well as productivity. The technology can also codify best practice from unstructured data obtained from top-performing stores and integrate that information into the chatbot. ​Vendor management is another operational area that’s ripe for AI assistance. Today, many retailers handle thousands of vendor relationships through large specialist teams. Generative AI can ease that load. It can automate—or semiautomate—some vendor interactions by performing tasks such as crafting request for proposals (RFPs), summarizing meetings, and drafting follow-up emails.​Other back-office uses include generating text for job descriptions. The technology will also unearth enhancements retailers don’t even know about yet—for instance, by identifying scalable best practice from masses of internally generated information that hasn’t yet been fully analyzed or collected. This “capture and codify” playbook should serve retailers well. And once they capture the best practices, they can use generative AI to automatically create or augment materials for training and onboarding.​Customer and industry insights New ways of analyzing customer sentiment and loyalty are emerging. Generative AI can monitor the content and tone of customer interactions—a phone call, an online chat—and then assess in real time whether that shopper is likely to be happy or frustrated with the information they received and the way in which it was delivered. Measuring customer advocacy through a metric such as Net Promoter Score℠ then becomes a predictive rather than reactive process.​Generative AI can make more sense of structured feedback such as surveys, generating sharper insights at the level of individual stores as well as offering a clearer view of the big picture. Its ability to process overwhelming quantities of unstructured feedback, such as freeform commentary on social media, will be transformational. Its capacity to synthesize this feedback with key industry-, market- and category-level data will enable real-time adjustments in products and assortment to match changes in sentiment, while also generating ideas for new products and services.​Online marketplace operators can also turn to generative AI to standardize the way third-party sellers list their items online, thanks to uniform keywords and product descriptions generated automatically from product photos. ​AI’s present is catching up with our imagined future As they analyze the potential uses of generative AI, executive teams face an acute strategic planning challenge. In many cases, the seemingly futuristic applications that emerge from their brainstorming sessions won’t be that futuristic at all: They will be possible within months rather than a decade or two. The recent strides taken by the technology really are that big.​That immediate applicability of these AI advances—so different from the slower burn associated with other futuristic technologies, such as blockchain and augmented reality—means that a wait-and-see approach is particularly high risk for retailers. There’s a strong case for bold application, even if the path ahead is uncertain. A test-and-learn approach today can develop a repeatable process that can be deployed widely tomorrow. AI will be core to the retail industry, and those that merely pause to take stock might never unwind the head start granted to their rivals. 본문 (일본어)小売業では、今までに多くの経営陣が、生成的な人工知能ツールを試す機会を得ています。 この実験は、上の写真のように、OpenAIのDALL-Eツールに、Times Squareでスニーカーを履いてスケートボードに乗っているパンダの写真を依頼することによって、時には超現実的な品質を帯びることもあった。 しかし、小売業におけるジェネレーティブAIの新たな用途は、実際には具体的であり、広範囲な結果をもたらすだろう。​まず、私たちがAIのどこにいたのか、そして何が変わったのかを思い出してください。 従来のAIは、特定の問題を解決したり、特定の予測をしたりする。アルゴリズムに依存して、データのパターンや構造を学習し、その学習を新しいデータに適用することができる。 生成AIも膨大なデータセットのパターンを分析するが、大きな違いは、テキスト、イメージ、または音楽（ビデオはそれほど離れてはいけない）という点で、世界のコンテンツに新しいオリジナルの生成にその分析を使うということだ。​この新しいテクノロジーにより、業界全体のアジャイル企業が革新を始めています。 例えば、コカ·コーラはデジタルクリエイティブが独特な輪郭のボトルなど象徴的なコカコーラブランド資産を活用してオリジナルアートを生成できるAIプラットフォームを素早く発売した。 OpenAIとBain & Companyと共同で構築されたこのプラットフォームは、DALL-Eとテキスト入力の姉妹であるGPT-4を利用している。コカコーラにとっては、より広範な生成的AIプッシュの始まりにすぎない。​現状では、小売業におけるジェネレーティブAIの新たなユースケースは、パーソナライズされたマーケティング、顧客エンゲージメントとサービスの変革、運用と生産性、顧客と業界の洞察の4つに大まかに分類できます。 ​パーソナライズされたマーケティングジェネレーティブAIは、これまで実用的ではなかった規模のパーソナライズされたマーケティングを可能にします。 何千人もの消費者を対象としたEメールキャンペーンを実施してください: この技術は、過去の好みに基づいて共鳴すべき言語とインセンティブを使用して、コアメッセージを各受信者に適応させることができます。 取引に最も関心のある買い物客にとっては、価値を強調した電子メールの草案を作成し、クーポンを含めることができます。また、自分自身をグルメだと思っている人にとっては、料理の起源やレシピを称賛することができます。​マーケティングのプロフェッショナルは、生成的なAIツールを使用して、生産性を向上させることができます。たとえば、ブログ投稿の最初のドラフトやクリエイティブなイメージ（写真写実的、漫画、抽象的など）を作成し、レビューして絞り込むことができます。 コンテンツが確定すれば、生成AIも派生資産(ソーシャルメディア用のサイズ変更写真など)をはるかに早く生成することができ、高次作業に時間を割くことができる。​個別にオーダーメイドしたランディングページ、商品説明、イラストなどを通じて、小売業者のウェブサイトやアプリの生地に生成AIを織り込むことができる。 アパレル小売業者は、顧客が自分の写真を変更するために生成的なAIを使用して、服のアイテムがどのように見えるかをよりよく視覚化するのを助ける機会があります。​サービス提携Better Together: OpenAIとBainが提携を結ぶ生成AIの力を活用してビジネスを変革​顧客の冷蔵庫に入っている写真を基にした推薦レシピリスト、既存の衣装写真と行事の性格を推薦する「クローゼットコンシェルジュ」など、今すぐマーケティングカテゴリーに簡単に合わない破壊的な可能性もある。​お客様との関わりとサービスの変革テンプレート化された応答を使用するより基本的なチャットボットと比較して、生成AIによって駆動される仮想アシスタントは、買い物客が助けやインスピレーションを必要とするときにはるかに良い経験を提供する。特に最新の技術は、以前収集した情報との相互作用をよりうまくコンテキスト化できるためだ。​会議の合間に新しいランニングシューズを探しているお客様を考えてみてください。 タイピングよりも自然に自分のデバイスに話しかけて、彼女は小売業者のAIアシスタントに、週に15キロまで時計を合わせるランナーのためにどのペアが最も良いか尋ねる。 アシスタントはオプションを強調表示し、リストの一番上にお客様が以前に購入した靴を表示します。 サイズの在庫があるかどうか、配送にどれくらい時間がかかるかについてのフォローアップの質問に正確な回答を得た後、彼女はおなじみのペアを購入します。​このようなツールは、APIを介して生成的なAIの専門知識を接続することが社内検索インフラをアップグレードするよりも簡単であることを考慮すると、一部の小売ウェブサイトの検索経験の不足に対する魅力的な修正を提供するだろう。 AIはまた、買い物客がこれ以上テキストやキーワードで検索するだけでなく、写真、音声、動画など他の可能な出発点を持つマルチモーダル検索も可能にする。​カスタマー サービス エージェントの場合、生成 AI は、コールで従うスクリプトを推奨し、お客様を誘惑する可能性のあるターゲットのオファーを提案し、各会話の要約を作成できます。 ジェネレーティブAIはすでに、顧客の質問やコメントに対して可能な範囲の回答を提案することで、ソーシャルメディアの相互作用を加速させている。   Learn moreGenerative AI Opportunities and Perspectives in RetailBain partners discuss how retailers can use OpenAI to anticipate disruption, find innovative solutions, and identify trends.  ​ "
1. 인공지능 소개 2. 인공지능 역사  ,https://blog.naver.com/iocean74/222629672828,20220124,"*http://www.kmooc.kr/courses/course-v1:SNUk+SNU048_011k+2021_T2/courseware/40974ac73d4d42faa9eb8f0335a12565/b3344bb87d164ff38c3961f0d393b9da/ 로그인 | K-MOOC로그인 메일 주소와 비밀번호를 입력하거나, 아래에 있는 방법으로 로그인하세요. 계정이 아직 없다면, 아래의 버튼을 눌러 가입하세요. 이메일 ERROR: K-MOOC 회원 가입 시 입력한 이메일 주소 이메일 찾기 비밀번호 비밀번호 찾기 로그인 상태 유지 ERROR: 로그인 로그인할 수 없는 경우 FAQ 를 참조하시기 바랍니다. 구글, 페이스북 간편로그인은 연동 오류로 현재 서비스 이용이 어려우며, 정상화 조치 중에 있습니다. 대체 로그인 및 서비스 이용을 위해 공지사항 '구글, 페이스북 간편로그인 오류...www.kmooc.kr 인공지능의 정의 컴퓨터가 생각한다, 마음을 가진 기계 컴퓨터에서 구현이 가능한 정신 기제(머리가 하는 일 : 기억 등)현재로서는 사람이 잘 하는 일을 컴퓨터가 하게 만드는 것 컴퓨터 공학의 한 분야​*지능은 뭔가?  -전체를 아우르는 정의가 가능한가? Is there a “holistic” definition for intelligence?지능에 대한 정의는 논쟁적-지능을 갖추기 위해서는 어떤 능력이 있어야 하는가? (요소) 이해, 추론, 문제해결, 상식, 일반화 -인간만이 아닌 다른 존재도 지능 있음. (예)개미 '군집 지능)​*어떻게 인공지능을 달성할 수 있는가? Thinking humanly / Thinking rationallyActing humanly     / Acting rationally​humanly 사람처럼 : 비이성적인 면까지 포괄 상식, 사회적 행동, 전문가 지식, 문제 해결 능력  rationally 이성적으로 ​ Thinking : 사람이 어떻게 지능적인 행동을 하는지?Acting     : 사람의 지능적인 행동만 모사 지능적인 행동 예) 기계 번역기 : 영어를 모름. 그러나 데이터를 모아 상관관계. 통계적으로. 예제로 학습​튜링 테스트 (1950)튜링은 인공지능에 대해 정의하려는 노력. 지능'적인' 행동 조사관. 방에 컴퓨터가 있나, 인간이 있나 모름. 질문을 통해 얻은 답으로 인간인지 컴퓨터인지 구별할 수 없다면 '인간'  Can machines think?Q: Please write me a sonnet on the subject of the Forth Bridge.A: Count me out on this one. I never could write poetry.Q: Add 34957 to 70764.A: (Pause about 30 seconds and then give as answer) 105621.​튜링 테스트 통과하려면 -지식 갖추기 -추론 통해 대답 만들기 -학습 통해 새로운 지식 만들기 -조서관의 질문을 이해하고 적절한 대답 -시각적인 이해 능력. ​*인공지능의 요소  감각 사고 행동 Translation ofsensory inputs(percepts) into aconceptualrepresentation• Computer vision• Speechrecognition• LanguageunderstandingManipulation of theconceptualrepresentation• KnowledgeRepresentation• Problem Solving/Planning• Learning (makingimprovements basedon the results of pastactions)Translation ofintent into(physical) actions(reflexive ordeliberative)• Robotics• Speech andLanguageSynthesis환경을 향해 표현, 로보틱스, 음성발화  *탐색 (게임 포함) 경우의 수를 탐색한 뒤, 결정 *지식 표현 *추론 *계획 *불확실성 처리 *학습 *다중​@강 인공지능 vs 약 인공지능 weak AI : 사람이 미리 프로그램하고 주어진 상황에서 판단, 행동 예)알파고 : 갑자기 장기를 두진 않는다. 공장 조립 로봇  strong : 사람의 지능에 도달하거나 뛰어넘는. 자유의지와 자각. '나는 누구인가' 꼭 구현될 필요가 있는지예)공각 기동대.  ​@특이점 singularity인공지능의 진화는 매우 빠름. 사람의 지능을 뛰어넘는 지점. 지능이 낮은 존재는 지능이 높은 존재의 행동을 이해하지 못한다. 특이점이 넘어가면, 인간은 인공지능의 행동을 이해하지 못한다. (알파고 사태. 사람이 생각하기에 이상한 수...그러나 알고 보면 좋은 수)​어원: 물리학에서 '블랙홀'에서 온 용어이벤트 호라이즌 안에 있으면 빛조차 빨려 들어감. 밖에 있으면 탈출 가능. 이벤트 호라이즌 안 '미지의 영역' ​<2>인공지능의 역사 *mechanical turk : 체스 두는 인형(사실은 안에 사람이 숨어 있음)*튜링 테스트 (1950) 튜링은 튜링 테스트를 '이미테이션 게임'이라 불렀음. *음성 인식  ' 오드리' 미국 전화국에서 발명. 사람들이 전화 번호를 말할 때 자동 인식하게.  '하피' *다트머스 회의(1956)   인공지능이 처음 시작된 워크샵 *체스 컴퓨터 (1959) 아서 사뮤엘.  1997년 딥 블루 *shakey셔키 (1966~1972)  최초의 이동 로봇 *딥 블루(1997) cmu에서 학생들이 개발. ibm에 입사하여 개발. *혼다의 '아시모' (2204)초등학교 어린이 크기의 휴머노이드. *다르파 urban chaiienge (2007)  자율주행차. 사막이 아니라 '도시'에서 경진*왓슨. (2011)제퍼리 퀴즈쇼 참가. 승리. 구글 검색이 가능한 컴퓨터..단답식 문제에서 유리 *알파고. (2016) 구글 딥마인드에서 개발 인간의 기보를 참고하지 않은 알파고제로  그 뒤 은퇴 ​@책 <<인공지능>> 스투어트 러셀, 피터 노빙 stuart russel  "
Let's cheer along for Artificial Intelligence! ,https://blog.naver.com/jeannie090802/223093728986,20230504,"​   Picture this: you're sitting at your desk, pondering a question that's been on your mind for days. You turn to your computer and start typing, but instead of searching Google, you're having a conversation with an AI chatbot. This may seem like a futuristic scenario, but it's already happening today with chatbots like ChatGPT. Those who invariably doubt the potential of AI technology are considered outliers; and here is an opportune evidence for it: this wondrous opening was generated by ChatGPT in less than five seconds. AI chatbots are subject to never-ending controversies, yet their potential outweighs the currently existing debates about its limits. Technology will constantly advance; and so will the accuracy and usefulness of AI chatbots. ​          The unique characteristics of AI chatbots, such as their efficacious method of responding to requests will allow people to apply them to real-life situations. Currently, people are in desperate need of efficient programs that can be their assistants, especially if those programs have an endless database, impeccable logic, and efficient problem-solving process. AI chatbots might be the closest that humans can get to formulating such an assistant that can support us behind the scenes. AI chatbots work effectively because they can gather relevant information from their databases in a blink of an eye. When an individual suggests a topic, AI chatbots immediately dive into their knowledge banks to find research that can benefit the individuals and release them. People can apply this into their lives by utilizing AI chatbots for tasks that do not require personal opinions but are rather of a lower level. ​Moreover, unlike humans, AI chatbots do not have any time limits – they do not get emotionally stressed, overworked, or unavailable due to health conditions. Clients can use them anywhere and at any given time because AI usage is not hindered by anything except an accessibility to wifi. In addition, unlike human assistants, AIs can handle diverse issues at the same time because they are several branches of a trunk that contains the database. A child in the United States can access the chatbot while a grown-up in Nigeria is using it as well! ​         Another characteristic of AI chatbots is that they learn from their users. Chatbots obtain information such as how humans naturally use language, updated information about global issues, and how to deal with biased individuals based on past experiences. Self-learning is a stepping stone for an advanced Artificial General Intelliegence system that does not require Through these experiences, AI chatbots have room to prosper limitlessly and prove their potential through implications in real life situations. ​          The field that will have its capabilities increased to the greatest degree by AI chatbots is arguably education. Educational experts habitually compare the impact of AI chatbots in education to the impacts of calculators. Calculators aid students in basic and repetitive processes that are not necessary for advancing one’s academic abilities; instead, they are utilized for convenience and free up time for studying more complex fields and solving complicated problems that involve multiple layers of thought processes. AI chatbots can assist students just like calculators - students would not be entirely dependent on the chatbots for their assignments, but using them will either reduce the amount of time that is wasted on mundane tasks, or will act as a stepping stone for building up comfort with idiosyncratic ideas. ​In particular, students often utilize ChatGPT for researching, a process which often takes up a considerable amount of time when writing persuasive or descriptive essays. Through the usage of ChatGPT, students can easily access studies or texts written by professionals. Such texts often involve advanced vocabularies and professional background knowledge, which can be difficult for students. AI chatbots can not only summarize such articles, but can also transform the alien language into a more easily accessible style, based on a student’s skills. AI chatbots can also list some credible research regarding certain topics for students to independently review and analyze. The tedious research processes without any assistance have led to students getting overworked due to the time and effort that is required to constantly repeat the same process; some even refuse to complete any assignments that involve such dedication. AI chatbots can act as an opportune solution for these issues because they free up more time for students to complete parts of projects that are less stressful and are rather engaging, such as writing a creative essay or drawing pictures based on the solid research carried out by the chatbot. ​Furthermore, AI chatbots can act as teaching assistants within the classroom. These days, teachers juggle lots of work, including scoring students’ assignments, providing individual feedback for students, and teaching countless classes. AI chatbots cannot enable teachers to be completely free, due to AI chatbots’ limits that they can only exist online, have a lack of emotional maturity, and cannot quickly adapt to emergency situations. However, AI chatbots can generate scores and feedback for students’ work through receiving training from teachers on the standards of grading and the criteria expected for high-level projects. When AI chatbots generate scores in a matter of seconds, teachers will have more spare time to focus on planning lessons and engaging activities, as well as supporting students individually. AI-based scoring is also beneficial for students because they can receive objective grades that are not affected by the personal biases that some students suspect their teachers to have. These positive implications of AI chatbots in education can open up numerous new opportunities for enhancing the quality of learning, taking quality education to another level and making it accessible to more and more students in the near future. ​          Another field that would be ideal for AI chatbots is supporting people with disabilities. Disabilities often hinder people’s ability to function within a society, simply because today’s society has insufficient programs to aid those in need. The existing products, such as magnifying glasses for people with low vision, or  braille for blind individuals, do benefit them in their lives but still cannot allow all disabled people to fully function in a society. AI chatbots possess a massive potential to aid disabled people to live in a similar way to those who do not have disabilities. AI chatbots can enable blind individuals to “read the world” by verbally informing them about their current location based on what is seen through a smartphone camera. AI chatbots are excellent at identifying and describing things, so if one could be connected to a smartphone camera, it could elucidate the view to the individual in real-time by describing the weather, shops, or the people who are around and what they are doing. ​The distinction between a smartphone app that provides verbal description and an online chatbot lies in chatbots’ capability to provide vivid descriptions of the environment that a blind person is standing in, instead of simply informing them about the names of the nearby stores. The limitation of diverse applications that are available in the market is that they might guide blind individuals to certain locations, but the systems do not consider other factors such as friends of the blind individual passing by, or a particular store or bakery that the individual loves to visit. However, AI chatbots can utilize their facial recognition features to inform the disabled individuals about the identities of passers by, which might include friends, family, or teachers. Also, with the database that the AI possesses, it can act as the individual’s ‘eyes’ to recommend different restaurants and bakeries nearby based on the individual’s tastes. This can potentially provide enhanced mobility for blind individuals to aid them in freely navigating and communicating with the world around them without the assistance of service dogs or family members. ​AI chatbots can additionally assist deaf people through their speech recognition abilities. Currently, sign language is the most common tool for communication among deaf people, because of the myriad limitations of previous speech recognition programs. Speech recognition programs that are utilized by people without disabilities do not require 100% accurate recognition skills because people can manually correct the errors in the recognition. On the other hand, quick recognition and high degrees of accuracy are essential for deaf people because they must be able to obtain real-time subtitles while people are speaking. AI chatbots can be an apt solution for this issue because they are capable of real-time comprehension and correction of people’s statements, and can formulate full sentences based on their interpretations. These full sentences could then be transcribed and displayed as subtitles on a smartphone screen for the deaf person to read. ​“Artificial intelligence is not just a technology, it is a mindset shift. Its potential lies not in replacing human intelligence, but in enhancing it to solve problems that were once thought impossible,” claims ChatGPT, when asked about itself. ChatGPT’s assessment of itself seems fair; it does possess endless potential as an inventive technology that can resolve global issues in the most efficacious manner possible. It has potential in a  wide range of areas, mainly regarding its capabilities of contributing to the educational field and the support that it can provide to disabled individuals. Experts concede that there are limits and potential harms, yet there were also limitations and risks with our most innovative technologies, such as smartphones and the internet. It is our responsibility to utilize AI chatbots in a sustainable and safe way to positively benefit our community.  "
포낙 로저 솔루션과 다른 업체 보청기 착용 전후 비교해봐요! ,https://blog.naver.com/phonakkr/222608524634,20211231,"​​안녕하세요. 포낙입니다.  먼 거리 및 소음 상황에서포낙의 로저 솔루션이 어음명료도를 향상시킨다는 것은이미 잘 알려져 있는 사실이죠.  하지만, 포낙보청기가 아닌다른 제조사의 보청기와 로저 솔루션을 함께 사용했을 때의 효과를 알아보는 연구는 많이 알려져 있지 않은데요.  오늘은 먼 거리 및 소음 상황에서다른 보청기 제조사와 로저 넥루프를 함께 사용했을 때,보청기만 사용했을 때와 비교하여주관적, 객관적으로 어떤 효과가 있는지 살펴본 연구를함께 알아보도록 하겠습니다. 😊      보청기만으로 모든 상황에서최상의 청취가 가능할까요?  지난 십년간 보청기 기술은 놀라운 발전을 거듭해왔습니다.방향성마이크 시스템과 소음제거 알고리즘은착용자의 청취 경험을 최적화하기 위해끊임없이 청취 환경에 대한 정보를 수집하고여러가지 변수들을 자동으로 조절해줍니다.    ​ 또한, 스마트폰, 태블릿, 컴퓨터 등 많은 전자기기와 보청기를다이렉트 연결하여 여러가지 앱을 사용할 수 있게 되었고,이러한 기술의 발전에 따라 보청기 사용자들의 만족도는 지난 몇 년간 지속적으로 높아졌습니다.  하지만, 가장 최신 기술을 보유한 보청기도많은 사람들과 함께 대화하거나 소음 속에서 대화하는 데에는 어려움이 있습니다.   ​ 보청기 착용자의 31% 가량은 청력상태에 맞게 완벽하게 피팅한 보청기를 사용함에도 불구하고 소음 속에서 말소리를 듣는 것에 어려움을 느낀다는 연구결과가 이를 뒷받침해줍니다.       포낙 로저 솔루션이 무엇인가요?  이렇게 소음 및 먼 거리 상황에서 청취에 어려움을 겪는 착용자들을 위해서는말소리를 보청기에 직접적으로 전달하는 방식이 필요한데요.  그 중 가장 흔히 사용되는 것은 ‘리모트마이크 시스템’으로 현재 다양한 보청기 제조사에서 개발되어 널리 사용되고 있습니다.  하지만 개발된 대부분의 시스템은 일정 수준 이하의 소음 상황에서만 적용이 가능하거나자사 제품과만 호환이 가능한 경우가 많습니다. ​  ​ 청각솔루션 글로벌리딩 기업 포낙의 로저 솔루션은 이러한 문제점들을 해결하고자 개발되었습니다.  로저 솔루션은 로저 다이렉트를 지원하는 보청기라면,수신기 없이 로저 송신기에서 보내는 소리를보청기로 다이렉트 청취할 수 있게 해줍니다.  로저 다이렉트를 지원하지 않는 보청기라면,별도의 수신기를 통해 소리를 보청기나 인공와우에 전달해주는데요.이 때 사용되는 수신기 중 한 가지가 바로 ‘로저 넥루프’입니다.   포낙 로저 넥루프  로저솔루션을 사용하면 화자가 멀리 떨어져 있는 상황, 또는 화자가 여러 명인 회의, 학교, 친구들 모임 등 다양한 청취 상황에서 난청인의 어음명료도를 향상​시켜 줍니다.    ​    로저 넥루프를 다른 회사 보청기와 함께 사용해도 어음명료도가 향상되나요?​ 스위스에 위치한 포낙 본사에서 연구진들은로저 넥루프와 다른 보청기를 함께 사용했을 때어음명료도 향상 효과를 확인해보았습니다.  감각신경성 난청을 가진 14명을 대상으로 연구가 진행되었는데요.  먼저, 로저 넥루프를 사용하지 않고두 종류의 다른 제조사 보청기와 포낙보청기를 착용한 뒤2m와 4m 떨어진 곳에서 들려준 문장을 따라 말하도록 했습니다.이 때, 실험실 안은 실제 회의실 같이 주변 소음도 함께 제공하였습니다.  그 후 로저 넥루프를 착용하고, 테이블 위에는 로저 테이블마이크II를 올려둔 후 같은 방식으로 실험을 한 번 더 진행했습니다.   실제 회의실처럼 세팅된 실험 환경  그 결과, 로저를 사용하기 전과 후의 어음 명료도는 3개의 제조사 보청기에서 모두 27%에서 81%로 대폭 상승하는 결과를 보였습니다.하지만 제조사에 따른 어음명료도 상승 차이는 크게 나타나지 않았습니다.   보청기만 사용했을 때와 로저솔루션을 함께 사용하였을 때의 어음명료도​ 즉, 소음 속 어음 명료도는 로저 솔루션의 사용으로 크게 상승하지만 보청기 업체에 따라 큰 차이를 보이지 않는다는 것인데요.  포낙 이외의 다른 제조사 보청기를 사용하고 있어도로저 솔루션과 함께 사용하시면더욱 명료하게 말소리를 들을 수 있게 됩니다. ​ 실험에 참여한 대상자 14명 모두""로저 솔루션을 사용하였을 때, 보청기만 사용했을 때보다어음명료도가 향상되어 만족스러웠다""고 하니객관적, 주관적으로 모두 긍정적인 효과가 있다고 할 수 있겠죠?​​       지금까지 포낙보청기, 다른 제조사의 보청기와 함께 로저 넥루프를 사용할 경우 소음 환경과 같이 청취가 어려운 상황에서 얼마만큼 도움을 주는지 살펴보았는데요.  로저 솔루션은 청취가 어려운 상황에서어음명료도를 확실하게 향상시켜주며,다양한 로저 수신기와 송신기를 사용하면현재 난청인이 착용하고 있는 보청기의 제조사와 상관없이그 효과를 누릴 수 있다는 점을 청각전문가 여러분들이 잘 활용하셔서난청인의 명료한 청취에 도움이 되기를 바랍니다. 😊  듣는 즐거움이 가득한 일상, 포낙이 함께 합니다!​     References 1. Abrams, H.B. & Kihm, J. (2015). An introduction to MarkeTrak IX: A new baseline for the hearing aid market. Hearing Review, 22(6), 12-20.2. Hällgren, M., Larsby, B. & Arlinger, S. (2006). A Swedish version of the Hearing in Noise Test (HINT) for measurement of speech recognition. International Journal of Audiology, 45(4), 227-237.3. Powers, T.A. & Rogin, C.M. (2019). MarkeTrak 10: Hearing aids in an era of disruption and DTC/OTC devices. Hearing Review, 26(8), 12-20.4. Thibodeau L. (2014) Comparison of speech recognition with adaptive digital and FM remote microphone hearing assistance technology by listeners who use hearing aids. Am J Audiol. 23(2):201-10.5. Thibodeau, L. (2020). Benefits in speech recognition in noise with remote wireless microphones in group settings. J Am Acad Audiol. 31(6), 404-411. "
언어재활사/언어치료사를 위한 '청각장애(1)' ,https://blog.naver.com/thinker_note/223054734425,20230324,"조음음운장애 과목 시험 범위에는청각장애로 인한 조음음운장애가 포함되어 있습니다.​언어재활사라면 청각장애에 대한 지식은 필수입니다. 출처: 한국보건의료인국가시험원   주관적 청력평가: 피검자의 능동적 참여 필요 O​1) #순음청력검사 (PureTone Audiometry, PTA) 출처: https://health.uct.ac.za/entdev​청력 상태의 종합적인 평가 및 진단, 보청기 및 인공와우 착용 전후 역치 비교청력이 좋은 쪽 귀부터 실시, 청력 차이가 없으면 오른쪽 귀부터 실시​1-1) #기도청력검사 (air conduction audiometry)1, 2, 4, 8, 0.5, 0.25kHz 주파수에서 검사   (인접한 옥타브 사이에서 20dB 이상 차이 → 반옥타브에서도 검사)0.5, 1, 2kHz에서의 역치 평균으로 순음역치평균(PureTone threshold average, PTA) 산출    * 세 주파수는 어음주파수로 고려됨각 주파수마다 제시할 수 있는 최대 출력 범위 다름전체 경로 중 어느 한 곳에 이상이 있어도 비정상 청력으로 나타남    (∵ 외이, 중이, 내이, 중추청신경의 청각능력을 총괄적으로 검사하여 역치 결정)​1-2) #골도청력검사 (bone conduction audiometry)0.25~4kHz 주파수 범위에서 검사mastoid(유양돌기)에 bone vibrator 밀착 출처: https://health.uct.ac.za/entdev​기도전도에 비해 상대적으로 최대 출력음 낮음외이나 중이가 비정상이더라도 내이에 이상이 없으면 정상 청력으로 나타남    (∵ 외이나 중이를 거치지 않고 내이를 직접 자극하여 역치 결정)ABG(Air-Bone Gap)가 10dB 초과하면 외이 또는 중이 비정상 의심  2) #어음청력검사 (speech audiometry, SA) 출처: http://www.tespia.kr/tp_mall/prd_detail.asp?num=41​2-1) #어음인지역치(Speech Recognition Threshold, SRT) 검사검사 전 피검자가 검사에 사용할 목표 단어를 모두 아는지 친숙화 과정 거침검사자 지시문: ""방금 들으신 단어들이 아주 작은 소리부터 큰 소리까지 다양한 소리 크기에서 제시될 것이니 들은 단어를 따라 말하세요​. 들은 단어가 무엇인지 확실하지 않을 때는 추측해서 대답해도 좋습니다.""검사자가 제시하는 spondee word의 50%를 따라하는 소리의 강도를 역치로 정함(50% 인지할 수 있는 최소 강도, 즉 민감성 측정)    * spondee word: 각 음절의 강도가 동일한 이음절 단어한국표준 이음절어표(KS-BWL)에 제시된 spondee word 사용    * KS-BWL: 학령전기용 이음절어표(KS-BWL-P), 학령기용 이음절어표(KS-BWL-S), 일반용 이음절어표(KS-BWL-A) 출처: 진소영 & 이정학,  ⌜한국표준 일반용 이음절어표를 사용한 어음인지역치의 검사-재검사 신뢰도⌟, Audiology, 2015, 158쪽​PTA(PureTone threshold average)와 SRT 차이가 10dB 이내일 경우 청력검사의 신뢰도가 좋은 것으로 판단. 이상의 차이를 보일 경우, PTA(PureTone Audiometry) 재측정어음을 듣고 따라 말할 수 없는 경우, SRT대신 어음탐지역치(Speech Detection Threshold, SDT) 검사 시행​2-2) #단어인지도(Word Recognition Score, WRS) 검사쾌적 레벨(Most Comfortable Loudness level, MCL)에서 검사검사자 지시문: ""지금 제 목소리가 편안하게 들리시죠? 이 소리에서 단어를 제시할 것이니 들은 단어를 따라 말하세요. 들은 단어가 무엇인지 확실하지 않을 때는 추측해서 대답해도 좋습니다.""단음절어를 얼마나 정확히 인지하는지 백분율로 표시(얼마나 잘 인지하는지 정확도 평가)한국표준 단음절어표(KS-MWL)    * KS-MWL: 학령전기용 단음절어표(KS-MWL-P), 학령기용 단음절어표(KS-MWL-S), 일반용 단음절어표(KS-MWL-A)​2-3) #문장인지도(Sentence Recognition Score, SRS)검사쾌적 레벨(Most Comfortable Loudness level, MCL)에서 검사검사자 지시문: ""지금 제 목소리가 편안하게 들리시죠? 이 소리에서 문장을 제시할 것이니 문장을 끝까지 잘 듣고 따라 말하세요. 전체 문장을 다 못 들었더라도 들은 단어가 있다면 모두 말해주시기 바랍니다.""문장 내 단어를 얼마나 정확히 인지하는지 백분율로 표시(얼마나 잘 인지하는지 정확도 평가)WRS보다 피검자의 일상생활 속 의사소통 능력을 더 반영한국표준 문장표(KS-SL)    * KS-SL: 학령전기용 문장표(KS-SL-P), 학령기용 문장표(KS-SL-S), 일반용 문장표(KS-SL-A)   PTA, SA 모두 masking(차폐) 실시    * masking: 검사 시, 청력이 나쁜 귀(test ear, TE)에 제시한 소리가 좋은 쪽 귀(non-test ear, NTE)로 전달되는 cross hearing 가능성으로 인해 시행  참고문헌심현섭 외, ⌜#의사소통장애의이해 (3판)⌟, 학지사, 2017한국청각학교수협의회, ⌜#청각학개론 (2판)⌟, 학지사, 2018첨부 사진의 경우, 아래에 출처 기입하였음  팅커와 함께 공부해요:) "
[머신러닝/이론] 머신러닝 기초 - 머신러닝 시스템 종류 ,https://blog.naver.com/justarose/222862615117,20220831,"머신러닝이란?머신러닝(Machine learning)이란 기계가 데이터로부터 패턴을 학습하는 것을 의미한다.  기존의 전통적인 프로그램 방식의 경우 사람이 직접 모든 로직과 룰을 정해주고, 컴퓨터는 데이터를 인풋으로 받아 정해준 룰에대해 수행후 아웃풋을 냈다면, 머신러닝은 사람이 일일이 로직을 다 정해주지 않고 컴퓨터가 직접 데이터를 학습해서 데이터 안의 패턴을 찾아 예측하는 모델을 만드는 것이다.  어떤 경우 머신러닝을 사용하면 좋을까?[머신러닝 방식이 더 효율적일 때]만약 스팸메일 분류 프로그램을 만든다고 가정해보면, 전통적인 프로그래밍 방식을 사용한다면, 스팸메일 데이터들을 살펴보고 공통적으로 많이 발견되는 단어와 문구들을 뽑아낸 다음, 어떤 경우에 스팸메일로 분류할지 하나하나 로직을 정해서 프로그램을 짜야할 것이다. ​스팸메일에 들어가는 단어나 문구의 종류도 엄청 많고, 시간이 지남에 따라 변화하기 때문에 프로그래밍 로직은 계속해서 길어지고 복잡해질 것이다. ​만약 머신러닝 기술을 적용한다면, 머신러닝 프로그램이 방대한 스팸메일 데이터를 학습하고, 그 안에서 공통 패턴을 찾아내서 스팸메일여부를 분류할 수 있다. (스팸이 아닌 메일에선 많이 등장하지 않는데, 스팸메일에서만 자주 등장하는 단어나 문구 패턴을 찾아내는 일) 스팸 발신자들이 점점 더 교묘해져서, 스팸 필터에 걸리지 않기 위해 계속 새로운 단어를 사용한다 하더라도, 머신러닝을 사용하면 기계가 재학습을 해서 새롭게 자주 등장하는 문구들도 예측할 수 있게 된다. ​[전통적인 프로그래밍으로는 해결하기 너무 복잡한 문제일 때]​예를들어, Speech recognition(스피치 인식) 프로그램을 만든다고하면, 모든 단어들이 어떤 소리를 내는지를 다 로직화 한다는 것은 거의 불가능에 가깝다. 모든 사람들이 다른 톤, 발음을 가지고 있고, 언어나 단어의 종류도 무수히 많기 때문이다. ​이 경우, 머신러닝을 활용하면 기계가 직접 수많은 스피치 데이터를 학습하여 인식할 수 있도록 할 수 있다. ​ 머신러닝 실제 응용 사례뇌 종양 검출 시스템(수많은 환자들의 뇌 MRI 사진으로 학습하여, 새로운 환자들의 MRI 사진에서 종양존재여부를 검출하는 시스템)부동산 가격 예측(Linear Regression)신용카드 부정 사용 예측 (Detecting credit card fraud - anomaly detection)추천 시스템 (ex.쇼핑앱, OTT앱에서 당신이 좋아할 수 있는 아이템, 프로그램 추천 - Neural network) 머신러닝 시스템의 종류 (Types of Machine Learning Systems)​Supervised Learning - training set(학습 데이터)이 솔루션에 해당하는 label(라벨)을 가진 경우ex) 스팸메일 분류 프로그램이라 할 때, 학습하는 데이터가 이미 스팸 해당/비해당으로 분류가 되어있는 경우(여기서 스팸여부를 라벨이라고 표현함)​Unsupervised Learning - training set이 라벨을 가지지 않은 경우ex) 특정 커뮤니티 회원 정보를 통해 회원들을 비슷한 성격끼리 묶어 그루핑 해보고 싶을 때(이렇게 그루핑하는 것을 Clustering이라고 표현함. 학습데이터에는 솔루션에 해당하는 그룹정보가 없는 상태이기 때문에 라벨이 없는 것이고, 기계학습을 통해 비슷한 성격끼리 묶어서 그루핑함 ) ​Semisupervised Learning - 데이터를 라벨링하는 작업은 사람이 직접 해줘야하기 때문에 비용이 많이 듬. 그래서 활용하는게 Semi-supervised learning이다. 이미 라벨링 된 데이터를 가지고 라벨링되지 않은 데이터를 예측하고, 예측되어 나온 데이터를 다시 학습데이터에 포함하여 기계를 train 시키는 경우. ​Reinforcement Learning - agent(학습 시스템)가 올바른 작업을 했을 때 reward를 받고 틀린 작업을 했을 때는 punishment를 받는다. 이런식으로 계속 학습을 하면서 best stratege를 찾아간다. ​ex) 예를들어 뉴스 앱에서 구독자별로 뉴스추천기능을 이 reinforcement learning을 활용할 수 있다. 뉴스를 추천했을 때 사용자의 반응을 통해 올바른 작업이었는지 틀린 작업이었는지를 알 수 있다. 사용자가 오래 머물고 뉴스를 공유하거나 좋아요를 누르거나 댓글을 단다면 사용자가 관심있는 뉴스를 추천한 경우이기 때문에 reward를 받고, 사용자가 클릭하지 않거나, 클릭했을 때 바로 뒤로가기를 누른 경우는 관심없는 경우에 해당하기 때문에 punishment를 받는다. 과정을 반복하면서 어떤 뉴스를 추천하는게 좋은지 best stratege를 찾는다.  ​​​​#공부로그 #study #머신러닝 #machinelearning #ML​​ "
[인공지능산업융합사업단] 2020년 AI (시)제품 제작 지원사업 공고 및 제작을 위한 3D프린팅 솔루션 소개 ,https://blog.naver.com/prototech/222113579447,20201012,"​안녕하세요.3D프린팅 토탈 솔루션 전문기업 (주)프로토텍입니다.​오늘은 AI (시)제품을 제작하기 위한 정부지원사업과 AI(시)제품을 제작할 수 있는 3D프린팅 솔루션​을 소개하고자 합니다. ​AI 산업은 미래에 등장하게 될 신사업 중 가장 많이 업급되고 있는 사업이라고 합니다.​그만큼 많은 관심을 받으며 발전되고 있는 분야입니다. ​  ​하단에 소개드리고자 하는 내용은인공지능산업융합사업단에서 추진했던 AI 지원 사업입니다. ​AI지원사업에 선정되신 분이라면 AI 외관 제작에 대해 고민하고 계실텐데요? ​지원사업 공고 내용 하단에 맞춤형 제품 제작이 가능한3D프린팅 및 시제품 제작 솔루션을 살펴보세요. ​​ 2020년 AI (시)제품 제작 지원사업광주정보문화산업진흥원●  사업 목적​- AI (시)제품 제작과 비즈니스 모델 발굴을 통한 AI 관련 비즈니스 활성화 및 AI 스타트업 생태계 조성을 위한 과제비 지원​​● 지원조건 및 내용​-   공고 기간 : ’20. 7. 8. ~ 8. 7. (접수 7. 20. ~ 8. 7. 17:00)-   사업 규모 :  총  4,125백만원 -   선정 규모 : 55개사(팀) 내외※ 접수결과 및 선정평가 등에 따라 선정기업(팀)의 수는 변동 가능​-   지원 내용 : 과제당 최소 50,000천 원 ~ 최대 150,000천 원 제작 지원금※ 기업별 지원금액은 과제신청자의 신청예산 및 평가 결과에 따라 차등 지원​-   지원 기간 : 협약일 ~ 2020.12.31※ 지원기간은 세부 일정 등 사업추진상황에 따라 변동될 수 있음​​● 지원대상 구분내용비고AI 관련 예비창업자• AI 관련 신규창업을 희망하는 예비창업팀(3인 이상)과 기존 기업의 사내벤처 및 분사        등 AI 관련 광주 지역 내 창업을 추진하고 있는 팀 등​AI 스타트업• 창업 7년 미만의 AI 관련 스타트업 및 기존 사업에 AI 도입을 통한 사업화를 추진하고자      하는 국내 중소벤처 기업※ 공고일 기준     창업 7년 미만 ​ㅇ 지원제외 대상① 정부 또는 지자체, 기관 등으로부터 보조사업 참여 제한조치를 받은 기업 또는 대표자 및 참여인력이 포함된 경우② 동일 또는 유사 과제로 정부자금을 지원받은 경우③ 현재 정부자금을 지원받아 수행중인 과제를 중복 신청한 경우④ 국세 또는 지방세를 체납중인 기업, 지급(이행)보증보험 발급이 불가한 기업 또는 대표자⑤ 정부 관련기관 수행사업의 정산환수금, 성공환원금(기술료) 미납 기업 또는 예비창업팀 대표자※ 신청기업과 참여인력은 1개 과제만 신청과 참여가능(중복신청‧불가)※ 지원제외 대상임에도 불구하고 선정 또는 지원받은 경우는 선정 및 지원을 취소하고 지원금 전액을 환수하며, 제재부가금, 지원사업 참여제한 등       제재조치를 받을 수 있음.​​● 공모 과제 구분내용비고신청과제• 공고일 기준 신규 제작 및 사업화를 위한 AI(시)제품‧서비스* 등* 자연어 처리(Natural Language Processing), 음성인식(Speech Recognition), 이미지 인식(Image Recognition), 기계 학습(Machine Learning), 딥러닝(Deep Learning), 전문가 시스템(Expert System) 등의 AI 기반 기술을 활용한 응용 솔루션, 서비스, AI융합 제품 등​ ※ 단순 기록 및 DB구축, 자료수집, 순수 학술 연구 등 사업화를 목적으로 하지 않은 과제는 신청 불가​● 추가 지원 프로그램 지원항목내용비고제품 고도화 지원• 차년도 글로벌 제품 고도화 비용지원(과제당 최소 30,000천 원 ~ 100,000천 원)※ 최종평가에서 우수     과제로 선정된 경우기업 입주지원•입주 및 사무공간 지원(1~3년 무상 지원)※ 공간구축 일정에     따라 변동 가능AI 펀드 지원•IR 참가를 통한 AI 펀드 투자 지원​마케팅 지원•AI 컨퍼런스 및 국내외 전시회 등 참가 지원•AI 제품 품질 테스트, 컨설팅, 인증 등 지원​교육‧컨설팅 지원•AI 기술 및 창업 교육 프로그램 지원•AI 관련규제 해소 컨설팅 지원​실증지원•AI 실증 장비(에너지‧자동차‧헬스케어 등) 이용 서비스 지원​ ※ AI (시)제품 제작 지원사업에서 선정된 예비창업자와 기업은 추가 지원 프로그램의 대상자 선정 시에 우대하며, 추가 지원 항목과 내용은 사업 추      진상황에 따라 변동될 수 있음​  AI (시)제품을 제작을 위한 3D프린팅 솔루션​​1. 제품제작 솔루션 - 3D프린터 ​3D프린팅은 원하는 제품을 사내에서 제작 가능하게 해줍니다. 모든 형상을 제작 할 수 있기 때문에 자유로운 디자인이 가능하여 AI의 외관  및 부품을 제작하는데 활용 될 수 있습니다.  ​보급형 3D프린터 부터 억대의 산업용 3D프린터까지다양한 솔루션을 살펴 보세요.^^ ​3D프린팅 솔루션 자세히 보기 >>​ 3D프린팅 솔루션​​​​2. 시제품 제작 서비스​AI를 위한 외관 및 부품의 시제품 제작을 위한솔루션을 살펴보세요. ​시제품 제작 뿐만 아니라 디자인 부터 양산품 제작 까지제품 개발 단계의 모든 과정을 상담 받으실 수 있습니다. ​​ ​디자인 및 제작서비스 자세히 보기  >>​​​ ​​홈페이지 ▶▶ http://www.prototech.co.kr/유튜브 ▶▶https://www.youtube.com/c/프로토텍3d "
[Deep Learning] DLHLP #1 Course overview ,https://blog.naver.com/after-glow-/222997641829,20230128,"우선 이 course는 2020년도 국립타이완 대학교 Hung-Yi Lee (李宏毅) 교수님의 강의다.졸업논문 주제를 정하다 보니 언어 처리에 대한 지식이 부족해서 찾게 된 수업이다.​아무래도 강의 전체가 중국어로 진행되기 때문에 중국에서 대학을 다니는 학생들에게 추천하고 싶은 수업이다!​강의의 전체적인 흐름이 매끄럽고 이해하기 쉽게 설명해 주셔서 너무 좋은 것 같다 ๑•‿•๑​강의 링크: Hung-yi Lee週次 日期 備註 自我學習 作業 作業繳交 課程主題 課程錄影 第一週 2/19 延後開學 第二週 2/26 延後開學 第三週 3/04 HW1 Speech Recognition 公告 ( slide , video ) Introduction , Rule 1 , Rule 2 , 加簽 Rule 1 , Introduction 第四週 3/11 HMM 1 2 HW1 助教時間 Speech Recognition Speech Recognition 1 2 3 第五週 3/18 n-gram LM HW1 助教時間 Speech Recognition Speech Recog...speech.ee.ntu.edu.tw DLHLP란?- Deep Learning for Human Language Processing ​그렇다면 NLP (Natural Language Processing, 자연어 처리)와 HLP (Human Language Processing)의 차이는 뭘까? NLP vs HLPNLP (Natural Language Proccesing):​• Natural Language can be Speech or Text• As contrasted with an artificial language  (e.g. JAVA, Python) • A language that has developed naturally in use  (e.g. Chinese, English) • Most NLP textbook and course mainly focus on text  (Text v.s. Speech = 9 : 1)​하지만 이 강의에서 HLP를 강조한 이유는?​• In this course, Text v.s. Speech = 5 : 5• Speech processing is NOT only speech recognition. (언어 처리는 단지 언어 식별뿐만이 아니다.)• Only 56% languages have written form (Ethnologue, 21st edition) .• We don't always know if the existing writing systems are widely used.  Human Language는 매우 복잡하다.1 second has 16K sample points Each point has 256 possible values. ​같은 구절을 똑같이 두 번 따라 할 수 있는 사람은 아무도 없다. 한 사람이 '안녕하세요'를 4번 반복한 결과: 모두 다름그래서 Human Language processing이 복잡하고 처리하기 힘든 이유다.  > Hung-Yi Lee 교수님의 강의를 요약하자면 아래의 그림과 같이 표시할 수 있을 것 같다: One slide for this course - 이 중에서 Model은 Deep Network로 생각하면 된다.​1. Automatic Speech Recognition (ASR) 자동 음성 인식 Traditional Speech Recognition의 모델은 여러 개의 모듈로 형성이 돼있어서 복잡하다. Traditional Speech Recognition하지만 핸드폰에서 구현한 End-to-end 형식의 음성인식은 80MB만 차지하게 된다,게다가 기존 Seq2seq(Sequence-to-sequence)와는 다르다는 점.​2. TTS 음성 합성 ​3.  Speech Seperation / Voice Conversion 이러한 방법은 주로 칵테일파티처럼 여러 사람의 목소리와 잡음이 많은 상황에서도 본인이 흥미를 갖는 이야기는 선택적으로 들을 수 있게 하는 것이다.칵테일파티 효과라고도 한다 |(•◡•)|또한 음성변조에서도 사용할 수 있는 방법이다.​4. Text-to-Text 이 파트에서도 응용할 수 있는 부분이 많은데 번역 또는 summary 요약과 chatbot 등에서 많이 사용된다.​5. Input Audio, Output Class 이 파트에서의 응용으로는 Speaker Recognition 즉 소리를 낸 주인을 찾을 수 있는 방법이다.또한 Keyword Spotting에서도 활용이 가능하다고 한다,즉 Siri야, 혹은 Ok 구글과 같이 음성명령에서도 활용된다.​6. Input Text, Output Class 이러한 모델은 Question Answering에서 많이 활용된다.  Hung-Yi Lee 교수님의 DLHLP course에서는 Meta learning,  Knowledge Graph,Adversarial Attack(적대적 공격), Explainable AI 등 내용들도 다뤘다.​DLHLP course에 대한 간략한 설명은 끝! 다음에는 Speech Rcognition에 대해서 다룰 예정이다|•̅ᴥ•̅) و "
이 기술만 있으면 영어 공부 안 해도 된다고?! :: 음성인식 기계 번역 ,https://blog.naver.com/alcherads/222833852060,20220729,"안녕하세요! 에디터 도리 입니다! ​  ​ 요즘 인기가 뜨거운 영화죠! <헤어질 결심>에서는 주인공인 서래와 해준이 ‘음성인식’ 번역 기술을 통해 각자의 언어로 소통하는 장면이 나옵니다. 서래가 중국어로 말하면, AI 인공지능이 한국어로 번역해 해준에게 서래의 진심을 전하죠.    인공지능 음성인식 서비스이처럼 음성 인식 기술은 우리 생활 곳곳에 스며있는데요. 그래서 오늘은  AI 히어로즈의 기술력 중 하나인, ‘음성인식’을 활용한 다양한 서비스에 대해 소개해드립니다! ​​     1. ‘인공지능 음성인식 솔루션’(AI-based Automatic Speech Recognition Solution)이 뭔가요? 딥러닝을 통해 사람이 발화한 음성을 텍스트로 변환시키는 겁니다. 얼마나 정확하게 인식하고 기록하는지, 그 속도는 어떠한 지가 관건이죠.  음성모델 훈련기 (Acoustic Model Trainer), 전사 도구(Transcription Tool), 언어모델 도구(Language Model Tool) 등 자동화 도구를 주로 이용합니다.   2. ‘기계번역’에 이 솔루션이 쓰인다고요? 기계번역은 컴퓨터가 언어를 이해하고 학습해서 스스로 문법 교정, 문장 요약, 객체 인식, 구문 분석 등의 자연어를 처리하는 기술입니다.  앞서 말씀드렸던 ‘인공지능 음성인식 솔루션’으로 기계 번역이 이루어지죠. 이 과정에서 STT와 TTS 기술이 들어갑니다.   3. STT 기술이 뭔가요? 먼저 STT(Speech-to-Text)은 사람이 말하는 음성 언어를 컴퓨터가 해석해 그 내용을 문자 데이터로 전환하는 기술이죠. Voice Recognition 또는 인공 청각이라고도 부릅니다.   4. 그럼 TTS는 어떤 기술인가요? ​반대로 TTS(Text-to-Speech)는 문자를 다시 사람 음성으로 변환하는 기술을 말합니다. ​미리 사람 목소리를 녹음해 일정한 음성 단위로 쪼개 모아 두죠. 텍스트가 입력되면 데이터베이스에서 문장에 걸맞은 목소리 조각을 찾아 조합해 내놓는 겁니다.​ 최근에는 TTS에 빅데이터와 AI 기술이 결합하면서 더욱 자연스러운 음성을 생성하는 기술이 날로 발전하고 있죠.  STT 기술을 통해 사람의 말을 텍스트로 변환시킨 다음, 인공지능이 이를 번역하여 다시 TTS 기술로 해당 언어를 음성으로 변환해 들려주는 것입니다.   5. 어떻게 활용되나요? 동시 번역 및 통역기에 들어갑니다. 영화 속 서래가 해준에게 휴대폰 번역기를 통해 이야기한 것과 비슷해요. 스마트폰으로 모국어로 작성된 텍스트를 다국어로 번역하거나 음성으로 읽어주고, 동시통역을 통해 원활한 해외 활동 또는 외국인과의 자연스러운 대화를 지원해 주는데 쓰입니다.  쉽고 빠르게 여러 언어에 대해 이해하고 생각을 전달할 수 있는 기회를 사용자들에게 제공해 소통을 돕고 글로벌 기업에게는 업무 효율을 배로 올려주죠.   6. 앞으로가 더 기대되는 음성인식 기술! 인공지능 전문가들은 음성 인식 기술을 이용해 콜센터에 STT 녹취 시스템을 적용시켜 실시간으로 고객과의 상담 내용을 데이터 디지털화시키는 서비스를 제공하고 있습니다.  또한 택배 기사에게 음성인식 가상 비서 시스템을 제공해 배송시간 절약시키고 고객 응대를 가능하게 해 줬죠.  이처럼 인공지능을 통한 음성인식 기술은 기업에는 글로벌 능력을 높여줄 뿐만 아니라, 노동자의 서비스 환경을 개선하는 효과까지 있습니다. 우리의 삶에 단비가 되어주는 음성인식 기술의 앞으로가 더 기대되네요!​    오늘은 음성인식 기술에 대해 알아봤는데요!앞으로도 유익하고 재미있는 기술 이야기 많이 전해드리겠습니다.​그럼, 다음에 뵈어요~!!​​​ 알체라 DS, Make your AI Dreams a Reality데이터바우처 지원 사업 지원금 7000만원 혜택, 데이터바우처 지원 사업 전담팀 구성. 프로젝트 100% 성공률. 데이터 구축 전문가 전담 배정. AI제품 서비스개발 및 영상, 음성, 텍스트 가공. 전문성과 효율성, 편의성을 제공받을 수 있습니다.data.alcherainc.com ​ "
[TIL] PORORO: Platform Of neuRal mOdels for natuRal language prOcessing ( KaKao Brain ) ,https://blog.naver.com/charzim0611/222956246586,20221215,"Github  GitHub - kakaobrain/pororo: PORORO: Platform Of neuRal mOdels for natuRal language prOcessingPORORO: Platform Of neuRal mOdels for natuRal language prOcessing - GitHub - kakaobrain/pororo: PORORO: Platform Of neuRal mOdels for natuRal language prOcessinggithub.com pororo docs  Welcome to PORORO’s documentation! — PORORO: Platform Of neuRal mOdels for natuRal language prOcessing 0.3.1 documentationPORORO: Platform Of neuRal mOdels for natuRal language prOcessing Notes Installation and Usage Configuration Text Classification Automated Essay Scoring Age Suitability Prediction Natural Language Inference Paraphrase Identification Review Scoring Semantic Textual Similarity Sentence Embedding Senti...kakaobrain.github.io ✅ kakobrain에서 발표한 pororo  카카오 브레인이 개발한 자연어 처리와 스피치 관련 여러 라이브러리 종합 선물 세트 , 일명 ""뽀로로"" 이다. 다양한 자연어 태스크에 대응 가능한 통합된 형태의 자연어 프레임워크이다. HuggingFace와 유사한 목적을 가지며 Pororo는 한국어 태스크들에 대해 좀 더 최적화되어있다.  음성 인식 등 오디오 처리도 함께 지원한다는 장점이 있다.   pororo performs Natural Language Processing and Speech-related tasks.It is easy to solve various subtasks in the natural language and speech processingfield by simply passing the task name. 📝 Pororo의 (https://kakaobrain.github.io/pororo/) 주요 태스크 ​Text ClassificationAutomated Essay ScoringAge Suitability PredictionNatural Language InferenceParaphrase IdentificationReview ScoringSemantic Textual SimilaritySentence EmbeddingSentiment AnalysisZero-shot Topic Classification​Sequence TaggingContextualized EmbeddingDependency ParsingFill-in-the-blankMachine Reading ComprehensionNamed Entity RecognitionPart-of-Speech TaggingSemantic Role Labeling​Seq2SeqConstituency ParsingGrammatical Error CorrectionGrapheme-to-PhonemePhoneme-to-GraphemeMachine TranslationParaphrase GenerationQuestion GenerationText SummarizationWord Sense Disambiguation​MiscAutomatic Speech RecognitionImage CaptioningCollocationLemmatizationMorphological InflectionOptical Character RecognitionSpeech SynthesisTokenizationWord TranslationWord Embedding​설치 pororo is based on torch=1.6(cuda 10.1) and python>=3.6You can install a package through the command below: torch 1.6 /   python 3.6 이후 버전  pip install pororo ​✅ 사용 방법 📝 공식 튜토리얼 예제 (Colab)  05-자연어처리(한글)-level1Colaboratory notebookcolab.research.google.com from pororo import Pororo from pororo import PororoPororo.available_tasks()""Available tasks are ['mrc', 'rc', 'qa', 'question_answering', 'machine_reading_comprehension', 'reading_comprehension', 'sentiment', 'sentiment_analysis', 'nli', 'natural_language_inference', 'inference', 'fill', 'fill_in_blank', 'fib', 'para', 'pi', 'cse', 'contextual_subword_embedding', 'similarity', 'sts', 'semantic_textual_similarity', 'sentence_similarity', 'sentvec', 'sentence_embedding', 'sentence_vector', 'se', 'inflection', 'morphological_inflection', 'g2p', 'grapheme_to_phoneme', 'grapheme_to_phoneme_conversion', 'w2v', 'wordvec', 'word2vec', 'word_vector', 'word_embedding', 'tokenize', 'tokenise', 'tokenization', 'tokenisation', 'tok', 'segmentation', 'seg', 'mt', 'machine_translation', 'translation', 'pos', 'tag', 'pos_tagging', 'tagging', 'const', 'constituency', 'constituency_parsing', 'cp', 'pg', 'collocation', 'collocate', 'col', 'word_translation', 'wt', 'summarization', 'summarisation', 'text_summarization', 'text_summarisation', 'summary', 'gec', 'review', 'review_scoring', 'lemmatization', 'lemmatisation', 'lemma', 'ner', 'named_entity_recognition', 'entity_recognition', 'zero-topic', 'dp', 'dep_parse', 'caption', 'captioning', 'asr', 'speech_recognition', 'st', 'speech_translation', 'ocr', 'srl', 'semantic_role_labeling', 'p2g', 'aes', 'essay', 'qg', 'question_generation', 'age_suitability']"" 특정 태스크를 수행하려는 경우 태스크 인수에 태스크 이름을, lang 인수에 언어 이름을 입력  >>> from pororo import Pororo>>> ner = Pororo(task=""ner"", lang=""en"") ner(""Michael Jeffrey Jordan (born February 17, 1963) is an American businessman and former professional basketball player."")[('Michael Jeffrey Jordan', 'PERSON'), ('(', 'O'), ('born', 'O'), ('February 17, 1963)', 'DATE'), ('is', 'O'), ('an', 'O'), ('American', 'NORP'), ('businessman', 'O'), ('and', 'O'), ('former', 'O'), ('professional', 'O'), ('basketball', 'O'), ('player', 'O'), ('.', 'O')] ​참고 자료 KakaoBrain의 Pororo - 통합 자연어 프레임워크 | Smilegate.AI카카오브레인에서 다양한 자연어 태스크에 대응 가능한 통합된 형태의 자연어 프레임워크인 Pororo를 오픈소스로 공개했습니다. Pororo는 Platform Of neuRal mOdels for natuRal language prOcessing의 약자이며 HuggingFace와 유사한smilegate.ai 개발자 커뮤니티 SQLER.com - kakobrain에서 발표한 pororo 리뷰잠시 kakobrain에서 발표한 pororo 리뷰 이름보니 주 개발자분의 아이가 한참 어린이집 다닐 정도 되시는 분일듯. Github repository 링크: kakaobrain/pororo: PORORO: Platform Of neuRal mOdels for natuRal langu...www.sqler.com ​ "
6 Minute English 해석/독해 - Can AI have a mind of its own? [BBC Learning English] ,https://blog.naver.com/softca/222998889013,20230130,"본 해설 자료는 첨부파일(PDF)로 올려져 있습니다. 다운받으셔서 활용하십시오​이 자료는 BBC Learning English 코너의 6 Minute English가 매주 재공하는 스크립트를 직역한 것입니다. 이번 주 스크립트의 주제는 'Can AI have a mind of its own?' 입니다. 해석은 철저하게 직역하였습니다. 해석된 문장은 한국어의 어순이 아니라 가급적 영어 원문 스크립트의 아순에 따라 정리했습니다. 이 글의 목적은 번역을 공부하는 게 아니라 원문을 영어적으로 이해하는 것이기 때문입니다.(함께 제공되는 PDF 파일에는 한글 어순으로 [직역]한 해석본도 함께 제공합니다.)​지금부터 독해를 시작합니다  [6 Min] [01] Hello.[초역] 안녕(하세요)​[6 Min] [02] This is 6 Minute English from BBC Learning English.[초역] 여기는 / 비비씨 / 학습 영어의 6분 영어입니다​[6 Min] [03] I'm Sam.[초역] 나는 샘입니다​[6 Min] [04] And I'm Neil.[초역] 그리고 / 나는 닐입니다​[6 Min] [05] In the autumn of 2021, something strange happened at the Google headquarters in California's Silicon Valley.[초역] (그) 가을에 / 2021년(의) / 무언가가 / 이상한 / 발생하였습니다 / (그) 구글 본사에서 / 캘리포니아 실리콘 밸리에 있는​[6 Min] [06] A software engineer called, Blake Lemoine, was working on the artificial intelligence project, 'Language Models for Dialogue Applications', or LaMDA for short.[초역] (어떤) / 소프트웨어 / 엔지니어는 / 불리는 / 블레이크 르모인이라고 / 일하고 있는 중이었습니다 / (그) 인공 지능 프로젝트에서 / '대화 언어 모형 프로그램('Language Models for Dialogue Applications)' / 또는 / LaMDA 라고 하는 / 줄여서​[6 Min] [07] LaMDA is a chatbot - a computer programme designed to have conversations with humans over the internet.[초역] 람다는 / 챗봇입니다 / (즉,) (어떤) / 컴퓨터 프로그램 / 설계된 / 대화하기 위해 / 인간과 함께 / (그) 인터넷 상에서​[6 Min] [08] After months talking with LaMDA on topics ranging from movies to the meaning of life, Blake came to a surprising conclusion: the chatbot was an intelligent person with wishes and rights that should be respected.[초역] 여러 달 후에 / 대화를 한 / 람다와 함꼐 / 주제에 관하여 / 포함하는 / 영화에서부터 / (그) 의미까지 / 삶의 / Blake는 / 이르게 되었습니다 / (어떤) 놀라운 / 결론에 / 즉, (그) / 챗봇은 / (어떤) / 지능을 가진 사람이었습니다 / 소망과 권리를 가진 / 존중되어야만 하는​[6 Min] [09] For Blake, LaMDA was a Google employee, not a machine.[초역] Blake에게 / 람다는 / 구글(의) 직원이었습니다 / 아니라 / (어떤) / 기계가​[6 Min] [10] He also called it his 'friend'.[초역] 그는 / 또한 / 불렀습니다 / 그것을 / 그의 / 친구라고​[6 Min] [11] Google quickly reassigned Blake from the project, announcing that his ideas were not supported by the evidence.[초역] 구글은 / 재빨리 / 전환 배치하였습니다 / Blake를 / (그) 프로젝트에서부터 / 발표하면서 / 그의 / 생각이 / 뒷받침된 것이 아니라고 / (그) 증거로​[6 Min] [12] But what exactly was going on?[초역] 그러나 / 정확하게 무슨 일이었습니까?​[6 Min] [13] In this programme, we'll be discussing whether artificial intelligence is capable of consciousness.[초역] 이 프로그램에서 / 우리는 논의를 하는 중일 것입니다 / / 인공 지능이 / 능력이 있는지 어떤 지를 / 의식의​[6 Min] [14] We'll hear from one expert who thinks AI is not as intelligent as we sometimes think, and as usual, we'll be learning some new vocabulary as well.We'll hear from one expert who thinks {that} AI is not as intelligent as we sometimes think, and as usual, we'll be learning some new vocabulary as well.[초역] 우리는 들을 것입니다 / 어느 한 전문가로부터 / 생각하는 / 인공 지능이 / 않다고 / 지능적이지 / 우리가 / 때때로 / 생각하는 것 만큼 / 그리고 / 평소처럼 / 우리는 몇몇 새로운 어휘를 배우고 있을 것입니다 / 마찬가지로​[6 Min] [15] But before that, I have a question for you, Neil.[초역] 그러나 / 그 전에 / 나는 당신에게 질문이 있습니다 / 닐​[6 Min] [16] What happened to Blake Lemoine is strangely similar to the 2013 Hollywood movie, Her, starring Joaquin Phoenix as a lonely writer who talks with his computer, voiced by Scarlett Johansson.[초역] 블레이크 르모인에게 일어난 일은 / 이상하게 / 비슷합니다 / (그) / 2013년의 / 할리우드 / 영화 / HER'와 / 주연을 맡았던 / Joaquin Phoenix이 / (어떤) 외로운 작가로 / 대화하는 / 그의 컴퓨터와 / 목소리를 냈던 / Scarlett Johansson에 의해​[6 Min] [17] But what happens at the end of the movie?[초역] 그런데, / 무엇이 / 발생합니까 / (그) 끝에 / (그) 영화의​[6 Min] [18] Is it:[초역] 이것입니다.​[6 Min] [19] a) the computer comes to life b) the computer dreams about the writer or, c) the writer falls in love with the computer?[초역] ⒜ 컴퓨터가 사람이 됩니다 ⒝ 컴퓨터가 작가의 꿈을 꿉니다 또는, ⒞ 작가가 컴퓨터와 사랑에 빠집니다.​[6 Min] [20] … c) the writer falls in love with the computer.[초역] … ⒞ 작가가 컴퓨터와 사랑에 빠집니다​[6 Min] [21] OK, Neil, I'll reveal the answer at the end of the programme.[초역] .오케이 / 닐 / 나는 이 프로그램의 말미에 답을 밝히겠습니다 /​[6 Min] [22] Although Hollywood is full of movies about robots coming to life, Emily Bender, professor of linguistics and computing at the University of Washington, thinks AI isn't that smart.Although Hollywood is full of movies about robots coming to life, Emily Bender, professor of linguistics and computing at the University of Washington, thinks {that} AI isn't that smart.[초역] 할리우드가 / 가득 차 있기는 하지만 / 영화로 / 로봇에 관한 / 삶을 얻게 되는 / Emily bender는 / 교수인 / 언어학과 컴퓨터 학과 / 워싱턴 대학교의 / 생각합니다 / 인공 지능이 / 영리하지 않다고 / 그 정도로​[6 Min] [23] She thinks the words we use to talk about technology, phrases like 'machine learning', give a false impression about what computers can and can't do.She thinks {that} the words {which/that} we use to talk about technology, phrases like 'machine learning', give a false impression about what computers can and can't do.[초역] 그녀는 / 생각합니다 / (그) / 단어 / 우리가 / 사용한 / 기술에 대해 이야기하기 위해 / 즉, 어구가 / 'machine learning'과 같은 / 준다고 / (어떤) 잘못된 / 인상을 / 것에 관한 / 컴퓨터가 할 수 있는 것과 할 수 없는​[6 Min] [24] Here is Professor Bender discussing another misleading phrase, 'speech recognition', with BBC World Service programme, The Inquiry:[초역] 여기에 / 있습니다 / bender 교수가 / 논의하는 / 또 다른 / 오도하는 / 어구인 / 'speech recognition'에 대해 / 비비씨-월드서비스프로그램 / 더인콰이어리와​[6 Min] [25] If you talk about 'automatic speech recognition', the term 'recognition' suggests that there's something cognitive going on, where I think a better term would be automatic transcription.If you talk about 'automatic speech recognition', the term 'recognition' suggests that there's something cognitive going on, where I think {that} a better term would be automatic transcription.[초역] 만약 / 당신이 / 이야기한다면 / 'automatic speech recognition(자동음성인식)'에 대해 / (그) 용어는 / 'recognition'이라는 / 암시합니다 / 있다는 것을 / 무언가가 / 인지하는 / 지속하고 있는 / 그러나 거기서 / 나는 / 생각합니다 / (어떤) 보다 더 좋은 / 용어는 / 자동화된 기록일 것이라고​[6 Min] [26] That just describes the input-output relation, and not any theory or wishful thinking about what the computer is doing to be able to achieve that.[초역] 그것은 / 단지 / 서술합니다 / (그) / 입출력 관계 만을 / 그리고 / (서술하지) 않습니다 / 어떤 이론이나 / (또는) / 희망 사항을 / (그) / 컴퓨터가 / 하고 있는 중인 것에 관한 / 성취할 수 있도록 / 그것을​[6 Min] [27] Using words like 'recognition' in relation to computers gives the idea that something cognitive is happening - something related to the mental processes of thinking, knowing, learning and understanding.[초역] 사용하는 것은 / 단어를 / 'recognition'과 같은 / 컴퓨터에 관하여 / 줍니다 / (그) / 견해를 / 무언가가 / 인지적인 / 벌어지는 중이라는 / (즉,) 무언가 (입니다) / (그) 정신적인 / 과정과 관련 있는 / 생각하는 것, 아는 것, 배우는 것 / 그리고 / 이해하는 것이라는​[6 Min] [28] But thinking and knowing are human, not machine, activities.[초역] 그러나 / 생각하는 것과 / (그리고) / 안다는 것은 / 인간의 (행동)입니다 / 아니라 / 기계의 행동이​[6 Min] [29] Professor Benders says that talking about them in connection with computers is wishful thinking - something which is unlikely to happen.[초역] 벤더스 교수는 / 말합니다 / 그 들에 대해 이야기하는 것은 / ] / 컴퓨터와 관련되어 / 희망 사항이라고 / (즉, 그것은) 무언가 (입니다) / 개연성이 낮은 / 벌어질​[6 Min] [30] The problem with using words in this way is that it reinforces what Professor Bender calls, technical bias - the assumption that the computer is always right.[초역] (그) / 문제는 / 사용하는 것이 갖는 / 단어를 / 이러한 방식으로 / 것입니다 / 그것이 / 강화한다는 / 교수가 / Bender / 부르는 것을 / 기술적인 / 편견이라고 / (즉,) / (그) / 추정 (입니다) / (그) / 컴퓨터가 / 항상 / 옳다는​[6 Min] [31] When we encounter language that sounds natural, but is coming from a computer, humans can't help but imagine a mind behind the language, even when there isn't one.[초역] 우리가 / 마주치면 / 언어를 / 자연스럽게 들리지만 / (그러나) / 나온 것인 / 컴퓨터에서 / 사람들은 / 상상하지 않을 수 없습니다 / (어떤) / 마음을 / (그) 언어 뒤에 있는 / 심지어 / 하나가 아닌 때에 조차도​[6 Min] [32] In other words, we anthropomorphise computers - we treat them as if they were human.[초역] 다시 말하면 / 우리는 / 의인화합니다 / 컴퓨터를 / (즉,) 우리는 / 대합니다 / 그들을 / 마치 / 그들이 / 사람인 것처럼​[6 Min] [33] Here's Professor Bender again, discussing this idea with Charmaine Cozier, presenter of BBC World Service's, the Inquiry.[초역] 여기에 / 있습니다 / Bender 교수가 / 다시 / 그리고 그는 / 논의합니다 / 이러한 / 생각을 / Charmaine Cozier와 함께 / 사회자인 비비씨-월드서비스(의) / 더인콰이어리의​[6 Min] [34] So 'ism' means system, 'anthro' or 'anthropo' means human, and 'morph' means shape… And so this is a system that puts the shape of a human on something, and in this case the something is a computer.[초역] 그래서 / 'ism'은 / 시스템을 뜻하고 / 'anthro' / 또는 / 'anthropo'는 / 인간을 의미합니다 / 그리고 / 'morph'는 형상을 뜻합니다. 그리고 그래서 / 이것은 / (어떤) / 시스템입니다 / 나타내는 / (그) / (어떤) 사람의 모양으로 / 무언가를 / 그리고 / 이번의 경우에 / (그) / 무언가는 / (어떤) / 컴퓨터입니다.​[6 Min] [35] We anthropomorphise animals all the time, but we also anthropomorphise action figures, or dolls, or companies when we talk about companies having intentions and so on.[초역] 우리는 / 동물을 의인화합니다 / 줄곧 / 그런데 / 우리는 / 또한 / 의인화합니다 / (움직이는) 캐릭터 인형이나 / 또는 / 인형 / 또는 / 회사 / 우리가 회사에 대해 이야기할 때에 / 갖고 있는 / 의도를 / 그리고 기타 등등 /​[6 Min] [36] We very much are in the habit of seeing ourselves in the world around us.[초역] 우리는 / 아주 / 많습니다 / 보려고 하는 습관이 / 우리들 자신을 / (그) 세계에서 / 우리를 둘러싸고 있는​[6 Min] [37] And while we're busy seeing ourselves by assigning human traits to things that are not, we risk being blindsided.[초역] 그리고 / 우리가 바쁜 와중에 / 보느라고 / 우리들 자신을 / 배정하는 것으로 / 인간의 특성을 / 것들에게 / (인간이) 아닌 / 우리는 / 위태롭게 되었습니다 / 기습을 당함으로​[6 Min] [38] The more fluent that text is, the more different topics it can converse on, the more chances there are to get taken in.[초역] (그) 더 많이 유창하면 할 수록 / 그 텍스트가 / (그) 더 많은 다른 / 주제들에 관해 / 그것은 / 대화를 할 수 있고, / (그) 더 많은 가능성이 / 있습니다 / 속임수에 넘어갈​[6 Min] [39] If we treat computers as if they could think, we might get blindsided, or unpleasantly surprised.[초역] 만약 / 우리가 / 대한다면 / 컴퓨터를 / 마치 / 그것들이 생각할 수 있는 것처럼 / 우리는 / 기습을 당하거나 / 또는 / 불쾌하게 / 놀라게 될 수도 있습니다​[6 Min] [40] Artificial intelligence works by finding patterns in massive amounts of data, so it can seem like we're talking with a human, instead of a machine doing data analysis.[초역] 인공 지능은 / 작동합니다 / 찾는 것에 의하여 / 패턴을 / 거대한 양의 데이터에서 / 그래서 / (그것은) / 보일 수 있습니다 / 우리가 말하는 중인 것처럼 / (어떤) 사람과 함께 / (어떤) 기계 대신에 / 자료 분석을 하는​[6 Min] [41] As a result, we get taken in - we're tricked or deceived into thinking we're dealing with a human, or with something intelligent.As a result, we get taken in - we're tricked or deceived into thinking {that} we're dealing with a human, or with something intelligent.[초역] 결과적으로 / 우리는 / 속임수에 넘어갑니다 / 즉, / 우리는 속거나 / 또는 / 기만을 당합니다 / 생각하는 것으로 / 우리가 / 다루는 중이라고 / (어떤) 인간이나 / 또는 / 무언가를 / 지능을 가진​[6 Min] [42] Powerful AI can make machines appear conscious, but even tech giants like Google are years away from building computers that can dream or fall in love.[초역] 강력한 / 인공 지능은 / 만들 수 있습니다 / 기계가 / 나타내도록 / 의식을 / 그러나 하이테크(의) / 거인 조차도 구글과 같은 / 여러 해 걸립니다 / 구축하는 데에 / 컴퓨터를 / 꿈을 꾸거나 / 또는 / 사랑에 빠질 수 있는​[6 Min] [43] Speaking of which, Sam, what was the answer to your question?[초역] 그래서 샘 / 그것에 관해서 말한다면 / 당신의 질문에 대한 답은 무엇입니까​[6 Min] [44] I asked what happened in the 2013 movie, Her.[초역] 나는 / 질문했습니다 / 무슨 일이 발생했는지를 / 2013년의 영화 / 'Her'에​[6 Min] [45] Neil thought that the main character falls in love with his computer, which was the correct answer![초역] 닐은 / 생각하였습니다 / (그) / 주인공이 / 사랑에 빠진다고 / 그의 컴퓨터와 / 그런데 그것은 맞는 답변이었습니다​[6 Min] [46] OK.[초역] .오케이​[6 Min] [47] Right, it's time to recap the vocabulary we've learned from this programme about AI, including chatbots - computer programmes designed to interact with humans over the internet.[초역] 알았어요 / 어휘를 살펴볼 시간입니다 / 우리가 이 프로그램으로부터 배운 / 인공 지능에 관한 / 포함하여 / 챗봇을 / (즉, 그것은) / 컴퓨터 / 프로그램 (입니다) / 설계된 / 교류하기 위해 / 사람들과 / 인터넷 상에서​[6 Min] [48] The adjective cognitive describes anything connected with the mental processes of knowing, learning and understanding.[초역] (그) / 형용사인 'cognitive'는 / 서술합니다 / 무엇을 / 관련된 / 정신적인 과정과 / 아는 것 / 배우는 것 그리고 / 이해하는 것 등의​[6 Min] [49] Wishful thinking means thinking that something which is very unlikely to happen might happen one day in the future.[초역] 'Wishful thinking(희망 사항)'은 / 의미합니다 / 생각하는 것을 / 무언가가 / 매우 / 개연성이 낮은 / 발생할 / 발생할 수도 있다고 / 언젠가 / (그) 미래에​[6 Min] [50] To anthropomorphise an object means to treat it as if it were human, even though it's not.[초역] 'anthropomorphise(의인화하다)'는 / 물체를 / 의미합니다 / 대하는 것을 / 그것을 / 마치 / 그것이 사람인 것처럼 / 비록 / 그것이 아님에도 불구하고​[6 Min] [51] When you're blindsided, you're surprised in a negative way.[초역] 당신이 'blindsided' 되면 / 당신은 놀라게 됩니다 / (어떤) 나쁜 방법으로​[6 Min] [52] And finally, to get taken in by someone means to be deceived or tricked by them.[초역] 그리고 / 마지막으로 / ' get taken in'은 / 의미합니다 / 속거나 / 또는 / 기만을 당하는 것을 / 그들에게​[6 Min] [53] My computer tells me that our six minutes are up![초역] 나의 / 컴퓨터기 / 전합니다 / 나에게 / 우리의 / 6분영어가 끝났다고​[6 Min] [54] Join us again soon, for now it's goodbye from us.[초역] 곧, 우리에게 합류하십시오 / 이제 이제는 우리가 안녕 인사를 할 시간입니다.​[6 Min] [55] Is artificial intelligence capable of consciousness?[초역] 인공 지능이 / 자각할 수 있을까요?​[6 Min] [56] We'll hear from an expert who thinks AI is not as intelligent as we sometimes think, and as usual, we'll be learning some new vocabulary as well.We'll hear from an expert who thinks {that} AI is not as intelligent as we sometimes think, and as usual, we'll be learning some new vocabulary as well.[초역] 우리는 들을 것입니다 / (어떤) 전문가로부터 / 생각하는 / 인공 지능이 / 지능적이지는 않다고 / 우리가 / 때때로 / 생각하는 것 만큼 / 그리고 / 늘 그렇듯이 / 우리는 몇몇 새로운 어휘를 배우고 있을 것입니다 / 마찬가지로​pdf 파일은 이곳을 클릭하세요.https://blog.naver.com/softca/222998788018을 클릭하세요.PDF 파일은 서로이웃에게만 제공됩니다.​ ​ 새내기할배 블로그 사이트에는 [6 Minute English] 자료와 관련하여 다음과 같은 글들이 매주 포스팅 됩니다단어공부(금주 대본에 나온 주요 어휘) https://blog.naver.com/softca/222998793603멀티단어공부(금주 대본에 사용된 멀티단어(구동사, 숙어) 간단 정리) https://blog.naver.com/softca/222998799992독해공부(금주 대본의 문장별 해석(직역) 자료) 지금 읽고 계신 글입니다.문장별 문법공부(문장별 문법 설명) 현재 작성 중입니다.금주의 어휘(금주 대본에 나온 금주의 어휘를 집중적으로 정리한 자료) 현재 작성 중입니다.항목별 문법공부(금주 대본에 나온 문법을 주제별/항목별로 정리한 자료) 현재 작성 중입니다.금주의 구동사(금주 대본에 사용된 구동사 상세 정리) 현재 작성 중입니다.금주의 PDF(금주의 PDF 및 기타 다운로드 파일) https://blog.naver.com/softca/222998788018 ​ ​BBC Learning English 사이트는 아래의 주소를 참고하십시요https://www.bbc.co.uk/learningenglish/english/features/6-minute-english_2023/ep-230126​이 블로그의 자료를 처음 보시는 분은 상단의 'PDF 자료보는 방법'과 '6 Minute English 공부법'을 꼭 읽어 주시기 바랍니다.​<끝> ​ "
"[고객 사례] 미래에셋캐피탈 ""OCR 기능을 내재화하여 여러 명이 필요한 업무를 한 명이서도 충분히 처리할 수 있게 됐습니다."" ",https://blog.naver.com/n_cloudplatform/222005076375,20200618,"안녕하세요, 네이버 클라우드 플랫폼입니다.​오늘 소개해드릴 고객 사례는고객의 니즈에 맞는 전문화된 금융 서비스를 제공하고, 신성장투자를 선도하는 여신전문금융회사 '미래에셋캐피탈'입니다!​미래에셋캐피탈은 최근 리테일 사업을 확장하며 CS팀을 신설하게 됐는데, 네이버 클라우드 플랫폼의 OCR(광학문자인식 기술)을 활용하여 기존 2~3명이 필요했던 업무를 1명이 처리할 수 있도록 프로세스를 효율적으로 개선하였다고 합니다!​그렇다면 OCR 기술이 가져다주는 업무 혁신,  아래 인터뷰를 통해 확인해보세요!​  Interviewee :미래에셋캐피탈 여신관리본부장미래에셋캐피탈MIRAEASSET CAPITAL 금융의 새로운 길을 만들어 갑니다 . 시작 일시정지 개인금융 성공적인 주식투자를 위한 미래에셋캐피탈 M스탁론! 기업금융 미래에셋캐피탈의 투자철학은 Extensive & Speedy 입니다. 자동차금융 미래에셋캐피탈에서 새차를 실속있게 구매하세요! 신성장투자 국내외 신성장 기업들에 투자하여 새로운 패러다임을 이끌어갑니다. 회사소개 미래에셋캐피탈. 고객님께 더 가까이 다가가겠습니다. 새소식 미래에셋캐피탈에 새로운 소식을 전해 드립니다. 공지사항 이벤트 대출현황 고객님의 대출내역을 확인하실 수 있습니다....capital.miraeasset.com  Q1. 회사 및 제공 서비스에 대해 소개해 주세요.A1. 당사는 1997년 6월 4일 중소기업 창업자에 대한 투자 및 융자업 등을 영위할 목적으로 설립되었습니다. 신기술사업자에 대한 투자, 융자, 경영 및 기술의 지도, 신기술사업 투자조합의 설립, 신기술사업투자조합 자금의 관리 및 운용, 시설대여업, 할부금융업, 여신업무 등을 종합적으로 행하고 있습니다. 주요 취급하는 스탁론, 임대차보증금 담보대출, 오토리스/론 등의 개인금융서비스 부터 기업금융/대체투자/신성장동력의 핵심인 4차 산업 기술 분야 투자 등 특정 영역이나 상품에 국한되지 않은 투자 등의 서비스를 제공하고 있습니다.​ Q2. 네이버 클라우드 플랫폼을 선택하시게 된 배경은 무엇인가요?A2. 신규 시스템 도입이 필요한 상황에서, 네이버 클라우드 플랫폼의 '금융 전용 클라우드'가 가장 적합하다 판단했습니다. 올해 3월부로 당사 내 여신 관리 본부가 신설되었고 동 본부 내 대고객 상담을 주 업무로 하는 CS 팀이 신설되었습니다. 2016년 말부터 수입 자동차 리스할부, 스탁론, 임대보증금 담보대출 등 리테일 사업을 확장해 나가는 중 대고객에 대한 In-Bound Call 서비스 전담조직의 니즈로 신설되게 되었습니다.  자체 전문적인 상담 관련 인프라가 부족한 상황에서 신규 시스템 도입이 필요했습니다. 업권내 구축형 레퍼런스 등을 중심으로 리서치 하던 중 최근 클라우드 서비스에 대한 정보를 바탕으로 인터넷 검색 중 국내 1위 Tech 기업인 네이버도 클라우드 서비스를 제공하고 있는 것으로 파악하였습니다. 다행히도 동 서비스 내 ‘금융권 컨택센터’가 있는 것을 확인하고 네이버 클라우드 플랫폼 담당자와 컨택 이후 서비스 도입에 대한 내부 검토가 계속 진행 중에 있습니다.​ Q3. 네이버 클라우드 플랫폼의 어떤 서비스를 사용하고 계시고, 사용 후 어떤 도움이 되셨나요?A3. AI Service 중 하나인 ‘OCR(광학문자인식 기술)'을 사용하여, 업무의 단순 반복을 줄이고 정확도를 향상시켰습니다. 앞서 말씀드린 CS 팀은 In-Bound Call 상담 이외에 당사 소유의 리스차량에 대한 사후관리도 일부 맡고 있습니다. 그중 당사 고객이 당사 리스차량 운행 중 부과되는 각종 교통 범칙금, 과태료 등은 차량 소유자인 당사에게 고지서(OCR) 형태로 부과되는데, 이에 CS 팀은 과태료 고지서 내용(차량 정보, 위반 정보 등)을 파악하여 다시 발급 관공서에 고객 변경 신청을 하는 업무를 하고 있습니다.​ 이 중 고지서 OCR 용지를 분리하고 관련 내용 추출하는 업무를 기존에는 직원이 수기로 하였으나, 당사 관리 차량의 증가로 인해 유무형의 관리 비용이 상당하였고, 직원의 실수가 많아 업무 개선이 시급했습니다. 이에 동종업계 리서치, OCR 아웃소싱 업체 미팅 등 다양한 방법을 찾았으나, 결국 당사의 현실에 맞는 최적화된 프로세스로 바꾸자는 판단하에 OCR 판독 기능을 내재화하기로 하였습니다. ​ OCR 관련된 기술 리서치 중 네이버 클라우드 플랫폼 OCR 서비스가 국내에서 가장 성능이 좋다는 기사를 접하였고, 평소 네이버 클라우드 플랫폼 서비스(컨택센터 포함)에 대해 관심이 많았던 지라 우선 개인적으로 Pilot 형태로 해당 API 서비스를 가입했습니다. 비전공자로써 프로그래밍 언어를 습득하고 API 연동까지 우여곡절이 많았으나, 네이버 클라우드 플랫폼의 잘 정리된 매뉴얼과 각 서비스별 담당 CP 등의 도움으로 현업 적용이 가능하다는 확신을 가지게 되었습니다. 그리하여 당사 법인으로 동 서비스를 정식으로 가입하게 되었으며, 현재에는 기존에 2~3명이 해도 벅찬 업무를 직원 1명이 다른 업무와 병행해서 처리하고 있습니다.​ Q4. 네이버 클라우드 플랫폼이 갖는 특징 혹은 장점은 무엇인가요?A4. 개인 및 기업 경영 상 앞으로는 클라우드 환경(시스템)이 대세가 될 것으로 개인적으로 보고 있습니다. 이 과정에서 자본력, 기술력, 대규모 선행투자로 규모의 경제를 가지고 있는 네이버가 국내시장에서 독보적일 것으로 보고 있습니다. 개인적으로 상기 OCR 서비스 외에 클로바 CSS, CPV도 사용해 봤습니다. 이 과정에 느낀 바는 타 클라우드 플랫폼에 비해 서비스 범위도 방대하고 특히 사용자에 대한 접근 및 기술 편리성 제공이 탁월하다고 느끼고 있습니다.​ Q5. 향후 네이버 클라우드 플랫폼을 활용하여 어떤 서비스를 제공하시고 싶으신가요?A5. AI 챗봇 / CSR(Clova Speech Recognition)을 도입해 업무의 효율성과 고객 만족도를 높이는 서비스를 계획하고 있습니다. 당사는 Digital Transformation과 관련하여 최우선 과제인 고객 경험을 중요하게 생각하여 고객 응대 서비스로 AI 챗봇을 검토하고 있습니다. 고객센터(콜센터) 업무를 지원하는 문의 응대와 예약, 확인 등 요청 업무를 수행하여 원하는 답변을 찾는 시간을 줄이며 CSR(clova speech recognition) 기능을 탑재하여 고객의 접근성을 높이고자 합니다. 또한, 고객 만족도 조사 및 불완전 판매 모니터링 업무까지 수행할 수 있는 기능을 통하여 업무의 효율성과 고객의 만족도를 제고할 수 있는 서비스를 계획하고 있습니다.​  지금까지 여신전문 금융회사 '미래에셋캐피탈'의네이버 클라우드 플랫폼 활용 사례를 만나보았습니다.​네이버 클라우드 플랫폼의 OCR / AI 챗봇 / 음성 인식 등의 상품을 활용한다면 단순 반복 업무를 줄여 효율성과 업무의 정확도를 높일 수 있습니다. ​또한 네이버 클라우드 플랫폼은 SOC 국제 인증을 취득한 금융 전용 클라우드를 운영하고 있기에더욱더 안전한 클라우드 환경을 제공합니다!​AI 상품들과 금융 클라우드에 대한 자세한 내용은아래 링크를 통해 확인해보시길 바랍니다!​감사합니다. ​ "
캐나다 EF 단기 어학연수 프로그램 비용 안내 ,https://blog.naver.com/raffles7/223041731032,20230311,"​많은 EF 캠퍼스 중에서도 캐나다는 단기 어학연수지로 인기가 높습니다.  한국인들이 북미권 영어 발음을 선호하기도 하고 미국보다는 안전하고 생활비도 적게 드는 편이라서  방학이나 휴가를 이용하여서  짧게는 2주부터 보통 한 달 내외로  여행 겸 어학연수로 많이 떠나시지요.​특히 캐나다가 미국하고 육로로도 이동이 가능하여 밴쿠버로 가시면 주말 이용하여 미 서부지역 시애틀까지도 여행이 가능하시고 토론토로 가시면 미 동부 뉴욕까지도 여행하실 수도 있고요.  EF 캐나다는 다음의 3 지역에 캠퍼스를 두고 있습니다. 밴쿠버 아일랜드(빅토리아) 밴쿠버토론토 ​EF의 캐나다 어학연수는 2주 부터 등록이 가능합니다.  매주 월요일 개강입니다​ 우선 EF의 수업 방식에 대해 먼저 말씀드리겠습니다. EF의 어학연수과정은 다른 어학원보다 좀 더 듣기와 말하기 능력 상에 포커스가 맞추어져 있습니다.​​ ​위 영상에서 보신 EF의 수업 체계는 다음과 같이 5 가지의 수업으로 구성되어 있습니다.1. General Class : 저희도 익히 경험한 강남역 영어학원 같은 소규모 렉처 스타일 수업2. Conversation class : 다양한 디스커션 수업3. iPAD Class : 아이패드를 이용한 2-3명의 pair conversation 수업4. i Lab computer class: 유창한 영어를 위해서는 파트너와의 공부도 필요하나 학습자 스스로의 혼자 해야 할 공부들이 있죠? 바로 올바른 문법을 익히고 올바른 발음으로 자신의 생각을 정리하고 전달해야 합니다. 그를 위한 video lesson(온라인 문법 공부)과 speech recognition(발음 수정)을 컴퓨터를 통해 공부하고 itracker라는 툴을 통해 본인의 프로그레스를 확인할 수 있습니다.5. SPIN Class: Special Interest Class로 대학 교양처럼 어학관련 수업은 물론 캐나다 문화, 비즈니스, 경제 및 법률, 세계문학 같은 흥미 있는 과목 수업을 영어로 참여하는 수업입니다.​​​​​​​​ 주당 수업 시간에 따라 일반과정(26강의)과 집중과정(32강의)을 고르 실 수 있습니다. ​ ​​​​​EF 캐나다 캠퍼스들의 한국인 학생 비율은 5~10% 내외이고 수강생들의 국적은 프랑스, 독일, 스위스, 멕시코, 이태리, 일본 등등 다양합니다. ​해외연수를 결정할 때 국내 어학원이 수준이 딸리거나 부족해서 해외연수를 떠나시는 것은 아닐 거예요. 영어를 쓰면서 낯선 곳에서 현지인의 삶을 체험해 보고 영어 공부에 대한 동기부여를 얻고 싶으셔서 이실 텐데요.성공적인 어학연수는 어학당 수업 방과 후를 어떻게 지내느냐가 매우 중요합니다. 한국인 학생들과 몰려다니면서 보낸다면 영어는 늘지 않을 거예요. EF는 한국인 학생과 중국인 학생 비율이 매우 적은 것이 특징입니다.​그리고 EF 캐나다 프로그램은 어학원 수업 외에도 다양한 액티비티가 있는 것도 특징입니다. EF 밴쿠버, 밴쿠버 아일랜드, 토론토 캠퍼스의  주중 액티비티를 통해 세계 각국에서 온 친구들과 교실 밖 활동을 통해 영어 유창성을 늘리고 외국 친구도 사귀고 주말 액티비티만 잘 참가하셔도 주변지역 여행 충분히 하실 수 있습니다.​ ​ ​​그러면  EF 싱가포르의 어학연수 기간 동안 숙박은 어떻게 해야 하는지 궁금해하실 텐데요?  ​​1. 캐네디안 호스트 패밀리에서 홈스테이 하시는 방법 : 가장 저렴​​​​2.  ​ EF 연계 숙소를 이용하시는 방법 : 연계 숙소에 공실이 있어야 입실 가능하고, 룸 타입에 따라 연수 비용이 늘어날 수 있습니다.​(1) EF 밴쿠버 숙소 ​(2) EF 토론토 숙소​ ​​​마지막으로 캐나다 EF로 어학연수 비용에 대해 안내드리겠습니다.캐나다의 밴쿠버나 토론토는 캐나다에서 드물게 대중교통도 좋고 인구 밀집 대도시라서 어학연수 비용도 비용이지만 주거비용이 만만치 않습니다. 그 점을 감안했을 때 EF의 연계 숙소의 위치 대비 비용은 합리적인 수준에서 책정되어 있어요. ​2023년 캐나다 EF의 영어 연수 최소 등록 단위인 2주 연수 가신 다는 가정으로 연수 비용 안내드리겠습니다.<EF 집중과정(2주 연수 비용) + 홈스테이 2인실 2주 비용>벤쿠버 아일랜드(빅토리아) 캠퍼스 : USD1350벤쿠버:USD1530토론토:USD1530​연수 기간을 늘리시거나, 홈스테이를 1인실 사용하거나, 홈스테이가 아닌 연계 숙소인 레지던스 사용 시 비용은 늘 수 있으니 궁금하신 분들은 개별적으로 연락 주세요.​​캐나다 EF의 어학연수 프로그램에 대해 더 궁금하신 사항은 저희 링크에이드로 연락 주시면 친절히 답변드리겠습니다. ​  링크에이드 유학원은  캐나다 EF 어학연수 입학 수속을 무료로 도와드리고 있습니다.입학 허가를 받은 이후 싱가포르로 출국 및 현지 정착까지 세심히 도와드리고 있습니다.관심 있으신 분들은 카카오톡 아이디 linkaid로 카톡 상담 가능합니다. 전화가 편하신 분들은 010-2473-0049로 전화 주시면 친절히 안내해 드리겠습니다.​ ​ "
"차세대 스마트ARS ‘Ai Call’ 최초 공개, Top레벨 AI기술로 지역 소상공인 사업 편의성 증진시킨다 ",https://blog.naver.com/naver_diary/221630920970,20190828,"Ai Call은 네이버의 최고 수준 AI기술을 활용해, ‘스마트콜’로는 대응하기 힘들었던 고객문의까지 자동으로 대응할 수 있는 차세대 스마트 도구다. 업무로 바쁜 사업주가 놓칠 수 있던 잠재적인 고객까지 확보할 수 있게 돼 지역 소상공인 사업 성장에도 도움을 줄 수 있을 것이다이건수 네이버 글레이스 CIC 대표네이버㈜(대표이사 한성숙)가 27일 ‘제3회 네이버 서비스 밋업’에서 AI기술을 접목한 차세대 스마트ARS ‘Ai Call(가칭)’을 최초로 선보이고 시연회를 진행했다.​이날 네이버는 ▲경청 ▲이해 ▲대화로 이어지는 Ai Call의 작동 프로세스와 자연스러운 대화를 가능케하는 기술에 대해 공개했다. 특히, 각 프로세스에는 ▲음성인식 ▲자연어처리 ▲음성합성 등 네이버의 최고 수준 AI기술이 적용돼, 고객의 문의에 자연스럽게 응대할 수 있다고 발표했다.  | ‘Ai Call’에 적용된 최고 수준의 네이버 AI 기술 |네이버의 음성인식, 자연어처리, 음성합성 기술은 세계 유수 컨퍼런스에서 최고 수준으로 입증된 바 있다.​네이버는 세계 최고 딥러닝 컨퍼런스 ‘ICLR2019’에 국내 기업 중 유일하게 발표자로 참석해, ‘음성인식’ 관련 논문 2개(▲DialogWAE: 대화 반응 다양화를 위한 조건부 Wasserstein 오토인코더 모델 ▲시각 대화 질의 생성을 위한 대규모 “질의자 의도 내 응답자”(AQM) 모델)를 발표, 연구성과를 공개하기도 했다.​‘자연어처리’ 기술 역시 2018년 관련 분야 최고 권위 국제학회 ‘EMNLP2018’의 정규세션의 발표자로 참여해 연구성과를 발표했으며, 2019년 6월에는 글로벌 AI컨퍼런스 ‘CVPR2019’의 ‘음성합성’ 분야 워크숍 챌린지에서 MS, 구글을 제치고 1위를 차지하며 기술력을 입증한 바 있다.Ai Call은 Clova의 음성인식기술(CSR, Clova Speech Recognition)을 이용해 고객의 음성데이터로부터 문자를 추출하고, 자연어 처리(NLP)와 대화엔진을 통해 문의 의도를 이해한다. 이후, 사업주가 등록한 ‘스마트플레이스’ 정보 중 고객이 원하는 정답형 정보를 찾아 자연어 처리해 문장으로 다듬는다. 정리된 답변은 음성합성기술(CSS, Clova Speech Synthesis)을 거쳐 자연스러운 목소리로 고객에게 전달된다. Ai Call은 일련의 프로세스를 0.2초 내에 빠르고 정확하게 수행한다.​실제로, 이날 진행된 시연에서 ‘Ai Call’은 ‘몇 시까지 영업을 하는지’, ‘주차할 공간이 있는지’, ‘주차비가 지원 되는지’와 같은 시연자의 문의에 정확하게 대답했다.​‘Ai Call’이 상용화되면 기존의 스마트ARS ‘스마트콜’과 함께 지역 소상공인의 사업 편의성을 한층 증진시킬 전망이다. 기본적인 정보를 알려줄 뿐만 아니라 인기메뉴를 추천해주고, 추가적으로 필요한 사항이 있는지 먼저 질문하는 등 고객과 능동적으로 소통하며 예약까지 도와줘, 중소상공인은 사업 본질에 더욱 집중할 수 있다.​   ​ "
Github 에서 가장인기 있는 오픈소스 음성인식 소프트웨어  ,https://blog.naver.com/abner33/222159270530,20201201,"1. DeepSpeech — 15, 340 stars​deepSpeech is an open-source speech to text engine which can run in real-time using a model trained by machine learning techniques based on Baidu’s Deep Speech research paper and is implemented using Tensorflow.DeepSpeech can run in real-time on devices ranging from a Rasberry Pi 4 to any power GPU Servers and it supports various platforms for its development such as: Linux, Android, Windows and macOS.Its API supports:C.NETJavaJavascriptPythonTo start using a pre-trained model or train your own model with DeepSpeech, you can follow the links below: GithubOfficial DocumentationSource: https://mycroft.ai/blog/deepspeech-update/?cn-reloaded=1​2. Leon — 7, 100 stars​Leon is an open-source personal assistant who can live on your server and is able to perform task when you ask him to. You can talk to him and he can talk to you, you can text him and he can text you back and the best part is Leon can communicate with you by being offline to protect your privacy.Leon is open-source and uses AI concepts. It is built mainly using Node.js and Python and supported operating systems include: Linux, MacOS and Windows.You can find what he is able to do by browsing the: packages list and read more about it by clicking on this Link: GithubOfficial DocumentationSource: https://github.com/leon-ai/leon​​3. Wav2letter — 5, 400 stars​Wav2letter++ is Facebook AI Research’s end-to-end Automatic Speech Recognition Toolkit written entirely in C++, supporting a wide range of models and learning techniques. It is often compared to DeepSpeech due to the many similarities in the two.Wav2letter++ also embarks a very efficient modular beam-search decoder, for both structured learning (CTC, ASG) and seq2seq approaches.Their Github repository includes recipes to reproduce the following research papers as well as pre-trained models.To start building Recipes, clone the project from: GithubSource: https://www.techleer.com/articles/455-wav2letter-a-facebook-ai-researchfair-automatic-speech-recognition-toolkit/​​4. Annyang — 5, 890 stars​Annyang is an Open-Source JavaScript Speech Recognition library that lets users control your site with your voice commands. It supports more than 75 languages, has no dependencies and is free to use and modify.You can easily add a GUI(Graphical User Interface) for the user to interact with Speech Recognition using Speech KITT. Speech KITT is fully customizable and comes with many different themes, and instructions on how to create your own designs. GithubPlay with some live speech recognition demosSource: https://github.com/TalAter/annyang​​5. SpeechRecognition — 5, 120 stars​SpeechRecognition is a free and open-source module for performing speech recognition in Python, with support for several engines and APIs in both online and offline mode.It has many usage examples:Recognize speech input from the microphoneTranscribe an audio fileSave audio data to an audio fileShow extended recognition resultsCalibrate the recognizer energy threshold for ambient noise levelsListening to a microphone in the backgroundVarious other useful recognizer features GithubOfficial DocumentationSource: https://pypi.org/project/SpeechRecognition/​ "
BBC Learning English 영어문법 - Can AI have a mind of its own? [6 Minute English] ,https://blog.naver.com/softca/222998894844,20230130,"본 해설 자료는 첨부파일(PDF)로 올려져 있습니다. 다운받으셔서 활용하십시오​이번주 6 Minute English(BBC Learning English)가 다룬 문장 및 문법 관련 항목은 생략 구문(5), that(5), of(4), but (등위)(3), 법조동사 용법(3), 복합 전치사구 [prep.+Word+prep. 구문](3), 현재분사(3), 5형식 동사(3), 과거분사(3), 동격구문(3), 동사 A 전치사 B 구문(3), 명사절(2), 격(2), 형태 및 종류(2), 파생 품사(2), which(2), who(2) 등입니다. 그 밖에 4형식 동사, like, to, what, what 구문, when, whether, 후위 수식, as, busy, 분사구문, 비교급, 비인칭 it, 관용구문, 단어 전치사구 [Word+prep. 구문], 법 조동사구, 명사적 용법, 생략 용법, 연결동사, 용법과 구문, 원급비교, 의문사 vs 관계사, 일반적인 수동태 구문, 전치사 관용 구문 등이 각각 1 건씩 다루어졌습니다.​다음은 문장별 문법 설명입니다.  [6 Min] [06] A software engineer called, Blake Lemoine, was working on the artificial intelligence project, 'Language Models for Dialogue Applications', or LaMDA for short.[직역] 블레이크 르모인이라고 불리는 소프트웨어 엔지니어는 '대화 언어 모형 프로그램('Language Models for Dialogue Applications)' 또는 줄여서 LaMDA 라고 하는 인공 지능 프로젝트에서 일하고 있는 중이었습니다.[문법설명] 『called ~』는 과거분사로 'A software engineer'를 후위수식합니다. 분사 앞에 '주격관계대명사와 be 동사'가 생략되어 있다고 생각할 수 있습니다.​[6 Min] [07] LaMDA is a chatbot - a computer programme designed to have conversations with humans over the internet.[직역] 람다는 챗봇입니다 (즉, 그것은) 인터넷 상에서 인간과 함께 대화하기 위해 설계된 컴퓨터 프로그램 (입니다).[문법설명] ⑴ 『designed ~』는 과거분사(구)로 'a computer programme'을 후위수식합니다. 분사 앞에 '주격관계대명사와 be 동사'가 생략되어 있다고 생각할 수 있습니다.⑵ 『have conversations』는 have 디렉시컬 동사 구문입니다. '대화하다'라는 뜻입니다.[참조 Blog 링크]☞ [영문법 용어의 이해(5)] 동사(5) - 디렉시칼 동사(De-lexical verb)와 동작명사 그리고 동사구[참조 Site 링크]☞ [영문법 용어의 이해(5)] 동사(5) - 디렉시칼 동사(De-lexical verb)와 동작명사 그리고 동사구[참조 링크]☞ [영문법 용어의 이해(5)] 동사(5) - 디렉시칼 동사(De-lexical verb)와 동작명사 그리고 동사구​[6 Min] [08] After months talking with LaMDA on topics ranging from movies to the meaning of life, Blake came to a surprising conclusion: the chatbot was an intelligent person with wishes and rights that should be respected.[직역] 영화에서부터 삶의 의미까지 포함하는 주제에 관하여 람다와 함꼐 대화를 한 여러 달 후에, Blake는 놀라운 결론에 이르게 되었습니다. 즉, 챗봇은 [존중되어야만 하는] 소망과 권리를 가진 지능을 가진 사람이었습니다.[문법설명] ⑴ 『from movies to the meaning of life』는 「from A to B」의 구문입니다. A는 movies이고, B는 'the meaning of life' 입니다.⑵ 『that should be ~』의 that은 관계대명사 입니다. 주격이고, 선행사는 'wishes and rights'입니다.⑶ surprising은 현재분사에서 동사적 기능이 소멸되어 형용사로 품사가 전환된 파생 형용사입니다.​[6 Min] [10] He also called it his 'friend'.[직역] 그는 또한 그것을 그의 친구라고 불렀습니다.[문법설명] 동사 call이 5형식 문장을 만듭니다. 목적어는 it이고 목적격 보어는 'his friend'입니다.[참조 링크]☞ 5형식 구조: 주어 + 동사 + 목적어 + 목적격 보어[참조 링크]☞ (영어) 5형식 문형에 대한 이애 (동사의 종류)​[6 Min] [11] Google quickly reassigned Blake from the project, announcing that his ideas were not supported by the evidence.[직역] 구글은 [그의 생각이 증거로 뒷받침된 것이 아니라고] 발표하면서, 재빨리 Blake를 프로젝트에서부터 전환 배치하였습니다.[문법설명] ⑴ 『announcing ~』은 '~하면서'의 뜻을 나타내는 분사구문입니다. 부대 상황(주절과 동시 상황)을 표현합니다. 주로 as나 while 절이 분사구문이 된 부사구입니다.⑵ 『that his ideas were not ~』의 접속사 that은 명사절을 이끕니다. 문장성분은 announcing의 목적어입니다.⑶ 『his ideas were not supported ~』는 수동태 문입니다. 행위자보다 행위 자체가 중요할 때 수동태 구문을 즐겨 씁니다.[참조 링크]☞ 명사절 that 2 (보어로 쓰이는 that절)​[6 Min] [13] In this programme, we'll be discussing whether artificial intelligence is capable of consciousness.[직역] 이 프로그램에서 우리는 [인공 지능이 의식의 능력이 있는지 어떤 지를] 논의를 하는 중일 것입니다.[문법설명] ⑴ 『whether artificial intelligence ~』의 「whether ~」 절은 명사절로 문장 성분은 목적어입니다.⑵ 『capable of』는 「형용사 of 명사」 구문입니다. 형용사 capable이 전치사 of의 목적어인 consciousness와 주어인 'artificial intelligence'과의 관계를 나타내 줍니다.[참조 링크]☞ whether 동격절​[6 Min] [14] We'll hear from one expert who thinks AI is not as intelligent as we sometimes think, and as usual, we'll be learning some new vocabulary as well.We'll hear from one expert who thinks {that} AI is not as intelligent as we sometimes think, and as usual, we'll be learning some new vocabulary as well.[직역] 우리는 [[인공 지능이 [우리가 때때로 생각하는 것 만큼] 지능적이지 않다고] 생각하는] 어느 한 전문가로부터 들을 것입니다. 그리고 마찬가지로, 평소처럼 우리는 몇몇 새로운 어휘를 배우고 있을 것입니다.[문법설명] ⑴ 『who thinks ~』의 who는 제한적용법 주격의 관계대명사입니다. 선행사는 'one expert'입니다.⑵ 'who thinks' 뒤에 명사절을 이끄는 접속사 that이 생략되었습니다. 절의 문장성분은 목적어입니다.⑶ 「as intelligent as」는 「as ~ as」 동등(원급) 비교구문입니다.[참조 링크]☞ as 형용사 as 비교구문 / 두 as의 차이점​[6 Min] [16] What happened to Blake Lemoine is strangely similar to the 2013 Hollywood movie, Her, starring Joaquin Phoenix as a lonely writer who talks with his computer, voiced by Scarlett Johansson.[직역] 블레이크 르모인에게 일어난 일은 [Scarlett Johansson에 의해 목소리를 냈던 그의 컴퓨터와 대화하는] 외로운 작가로 Joaquin Phoenix이 주연을 맡았던 2013년의 할리우드 영화 'HER'와 이상하게 비슷합니다.[문법설명] ⑴ 「What happened to+명사」 구문으로 '「Blake Lemoine」에 일어난 일'이란 뜻의 명사절(관계대명사절)을 만듭니다.⑵ 『~ is strangely similar to the 2013 ~』의 「similar to」는 형용사 similar가 주어 'What happened to Blake Lemoine'와 전치사 to의 목적어인 'the 2013 Hollywood movie, Her'와의 비교 관계를 구체적으로 설명해 줍니다.⑶ 『starring ~』은 현재분사 구문으로 'the 2013 Hollywood movie, Her'를 후위수식합니다. 분사 앞에 「주격관계대명사와 be 동사」가 생략되어 있다고 생각할 수 있습니다.⑷ 『as a lonely writer』의 전치사 as가 자격, 기능, 의무, 역할 등을 나타냅니다. '~로(서)'의 뜻.⑸ 『who talks with ~』의 who는 제한적용법 주격의 관계대명사입니다. 선행사는 'a lonely writer'입니다.⑹ 『voiced ~』는 과거분사로 'his computer'를 후위수식합니다. 분사 앞에 '주격관계대명사와 be 동사'가 생략되어 있다고 생각할 수 있습니다.​[6 Min] [22] Although Hollywood is full of movies about robots coming to life, Emily Bender, professor of linguistics and computing at the University of Washington, thinks AI isn't that smart.Although Hollywood is full of movies about robots coming to life, Emily Bender, professor of linguistics and computing at the University of Washington, thinks {that} AI isn't that smart.[직역] 할리우드가 삶을 얻게 되는 로봇에 관한 영화로 가득 차 있기는 하지만, 워싱턴 대학교의 언어학과 컴퓨터 학과 교수인 Emily bender는 [인공 지능이 그 정도로 영리하지 않다고] 생각합니다.[문법설명] ⑴ 『full of』는 「형용사 of 명사」 구문입니다. 형용사 full이 주어 Hollywood와 전치사 of의 목적어인 'movies about robots ~'와 의 관계를 나타내 줍니다.⑵ thinks 뒤에 명사절을 이끄는 접속사 that이 생략되었습니다. 절의 문장성분은 목적어입니다.​[6 Min] [23] She thinks the words we use to talk about technology, phrases like 'machine learning', give a false impression about what computers can and can't do.She thinks {that} the words {which/that} we use to talk about technology, phrases like 'machine learning', give a false impression about what computers can and can't do.[직역] 그녀는 [[우리가 기술에 대해 이야기하기 위해 사용한] 단어 즉, 'machine learning'과 같은 어구가 [컴퓨터가 할 수 있는 것과 할 수 없는 것]에 관한 잘못된 인상을 준다고] 생각합니다.[문법설명] ⑴ 'She thinks' 뒤에 명사절을 이끄는 접속사 that이 생략되었습니다. 절의 문장성분은 목적어입니다.⑵ 선행사 'the words' 뒤에 목적격 관계대명사 which 또는 that이 생략되어 있습니다.⑶ 「the words ~」와 「phrases」가 동격입니다. 콤마가 동격을 나타냅니다.⑷ 『what computers can ~』의 what은 관계대명사일 수도 의문대명사일 수도 있습니다. 문맥에 맞추어 편하게 선택하여 적용하면 됩니다. what 절은 전치사 about의 목적어입니다.​[6 Min] [24] Here is Professor Bender discussing another misleading phrase, 'speech recognition', with BBC World Service programme, The Inquiry:[직역] 여기에 비비씨-월드서비스프로그램 더인콰이어리와 또 다른 오도하는 어구인 'speech recognition'에 대해 논의하는 bender 교수가 있습니다.[문법설명] ⑴ 『discussing ~』은 현재분사 구문으로 Bender를 후위수식합니다. 분사 앞에 「주격관계대명사와 be 동사」가 생략되어 있다고 생각할 수 있습니다.⑵ misleading은 현재분사에서 동사적 기능이 완전히 소멸되어 형용사로 품사가 전환된 파생 형용사입니다.​[6 Min] [25] If you talk about 'automatic speech recognition', the term 'recognition' suggests that there's something cognitive going on, where I think a better term would be automatic transcription.If you talk about 'automatic speech recognition', the term 'recognition' suggests that there's something cognitive going on, where I think {that} a better term would be automatic transcription.[직역] 만약 당신이 'automatic speech recognition(자동음성인식)'에 대해 이야기한다면, 'recognition'이라는 용어는 [지속하고 있는, 인지하는 무언가가 있다]는 것을 암시합니다. 그러나 거기서 나는 [보다 더 좋은 용어는 자동화된 기록일 것이라고] 생각합니다.[문법설명] ⑴ if종속절을 갖는 복문에 계속적 용법의 관계부사절이 연결된 구조의 문장입니다. 복문 주절의 주어는 he term 'recognition', 동사는 suggests, 목적어는 that 명사절입니다.⑵ 『something cognitive』의 something과 같이 thing으로 끝나는 대명사'를 수식하는 형용사는 뒤에서 수식합니다.⑶ 『something ~ going on』의 『going on』은 현재분사 구문으로 something을 후위수식합니다. 분사 앞에 「주격관계대명사와 be 동사」가 생략되어 있다고 생각할 수 있습니다.⑷ 『where I think ~』의 where는 계속적 용법의 관계부사로 「접속사+부사(and+there)」의 기능을 합니다. '그런데, 그 결과로' 등의 의미를 갖습니다.⑸ 'where I think' 뒤에 명사절을 이끄는 접속사 that이 생략되었습니다. 절의 문장성분은 목적어입니다.​[6 Min] [26] That just describes the input-output relation, and not any theory or wishful thinking about what the computer is doing to be able to achieve that.[직역] 그것은 단지 입출력 관계 만을 서술합니다. 그리고 [컴퓨터가 그것을 성취할 수 있도록 하고 있는 중인 것에 관한] 어떤 이론이나 희망 사항을 (서술하지) 않습니다.[문법설명] ⑴ 본문은 「A, not B」구문입니다. 이는 「B가 아니라 A이다」라는 뜻입니다. A는 'the input-output relation', B는 'any theory or wishful ~' 입니다.⑵ 『what the computer is doing ~』은 what 목적격 관계대명사절로 절의 문장 성분은 전치사 about의 목적어입니다.⑶ 『be able to achieve ~』의 「able to」는 be동사와 결합하여 can의 의미를 갖는 (법)조동사의 기능을 합니다.⑷ 『to achieve that』의 that은 「any theory or wishful thinking」을 지시합니다.​[6 Min] [27] Using words like 'recognition' in relation to computers gives the idea that something cognitive is happening - something related to the mental processes of thinking, knowing, learning and understanding.[직역] 컴퓨터에 관하여 'recognition'과 같은 단어를 사용하는 것은 [인지적인 무언가가 벌어지는 중이라는] 견해를 줍니다. (즉, recognition은) 생각하는 것, 아는 것, 배우는 것 그리고 이해하는 것이라는 정신적인 과정과 관련 있는 무언가 (입니다).[문법설명] ⑴ 『in relation to computers 』의 「in relation to」는 복합전치사구로 '~와 관련된'의 의미를 갖습니다.⑵ 『that something cognitive is ~』 절의 접속사 that은 동격의 that입니다. 'the idea'와 that 명사절은 동격관계에 있습니다.⑶ 『related to the mental ~』의 「related to」는 전치사구로 '~와 관련있는'의 뜻을 갖습니다.⑷ 「the mental processes」와 「thinking, knowing, ~」은 동격관계입니다. 이를 동격의 of라고 합니다. '~라는, ~와(과) 같은'으로 해석합니다.[참조 링크]☞ 해석의 기술 for 독해, 동격 명사 + of ~ing​[6 Min] [28] But thinking and knowing are human, not machine, activities.[직역] 그러나 생각하는 것과 안다는 것은 기계의 행동이 아니라 인간의 (행동)입니다.[문법설명] ⑴ 『thinking and knowing』은 동명사(구)입니다. 문장성분은 주어입니다.⑵ 본문은 「A, not B」구문입니다. 이는 「B가 아니라 A이다」라는 뜻입니다.​[6 Min] [29] Professor Benders says that talking about them in connection with computers is wishful thinking - something which is unlikely to happen.[직역] 벤더스 교수는 [컴퓨터와 관련되어 그 들에 대해 이야기하는 것은 희망 사항이라고] 말합니다. (즉, wishful thinking은) 벌어질 개연성이 낮은 무언가 (입니다).[문법설명] ⑴ 『that talking about ~』의 접속사 that은 명사절을 이끕니다. 문장성분은 목적어입니다.⑵ 『in connection with computers』의 「in connection with」은 [전치사+명사+전치사]의 구조의 복합전치사구를 만들며 명사와 결합하여 부사구로 사용됩니다.⑶ 『is unlikely to happen』의 「unlikely」는 「be+형용사/분사+to」의 문형으로 동사원형(=부정사)과결합하여 동사의 (법)조동사 기능을 합니다. '~할 개연성이 없다/낮다'의 뜻을 갖습니다.⑷ 'unlikely to'⑸ 『which is unlikely ~』의 which는 주격의 관계대명사입니다. 선행사 something을 수식하는 제한적 용법입니다.⑹ 『wishful thinking』의 thinking은 동사의 기능이 완전히 사라진 명사입니다.[참조 링크]☞ 명사절 that 2 (보어로 쓰이는 that절)​[6 Min] [30] The problem with using words in this way is that it reinforces what Professor Bender calls, technical bias - the assumption that the computer is always right.[직역] 이러한 방식으로 단어를 사용하는 것이 갖는 문제는 [그것이 [Bender 교수가 기술적인 편견이라고 부르는 것]을 강화한다는] 것입니다. (즉, 그것은) [컴퓨터가 항상 옳다는] 추정 (입니다).[문법설명] ⑴ 「The problem ~」이 주어, is가 동사, that 명사절이 보어인 2형식 문장입니다.⑵ that 명사절은 it가 주어 reinforces가 동사 what 관계사절이 목적어인 3형식 문장입니다.⑶ what 명사절은 목적격 관계대명사절입니다. 절은 5형식으로 동사 call이 5형식 문장을 만듭니다. 목적어는 관계대명사인 what이고, 'technical bias'가 목적격 보어입니다.⑷ 『that the computer is ~』 절의 접속사 that은 동격의 that입니다. 'the assumption'과 that 명사절은 동격관계에 있습니다.[참조 링크]☞ 5형식 구조: 주어 + 동사 + 목적어 + 목적격 보어[참조 링크]☞ (영어) 5형식 문형에 대한 이애 (동사의 종류)​[6 Min] [31] When we encounter language that sounds natural, but is coming from a computer, humans can't help but imagine a mind behind the language, even when there isn't one.[직역] 우리가 [자연스럽게 들리지만 컴퓨터에서 나온 것인] 언어를 마주치면, [심지어 하나가 아닌 때에 조차도] 사람들은 언어 뒤에 있는 마음을 상상하지 않을 수 없습니다.[문법설명] ⑴ 『When we encounter ~』의 접속사 when은 조건의 의미를 갖는 종속절(부사절)을 이끌고 있습니다.⑵ 등위 접속사 but이 주어를 공유하는 술부 「sounds natural」과 「is coming from a computer」를 연결합니다.⑶ 『sounds natural』의 sound는 be 동사 형식의 2형식 문장을 만드는 연결동사로 '상태의 유지'를 나타냅니다. '~처럼 보이다/들리다'의 뜻을 갖습니다.⑷ 『humans can't help but imagine ~』의 「can't help but」 구문은 (법)조동사구로 동사원형을 취하여 (술어)동사구를 만듭니다. '…하지 않을 수 없다'의 뜻을 갖습니다.​[6 Min] [33] Here's Professor Bender again, discussing this idea with Charmaine Cozier, presenter of BBC World Service's, the Inquiry.[직역] 여기에 다시 Bender 교수가 있습니다. 그리고 그는 이러한 생각을 비비씨-월드서비스(의) 더인콰이어리의 사회자인 Charmaine Cozier와 함께 논의합니다.[문법설명] 『discussing this idea with Charmaine Cozier』는 「동사+목적어+with+전치사목적어」 구문입니다. with는 「참여자, 행위자, 상대방」을 나타냅니다. 'A를 B와 논의하다'의 뜻입니다.​[6 Min] [34] So 'ism' means system, 'anthro' or 'anthropo' means human, and 'morph' means shape… And so this is a system that puts the shape of a human on something, and in this case the something is a computer.[직역] 그래서 'ism'은 시스템을 뜻하고, 'anthro' 또는 'anthropo'는 인간을 의미합니다 그리고 'morph'는 형상을 뜻합니다. 그리고 그래서 이것은 [무언가를 사람의 모양으로 나타내는] 시스템입니다. 그리고 이번의 경우에 무언가는 컴퓨터입니다.[문법설명] ⑴ 『that puts the shape of a ~』의 that은 관계대명사 입니다. 격은 주격이고, 선행사는 'a system'입니다.⑵ 『~ puts the shape of a human on ~』의 「put A on B」는 「동사+A+on+B」 구조의 구문. on은 'B가 A의 영향을 받는 관계'의 의미를 갖습니다. 구문의 의미는 '인간의 형상을 무언가에게 두다'입니다.​[6 Min] [35] We anthropomorphise animals all the time, but we also anthropomorphise action figures, or dolls, or companies when we talk about companies having intentions and so on.[직역] 우리는 줄곧 동물을 의인화합니다. 그런데 우리는 또한 (움직이는) 캐릭터 인형이나 또는 인형 또는 [우리가 의도를 갖고 있는 회사에 대해 이야기할 때에] 회사, 그리고 기타 등등 의인화합니다.[문법설명] 『all the time』은 부사구입니다. 명사(구)가 부사(구)로 쓰이는 명사의 부사적 용법입니다.[참조 Blog 링크]☞ 영어의 명사+명사 구문 ? 명사의 부사적, 형용사적 용법 [부사격, 형용사격]​[6 Min] [36] We very much are in the habit of seeing ourselves in the world around us.[직역] 우리는 우리를 둘러싸고 있는 세계에서 우리들 자신을 보려고 하는 습관이 아주 많습니다.[문법설명] 『in the habit of seeing ourselves』의 'in the habit of'는 in front of, on account of 등과 같은 「전치사+명사+of」 구조로 이루어진 복합전치사구입니다. '~의 버릇이 있는'의 뜻을 갖습니다.​[6 Min] [37] And while we're busy seeing ourselves by assigning human traits to things that are not, we risk being blindsided.[직역] 그리고 우리가 인간의 특성을 [(인간이) 아닌] 것들에게 배정하는 것으로 우리들 자신을 보느라고 바쁜 와중에, 우리는 기습을 당함으로 위태롭게 되었습니다.[문법설명] ⑴ 『we're busy seeing ~』은 「be busy V-ing」 구문으로 '~하느라 바쁘다'의 뜻입니다.⑵ 『assigning human traits to things』은 「동사 A to B」 구문으로 이 때의 to는 '주제, 대상' 등의 의미를 내포합니다. 「assign A to B」의 의미는 'A를 B에 배정하다'입니다.[참조 Blog 링크]☞ be busy ~ing 구문과 busy의 세가지 용법​[6 Min] [38] The more fluent that text is, the more different topics it can converse on, the more chances there are to get taken in.[직역] 그 텍스트가 더 많이 유창하면 할 수록, 그것은 더 많은 다른 주제들에 관해 대화를 할 수 있고, 속임수에 넘어갈 더 많은 가능성이 있습니다.[문법설명] 본문은 「the 비교급+S+V, the 비교급+S+V」 구문으로 「~하면 할수록 더욱 … 하다」의 의미를 갖습니다. .★ get taken in '속임수에 넘어가다'[참조 링크]☞ 기초영어문법 68 'the 비교급 S V, the 비교급 S V' 구문 완전정복​[6 Min] [40] Artificial intelligence works by finding patterns in massive amounts of data, so it can seem like we're talking with a human, instead of a machine doing data analysis.[직역] 인공 지능은 거대한 양의 데이터에서 패턴을 찾는 것에 의하여 작동합니다. 그래서 [우리가 자료 분석을 하는 기계 대신에 사람과 함께 말하는 중인 것처럼] 보일 수 있습니다.[문법설명] 『~ seem like we're talking ~』의 like는 접속사로 as if, as though의 의미로 사용되고 있습니다.​[6 Min] [41] As a result, we get taken in - we're tricked or deceived into thinking we're dealing with a human, or with something intelligent.As a result, we get taken in - we're tricked or deceived into thinking {that} we're dealing with a human, or with something intelligent.[직역] 결과적으로 우리는 속임수에 넘어갑니다. 즉, 우리는 [우리가 인간이나 또는 지능을 가진 무언가를 다루는 중이라고] 생각하는 것으로 속거나 또는 기만을 당합니다.[문법설명] thinking 뒤에 명사절을 이끄는 접속사 that이 생략되었습니다. 절의 문장성분은 목적어입니다.​[6 Min] [42] Powerful AI can make machines appear conscious, but even tech giants like Google are years away from building computers that can dream or fall in love.[직역] 강력한 인공 지능은 기계가 의식을 나타내도록 만들 수 있습니다. 그러나 구글과 같은 하이테크 거인 조차도 [꿈을 꾸거나 또는 사랑에 빠질 수 있는] 컴퓨터를 구축하는 데에 여러 해 걸립니다.[문법설명] ⑴ 『AI can make machines ~』은 make 5형식 문장입니다. 목적어가 machines, 목적격보어가 'appear ~'입니다. make가 사역동사이므로 동사원형이 목적격 보어로 쓰이고 있습니다.⑵ 『that can dream ~』의 that은 관계대명사 입니다. 격은 주격이고, 선행사는 computers입니다.​[6 Min] [47] Right, it's time to recap the vocabulary we've learned from this programme about AI, including chatbots - computer programmes designed to interact with humans over the internet.[직역] 알았어요 챗봇을 포함하여 우리가 이 프로그램으로부터 배운 인공 지능에 관한 어휘를 살펴볼 시간입니다. (즉, 그것은) 인터넷 상에서 사람들과 교류하기 위해 설계된 컴퓨터 프로그램 (입니다).[문법설명] 『it's time to recap ~」 에서 it는 시간을 나타냅니다. 시간을 나타내는 주어 it는 문장에서 해석하지 않습니다.​[6 Min] [48] The adjective cognitive describes anything connected with the mental processes of knowing, learning and understanding.[직역] 형용사인 'cognitive'는 아는 것, 배우는 것 그리고 이해하는 것 등의 정신적인 과정과 관련된 무엇을 서술합니다.[문법설명] 「the mental processes」와 「knowing, learning and understanding」은 동격관계입니다. 이를 동격의 of라고 합니다. '~라는, ~와(과) 같은' 등으로 해석합니다.[참조 링크]☞ 해석의 기술 for 독해, 동격 명사 + of ~ing​[6 Min] [49] Wishful thinking means thinking that something which is very unlikely to happen might happen one day in the future.[직역] 'Wishful thinking(희망 사항)'은 [[매우 발생할 개연성이 낮은] 무언가가 미래에 언젠가 발생할 수도 있다고] 생각하는 것을 의미합니다.[문법설명] ⑴ 『that something which ~』은 접속사 that이 이끄는 명사절입니다. 절의 문장성분은 목적어입니다.⑵ 『which is very unlikely ~』의 which는 주격의 관계대명사입니다. 선행사 something을 수식하는 제한적 용법입니다.⑶ 『which is very unlikely to ~』의 「unlikely to」는 be동사와 결합하여 '~할 개연성이 낮다의 의미를 갖는 (법)조동사의 기능을 합니다.⑷ 『one day』는 부사구입니다. 명사(구)가 부사(구)로 쓰이는 명사의 부사적 용법입니다.[참조 링크]☞ 명사절 that 2 (보어로 쓰이는 that절)[참조 Blog 링크]☞ 영어의 명사+명사 구문 ? 명사의 부사적, 형용사적 용법 [부사격, 형용사격]​[6 Min] [53] My computer tells me that our six minutes are up![직역] 나의 컴퓨터기 나에게 [우리의 6분영어가 끝났다고] 전합니다.[문법설명] ⑴ 『that our six minutes ~』은 접속사 that이 이끄는 명사절입니다. 절의 문장성분은 (직접) 목적어입니다.⑵ 『~ tells me that ~』는 tell은 4형식 문장을 구성합니다. 간접목적어는 me, 직접목적어는 that절 입니다.[참조 링크]☞ 명사절 that 2 (보어로 쓰이는 that절)[참조 링크]☞ 이런 표현 어떻게 - It takes kevin a long time to come ~​​pdf 파일은 이곳을 클릭하세요.https://blog.naver.com/softca/222998788018을 클릭하세요.PDF 파일은 서로이웃에게만 제공됩니다.​​ ​ 새내기할배 블로그 사이트에는 [6 Minute English] 자료와 관련하여 다음과 같은 글들이 매주 포스팅 됩니다단어공부(금주 대본에 나온 주요 어휘) https://blog.naver.com/softca/222998793603멀티단어공부(금주 대본에 사용된 멀티단어(구동사, 숙어) 간단 정리) https://blog.naver.com/softca/222998799992독해공부(금주 대본의 문장별 해석(직역) 자료) https://blog.naver.com/softca/222998889013문장별 문법공부(문장별 문법 설명) 지금 읽고 계신 글입니다.금주의 어휘(금주 대본에 나온 금주의 어휘를 집중적으로 정리한 자료) 현재 작성 중입니다.항목별 문법공부(금주 대본에 나온 문법을 주제별/항목별로 정리한 자료) 현재 작성 중입니다.금주의 구동사(금주 대본에 사용된 구동사 상세 정리) 현재 작성 중입니다.금주의 PDF(금주의 PDF 및 기타 다운로드 파일) https://blog.naver.com/softca/222998788018 ​ ​BBC Learning English 사이트는 아래의 주소를 참고하십시요https://www.bbc.co.uk/learningenglish/english/features/6-minute-english_2023/ep-230126​이 블로그의 자료를 처음 보시는 분은 상단의 'PDF 자료보는 방법'과 '6 Minute English 공부법'을 꼭 읽어 주시기 바랍니다.​<끝> ​ "
Victor Bruzzone-Political Ontology in Rawls and Honneth: The Difference Recognition Makes ,https://blog.naver.com/2h4jus/223090901582,20230502,"​Introduction​Through Rawls’ concept of the original position, we imagine representative parties behind a veil of ignorance tasked with securing the best arrangement for the citizens they represent. By limiting the parties’ particular knowledge of particular citizens, both rational and common interests are balanced, compelling the parties to propose fair principles of cooperation. The idea is to decide the fairest terms of social cooperation and to balance self‐interest (the rational) with the common interest (the reasonable) (Rawls, 1999, pp. 118‐120). While Rawls’ theory has been tremendously influential it has attracted many critics. Such critics assert that Rawls’ use of the device of the original position implies an untenable conception of the person, conceived as individualistically rational.​In contrast to Rawls, Axel Honneth approaches the question of justice by positing intersubjective affirmation through recognition as a crucial condition. According to Honneth, for agency, autonomy, or cooperation to be realized, recognition as understood through three broad spheres, love, solidarity, and respect, must be considered. For Honneth, the way different types of intersubjective relations impact overall human wellbeing is of central concern. While Honneth builds his theory from the development of subjectivity, for Rawls the starting point free and equal citizens, fair distribution, and public institutions.​In the following pages Rawls’ theory of justice as fairness and Axel Honneth’s recognitional approach to justice will be compared. The question to begin with is this: what is the presupposed object domain of justice? For Rawls’ the domain is the structure of public institutions associated with distribution and equal basic rights, whereas for Honneth the appropriate domain is mutual recognition and how this affects subjectivity. Ultimately I will argue that Honneth’s political ontology is preferable because of the broader range of political considerations that can be taken into account. For instance, a Honnethean approach can consider the ways in which the cultural value patterns in everyday life can lead to recognitional distortions. In contrast, Rawls’ focus on public institutions makes it difficult to theorize about recognitional distortions caused by cultural value patterns not directly subject to public institutions (e.g. systematic racism or sexism). I will argue that, because recognitional distortions caused by cultural value patterns are relevant to justice, we should prefer Honneth’s theory of justice. Moreover, I will offer reasons to think Honneth’s theory offers more theoretical tools for addressing injustice at the level of everyday cultural value patterns through cultural critique.​Before proceeding, the concept of political ontology needs to be introduced. Ontology (also interchangeable with “conception of the person”, or “metaphysics of the person”) should be understood as a premise necessary in any political or ethical argument. That is, if a political theory relies on incorrect or incomplete ontological presuppositions that theory’s normative force is weakened. This follows from the idea that political claims are essentially claims about people. This is because political claims are trying to prescribe what is fairest or most just for people. Any concepts of what is fair or just for people must necessarily imply some knowledge of what those people are (how they function). This is always necessarily the case, even if not explicit, it is always at least implicit.​Honneth’s view starts with the metaphysical assertion that healthy human subjectivity is dependent on intersubjective recognition. Rawls, on the other hand, insists throughout his work that he has a political and not metaphysical conception of the person. Rawls avoids making metaphysical claims about persons because of his concern with sectarianism and the possibility of a plural society with multiple cultures and belief systems. For Rawls, a metaphysics of the person could count as one belief system, and thus threaten the possibility of a plural society.​Rawls’ Implied Political Ontology​Rawls conceives of citizens as having the capacity to exercise two moral powers: the rational and the reasonable1. A citizen’s rational power is concerned with “judgment and deliberation in seeking ends and interests peculiarly its own (Rawls, 2005, p. 50)”. This ability to pursue a conception of the good is motivated by the individualized interest in several all purpose goods (ibid, p. 178). These include basic rights and freedoms, basic income, equality of opportunity, and the social basis for self‐respect (ibid, p. 181). The motivating force behind these primary goods is an agent’s ‘higher‐order interests’. This refers to an agent’s ability to develop and exercise its two moral powers. Rawls explains these two moral powers as:​[Citizens] being reasonable and rational means that they can understand, apply, and act from the two kinds of practical principles. This means in turn that they have a capacity for a sense of justice and for a conception of the good (Rawls, 2005, p. 108).​Primary goods are the means by which an agent can secure its higher‐order interest through its two moral powers (ibid, p. 106). This is what Rawls refers to as “full autonomy” (ibid, p. 78‐9).​Based on Rawls’ political conception of the person he understands full autonomy as necessary for a society based on fair terms of cooperation. At this point it is important to get clear on full autonomy. It is not simply the unhindered pursuit of an agent’s rational ends. Rather, it is the actualization of both moral powers. Therefore, we must explain more specifically how the reasonable fits into Rawls’ view. This is essentially an agent’s ability and desire to act according to, and propose, principles of fairness. Rawls explains the idea of a reasonable citizen helpfully in the following:​Reasonable persons, we say, are not moved by the general good as such but desire for its own sake a social world in which they, as free and equal, can cooperate with others on terms all can accept. They insist that reciprocity should hold within that world so that each benefits along with others (Rawls, 2005, p. 50).​This demonstrates the way the reasonable is deeply connected to a desire for social cooperation. Thus, social cooperation is seen as an inherent good in‐and‐of‐itself in the context of the full expression of a citizen’s two moral powers (full autonomy). This is because without the reasonable, agents would only act in accordance with self‐interested rationality, and could be seen as losing a measure of social autonomy (because no one likes people who are only selfish).​Earlier I mentioned that Rawls insists he does not rely on a metaphysical account of the person. The reason for this is that it allows for a certain impartial distance that helps to ensure fair terms of cooperation. It also avoids potentially troublesome normative commitments that might be included with certain metaphysical outlooks. For example, some conceptions of the person might include an appeal to human nature with specific prescriptive content (i.e. it is natural for women to take care of children). Nevertheless, I maintain that a theory of justice without some metaphysical commitment to political ontology is prima facie impossible. I claim this because the plausibility of deriving an accurate account of justice depends on understanding the people for whom justice is intended and that requires a metaphysics of the self. Rawls defends his avoidance of metaphysics in a footnote in Political Liberalism arguing that he is not quite clear what a metaphysical conception of the person would entail. He does admit that “ordinary conceptions of persons” might indeed imply certain metaphysical theses regarding persons as moral and political. However, such commitments are so general that one could both agree with Rawls’ political conception and still hold to any number of metaphysical views (Rawls, 2005, p. 29 n31).​I will make two points about why I think avoiding a conception of the person confuses the matter. Firstly, a broad metaphysics does not preclude us from drawing universal conclusions about justice. Indeed, metaphysical claims about thegeneral tendency of people, that they can be rational and reasonable, and that they 4seek primary goods, does not commit us to claim anything about the content of particular goods and for this reason does not necessarily risk sectarianism.​The second point I want to make is that in order for a political conception of the person to be plausible, there must be a presupposed metaphysical conception of the person. To demonstrate what I mean consider some remarks made by Samuel Freeman in his book, Rawls. In order to clarify Rawls’ distinction between a metaphysical and political conception of the person Freeman draws on the legal conception of the person in western judicial systems. In such systems persons are assumed to have certain capacities for autonomy, be of sound mind, etc. During such legal proceedings a determination is made in light of such assumptions as to whether a particular person is fit to stand trial (Freeman, 2007, pp. 335‐6). This seems to clarify what Rawls is aiming at. However, for such a legal system to have any validity must it not aim at a true general conception of the person? What I mean is, even if we say a conception of the person is political (or legal), we are still relying on some metaphysical assumptions. Namely, in most cases people have the capacity to be reasonable and rational (or of sound mind), etc. It seems to me that claiming to have a political conception of the person only pushes the metaphysical question one step back, all the while remaining tacitly there.​The above offers good reasons for believing that Rawls indeed relies on a political ontology (metaphysical conception) of the person. This conception is nevertheless quite general and includes a person’s two moral powers, a person’s desire to secure certain goods and interests, as well as, and quite crucially, the desire to value social cooperation on its own terms (the reasonable). I see no reason to accept the idea that Rawls avoids metaphysics. Rather, we can acknowledge his conception’s generality in its attempt to preserve the possibility of pluralism. One way to think about this generality is to offer an analogous case. If we think about the idea of a chair we can make the general claim that they are objects that, in ideal conditions, possess the capacity to be sat on. Yet this description of a chair says nothing other than its ideal capacity to be sat on. This means that a plural concept of chair is preserved. I interpret Rawls’ conception of the person to be general in a similar way. Nevertheless, in both cases I want to claim that we are saying something metaphysical regardless of its generality. As I will argue later however, this metaphysical account ends up being overly general in a way that limits our ability to theorize about situations obviously relevant to justice.​Before proceeding it is important to address the ontological implications of the original position and arguments from behind the veil of ignorance. Some have incorrectly claimed that the parties in the original position suggest something factually true about moral psychology – the implication is that somehow the device of the original position commits Rawls to a conception of the person as “unencumbered by prior moral ties” and essentially reducible to disembodied rationality (Sandel, 2007, p. 362). Rawls responds to this by claiming that​Justice as fairness is badly misunderstood if the deliberations of the parties, and the motives we attribute to them, are mistaken for an account of moral psychology, either of actual persons or of citizens in a well‐ordered society (Rawls, 2005, p. 28).​Indeed, the original position is a device of representation intended to clarify matters of fairness and compel readers to consider what it would be like to be the member of a less advantaged group. I read the device as stimulating one’s ability to empathize with the plight of others. For it is the intuition that such groups are at an unfair disadvantage that motivates the device’s soundness. Technically speaking this amounts to tracking both of an agent’s moral powers. For the rational interests of agent’s is tracked with reasonable interests of others. Moreover, I do not see a substantive difference between this type of moral reasoning and that which Honneth is committed to. Indeed, how are we to judge matters of misrecognition without being able to imagine alternative social locations, or similarly, as critical theorists to consider the oppressive material conditions of society, even from the standpoint of privilege? To deny this would seriously threaten the validity of any moral reasoning.​In the above I have provided a sketch of Rawls’ implied ontological commitments by examining his political conception of citizens. I argued that Rawls is unable to plausibly hold to the idea that his conception of citizens avoids metaphysics. Essentially, he conceives of citizens as having competing rational interests (all‐purpose goods and/or comprehensive doctrines), and a shared sense of wanting to cooperate (the reasonable) as free and equal moral persons. Rawls seems to think that if this is left unregulated individual or group interests will trump any possibility of reciprocal cooperation and pluralism. As I have discussed above, he proposes a procedural system meant precisely to address this imbalance. From his proceduralism Rawls derives principles of justice meant to regulate public institutions related to equal rights, liberties, and distribution of primary goods.​Honneth’s Hegelian Ontology​Honneth’s recognitional approach to justice relies on a conception of the person drawn heavily from Hegel’s philosophy of human consciousness. In this section I will reconstruct some of its most relevant points. I will also argue that, like with Rawls’ political conception of the person, a Hegelian conception of the person preserves the possibility of a plural society.​One of Hegel’s initial premises is that the subject is ontologically understood as desire (Honneth, 2012, p. 13). Desire, naively understood, is often imagined to be somehow extrinsic. For example, hunger extrinsically compels us to desire food, or social pressures compel us to desire status symbols. Such an understanding of desire would be to misunderstand what Hegel (and Honneth) has in mind. Desire, ontologically understood is the motivation behind any sort of human activity. To take one example, phenomenological observation of our stream of consciousness reveals it as a continual movement driven by incompleteness. Thoughts always lead to other thoughts because somehow previous thoughts are incomplete. There is something that compels thought to continue thinking at an insatiable rate. In this way, incompleteness or desire is an ontological feature of human subjectivity. It is thus not something extrinsic, but intrinsic, and it is expressed in several ways. This still does not tell us how recognition is connected to this ontological desire. Indeed, we can easily imagine a scenario in which thought is able to continue thinking itself all the while remaining solipsistic. Solipsism turns out to be untenable on Hegel’s account of subjectivity – for, according to Hegel, consciousness depends on mutual recognition.​The subject begins as a ‘pure negativity’ in the sense that it notices its ability to freely consume the world it encounters (Hegel, 1979, p. 109). At the same time, the subject encounters resistance from the world; it is not free to completely constitute its reality. As a consequence of encountering the world as independent of it, the subject desires to confirm its own experience. The only way to do this, and to establish self‐consciousness is by encountering another subject with the same negative abilities – an ability to affirm and negate the world (Honneth, 2012, p. 13). One expression of this ontological desire refers to the fundamental need to have our experiences confirmed by another being of similar capabilities. In doing this however, the subject surrenders its illusion of individual omnipotence over the world. Subjectivity is confirmed into self‐consciousness by the encounter with the other. This encounter establishes itself as an act of reciprocal confirmation (ibid, p. 14).​Now, how does such an account lead to an ethical or political moment? Robert Williams comments helpfully on this:​The other is a self‐limitation and decentering of the ego, but at the same time is a liberation of the self from its natural solipsism and immediate desire. Thus the “natural subject of desire” undergoes an ethical transformation. Return to self out of otherness is not a return to the status quo ante recognition but a development to a higher level, mediated by the acknowledgment of the other in its difference (Williams, 2000, p. 57).​Williams goes on to elaborate claiming that for Hegel recognition is a condition of possibility for a fully autonomous self. “Not only must the other be allowed to be; the other’s free, uncoerced recognition is crucial to the self” (ibid, p. 57).​The self’s reciprocal encounter with the other should not be understood as a kind of modus vivendi where each subject agrees to work together for the sake of individual survival. Rather, the claim is that the encounter with the other is: firstly, necessary for coherent self‐consciousness to emerge, and secondly, leads to an improved more resilient type of consciousness. Thus, reciprocal recognition is more than merely an advantageous good, but ontologically crucial. The idea of a “decentered ego” captures a really important aspect of this view. The ego is decentered in the sense that its stability relies on more than just its own internal essence. That is, its very essence is in many ways external, and in this way decentered. This externality includes recognitional relations with other subjectivities expressed in the form of social experience. In other words, human beings are more than situated in particular societies, but in many ways, are expressions of their societies.​Before connecting the above remarks to Honneth’s theory of justice a few remarks should be made to more clearly situate this argument. Firstly, it should be stressed that the above is not intended to be a literal philosophical anthropology, but rather to show how human consciousness primordially depends on recognitional factors. The above is way of conceptually showing this. Secondly, there are many examples of empirical psychological research that demonstrate the importance of self‐other relations for the development of human subjectivity. For this reason this account cannot be said to be groundless conjecture. The fact that the Hegelian account of reciprocal recognition says nothing about the content of recognition itself should be noticed. In other words, it is merely pointing something out about the structure of subjectivity, not about its contents. In this way it is not prescribing anything in particular other than the crucial role of recognition in general (and humans as social).​I have thus far described the subject’s basic desire for recognition, and how this desire conceptually emerges. I also noted the way the subject is understood as ontologically desiring in the sense that desire is condition of possibility for recognition (i.e. recognition is something desired). The insatiability of the desiring subject leads to inevitable struggles for recognition. This is because the subject is in a constant process of self‐overcoming and movement. These struggles for recognition are where the ethical and political dimension emerges. For it is in these struggles that the need for politics is grounded. Politics is a kind of attempt at resolving such struggles, whether by domination or agreement. This is clearly evident in the movement of history and, on the political side, social struggles for recognition. We see this with the changes in culture and moral traditions over time (civil right etc.). A core aspect of Hegelian subjectivity is its historical contingency. As Deranty and Renault point out, this contingency allows for calling the status quo into question, by for example, occupying an identity category previously unexpressed (2007, p. 106). These examples are demonstrations of Hegelian ontological desire as a constant and historically contingent self‐overcoming movement.​To summarize, the above serves as a very plastic conception of the person defined by desire and intersubjective recognition. Yet the content of this formal structure remains historically contingent and normatively underdetermined. Injustices then are factors that interfere with a subject’s ability to fully participate in the economy of recognition. Honneth argues that there is a normative core to the ontological importance of recognition, namely: “one’s autonomy is vulnerable to disruptions in one’s relationship to others (Anderson & Honneth, 2005, p. 130)”. Honneth takes this Hegelian insight and roughly demarcates it along three spheres of recognition: love, formal‐legal, solidarity. Interference in recognition can then be better addressed when considered along each sphere of recognition. This point is crucial to Honneth’s larger recognitional theory of justice.​Honneth’s Hegelian account of the subject not only leaves room for, but also strongly leans in the direction of a pluralistic theory of justice. Through Rawlsian eyes it might be tempting to accuse Honneth’s conception of the person as being a comprehensive doctrine. Contra this temptation we should consider the open ended‐ness of the above conception of the person. The normative contents of recognition remain highly underdetermined. The above is describing the structure of human subjectivity and intersubjectivity, and its capacity to participate in forms of recognition. Rawls resisted making any metaphysical commitments in his conception of the person because he worried about the way metaphysics might lead to certain normative conclusions incompatible with the fact of pluralism. I see nothing in the above that might be incompatible with the fact of pluralism. If anything, understanding recognition helps to clarify the reasons why pluralism emerges and how we might respond to it.​Honneth’s Recognitional Account of Justice​Based broadly on the above‐discussed ontology Axel Honneth develops a recognitional approach to justice. This approach is significantly different from theories, such as Rawls’, that frame justice in terms of the regulation of public institutions. Rather than a just set of institutions designed to regulate primary goods, Honneth’s focus is on recognitional conditions necessary for self‐realization (Honneth, 2004, p. 354).​The justice or wellbeing of a society is measured according to the degree of its ability to secure conditions of mutual recognition in which personal identity formation, and hence individual self‐realization, can proceed sufficiently well (Honneth, 2004, p. 354).​On this view, self‐realization is grounded in an inherent human desire forrecognition. Honneth situates the need for recognition into a theory of justice through his three spheres of recognition. As mentioned above, these include, love, formal‐legal and solidarity (Honneth, 2004, p. 358; Fraser & Honneth, 2004, p. 142). Each of these spheres of recognition corresponds to a way of addressing an agent’s ability to form a stable self‐identity.​When the sphere of recognition is love the principle of fundamental human need is applied (Honneth, 2004, p. 358). This principle would address psychological abuses relating to an individual’s close intersubjective relationships, protect individuals from physical harm, and address the conditions for healthy affective relations (Honneth, 1997, pp. 32; Anderson & Honneth, 2005, p. 135). The second sphere of recognition, the formal‐legal, concerns being recognized to have “the same moral accountability as every other human” (Honneth, 1997, p. 30). This is essentially the protection of basic rights and freedoms, to be treated equally in the eyes of the law, equal opportunity, etc. (Anderson & Honneth, 2005, p. 133). This sphere of recognition is essentially equivalent to recognitional concerns addressed in Rawls’ theory expressed through the regulation of public institutions and his principles of justice.​The third sphere of recognition, referred to as solidarity, is the sphere most relevant to the politics of difference. The types of misrecognitions that are to be addressed here are connected to the cultural value patterns of society. For example, the possibility of being openly gay depends on a kind of semantic‐social acceptance (Anderson & Honneth, 2005, p. 136). The central concern within this sphere is recognition of subjective particularity. For this to happen cultural value patterns must be receptive to new ways of accepting (recognizing) people’s emerging experiences. Thus, group difference, oppression and marginalization are relevant to this sphere (ibid, p. 136). The valuation of people’s particular achievements is also an important aspect. Thus, it is not only about valuing particular forms of life, but also the achievements associated with such forms of life (for example, same‐sex marriage) (Honneth, 2001, p. 49).​To further elaborate on the three aforementioned spheres of recognition, we can consider each from a subjective perspective. Honneth describes these spheres subjectively as self‐confidence (love), self‐respect (formal‐legal sphere), and self‐ esteem (solidarity) (Anderson & Honneth, 2005, pp. 131‐2). To be self‐confident we need to feel loved and valued by others. Honneth argues that a close network of self‐ other relations leads to the kind of self‐confidence necessary for political and social cooperation. When we consider self‐respect from a similar perspective we can understand it as the desire to be viewed equally within civil society. Honneth believes that self‐respect is supported by a formal principle of social equality and dignity, similar to the Kantian notion (Anderson & Honneth, 2005, p. 131). Addressing this sphere would ensure that all humans be recognized as having an inherent value in the public realm.​Finally, in order to understand self‐esteem we need only to think about how others in our society, whether close to us or otherwise, might judge the legitimacy of our life preferences and achievements. This sphere of recognition is deeply concerned with the ways in which society as a whole, in its semantic‐symbolic economy of value patterns, judges the legitimacy of certain activities. An obvious example would be the way society has judged homosexuals. I interpret this sphere to be connected with normative creativity, or the possibility of social change. What I mean is that it is usually at the level of cultural value patterns that demands for new forms of recognition take place. For example, there are currently some cultural changes occurring regarding different gender categories. Evidence of this ongoing cultural shift is even noticeable in the way Facebook has added 56 new gender categories for users to choose from when creating or editing their profiles (Weber, 2014).​It is important to emphasize that all three spheres of recognition are interrelated. To help understand this we can use the example of transgendered people. We can imagine a case in which a transgendered person receives unconditional love and support from friends and family. This person develops the kind of self‐confidence that allows them to call the prescribed and pre‐existing categories into question (and confidently come out). What I am suggesting is that recognition in the sphere of love is extremely helpful for developing coherent self‐ esteem. This level of recognition enables the semantic‐symbolic economy of cultural value patterns to begin to shift (solidarity‐recognition). As we have seen in the contemporary world, once this level of recognition shifts value patterns, laws are often altered accordingly. All three levels, no doubt, are reciprocally interconnected. Indeed, we can imagine the way laws reinforce cultural value patterns related to solidarity. No doubt both spheres influence the possibility of love recognition in the private realm as well. Families look to law and tradition to ground the private values of their home. We need only to remind ourselves of the recent cases of transgendered teens committing suicide because their parents refuse to accept them (Fantz, 2015). The parents in this case were influenced by cultural value patterns that oppose the legitimacy of transgendered persons. In this way we can notice how Honneth’s concern related to justice extends into the private realm.​From the perspective of justice, a society will be just when recognitional concerns in all spheres are continually addressed. It is important to notice that Honneth makes no explicit mention of economic distribution. There is a reason for this. Honneth believes distributional concerns will necessarily be rectified as a consequence of addressing his spheres of recognition (Fraser & Honneth, 2004, p. 126). The main idea here is that the only reason economic inequalities are of issue is because they bear some important connection to one of his three spheres of recognition (ibid, p. 132). Specifically, Honneth claims redistribution would be addressed by two of his spheres of recognition. The formal‐legal sphere (self‐ respect) would ensure equal treatment under the law including equality of opportunity. Also, the sphere of solidarity (self‐esteem) would bear on recognitional concerns related to class struggle. This is because maldistribution affects cultural value patterns and the fabric of social esteem (Honneth, 2001, p. 54). That is, class is connected to a society’s understanding of status and value.​In the above I have sketched the basics of Honneth’s recognitional theory of justice. At the center of his theory is the appeal to three spheres of recognition: love (social‐subjective wellbeing), formal‐legal (equality, social, universal), and solidarity (particularity, achievement, esteem). Thus, a society will be just if it continues to address concerns within each sphere of recognition as it contributes to healthy self‐ development. ‘Healthy’ here is taken to mean ability to reciprocally participate in society in ways that encourage continued recognitional movement within each sphere of recognition.​Recognition in Justice as Fairness​Earlier in this paper I reconstructed the central ideas of Honneth’s Hegelian ontology. As we saw, one of the central ontological fundamentals for Honneth is a subject’s desire for recognition and its crucial role in self‐development. In the following section I will consider the possibility of retrieving a similar concept of recognition from Rawls’ theory. To this end, I will consider one of Rawls’ primary goods that I see as the most promising. That is, the social bases of self‐respect. Rawls claims that the ‘social bases of self‐respect’ is the most important primary good of all. Indeed, for Rawls, self‐respect helps to ground the value of all other primary goods (Rawls, 2005, p. 318). Here we can notice some affinity with Honneth’s theory. For Honneth, self‐development is primarily grounded in subjective wellbeing as a result of addressing the three spheres of recognition. Similarly Rawls seems to be arguing that no other primary goods matter without first securing the social bases of self‐respect. As Pilapil argues, it is not so much that maldistributions are morally objectionable because they violate the principle of equality, but rather because “they potentially undermine self‐respect (Pilapil, 2014, p. 290)”. This suggests that for Rawls, self‐respect is a condition of the possibility for an agent to form its higher‐order interests in actualizing both moral powers (the rational and the reasonable).​To test the plausibility of this idea it is useful to take a closer look at the social bases of self‐respect. According to Rawls, self‐respect is “rooted in our self‐confidence as a fully cooperating member of society capable of pursuing a worthwhile conception of the good over a complete life (Rawls, 2005, pp. 318)”. Moreover, self‐respect depends on the ability of agents to fully exercise their two moral powers. This is the foundation that allows agents to confidently pursue life goals as valuable and worthwhile (ibid, pp. 318). Rawls further explains the social bases for self‐respect in the following:​Certain public features of basic social institutions, how they work together and how people accept these arrangements are expected to (and normally do) regard and treat one another. These features of basic institutions and publically expected (and normally honored) ways of conduct are the social bases of self‐respect (Rawls, 2005, p. 319).​This suggests that self‐respect is directly tied to public institutions. He further explains how self‐respect is composed of two elements. The first element is self‐ confidence as fully cooperating members of society through the actualization of the two moral powers. The second element is a secure sense of one’s own value, and ability to pursue a worthwhile life plan (ibid, p. 319).​He goes on to elaborate on how institutions might secure these elements of self‐respect:​Our sense of value, as well as our self‐confidence, depends on the respect and mutuality shown us by others. By publicly affirming the basic liberties citizens in a well‐ordered society express their mutual respect for one another as reasonable and trust‐worthy, as well as their recognition of the worth all citizens attach to their way of life (Rawls, 2005, p. 319).​Rawls is focusing on the public affirmation of certain liberties and rights. Accordingly, the types of institutions he is prescribing are those of the public realm. Rawls appears to be claiming that the affirmation of public institutions, which ensure a fair distribution and equality of rights, are sufficient for the subjective development of self‐respect.​When we consider Honneth’s three recognitional spheres Rawls appears only able to address the public affirmations of recognition. This would be equivalent to Honneth’s formal‐legal sphere of recognition. Based on this it would be rather difficult to see how Rawls’ political conception of justice can address Honneth’s sphere of solidarity. If we want to hold to the idea that we can retrieve a richer idea of recognition from the social bases of self‐respect then we would have to interpret the above to mean that distribution and publicly affirmable institutions are sufficient for self‐respect, and thus sufficient to address all spheres of recognition. However, it would be rather odd to hold such a view. This follows from the idea that recognitional distortions based on solidarity are not clearly linked to public institutions. For example, it would be difficult to imagine how public institutions might directly alter cultural value patterns that contribute to the oppression of minority groups like implicit sexism or racism. That is not to say that Honneth’s approach can guarantee just cultural value patterns. Rather, Honneth can at least provide helpful theoretical vocabulary to criticize culture in a way that Rawls cannot.​While it seems impossible to retrieve language relevant to all spheres of recognition in Rawls, there are reasons to think they are at least presupposed even if they are not explicit. As we saw earlier, Rawls is primarily concerned with the basic structure of society based on fair terms of cooperation (Rawls, 2005, p. 16). For his conception of justice to be possible Rawls requires that citizens be “normal and fully cooperating members of society (ibid, p. 18)”. For Rawls, fully cooperating members of society require a conception of the good. However, without self‐respect agents may be unable to formulate such a conception of the good (Rawls, 2005, p. 318). This is why Rawls argues that​the parties [in the original position] give great weight to how well principles of justice support self‐respect, otherwise these principles cannot effectively advance the determinate conceptions of the good of those the parties represent (Rawls, 2005, p. 318‐9).​It seems clear that any factors that interfere with an agent’s ability to formulate a conception of the good should be addressed by a theory of justice. As Miriam Bankovsky argues, “Rawls’ constructivist procedure aims – at least in intention – to guarantee the actual realization of” an agent’s two moral powers (2011, p. 105).​It seems reasonably obvious that recognitional distortions suffered from within all three spheres of recognition (not just from within the public‐legal sphere) can interfere with an agent’s ability to fully realize its moral powers3. Rawls’ theory must presuppose all spheres of recognition otherwise he cannot ensure the normal functioning of citizens required for social cooperation. This leads us to the central claim of this paper. Even though Rawls presupposes all levels of recognition, it is unclear how his theory can get us there – even if as Bankovsky argues, he intends to. Honneth’s theory can identify the recognitional presuppositions of justice in a way that Rawls cannot. Moreover, Honneth can give us everything Rawls can since social cooperation seems to necessarily follow from the three spheres of recognition. Even if both theorists are aiming at the same outcome, it is only Honneth, as a consequence of his ontology, that can achieve this aim. Rawls seems to explicitly emphasize public institutions and formal equality as a way of ensuring self‐respect. Such public institutions are an important aspect of recognition, though not sufficient. For the aim of Rawls’ theory to hold he must presuppose all forms of recognition. Honneth, on the other hand, offers a full account of such injustices.​Rawls’ upholding of a conception of the person that avoids metaphysics leads him to frame justice as abstract social cooperation between citizens and their two moral powers. What is missing from this view is the ontological justification for why developing an agent’s two moral powers is worthwhile. The only way to get to such a justification is by offering some ontological grounding for justice. Contrary to Rawls’ worry that asserting metaphysical claims might lead to sectarianism, Honneth claims that “human needs and characteristics are not obviously incompatible with a commitment to inclusive, universalistic forms of liberalism (Anderson & Honneth, 2005, p. 143)”. Honneth’s ontological grounding leads to a more complete account of justice and can fill in the gaps of what Rawls would have to presuppose in justice as fairness because it has a thin metaphysics of the person.​Honneth and Cultural Criticism​In this concluding section I will argue that Honneth’s ontology allows us to consider solutions to recognitional distortions not related to the public legal‐formal sphere of recognition. First I will argue that neither public institutions nor legislation can wholly address recognitional distortions connected to cultural value patterns like systematic racism and sexism. Secondly, I will offer examples of how cultural criticism might help to address aforementioned recognitional distortions in a way that is not dependent on public institutions or legislation.​The problem with injustice from the perspective of recognition is that it occurs at all levels of recognitional experience. The social meanings and stigmas in the public and private realm have deep political implications and thus, implications for justice. We cannot legislate‐away oppressive attitudes that occur at the cultural level (equivalent to Honneth’s sphere of solidarity) even if they lead to recognitional distortions. While it’s true we can enact laws regarding hate speech or ensure formal equality to marginalized groups, it seems entirely another thing to change the underlying value patterns of society. For example, just because women gained formal equality under the law does not mean that systematic sexism no longer exists. There is a way in which cultural value patterns and formal law are two different things. To see why the idea of legislating value patterns is so problematic we can consider the analogous case of language. In language we have formal rules for syntax and grammar. Yet such ideal correctness is rarely if ever realized at the lived level of spoken word, where uses of language take on spontaneous and unexpected forms. Similarly, in the case of cultural value patterns there is a disconnect between the formal laws of society and the lived experience of cultural value patterns. Just like pedants can never successfully contain language, legislators can never contain cultural value patterns.​The question then arises, if we cannot fully address potentially harmful societal value patterns by way of legislation, how are we to change them to the extent that we can minimize recognitional distortions? The answer amounts essentially to the practice of cultural criticism. As Geraldine Finn rightfully points out,​our struggles against the political hierarchies... against racism and sexism, for example, is one that is waged within and against ourselves and our own “subjectivity” and experience, as much as within and against the “objectivity” of the institutions that govern us (Finn, 1990, p. 142).​I take this to mean that social struggles occur in everyday experience as much as with policy makers – although the latter only emerges as a consequence of the former struggles. Cultural criticism is a form of calling implicit and explicit cultural value patterns into question. Such questioning should happen at all levels of lived experience and we should prefer political theories that bolster this effort. Honneth’s approach allows us to do this because we can ground criticisms in relevant recognitional spheres. To see how this is the case I will consider some concrete examples.​While it is clearly true that women today enjoy all the same formal rights as men, many point out that there remain several cultural inequalities. One expression of this is so‐called “slut shaming”. Slut shaming is the practice of putting women down (in various ways) for their sexual promiscuity. This is the result of cultural value patterns that treat women according to different standards than men. Accordingly, a woman’s sexual autonomy is culturally limited in a way that a man’s is not. The issue of recognitional distortion enters the discussion when we consider kinds of damage that can be done to women’s self‐esteem who make choices contrary to the dominant cultural value patterns. For example, in such cases we might see a loss of self‐worth, social marginalization, or even alienation from family. Cultural value patterns can cause harms that are intuitively at odds with the idea of a just society.​As argued earlier, there are good reasons to think Rawls is aiming at the same kind of just society as Honneth. The point is not to question the intent of Rawls but rather the theoretical tools he gives us. Indeed, because Rawls’ ontology is so abstract he is ill equipped to handle injustice that falls outside of the formal‐legal sphere of justice and has limited himself to language about regulating public institutions that ensure an agent’s higher‐order interests. He therefore seems unhelpful when it comes to many types of cultural criticism.​Let’s consider the recent (and particularly heartbreaking) case of Leelah (born Josh) Aclorn. Alcorn biologically born a male but went on to identify as female before committing suicide at age 17. Alcorn’s suicide note offers valuable insight into the kind of recognitional damage that can be done at the solidarity and love spheres of recognition.​When I was 14, I learned what transgender meant and cried of happiness. After 10 years of confusion I finally understood who I was. I immediately told my mom, and she reacted extremely negatively, telling me that it was a phase, that I would never truly be a girl, that God doesn’t make mistakes, that I am wrong. If you are reading this, parents, please don’t tell this to your kids. Even if you are Christian or are against transgender people don’t ever say that to someone, especially your kid. That won’t do anything but make them hate them self. That’s exactly what it did to me (Malm, 2014).​Alcorn goes on to explain her feelings of hopelessness at the prospects of getting help since going to church, her supposed support system, would be to surround herself with people who are “against everything [she] lives for”. Her note ends with an emotional plea: “the only way I will rest in peace is if one day transgender people aren’t treated the way I was, they’re treated like humans, with valid feelings and human rights... fix society. Please”. It is clear from this that Alcorn endured recognitional distortions from value patterns related to her particular social context (Conservative Christianity). This led her family to misrecognize her identity since they them‐selves were so rooted in Conservative Christian cultural value patterns.​It is Alcorn’s plea to “fix society” that seems particularly salient here. How are we to do this from the perspective of a theory that limits its language to formal‐legal principles? It is true that we have, and can continue to enact anti‐discrimination laws to protect transgender people. Nevertheless, these possible solutions are almost irrelevant when compared to the effects caused by discriminatory cultural value patterns. Even if we have laws to protect such people, the remaining cultural value patterns may not (and often do not) change to reflect the law. Doubtless, enacting new laws can have important effects on cultural value patterns, however there are still missing pieces. Some of this missing element is cultural criticism at all levels of experience. Laws alone do not change cultural consciousness. Real social change happens at the lived level of experiences of injustice. Thus we should have a theory of justice that provides us with a vocabulary to describe this lived level of injustice. With Honneth’s spheres of recognition we can mount a cultural critique that grounds itself in a phenomenology of harm. With Rawls it is less clear what tools we would have to criticize the culture other than to consider a comprehensive doctrine to be unreasonable. While it is true that Rawls considers the social bases of self‐respect to be the most important primary good of all, his language limits itself to public institutions. Clearly we need more than that to address subjective wellbeing; something I have argued is clearly relevant to justice.​​ "
"[7월 신규 상품] ""AI Service"" - 네이버의 인공지능 플랫폼 '클로바(Clova)'와 통번역 기술 '파파고(Papago)의 API 제공! ",https://blog.naver.com/n_cloudplatform/221040483365,20170630,"feat. Clova, Papago​ AI Service가 여러분의 서비스를 한 걸음 앞선 비즈니스로 만들어드립니다.​궁금한 걸 물어보면 바로 찾아 대답해주는 스마트한 비서 애플리케이션부터수년의 외국어 공부를 무색하게 만들 만큼 자연스러워진 통번역 기술까지,하루가 지나기 무섭게 IT 기술은 지금도 진화하고 있습니다. ​나의 서비스 혹은 나의 비즈니스가 뒤쳐지지 않으려면지속적으로 공부하고, 새로 나온 좋은 것들이 있다면 적용해야 합니다.​방대한 양의 데이터를 생성하며 빠른 속도로 변화하는 현대에서 좀 더 똑똑하고 발빠른 서비스를 손쉽게 만들고 싶다면네이버 클라우드 플랫폼에서 제공하는 AI Service에 주목하세요!​네이버에 축적된 빅 데이터로 매일매일 똑똑해지는'클로바(Clova)'와 '파파고(Papago)' 의 API가여러분의 온라인 서비스를 더욱 똑부러지게 만들어드립니다.​​ ​​​ 비서 앱 서비스, 음성 안내 시스템을 만들고 싶다면? - Clova Speech Recognition(CSR) : 인식한 음성 내용을 텍스트로 변환- Clova Speech Synthesis(CSS) : 텍스트를 자연스러운 목소리로 재생스마트 폰으로 비서 애플리케이션 서비스를 이용할 때 내가 질문한 내용이 폰 화면에 텍스트로 표시되고,비서 애플리케이션이 필요한 정보를 찾으면 명료하고 상냥한 목소리로 읽어주는 것!​비서 애플리케이션의 기본이 되는 음성 인식과 음성 합성 기술인데요.​국내 최고의 음성 인식, 음성 합성 기술이라고 할 수 있는 '클로바(Clova)'의 API를 네이버 클라우드 플랫폼을 통해 사용할 수 있습니다. 네이버-LINE의 인공지능 플랫폼 기술이 적용된 비서 애플리케이션 '클로바(Clova)' ​'클로바(Clova)'는 네이버와 LINE에서 수년 간 공동 연구해온 인공지능 플랫폼으로,네이버에 쌓여가고 있는 방대한 양의 데이터를 통해 매시간 진화 중인데요.현재 국내에서 가장 뛰어난 한국어 음성 인식률과 합성률을 자랑합니다.​한국어, 영어, 일본어, 중국어까지 4개 언어에 대한 음성 인식 및 합성이 가능해서해외 이용자를 대상으로 한 서비스에도 활용 가능합니다.​ ​​​​​ 얼굴 인식 애플리케이션을 만들고 싶다면? Clova Face Recognition(CFR) : 얼굴을 감지하고 인식하여 얻은 정보 제공 다운 받은 얼굴 인식 애플리케이션으로 친구들과 모여서서로 닮은 연예인을 찾으며 깔깔 웃던 경험, 다들 있으시죠?​ 이미지 속 얼굴과 가장 닮은 유명인의 이름과 일치율을 제공하는 얼굴 인식 API도 네이버 클라우드 플랫폼에서 만나볼 수 있습니다. ​이 상품 역시 네이버에 있는 방대한 데이터를 통해 지속적인 학습을 하기 때문에 얼굴에 관련된 정보를 어제보다 오늘 더 풍부하게 제공한다고 말씀드릴 수 있습니다.​유명인 얼굴 인식 기능 외에도 얼굴의 윤곽과 눈/코/입의 위치, 표정 값을 제공하는 얼굴 감지 기능도 있으니, 얼굴에 대한 더욱 다양한 정보를 활용해보세요.​ ​​​​​ 자연스러운 기계 번역 서비스를 만들고 싶다면? Papago SMT : 대규모 학습 데이터에 기반한 언어 통번역SMT(Statistical Machine Translation)는 통계 기반 기계 번역 기술로,문장을 단어나 구문 단위로 쪼개서 통계상 가장 자연스럽다고 판단되는 번역 결과를 제시합니다.​Papago SMT는 단순 통계에만 의존하는 기존 기술의 한계를 보완하기 위해 네이버를 통해 축적해온 번역 품질에 대한 피드백들을 적극적으로 반영해왔습니다.​현재 네이버의 연예 뉴스 번역, V live 자막 번역 등 다양한 서비스에 활용되고 있는 Papago SMT는 한국어, 영어, 중국어, 일본어까지 4개 국어에 대한 번역 기능을 제공하기 때문에 서비스 영역을 넓힐 수 있습니다.​곧이어, 실제 서비스 되고 있는 Papago와 동일한 수준으로 더욱 더 자연스러운 통번역 결과를 제공하는 인공신경망 기반 기계 번역(NMT) API도 출시할 예정입니다. ​많은 관심 부탁드려요 :-)​ ​​​​​ 로마자 표기법에 맞는 영문 이름이 궁금하다면? Papago Korean Name Romanizer : 한글 이름을 표기법에 맞춰 로마자 이름으로 변환​​여권이나 신용카드를 발급할 때, 혹은 해외 서류 등 중요한 자료를 작성할 때에로마자 표기법에 맞는 나의 영문 이름이 이게 정확한지 헷갈려 공연스레 긴장할 때가 있습니다.​이처럼, 현행 로마자 표기법(문화관광부 고시 2000-8호)에 맞는로마자 이름을 궁금해 하는 고객이 많은 서비스(ex. 금융권, 관광 분야 등)라면 이 상품에 주목하세요! ​입력한 한글 이름을 현행 로마자 표기법(문화관광부 고시 2000-8호)에 맞는 표기로도 변환해주고, 사용 빈도가 높은 표기로도 변환해줍니다.​ ​  AI Service의 API 상품을 이용하면서 궁금한 부분이 있거나 어려운 점이 있다면 24시간 365일 열려 있는 고객지원 (1544-5876)으로 언제든지 연락주세요.​여러분의 불편사항을 정확하고 친절하게 해결해드리겠습니다!감사합니다.  [고객 사례] 1등 반려동물 앱 '아지냥이', 세대공감 스토리 '인생락서'도 네이버 클라우드 플랫폼!​국내 이용자 수 1위 반려동물 앱 '아지냥이'도,중장년 세대공감 글쓰기 서비스 '인생락서...blog.naver.com [고객사례] ""타사의 음성 인식 API보다 옵션이 다양하고 속도도 빨라서 만족스럽습니다."" - 클로바 음성 인식 기술을 이용한 앱 개발 사례　추후엔 네이버 클라우드 플랫폼의 얼굴 인식 API인 Clova Face Recognition(CFR) 상품을 사용해...blog.naver.com ​​ "
<헤어질 결심>에서 슈타이얼까지?: '자동화 시대의 예술과 매체' 세미나 티저2 ,https://blog.naver.com/ahjabie/222801849496,20220706,"안녕하세요 곽영빈입니다.​지난주, 제 #디지털 마음의 고향이라할 트위터에서 다음의 트윗을 보신 분들이 저 말고도 계시지 않을까 싶습니다.​ ​아시겠지만 박찬욱 감독의 <헤어질 결심> 관련 트윗이죠. (kt VIP 포인트를 쓰긴 했지만) 극장에서, 제 의지로, 두 번 연속 관람한 영화는 최근 몇 년 간 처음이 아닐까 싶습니다. (오늘 개봉하는 <토르>에 밀려 곧 사라지질 것 같으니 빨리 보시는 게 좋을 듯 해요)​혹시나 싶어 한글과 몇몇 외국어로 검색해봤지만 다행히(?) 좋은 글들이 #아직은 안 나와서 글을 쓰고 있긴한데, 계속 불어나 걱정입니다만, 하여간 제 나름대로 <헤어질 소설가의 기생충>이라 이름 붙인 전 이 트윗이 참 흥미롭더라구요.​해서 지난 해부터 꾸준히 관심을 두고 주변의 몇몇 분들과 실험/고민/연구 중인 몇몇 AI, 특히 이미지 생성기들에 이와 연관된 단어들을 혼자서 prompt해 보았는데요, 다음은 그 중 몇몇 결과물들입니다. ​  ​흥미롭죠? ^^​그럼 지금 국현에서 열리고 있는 슈타이얼 전시에서 아래 작업 보신 분들은 계실까요? ​https://www.youtube.com/watch?v=AYQB4_riCCg ​제가 찍은 게 있지만 혹시나 싶어 유튜브에서 가져온 <이것이 미래다 This is the Future>라는 제목의 이 작업은, 주어진 (이미지) 데이터들의 0.04초 후의 변이/진화 양태를 예측해 보여주는, AI의 핵심으로 간주되기도 하는 '인공신경망(ANN: Artificial Neural Network)'을 이용한 작업입니다. ​대략 파악하셨겠지만, 위의 사례들은 편차가 있지만 근원적으로는 같은 중핵을 공유하는 사례들이죠.​여기서, 내일부터 시작하는 세미나 교재로 선정한 텍스트의 저자인 안드레예비치가 환기하는 레이 커즈와일 Ray Kurzweil처럼 '인간과 기계의 의식이 융합될 특이점 Singularity'까지 살아남아 자신과 아버지를 AI로 환생/부활(!)시킬 '꿈을 꾸는' 건 황당해보일 지언정 그리 어려운 일이 아닐지도 모릅니다. ​(이렇게만 쓰면 소위 '허경영'류로 보일지도 모르나(ㅎ), 그는 우리가 이제 일상적으로 쓰는 OCR(Optical Character Recognition)부터 TTS(Text-To-Speech), 음성인식기술(Speech Recognition Technology) 개발에 핵심적으로 기여한 인물중 하나이기도 합니다)​내일(7일)부터 제 오프닝 렉처로 시작될 '2022년 여름의 미디어철학 세미나- 자동화 시대의 예술과 매체'는 바로 이러한 사례들이 만들어내는 별자리가 무엇을 뜻하는지를 다룰 예정입니다. 물론 이들을 단지 '이론적으로 해석하겠다'는 건 아닙니다. 이 둘은 예술작업이기도 하지만, 동시에 좁은 의미의 예술계를 넘어서는 것이기도 하고, 더 넓게는 '실시간으로 재규정되고 있는 이 세상 자체'를 드러내는 것이기도 하니까요. (어떤 의미에선 이런 함의를 갖지 않는 예술이란 예술이 아니라고도 할 수 있겠구요)​제가 지속적으로 제기해온 질문을 다시 드리면: ​이들은 우리의 최신식 '도구(tool/instrument)'일까요? 아니면 제가 항상 환기하는 근원적인 의미에서, 즉 '우리를 어디로 데려다 놓을지 알 수 없다'는 의미에서 전자와 구분되는 '미디어(media)'일까요? ​(사실 이와 연관된 책과 작업들이 있기도하고, 지속적인 요청으로 관련 프로젝트 하나를 더 런칭할 계획중이기도 합니다만) 이러한 질문들에 관심이 있으신 분들이라면 이번 세미나 역시 놓치지 않으시길 바라고, 이번에 사정상 함께 하시지 못하는 분들 또한 찬찬히 곱씹어 보시거나 전시 들러보시길 권유드리며~  :)​* ‘2022년 여름의 미디어철학과 동시대이론 세미나’ 공지 (7월 4일/7일 개강)강좌상세공지: https://blog.naver.com/ahjabie/222791049201* 구글신청폼: https://forms.gle/uwTUBRvkCqgKAZiK6​YBp.s. 계속 질문 주셔서 다시 덧붙이자면, 두 세미나 모두 첫 주는 저의 '오프닝 렉처' 위주로 이뤄지는 만큼, 실질적 신청은 두번째 세션 전까지 가능합니다.  "
"카카오, 카카오브레인·카카오엔터프라이즈 통해 올해 글로벌 25개 학회서 총 40건 AI 논문 등재 ",https://blog.naver.com/tearhunter/222607373815,20211229,"​카카오가 올 한해 다수의 글로벌 유명 학회에서 AI 기술 관련 논문을 등재하며 AI 분야 기술력을 입증했다고 29일 밝혔다.​카카오의 AI 기술 기업인 카카오브레인은 28일, 글로벌 10개 학회에 총 15건의 논문을 등재했다고 밝혔다. 앞서 카카오 엔터프라이즈 역시 올 한해동안 25건의 논문을 등재했다고 밝힌바 있다. 이로써 카카오는 2021년, 총 25개 학회에 40건의 AI 논문을 등재하며 글로벌 최고 수준의 AI 연구 역량을 인정받았다.​카카오브레인은 올 상반기에 의료, 자연과학 등 다양한 분야에 활용 가능한 기술에서 인상적인 성과를 보였다.​3월 메디컬이미지애널리시스(Medical Image Analysis) 저널에서 열린 LNDb 경쟁부문에 참가해 논문 1건을 발표하고 종합 1위를 수상하는 쾌거를 거뒀다. 폐 결절을 진단할 수 있는 자동화된 인공지능 모델을 제안하는 내용이었다. ​자연과학 저널 '과학통보(Science Bulletin)'에는 서울대학교, 전남대학교와 공동 연구한 논문을 1건 발표했다. 페루와 칠레 앞바다에 일어나는 해수 온난화 현상을 일컫는 ‘엘니뇨 현상’은 인도양과 적도 태평양 사이의 기압대 변화와 관계가 있다고 알려져 있는데, 이러한 기압 진동 현상을 사전 예측할 수 있는 모델을 개발해 발표했다.​6월에는 세계적 권위의 학술 대회 'CVPR 2021'에서는 2건의 논문을 등재하며 상위 4%에게 제공되는 구두 발표의 영예를 얻기도 했다.​ ​카카오브레인은 하반기에도 활발한 AI 연구 성과를 이어갔다. 지난 7월 대표적인 국제 AI 학술대회인 'ICML'에 논문 2건을 등재했다. 1건은 이미지와 언어를 동시에 사전 학습하는 새로운 방법을 제안한 논문으로 기존 모델 대비 최대 60배 효율적인 성능을 검증했다. 다른 1건은 다양한 배치(한번에 처리 가능한 데이터 크기) 조건에 잘 적용되는 자동화된 러닝 레이트 스케쥴러(딥러닝 모델 학습률)를 설명한 논문이었다.​11월에는 자연어처리 분야 세계 최고 학회 EMNLP에서 한양대와 협업한 논문 1건을 발표했다. 다양한 언어 문장 사이 유사도를 잘 측정할 수 있는 '다국어 문장 임베딩 모델 학습' 방법을 제안하는 내용이다.​국제 컴퓨터 비전학회 'ICCV' 챌린지에서는 비디오 도메인 관련 품질 비교 과제에서 우승했고 인공지능과 기계학습 분야 국제학회 '뉴립스(NeurIPS)' 챌린지에서는 컴퓨팅 능력으로 준우승을 차지하기도 했다.​ ​카카오엔터프라이즈는 자연어처리 분야와 컴퓨터 비전 영역에서 두각을 나타내며 15개 학회에 총 25개 논문을 등재했다. 특히 자연어처리 분야 세계 최고 학회 'EMNLP'와 'EMNLP'가 주최한 워크샵 2곳을 비롯해 컴퓨터 과학 분야 최고 수준 학회 중 하나인 국제 컴퓨터 비전학회 'ICCV'에 논문을 발표했다. 인공지능과 기계학습 분야 국제학회 '뉴립스(NeurIPS)'에서는 컴퓨터 비전 분야의 데이터 편향성 문제를 개선한 방법론 연구를 인정 받아 연구 상위 1%에게 제공되는 구두 발표의 영예를 얻었다.​지난 10월 글로벌 AI 자연어이해 경진대회 'MS MARCO'에서는 'Passage Ranking' 분야 1위를 차지했다. CCV-MFR·FRVT 등 컴퓨터 비전 관련 챌린지에서도 각각 2위, 4위를 달성한 바 있다.​카카오 측은 ""지난해 26건의 논문을 등재한데 이어, 올해에도 글로벌 수준의 AI 기술력을 인정받은 것""이라며 ""초거대 AI 모델 등 AI 기술을 외부에 공개해 다양한 영역 기업들의 디지털 전환을 돕고, 헬스케어-교육 영역 등의 문제를 해결해 나갈 것""이라고 전했다.   ※ 카카오 세부 논문 리스트 ​ㅇ 카카오엔터프라이즈 : 15개 학회 25건 논문 채택[상반기] 총 9개 학회 총 16건 논문 등재 및 챌린지 입상​1. INTERSPEECH (2021.8) : 음성 처리 과학기술 분야 세계 최대 규모 학술대회​(1) UnivNet: A Neural Vocoder with Multi-Resolution Spectrogram Discriminators for High-Fidelity Waveform Generationhttps://kakaoenterprise.github.io/papers/interspeech2021-univnet UnivNet: A Neural Vocoder with Multi-Resolution Spectrogram Discriminators for High-Fidelity Waveform GenerationAbstractkakaoenterprise.github.io - 저자: 장원(카카오엔터프라이즈), 임단(카카오), 윤재삼(카카오엔터프라이즈), 김봉완(카카오엔터프라이즈), 김준태(카카오엔터프라이즈)​(2) Auxiliary Sequence Labeling Tasks for Disfluency Detectionhttps://kakaoenterprise.github.io/papers/interspeech2021-sl-tasks-for-disfluency-detection Auxiliary Sequence Labeling Tasks for Disfluency DetectionAbstractkakaoenterprise.github.io - 저자: 이동엽(카카오), 고병일(카카오엔터프라이즈), 신명철(카카오엔터프라이즈), 황태선(와이즈넛), 조재춘(한신대학교)​(3) SE-Conformer: Time-Domain Speech Enhancement using Conformerhttps://kakaoenterprise.github.io/papers/interspeech2021-se-conformer SE-Conformer: Time-Domain Speech Enhancement using ConformerAbstractkakaoenterprise.github.io - 저자: 김의성(카카오엔터프라이즈), 서혜지(카카오엔터프라이즈)​2. SIGKDD Oral (2021.8) : 데이터마이닝 및 인공지능 분야 최고 학술대회​(1) Learning to Walk across Time for Interpretable Temporal Knowledge Graph Completionhttps://kakaoenterprise.github.io/papers/sigkdd-t-gap Learning to Walk across Time for Interpretable Temporal Knowledge Graph CompletionAbstractkakaoenterprise.github.io - 저자: 정재훈(카카오엔터프라이즈), 정진홍(전북대학교), 강유(서울대학교)​3. ACL-IJCNLP (2021.8) : 세계적인 자연어처리 학회​(1) Deep Context- and Relation-Aware Learning for Aspect-based Sentiment Analysishttps://kakaoenterprise.github.io/papers/acl-ijcnlp2021-dcran Deep Context- and Relation-Aware Learning for Aspect-based Sentiment AnalysisAbstractkakaoenterprise.github.io - 저자: 오신혁(넷마블), 이동엽(카카오), 황태선(고려대학교), 박일남(카카오엔터프라이즈), 서가은(카카오엔터프라이즈), 김응균(카카오엔터프라이즈), 김학수(건국대학교)​(2) OutFlip: Generating Examples for Unknown Intent Detection with Natural Language Attackhttps://kakaoenterprise.github.io/papers/acl-ijcnlp2021-outflip OutFlip: Generating Examples for Unknown Intent Detection with Natural Language AttackAbstractkakaoenterprise.github.io - 저자: 최동현(카카오엔터프라이즈), 신명철(카카오엔터프라이즈), 김응균(카카오엔터프라이즈), 신동렬(성균관대학교)​4. ICML (2021.7) : 세계적인 인공지능 컨퍼런스 머신러닝국제학회​(1) ViLT: Vision-and-Language Transformer Without Convolution or Region Supervisionhttps://kakaoenterprise.github.io/papers/icml2021-vilt ViLT: Vision-and-Language Transformer Without Convolution or Region SupervisionAbstractkakaoenterprise.github.io - 저자: 김원재(카카오), 손보경(카카오엔터프라이즈), 김일두(카카오브레인)​(2) Conditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-Speechhttps://kakaoenterprise.github.io/papers/icml2021-e2e-tts Conditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-SpeechAbstractkakaoenterprise.github.io - 저자: 김재현(카카오엔터프라이즈), 공정일(카카오엔터프라이즈), 손주희(카카오엔터프라이즈/카이스트)​5. IEEE Access (2021.6) : AI 분야 권위있는 국제 학술잡지​(1) Korean Erroneous Sentence Classification with Integrated Eojeol Embeddinghttps://kakaoenterprise.github.io/papers/ieeeaccess2021-iee Korean Erroneous Sentence Classification with Integrated Eojeol EmbeddingAbstractkakaoenterprise.github.io - 저자: 최동현(카카오엔터프라이즈), 박일남(카카오엔터프라이즈), 신명철(카카오엔터프라이즈), 김응균(카카오엔터프라이즈), 신동렬(성균관대학교)​(2) Suppressing Spoof-irrelevant Factors for Domain-agnostic Face Anti-spoofinghttps://kakaoenterprise.github.io/papers/ieeeaccess2021-dasn Suppressing Spoof-irrelevant Factors for Domain-agnostic Face Anti-spoofingAbstractkakaoenterprise.github.io - 저자: 김태욱(카카오엔터프라이즈), 김용현(카카오엔터프라이즈)​6. ICASSP (2021.6) : 국제 음향·음성·신호처리 학회​(1) U-Convolution Based Residual Echo Suppression With Multiple Encodershttps://kakaoenterprise.github.io/papers/icassp2021-u-convolution-based-residual-eco-suppression U-Convolution Based Residual Echo Suppression With Multiple EncodersAbstractkakaoenterprise.github.io - 저자: 김의성(카카오엔터프라이즈), 전재진(카카오엔터프라이즈), 서혜지(카카오엔터프라이즈)​(2) Multitask Learning and Joint Optimization For Transformer-Rnn-Tranducer Speech Recognitionhttps://kakaoenterprise.github.io/papers/icassp2021-transformer-rnn-tranducer-speech-recognition Multitask Learning and Joint Optimization For Transformer-Rnn-Tranducer Speech RecognitionAbstractkakaoenterprise.github.io - 저자: 전재진(카카오엔터프라이즈), 김의성(카카오엔터프라이즈)​7. Computational Linguistics (2021.3) : 컴퓨터언어학 및 NLP 분야 학술잡지​(1) RYANSQL: Recursively Applying Sketch-based Slot Fillings for Complex Text-to-SQL in Cross-Domain Databaseshttps://kakaoenterprise.github.io/papers/cljournal2021-ryansql RYANSQL: Recursively Applying Sketch-based Slot Fillings for Complex Text-to-SQL in Cross-Domain Databases중첩된 SELECT문을 좀 더 정확하게 생성하는 SPC 기법을 적용한 Text-to-SQL 알고리즘 ‘RYANSQL’ 제안kakaoenterprise.github.io - 저자: 최동현(카카오엔터프라이즈), 신명철(카카오엔터프라이즈), 김응균(카카오엔터프라이즈), 신동렬(성균관대학교)​8. IEEE Transactions on Neural Networks and Learning Systems (2021.2) : AI 분야 권위있는 국제 학술잡지​(1) A Plug-in Method for Representation Factorization in Connectionist Modelshttps://kakaoenterprise.github.io/papers/ieee2020-fden https://github.com/wltjr1007/Factors-Decomposer-Entangler-Network- 저자: 윤재석(고려대학교), 노명철(카카오엔터프라이즈), 석흥일(고려대학교)​9. AAAI (2021.2) : 세계 최고 수준의 국제인공지능학회(1) Do Response Selection Models Really Know What's Next? Utterance Manipulation Strategies for Multi-turn Response Selectionhttps://kakaoenterprise.github.io/papers/aaai2021-multi-turn-response-selection Do Response Selection Models Really Know What’s Next? Utterance Manipulation Strategies for Multi-turn Response Selection응답 선택에서 대화 맥락에 호응하면서도 의미적 유사도가 높은 문장을 선택하는 기법 ‘UMS’ 제안kakaoenterprise.github.io - 저자: 황태선(고려대학교), 이동엽(카카오), 오동석(고려대학교), 이찬희(고려대학교), 한기종(카카오엔터프라이즈), 이동훈(카카오엔터프라이즈), 이새벽(와이즈넛/고려대학교)​(2) Multi-level Distance Regularization for Deep Metric Learninghttps://kakaoenterprise.github.io/papers/aaai2021-mdr Multi-level Distance Regularization for Deep Metric Learning딥러닝 기반 거리 학습에 적합한 새로운 정규화 기법 ‘MDR’ 제안kakaoenterprise.github.io - 저자: 김용현(카카오엔터프라이즈), 박원표(카카오)​(챌린지) CVPR-NAS 2021 (2021.6) : 컴퓨터 비전 분야 top-tier 학회 CVPR의 neural architecture search 대회- Unseen Data Track에서 3위 차지- 이흥창(카카오엔터프라이즈), 김도국(카카오엔터프라이즈)- NAS 알고리즘이 처음 보는 (unseen) 여러 데이터셋들에 대해 얼마나 높은 성능을 낼 수 있는지에 대한 track에서, 다양한 데이터셋들에 대해서 잘 동작할 수 있는 NAS 알고리즘을 제안​[하반기] 총 7개 학회 및 저널 총 9건 논문 등재, 총 5건 챌린지 입상​1. NeurIPS (2021.12) : 세계 최고 수준의 AI 학회​(1) Learning Debiased Representation via Disentangled Feature Augmentation (Oral)https://kakaoenterprise.github.io/papers/neurips-learning-debiased-representation Learning Debiased Representation via Disentangled Feature AugmentationAbstractkakaoenterprise.github.io - 저자: 이정수(카카오엔터프라이즈/카이스트), 김응엽(카카오엔터프라이즈/카이스트), 이주영(카카오엔터프라이즈), 이지현(카이스트), 주재걸(카이스트)​(2) SmoothMix: Training Confidence-calibrated Smoothed Classifiers for Certified Robustnesshttps://kakaoenterprise.github.io/papers/neurips-smoothmix SmoothMix: Training Confidence-calibrated Smoothed Classifiers for Certified RobustnessAbstractkakaoenterprise.github.io - 저자: 정종현(카이스트), 박세준(카이스트), 김민규(카이스트), 이흥창(카카오엔터프라이즈), 김도국(카카오엔터프라이즈), 신진우(카이스트)​2. WMT21 (2021.11) : EMNLP 워크샵 학회​(1)Kakao Enterprise’s WMT21 Machine Translation using Terminologies Task Submissionhttps://kakaoenterprise.github.io/papers/wmt21-terminology-translation Kakao Enterprise’s WMT21 Machine Translation using Terminologies Task SubmissionAbstractkakaoenterprise.github.io - 저자: 박윤주(카카오엔터프라이즈), 선지민(카카오엔터프라이즈/카네기멜론대), 김현재(카카오엔터프라이즈), 류성원(카카오엔터프라이즈), 이창민(카카오엔터프라이즈)​3. NewSum (2021.11) : EMNLP 워크샵 학회​(1) Capturing Speaker Incorrectness: Speaker-Focused Post-Correction for Abstractive Dialogue Summarizationhttps://kakaoenterprise.github.io/papers/newsum-csi Capturing Speaker Incorrectness: Speaker-Focused Post-Correction for Abstractive Dialogue SummarizationAbstractkakaoenterprise.github.io - 저자: 이동엽(카카오), 임정우(고려대), 황태선(와이즈넛), 이찬희(고려대), 조승우(카카오엔터프라이즈), 박민건(마이크로소프트), 임희석(고려대)​4. EMNLP (2021.11) : NLP 분야 세계 최고 학회​(1) An Evaluation Dataset and Strategy for Building Robust Multi-turn Response Selection Modelhttps://kakaoenterprise.github.io/papers/emnlp-evaluation-dataset-and-strategy An Evaluation Dataset and Strategy for Building Robust Multi-turn Response Selection ModelAbstractkakaoenterprise.github.io - 저자: 한기종(카카오엔터프라이즈), 이서진(SK텔레콤), 이우인(카카오엔터프라이즈), 이주성(카카오엔터프라이즈), 이동훈(카카오엔터프라이즈)​(2) AligNART: Non-autoregressive Neural Machine Translation by JointlyLearning to Estimate Alignment and Translatehttps://kakaoenterprise.github.io/papers/emnlp-alignart AligNART: Non-autoregressive Neural Machine Translation by Jointly Learning to Estimate Alignment and TranslateAbstractkakaoenterprise.github.io - 저자: 송종윤(카카오엔터프라이즈/서울대), 김성원(서울대), 윤성로(서울대)​5. ICCV (2021.10) : CVPR과 더불어 컴퓨터 비전 및 패턴 인식 분야에서 최고 권위를 가진 AI 학회​(1) Distilling Global and Local Logits with Densely Connected Relationshttps://kakaoenterprise.github.io/papers/iccv-distilling-global-and-local-logits Distilling Global and Local Logits with Densely Connected RelationsAbstractkakaoenterprise.github.io - 저자: 김유민(경희대/카카오엔터프라이즈), 박진배(경희대), 장윤호(경희대), Muhammad Ali(경희대), 오태현(포항공대), 배성호(경희대)​6. IEEE Signal Processing Letters (2021.10) : AI 분야 권위있는 국제 학술잡지​(1) Improving End-to-End Contextual Speech Recognition via a Word-Matching Algorithm with Backward Searchhttps://kakaoenterprise.github.io/papers/ieee-e2e-csr Improving End-to-End Contextual Speech Recognition via a Word-Matching Algorithm with Backward SearchAbstractkakaoenterprise.github.io - 저자: 김준태(카카오엔터프라이즈), 이윤한(카카오엔터프라이즈)​7. IEEE Access (2021.8) : AI 분야 권위있는 국제 학술잡지​(1) Adaptive Batch Scheduling for Open-Domain Question Answeringhttps://kakaoenterprise.github.io/papers/ieeeaccess-adaptive-batch-scheduling Adaptive Batch Scheduling for Open-Domain Question AnsweringAbstractkakaoenterprise.github.io - 저자: 최동현(카카오엔터프라이즈), 신명철(카카오엔터프라이즈), 김응균(카카오엔터프라이즈), 신동렬(성균관대학교)​(챌린지) ICCV21 Masked Face Recognition Challenge (2021.10) : 비전 분야 세계 최고 학회 ICCV 주최 대회- 2위 차지- 김태완(카카오엔터프라이즈)- 마스크를 착용한 얼굴과 착용하지 않은 얼굴에 대한 인식 성능을 합쳐 순위를 매기는 과제에서, 다양한 모델(resnet 100, 200, 240, 300 등)을 대상으로 한 실험과 지식 증류(knowledge distillation)기법을 활용해 성능 고도화​(챌린지) FRVT 1:1 Mugshot (2021.10) : 미국표준기술연구소(NIST) 주최 얼굴인식 대회 Mugshot 부문- 4위 차지- David 파트 (카카오엔터프라이즈)- 얼굴 정보 대조를 통한 출입국심사, 여권 불법 복제 탐지 처럼 민간∙사법∙국가 보안 영역에서 활용되는 자동화된 얼굴 인식 응용 프로그램의 성능을 측정하는 대회​(챌린지) DSTC (Dialog State Tracking Challenge)10 (2021.10) : AI 대화 시스템 분야의 대표적인 국제 경진대회- 2위차지- 이주성(카카오엔터프라이즈)- 멀티모달 대화 시스템 구축을 주제로 하는 SIMMC 2.0 분야(페이스북 주관)에서 2위를 차지함. 주어진 장면에서 AI 어시스턴스 대화를 통해 적절한 응답을 생성하는 챌린지로, 예를 들어 상점에서 의류를 구매하는 사용자에게 맞는 AI 어시스턴스를 만드는 과정을 평가함​(챌린지) MS MARCO Passage Ranking (2021.11) : 글로벌 AI 자연어이해 경진대회- 1위 차지- 최동현(카카오엔터프라이즈)- 주어진 질문에 대해서 수백만개의 구절 중 관련도가 높은 구절을 순위화하여 점수를 매기는 과제에서, 고도화된 검색 방법 활용과 검색 엔진 기술로 최상위 성능 구현​(챌린지) WMT21 Machine Translation using Terminologies Task (2021.11) : 자연어처리 분야 세계 최고 학회 'EMNLP'의 워크샵으로 진행된 국제 기계번역 대회 중 최다 참가(21개 시스템)를 기록한 English-French 부문- 공동 1위 차지- 박윤주(카카오엔터프라이즈), 김현재(카카오엔터프라이즈), 류성원(카카오엔터프라이즈), 이창민(카카오엔터프라이즈)- 새로운 도메인 혹은 특정 도메인의 용어 사전을 번역에 얼마나 더 효과적으로 반영하는지 평가하는 과제에서, 도메인의 일부 말뭉치와 용어 단위 번역만으로 효과적인 적응을 구현ㅇ 카카오브레인 : 10개 학회 15건 논문 채택1. AAAI 2021 (2021.2.2 - 9)​(1) Regularizing Attention Networks for Anomaly Detection in Visual Question Answeringhttps://kakaobrain.com/publication/142 카카오브레인Unthinkable question makes impactful answer.kakaobrain.com - 저자: 이도엽(POSTECH), 천영재(카카오브레인), 한욱신(POSTECH)- 요약: 텍스트와 이미지를 동시에 이해하고 답을 제시해야 하는 VQA 문제에서 정확도를 유지하면서도 동시에 이상검출 성능을 크게 개선하는 방법을 제안하였습니다.​(2) Image-to-Image Retrieval by Learning Similarity between Scene Graphshttps://kakaobrain.com/publication/141 카카오브레인Unthinkable question makes impactful answer.kakaobrain.com - 저자: 윤상웅(서울대학교), 강우영(카카오브레인), 전승욱(서울대학교), 이성은(서울대학교), 한창진(서울대학교), 박종헌(서울대학교), 김은솔(카카오브레인)요약: 콘텐츠 기반의 이미지 검색 문제를 그래프 뉴럴 네트워크를 이용하여 해결하는 방법을 제안했습니다.​(3) Visual Concept Reasoning Networkshttps://kakaobrain.com/publication/139 카카오브레인Unthinkable question makes impactful answer.kakaobrain.com - 저자: 김태섭(MILA), 김성웅(카카오브레인), 요슈아벤지오(MILA)- 요약: 단순한 패턴 인식을 넘어 인공지능 스스로 학습한 시각적인 개념들을 조합해 추론할 수 있는 모듈을 제안함으로써 다양한 영상 인식 문제에서 성능을 향상할 수 있음을 보여줬습니다.​2. AAAI 2021 Workshop on Deep Learning on Graphs: Method and Applications (2021.2.8)(1) Solving Cold Start Problem in Semi-Supervised Graph Learninghttps://kakaobrain.com/publication/152 카카오브레인Unthinkable question makes impactful answer.kakaobrain.com - 저자: 권일재(서울대학교), 온경운(카카오브레인), 이동근(서울대학교), 장병탁(서울대학교)- 요약: 본 논문은 그래프 내에서 semi-supervised learning을 할 때 cold-start 문제를 해결하고자 하는 것으로, multi-task learning strategy를 이용하여 이 문제를 접근하는 방법론에 대해 다룹니다.​3. IEEE Access (2021.3.2)​(1) A Body Part Embedding Model With Datasets for Measuring 2D Human Motion Similarityhttps://kakaobrain.com/publication/151 카카오브레인Unthinkable question makes impactful answer.kakaobrain.com - 저자 : 박종혁(서울대학교), 조석현(서울대학교), 김동우(카카오브레인), Oleksandr Bailo(카카오브레인), 박희웅(서울대학교), 홍상훈(카카오브레인), 박종헌(서울대학교)Journal- 요약 : 서로 다른 사람의 행동을 비교하고 유사도를 측정할 수 있는 딥러닝 모델을 연구하고, 이를 학습하기 위한 데이터셋을 공개하였습니다.​4. Medical Image Analysis (2021.03.05)​(1) LNDb Challenge on automatic lung cancer patient managementhttps://kakaobrain.com/publication/150 카카오브레인Unthinkable question makes impactful answer.kakaobrain.com - 저자: João Pedrosa(INESC TEC) 외, (카카오브레인 김일두 포함)- 요약: 폐암은 세계적으로 치사율 높은 암 질환으로 그 원인으로 늦은 진단이 꼽힙니다. 폐 결절을 진단할 수 있는 자동화된 인공지능 모델을 개발하면, 조기 진단이 가능해지고 지속적인 후속 조치가 가능해 폐암의 사망률을 크게 낮출 수 있을 것으로 기대합니다. 카카오브레인이 LNDb Competition에 참가하여 종합 1위를 수상한 방법론이 포함된 논문입니다.​5. Science Bulletin (2021.03.13)​(1) Unified Deep Learning Model for El Nino/Southern Oscillation Forecasts by Incorporating Seasonality in Climate Datahttps://kakaobrain.com/publication/149 카카오브레인Unthinkable question makes impactful answer.kakaobrain.com - 저자: 함유근(전남대학교), 김정환(전남대학교), 김은솔(카카오브레인), 온경운(서울대학교)- 요약: 페루와 칠레 앞바다에 일어나는 해수 온난화 현상을 일컫는 엘리뇨 현상은 인도양과 적도 태평양 사이의 기압대 변화와 관계가 있다고 알려져 있습니다. 이러한 기압 진동 현상을 엘리뇨 남방진동, 즉 ENSO 현상이라고 하는데, 이 현상을 예측할 수 있는CNN 기반 모델을 개발하여 논문으로 발표하였습니다.​6. CVPR 2021 (2021.6.19 - 25)​(1) HOTR: End-to-End Human-Object Interaction Detection with Transformershttps://kakaobrain.com/publication/146 카카오브레인Unthinkable question makes impactful answer.kakaobrain.com - 저자: 김범수(카카오브레인), 이준현(고려대학교), 강재우(고려대학교), 김은솔(카카오브레인), 김현우(고려대학교)- 요약: 이미지에서 사람과 물체 사이의 상호작용에 대한 내용을 자동으로 검출하는 HOI 문제를 효율적으로 풀기 위한 트랜스포머(Transformer) 기반의 알고리즘을 새롭게 제안하고, 정확도 뿐만 아니라 속도 측면에서도 최고 수준의 성능(State-of-the-art)을 달성했습니다.​(2) Spatially Consistent Representation Learninghttps://kakaobrain.com/publication/147 카카오브레인Unthinkable question makes impactful answer.kakaobrain.com - 저자: 노병석(카카오브레인), 신우현(카카오브레인), 김일두(카카오브레인), 김성웅(카카오브레인)요약: 이미지에 대한 레이블이 주어지지 않은 이미지를 스스로 학습하는 방법(Self-Supervised Representation Learning)의개선을 제안함으로써, 여러 물체가 등장하거나 물체의 위치 파악 등이 중요한 태스크에서 성능이 기존 최고 수준의 방법들 대비 눈에 띄게 높아지는 결과를 얻었습니다.​7. ICML (2021.07.18 - 24)​(1) ViLT: Vision-and-Language Transformer Without Convolution or Region Supervisionhttps://kakaobrain.com/publication/144 카카오브레인Unthinkable question makes impactful answer.kakaobrain.com - 저자 : 김원재(카카오), 손보경(카카오엔터프라이즈), 김일두(카카오브레인)Long Talk- 요약 : 이미지와 언어를 동시에 사용해야 하는 태스크(이미지 질문답변 등)를 위해서 이미지와 언어 쌍을 사전학습하는 더 나은 방법을 제안함으로써 기존의 모델 대비 최대 60배 효율적이면서도 경쟁력있는 성능을 보였습니다.​(2) Automated Learning Rate Scheduler for Large-batch Traininghttps://openreview.net/forum?id=ljIl7KCNYZH Automated Learning Rate Scheduler for Large-batch TrainingLarge-batch training has been essential in leveraging large-scale datasets and models in deep learning. While it is computationally beneficial to use large batch sizes, it often requires a...openreview.net - 저자 : 김치헌(카카오브레인), 김세훈(카카오브레인), 김종민(카카오브레인), 이동훈(카카오브레인), 김성웅(카카오브레인)Workshop on Automated Machine Learning- 요약 : 다양한 배치 크기에 잘 적용되는 자동화된 러닝 레이트 스케쥴러를 제안하였습니다. 특히, 해당 기술을 활용하여 큰 배치 크기 학습에서 러닝 레이트 조정에 드는 막대한 자원을 감소시킬 수 있었습니다.​8. ICCV 2021 (2021.10.12 - 17)​(1) VALUE Challenge 2021 우승https://arxiv.org/abs/2110.06476 Winning the ICCV'2021 VALUE Challenge: Task-aware Ensemble and Transfer Learning with Visual ConceptsThe VALUE (Video-And-Language Understanding Evaluation) benchmark is newly introduced to evaluate and analyze multi-modal representation learning algorithms on three video-and-language tasks: Retrieval, QA, and Captioning. The main objective of the VALUE challenge is to train a task-agnostic model t...arxiv.org - 저자: 신민철(카카오브레인), 문종환(카카오브레인), 온경운(카카오브레인), 강우영(카카오브레인), 한건수(카카오브레인), 김은솔(한양대)Workshop on Closing the Loop Between Vision and Language- 요약: 비디오 도메인에 관련된 여러가지 태스크(Retrieval, QA, and Captioning) 에서 task-agnostic 한 foundation model 의 품질을 비교하는 챌린지에서 VALUE/QA phase 우승​9. EMNLP 2021 (2021.11-7-11)​Semantic Alignment with Calibrated Similarity for Multilingual Sentence Embeddinghttps://aclanthology.org/2021.findings-emnlp.153/ Semantic Alignment with Calibrated Similarity for Multilingual Sentence EmbeddingJiyeon Ham, Eun-Sol Kim. Findings of the Association for Computational Linguistics: EMNLP 2021. 2021.aclanthology.org - 저자: 함지연(카카오브레인), 김은솔(한양대)Findings of EMNLP 2021- 요약: 다양한 언어의 문장 사이의 유사도를 잘 측정할 수 있는 다국어 문장 임베딩 모델 학습 방법 제안​10. NeurIPS 2021 (2021.12.6-14)​(1) the nethack challenge 준우승https://www.aicrowd.com/challenges/neurips-2021-the-nethack-challenge/leaderboards?challenge_leaderboard_extra_id=971&challenge_round_id=911 AIcrowd | NeurIPS 2021 - The NetHack Challenge | LeaderboardsASCII-rendered single-player dungeon crawl gamewww.aicrowd.com - 저자 : 권태환(카카오브레인), 변민우(카카오브레인), 김종민(카카오브레인), 이동훈(카카오브레인), 조대진(카카오브레인), 김성웅(카카오브레인)- 요약 : NetHack 이라는 single RPG 게임에서 AI를 훈련하여 높은 점수를 획득하는 대회에서 게임 내의 중요한 정보가 담긴 in-game message를 효율적으로 처리하기 위해 단어 단위로 encoding 적용, 게임 내에서 다양한 전략을 구사하는데 필요한 정보(아이템, 스펠등) 제공하고 이를 효과적으로 처리하기 위해 Transformer 기반의 네트워크 구조 설계, 카카오 브레인의 우수한 컴퓨팅 자원으로 준우승​(2) SWAD: Domain Generalization by Seeking Flat Minimahttps://arxiv.org/abs/2102.08604Code: https://github.com/khanrc/swad- 저자 : - 저자 : 차준범 (카카오브레인), 전상혁 (네이버), 이경재 (중앙대), 조한철 (네이버), 박승현 (네이버), 이윤성 (고려대), 박성래 (업스테이지)- 요약 : 실제 필드에서 AI가 학습할 때 적합하지 않은 사용자 데이터가 들어올 경우 flat minima를 찾아 데이터를 인식해 AI 학습의 성능을개선하는 과정. Flat solution을 찾는 것이 Domain generalization (DG) task를 풀 때에 필요함을 이론적으로 보이고, 기존에 제안된flatness-aware solver인 SWA를 DG task에 적합하게 수정한 SWAD를 제안. 기존 DG benchmark들에서 (PACS, VLCS, OfficeHome, DomainNet) SOTA를 갱신 "
ChatGPT로 영어 공부하는 방법! ① 스피킹(회화) ,https://blog.naver.com/tokiya_o1/223040702139,20230310,"#chatgpt #ChatGPT_영어공부 #챗지피티_영어공부 #영어공부 #chatgpt_영어회화 #ChatGPT_크롬확장 #영어스피킹 #영어Speaking #TTS #영어말하기 #영어리스닝 #일상대화 #프리토킹 #Talk_to_ChatGPT #텍스트를음성으로 #ChatGPT_노하우 #EnglishConversation #chatgpt음성인식 #chatgpt_tts​한달에 10만원 가까이 드는 원어민과의 전화 영어를 하면서매번 비슷한 자기소개만 하다가 10분이 가버려서 현타 온 적 있나요?저는 있어요. 주어진 시간 내에 모르는 사람과 전화로 대화하는 전화 영어 특성상 어쩔 수 없는 것..물론 심도있는 화상(전화)영어 프로그램도 있지만 비싸... 너무 비싸... ㅠㅠ 그러던 중, ChatGPT를 잘 활용하면 전화 영어 뺨을 때릴 수 있다는 소문을 듣고 직접 실험해보았어요!그런데 ChatGPT는 음성 지원이 아직 안되는 게 어떻게 한다는거야?ChatGPT와 TTS(Text-to-Speech)기능을 가진 크롬 확장 프로그램을 활용하니 가능했어요.​​​ 제 목소리가 녹음이 안 되서 중간에 공백이 있네요! ​방법은 간단하더라구요.① 마이크가 있는 컴퓨터(또는 컴퓨터와 마이크)에서 크롬 브라우저를 실행한다.② 크롬에 Talk-to-ChatGPT 확장 프로그램을 설치한다.③ OpenAI의 ChatGPT(https://chat.openai.com/chat)에서 대화한다.  크롬 확장 프로그램(Talk-to-ChatGPT)설치  방법크롬 브라우저 메뉴 우측 상단에서 퍼즐 모양 확장 프로그램 아이콘 클릭 2. 확장 프로그램 관리 클릭 3.  좌측 상단에서 세 줄 클릭 4. 좌측 하단에서  Chrome 웹 스토어 열기 클릭 ​5. Talk-to-ChatGPT 검색하고 Chrom에 추가 아이콘 클릭   ChatGPT에서 Talk-to-ChatGPT 실행하기https://chat.openai.com/chat 접속Talk-to-ChatGPT 실행하기 3. 언어 설정 하기      저는 처음에 Talk-to-ChatGPT에서 한국어 로봇 억양으로 영어를 읽어서 당황하다가     톱니 바퀴 클릭해서 언어 설정을 해주었어요. 4. AI voice and Language와 Speech recognition language를 모두 영어로 변경하고 save 누르기 저는 이때 save 버튼이 안 보여서 한참 헤맸는데 크롬 브라우저 화면 배율을 90%로 조정해주니 보이더라구요.  ChatGPT 활용 tip내가 말하는 소리를 듣고 있을 땐 빨간 불, bot이 말하고 있을 땐 초록 불   ChatGPT로 영어회화 연습을 해보니 단점은...물론 확장프로그램이 음성 인식이다보니 (제 발음이 안 좋아서ㅎㅎ) 제 말을 잘못 알아듣기도 하고 너무 로봇 같은 억양이라 실제 원어민과 대화하는 것과는 다르고자막이 보이다보니 듣기 연습은 별로 되지 않지만​장점은...관심없는 주제에 대해 이야기 하기 싫어하는 사람과 달리, 대부분의 건전한 주제에 대해서 연습할 수 있고ChatGPT가 설명충이라 말을 많이 해서 얘가 쓰는 표현들 중에 배울만 한 것들이 많고""오늘 한 대화 정리해서 자연스럽게 고쳐주고 왜 고쳤는지 알려줘""라던가 무리한 요구를 해도 다 해주더라구요?       10분짜리 전화영어에서는 튜터에게 그런 노가다 시킬 수 없잖아요 ㅠㅠ "
인공지능이란 무엇인가?   ,https://blog.naver.com/ai_era/223087940452,20230428,"인공지능(Artificial Intelligence, AI)은 인간의 지능을 컴퓨터와 같은 기계로 구현하는 기술로, 컴퓨터 과학의 한 분야로서 꾸준한 발전을 거듭해 오고 있습니다. 인공지능은 기계가 인간처럼 생각하고 학습할 수 있게 하는 기술로, 이를 통해 기계가 인간의 능력을 향상시키거나 인간의 일을 대신 수행할 수 있게 됩니다.​인공지능의 기본 개념은 크게 네 가지로 나눌 수 있습니다.​지식 표현(Knowledge Representation): 인공지능은 기계가 인간의 지식을 이해하고 처리할 수 있도록 지식을 표현하는 방법을 연구합니다. 지식 표현은 데이터, 개념, 사건 등의 관계를 기계가 이해할 수 있는 형태로 변환하는 과정입니다.추론(Reasoning): 인공지능은 기계가 주어진 정보를 바탕으로 새로운 정보를 도출하는 과정인 추론을 수행하게 합니다. 추론은 논리적인 결론을 도출하기 위해 규칙이나 알고리즘을 사용하여 주어진 정보를 조합하는 과정입니다.학습(Learning): 인공지능의 핵심 기능 중 하나는 기계가 경험을 통해 스스로 개선하는 능력입니다. 기계 학습(Machine Learning)은 기계가 데이터를 수집하고 분석하여 지식을 쌓고 성능을 향상시키는 과정입니다.인지(Cognition): 인공지능은 인간의 인지 능력을 모방하여 기계가 인간처럼 문제를 이해하고 해결하는 능력을 갖추게 합니다. 이를 위해 자연어 처리(Natural Language Processing), 컴퓨터 비전(Computer Vision), 음성 인식(Speech Recognition) 등 다양한 기술을 활용합니다.​​인공지능은 이미 우리 일상 생활에 깊숙이 들어와 있습니다. 스마트폰의 음성 비서, 자율주행 자동차, 추천 시스템 등 다양한 분야에서 인공지능은 우리의 삶을 편리하게 만들어주고 있습니다. 그러나 인공지능의 발전은 여전히 진행 중이며, 앞으로 더욱 다양한 분야에서 인공지능의 능력이 활용될 것으로 예상됩니다.​미래의 인공지능은 다음과 같은 기대를 갖고 있습니다.​인간과의 협력: 인공지능은 인간과의 협력을 통해 작업 효율성을 높이고, 더 나은 결과를 도출할 수 있습니다. 공동 작업을 통해 인간과 기계가 서로의 능력을 보완하며, 전문 지식이 필요한 분야에서도 더 빠른 결정을 내릴 수 있게 됩니다.창의력 발휘: 인공지능은 인간의 창의력을 향상시킬 수 있는 도구로 활용될 수 있습니다. 예컨대 디자인, 음악, 예술 등의 분야에서 인공지능은 인간의 창의적인 아이디어를 뒷받침해 새로운 작품을 창조할 수 있습니다.인간의 복잡한 문제 해결: 인공지능은 인간이 해결하기 어려운 복잡한 문제를 분석하고 해결할 수 있는 능력을 갖추게 됩니다. 기후 변화, 전염병 대응, 에너지 관리 등 인류가 직면한 문제에 대한 해결책을 찾는 데 인공지능이 큰 역할을 할 것으로 기대됩니다.인간의 능력 향상: 인공지능은 인간의 능력을 향상시키는 도구로 활용될 수 있습니다. 교육, 의료, 건강 관리 등 다양한 분야에서 인공지능은 개인별 맞춤형 서비스를 제공하여 인간의 능력을 높일 것입니다.​인공지능은 현재와 미래의 사회와 산업에 큰 변화를 불러올 것으로 예상되며, 이러한 기술의 발전을 이해하고 활용하는 것이 중요합니다. 지금까지 인공지능의 기본 개념에 대해 알아보았습니다. 인공지능은 우리의 삶에 어떤 영향을 미칠지 알아보기 위해 계속해서 관심을 기울여야 할 분야입니다.​​​#인공지능 #기계학습 #지식표현 #추론 #인지 #자연어처리 #컴퓨터비전 #음성인식 #협력 #창의력 #문제해결 #능력향상 #산업변화 #사회변화​ "
[인공지능] AI(인공지능)는 사람 대신 모든 일을 할 수 있을까? ,https://blog.naver.com/acornedu/223026098777,20230224,"  인공지능(Artificial Intelligence, AI)은인간의 지능을 모방하고 컴퓨터를 통해 구현한 기술로 최근 몇 년간 놀라운 발전을 이루고 있습니다. 이러한 발전은 우리의 일상생활과 산업 전반에 걸쳐 큰 변화를 가져올 것으로 예상합니다. 오늘은 인공지능이 우리를 대신하여 모든 일을 처리할 수 있을지 알아볼까 합니다.​  일상생활에서의 인공지능  대표적인 음성 비서 ( 빅스비, 시리, 구글 어시스턴트)인공지능은 이미 우리의 일상생활에서 다양하게 활용되고 있습니다. 예를 들어, 음성인식 기술을 이용한 가상 비서나, 추천 알고리즘을 이용한 쇼핑몰 추천 서비스,언어 번역 등이 그 예입니다. 이러한 인공지능 기술은 우리의 생활을 편리하게 만들어주고 있습니다. 일상생활에서의 인공지능 기술들은 아래와 같습니다​음성 비서(Assistant) : Siri, Bixby, Google 어시스턴트 등 음성 인식 기술을 이용한 음성비서 서비스는 인공지능 기술의 대표적인 예시입니다. 사용자가 명령을 내리면 음성 비서가 그에 대한 응답을 하여 일상생활에서 유용하게 활용됩니다. ​2. 검색 엔진(Search engien) : 검색 엔진은 사용자가 원하는 정보를 빠르게 검색할 수 있도록인공지능 기술을 활용합니다. 검색 엔진은 사용자가 입력한 검색어와 유사한 정보를 찾아 제공하며,검색어에 대한 문맥 정보를 이해하여 더욱 정확한 결과를 제공합니다. ​3. 추천 시스템(Recommendation system) : 온라인 쇼핑몰, 영화나 음악 스트리밍 서비스,SNS 등에서 우리는 자주 추천 시스템을 이용합니다. 추천 시스템은 사용자의 이전 구매 이력이나 검색 이력을 바탕으로 맞춤형 추천을 제공하며, 인공지능 기술을 이용하여 보다 정확한 추천을 제공합니다. ​4. 언어 번역(Language translation) : 인공지능 기술은 다양한 언어를 번역할 수 있는 기술을 제공하며,온라인 번역 서비스를 통해 일상에서 다른 언어를 읽고 이해하는데 많은 도움을 줍니다. ​5. 음성 인식 기술(Speech recognition) : 음석 인식 기술은 일상생활에서 주변 환경의 소음이나다른 인간의 음성과 같은 장애 요소를 감지하고 분리할 수 있어 우리의 음성 명령을 정확히 인식하고이해할 수 있습니다. 음성 인식 기술은 자동차 내비게이션, 음성 인식 기반 가전제품, 음성 검색, 음성 메모 등에 사용됩니다.  ​ ​ 산업 분야에서의 인공지능  산업 분야의 인공지능 (의료분야, 교통수단, 금융업 분야) 인공지능은 일생생활에서뿐만 아니라 산업 분야에서도 다양하게 활용되고 있습니다. 예를 들어, 자율주행 자동차, 로봇, 스마트 팩토리 등에 적용되어 생산성을 높이고인적 자원의 부족 문제를 해결해 주고 있습니다. 산업 분야의 인공지능은 다음과 같습니다. ​제조업 : 인공지능은 생산라인을 자동화하고, 생산량을 예측하여 재고를 줄이고생산 계획을 최적화하는 등 생산성을 높일 수 있습니다. ​2. 의료업 : 의료 이미지를 분석하여 질병을 예측하고, 약물 개발 과정에서의 시험-오류 비용을 줄이는 등의료 분야에서 매우 유용하게 사용될 수 있습니다. ​3. 금융업 : 인공지능은 금융기관에서 보안성과 비즈니스 성능을 높이는 데 큰 역할을 합니다.금융 거래 및 거래 패턴을 분석하고, 부정 거래를 탐지하고, 신용 스코어링 모델을 개발하는 등의 작업에 사용됩니다. ​4. 교통수단 : 운전자 도움 시스템(ADAS), 자율주행차, 운송 서비스 등다양한 차세대 교통수단에서 인공지능이 활용됩니다.​5. 판매 및 마케팅 : 인공지능은 맞춤형 광고, 상품 추천, 구매 이력 및 판매 기록 등을 분석하여고객들에게 보다 효과적인 마케팅 전략을 제시할 수 있습니다.​​ 이렇듯 인공지능은 빠른 계산과 처리 속도를 가지고 있어, 데이터 처리, 분석, 예측 등 다양한 분야에서 사용되고 있습니다. 또한 반복적이고 단순한 작업을 자동화하여인간의 시간과 노력을 대폭 줄일 수 있습니다. 정확한 데이터 처리와 분석을 기반으로 작동하여인간의 오류로 인한 문제도 줄일 수 있습니다. 인공지능은 사람들이 처리하기 어려운 복잡한 데이터를 처리하거나 빠른 시간 내에 판단을 내려야 하는 상황에서 사람의 능력을 보안하는 역할도 수행합니다. ​  ​ 인공지능의 문제점 ​​인공지능이 인간 대신 모든 일을 처리하게 되면 좋은 일만 있지는 않습니다. ​인공지능이 모든 일을 처리하면서 인간의 능력이 저하될 수 있습니다. 인간은 지속적으로 노력하고 학습함으로써 더 나은 결과를 얻을 수 있습니다. 그러나 인공지능이 모든 일을 처리하면 인간은 더 이상 노력하지 않고, 그에 따라 능력이 떨어질 수 있습니다. ​2. 인공지능은 인간과는 다른 지각과 판단 능력을 가지고 있습니다. 따라서 어떤 경우에는 인공지능의 판단이 인간의 판단과 다를 수 있습니다. 인공지능이 예측하지 못한 상황에서는 인간이 대처해야 합니다. ​3. 인공지능은 데이터를 기반으로 학습합니다. 데이터가 부족하거나 편향되어 있을 경우, 인공지능의 결과도 그에 따라 부정확하거나 편향될 수 있습니다. ​4. 인공지능은 논리적인 사고에 강점을 가지고 있지만, 창의적인 사고나 인간의 감성적인 요소를 이해하는 능력은 부족합니다.따라서, 예술, 문화, 창작 등과 같은 분야에서는 인공지능이 인간을 대신하기는 어렵습니다.   ​따라서 인공지능이 사람을 대신하여 모든 일을 처리할 수 있는지는 아직까지 명확하지 않습니다. 인공지능은 데이터 분석, 패턴 인식 등의 분야에서는 매우 뛰어난 성과를 보여주고 있지만,인간과 같은 모든 면에서 인공지능이 대체될 수 있다는 것은 아닙니다. ​오히려 인간과 인공지능이 상호보완적으로 일하는 것이 가장 좋은 방법일 것입니다. 인간은 감성적인 측면에서 우수하고, 인공지능은 계산적인 측면에서 우수하기 때문입니다. 따라서 앞으로 인간과 인공지능이 함께 일하는 일상이 보다 확대될 것으로 예상됩니다. ​​​​ 사실은 이 블로그는 100% 인공지능을 통해서 작성되었고사람인 저는 옮겨 적는 일만 했습니다. 또한 AI 아나운서를 통하여 블로그를 들어볼 수 있습니다.  ​​* 블로그를 작성해준 인공지능 Chat GPT : https://chat.openai.com/* AI 아나운서 : 플루닛 스튜디오 : https://studio.ploonet.com/ ​ ​​​ ​​​​​ "
[파이썬(Python)] 인공지능 스피커 만들기 / 유튜버 '나도코딩'님 동영상 강의 소개 ,https://blog.naver.com/human_intelligence/222922687209,20221109,"안녕하세요, 사람지능입니다.​아래의 동영상을 3부분으로 나누어 포스팅하고 있습니다.​오늘은 아래 동영상의 세 번째 강의인 '인공지능 스피커 만들기' 강의를 들으며 기록하려 합니다.​▶ 아래 동영상 33:34부터 마지막까지가 '인공지능 스피커 만들기' 강의입니다.​https://youtu.be/WTul6LIjIBA ▶ 아래는 인공지능 스피커 프로그램의 개요입니다. ※ 인공지능 스피커 프로그램의 개요​import time, osimport speech_recognition as srfrom gtts import gTTSfrom playsound import playsound​# 음성인식 (듣기, STT)def listen(recognizer, audio): pass​# 대답def answer(input_text): pass​# 소리내어 읽기(TTS)def speak(text):    pass​ ▶ 아래는 강의에서 사용된 인공지능 스피커 만들기 코드입니다.  import time, osimport speech_recognition as srfrom gtts import gTTSfrom playsound import playsound​# 음성인식 (듣기, STT)def listen(recognizer, audio): try: text = recognizer.recognize_google(audio, language='ko') print('[나도코딩]' + text) answer(text) except sr.UnknownValueError: print('인식 실패') # 음성 인식 실패한 경우 except sr.RequestError as e: print('요청 실패:{0}'.format(e)) # API Key 오류 또는 네트워크 단절 등​# 대답def answer(input_text): answer_text = '' if '안녕' in input_text: answer_text = '안녕하세요? 반갑습니다.' elif '날씨' in input_text: answer_text = '오늘의 서울 기온은 20도입니다. 맑은 하늘이 예상됩니다.' elif '환율' in input_text: answer_text = '원 달러 환율은 1380원입니다.' elif '고마워' in input_text: answer_text = '별 말씀을요.' elif '종료' in input_text: answer_text = '다음에 또 만나요' stop_listening(wait_for_stop=False) # 더 이상 듣지 않음 else: answer_text = '다시 한 번 말씀해 주시겠어요?' speak(answer_text)​# 소리내어 읽기(TTS)def speak(text): print('[인공지능]' + text) file_name = 'voice.mp3' tts = gTTS(text=text, lang='ko') tts.save(file_name) playsound(file_name) if os.path.exists(file_name): # vioce.mp3 파일 삭제 os.remove(file_name)​r = sr.Recognizer()m = sr.Microphone()​speak('무엇을 도와드릴까요')stop_listening = r.listen_in_background(m, listen)​while True: time.sleep(0.1) ▶ 위 코드에서 answer_text 부분을 웹스크래핑 코드로 대체하면 실제로 이웃님들의 필요에 대응하는 맞춤형 인공지는 스피커를 만들 수 있다는 것이 나도코딩님 가르침의 핵심입니다.​▶ 아래는 위 코드를 실행한 결과물입니다. 위 코드를 실행하면 [나도코딩]의 질문이 마이크를 통해 음성으로 인식되어 문자(text)로 변환이 되고 그에 대응하는 [인공지능]의 말이 스피커를 통해서 나오는 것을 들으실 수 있습니다. ▶ 이상입니다. 매우 즐거운 경험이었습니다.​▶ 나도코딩님! 감사합니다.​#파이썬 #Python #인공지능 #스피커 #만들기 #유튜버 #나도코딩 #동영상 #강의 #소개 "
[논문리뷰-25] Automatic identification of defect patterns in Semiconductor Wafer Map ,https://blog.naver.com/mindmerge/223059614438,20230330,"#Automatic Identification of Defect Patterns in Semiconductor Wafer Maps Using SpatialCorrelogram and Dynamic Time Warping​ ​This paper proposes a new methodology for analyzing wafer map data, which can provide useful information on the process of integrated circuit (IC) fabrication. ​The proposed methodology uses spatial correlogram for detecting the presence of spatial autocorrelations and for the classification of defect patterns on the wafer map. The dynamic time warping algorithm is used for automatic classification based on spatial correlogram. The proposed method also includes generalized join-count (JC)-based statistics, with a procedure to determine optimal weights. The method is illustrated using real-life examples and simulated data sets and is shown to be robust to random noise and defect location and size. Overall, this methodology can help better understand ongoing process problems in IC fabrication.​The dynamic time warping (DTW) algorithm is a method used for measuring similarity between two temporal sequences that may vary in speed. ​It works by finding an optimal non-linear alignment between the two sequences, which allows for a comparison of their shape regardless of the differences in timing and speed. The algorithm works by computing a distance matrix that measures the pairwise distances between each point in the two sequences, and then finding the optimal path through this matrix that minimizes the distance between the sequences. DTW has many applications in fields such as speech recognition, image processing, and pattern recognition, including the automatic classification of defect patterns on wafer maps as proposed in the paper.​ ​ ​ ​In the context of analyzing wafer defect maps, ​the DTW algorithm can be used to automatically classify spatial patterns of defects on the wafer. After detecting the presence of spatial autocorrelations using the spatial correlogram, the DTW algorithm is used to find the optimal alignment between each defect pattern and a set of reference patterns, which have been previously classified. T​his is done by measuring the distance between each defect pattern and the reference patterns using the DTW distance measure. The optimal alignment is then used to classify the defect pattern based on the reference pattern with the smallest distance. This process is repeated for all defect patterns on the wafer map, resulting in an automatic classification of the spatial patterns of defects. The advantage of using DTW for this task is that it can handle temporal variations in the defect patterns, which may arise due to the manufacturing process, and can provide a more accurate classification compared to methods that rely on Euclidean distance measures.​ ​The paper proposes a new methodology for automatic detection and classification of anomaly defect patterns on a wafer map. ​The methodology combines the use of spatial correlogram and DTW algorithm. The spatial correlogram is used to detect the presence of anomaly defect patterns on the wafer map. The DTW algorithm is then used to classify the anomaly defect patterns into one of the existing spatial defect patterns.​The proposed methodology is shown to be more effective in detecting and classifying spatial defect patterns on the wafer map compared to methods that use a single lag. The methodology is also robust to random noise, defect location, and defect size, making it a valuable tool for monitoring and diagnosing IC manufacturing processes.Further studies can focus on developing more advanced classification techniques based on spatial correlogram and extending the methodology to the wafer bin map, which contains more information than the binary wafer map. Overall, the proposed methodology is a promising approach for automatic defect classification and can help improve the efficiency and quality of IC fabrication processes. "
AI 발전의 방향성 ,https://blog.naver.com/stcyrr87osnii/223036829578,20230327,"AI 발전의 방향성​ 인공 지능(AI)은 광범위한 산업과 애플리케이션을 변화시킬 수 있는 잠재력을 지닌 빠르게 진화하는 분야입니다. AI의 발전은 기술 발전, 데이터 가용성 증가, 보다 지능적이고 자동화된 시스템에 대한 수요 증가에 의해 주도되고 있습니다.​다음은 AI 개발을 위한 몇 가지 주요 방향입니다.​자연어 처리(NLP): NLP는 자연어를 사용하여 컴퓨터와 인간 간의 상호 작용에 중점을 둔 AI의 하위 분야입니다. NLP의 개발은 컴퓨터와 인간 사이에 보다 인간적이고 효과적인 의사소통을 만드는 것을 목표로 합니다.​컴퓨터 비전: 컴퓨터 비전은 컴퓨터가 세상의 시각적 정보를 해석하고 이해할 수 있도록 하는 데 중점을 둔 AI의 하위 분야입니다. 여기에는 객체 인식, 이미지 분류 및 장면 이해와 같은 작업이 포함됩니다.​강화 학습(Reinforcement Learning): 강화 학습은 경험을 통해 학습하고 보상과 벌칙에 따라 결정을 내릴 수 있는 AI 시스템 개발에 중점을 둔 기계 학습의 한 유형입니다. 강화 학습은 로봇 공학, 게임 및 자율 주행 차량을 포함한 광범위한 애플리케이션을 위한 지능형 시스템을 개발하는 데 사용되고 있습니다.​딥 러닝: 딥 러닝은 인공 신경망 개발에 중점을 둔 기계 학습의 하위 분야입니다. 딥 러닝은 많은 양의 데이터에서 학습하고 이미지 인식 및 음성 인식과 같은 작업을 높은 정확도로 수행할 수 있는 보다 진보된 AI 시스템을 개발하는 데 사용되고 있습니다.​설명 가능성 및 투명성: AI가 사회에서 널리 보급됨에 따라 AI 시스템이 설명 가능하고 투명해야 할 필요성이 커지고 있습니다. 여기에는 그들의 결정과 행동에 대한 명확하고 이해하기 쉬운 설명을 제공할 수 있는 AI 시스템의 개발이 포함됩니다.​결론적으로 AI는 기술 발전, 데이터 가용성 증가, 보다 지능적이고 자동화된 시스템에 대한 수요 증가로 인해 빠르게 진화하는 분야입니다. AI 개발은 NLP, 컴퓨터 비전, 강화 학습, 딥 러닝, 설명 가능성 및 투명성을 포함한 다양한 영역에 중점을 둡니다.​​​Artificial Intelligence (AI) is a rapidly evolving field that has the potential to transform a wide range of industries and applications. The development of AI is being driven by advances in technology, increasing data availability, and growing demand for more intelligent and automated systems.​Here are some of the key directions for AI development:​Natural Language Processing (NLP): NLP is a subfield of AI that focuses on the interaction between computers and humans using natural language. The development of NLP is aimed at creating more human-like and effective communication between computers and humans.​Computer Vision: Computer vision is a subfield of AI that focuses on enabling computers to interpret and understand visual information from the world. This includes tasks such as object recognition, image classification, and scene understanding.​Reinforcement Learning: Reinforcement learning is a type of machine learning that focuses on the development of AI systems that can learn from experience and make decisions based on rewards and penalties. Reinforcement learning is being used to develop intelligent systems for a wide range of applications, including robotics, gaming, and autonomous vehicles.​Deep Learning: Deep learning is a subfield of machine learning that focuses on the development of artificial neural networks. Deep learning is being used to develop more advanced AI systems that can learn from large amounts of data and perform tasks such as image recognition and speech recognition with high accuracy.​Explainability and Transparency: As AI becomes more prevalent in society, there is a growing need for AI systems to be explainable and transparent. This includes the development of AI systems that can provide clear and understandable explanations for their decisions and actions.​In conclusion, AI is a rapidly evolving field that is being driven by advances in technology, increasing data availability, and growing demand for more intelligent and automated systems. The development of AI is focused on a range of areas, including NLP, computer vision, reinforcement learning, deep learning, and explainability and transparency.​​​ "
"[기본단어/영어어휘] DAY 100. 오늘은 ‘동사, 명사’의 날 recognize, recognition, reconcile, reconciliation, stimulate ",https://blog.naver.com/jamesprep/222696650148,20220410,"DAY 100. 오늘은 ‘동사, 명사’의 날​1. recognize v. 인식하다US leaders should recognize the Korean people’s opposition ti the FTA.미국 지도자들은 FTA에 대한 한국인들의 반대를 알아야 한다.​2. recognition n. 인식 (인정) a. recognizable 인식할 수 있는The elderly lady has helped others without expecting public recognition.노파는 사람들에게 알리지 않고 다른 사람들을 도와 왔다.​3. reconcile v. 조정하다 (화해시키다)He was busy drafting a speech on the subject of ways to reconcile the two Koreas.그는 남한과 북한을 화해시키기 위한 방법을 주제로 한 연설 초안을 작성하느라 바빴다.​4. reconciliation n. 조정 (화해)ANOC has greatly contributed to the reconciliation and peace of humankind.국가 올림픽 연합회(ANOC)는 인류의 화해와 평화에 큰 공헌을 했다.​5. stimulate v. 자극하다He stressed the need for the government to take measures to stimulate the economy.그는 정부가 경제를 살리기 위한 조치를 취할 필요성에 대해 강조했다.​6. stimulation n. 자극 a. stimulating 자극적인Electric stimulation can help to heal fractured bones.전기 자극은 골절된 뼈를 치료하는 데 도움이 될 수 있다.​7. transform v. 변형시키다There are plans to transform Jeju island into a ""free international zone.""제주도를 ‘국제 자유 도시’로 변모시킬 계획들이 나돈다.​8. transformation n. 변형 n. transformer 변압기Nowadays, the transformation into a knowledge-based society is taking place.오늘날, 지식 기반 사회로의 변형이 이루어지고 있다. "
자연어 처리 ,https://blog.naver.com/bearmom215/223006070695,20230205,"Natural language processing#자연어 처리​58 languagesArticleTalkReadEditView historyFrom Wikipedia, the free encyclopedia​이 문서는 컴퓨터가 수행하는 자연어 처리에 관한 것입니다. 인간의 뇌가 수행하는 자연어 처리에 대해서는 뇌의 언어처리 문서를 참조하십시오.This article is about natural language processing done by computers. For the natural language processing done by the human brain, see Language processing in the brain. 웹페이지에서 고객 서비스를 제공하는 자동화된 온라인 비서, 자연어 처리가 주요 구성요소인 애플리케이션의 예[1] An automated online assistant providing customer service on a web page, an example of an application where natural language processing is a major component[1] ​자연어처리(NLP)는 컴퓨터와 인간언어 간 상호작용, 특히 대량의 자연어 데이터를 처리하고 분석하도록 컴퓨터를 프로그래밍하는 방법과 관련된 언어학, 컴퓨터과학, 인공지능의 학제 간 하위분야입니다. 목표는 문서에 포함된 언어의 문맥적 뉘앙스를 포함하여 문서 내용을 ""이해""할 수 있는 컴퓨터입니다. 그런 다음 이 기술은 문서에 포함된 정보와 통찰력을 정확하게 추출하고 문서 자체를 분류하고 구성할 수 있습니다.자연어처리 과제는 종종 #음성인식 #자연어이해 및 #자연어생성 과 관련됩니다.Natural language processing (NLP) is an interdisciplinary subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human language, in particular how to program computers to process and analyze large amounts of natural language data. The goal is a computer capable of ""understanding"" the contents of documents, including the contextual nuances of the language within them. The technology can then accurately extract information and insights contained in the documents as well as categorize and organize the documents themselves.Challenges in natural language processing frequently involve speech recognition, natural-language understanding, and natural-language generation.​​역사 History[edit]​추가 정보: 자연어 처리의 역사Further information: History of natural language processing​자연어 처리는 1950년대에 시작되었습니다. 이미 1950년에 Alan Turing은 ""Computing Machinery and Intelligence""라는 제목의 기사를 발표했는데, 당시에는 인공지능과 별개의 문제로 표현되지 않았지만 현재 Turing 테스트라고 불리는 것을 지능의 기준으로 제안했습니다. 제안된 테스트에는 자동해석 및 자연어생성과 관련된 작업이 포함됩니다.Natural language processing has its roots in the 1950s. Already in 1950, Alan Turing published an article titled ""Computing Machinery and Intelligence"" which proposed what is now called the Turing test as a criterion of intelligence, though at the time that was not articulated as a problem separate from artificial intelligence. The proposed test includes a task that involves the automated interpretation and generation of natural language.​건강전문 몰www.dopza.com 돕자몰www.dopza.com ​상징적 NLP(1950년대~1990년대 초반)Symbolic NLP [edit]​상징적 NLP의 전제는  John Searle's Chinese room 실험으로 잘 요약되어 있습니다: 규칙모음(예: 질문과 대답이 일치하는 중국어 숙어집)이 주어지면 컴퓨터는 직면하는 데이터에 대한 규칙을 적용하여 자연어 이해(또는 기타 NLP 작업)를 에뮬레이션합니다. The premise of symbolic NLP is well-summarized by John Searle's Chinese room experiment: Given a collection of rules (e.g., a Chinese phrasebook, with questions and matching answers), the computer emulates natural language understanding (or other NLP tasks) by applying those rules to the data it confronts.​1950s 1950년대: 1954년 조지타운 실험에서는 60개 이상의 러시아어 문장을 영어로 완전 자동번역하는 작업이 포함되었습니다. 저자는 3~5년 안에 기계번역이 문제를 해결할 것이라고 주장했습니다.[2] 그러나 실제 진행은 훨씬 더디었고 1966년 ALPAC 보고서에서 10년 간의 연구가 기대치를 충족시키지 못했다는 사실이 밝혀진 후 기계번역에 대한 자금지원이 크게 줄었습니다. 최초의 통계적 기계번역시스템이 개발된 1980년대 후반까지 기계번역에 대한 추가 연구는 거의 수행되지 않았습니다. The Georgetown experiment in 1954 involved fully automatic translation of more than sixty Russian sentences into English. The authors claimed that within three or five years, machine translation would be a solved problem.[2] However, real progress was much slower, and after the ALPAC report in 1966, which found that ten-year-long research had failed to fulfill the expectations, funding for machine translation was dramatically reduced. Little further research in machine translation was conducted until the late 1980s when the first statistical machine translation systems were developed.​1960s  1960년대: 1960년대에 개발된 주목할 만한 성공적인 자연 언어처리 시스템 중 일부는 제한된 어휘를 사용하여 제한된 ""블록 세계""에서 작동하는 자연언어 시스템인 SHRDLU와 1964년에서 1966년 사이에 Joseph Weizenbaum이 작성한 Rogerian 심리치료사의 시뮬레이션인 ELIZA였습니다. ELIZA는 인간의 생각이나 감정에 대한 정보를 거의 사용하지 않고 때때로 놀랍도록 인간과 같은 상호작용을 제공했습니다. ""환자""가 매우 작은 지식 기반을 초과하면 ELIZA는 일반적 응답을 제공할 수 있습니다. 예를 들어 ""머리가 아프다""에 대해 ""왜 머리가 아프다고 하시나요?""로 응답합니다. Some notably successful natural language processing systems developed in the 1960s were SHRDLU, a natural language system working in restricted ""blocks worlds"" with restricted vocabularies, and ELIZA, a simulation of a Rogerian psychotherapist, written by Joseph Weizenbaum between 1964 and 1966. Using almost no information about human thought or emotion, ELIZA sometimes provided a startlingly human-like interaction. When the ""patient"" exceeded the very small knowledge base, ELIZA might provide a generic response, for example, responding to ""My head hurts"" with ""Why do you say your head hurts?"".​1970s 1970년대: 1970년대에 많은 프로그래머들이 실제 정보를 컴퓨터가 이해할 수 있는 데이터로 구조화한 ""개념적 온톨로지""를 작성하기 시작했습니다. 예는 MARGIE(Schank, 1975), SAM(Cullingford, 1978), PAM(Wilensky, 1978), TaleSpin(Meehan, 1976), QUALM(Lehnert, 1977), Politics(Carbonell, 1979) 및 Plot Units(Lehnert 1981)입니다. 이 기간 동안 첫 번째 채터봇이 작성되었습니다(예: PARRY). During the 1970s, many programmers began to write ""conceptual ontologies"", which structured real-world information into computer-understandable data. Examples are MARGIE (Schank, 1975), SAM (Cullingford, 1978), PAM (Wilensky, 1978), TaleSpin (Meehan, 1976), QUALM (Lehnert, 1977), Politics (Carbonell, 1979), and Plot Units (Lehnert 1981). During this time, the first chatterbots were written (e.g., PARRY).​1980s 1980년대: 1980년대와 1990년대 초반은 NLP에서 상징적 방법의 전성기를 맞았습니다. 당시의 초점 영역에는 규칙기반 구문 분석(예: 생성문법의 전산 조작화로서 HPSG 개발), 형태학(예: 2단계 형태[3]), 의미론(예: Lesk 알고리즘), 참조에 대한 연구가 포함되었습니다. (예: 센터링 이론[4] 내) 및 기타 자연언어 이해 영역(예: 수사구조 이론). Racter 및 Jabberwacky와 함께 채터봇 개발과 같은 다른 연구 라인도 계속되었습니다. 중요한 발전(결국 1990년대에 통계적 전환으로 이어짐)은 이 기간 동안 정량적 평가의 중요성이 증가한 것입니다.[5] The 1980s and early 1990s mark the heyday of symbolic methods in NLP. Focus areas of the time included research on rule-based parsing (e.g., the development of HPSG as a computational operationalization of generative grammar), morphology (e.g., two-level morphology[3]), semantics (e.g., Lesk algorithm), reference (e.g., within Centering Theory[4]) and other areas of natural language understanding (e.g., in the Rhetorical Structure Theory). Other lines of research were continued, e.g., the development of chatterbots with Racter and Jabberwacky. An important development (that eventually led to the statistical turn in the 1990s) was the rising importance of quantitative evaluation in this period.[5]​​Statistical NLP (1990s–2010s)[edit]​Up to the 1980s, most natural language processing systems were based on complex sets of hand-written rules. Starting in the late 1980s, however, there was a revolution in natural language processing with the introduction of machine learning algorithms for language processing. This was due to both the steady increase in computational power (see Moore's law) and the gradual lessening of the dominance of Chomskyan theories of linguistics (e.g. transformational grammar), whose theoretical underpinnings discouraged the sort of corpus linguistics that underlies the machine-learning approach to language processing.[6]1990s: Many of the notable early successes on statistical methods in NLP occurred in the field of machine translation, due especially to work at IBM Research. These systems were able to take advantage of existing multilingual textual corpora that had been produced by the Parliament of Canada and the European Union as a result of laws calling for the translation of all governmental proceedings into all official languages of the corresponding systems of government. However, most other systems depended on corpora specifically developed for the tasks implemented by these systems, which was (and often continues to be) a major limitation in the success of these systems. As a result, a great deal of research has gone into methods of more effectively learning from limited amounts of data.2000s: With the growth of the web, increasing amounts of raw (unannotated) language data has become available since the mid-1990s. Research has thus increasingly focused on unsupervised and semi-supervised learning algorithms. Such algorithms can learn from data that has not been hand-annotated with the desired answers or using a combination of annotated and non-annotated data. Generally, this task is much more difficult than supervised learning, and typically produces less accurate results for a given amount of input data. However, there is an enormous amount of non-annotated data available (including, among other things, the entire content of the World Wide Web), which can often make up for the inferior results if the algorithm used has a low enough time complexity to be practical.​​Neural NLP (present)[edit]In the 2010s, representation learning and deep neural network-style machine learning methods became widespread in natural language processing. That popularity was due partly to a flurry of results showing that such techniques[7][8] can achieve state-of-the-art results in many natural language tasks, e.g., in language modeling[9] and parsing.[10][11] This is increasingly important in medicine and healthcare, where NLP helps analyze notes and text in electronic health records that would otherwise be inaccessible for study when seeking to improve care.[12]​Methods: Rules, statistics, neural networks[edit]​In the early days, many language-processing systems were designed by symbolic methods, i.e., the hand-coding of a set of rules, coupled with a dictionary lookup:[13][14] such as by writing grammars or devising heuristic rules for stemming.More recent systems based on machine-learning algorithms have many advantages over hand-produced rules:The learning procedures used during machine learning automatically focus on the most common cases, whereas when writing rules by hand it is often not at all obvious where the effort should be directed.Automatic learning procedures can make use of statistical inference algorithms to produce models that are robust to unfamiliar input (e.g. containing words or structures that have not been seen before) and to erroneous input (e.g. with misspelled words or words accidentally omitted). Generally, handling such input gracefully with handwritten rules, or, more generally, creating systems of handwritten rules that make soft decisions, is extremely difficult, error-prone and time-consuming.Systems based on automatically learning the rules can be made more accurate simply by supplying more input data. However, systems based on handwritten rules can only be made more accurate by increasing the complexity of the rules, which is a much more difficult task. In particular, there is a limit to the complexity of systems based on handwritten rules, beyond which the systems become more and more unmanageable. However, creating more data to input to machine-learning systems simply requires a corresponding increase in the number of man-hours worked, generally without significant increases in the complexity of the annotation process.Despite the popularity of machine learning in NLP research, symbolic methods are still (2020) commonly used:when the amount of training data is insufficient to successfully apply machine learning methods, e.g., for the machine translation of low-resource languages such as provided by the Apertium system,for preprocessing in NLP pipelines, e.g., tokenization, orfor postprocessing and transforming the output of NLP pipelines, e.g., for knowledge extraction from syntactic parses.Statistical methods[edit]Since the so-called ""statistical revolution""[15][16] in the late 1980s and mid-1990s, much natural language processing research has relied heavily on machine learning. The machine-learning paradigm calls instead for using statistical inference to automatically learn such rules through the analysis of large corpora (the plural form of corpus, is a set of documents, possibly with human or computer annotations) of typical real-world examples.Many different classes of machine-learning algorithms have been applied to natural-language-processing tasks. These algorithms take as input a large set of ""features"" that are generated from the input data. Increasingly, however, research has focused on statistical models, which make soft, probabilistic decisions based on attaching real-valued weights to each input feature (complex-valued embeddings,[17] and neural networks in general have also been proposed, for e.g. speech[18]). Such models have the advantage that they can express the relative certainty of many different possible answers rather than only one, producing more reliable results when such a model is included as a component of a larger system.Some of the earliest-used machine learning algorithms, such as decision trees, produced systems of hard if-then rules similar to existing hand-written rules. However, part-of-speech tagging introduced the use of hidden Markov models to natural language processing, and increasingly, research has focused on statistical models, which make soft, probabilistic decisions based on attaching real-valued weights to the features making up the input data. The cache language models upon which many speech recognition systems now rely are examples of such statistical models. Such models are generally more robust when given unfamiliar input, especially input that contains errors (as is very common for real-world data), and produce more reliable results when integrated into a larger system comprising multiple subtasks.Since the neural turn, statistical methods in NLP research have been largely replaced by neural networks. However, they continue to be relevant for contexts in which statistical interpretability and transparency is required.Neural networks[edit]Further information: Artificial neural networkA major drawback of statistical methods is that they require elaborate feature engineering. Since 2015,[19] the field has thus largely abandoned statistical methods and shifted to neural networks for machine learning. Popular techniques include the use of word embeddings to capture semantic properties of words, and an increase in end-to-end learning of a higher-level task (e.g., question answering) instead of relying on a pipeline of separate intermediate tasks (e.g., part-of-speech tagging and dependency parsing). In some areas, this shift has entailed substantial changes in how NLP systems are designed, such that deep neural network-based approaches may be viewed as a new paradigm distinct from statistical natural language processing. For instance, the term neural machine translation (NMT) emphasizes the fact that deep learning-based approaches to machine translation directly learn sequence-to-sequence transformations, obviating the need for intermediate steps such as word alignment and language modeling that was used in statistical machine translation (SMT).​Common NLP tasks[edit]​The following is a list of some of the most commonly researched tasks in natural language processing. Some of these tasks have direct real-world applications, while others more commonly serve as subtasks that are used to aid in solving larger tasks.Though natural language processing tasks are closely intertwined, they can be subdivided into categories for convenience. A coarse division is given below.​Text and speech processing[edit]​Optical character recognition (OCR)Given an image representing printed text, determine the corresponding text.Speech recognitionGiven a sound clip of a person or people speaking, determine the textual representation of the speech. This is the opposite of text to speech and is one of the extremely difficult problems colloquially termed ""AI-complete"" (see above). In natural speech there are hardly any pauses between successive words, and thus speech segmentation is a necessary subtask of speech recognition (see below). In most spoken languages, the sounds representing successive letters blend into each other in a process termed coarticulation, so the conversion of the analog signal to discrete characters can be a very difficult process. Also, given that words in the same language are spoken by people with different accents, the speech recognition software must be able to recognize the wide variety of input as being identical to each other in terms of its textual equivalent.Speech segmentationGiven a sound clip of a person or people speaking, separate it into words. A subtask of speech recognition and typically grouped with it.Text-to-speechGiven a text, transform those units and produce a spoken representation. Text-to-speech can be used to aid the visually impaired.[20]Word segmentation (Tokenization)Separate a chunk of continuous text into separate words. For a language like English, this is fairly trivial, since words are usually separated by spaces. However, some written languages like Chinese, Japanese and Thai do not mark word boundaries in such a fashion, and in those languages text segmentation is a significant task requiring knowledge of the vocabulary and morphology of words in the language. Sometimes this process is also used in cases like bag of words (BOW) creation in data mining.​Morphological analysis[edit]​LemmatizationThe task of removing inflectional endings only and to return the base dictionary form of a word which is also known as a lemma. Lemmatization is another technique for reducing words to their normalized form. But in this case, the transformation actually uses a dictionary to map words to their actual form.[21]Morphological segmentationSeparate words into individual morphemes and identify the class of the morphemes. The difficulty of this task depends greatly on the complexity of the morphology (i.e., the structure of words) of the language being considered. English has fairly simple morphology, especially inflectional morphology, and thus it is often possible to ignore this task entirely and simply model all possible forms of a word (e.g., ""open, opens, opened, opening"") as separate words. In languages such as Turkish or Meitei,[22] a highly agglutinated Indian language, however, such an approach is not possible, as each dictionary entry has thousands of possible word forms.Part-of-speech taggingGiven a sentence, determine the part of speech (POS) for each word. Many words, especially common ones, can serve as multiple parts of speech. For example, ""book"" can be a noun (""the book on the table"") or verb (""to book a flight""); ""set"" can be a noun, verb or adjective; and ""out"" can be any of at least five different parts of speech.StemmingThe process of reducing inflected (or sometimes derived) words to a base form (e.g., ""close"" will be the root for ""closed"", ""closing"", ""close"", ""closer"" etc.). Stemming yields similar results as lemmatization, but does so on grounds of rules, not a dictionary.Syntactic analysis[edit]Grammar induction[23]Generate a formal grammar that describes a language's syntax.Sentence breaking (also known as ""sentence boundary disambiguation"")Given a chunk of text, find the sentence boundaries. Sentence boundaries are often marked by periods or other punctuation marks, but these same characters can serve other purposes (e.g., marking abbreviations).ParsingDetermine the parse tree (grammatical analysis) of a given sentence. The grammar for natural languages is ambiguous and typical sentences have multiple possible analyses: perhaps surprisingly, for a typical sentence there may be thousands of potential parses (most of which will seem completely nonsensical to a human). There are two primary types of parsing: dependency parsing and constituency parsing. Dependency parsing focuses on the relationships between words in a sentence (marking things like primary objects and predicates), whereas constituency parsing focuses on building out the parse tree using a probabilistic context-free grammar (PCFG) (see also stochastic grammar).​Lexical semantics (of individual words in context)[edit]​Lexical semanticsWhat is the computational meaning of individual words in context?Distributional semanticsHow can we learn semantic representations from data?Named entity recognition (NER)Given a stream of text, determine which items in the text map to proper names, such as people or places, and what the type of each such name is (e.g. person, location, organization). Although capitalization can aid in recognizing named entities in languages such as English, this information cannot aid in determining the type of named entity, and in any case, is often inaccurate or insufficient. For example, the first letter of a sentence is also capitalized, and named entities often span several words, only some of which are capitalized. Furthermore, many other languages in non-Western scripts (e.g. Chinese or Arabic) do not have any capitalization at all, and even languages with capitalization may not consistently use it to distinguish names. For example, German capitalizes all nouns, regardless of whether they are names, and French and Spanish do not capitalize names that serve as adjectives.Sentiment analysis (see also Multimodal sentiment analysis)Extract subjective information usually from a set of documents, often using online reviews to determine ""polarity"" about specific objects. It is especially useful for identifying trends of public opinion in social media, for marketing.Terminology extractionThe goal of terminology extraction is to automatically extract relevant terms from a given corpus.Word-sense disambiguation (WSD)Many words have more than one meaning; we have to select the meaning which makes the most sense in context. For this problem, we are typically given a list of words and associated word senses, e.g. from a dictionary or an online resource such as WordNet.Entity linkingMany words—typically proper names—refer to named entities; here we have to select the entity (a famous individual, a location, a company, etc.) which is referred to in context.​Relational semantics (semantics of individual sentences)[edit]​Relationship extractionGiven a chunk of text, identify the relationships among named entities (e.g. who is married to whom).Semantic parsingGiven a piece of text (typically a sentence), produce a formal representation of its semantics, either as a graph (e.g., in AMR parsing) or in accordance with a logical formalism (e.g., in DRT parsing). This challenge typically includes aspects of several more elementary NLP tasks from semantics (e.g., semantic role labelling, word-sense disambiguation) and can be extended to include full-fledged discourse analysis (e.g., discourse analysis, coreference; see Natural language understanding below).Semantic role labelling (see also implicit semantic role labelling below)Given a single sentence, identify and disambiguate semantic predicates (e.g., verbal frames), then identify and classify the frame elements (semantic roles).​Discourse (semantics beyond individual sentences)[edit]​Coreference resolutionGiven a sentence or larger chunk of text, determine which words (""mentions"") refer to the same objects (""entities""). Anaphora resolution is a specific example of this task, and is specifically concerned with matching up pronouns with the nouns or names to which they refer. The more general task of coreference resolution also includes identifying so-called ""bridging relationships"" involving referring expressions. For example, in a sentence such as ""He entered John's house through the front door"", ""the front door"" is a referring expression and the bridging relationship to be identified is the fact that the door being referred to is the front door of John's house (rather than of some other structure that might also be referred to).Discourse analysisThis rubric includes several related tasks. One task is discourse parsing, i.e., identifying the discourse structure of a connected text, i.e. the nature of the discourse relationships between sentences (e.g. elaboration, explanation, contrast). Another possible task is recognizing and classifying the speech acts in a chunk of text (e.g. yes-no question, content question, statement, assertion, etc.).Implicit semantic role labellingGiven a single sentence, identify and disambiguate semantic predicates (e.g., verbal frames) and their explicit semantic roles in the current sentence (see Semantic role labelling above). Then, identify semantic roles that are not explicitly realized in the current sentence, classify them into arguments that are explicitly realized elsewhere in the text and those that are not specified, and resolve the former against the local text. A closely related task is zero anaphora resolution, i.e., the extension of coreference resolution to pro-drop languages.Recognizing textual entailmentGiven two text fragments, determine if one being true entails the other, entails the other's negation, or allows the other to be either true or false.[24]Topic segmentation and recognitionGiven a chunk of text, separate it into segments each of which is devoted to a topic, and identify the topic of the segment.Argument miningThe goal of argument mining is the automatic extraction and identification of argumentative structures from natural language text with the aid of computer programs.[25] Such argumentative structures include the premise, conclusions, the argument scheme and the relationship between the main and subsidiary argument, or the main and counter-argument within discourse.[26][27]Higher-level NLP applications[edit]Automatic summarization (text summarization)Produce a readable summary of a chunk of text. Often used to provide summaries of the text of a known type, such as research papers, articles in the financial section of a newspaper.Grammatical error correctionGrammatical error detection and correction involves a great band-width of problems on all levels of linguistic analysis (phonology/orthography, morphology, syntax, semantics, pragmatics). Grammatical error correction is impactful since it affects hundreds of millions of people that use or acquire English as a second language. It has thus been subject to a number of shared tasks since 2011.[28][29][30] As far as orthography, morphology, syntax and certain aspects of semantics are concerned, and due to the development of powerful neural language models such as GPT-2, this can now (2019) be considered a largely solved problem and is being marketed in various commercial applications.Machine translation (MT)Automatically translate text from one human language to another. This is one of the most difficult problems, and is a member of a class of problems colloquially termed ""AI-complete"", i.e. requiring all of the different types of knowledge that humans possess (grammar, semantics, facts about the real world, etc.) to solve properly.Natural-language understanding (NLU)Convert chunks of text into more formal representations such as first-order logic structures that are easier for computer programs to manipulate. Natural language understanding involves the identification of the intended semantic from the multiple possible semantics which can be derived from a natural language expression which usually takes the form of organized notations of natural language concepts. Introduction and creation of language metamodel and ontology are efficient however empirical solutions. An explicit formalization of natural language semantics without confusions with implicit assumptions such as closed-world assumption (CWA) vs. open-world assumption, or subjective Yes/No vs. objective True/False is expected for the construction of a basis of semantics formalization.[31]Natural-language generation (NLG):Convert information from computer databases or semantic intents into readable human language.Book generationNot an NLP task proper but an extension of natural language generation and other NLP tasks is the creation of full-fledged books. The first machine-generated book was created by a rule-based system in 1984 (Racter, The policeman's beard is half-constructed).[32] The first published work by a neural network was published in 2018, 1 the Road, marketed as a novel, contains sixty million words. Both these systems are basically elaborate but non-sensical (semantics-free) language models. The first machine-generated science book was published in 2019 (Beta Writer, Lithium-Ion Batteries, Springer, Cham).[33] Unlike Racter and 1 the Road, this is grounded on factual knowledge and based on text summarization.Document AIA Document AI platform sits on top of the NLP technology enabling users with no prior experience of artificial intelligence, machine learning or NLP to quickly train a computer to extract the specific data they need from different document types. NLP-powered Document AI enables non-technical teams to quickly access information hidden in documents, for example, lawyers, business analysts and accountants.[34]Dialogue managementComputer systems intended to converse with a human.Question answeringGiven a human-language question, determine its answer. Typical questions have a specific right answer (such as ""What is the capital of Canada?""), but sometimes open-ended questions are also considered (such as ""What is the meaning of life?"").Text-to-image generationGiven a description of an image, generate an image that matches the description.[35]Text-to-scene generationGiven a description of a scene, generate a 3D model of the scene.[36][37]Text-to-videoGiven a description of a video, generate a video that matches the description.[38][39]General tendencies and (possible) future directions[edit]  Based on long-standing trends in the field, it is possible to extrapolate future directions of NLP. As of 2020, three trends among the topics of the long-standing series of CoNLL Shared Tasks can be observed:[40]Interest on increasingly abstract, ""cognitive"" aspects of natural language (1999–2001: shallow parsing, 2002–03: named entity recognition, 2006–09/2017–18: dependency syntax, 2004–05/2008–09 semantic role labelling, 2011–12 coreference, 2015–16: discourse parsing, 2019: semantic parsing).Increasing interest in multilinguality, and, potentially, multimodality (English since 1999; Spanish, Dutch since 2002; German since 2003; Bulgarian, Danish, Japanese, Portuguese, Slovenian, Swedish, Turkish since 2006; Basque, Catalan, Chinese, Greek, Hungarian, Italian, Turkish since 2007; Czech since 2009; Arabic since 2012; 2017: 40+ languages; 2018: 60+/100+ languages)Elimination of symbolic representations (rule-based over supervised towards weakly supervised methods, representation learning and end-to-end systems)Cognition and NLP[edit]Most higher-level NLP applications involve aspects that emulate intelligent behaviour and apparent comprehension of natural language. More broadly speaking, the technical operationalization of increasingly advanced aspects of cognitive behaviour represents one of the developmental trajectories of NLP (see trends among CoNLL shared tasks above).Cognition refers to ""the mental action or process of acquiring knowledge and understanding through thought, experience, and the senses.""[41] Cognitive science is the interdisciplinary, scientific study of the mind and its processes.[42] Cognitive linguistics is an interdisciplinary branch of linguistics, combining knowledge and research from both psychology and linguistics.[43] Especially during the age of symbolic NLP, the area of computational linguistics maintained strong ties with cognitive studies.As an example, George Lakoff offers a methodology to build natural language processing (NLP) algorithms through the perspective of cognitive science, along with the findings of cognitive linguistics,[44] with two defining aspects:Apply the theory of conceptual metaphor, explained by Lakoff as ""the understanding of one idea, in terms of another"" which provides an idea of the intent of the author.[45] For example, consider the English word big. When used in a comparison (""That is a big tree""), the author's intent is to imply that the tree is physically large relative to other trees or the authors experience. When used metaphorically (""Tomorrow is a big day""), the author's intent to imply importance. The intent behind other usages, like in ""She is a big person"", will remain somewhat ambiguous to a person and a cognitive NLP algorithm alike without additional information.Assign relative measures of meaning to a word, phrase, sentence or piece of text based on the information presented before and after the piece of text being analyzed, e.g., by means of a probabilistic context-free grammar (PCFG). The mathematical equation for such algorithms is presented in US patent 9269353: Where,RMM, is the Relative Measure of Meaningtoken, is any block of text, sentence, phrase or wordN, is the number of tokens being analyzedPMM, is the Probable Measure of Meaning based on a corporad, is the location of the token along the sequence of N-1 tokensPF, is the Probability Function specific to a languageTies with cognitive linguistics are part of the historical heritage of NLP, but they have been less frequently addressed since the statistical turn during the 1990s. Nevertheless, approaches to develop cognitive models towards technically operationalizable frameworks have been pursued in the context of various frameworks, e.g., of cognitive grammar,[46] functional grammar,[47] construction grammar,[48] computational psycholinguistics and cognitive neuroscience (e.g., ACT-R), however, with limited uptake in mainstream NLP (as measured by presence on major conferences[49] of the ACL). More recently, ideas of cognitive NLP have been revived as an approach to achieve explainability, e.g., under the notion of ""cognitive AI"".[50] Likewise, ideas of cognitive NLP are inherent to neural models multimodal NLP (although rarely made explicit).[51]See also[edit]  1 the RoadAutomated essay scoringBiomedical text miningCompound term processingComputational linguisticsComputer-assisted reviewingControlled natural languageDeep learningDeep linguistic processingDistributional semanticsForeign language reading aidForeign language writing aidInformation extractionInformation retrievalLanguage and Communication TechnologiesLanguage technologyLatent semantic indexingMulti-agent systemNative-language identificationNatural-language programmingNatural-language understandingNatural-language searchOutline of natural language processingQuery expansionQuery understandingReification (linguistics)Speech processingSpoken dialogue systemsText-proofingText simplificationTransformer (machine learning model)TruecasingQuestion answeringWord2vecReferences[edit]  ^ Kongthon, Alisa; Sangkeettrakarn, Chatchawal; Kongyoung, Sarawoot; Haruechaiyasak, Choochart (October 27–30, 2009). ""Implementing an online help desk system based on conversational agent"". Proceedings of the International Conference on Management of Emergent Digital Eco Systems - MEDES '09. MEDES '09: The International Conference on Management of Emergent Digital EcoSystems. France: ACM. p. 450. doi:10.1145/1643823.1643908. ISBN 9781605588292.^ Hutchins, J. (2005). ""The history of machine translation in a nutshell"" (PDF).[self-published source]^ Koskenniemi, Kimmo (1983), Two-level morphology: A general computational model of word-form recognition and production (PDF), Department of General Linguistics, University of Helsinki^ Joshi, A. K., & Weinstein, S. (1981, August). Control of Inference: Role of Some Aspects of Discourse Structure-Centering. In IJCAI (pp. 385-387).^ Guida, G.; Mauri, G. (July 1986). ""Evaluation of natural language processing systems: Issues and approaches"". Proceedings of the IEEE. 74 (7): 1026–1035. doi:10.1109/PROC.1986.13580. ISSN 1558-2256. S2CID 30688575.^ Chomskyan linguistics encourages the investigation of ""corner cases"" that stress the limits of its theoretical models (comparable to pathological phenomena in mathematics), typically created using thought experiments, rather than the systematic investigation of typical phenomena that occur in real-world data, as is the case in corpus linguistics. The creation and use of such corpora of real-world data is a fundamental part of machine-learning algorithms for natural language processing. In addition, theoretical underpinnings of Chomskyan linguistics such as the so-called ""poverty of the stimulus"" argument entail that general learning algorithms, as are typically used in machine learning, cannot be successful in language processing. As a result, the Chomskyan paradigm discouraged the application of such models to language processing.^ Goldberg, Yoav (2016). ""A Primer on Neural Network Models for Natural Language Processing"". Journal of Artificial Intelligence Research. 57: 345–420. arXiv:1807.10854. doi:10.1613/jair.4992. S2CID 8273530.^ Goodfellow, Ian; Bengio, Yoshua; Courville, Aaron (2016). Deep Learning. MIT Press.^ Jozefowicz, Rafal; Vinyals, Oriol; Schuster, Mike; Shazeer, Noam; Wu, Yonghui (2016). Exploring the Limits of Language Modeling. arXiv:1602.02410. Bibcode:2016arXiv160202410J.^ Choe, Do Kook; Charniak, Eugene. ""Parsing as Language Modeling"". Emnlp 2016. Archived from the original on 2018-10-23. Retrieved 2018-10-22.^ Vinyals, Oriol; et al. (2014). ""Grammar as a Foreign Language"" (PDF). Nips2015. arXiv:1412.7449. Bibcode:2014arXiv1412.7449V.^ Turchin, Alexander; Florez Builes, Luisa F. (2021-03-19). ""Using Natural Language Processing to Measure and Improve Quality of Diabetes Care: A Systematic Review"". Journal of Diabetes Science and Technology. 15 (3): 553–560. doi:10.1177/19322968211000831. ISSN 1932-2968. PMC 8120048. PMID 33736486.^ Winograd, Terry (1971). Procedures as a Representation for Data in a Computer Program for Understanding Natural Language (Thesis).^ Schank, Roger C.; Abelson, Robert P. (1977). Scripts, Plans, Goals, and Understanding: An Inquiry Into Human Knowledge Structures. Hillsdale: Erlbaum. ISBN 0-470-99033-3.^ Mark Johnson. How the statistical revolution changes (computational) linguistics. Proceedings of the EACL 2009 Workshop on the Interaction between Linguistics and Computational Linguistics.^ Philip Resnik. Four revolutions. Language Log, February 5, 2011.^ ""Investigating complex-valued representation in NLP"" (PDF).^ Trabelsi, Chiheb; Bilaniuk, Olexa; Zhang, Ying; Serdyuk, Dmitriy; Subramanian, Sandeep; Santos, João Felipe; Mehri, Soroush; Rostamzadeh, Negar; Bengio, Yoshua; Pal, Christopher J. (2018-02-25). ""Deep Complex Networks"". arXiv:1705.09792 [cs.NE].^ Socher, Richard. ""Deep Learning For NLP-ACL 2012 Tutorial"". www.socher.org. Retrieved 2020-08-17. This was an early Deep Learning tutorial at the ACL 2012 and met with both interest and (at the time) skepticism by most participants. Until then, neural learning was basically rejected because of its lack of statistical interpretability. Until 2015, deep learning had evolved into the major framework of NLP. [Link is broken, try http://web.stanford.edu/class/cs224n/]^ Yi, Chucai; Tian, Yingli (2012), ""Assistive Text Reading from Complex Background for Blind Persons"", Camera-Based Document Analysis and Recognition, Springer Berlin Heidelberg, pp. 15–28, CiteSeerX 10.1.1.668.869, doi:10.1007/978-3-642-29364-1_2, ISBN 9783642293634^ ""What is Natural Language Processing? Intro to NLP in Machine Learning"". GyanSetu!. 2020-12-06. Retrieved 2021-01-09.^ Kishorjit, N.; Vidya, Raj RK.; Nirmal, Y.; Sivaji, B. (2012). ""Manipuri Morpheme Identification"" (PDF). Proceedings of the 3rd Workshop on South and Southeast Asian Natural Language Processing (SANLP). COLING 2012, Mumbai, December 2012: 95–108.^ Klein, Dan; Manning, Christopher D. (2002). ""Natural language grammar induction using a constituent-context model"" (PDF). Advances in Neural Information Processing Systems.^ PASCAL Recognizing Textual Entailment Challenge (RTE-7) https://tac.nist.gov//2011/RTE/^ Lippi, Marco; Torroni, Paolo (2016-04-20). ""Argumentation Mining: State of the Art and Emerging Trends"". ACM Transactions on Internet Technology. 16 (2): 1–25. doi:10.1145/2850417. hdl:11585/523460. ISSN 1533-5399. S2CID 9561587.^ ""Argument Mining - IJCAI2016 Tutorial"". www.i3s.unice.fr. Retrieved 2021-03-09.^ ""NLP Approaches to Computational Argumentation – ACL 2016, Berlin"". Retrieved 2021-03-09.^ Administration. ""Centre for Language Technology (CLT)"". Macquarie University. Retrieved 2021-01-11.^ ""Shared Task: Grammatical Error Correction"". www.comp.nus.edu.sg. Retrieved 2021-01-11.^ ""Shared Task: Grammatical Error Correction"". www.comp.nus.edu.sg. Retrieved 2021-01-11.^ Duan, Yucong; Cruz, Christophe (2011). ""Formalizing Semantic of Natural Language through Conceptualization from Existence"". International Journal of Innovation, Management and Technology. 2 (1): 37–42. Archived from the original on 2011-10-09.^ ""U B U W E B :: Racter"". www.ubu.com. Retrieved 2020-08-17.^ Writer, Beta (2019). Lithium-Ion Batteries. doi:10.1007/978-3-030-16800-1. ISBN 978-3-030-16799-8. S2CID 155818532.^ ""Document Understanding AI on Google Cloud (Cloud Next '19) - YouTube"". www.youtube.com. Archived from the original on 2021-10-30. Retrieved 2021-01-11.^ Robertson, Adi (2022-04-06). ""OpenAI's DALL-E AI image generator can now edit pictures, too"". The Verge. Retrieved 2022-06-07.^ ""The Stanford Natural Language Processing Group"". nlp.stanford.edu. Retrieved 2022-06-07.^ Coyne, Bob; Sproat, Richard (2001-08-01). ""WordsEye: an automatic text-to-scene conversion system"". Proceedings of the 28th Annual Conference on Computer Graphics and Interactive Techniques. SIGGRAPH '01. New York, NY, USA: Association for Computing Machinery: 487–496. doi:10.1145/383259.383316. ISBN 978-1-58113-374-5. S2CID 3842372.^ ""Google announces AI advances in text-to-video, language translation, more"". VentureBeat. 2022-11-02. Retrieved 2022-11-09.^ Vincent, James (2022-09-29). ""Meta's new text-to-video AI generator is like DALL-E for video"". The Verge. Retrieved 2022-11-09.^ ""Previous shared tasks | CoNLL"". www.conll.org. Retrieved 2021-01-11.^ ""Cognition"". Lexico. Oxford University Press and Dictionary.com. Archived from the original on July 15, 2020. Retrieved 6 May 2020.^ ""Ask the Cognitive Scientist"". American Federation of Teachers. 8 August 2014. Cognitive science is an interdisciplinary field of researchers from Linguistics, psychology, neuroscience, philosophy, computer science, and anthropology that seek to understand the mind.^ Robinson, Peter (2008). Handbook of Cognitive Linguistics and Second Language Acquisition. Routledge. pp. 3–8. ISBN 978-0-805-85352-0.^ Lakoff, George (1999). Philosophy in the Flesh: The Embodied Mind and Its Challenge to Western Philosophy; Appendix: The Neural Theory of Language Paradigm. New York Basic Books. pp. 569–583. ISBN 978-0-465-05674-3.^ Strauss, Claudia (1999). A Cognitive Theory of Cultural Meaning. Cambridge University Press. pp. 156–164. ISBN 978-0-521-59541-4.^ ""Universal Conceptual Cognitive Annotation (UCCA)"". Universal Conceptual Cognitive Annotation (UCCA). Retrieved 2021-01-11.^ Rodríguez, F. C., & Mairal-Usón, R. (2016). Building an RRG computational grammar. Onomazein, (34), 86-117.^ ""Fluid Construction Grammar – A fully operational processing system for construction grammars"". Retrieved 2021-01-11.^ ""ACL Member Portal | The Association for Computational Linguistics Member Portal"". www.aclweb.org. Retrieved 2021-01-11.^ ""Chunks and Rules"". www.w3.org. Retrieved 2021-01-11.^ Socher, Richard; Karpathy, Andrej; Le, Quoc V.; Manning, Christopher D.; Ng, Andrew Y. (2014). ""Grounded Compositional Semantics for Finding and Describing Images with Sentences"". Transactions of the Association for Computational Linguistics. 2: 207–218. doi:10.1162/tacl_a_00177. S2CID 2317858.Further reading[edit]  Bates, M (1995). ""Models of natural language understanding"". Proceedings of the National Academy of Sciences of the United States of America. 92 (22): 9977–9982. Bibcode:1995PNAS...92.9977B. doi:10.1073/pnas.92.22.9977. PMC 40721. PMID 7479812.Steven Bird, Ewan Klein, and Edward Loper (2009). Natural Language Processing with Python. O'Reilly Media. ISBN 978-0-596-51649-9.Daniel Jurafsky and James H. Martin (2008). Speech and Language Processing, 2nd edition. Pearson Prentice Hall. ISBN 978-0-13-187321-6.Mohamed Zakaria Kurdi (2016). Natural Language Processing and Computational Linguistics: speech, morphology, and syntax, Volume 1. ISTE-Wiley. ISBN 978-1848218482.Mohamed Zakaria Kurdi (2017). Natural Language Processing and Computational Linguistics: semantics, discourse, and applications, Volume 2. ISTE-Wiley. ISBN 978-1848219212.Christopher D. Manning, Prabhakar Raghavan, and Hinrich Schütze (2008). Introduction to Information Retrieval. Cambridge University Press. ISBN 978-0-521-86571-5. Official html and pdf versions available without charge.Christopher D. Manning and Hinrich Schütze (1999). Foundations of Statistical Natural Language Processing. The MIT Press. ISBN 978-0-262-13360-9.David M. W. Powers and Christopher C. R. Turk (1989). Machine Learning of Natural Language. Springer-Verlag. ISBN 978-0-387-19557-5.External links[edit]   Media related to Natural language processing at Wikimedia Commons  showvteNatural language processing  Portal: Language  Authority control: National libraries IsraelUnited StatesJapanCzech Republic  Categories: Natural language processingComputational fields of studyComputational linguisticsSpeech recognition ​ "
What is the Future of Conversational AI? ,https://blog.naver.com/hyundai-autoever/222878661930,20220919,"Will there be a world where we can control all features of our homes and cars through voice command? ​Since the first appearance of ‘Alexa’, an artificial intelligence interactive service platform developed by Amazon in 2014, AI speakers have become  pervasive in our  lives. For instance, they can adjust room temperature, request songs from your favorite singer, ask the meaning of a word you are curious about, and much more. However, it still lacks speech recognition accuracy to be called as a perfect AI service.​Hyundai AutoEver is developing a conversational AI  platform to provide a multi-cluster environment, which can be applied to work environments and various devices such as vehicles and robots. Then how will AI technolgy enable communication with human beings in the near future? Let’s hear more details from Jaehong Eom, team leader of Hyundai AutoEver's AI technology team.​ ​  Please introduce your team and the work you are responsible for.​Good morning. I am Jaehong Eom, team leader of Hyundai AutoEver's AI technology team. Currently, our team consists of about 40 members and we conduct research and development on various AI topics including image recognition, speech recognition, data analysis, prediction of traffic information, and robots. I am in charge of the overall management of our  team.​​  I heard that you are an expert in the field of conversational AI service.Why did you choose AI as your expertise and what achievements have you made so far?​I went to graduate school and became interested in AI, so I decided to pursue a career in the field of AI machine learning.  During my Ph.D. course, I  researched how to select and extract valuable data with AI. For example, if a user inputs a text from the novel <Harry Potter> into a computer, a graph of Harry Potter’s relationships will be displayed. Based on the data, if a user asks, ‘Which one of Harry Potter's closest friends is the oldest?’, AI finds the most accurate answer to the question.​Just like the <Harry Potter> example, I thought about a field I would prefer to analyze AI data that can save time and I chose to participate in a medical project. It was a project tailored to the needs of doctors who wanted to check the latest research results based on thesis data. With the experience in this project, I was able to participate in the research and development of IBM Watson at the German National Institute of Basic Science and Technology. Then after returning to Korea, I joined the ‘Exobrain’ project and  developed Korea's first AI speaker ‘NUGU’ in 2016.​ ​​  What part did you take charge in developing an AI speaker?​Artificial intelligence sounds sophisticated but it is useless if you input the wrong data. Therefore, I'm thinking of ways to automate this. When developing an AI speaker, I was responsible for giving semantics to the data so that the AI can understand the context of the conversation. It can also be referred to as background knowledge. The most important part in the process of an AI recognizing the speech, processing it, and converting it back into a speech is to properly understand the meaning behind the user's words. When someone asks an AI for the word 'cookie', the result should be different when the user is a cook and when the user is a web designer. As such, background content is essential in order to retrieve the right answer, which was what I was reponsible for.  What conversational AI service does Hyundai AutoEver provide?​Hyundai AutoEver provides conversational AI services such as 'Voice Home' and AI chatbot. 'Voice Home' is a speech recognition application that can control air conditioning, indoor lighting, and etc. that is initiated by installing a device in the living room or elsewhere. Another service of Hyundai AutoEver is an AI chatbot, a program agent that provides answers to human questions. AI chatbots are most often used in call centers to increase work efficiency by increasing the number of customers per unit time. We are also developing speech recognition technology that can secure driver safety in collaboration with group affiliates based on  cooperated technology development and agreement.​ ​​  What kind of AI-related technology research is Hyundai AutoEver currently focusing on?​We are striving to complete a ‘self-developing AI’ through machine learning. Products like electronic devices are no longer smart after 10 years of purchase. However, if the embedded AI is capable of learning from data, the device will maintain at its highest performance as data accumulates over time and its software constantly upgrades to the newest version. Taking car as an example, if you buy a smart car with 80 functions, it will develop into a smart car with 100 functions a year later through firmware updates. ​The development of AI  can also be customized to the user. The more the user repeatedly uses the AI, the smarter the AI ​​becomes as it accumulates personalized data such as pronunciation and accent. This has been possible by the application of machine learning, a technology that allows computers to learn on their own to improve performance, which has emerged in AI technology since 2010. Hyundai AutoEver is also developing a self-learning AI  through the extensive renewal of previously developed conversational AI services such as 'Voice Home' and AI chatbot. In order to do so, cooperation between relevant business department and  speech recognition team is most important. Therefore, our team is encouraging to openly collaborate with other teams.​ ​​  What is the global trend of conversation AI services?​In the past, AI technology was developed according to its type such as video and audio. But today, multimodality is the new trend in which everything is integrated. Just because you hear the voice when you are talking doesn't mean that you close your eyes and listen to the sound, right? We make judgements based on the person's tone of voice, gestures, and facial expressions. Therefore, the global trend is to develop AI in a way that can acquire all data such as video and audio in order to understand the proper meaning and emotion of the speaker.​The latest version of an AI speaker has an embedded security camera for security reasons and recently, technology that uses this camera has been developed to understand the user’s command. For example, if a user points a hand gesture to the camera, it executes the command by reading the user’s lip shape. In a noisy environment, a voice saying ‘Please, turn down the music volume.’ will not be recognized. However, video recognition enables AI to execute command by recognizing gestures and lip shape of the user. Speech recognition is continuously developing and expanding to other fields as a multimodal through the utilization of video data. Also, in order to incorporate machine learning into products, we are working on lightening the weight of the hardware. Who knows someday we will be able to communicate with various embedded AI devices in the future?​​  ​How do you think conversational AI service will develop in the future?​I believe that AI will develop into a ‘digital human’ through multimodality. AI technology such as natural language processing, computer vision, speech recognition, voice synthesis, and data analysis of the body structure and gesture will complete the digital human. This will enable ‘real’ conversation between AI and human, which will be one of the main features of future conversational AI service.​For instance, instead of doing business on an ATM machine, a digital human will interact with the user and handle the business in the future. In a way, the most analog becomes the most advanced. The only difference is that the subject is an AI and not a human being. I believe that such advancement is due to the social characteristic of human beings. You probably would have seen a science fiction movie where the AI becomes a friend of a human being, which may come to reality soon.​ ​​  What is your long-term goal as a SW developer?​I believe that Hyundai AutoEver has a tremendous potential to grow as it is part of the Hyundai Motor Group. Hyundai Motor Group has a mobility platform that is getting increasingly important in the future and Hyundai AutoEver has the opportunity to develop various AI technology based on the platform. It is also a good environment to have research and development targets such as AI chatbots and 'Voice Home' that can further develop in the future. As Hyundai AutoEver is a flexible corporation that can expand its technology into various fields, I would like to take a role that can contribute to this corporation and to the career advancement of my fellow developers.​Personally, as software development becomes more important, it seems to be focused on coding education. However, we are entering an era where coding is automatically done, known as  ‘Software 2.0’. Therefore, I believe that product design and service design is essential in the previous stage before the actual coding. In this regard, I’m dreaming of nurturing talents with liberal arts and more broadly, general knowledge. I would like to create an environment where members from junior, middle, and senior level can make an achievement together. To this end, I will strive to make achievements in my projects first.​​   Out of interest, are there any AI-related movies that you recommend?​I have a keen interest in movies  with AI as the subject. Personally, the movie <Her> impressed me the most. It was nice to be able to see the situation when the computer OS learns by itself and exceeds a development tipping point through deep learning, an AI can be a great partner that can perfectly understand humans and help relieve loneliness. After all, AI has no emotions, so it was worth watching how the movie portrays the connection between the human and an AI. ​Another movie <Jexi>  is a story about the damage an AI can cause to mankind when it is malfunctioned. Malfunctioning AI is a terrifying situation from a developer’s point of view, which made it a little disappointing that the movie handled the topic only comically. Last but not least, <Free Guy>  is a movie that embodies the story in the metaverse that has become a hot topic recently. It was interesting to see that conversational AI services are positioned as an indispensable technology for NPCs to communicate with users.​ ​TEXT_ Dezign21​​​​​ ​ "
음성인식 인공지능 체험 ,https://blog.naver.com/sunghy21/223008417859,20230207,음성인식 인공지능 체험하기: STT와 TTS를 이용한 음성인식 인공지능 체험​​📚 이번 시간에는인공지능을 통해 컴퓨터가 사람의 말을 인식할 수 있음을 확인합니다.인공지능은 어떻게 사람의 말을 인식할 수 있는지 생각해 봅니다.이러한 기술을 응용하여 현재 만들어진 것은 무엇이 있는지 찾아봅니다.현재는 없지만 이러한 기술을 응용하여 앞으로 무엇을 만들 수 있는지 생각해 봅니다.​​🎓 수업 목표학생들의 인공지능 기술에 대한 호기심을 자극한다.학생들이 감각적 경험을 통해 음성 인식 기술에 대해 인지하도록 한다.음성 인식 기술이 제품이나 서비스의 형태로 우리의 생활에 어떻게 영향을 주고 있는지 생각하도록 한다.​​🌿 음성 인식 체험사용 도구인터넷 사용이 가능한 PC매직에꼴 STT/TTS 데모 페이지STT(Speech To Text)음성 인식 기술을 이용하여 사람의 음성을 문자로 바꿔주는 기술입니다.세상에는 다양한 언어가 있으므로 얼마나 많은 언어를 정확하게 문자로 바꿀 수 있는지가 STT 기술력의 핵심입니다.TTS(Text To Speech)음성 인식 기술을 이용하여 문자를 사람의 음성으로 바꿔 출력하는 기술입니다.STT와 마찬가지로 TTS도 얼마나 많은 언어를 정확하게 문자에서 음성으로 바꿀 수 있는지가 중요합니다.또한 최대한 실제 사람의 목소리와 비슷하게 출력하여 듣기에 불편함이 없도록 하는 것이 TTS 기술의 중요한 부분이라 할 수 있습니다.​음성 인식 체험URL 링크 : https://lab.magicecole.com/speech-recognition/​사용방법 :STT링크를 클릭하여 데모를 실행합니다.웹 브라우저에서 마이크 사용을 허용합니다.화면에서 [마이크] 버튼을 클릭합니다.마이크를 통해 여러가지 문장을 말해봅니다. 영어로도 말해 봅니다.영어 외에 다른 언어를 알고 있다면 그 언어로도 말해봅니다.화면에 자신이 말한 내용이 문자로 표시되는 것을 확인합니다.이번에는 마이크를 통해 예약어를 하나씩 읽어봅니다.예약어를 읽을 때마다 컴퓨터가 어떻게 동작하는지 확인합니다.​TTSSTT를 통해 문자를 입력하고 화면에 있는 [Text to speech] 버튼을 클릭합니다.화면에 나타나 있는 문자열이 음성으로 출력되는 것으로 확인합니다.​​생각해보기 :STT는 어떻게 우리의 말을 문자로 바꿔줄 수 있는지 상상해 봅시다.STT와 TTS를 이용하면 우리가 무엇을 할 수 있을지 생각해 봅시다.TTS의 성능을 더 좋게 하려면 무엇이 필요한지 생각해 봅니다.  ✍️ 정리하기​🙄 체험 내용 정리음성인식 인공지능 기술 체험한 내용에 관하여 정리해주세요.체험한 음성인식 기술에 대하여 써주세요.음성인식 인공지능 체험 내용 중에서 가장 재미있었던 것에 대하여 적어주세요.가장 재밌다고 생각한 이유를 적어주세요.음성인식 인공지능 체험 내용 중에서 가장 별로라고 생각한 것에 대하여 적어주세요.가장 별로라고 생각한 이유를 적어주세요.​🤔 자기 생각 정리음성인식 인공지능 기술을 체험 후 어떤 생각을 하게 되었는지 적어주세요.음성인식 인공지능을 사용하면 좋은 것이 무엇일까 짧게 생각을 적어주세요.음성인식 인공지능이 널리 이용되는 미래는 어떤 모습 일까요? 내용을 적어주세요. 
화자인식 기초 #4 - eigenvoice에서 i-vector까지 ,https://blog.naver.com/kwh1990/222132707946,20201101,"지난번 글에서는 서로 길이가 다른 음성들을 비교하기 위하여 GMM supervector를 활용하는 방안을 소개하였다. 확실히 쉽고 효과적인 방법이긴 하지만, GMM supervector에는 2가지 큰 문제점이 존재한다:--> 차원이 너무 크다: GMM supervector는 GMM mixture의 mean vector들을 연결시켜서 획득되기 때문에 그 크기는 (acoustic feature 차원)×(GMM mixture 수)로 결정된다.예를 들어 60차원 MFCC를 사용하고 1024 mixture GMM을 학습하면, supervector의 크기는 61,440이 된다.입력 벡터의 차원이 크면 인식 모델의 학습이 어려워지는 등 다양한 문제를 야기할 수 있다. (curse of dimensionality)--> 화자 이외의 정보가 너무 많다: GMM은 단순히 frame-level feature의 분포를 표현하기 때문에 화자 이외의 화자 인식에는 다소 불필요할 수 있는 정보들이 모두 포함되어있다.E.g., recording channel, background noise, voice stress, language, 등위에 소개된 GMM supervector의 고질적인 문제들을 해결하기 위하여 다양한 기법들이 연구되어왔으며, 대략 순서대로 나열한다면 아래와 같다:   <Eigenvoice adaptation>본래 eigenvoice adaptation은 speaker verification이 아니라 speech recognition 시스템을 샘플 수가 적은 특정 화자에 대하여 adaptation시키기 위하여 제안된 기법이다.Eigenvoice adaptation의 기본적인 아이디어는: GMM supervector에 있는 화자와 관련있는 variability (분포가 퍼져있는 정도, 즉 speaker variability는 speaker information이 speech 분포에 미치는 영향이라 생각하면 된다)을 GMM supervector보다 작은 dimension의 subspace로 projection시키는 것이다.수식으로 나타내면 아래와 같다: 위 수식에서 볼 수 있듯, eigenvoice adaptation에서는 GMM supervector와 UBM supervector (speaker-independent GMM) 사이의 차이가 eigenvoice matrix라고 하는 speaker subspace와 이에 projection되는 latent variable로 factorize된다고 가정한다.여기서 만약 GMM supervector가 특정 화자의 음성을 이용하여 얻어졌다면, GMM supervector와 UBM supervector (다량의 화자 음성으로부터 학습된 특정 화자와 연관되지 않은, 전반적인 인간의 발음 분포를 표현하고 있다고 가정하는 GMM)은 해당 화자의 variability라고 할 수 있으며, 이를 factorize하여 얻어지는 low-dimensional latent variable은 해당 화자의 variability를 표현하는 speaker factor라고 할 수 있다.다만 실제로 speaker factor 및 eigenvoice matrix를 구할때는 GMM supervector를 직접 구하지 않는다. 어디까지나 가정을 저렇게하는거지, 학습과정에서는 UBM이 주어져있을때 GMM supervector의 likelihood가 maximize될 수 있는 eigenvoice matrix를 EM 알고리즘으로 학습한다.    <Joint factor analysis (JFA)>Eigenvoice 기법에서 한 술 더 떠서 GMM supervector의 variability를 speaker 및 channel factor로 나누어 표현하는 기법이다.수식으로 나타내면 아래와 같다: 위 수식에서 볼 수 있듯 eigenvoice와 유사하게 speaker subspace와 channel subspace가 정의되며, 학습 역시 eigenvoice와 마찬가지로 EM 알고리즘을 통해 진행된다. 다만 eigenvoice와 달리 speaker 뿐 아니라 channel 및 residual factor가 존재하기 때문에 한번에 학습하는 것이 아니라 speaker subspace를 먼저 학습 한 후 channel subspace, residual을 학습하게 된다.JFA는 channel에 대한 정보를 따로 모델링함으로써 기존 기법들에 존재했던 channel variability를 완화시켜 기존 기법들에 비하여 높은 성능을 보였다.하지만 JFA에 한가지 큰 성능 한계가 존재하였다: 화자에 대한 정보가 배제되어있어야하는 channel factor에 speaker-dependent information이 남아있다는 것이 발견된 것이다. 이는 speaker factor만을 이용해서는 음성 내에 있는 화자 정보를 최대로 활용할 수 없다는 것을 의미하며, 이러한 한계를 극복하기 위하여 이후 설명할 i-vector가 제안되었다.   <I-vector>I-vector framework는 speaker와 channel factor를 따로 모델링하여 생길 수 있는 화자 정보 누수를 막기 위해 아주 단순한 해결책을 제시한다: speaker, channel, 등 모든 variability를 모두 하나의 subspace로 표현하는 것이다. 이렇게 음성 내에 있는 온갖 정보를 비교적 작은 차원의 subspace (total variability subspace)에 표현하면 채널과 같이 화자와 전혀 상관없는 정보들이 혼재할 수밖에 없지만, speaker-dependent information도 보존되어있을 것이다. 이런 이유로 인하여 i-vector framework는 total variability modeling으로도 불리운다. 수식으로 나타내면 아래와 같다: 위 수식에서 느낄 수 있듯 앞서 나온 eigenvoice 기법과 거의 똑같다. 다만 한가지 다른 것은 학습 화자들에 대하여 modeling한 eigenvoice와는 달리 i-vector 기법에서는 학습 utterance들에 대하여 모델링한다.  쉽게 설명하자면 i-vector의 total variability matrix를 학습하는 과정은 eigenvoice에서 speaker subspace를 학습할때 서로 다른 utterance를 다른 speaker로 가정하는 경우와 동일하다.Total variability subspace로 projection된 i-vector는 GMM supervector보다 작은 dimensionality를 가지므로 후처리를 하기에 용이하다 (dimension이 큰 경우 간단한 feature compensation도 계산량이 크다)I-vector에는 앞서 계속 언급되듯 speaker 외 다양한 정보가 있기에, 높은 화자 인식 성능을 위해서는 speaker 이외 정보를 최대한 제거해야 한다. 다행히도 i-vector는 dimension이 GMM supervector 처럼 무지막지하게 크지 않기 때문에 다양한 feature compensation 기법들을 이용하여 nuisance variability를 제거할 수 있다. (e.g., LDA, WCCN, NAP) I-vector 기법은 제안된 이후 괜찮은 성능으로 인해 널리 연구 및 활용되었고, 요즘도 간간히 관련 논문이나 기사가 나오는 것을 볼 수 있다. 다만, 기존 embedding 기법들에도 존재하는 짧은 음성에서의 성능 저하 및 GMM이라는 모델 구조에 대한 의존성 등 여러 한계점이 있으며, 대량의 데이터 (e.g., VoxCeleb)에서는 간단한 deep learning 모델을 이용하여 추출한 speaker embedding이 더 좋은 성능을 보인다는 것이 검증되었다. (단, 대부분 deep learning 모델들은 speaker label을 이용하여 학습하는 supervised training인데 비해 i-vector는 speaker label을 전혀 활용하지 않는 unsupervised training으로 학습되기에 공평한 비교라고는 보기 힘들다) 어쨌든 재밌는 알고리즘이고 돌려보려면 이곳저곳에서 개발한 툴들이 많아서 쉽게 활용할 수 있다:MSR identity toolbox AlizeKaldiBob.bio.spear   논문 작업 및 과제 업무로 인해서 미루고 미루다가 아주 오랜만에 글을 올렸다. 본 글에서 언급된 i-vector의 한계를 극복하기 위한 다양한 논문들이 나오긴 하였지만, 최근 몇년간은 deep learning 기반의 speaker embedding 기법들에 대한 연구가 압도적으로 더 많이 이루어졌기에 GMM factorization 기반의 embedding 기법 이야기는 여기까지 쓸까한다. 다음 글을 언제 쓸지는 모르겠지만 아마 d-vector부터 시작해서 (그 이전에도 deep learning 기반의 embedding 기법이 제안되긴 하였지만 성능이 그닥 좋지 않거나 i-vector의 연장선에 있는 기법들이 많았다) end-to-end speaker verification system까지 나누어 설명하지 않을까 싶다. 참고자료:1. P. Kenny, et al., ""Eigenvoice modeling with sparse training data,"" IEEE Transactions on Speech and Audio Processing, vol. 13, no. 3, 2005.2. P. Kenny, et al., ""Joint factor analysis versus eigenchannels in speaker recognition,"" IEEE Transactions on Audio, Speech, and Language Processing, vol. 15, no. 4, 2007.3. N. Dehak, et al., ""Front-end factor analysis for speaker verification,"" IEEE Transactions on Audio, Speech, and Language Processing, vol. 19, no. 4, 2010.4. J. Hansen and T. Hasan, ""Speaker recognition by machines and humans: a tutorial review,"" IEEE Signal Processing Magazine, vol. 32, no. 6, 2015. "
로봇-사용자 인터페이스의 기술 동향 ,https://blog.naver.com/isaict/221135330609,20171108,"1. HRI (Human-Robot Interaction)이란?​최근 로봇 기술이 빠르게 발전함에 따라 단순 반복 작업이 요구되는 산업현장에서 주로 활용되던 로봇들이  인간과 능동적으로 교감 할 수 있는 형태로 개발되면서 인간이 생활하는 환경으로 들어오고 있다. [그림 1]과 같은 소프트뱅크의 로봇 ‘페퍼(Pepper)’를 시작으로 MIT의 지보(JIBO), 에이수스의 젠보(Zenbo)와 같은 로봇들이 그 예이다.  인간과 능동적으로 교감 할 수 있는 로봇들이 폭발적으로 개발되고 사회적인 주목을 받게 되기 시작하면서  ‘소셜 로봇’이라는 새로운 단어까지 탄생했으며 로봇에 대한 사람들의 인식 또한 과거 제조업 현장에서 사용되는 산업용 로봇의 이미지로부터 사람들과 소통하고 능동적으로 교감하는 로봇의 이미지로 변화해 가고 있다.​ © askkell, 출처 Unsplash​로봇이 인간이 생활하는 환경으로 들어오게 되면서 인간 환경과 로봇 환경의 구분이 허물어지게 되고 로봇이 인간과 공존하는 것이 점점 자연스러워지고 있다. 그에 따라 인간, 로봇 그리고 각 개체가 서로 영향을 주는 방식을 연구하는 학문인 인간-로봇 상호작용, HRI(Human-Robot Interaction)가 새롭게 등장하였다. 본 글에서는 이러한 HRI의 연구 동향과 미래에 대해 살펴보고자한다.​​2-1. 컨트롤러를 이용한 로봇 제어 인간이 컨트롤러를 사용하여 로봇을 제어하는 것은 가장 전통적으로 로봇을 제어하기 위해 사용되어져 왔던 방법이다. 일상생활에서 흔히 볼 수 있는 스마트폰, 조이스틱, 키보드 등의 다양한 컨트롤러를 이용하여 직접 작업 수행 시 로봇의 동작 또는 움직임을 제어 할 수 있다. 사용자가 직접적으로 로봇을 원격으로 제어하기 때문에 작업자가 능동적으로 불규칙한 환경에서 High-level Decision을 할 수 있다는 장점이 있다. ​ ▲Ashish U.Bokade의 연구에 사용된 사륜구동 로봇​대표적인 연구로는 Ashish U. Bokade가 안드로이드 스마트폰과 라즈베리 파이를 이용하여  [그림1]과 같은 로봇을 컨트롤 하는 방법을 제안한 연구가 있다.​2-2. 직접 교시 (Direct Teaching)사람사이에도 한 사람이 물리적인 접촉을 가해 다른 사람의 작업 수행 시 위치나 자세를 조정하여 교시할 수 있듯이 로봇에게도 물리적인 접촉을 통해 로봇의 움직임이나 자세를 변화시킴으로 작업 수행에 필요한 동작을 교시할 수 있다. 이를 직접교시라고 하며 직접교시는 인간과 로봇이 공존하여 작업하는 환경에서  효율적이며 안전한 인터페이스이다.​ ​대표적이 연구로는 Sang-Duck Lee가 로봇의 유연한(compliant) 메니퓰이터에 물리적인 접촉을 가해 교시하면 사람이 힘을 가한 방향으로 메니퓰레이터가 움직이는 방식으로 로봇을 제어하였다 [동영상 1].​2-3.  언어적 의사소통  (Verbal-Communication)사람과 사람 사이에서 말을 통해 주로  의사소통을 하듯이 사람에게 말을 하듯이 로봇에게 명령을 주는 인터페이스가 연구되고 있다. 실제로 음성 인식 시스템(Speech recognition system)이 발전함에 따라 로봇이 사람의 음성 명령을 인식하여 작업을 로봇 스스로가 판단하여 수행하는 방식으로 연구가 진행되고 있다.​ - YouTubeyoutu.be ​대표적인 연구로는 Matthias Scheutz는 인간이 일상의 대화에서 사용하는 언어 (natural language dialogue) 로부터 로봇이 수행해야할 작업을 스스로 판단할 수 있게 하는 음성 인식 시스템 (Speech recognition system)을 로봇 플랫폼에 적용하였다. 위의 [동영상 2]는 모바일 메니퓰레이터가 사람의 음성 명령에 따라 반응하여 작업을 수행하는 영상이다,​3. 글을 마치며.​지금까지 HRI(Human-Robot Interaction)이 무엇이며 연구 동향들에 대해 알아보았다.최근들어 로봇이 일상생활에도 상용화되기 시작하면서 로봇 기술 중에서도  HRI의 중요도가 높아지게 되었다. HRI의 중요도가 높아지게 됨에 따라 우리 차세대융합기술연구원의 디지털 휴먼센터 박재흥 교수팀에서도 스마트폰을 이용한 휴머노이드의 보행 제어 인터페이스를 개발중에 있다. 차세대융합기술연구원의 박재흥 교수팀에서 개발되는 HRI 기술을 통해 로봇이 우리들의 실생활에서 함께 할 그날이 하루 빨리 다가오길 기대해 본다.​​​​글쓴이: 김준형 석사과정(john3.16@snu.ac.kr)소속: 지능형 융합전공관심분야: 휴머노이드 보행 제어, HRI​​​​​​​​​ ​ "
Input a local file or url and this service will transcribe it using Whisper AI ,https://blog.naver.com/mssixx/223029791779,20230227,USES WHISPER AI​Input a local file or url and this service will transcribe it using Whisper AI into subtitle files and raw text. ​Uses whisper AI so this is state of the art translation service - completely free.  ​ GitHub - zackees/transcribe-anything: Input a local file or url and this service will transcribe it using Whisper AIInput a local file or url and this service will transcribe it using Whisper AI - GitHub - zackees/transcribe-anything: Input a local file or url and this service will transcribe it using Whisper AIgithub.com Whisper is a general-purpose speech recognition model. ​It is trained on a large dataset of diverse audio and is also a multi-task model that can perform multilingual speech recognition as well as speech translation and language identification.​ GitHub - openai/whisper: Robust Speech Recognition via Large-Scale Weak SupervisionRobust Speech Recognition via Large-Scale Weak Supervision - GitHub - openai/whisper: Robust Speech Recognition via Large-Scale Weak Supervisiongithub.com 
"[뷰노]뷰노 AI 기반 의료 음성인식 솔루션, 서울아산병원에 도입 /이지메디컴, 카리스,솔루엠,솔젠트,메디셀,이노그리드,비비비,뷰노,화성비엔텍,울트라브이 ",https://blog.naver.com/kkbs6161/222124185475,20201023,"뷰노 AI 기반 의료 음성인식 솔루션, 서울아산병원에 도입 prev next​뷰노 AI 기반 의료 음성인식 솔루션, 서울아산병원에 도입 prev[2020 국감] ""상반기 개인 해외파생 손실 8,800억…투자 보호 절실""‘27일 컴백’ 폴킴, 새 싱글 ‘너도 아는’ 40초 티저 공개…애절한 보이스 예고next​ 의료 인공지능(AI) 솔루션 개발 기업 뷰노는 자사의 인공지능 기반 의료 음성인식 소프트웨어 뷰노메드 딥ASR™(VUNO Med®-DeepASR™)이 서울아산병원에 도입됐다고 23일 밝혔다.서울아산병원의 영상의학과에서는 매년 50만 건 이상의 X-ray, CT, MR 등 의료 영상 검사가 시행된다.​이번 뷰노메드 딥ASR™ 도입으로 대량의 의료 영상 판독을 진행하는 영상의학과 의료진들은 국내 임상 환경에 최적화된 자동 음성인식(Automatic Speech Recognition, 이하 ASR) 기술을 활용해 영상 판독문 작성 시간을 단축하는 등 의료 문서 작성 효율성을 향상시킬 수 있을 것이란 기대가 나온다.​기존 의료 영상 판독문 작성은 먼저 의료진이 영상 판독 내용을 녹음하여 음성파일로 저장하고, 이를 전사자가 청취해 녹음 내용을 입력한 후 의료진의 검증 단계를 거쳐야 하는 등 복잡한 프로세스로 진행돼, 판독문 작성에 많은 시간이 소요된다는 어려움이 있었다.​뷰노메드 딥ASR™은 의료 현장에서 의료진이 구두로 전달하는 판독 내용을 실시간으로 문서화하거나, 녹음된 의료진의 음성파일을 판독문으로 변환함으로써, 기존 판독 과정을 획기적으로 개선해 영상의학 전문의와 전사자들의 의료 문서 작성 업무 효율성을 향상시킨다는 평가다.​뷰노메드 딥ASR™은 뷰노 자체 개발 딥러닝 엔진 뷰노넷(VUNO Net)을 기반으로 개발돼, 독보적인 수준의 음성 인식 성능을 보유한 것이 특징이다.​일반 오픈소스 엔진 대비 빠른 학습 속도로 동일 시간 내 더 많은 양의 데이터 학습이 가능하며, 수천 시간에 달하는 국내 의료 영상 판독 데이터 수십만 건을 학습해 국영문이 혼재된 의학용어를 처리하는 등 국내 의료 환경에 최적화됐다.​뿐만 아니라, 의료진의 영상 판독 결과를 포함한 환자 기본정보, 생체신호 등 다양한 항목을 음성인식으로 입력 가능하며, 영상전송시스템(PACS), 전자의무기록(EMR) 등 다양한 전자 의료 시스템에 탑재가 가능해 각 의료기관의 환경에 따라 설정할 수 있어 보다 사용 편의성이 높은 것이 특징이다.​서울아산병원 영상의학과 관계자는 “이번 뷰노메드 딥ASR™ 도입을 통해 영상 판독 업무 효율화를 도모함과 동시에 본 원의 의료진들의 업무 부담을 덜어 의료서비스 향상으로 이어질 것으로 기대한다”며 “대한민국 의료 선도 병원으로서 앞으로도 다양한 혁신적인 기술을 적극적으로 활용, 최상의 의료 서비스를 제공할 수 있도록 하겠다”고 말했다.​김현준 뷰노 대표는 “세계적인 의료 수준을 자랑하는 서울아산병원 내 판독문 작성 과정에 세계 최고 수준의 음성 인식 기술이 집약된 뷰노메드 딥ASR™이 활용돼 매우 기쁘게 생각한다”며 “앞으로도 뷰노는 독보적인 기술과 사용 편의성을 적용한 인공지능 기반 의료 음성 솔루션을 후속 개발 및 보완해 의료 현장의 프로세스 효율성을 제고할 수 있도록 노력하겠다”고 말했다.​한편, 2019년 출시된 뷰노메드 딥ASR™은 국내 주요 중·대형 병원에 도입되돼 의료진들의 다양한 의료 문서 작업 효율화 및 의료행정 디지털화를 도모하고 있다.​해당 솔루션에 적용된 인공지능 음성인식 기술은 세계적으로 저명한 음성 및 신호처리 분야 학회인 국제음향음성신호처리학회(ICASSP) 학회지에 소개됨으로써 우수한 기술력을 입증한 바 있다.​ ​#월마커바이오 #솔젠트 #엔게인 #엠비지#카리스 #크래프톤 #멈스 #에스엠티바이오#퓨쳐메디신 #온페이스 #온페이스게임즈#메디오젠 #케마스 #젠바디 #제이시스메디칼#지아이이노베이션 #메디셀 #필로시스 #큐라티스#비비비 #이노그리드 #넥스틴 #뷰노 #프로젠​ "
안면 마스크가 어린이와 성인의 음성 인식에 미치는 영향 ,https://blog.naver.com/jahearing/222993811430,20230125,"안녕하세요.필립스보청기 춘천센터 청능사 원장 윤보한입니다.​명절은 잘 보내셨나요? 설이 지나자마자 엄청난 추위가 시작되었네요. 이번 추위도  잘 이겨내세요!!​오늘은 안면 마스크가 어린이와 성인의 음성 인식에 미치는 영향에 대한 저널 한 편을 소개해 드리겠습니다. 페이스 마스크 또는 안면 마스크라고 정의하는 것은 마스크의 여러 종류를 하나로 묶기 위함이니 참고하고 읽어주시면 되겠습니다.​​ ​​ 목표본 연구는 다양한 안면 마스크가 학령기 아동과 성인의 단어 인식에 미치는 영향에 대해 알아보고자 합니다.​​ ​​ 설계어음인지역치(Speech Recognition Threshould; SRT) 값은 검사 단어를  그림에서 고르도록 하는 방법을 사용하였습니다. 검사에 사용된 대상 단어는 다섯 가지 조건에서 여성 화자가 녹음하였습니다. (다섯 가지 조건 : 마스크 없이 녹음, 투명 마스크, 안면 보호대, N95 마스크, 수술용 마스크를 착용 후 녹음)​​ ​​ 연구 샘플연구에 참여한 대상자는 정상 청력을 가진 30명의 어린이(8세~12세)와 성인 25명(18세~25세)이었습니다.​​ 결과어린이와 성인 모두 단어 인식에서 안면 보호대가 가장 부정적인 영향을 미쳤습니다. 투명 마스크 또한 어린이들의 인지에 좋지 못한 영향을 미쳤습니다. 두 연령대 모두 N95 마스크와 수술용 마스크에  대해서는 부정적인 영향이 관찰되지는 않았습니다.​​ ​​ 결론젊은 성인과 마찬가지로 학령기의 어린이들은 음성을 인식할 때 안면 마스크에 부정적인 영향을 받지만 그 효과는 착용하는 안면 마스크의 유형에 따라 달라집니다. 음향 분석에 따르면 마스크에 사용되는 반사 물질(필름 등)은 음성 신호 품질에 여향을 미치고 단어 인식에 좋지 못한 영향을 줍니다.​​간단하게 저널 요약본을 살펴보았는데요, 우리가 흔히 사용하는 KF94 등의 마스크는 정상 청력을 가진 사람에게는 말소리를 인식하는 데 어려움을 주거나 하지는 못합니다. 물론 청력이 좋지 못한 난청인들에게는 좋지 못한 영향을 주는 것이 사실입니다. 가까운 곳에서 접하기 쉬운 필름으로 제작된 마스크나 안면 쉴드의 경우 소리를 전달하는데 어려움이 있는 것 같습니다. 입모양을 보고 추정하는 것에 도움을 주기는 하나 음성 품질은 다소 좋지 못하기 때문이죠. 말소리를 전달함에 있어 방해가 되는 부분은 다른 대안으로 도움을 줄 수도 있습니다. ​​​ 난청 인식_마스크안녕하세요. 청능사 제리입니다. 새해 인사가 많이 늦었네요. 여러분 새해 복 많이 받으세요. 오늘은 간단...blog.naver.com ​​얼마 뒤부터 실내에서도 마스크의 착용이 의무는 아니라고 합니다. 그래도 당분간 습관처럼 마스크를 사용할 것 같네요. ​감사합니다.​ International Journal of AudiologyAn international journal publishing research into audiology and hearing sciences including psychoacoustics, hearing impairment and rehabilitation devices. www.tandfonline.com ​ ​ 지정된 주차장을 이용해 주세요.주차권을 제공해 드립니다.춘천시청공영주차장강원도 춘천시 옥천동새명동공영주차장강원도 춘천시 시청길12번길 9춘천지하상가주차장입구강원도 춘천시 조양동 ​ 필립스보청기 춘천센터시그니아 렉스톤 본사 교육팀장 출신이 직접 상담합니다. 필립스보청기는 오직 필립스 히어링 솔루션 전문센터에서만 상담할 수 있습니다. 필립스 인증 프리미엄 우수센터에서 보청기 상담부터 관리까지 원스톱 시스템으로 편리하게 이용해보세요.philips-hearing-chuncheon.kr ​ "
"[이렇게 사용하세요!]TTS의 무한 진화, ‘Clova Premium Voice’ 활용사례   ",https://blog.naver.com/n_cloudplatform/221826222867,20200226,"​안녕하세요, 네이버 클라우드 플랫폼입니다.​일상에서 들리는 다양한 소리 중에서 사람의 녹음된 목소리가 아닌 TTS(Text To Speech)기술을 활용한 합성 음성이 많이 사용되고 있다는 사실, 알고 계셨나요? ​지난 포스팅에서는 사람보다 더 사람같은 합성 음성을 제공하는 Clova Premium Voice(CPV) 서비스에 대해 소개해 드렸습니다.​[지난 포스팅 바로가기]​CPV는 네이버 Clova의 End-to-End 음성 합성 기술에 고품질 Neural Vocoder를 적용한 합성음성 서비스로서, 기존의 딱딱한 기계음과는 달리 기쁘거나 슬픈 감정을 포함해 문맥을 고려하여 텍스트를 전달하고, 사람의 목소리처럼 자연스러운 음성을 제공합니다. ​  이번 포스팅에서는 일상 속에서 다양하게 사용되고 있는 CPV의 활용 사례 및 사용법을 소개합니다. CPV 음성 “아라”를 실생활 속에서 어떻게 사용할 수 있을지 확인해보세요!​  1. CPV 활용사례언제 어디서나, 클로바 프리미엄 보이스클로바 프리미엄 보이스는 TTS기술이 사용되고 있는 모든 영역에 활용될 수 있으며, 상황에 맞는 최적화된 음성으로 우리의 일상생활 어디에서나 자연스럽게 사용될 수 있습니다.   ​■""CPV 아라""의 음성으로  IoT기기를  더 친근하게 [네이버 AI스피커, 웨이브]​네이버 클로바 프렌즈, 웨이브와 같이 우리 주변에서 많이 접할 수 있는 AI 스피커와 다양한 IoT기기의 음성 역시 CPV를 활용할 수 있는 대표적인 사례입니다. 일상에서 자주 사용하는 AI 스피커는 딱딱한 기계이 아닌, 듣기 좋으면서 자연스러운 음성이 매우 중요하며 사용자에게 친근하고 다양한 목소리를 제공하는 것이 필수적입니다. ​■세상의 모든 안내방송/ 안내예약, ""CPV 아라""와 함께 [아웃백 미금점 AICALL 도입 사례]지하철 안내방송 목소리 역시 실제 사람의 음성의 목소리가 아니었다는 사실, 알고 계셨나요? 사내 안내 방송을 성우가 직접 하지 않아도 합성음성을 통해 전달하고자 하는 텍스트를 더욱 쉽게 전달할 수 있으며, CPV를 통해 더욱 사람에 가까운 음성을 전달할 수 있어 다양한 비즈니스에 적용할 수 있습니다.      ​■뉴스/책 등 다양한 컨텐츠를 대신 읽어주는 ""CPV 아라"" [출처 : 게티스이미지뱅크]E-book을 포함한 다양한 컨텐츠의 텍스트를 들을 수 있는 서비스들이 점차 늘어나고 사용자도 증가하고 있는데요, 뉴스를 읽거나 동화책을 읽을 때, 단순히 딱딱한 기계음으로 듣는 것보다 문맥에 맞추어 더욱 자연스러운 음성을 듣는다면 더욱 몰입할 수 있지 않을까요?    ​■챗봇 서비스 및 음성 인식 기술도 CPV 음성과 연동해 시너지 효과 창출 [아지냥이 – 대한민국 1등 반려동물 앱]네이버 클라우드 플랫폼의 챗봇과 CSR(Clova Speech Recognition)을 적용한 반려동물 앱 “아지냥이”역시 추후 CPV를 적용할 계획을 가지고 있는데요, 이와 같이 STT(Speech To Text), TTS(Text To Speech)를 연계한 AI 비서 서비스에도 CPV 적용이 가능합니다.​이 외에도 금융 환경에서의 불완전 판매 모니터링, AI Contact Center 등 기존 콜센터에서 인력으로 수행하는 업무들을 효율적으로 변환하며 음성 합성 기술을 활발히 사용할 수 있습니다. ​​■영상 크리에이터들도 이제 CPV로 더 쉽게! [클로바 더빙: https://clovadubbing.naver.com/]​영상 크리에이터들도 더 이상 본인의 목소리를 사용하지 않고 CPV를 활용하여 영상 음성을 손쉽게 만들어낼 수 있습니다. 자연스러운 음성으로 더욱 빠르고 편리하게 영상을 완성해보세요!  ​​이 외에도 CPV는 현재 TTS기술이 사용되고 있는 모든 분야에 활용할 수 있으며, 합성음성에 개성과 감정을 표현할 수 있는 CPV는 상황에 따른 최적화된 음성을 제공하여 더 넓은 영역에서 활용할 수 있습니다. ​그럼 이제부터 ""CPV를 사용할 수 있는 방법""을 알려드리겠습니다.     2. CPV 사용법개발자 / 비개발자 모두 쉽게 사용하는 CPV네이버 클라우드 플랫폼의 CPV는 개발자 친화적인 웹 콘솔을 제공하며, 개발자가 아니더라도 서비스를 사용해보고 싶은 분들도 사용하기 쉬운 웹 페이지를 따로 제공하고 있습니다. 네이버 클라우드 플랫폼 콘솔을 활용한 CPV 사용법과 직접 CPV 음성을 직접 들어볼 수 있는 간단한 방법, 총 2가지 사용법을 소개합니다.​ 첫번째, 개발자라면?+API 사용법 ncloud.com 콘솔을 활용한 CPV 서비스 사용 (+영상 포함)개발자라면 네이버 클라우드 플랫폼 콘솔에 접속하여  세 단계를 거치면 바로 사용 가능합니다. [STEP 1] 네이버 클라우드 플랫폼 회원가입 후 로그인하기 (+무료 크레딧받기)[STEP 2]     콘솔에서 CPV 서비스 신청 후 고유 ID 및 비밀번호 확인[STEP 3] 사용자가이드와 API 참조서를 확인하여 CPV 사용하기※샘플 소스코드를 아래에 첨부하였으니, 참고바랍니다. ​[영상 가이드]   ​[참고] CPV 샘플코드 (python) import osimport sysimport urllib.requestclient_id = ""YOUR_CLENT_ID""client_secret = "" YOUR_CLENT_SECRET""encText = urllib.parse.quote(""출력하고 싶은 텍스트 입력"")data = ""speaker=nara&volume=0&speed=0&pitch=0&emotion=0&format=mp3&text="" + encText;url = ""https://naveropenapi.apigw.ntruss.com/voice-premium/v1/tts""request = urllib.request.Request(url)request.add_header(""X-NCP-APIGW-API-KEY-ID"",client_id)request.add_header(""X-NCP-APIGW-API-KEY"",client_secret)response = urllib.request.urlopen(request, data=data.encode('utf-8'))rescode = response.getcode()if(rescode==200):    print(""TTS mp3 저장"")    response_body = response.read()    with open('1111.mp3', 'wb') as f:        f.write(response_body)else:    print(""Error Code:"" + rescode) 두번째, 개발자가 아니라도?간단하게 사용하기![클로바 보이스 페이지 : https://clova.ai/voice]개발자가 아닌 일반 사용자들이 CPV를 사용할 수 있는 방법은 클로바 더빙 페이지를 이용하는 방법입니다. 단순히 음성 테스트를 하고 싶은 경우라면 클로바 보이스 페이지에서도 가능합니다.​  지금까지 클로바 프리미엄 보이스의 활용 사례 및 사용방법을 알려드렸습니다. 네이버 클라우드 플랫폼은 엄격한 뉴스 앵커 스타일, 부드러운 친구 스타일, 담백한 일반인 스타일 등 앞으로 더 다양한 감정과 스타일의 합성음을 API 형태로 제공할 예정입니다. ​CPV를 통해 더욱 다양한 분야에서 활용할 수 있을 것으로 기대되며, CPV 관련 사용 문의는 네이버 클라우드 플랫폼 고객지원(1544-5876) 및 홈페이지 문의로 연락 부탁드립니다.감사합니다. ​​ ​ "
장단기 메모리 - LSTM ,https://blog.naver.com/realmercy_/223055520976,20230325,"​LSTM은 Long Short-Term Memory의 약자로, 장기적인 의존 관계(long-term dependencies)를 학습하기 위한 딥러닝 모델이다. LSTM은 RNN(Recurrent Neural Network)의 한 종류로, RNN의 한계점인 기울기 소실(Vanishing Gradient) 문제를 해결하기 위해 고안되었다. LSTM은 시퀀스 데이터(sequence data)를 처리하는데 매우 효과적이며, 자연어 처리(Natural Language Processing) 분야에서 많이 활용된다.​​LSTM 특징LSTM(Long Short-Term Memory)은 기존의 RNN(Recurrent Neural Network)의 단점을 극복하기 위해 고안된 모델로, 장기적인 의존 관계(long-term dependencies)를 학습하는 데 특화되어 있다. LSTM의 주요 특징은 다음과 같다.​1. 기억 셀(Memory Cell) LSTM은 기억 셀이라는 구조를 갖고 있다. 기억 셀은 시간에 따라 정보를 저장하거나 삭제할 수 있는 일종의 메모리 역할을 한다. 이 기억 셀은 LSTM의 핵심 구성 요소이며, LSTM이 장기적인 의존 관계를 학습할 수 있는 기반이 된다.​2. 게이트(Gate) LSTM은 게이트라는 구조를 이용하여 기억 셀에 저장되는 정보의 양을 제어한다. 게이트는 정보의 흐름을 조절하여 필요한 정보만을 남기고 불필요한 정보는 삭제한다. LSTM은 세 개의 게이트를 사용한다.​  - 입력 게이트(Input Gate) : 새로운 정보를 기억 셀에 추가하는 게이트  - 삭제 게이트(Forget Gate) : 이전 정보를 삭제하는 게이트  - 출력 게이트(Output Gate) : 현재 상태의 기억 셀에서 어떤 정보를 출력할 것인지 결정하는 게이트​3. 순환 구조 LSTM은 RNN과 마찬가지로 순환 구조를 갖고 있다. 이는 LSTM이 이전 상태에서 학습한 정보를 현재 상태에서도 계속 사용할 수 있게 한다.​4. 학습이 가능한 가중치 LSTM은 학습이 가능한 가중치를 갖고 있어, 입력 데이터에 맞게 자동으로 조절된다. 이를 통해 LSTM은 입력 데이터에 대한 패턴을 학습하며, 이를 바탕으로 새로운 데이터에 대한 예측이 가능해진다.​​LSTM의 약점계산량이 많다. LSTM은 입력과 출력을 분리하는 등의 복잡한 구조를 갖고 있기 때문에, 계산량이 많다. 따라서 모델의 크기가 커지면 학습 시간이 길어지는 단점이 있다.과적합(Overfitting)의 가능성이 있다. LSTM은 학습이 가능한 가중치를 갖고 있어, 데이터셋에 과적합될 가능성이 있다. 따라서 적절한 정규화 기법을 이용하여 과적합을 방지해야 한다.초기 가중치 설정이 중요하다. LSTM은 초기 가중치 설정이 학습의 성능에 큰 영향을 미친다. 따라서 초기 가중치를 적절하게 설정하는 것이 중요하다.​종합적으로 LSTM은 기존의 RNN에 비해 장기적인 의존 관계를 처리할 수 있으며, 자연어 처리 등의 분야에서 유용하게 사용된다. 하지만 계산량이 많고, 과적합의 가능성이 있으며 초기 가중치 설정이 중요하다는 단점이 있다.​​LSTM의 활용 분야1. 자연어 처리(Natural Language Processing) \LSTM은 문장, 문서 등과 같은 자연어 데이터를 처리하는 데 매우 유용하다. 예를 들어, LSTM을 이용하여 기계 번역, 감성 분석, 텍스트 분류 등을 수행할 수 있다. 이는 LSTM이 단어의 시퀀스를 처리할 수 있으며, 이를 바탕으로 문맥을 파악하여 자연어 처리 작업을 수행할 수 있기 때문이다.​2. 음성 인식(Speech Recognition) LSTM은 음성 인식 분야에서도 유용하게 사용된다. 음성 인식 시스템은 시간적으로 일련의 특징 벡터를 입력으로 받아, 이를 통해 음성을 인식한다. LSTM은 이러한 시퀀스 데이터를 처리하는 데 적합하며, 음성 인식 정확도를 향상시키는 데 기여한다.​3. 이미지 분석(Image Analysis) LSTM은 이미지 분석 분야에서도 사용된다. 예를 들어, LSTM을 이용하여 이미지 캡셔닝(Image Captioning)을 수행할 수 있다. 이는 LSTM이 이미지와 텍스트 데이터를 모두 처리할 수 있기 때문이다.​4. 시계열 데이터 분석(Time Series Analysis) LSTM은 시계열 데이터 분석 분야에서도 유용하게 사용된다. 예를 들어, LSTM을 이용하여 주가 예측, 날씨 예측 등을 수행할 수 있다. 이는 LSTM이 시계열 데이터의 패턴을 파악하여 예측 모델을 학습할 수 있기 때문이다.​5. 게임 AI(Game AI) LSTM은 게임 AI 분야에서도 사용된다. 예를 들어, LSTM을 이용하여 게임 내 캐릭터의 행동을 예측하거나, 게임 내 다음 스테이지를 예측하는 등의 작업을 수행할 수 있다. 이는 LSTM이 게임에서 발생하는 다양한 데이터를 처리하고, 이를 바탕으로 예측 모델을 학습할 수 있기 때문이다.​종합적으로 LSTM은 다양한 분야에서 활용될 수 있으며, 장기적인 의존 관계를 처리하는 데 특화되어 있어 자연어 처리, 음성 인식, 이미지 분석, 시계열 데이터 분석, 게임 AI 등의 분야에서 널리 활용된다. LSTM의 특징인 장기 기억 메커니즘은 시계열 데이터와 같은 순차적인 데이터 처리에 적합하기 때문에, 이를 처리하는 분야에서 특히 유용하게 사용된다.​​​ "
"[머신러닝, 논문리뷰] 이직준비 1- Machine Learning for Fluid Mechanics ",https://blog.naver.com/hodong32/223050527708,20230320,"#머신러닝 #논문리뷰 #이직준비 #Machine #learning #Fluid #mechanis #Engineering​유튜브에서 유체역학과 머신러닝 주제로 강의하시는 Steven 교수님을 알게 되었고, 논문을 통해 유체역학에 머신러닝 적용 가능성을 파악해보고자 논문 리뷰를 시작하였습니다.​해당 논문은 리뷰 논문에 해당하며 Mahcine Learning For Fluid Mechanis 라는 대단한 타이틀을 가진 제목에 해당합니다. ​여기서 얻고자 하는 정보는 대략적인 머신러닝과 유체역학의 적용성에 대한 내용입니다. 해당 논문 또한 초심자들에게도 적합할 수 있는 내용입니다. 논문 요약과정은 개인적인 해석적인 관점이 있으니, 사실이 필요한 경우 아래 논문을 참고해주세요.​간단히 말하면 해당 논문은 Machine Learning (ML)에 대한 역사와 방법론, 유체역학의 역사와 ML과의 접합점에 대한 내용 정리 및 회고에 대한 논문에 해당합니다. ​https://doi.org/10.1146/annurev-fluid-010719-060214 Abstract << 모든 내용이 요약 되어 있는 공간 배경 : ML은 Data로 부터 좋은 정보를 지식으로 변형하는데 의의가 있다. 자동학습 관련 부분은 Flow Control과 최적화 부분에 연관이 있다. 이에 해당 아티클은 Fluid Dynamics에서 적용할 수 있는 여러 기회들을 제공하고자 한다. 대략적인 머신러닝 방법과 적용에 대한 개요를 보여줄 것이다. ML은 유체역학에 정보  연구에 대한 막강한 정보처리와 적용이 될 것이다.​Abstract에서 엿볼 수 있는 것은 바로 ML에 대한 대략적인 윤곽을 소개하겠다는 것과 Fluid Dyanmics에서 어떤식으로 적용할 수 있는지에 대한 Steven 교수님의 생각을 Article로 풀어 낼 것으로 보입니다. ​ Introduction <<현재의 연구 결과 히스토리와 배경을 보여주는 공간 유체역학은 많은 데이터를 이용하여 실험 식을 만들어 왔다. 하지만, 여기에는 유체역학 지식을 기반으로 연구되어 왔습니다. ​반면, 머신러닝은 많은 데이터를 효율적으로 처리하기 위하여 발전되어 왔습니다. 머신러닝은 대분류에서 소분류로 나누어서 분류가 됩니다. 큰 범주 아래에서 Supervised, Semisupervised, Unspurvsised 형태로 나누어 지며, 각 카테고리에서 세분화될 수 있습니다. 그림 1과 같이 머신러닝을 분류 할 수 있습니다. ​그림 1은 매우 중요한 내용입니다. 아래 그림 1을 기반으로 ML에 대한 대부분에 대한 내용이 전개되었다고 해도 무방합니다. 그림 1에 대한 특징들을 설명하면서, Fluid Dynamics에서는 어떻게 적용할 것인가를 계속 연결해보려고 하고 있습니다. 유체역학에서 주로 다룰 부분은 Dimensionality reduction 부분이 되겠네요. (POD/PCA AutoEncoder) [그림 1]  ​그림 1을 설명하기 앞서, ML과 Fluid Dynamics의 전반적인 역사 내용을 다루고 있습니다. 특별한 내용은 아니지만, ML에 대한 역사를 잘정리되어 있어서 한번 상기 시킬 수 있어서 좋았습니다.​Histroy 1940 CFD난류 모델 Stastical (Kolmogorov)1950s and 1960s (ML)cybernetic and epxert stystems (human brain)perseptron (classification and regression) = not capable of learning the xor function1960s ~ 1980s ML Winter1980sdevelopment of the backpropagation algorithm (Rumelhart et al. 1986).did not attract many researchers from fluid mechanics. 1990sclassification for particle tracking velocimetry (PTV) and particle image velocimetry (PIV). (Teo et al. 1991, Grant & Pan 1995) 1990s a number of applications of NNs in flow-related problems.identifying the phase configurations in multiphase flows (Bishop & James 1993).The link between proper orthogonal decomposition (POD) and linear NNs (Baldi & Hornik 1989)reconstruct turbulence flow fields and the flow in the near-wall region of a channel flow using wall-only informationfirst to also use multiple layers of neurons to improve compression results, marking perhaps the first use of deep learning, as it is known today, in the field of fluid mechanics.​We believe that this confluence of first principles and data-driven approaches is unique and has the potential to transform both fluid mechanics and ML.역사적으로 보면 충분하게 ML과 Fluid Dynamics 는 연결 될 수 있다고 언급하였네요.​처음으로 연결 된 것은 POD라는 Proper Orthogonal decomposition 입니다. 이 애플리케이션은 압축 결과를 개선하기 위해 여러 층의 뉴런을 사용한 최초의 애플리케이션 중 하나로, 오늘날 알려진 것처럼 유체 역학 분야에서 딥 러닝을 최초로 사용한 사례입니다. 선형대수적 관점으로 보면,  유체의 특성을 뽑아내서 계산의 비용을 줄인 것으로 볼 수 있습니다. ​여기서의 의의는 Deep Learning 구조와 비슷하다는 거죠. Deep Learning 또한 정보에서의 특징들을 뽑아서 학습을 한다고 볼 수 있죠.We believe that this confluence of first principles and data-driven approaches is unique and has the potential to transform both fluid mechanics and ML.​하지만, ML은 현재 이미지 인식이나 광고 부분에서 많이 사용하기 때문에, 유체역학에 있어서는 많은 연구과제를 가지고 있습니다. 즉, 연구할 거리가 무궁무진하다는 거죠 ㅎㅎ​유체에서 제일 적용이 어려운 것은 불안정한 흐름 필드에는 일반적인 ML 알고리즘에 존재하지 않을 수 있는 비선형성 및 다양한 시공간 스케일을 처리할 수 있는 방법을 찾아주어야 합니다. 또한, 유체가 적용이 된다면 Robot과 같은 분야와 시너지가 있을 것으로 예상되어 연구의 가치로는 충분하다고 제안하였습니다.​2010년도 Neruon Network는 많은 데이터로 인하여 빠르게 발전하였습니다. 유체역학 또한 많은 실험 결과를 가지고 있어 2010년도 때의 NN발전과 유사하게 Fluid Dynamis 또한 빠르게 발전할 수 있는 기회를 가지고 있습니다. ML에는 여러가지 주요 특성이 있는데, 이를 어떻게 유체역학의 적용할지를 판단하는 것이 필요하죠.​이 부분을 대략적으로 제안한 것은 다음과 같습니다.​​​​ Chapter 2. fundamental algorithms of ML Chpater 2에서는 Machine Learning에 대한 알고리즘에 대해서 간단하게 소개하고 있습니다.​아래 그림은 Machine Learing을 추정하는 대표적인 알고리즘에 해당합니다. 여기서 x라는 input이 주어졌을 때 적합한 출력 값에대한 확률 분포를 이용하여 Loss Function의 Weight를 주는 방법에 해당합니다. 이렇게 되면 장점으로는 Outlier들을 쉽게 제거할 수 있을 것입니다. Loss (L 함수) 에 설정에 따라서 대표적인 알고리즘 형태를 나눠볼 수 있습니다. Supervised Learning( 대표적으로) 라벨링 된 데이터를 학습을 한다고 생각해보겠습니다. 이 경우 간단하게 선형화 된 방정식을 가정하게 되면, 다음과 같이 될 것입니다. Loss를 최소화 하는 Weight와 Bias를 구해주게되면 예측률이 올라가는 간단한 회귀 알고리즘이 됩니다. 여기서 중요한 것은 p(x,y)를 이용하는 것인데 이경우 prior knowledge를 응용할 수 있다는 것이 되죠. 이전에 데이터로 부터 p(x,y)의 확률 분포는 어느정도 확보가 되었을 것이고, 이에 맞는 weight를 찾아준다면 적합한 학습 알고리즘이 된다는 것이 기본 아이디어 입니다.​ What is Neron networks? 뉴런 네트워크라고 불리는 이유를 생각해보죠. 뉴런이라는 사고의 과정들은 일정들의 신호들을 주고 받습니다. 이 신호들의 조합으로 부터 우리는 최종 결과물을 예측을 하게 되는 것입니다. 예를 들면 스무고개를 한다고 보았을 때, 공통점들에 대한 신호들이 모인 다면 어떠한 결과물을 예측을 하는 과정으로 생각해볼 수 있겠습니다.​이를 구성하기 위해서 판단할 수 있는 근거를 주어야합니다. 이에 Feed foward networks 를 이용하여 판단의 근거를 마련하는 가중치를 전달하는 역할을 합니다. 다음 레이에서는 해당 가중치를 이용해서 예측을 하게 됩니다.​주어진 가중치로 예측 결과가 좋다면, 문제가 없겠지만 예측률이 떨어지게 됩니다.  이 과정에서 Backpropagation이라는 이전 레이어 한테 목적 함수를 최소화 할 수 있는 weight를 다시 요청합니다. 이러한 비선형 최적화 과정을 거치면서 좋은 weight를 모아두는 과정이 NN의 학습 과정이 됩니다.​Deep NN조금 더 발전을 하다보면 Deep Neruon Netoworks(DNN)이라는 것을 배우는데, 이는 NN을 깊게 쌓은 것에 해당합니다. 여기에는 뉴런을 활성화 시키는 Activation Layer들이 많이 쌓여 있습니다. 이 말은 어떠한 결과를 판단할 수 있는 요소들이 많다고 볼 수 있습니다.​Convolution Neral Network (CNN)CNN으로 알려진 것은 recognition에 특화된 알고리즘입니다. convolution은 합성이라는 것이 있는데 이는 필터라는 효과를 가지고 있습니다. 우리가 이미지를 판단하는데, 특정한 부분만 가지고 판단하는 아이디어와 유사합니다. 이러한 아이디어들은 자연스럽게 Dimension Reedcution으로 이어질 수 있게 됩니다.​Recurrent neural networks (RNNs)스티븐 교수는 CFD에서 RNN은 어떠한 관련성이 있다고 생각합니다. 이는 바로 시간에 따른 Back Propagation을 처리할 수 있다는 것입니다. 이전 까지는 시계열 데이터가아닌 결과물을 가지고 처리를 하였다면, 이는 시간 개념이 추가된 알고리즘입니다. 신호처리에서 처음 응용되었습니다.​단점으로는 많은 데이터를 저장해야 했기 때문에, 시간의 한계가 있었죠. 이를 개선한 것이 LSTM(Long short term memory) 알고리즘 입니다.​LSTM(long short term memory)RNN과는 달리 sigma 함수가 도입 되어서 gating mechanism 이라는 어떠한, 특수 조건에 대한 내용을 포함한 알고리즘 인듯 합니다. 데이터를 지우면서 기억하는 방법이라고 하는데, 아마 빈도수가 높은 데이터는 남아 있고 적은 것들에 대해서는 빠르게 지우는 방법에 해당하는 알고리즘인 듯 합니다. 중요한 것은 RNN보다 오랜 시간 데이터를 보존할 수 있다는 장점이 있습니다. ​RNN과 LSTM은 언어 인식에 적합한 알고리즘으로 시계열 데이터를 효율 적으로 처리하는 것에 해당합니다. Chat gpt에서 사용되는 단어 조합의 문제와는 조금 다른 알고리즘 일 것입니다.​​ <해당 설명 요약  그림 추가>​​2.1.2 분류 알고리즘  (SVM, Support Vector Machine) SVM 방법은 매우 강력한 도구로 이용될 수 있습니다. 이 특정한 확률 분포에 부합한 경우 0, 아닌 경우 1로 두어서 특정 확률 분포로 수렴할 수 있는 방법론에 해당하여 분류 알고리즘에서 사용될 수 있는 목적함수 입니다.​ 2.2 Unspervised Learning 여태까지는 이미 알려진 결과를 이용한 방법에 해당하였다면, Unsupervised learning은 새로운 환경을 경험하면서 NN이 스스로 학습하는 단계입니다. <내용 추가>​2.2.1 Dimension Reudciton (POD nad PCA)유체와 ML을 적용하기 위한 매우 중요한 초석(cornerstone)입니다. PCA는 선형 대수적으로 SVD (Singular Value Decomposition) 과정에 해당합니다. 필요한 Eigen vector space에서만 해석을 하여 계산 속도를 증가 시킬 수 있습니다. PCA는 SVD의 아이디어를 이용한 것에 해당하며, POD는 아마 PCA와 비슷하지만 최적화 방법과 조금 더 유사한 방법에 가까운 것으로 보입니다. PCA와 POD에서 중요한 것은 Encoder 했던 것을 어떻게 Decoding 할 것인가 입니다. 기존의 차원으로 되돌리기 위해서  Encoding과 Decoding의 Map을 만들어 주는게 중요합니다.​2.2.3 Clusting and Vecotr QunatizationQuantization이라는 뜻을 보면 어떠한 한상태로 만드는 식에 해당합니다. k-means clusting이라는 것은 어떠한 center pointer 중심으로 k 거리만큼 있는 모든 데이터를 하나의 set으로 보는 것을 의미합니다. 이를 기준으로 Loss function을 구성한 것이 Clustering 알고리즘에 해당합니다.​ 2.3 Semisupervised Learning GANS 와 Reinforce learning이 여기에 해당합니다. 어떠한 환경에서 스스로 학습하는 알고리즘입니다. 이전의 경우 Loss를 최소화 하였다면, 이 경우Actor가 Policy에 따라서 Reward를 최대화 하는 방법으로 학습을 하게 됩니다.​2.3.1 GANSGenartive Model은 newtork는 생성 네트워크를 이용하여 최적화 과정을 이루어지게 됩니다. 최적화 과정에서 예시를 생성하여 스스로 최적화 해가는 과정이 됩니다. 예시 생성을 위해서는 생성 네트워크는 판별 네트워크와 비평 네트워크를 이용합니다.​2.3.2 강화학습agent가 policy 에 따라서 학습하는 알고리즘​2.4 최적화Stohcastic Optimization 방법이 많이 사용 됩니다. 이는 시행착오 과정을 통해서 최적점을 찾는 방법으로 느린 대신에, 많은양의 데이터를 학습하여 글로벌한 최소점을 찾을 수 있다는 장점이 있습니다.​​여기까지 대략적인 Chapter 1 ~ Chapter 2 에 대한 내용입니다. 다음 포스팅에서는 Flow Modeling 과정으로 해당 논문 리뷰를 마치겠습니다. 그 다음 포스팅에서는 여기에 나왔던 알고리즘 일부들을 구현하며 어떻게 구현할 수 있는지 실습하는 과정을 다룰 것입니다.​​3.Flow Modeling(Part II 업데이트 예정)​3.1.3. Sparse and randomized methods.Decreasing the amount of data to train and execute a model is important when a fast decision is required, as in control. 기본 아이디어는 큰 행렬이 저차원 구조를 갖는 경우, 열 또는 행을 임의의 저차원 하위 공간에 투영한 후에도 높은 확률로 이 구조가 보존되어 효율적인 다운스트림 계산을 용이하게 한다는 것입니다. (행렬 분해 -> 랜덤 샘플링에 의해서)​3.1.4. Superresolution and flow cleansing.통계적 추론에 근거한 Resoultion 향상 (예시, 사진 )LES 응용 Recently, Fukami et al. (2018) developed a CNN-based superresolution algorithm and demonstrated its effectiveness on turbulent flow reconstruction, showing that the energy spectrum is accurately preserved.​High Cost - > Cost Down - (NN based Approach) ->GANS for superr ->Xie et al. (2018) recently employed GANs for superresolution.​Dimensionality Reduction​POD :Sirovich (1987) -> sanpshot involving singular vlaue decomposition. it is similar pca. In ML -> Auto Encoder (NN)​K-Clustering Algorithm : (난류, 층류 모델 분류 -> Wall Region 특정화)Employed by Kaiser (2014) - to develop a data-driven discretization of high-dimensional phase space for fluid mixing layer.Markov Transition Model (how the evolve in time frome one state to another)earliest examples of ML classification in fluid dynamics by Bright et al. (2013) was based on sparse representation (Wright et al. 2009).3.2. Modeling Flow DynamicsA central goal of modeling is to balance efficiency and accuracy.​3.2.1. Linear models through nonlinear embeddings: dynamic mode decomposition and Koopman analysis.Dynamic mode decomposition (DMD) (Schmid 2010,Kutz et al. 2016) is a modern approach to extract spatiotemporal coherent structures from time series data of fluid flows, resulting in a low-dimensional linear model for the evolution of these dominant coherent structures​DMD is based on data-driven regressionequally valid for time-resolved experimental and numerical dataKoopman operator (Rowley et al. 2009, Mezic 2013), which is an infinite-dimensional linear operator that describes how all measurement functions of the system evolve in time.DMD algorithm is based on linear flow field measurements the resulting models may not be able to capture nonlinear transients.​concerted effort to identify a coordinate system where the nonlinear dynamics appears linear.​유체 역학은 VAMPnet의 성능을 기반으로 확률론, 거친 입자 역학, 시간 스케일 분리 등 유사한 모델링 문제를 가진 분자 역학과 같은 인접 분야의 이점을 활용할 수 있습니다.​3.2.2. Neural network modeling.Early examples include the use of NNs to learn the solutions of ordinary and partial differential equations (Dissanayake & Phan-Thien 1994, Gonzalez-Garcia et al. 1998, Lagaris et al. 1998).이러한 작업의 잠재력은 아직 완전히 탐구되지 않았으며, 최근 몇 년 동안 불연속 및 연속 시간 네트워크 등 추가적인 발전이 있었다는 점에 주목합니다(Chen et al. 2018, Raissi & Karniadakis 2018).NN은 유체 시스템을 모델링하는 데 자주 사용되는 NARMAX와 같은 비선형 시스템 식별 기법에도 자주 사용됩니다(Glaz et al. 2010, Semeraro et al. 2016).​In fluid mechanics,NNs were widely used to model heat transfer ( Jambunathan et al. 1996), turbomachinery (Pierret &Van den Braembussche 1999), turbulent flows (Milano & Koumoutsakos 2002), and other problems in aeronautics (Faller & Schreck 1996).​RNNs with LSTMs (Hochreiter & Schmidhuber 1997) have been revolutionary for speech recognition, and they are considered one of the landmark successes of AI.​They are currently being used to model dynamical systems and for data-driven predictions of extreme events (Vlachaset al. 2018,Wan et al. 2018).An interesting finding of these studies is that combining data-driven and reduced-order models is a potent method that outperforms each of its components on several studies​GANs have potential to aid in the modeling and simulation of turbulence (Kim et al. 2018), although this field is nascent.​inally, it is important to explicitly incorporate partially known physics, such as symmetries, constraints, and conserved quantities.​Parsimonious nonlinear models.Loiseau & Brunton (2018) identified sparse reduced-order models of several flow systems,enforcing energy conservation as a constraint. In both genetic programming and sparse identification,a Pareto analysis is used to identify models that have the best trade-off between modelcomplexity, measured in number of terms, and predictive accuracy.Trade - off !! (물리학이 알려진 경우 이 접근 방식은 일반적으로 올바른 지배 방정식을 발견하여 ML의 주요 알고리즘에 비해 탁월한 일반화 가능성을 제공합니다.​3.2.4 Closure models with machine learning.난류 모델 시공간에 대한 것을 해야하기 때문에 모든 스케일을 해결하는데 많은 시간이 필요로함. 일반적으로 작은 규모를 잘라내고 폐쇄 모델을 사용하여 큰 규모에 미치는 영향을 모델링하는 것이 일반적이며, 일반적인 접근 방식에는 레이놀즈 평균 내비어-스토크스(RANS) 및 LES가 있습니다. 그러나 이러한 모델은 완전히 해결된 시뮬레이션 또는 실험의 데이터와 일치하도록 세심한 조정이 필요할 수 있습니다.​ML은 RANS 모델과 고충실도 시뮬레이션 간의 레이놀즈 응력 텐서 불일치를 식별하고 모델링하는 데 사용되었습니다(Ling & Templeton 2015, Parish & Duraisamy 2016, Ling 외. 2016b, Xiao 외. 2016, Singh 외. 2017,Wang 외. 2017). Ling & Templeton(2015)은 레이놀즈 스트레스 텐서에서 불확실성이 높은 영역을 분류하고 예측하기 위해 SVM, Adaboost 의사결정 트리, 랜덤 포레스트를 비교했습니다.​베이지안 추론으로 인한 Rynolds stress tensor 프레임워크는 나중에 Singh 등(2017)이 뛰어난 성능의 Spalart-Allmaras RANS 모델에 대한 NN 강화 보정을 개발하는 데 사용되었습니다. 3.2.5. Challenges of machine learning for dynamical systems.​동역학용 ML에서는 미지의 물리학을 발견하는 것과 알려진 물리학을 통합하여 모델을 개선하는 두 가지 작업을 구분합니다. 많은 학습 아키텍처는 대칭, 경계 조건, 전역 보존 법칙과 같은 물리적 제약을 쉽게 통합할 수 없습니다.​이는 지속적인 개발을 위해 중요한 분야이며, 최근 여러 연구에서 일반화 가능한 물리 모델을 제시했습니다.​4. FLOW OPTIMIZATION AND CONTROL USING MACHINE LEARNING흐름 모델링과 달리, 최적화 및 제어를 위한 학습 알고리즘은 여러 가지 방식으로 데이터 샘플링 프로세스와 상호 작용합니다. 첫째, 이전 섹션에서 설명한 모델링 노력에 따라 비용 함수와 제어/최적화 매개변수를 연결하는 명시적 대리 모델을 개발하는 데 ML을 적용할 수 있습니다.​학습이 진행됨에 따라 최적화 결과에 따라 새로운 데이터가 요청됩니다.Furthermore, the high imensional and nonconvex optimization procedures that are currently employed to train nonlinear LMs are well suited to the high-dimensional, nonlinear optimization problems in flow control.​4.1. Stochastic Flow Optimization: Learning Probability Distributions 이 리뷰에서는 유체 역학의 관점에서 ML 알고리즘을 소개합니다. 이 두 분야의 인터페이스는 오랜 역사를 가지고 있으며 지난 몇 년 동안 새로운 관심을 불러일으켰습니다. 이 리뷰에서는 실험 및 시뮬레이션의 흐름 모델링, 최적화 및 제어 문제에서 ML의 응용을 다룹니다. 차원 축소, 특징 추출, PIV 처리, 초해상도, 환원 차수 모델링, 난류 폐쇄, 형상 최적화, 흐름 제어와 같은 중요한 유체 역학 작업에서 ML이 거둔 몇 가지 성공을 중점적으로 다룹니다. 이러한 노력에서 얻은 교훈을 논의하고 우리 시대의 기술 발전에 비추어 현재의 관심을 정당화합니다. 우리의 목표는 유체 역학에서 ML과 그 맥락에 대한 더 깊은 이해를 제공하는 것입니다. ML은 유체 역학에서 발생하는 것과 같은 고차원 비선형 문제에 적합한 데이터 기반 최적화 및 응용 회귀 기법으로 구성되며, 이러한 최적화 및 회귀 문제를 공식화하려면 유체 역학에 대한 전문 지식이 필요합니다. 머신러닝 알고리즘은 유체 역학 연구 분야에서 대부분 미개척 분야였던 기존 탐구 방식을 보강할 수 있는 다양한 도구를 제공합니다. 유체 역학 지식과 수백 년 된 보존 법칙은 빅 데이터 시대에도 여전히 유효합니다. 이러한 지식은 보다 정확한 질문의 틀을 짜는 데 도움이 될 수 있습니다.  질문을 구성하고 흐름 제어 및 최적화에 ML 알고리즘을 적용할 때 종종 발생하는 막대한 계산 비용을 줄이는 데 도움이 됩니다. 고차원 검색 공간의 탐색과 시각화는 ML을 통해 간소화될 수 있으며, 점점 더 풍부해지는 고성능 컴퓨팅 리소스를 통해 간소화할 수 있습니다.가까운 미래에 ML 알고리즘에 대한 경험은 유체 역학의 새로운 질문을 구성하는 데 도움이 될 것이며, 수십 년 된 선형화된 모델과 선형 접근법을 비선형 영역으로 확장할 것입니다. ML의 비선형 영역으로의 전환은 풍부한 오픈 소스 소프트웨어 및 방법과 ML 커뮤니티의 광범위한 개방성에 의해 촉진됩니다. 장기적으로 볼 때, ML은 유체 역학의 오래된 문제를 데이터에 비추어 새롭게 바라볼 수 있는 기회를 제공할 것입니다. ML 솔루션을 해석하고 문제 진술을 구체화하려면 다시 유체 역학 전문 지식이 필요할 것입니다.Translated with DeepL "
NCE 1일차 - 1교시 ,https://blog.naver.com/alex6k/223067533673,20230407,"강사님이 바뀌었다. 이해하기 쉽게 잘가르쳐줘서 개좋음; ​ 강의내용 클라우드 서비스란 무엇인가?기업이 인프라를 직접 소유하는 것이 아니라, 대여하고 사용한 만큼 돈을 내는 것.직접 구축할 필요가 없고, 관리의 수고가 무척 줄어든다.​각종 클라우드 서비스는 온디맨드. 필요할 때 직접 생성해 수 분 내에 사용할 수 있다.​초기에는 인프라 도입의 고정비용을 감당할 수 없는 스타트업이 이용.현재는 금융, 공공기관 많은 곳에서 적극적으로 활용한다.​대세가 된 이유, 즉 클라우드의 장점들.​CAPEX 가 OPEX로 대체된다.  = 고정비용 감소클라우드사에서 대규모의 서버를 집적 = 규모의 경제 달성 가능.클라우드사 측에서 연구한 신규 서비스를 지속적으로 제공 = 기업 자체적 R&D 비용 절감빠른 프로비저닝 & 유연한 확장. = 빠르고 값싼 오토스케일링인프라 유지관리 부담 제거 = VM 내부의 나의 서버만 관리하면 되고, 그 하부는 사측에서 관리.​ Iaas = 인프라. OS단위부터 사용자가 직접 관리CaaS = OS는 클라우드사가. 사용자에게는 컨테이너를 제공 (쿠버네틱스)PaaS = platform as a service = 기업에게 클라우드 환경을 제공SaaS = software as a service  = 이미 필요한 것이 모두 구현되어있고, 그 서비스가  클라우드 환경 위로(예시>Zoom, salesForce)​네이버 : 한국을 포함한 6개의 글로벌 리전을 보유. 한국, 일본, 홍콩, 싱가폴, 독일, 미국서부​AI알고리즘은 많이 풀려 있다.차이를 만들어내는 요인은 학습할 데이터의 질과 양, GPU등의 고용량의 컴퓨팅 자원, 계속 학습할 수 있는 메모리 자원​네이버 클라우드 업체의 특장점​대규모 서비스 운영 노하우  네이버, 라인세계적으로 인정받은 보안 기술. = 민간존 / 공공존 (산업특화존 : 공공기관 / 금융권을 위한 특수한 환경 Certification 통과한 우수한 보안존)국내 최대 IT서비스, 안정적 인프라.24시간 365일 사용자 지원​네이버클라우드 인프라 상품군.​컴퓨팅 자원서버 ​VDS 와 베어메탈은 둘 다 간섭현상을 피하고, 안정적인 서비스를 위해 사용한다.VDS(Virtual Dedicated Server) = 하이퍼바이저가 있긴 하지만, 그 위에 VM이 딱 하나만 올라간다.베어메탈 서버 = 서버 하나를 통째로 사용자가 대여 ~ 하이퍼바이저 없이 물리 서버처럼 사용. Cloud Function (AWS 람다) = Server Event 를 트리거로 삼아, Action을 취한다. (서버리스환경구축, 오토스케일링)​DevOps 개념으로, Cloud Insight = 모니터링​​컨테이너컨테이너 레지스트리 = 컨테이너 이미지 저장/관리쿠버네티스 ㄴ 마스터노드 ~ 워커노드의 클러스터 통신환경을 자동으로 잡아준다.ㄴPOD, Service, Deployment의 개념이 중요하다.스토리지ㄴ블록스토리지 = 서버에 마운트. 서버당 최대 15개 [linux OS 50GB / window OS 100GB]   HDD / SSD = SSD 만 IOPS 개런티가 있다. [1GB당 40 IOPS ~ 최대 2만]ㄴ오브젝트스토리지 = 용량제한 없다​NAS여러대의 서버가 공유를 해야 할 때. (window NFS, linux CIFS)​아카이브 스토리지 콜드 / 핫 라이프사이클​마이그레이션데이터텔레포터 :대용량데이터 이관할때 쓰는 하드웨어장비를 대여오브젝트마이그레이션 : 타사 클라우드의 오브젝트르스토리지를 이관시​네트워킹VPC각 고객마다의 논리적 사설 네트워크. 한국 일본 싱가폴 리전에만 제공서브넷서버는 각기 다른 서브넷에 위치해야 한다.로드밸런서App(Http/Https) / NetworkProxy(TCP_Proxy) / Network (TCP/DSR)​멀티 존(한국 싱가폴 일본 kr-1 kr-2 두개의 존을 제공)로드밸런서는 존에 종속적이지 않으며, ​글로벌트래픽매니저 : 서비스가 잘되서 외국에 서버를 두고싶을때, 사용자의 IP를 자동판별해 외국의 서버로 라우팅.​CDNCDN+ (국내) / global CDN으로 나눔​플랫폼 상품군DBCloud DB for... / DB = 좌측은 완전관리형 DB서버. 우측은 서버에 이미지만 제공하니 알아서.완전관리 = 서버 이중화, 자동백업, 모니터링 ~ 이벤트 알림(시험문제는 어떤 DBMS가 완전관리형으로 제공되는지가 중요.)​AnalyticsCloud Hadoop빅데이터 분석Cloud Search한국어 형태로 분석기가 내장된 네이버 검색엔진 = 한국어 검색에 최적​Media라이브방송 = 라이브스테이션​어플리케이션 상품군ai클로바 voice = 음성합성클로바 speech recognition = 음성을 인식해서 텍스트로 변환클로바 AI call = AI 전화 고객센터클로바 sentiment = 텍스트로부터 작성자의 감정을 읽어낸다. (와 sans!! = 긍정100% 부정0%) 클로바 Studio = 생성 AI. No code AI 도구​Cloud outboundMailer = 대량 메일전송Maps = 네이버지도 apisens = SMS, Push 메세지 전송​관리/ 보안WMSSubAccountCloud Activity Tracer / Resource Manager 행동 로그 / 자원 로그OrganizationCLA cloud log analytics = 네이버클라우드에서 생기는 모든 로그를 모아서 본다. AGent 필요. / insight 는 필요없음.RUA 사용자 분석​시큐리티시큐리티 모니터링베이직 시큐리티​​ 용어정리 Capital expenditures = 미래의 이윤을 창출하기 위해 지출한 비용. 경제학적으로는 고정비용을 의미하는 것 같다.Operating Expense = 운영비용latencyPOD, Service, DeploymentHA high availa 고가용성SPOF single point of Failure 를 없애면 고가용성 추구 가능. "
Cisco CVP 메모리 문제 ,https://blog.naver.com/osu0811/222450950729,20210730,"https://www.cisco.com/c/ko_kr/support/docs/contact-center/unified-customer-voice-portal-1101/214553-troubleshoot-cvp-vxml-server.html CVP VXML 서버 문제 해결이 문서에서는 CVP VXML(Voice eXtensible Markup Language) 서버의 메모리 부족, 라이센스 및 통화 흐름 관련 문제를 해결하는 방법에 대해 설명합니다.www.cisco.com 첨부파일214553-troubleshoot-cvp-vxml-server.pdf파일 다운로드 ​소개이 문서에서는 Cisco CVP(Customer Voice Portal) VXML(Voice eXtensible Markup Language) Server Out ofMemory, 라이센스 및 통화 흐름 문제를 해결하는 방법에 대해 설명합니다.사전 요구 사항요구 사항다음 주제에 대한 지식을 보유하고 있으면 유용합니다.Cisco UCCE(Unified Contact Center Enterprise)Cisco 음성 포털VXML 서버 배포ASR(Automatic Speech Recognition) 및 TTS(Text To Speech)사용되는 구성 요소​이 문서는 특정 소프트웨어 및 하드웨어 버전으로 한정되지 않습니다.이 문서의 정보는 특정 랩 환경의 디바이스를 토대로 작성되었습니다.이 문서에 사용된 모든 디바이스는 초기화된(기본) 컨피그레이션으로 시작되었습니다.네트워크가 작동 중인 경우 모든 명령의 잠재적인 영향을 이해해야 합니다.​메모리 부족 문제 해결CVP에서 가장 일반적인 문제 중 하나는 VXML Server의 메모리 부족 및 보고 또는 오류 ""java.lang.OutOfMemoryError""로 인해 충돌이 발생한 경우입니다.CVP 11.0 버전까지는 이미지에 표시된 대로 VXML 서버에 할당된 메모리의 총 크기가 제한됩니다. Heap Memory, PermGen 및 Native Memory의 이 모든 컨피그레이션은 HKEY_LOCAL_MACHINE\SOFTWARE\Wow6432Node\Apache Software Foundation\Procrun2.0 \VXMLServer\Parameters\Java\Options에서 구성합니다.메모리 부족 문제 분류충돌 Heap 문제:오류:OutOfMemory ""java.lang.OutOfMemoryError""JVM 스레드:JVM 스레드를 생성할 수 없습니다.해결책:필요한 경우에만 맞춤형 애플리케이션 문제 해결/힙메모리 증가11.0의 경우:알려진 VXML 문제에 대해 ES 15 및 ES-25 설치PermGen 문제:오류:""lang.OutOfMemory오류:새 네이티브 스레드를 만들 수 없습니다.""해결 방법: Permgen을 256으로 증가(최대)11.5의 경우:기본 PermGenSize는 512입니다(CSCvc71931 참조).JVM 충돌:로그 파일:hs_err_pid*.logJVM과 타사 라이브러리 통합(프레젠테이션에서 타사 라이브러리를 구축하는 방법 참조)관련 로그C:\Cisco\CVP\VXMLServer\logs에서 가능한 Java Heapdump(*.hprof)를 찾습니다.PermGen 오류를 찾습니다.C:\Cisco\CVP\VXMLServer\Tomcat\logsJVM Crash L hss_err_pid_* 찾기:C:\Cisco\CVP\VXMLServer\Tomcat\bin & C:\windows\system32.툴JConsoleVisualVM이클립스 매트JConsole온라인 모니터에 jConsole을 사용합니다.VisualVMVisualVM을 사용하여 VXML 서버 및 애플리케이션 성능을 모니터링합니다.Heap 및 permgen 모니터링PermGen 모니터링애플리케이션 스냅샷스레드 덤프 가져오기 - 차단된 스레드 찾기Heap Dump/스냅샷엘리프 MATEclipse MAT를 사용합니다. VXML 서버가 이미 crash되어 충돌 원인을 알고 싶을 때MAT에 hprof를 로드합니다.File(파일) > Open Heap Dump(힙덤프 열기)로 이동합니다.보고서에 누설 시 문제 의심 항목이 표시됩니다.VXML 업그레이드 문제 해결VXML을 업그레이드하는 동안 발생하는 다른 일반적인 문제는 릴리스 정보 및 호환성 매트릭스에서 자세한 내용을 참조하십시오. 다른 TOMCAT 버전을 고려하십시오. 응용 프로그램을 업그레이드할 때 사용자 지정 응용 프로그램 개발이 이 클래스 계층 구조를 따라야 합니다.한 TOMCAT 버전에서 제대로 작동하는 타사 라이브러리는 새 TOMCAT에서 제대로 작동하지 않을 수 있습니다.   참고:  CVP_War를 사용하여 사용자 지정/타사 Jar 파일을 배치하지 마십시오.  ​비메모리 문제 해결(통화 흐름, ASR 및 TTS)다음 위치에서 로그를 수집합니다.C:\Cisco\CVP\logs\VXML - 이러한 로그를 확인하고 여러 애플리케이션에서 라이센스 관련 문제/VXML 호출을 추적할 수 있습니다.C:\Cisco\CVP\VXMLServer\logsAdminLogger(응용 프로그램 배포 관련 문제)전역 통화 로거(VXML 애플리케이션 통화 문제)오류 로거(전역 응용 프로그램 오류 로거, 응용 프로그램 실행과 관련된 오류 로그)C:\Cisco\CVP\VXMLServer\Tomcat\logsTomcat 로그(Tomcat과의 통합으로 문제가 발생하는 애플리케이션을 디버깅하는 데 유용함)애플리케이션 로그:C:\Cisco\CVP\VXMLServer\applications\>애플리케이션>\로그활동:응용 프로그램 실행오류:응용 프로그램에 오류 로그관리자:일반적임  Cisco 엔지니어가 작성Raghu GuvvalaCisco 엔지니어이 문서가 도움이 되셨습니까? 피드백지원 문의지원 케이스 접수(시스코 서비스 계약 필요)관련 지원 커뮤니티 토론CCMP 11.0.1 installation issue "
"[파이썬] 인공지능 스피커 만들기 : TTS(텍스트를 소리로), STT(소리를 텍스트로) 활용방법 ",https://blog.naver.com/themail0/223021852888,20230220,"#파이썬으로_인공지능_스피커 만들기#TTS(텍스트를 소리로)_활용방법#STT(소리를 텍스트로)_활용방법 1. TTS 이용하기 코딩from gtts import gTTS #1) 에러 시 대응from playsound import playsound#2) 에러 시 대응import os file_name = 'sample tts.mp3'####### english 연습text_en = ""can i help you?""tts_en = gTTS(text=text_en, lang='en', slow=False)tts_en.save(file_name)playsound(file_name)####### korea 연습text_ko = ""안녕하세요... 작동이 잘 됩니다...크크""tts_ko = gTTS(text=text_ko, lang='ko', slow=False)tts_ko.save(file_name)playsound(file_name) ####### korea : txt_file 연습with open('TTS_sample.txt', 'r', encoding='utf8') as f:text_ko=f.read()tts_ko = gTTS(text=text_ko, lang='ko', slow=False)tts_ko.save(file_name)playsound(file_name)     #1) 에러 => 명령 프롬프트에서 gTTS 설치C:\Users\a\AppData\Roaming\Python\Python310\Scripts>pip install gTTS#2) 에러 => 명령 프롬프트에서 playsound 설치 (구버전)C:\Users\a\AppData\Roaming\Python\Python310\Scripts>pip install playsound=1.2.2    2. STT 이용하기 코딩import speech_recognition as sr #1) 에러 시 대응r = sr.Recognizer() '''# 마이크에서 오디오 읽기with sr.Microphone() as source:print(""말해보세요."")audio = r.listen(source) #2) 에러 시 대응'''### file sound to text (wav 0, mp3 X)with sr.AudioFile(""sample.wav"") as source: audio = r.record(source) # 음성을 텍스트로 변환하기try:text = r.recognize_google(audio, language='ko-KR')print(""인식된 문장: "" + text)except sr.UnknownValueError:print(""음성을 이해할 수 없습니다."")except sr.RequestError as e:print(""Google Speech Recognition 서비스에 접속할 수 없습니다.; {0}"".format(e)) #1) 에러 => 명령 프롬프트에서 STT 설치C:\Users\a\AppData\Roaming\Python\Python310\Scripts>pip install SpeechRecognition#2) 에러 => 명령 프롬프트에서 PyAudio 설치C:\Users\a\AppData\Roaming\Python\Python310\Scripts>pip install PyAudio    "
네이버 클로바노트 핸드폰으로 음성녹음하기 ,https://blog.naver.com/put31120/222759461588,20220604," ​​​​​ 안녕하세요연휴를 만끽하고 하며 블로그에 진심을 담는 집사입니다.​​오늘은 일상생활에 아주 꼭 필요한 음성 녹음 앱을 포스팅해 보려 합니다.​ ​​​네이버에서도 개발 중인 음성녹음 클로바 서비스의 하나로 제가 일상생활에 핸드폰으로 주로 사용하고 있는 클로바노트 앱입니다.​​예전에 이런 종류의 앱을 사용하면 조잡하다고 해야 하나??완성도가 너무 좋지 않아서 실생활에 크게 사용하지 않았습니다만요즘에는 기술의 발전으로 인해 AI가 소리를 인식하고 스스로 글자로 변환해 기록해 주는 기술로까지 이어졌어요​ ​​​IOT라는 사물 인식 서비스가 발전함에 따라 사람의 말 하는 소리를 인식하는 부분 역시 엄청 중요해져서 기업마다 이 서비스에 빠르게 투자해서 기술력을 늘리고 있어요애플의 시리. 삼성의 빅스비가 대표적인 예로 들 수 있습니다. ​네이버님의 클로버 서비스는 쉽게 말하면 AI 인식 서비스로 기억하면 되는데 이 서비스를 여러 상황에 따라 사용할 수 있게끔 나눠 놓은 게 마음에 드네요​저는 이 중에 클로버 노트라는 녹음 기록에 관련된 내용을 포스팅하려 합니다.​​​네이버 클로버 서비스  네이버의 인공지능 플랫폼 음성 인식 비서 서비스. LG U+의 주력 AI/IoT 서비스이기도 하다.​네이버의 인공지능 플랫폼. 보통 클로바 스마트 스피커로만 알고 있으나, AI 스피커 이외에도 Clova Speech Recognition, Clova Speech Synthesis, Clova Face Recognition, Clova Premium Voice 등의 서비스를 제공하고 있다​음성 인식 서비스의 경우에는, 대부분의 인식 비서 서비스들과 비슷하게 자연어 처리,  딥러닝 기술을 사용한다.나무위키​ 클로바 노트 - 무료 (IOS 앱스토어 화면)​​이 앱은 무료로 제공되고 있어요 모바일과 태블릿 두 가지 버전 모두 활용이 가능합니다.네이버 계정이 연동돼서 크로스로 사용할 수도 있어요​​기술이 발전하기 전 시절에는 녹음을 하고 나서 하나씩 들어보면서 이걸 글자로 적기 바빴던 거 같은데 이제 알아서 글자로 변환을 자동으로 해줍니다..​시간이 흐를수록 점점 기술의 발전은 사람을 편리하게 하네요​​ 긴 대화록을 문자로 바꾸어주는 speech-to-text(STT) 서비스이다.음성 파일 한 개당 180분까지 변환 가능하다.총 이용 시간은 매달 기본 300분에 최대 600분이며, 다음 달에 또 제공된다.나무위키​​  ​​​처음 들어가면 제 계정을 눌러서 요렇게 확인이 가능한데 600분 사용이 가능합니다. 친구 초대를 하면 추가 사용시간이 늘어나요​한 달이 지나면 다시 사용시간이 초기화되니 제 라이프 스타일로 볼 때 크게 부족하지 않은 것 같습니다.​​​​ 아이패드 (태블릿) 세로 모드 화면​​​이 화면이 홈 화면입니다그동안 녹음해서 사용했던 리스트가 보입니다리스트는 서로 공유할 수 있으며 파일 업로드로도 사용하실 수도 있어요 ​​​ 음성 녹음 실기 이미지 (태블릿 세로 화면)  ​​​기기의 스피커를 통해 바깥의 소리를 녹음하는 일반적인 상황도 있지만 이렇게 유튜브나 영상을 틀어놓고 사용하실 수도 있습니다. ​​AI의 인식률이 굉장히 높아요 거의 완벽할 정도의 인식률을 보여주고 있습니다.녹음된 소리는 본인이 원하는 위치로 돌아가서 다시 들어볼 수 있고요 ​중요한 키워드 부분을 자동으로 잡아주기도 합니다.  ​​ ​​​최근 업데이트된 버전의 내용에 중국어도 새롭게 인식해 준다고 공지가 되어있습니다.외국어도 선택을 해서 사용하면 자동으로 변환이 가능하다고 하니 참 편리하게 적용될 거 같습니다.​  ​​​음성인식이 텍스트로 변환되어서 보이면 해당 부분을 하이라이트 해서 중요 부분을 저장해놓을 수 있습니다. 회의를 기록을 하거나 영어 공부를 하시는 분들에게 크게 도움이 되시는 부분 같아요  ​​녹음이 끝나고 나서 이렇게 카테고리를 나눠놓을 수 있어서  종류를 선택하여 보관할 수도 있습니다.​ 참석자를  AI 스스로 구분하여 판단 ​​녹음 시 예를 들면 들리는 소리가 다른 부분을 AI가 스스로 인지해서 사용자를 자동으로 나누어 주기도 합니다​문단이 나눠진 부분을 따로 탭 해서 보관이나 북마크, 메모 등이 가능해요​​ 모바일 버전 클로바노트 화면 ​​​모바일 버전의 화면입니다. 네이버 계정으로 연동이 가능하기 때문에 기기마다 같은 녹음 내용을 편집하거나 저장을 해놓을 수 있어요 ​선택 후 하이라이트 처리된 부분은 텍스트 부분에 연한 연두색으로 음영 처리되어 있으며하단의 녹음 재생 위치에도 같은 색으로 처리됩니다. ​재생하는 부분을 동시에 위치를 따라가면서 북마크 지정 역시 가능하며 재생 속도 조절도 가능합니다. ​​ 태블릿 버전 (가로 화면)​​재생하는 내용을 들으면서 옆 노트 영역에 필요한 메모를 추가하여 작성이 가능하고 하래의 재생 구간 부분 역시 선택해서 다시 재생하거나 위의 텍스트 창을 통해 재생 선택도 가능합니다.​​ ​​뉴스를 녹음했을 때 이미지입니다. ​아나운서와 리포터가 서로 주고받는 내용을 정확하게 구분하여 참석자 1 참석자 2로 나누어 처리되는 모습이 확인되는 것을 보실 수 있어요​  ​​포스팅하기 위해 방금 유튜브 뉴스를 녹음하여 클로바노트를 사용한 영상입니다. ​핸드폰으로 누구나 쉽게 사용이 가능하고예전처럼 따로 다시 듣고 쓰는 일을 줄여줘서 편리함이 가득한 착한 AI 음성녹음 앱입니다.​​​​클로바 노트 공식 홈페이지 링크입니다   클로바노트눈으로 보며 듣는 음성 기록search.naver.com ​ 눈으로 보며 듣는 음성녹음똑똑한 AI가 정확하게 기록과 재생을 해주는 편리한 프로그램 핸드폰으로도 사용할 수 있는클로바노트 였습니다.​감사합니다 오늘도 즐거운 하루 보내세요   ​ "
영어숙어 및 구동사 (BBC 6분영어) - Can AI have a mind of its own? ,https://blog.naver.com/softca/222998799992,20230130,"본 해설 자료는 첨부파일(PDF)로 올려져 있습니다. 다운받으셔서 활용하십시오​Is artificial intelligence capable of consciousness? We’ll hear from an expert who thinks AI is not as intelligent as we sometimes think, and as usual, we’ll be learning some new vocabulary as well.​인공 지능이 자각할 수 있을까요? 우리는 [인공 지능이 [우리가 때때로 생각하는 것] 만큼 지능적이지는 않다고 생각하는] 전문가로부터 들을 것입니다. 그리고 마찬가지로, 늘 그렇듯이 우리는 몇몇 새로운 어휘를 배우고 있을 것입니다.​이 글은 금주의 6 Minute English에 수록된 구동사, 숙어 등 여러 단어로 구성된 어휘를 간단히 정리한 자료입니다.이 글에는 한영(Eng/Kor) 번역과 구성단어 정도만 포함합니다. 이것만으로도 6 Minute English를 듣기와 독해 중심으로 공부하시는 분들에게는 충분한 힌트가 됩니다.​​work on [동사] [6Rank; 104] [구동사][E/K] ① (설득시키려고) ~에게 공을 들이다 ② (해결/개선하기 위해) ~에 애쓰다[공들이다]​​artificial intelligence [명사] [6Rank; 694][E/K] 인공 지능​​capable of [형용사] [6Rank; 435][E/K] ~할 수 있는​​as usual [부사] [6Rank; 270][E/K] 늘 그렇듯이[평상시처럼]​​as well [부사] [6Rank; 18] [전치사부사구][E/K] ① (~뿐만 아니라/~은 물론) …도 ② 게다가, 또한, 마찬가지로​​what happened to [명사] [6Rank; 668][E/K] ① ~에게 일어난 일 ② ~에게 무슨일 있었는지​​similar to [형용사] [6Rank; 236][E/K] ~와 비슷한​​come to [동사] [Oxf5] [6Rank; 20] [구동사][E/K] ① (총계가) ~이 되다 ② (특히 좋지 않은 상황이) 되다 ③ 알아차리다. ④ (생각이) 들다[나다]​​come to life [동사] [6Rank; 517] [숙어동사][E/K] ① 활기를 띠다 ② 살아 움직이다​​fall in love [동사] [6Rank; 255][E/K] 사랑에 빠지다​​full of [형용사] [6Rank; 189][E/K] ~로 가득찬​​talk about [동사] [6Rank; 1] [구동사][E/K] ① …에 대해 이야기하다. ② (강조의 뜻으로) …하기란[…하기가 말도 못한다]​​automatic speech recognition [명사] [6Rank; 1506][E/K] ① 음성인식기술 ② 자동음성인식​​go on [동사] [Oxf5] [6Rank; 23] [구동사][E/K] ① (공연을) 시작하다, (무대에) 나오다 ② (스포츠 경기 중에 어떤 선수 대신에) 들어가다 ③ (불/전기 등이) 들어오다 ④ (시간이) 흐르다 ⑤ 일어나다[벌어지다] ⑥ (어떤 상황이) 계속되다 ⑦ (잠깐 쉬었다가) 말을 계속하다 ⑧ 자자[어서](무엇을 하도록 장려하는 말) ⑨ (부정문/의문문에 쓰여) ~을 (판단/의견의) 근거로 삼다 ⑩ 말도 안 돼​​wishful thinking [명사] [6Rank; 1024][E/K] 희망 사항​​in relation to [전치사] [6Rank; 485][E/K] ~에 관하여​​related to [파생전치사] [6Rank; 336] [형용사전치사구][E/K] ~와 관련 있는​​in connection with [전치사] [6Rank; 1814][E/K] ~과 관련되어​​in this way [부사] [6Rank; 366][E/K] 이렇게 하여​​come from [동사] [Oxf5] [6Rank; 22] [구동사][E/K] ① (진행형으로는 안 씀) ~ 출신이다(고향이나 사는 지역을 나타냄) ② ~에서 나오다[비롯되다/생산되다] ③ ~의 결과이다​​as if [접속사] [6Rank; 819][E/K] 마치 …인 것처럼, 흡사 …와도 같이​​put on [동사] [Oxf5] [6Rank; 44] [★★] [구동사][E/K] ① ~을 입다[쓰다/끼다/걸치다] ② (얼굴/피부 등에) ~을 바르다 ③ (기구 등을) 가동[작동]시키다 ④ (시디/테이프 등을) 틀다 ⑤ (무게 등이) 더 무거워지다, (살이) 찌다 ⑥ ~을 특별 공급하다 ⑦ (연극/쇼 등을) 무대에 올리다[공연하다], 상연하다 ⑧ 가장하다[꾸미다/…인 척하다] ⑨ (전화를) ~에게 바꿔 주다 ⑩ 놀리다​​all the time [부사] [6Rank; 112][E/K] ① 내내[줄곧] ② 아주 자주​​action figure [명사] [6Rank; 1472][E/K] (영화 등에 나온) 영웅이나 캐릭터 인형​​and so on [부사] [6Rank; 1489][E/K] 기타 등등, …등​​in the habit of [전치사] [6Rank; 1832][E/K] …하는 버릇이 있는.​​assign to [동사] [6Rank; 1496][E/K] ① …에 배속[배정]하다, (사건의 일시, 장소 등을) …(로) 정하다. ② 소속시키다​​taken in [형용사] [6Rank; 1004][E/K] 속임수에 넘어간​​instead of [전치사] [Oxf5] [6Rank; 682][E/K] … 대신에​​data analysis [명사] [6Rank; 1635][E/K] 자료 분석​​trick into [동사] [6Rank; 496] [구동사][E/K] ~를 속여서[~에게 사기를 쳐서] ~하게 하다​​as a result [부사] [6Rank; 132][E/K] 결과적으로​​deal with [동사] [Oxf5] [6Rank; 36] [구동사][E/K] ① (문제/과제 등을) 처리하다 ② (주제/소재로) ~을 다루다 ③ ~를 (상)대하다 ④ ~와 거래하다​​speaking of [전치사] [6Rank; 418][E/K] …에 관해서 말한다면, …의 이야기라면​​main character [명사] [6Rank; 1881][E/K] 주인공​​connect with [동사] [6Rank; 1606][E/K] ① …와 연락[접속]하다. ② …와 관련시키다[연결하다].​​one day [부사] [6Rank; 1277][E/K] 언젠가[어느 날]​​even though [접속사] [6Rank; 11022][E/K] 비록 …일지라도, 설사 …이라고 할지라도​​for now [부사] [6Rank; 1726][E/K] ① 우선은, 현재로는, 당분간은 ② 이제 곧​​ ​ 새내기할배 블로그 사이트에는 [6 Minute English] 자료와 관련하여 다음과 같은 글들이 매주 포스팅 됩니다단어공부(금주 대본에 나온 주요 어휘) https://blog.naver.com/softca/222998793603멀티단어공부(금주 대본에 사용된 멀티단어(구동사, 숙어) 간단 정리) 지금 읽고 계신 글입니다.독해공부(금주 대본의 문장별 해석(직역) 자료) 현재 작성 중입니다.문장별 문법공부(문장별 문법 설명) 현재 작성 중입니다.금주의 어휘(금주 대본에 나온 금주의 어휘를 집중적으로 정리한 자료) 현재 작성 중입니다.항목별 문법공부(금주 대본에 나온 문법을 주제별/항목별로 정리한 자료) 현재 작성 중입니다.금주의 구동사(금주 대본에 사용된 구동사 상세 정리) 현재 작성 중입니다.금주의 PDF(금주의 PDF 및 기타 다운로드 파일) https://blog.naver.com/softca/222998788018 ​​금주의 PDF 파일은 이곳을 클릭하세요.PDF파일을 서로이웃에게만 공유됩니다.​ 그러나 어휘 공부가 필요하신 분은 PDF 파일을 참고하실 것을 추천합니다. 제공하는 PDF 파일에는 관련문장, 영영(Eng-Eng), 유의어, 반대어, 같은 철자의 다른 품사, 구성단어 등 어휘를 보다 입체적으로 익힐 수 있는 자료가 들어가 있습니다. 특히 구동사에 관한 자료는 이 글과 별도로 상세히 정리된 새로운 글이 게제될 예정입니다. <끝> ​ "
"소리, 그 이상의 가치를 지닌 와이덱스 모멘트 쉬어(Widex Moment Sheer™) 보청기에 대해 함께 살펴볼까요? aka. 덴마크 하이엔드 보청기 ",https://blog.naver.com/wlsxo80/223091271664,20230502,"Widex Moment Sheer™ 대명보청기 청능재활센터의 이진태 전문청능사입니다.​현재 기준으로 와이덱스 보청기의 최신 플랫폼은 모멘트(MOMENT™), 그리고 매그니파이(MAGNIFY™) 제품군이 있는 M-플랫폼인데요.​이 M-플랫폼 모델 중 모멘트 보청기의 가장 큰 특징은 제로딜레이(ZeroDelay™) 기술을 기반으로 보다 자연스러운 소리를 제공하는 퓨어사운드(PureSound™) 라 할 수 있으며, 매그니파이 보청기의 경우에는 모멘트에 있는 퓨어사운드 기능은 탑재되어 있지 않지만, 각 개인별 음향 측정을 통해 더욱 정교한 사운드를 제공해주는 트루어쿠스틱(TruAcoustics™) 기능은 모멘트와 동일하게 탑재되어 있다는 특징을 지니고 있습니다.​그리고 모멘트, 매그니파이에 이어서 M-플랫폼의 1.5버전 같은 세 번째 모델인 와이덱스 모멘트 쉬어(Widex Moment Sheer™) 보청기가 출시됨에 따라, 이번 포스팅에서는 와이덱스 모멘트 쉬어 보청기에 대해 함께 살펴볼까 합니다.​새로운 패키지 새롭게 바뀐 패키지 - 이건 소장각!이번에 출시된 와이덱스 모멘트 쉬어는 보청기 패키지부터 새롭게 바꼈는데요.​기존 패키지에 비해 전체적인 색상 자체도 브랜드 컬러로 되어 있고, 무엇보다 와이덱스의 브랜드 슬로건인 SOUND LIKE NO OTHER 라는 문구가 음각으로 딱! 새겨져 있는 만큼, 와이덱스가 사운드에 얼마나 진심인지를 엿볼 수 있는 것 같기도 합니다. 와이덱스 모멘트 쉬어 보청기 케이스그리고 보청기를 보관하는 케이스 역시 새롭게 바꼈는데요.​기존 케이스에 비해 그 크기도 작아졌고, 전체적으로 무광에 메탈릭 컬러가 추가되어, 보다 깔끔하고 세련돼 보이는 것 같습니다. 뒤에서 또 알아보겠지만, 이 보청기 케이스는 와이덱스 모멘트 쉬어 제품군의 통일성 있는 디자인에 일조하고 있다는 것! ​품격 높은 디자인과 자연스러운 사운드 Good Design Award 2022 본상 수상 👏국제 디자인 공모전인 일본 굿 디자인 어워드 2022 에서 그 특유의 세련되고 컴팩트한 디자인으로 본상을 수상한 와이덱스 모멘트 쉬어 보청기는 앞서 언급했던 것처럼 새로운 플랫폼의 모델이 아니라, 기존 모멘트 플랫폼을 바탕으로 몇몇 부분을 업그레이드 한, 말하자면 모멘트의 1.5 버전 같은 모델입니다.​따라서, 기존 모멘트 보청기의 주요 특징인 퓨어사운드(PureSound™)나 트루어쿠스틱(TruAcoustics™)과 같은 기능들은 모멘트 쉬어 보청기에도 그대로 포함되어 있으며, 이런 소프트웨어적인 기능들은 기존 모멘트 보청기에 대해 알아보는 포스팅에서 다룬 적이 있지만, 다시 한 번 살펴본 후 계속해서 모멘트 쉬어 보청기에 대해 알아보도록 하겠습니다.​TruAcoustics™ & PureSound™​와이덱스 모멘트 보청기의 음질과 관련된 핵심 기술은 개인별 음향 특성을 반영하는 트루어쿠스틱과 보다 자연스러운 음질을 제공하는 퓨어사운드, 이 두 가지라고 볼 수 있습니다. 개별 음향 측정이 가능한 TruAcoustics™  ft.센소그램트루어쿠스틱(TruAcoustics™)은 사용자의 청력 및 외이도 환경에 따라, 개인별 음향 측정값을 통해 사용자의 환경에 최적화된 음질을 제공해주는 알고리즘으로, 개인마다 모두 다른 외이도 특성이 반영돼, 보청기 착용 후 울리는 소리에 대한 불편함이나 선명하지 않은 음질에 대한 불편함 등을 개선할 수 있습니다.​즉, 다른 사람에겐 적당한 소리지만, 나에겐 불편한 소리가 될 수 있는 주파수별 이득 값을 나에게 적당한 소리가 되도록 이득 및 파라미터를 보정 시켜주는 역할을 하는 기술이 트루어쿠스틱인 것이죠.​그리고 0.5ms 미만의 빠른 신호처리 속도로 인해, 지연(Delay)된 신호로 인한 소리의 왜곡을 방지하는 제로딜레이 기술이 기반이 된 퓨어사운드(PureSound™)를 통해, 특히 경중도 난청이 있는 경우, 보청기 착용 시 Comb filter effect 로 인해 느낄 수 있는 부자연스러운 음질을 제거할 수 있습니다.​여기서의 Comb filter effect 란 무엇을 말하냐.​일단, 청력 손실의 정도가 심하지 않은 경중도 난청이 있는 경우에는, 보다 자연스러운 음질을 위해 오픈 이어팁이나 벤트(Vent, 환기구)가 있는 이어팁 등을 주로 사용하게 되는데요. Comb filter effect 예시이 경우, 보청기의 마이크로폰을 통해 전달되어 증폭되는 소리와, 외이도의 이어팁 사이로 직접 전달되는 직접음(Direct sound)이 합쳐지면서, 원음과 다른 소리의 왜곡이 발생하게 되며, 이 왜곡된 소리의 주파수 반응이 빗의 형태를 지녀 Comb filter effect 라 일컫는 것입니다.​그럼 이런 왜곡이 왜 발생하느냐. 각각의 시간차가 발생하게 되는 직접음(흰색)과 증폭음(파란색)그건 바로 외이도로 직접 유입되는 소리와는 달리 보청기로 유입된 소리는 보청기 내부의 신호 처리 과정에 의해 약간 지연되어 고막에 도착하기 때문입니다.​즉, 증폭된 소리와 직접음이 고막에 도달할 때까지 각각의 시간차가 발생하게 되고, 이로 인해 원음과는 다른, 음의 왜곡이 발생하게 되는 것입니다. 와이덱스의 제로딜레이(ZeroDelay™) 기술이러한 음의 왜곡을 방지하기 위해서, 보청기를 통해 증폭되는 소리와 직접음과의 딜레이를 제로(0.5ms 미만)에 가깝게 처리하는 기술이 바로 제로딜레이 기술이며, 이 제로딜레이를 기반으로 생성된 프로그램이 퓨어사운드(PureSound™)인 것이죠. 좌측 : 타사 보청기 일반 프로그램 / 우측 : 와이덱스 보청기 PureSound™ 실제로 타사 보청기의 주파수 반응 곡선과 와이덱스 퓨어사운드 활성 시의 주파수 반응 곡선을 비교해보면, 증폭음과 직접음의 딜레이가 있는 경우에는 빗살 모양의 피크와 노치(Teeth of comb)가 있는 주파수 반응을 나타내는 반면, 퓨어사운드의 경우, 제로딜레이로 인해 comb filter effect 없이, 자연스러운 주파수 반응을 나타내는 것을 볼 수 있습니다.​새로운 하드웨어 Q : 모멘트와 모멘트 쉬어는 완전히 동일한 폼팩터를 가진다?                            A : 아니용 와이덱스 모멘트 쉬어 보청기는 얼핏 보면, 기존 모멘트 보청기와 비슷하게.. 아니 똑같아 보이지만, 완전히 똑같지는 않습니다.​자세히 보면, 하우징 옆면에 WIDEX 로고가 기존에는 볼드체 였다면, 모멘트 쉬어에서는 일반체로 바뀐 걸 알 수 있으며, 모멘트와 달리 모멘트 쉬어에서는 마이크로폰 커버가 재도입돼 보다 깔끔해 보이기도 합니다.  착용감 Good! 음질 Good! - 가장 자연스러운 소리는 외이도에서 시작됩니다!그리고 모멘트 쉬어 출시와 함께 새로운 이어팁인 슬리브 이어팁(Sleeve Ear-Tip)도 함께 소개되었습니다.​슬리브 이어팁에는 벤트가 있어 착용 시 폐쇄감이 적은 슬리브 벤트 이어팁과 벤트 없이 이득을 충분히 제공할 수 있는 슬리브 파워 이어팁, 이 두가지 종류가 있으며 각각 S, M, L 사이즈에다, 외이도가 협소한 경우에도 사용할 수 있는 XS 사이즈의 이어팁도 마련되어 있습니다.​이어팁 부분은 피지컬적인 착용감도 중요하지만, 소리의 유입 및 유출 등과 관련된 음향적인 부분 역시 사용자의 만족도와 직결되는 중요한 부분인데요. 새로운 이어팁을 사용한 보청기 착용자의 음질 관련 평가 척도모멘트 쉬어 보청기와 함께 사용할 수 있는 슬리브 벤트 이어팁과 슬리브 파워 이어팁 모두는 한단계 높은 음향적 안정성과 편안한 착용감으로 보다 높은 만족도를 제공해 줍니다.​새로운 충전기 WIDEX sRIC Charger (WPT103)와이덱스 모멘트 쉬어 보청기와 함께, 모멘트 쉬어 전용 충전기도 새롭게 출시되었는데요.​위 사진에서 보시다시피 콤팩트한 디자인의 모멘트 쉬어 전용 충전기에서 한눈에 띄는 점은! 바로 모멘트 쉬어 제품군 간의 통일성 있는 메탈릭 마감 처리입니다. 번거로움은 제거!그리고 거치 안정성 향상을 위해 기존 클래식 충전기에 있던 힌지 커버를 제거한 점도 눈에 띄며 안정성은 향상!충전기 자체가 네모반듯한 형태가 아니라 하단으로 갈수록 넓어지는 형태를 지녀, 실제 사용시 더욱 향상된 거치 안정성을 느낄 수도 있습니다. 불편한 점도 제거!보청기를 삽입하는 충전 슬롯의 방향도 기존 클래식 충전기의 슬롯 방향에서 180도 회전함에 따라 오른쪽 보청기를 왼쪽 슬롯에 삽입하는 등의 사용하기 불편했던 점도 개선되었습니다.  참고로 현재까지 출시된 와이덱스 충전기는 모두 인덕티브 방식의 충전이 이뤄지는데요.​그렇다 보니 보청기 자체에 충전을 위한 단자가 필요없고, 단자가 없으니 습기 등이 단자 틈으로 유입될 일이 없기도 하고, 자칫 단자가 맞물리지 않아 방전이 될 일도 없어, 확실히 배터리 부분의 고장률이 현저히 낮다는 게 와이덱스 충전식 보청기의 장점이기도 합니다.​와이덱스 모멘트 앱 CES 2023 소프트웨어 & 모바일 앱 부문 혁신상 수상 👏 iOS 및 Android 기기에서 와이덱스 모멘트 앱을 사용하면 보다 다양한 기능과 각종 설정들을 제어할 수 있는데요.​보청기 볼륨 조절이나 프로그램 변경 등의 일반적인 기능 제어는 물론, 머신러닝 기반의 맞춤형 사운드를 제공하는 마이사운드(MySound) 2.0, 이명 완화 솔루션인 젠(Zen) 을 기반으로 편안한 사운드를 제공하는 사운드릴렉스(SoundRelax™), 그리고 보청기 사용에 대한 다양한 팁과 정보를 제공해주는 마이가이드(My Guide) 까지, 모멘트 앱 하나에 보청기 사용자를 위한 다양한 기능들이 탑재되어 있습니다.​이 모멘트 앱의 다양한 편의 기능과 제어 기술의 우수성으로 인해, 와이덱스 모멘트 앱은 세계 최대 IT 가전 전시회 ‘CES 2023’ 소프트웨어 & 모바일 앱 부문 혁신상을 수상했습니다.​여튼 모멘트 앱의 각 기능들에 대해 조금만 더 자세히 살펴보면. 인공지능(AI) 기반의 머신러닝 기술이 탑재된 MySound 2.0일단, 머신러닝 기반의 개인 맞춤형 사운드를 제공하는 MySound 는 인공지능 기술과 보청기 착용자의 의도를 결합해 사용자가 다양한 청취 환경에서 항상 원하는 소리를 들을 수 있도록 해줍니다.​게다가 보다 업그레이드된 MySound 2.0 은 새로운 파라미터인 압축을 통해 더욱 개인화된 청취 경험을 제공해 주는데요.​이는 다양한 압축 설정에 따라 사용자의 청취 선호도가 많은 차이를 보임에 따라, MySound 2.0 부터는 그 압축 설정을 개인화 하기 위해 압축을 필수 파라미터로 설정한 것입니다.​즉, 이퀄라이저 조정을 통해 프로그램을 생성한 MySound 와 달리 MySound 2.0 에서는 이퀄라이저와 압축을 모두 조정하는 프로그램을 생성할 수 있으며, 이를 통해 사용자가 있는 청취 환경과 개인의 취향에 따라 훨씬 더 개인 맞춤화 된 프로그램을 만들 수 있는 것이죠.​ 보청기 기능에 있어 압축(Compression)이란 개념은 매우 중요합니다. 와이덱스 MySound 2.0 에서는 느린 압축과 빠른 압축 비교를 통해 사용자의 청취 환경에 적합한 음질 또는 가청도를 제공해 줍니다. MySound 2.0 은 개인 맞춤형 사운드에 대한 두 가지 접근 방식인 Made for You 그리고 Made by You 를 기반으로 합니다. Made for You = 사용자를 위한 추천 사운드Made for You 는 전세계 와이덱스 보청기 착용자의 사운드 선호도를 수집한 데이터 베이스를 기반으로 사용자가 있는 청취 환경에 최적화된 두 가지 사운드를 권장 사항으로 추천해 줍니다.​이 두 가지 추천값은 이 전 비슷한 상황에서 비슷한 의도로 앱을 사용한 다른 와이덱스 사용자들의 선호도와 청취 상황이 반영된 것으로, 사용자는 실시간으로 두 가지 사운드를 비교해서 들어본 후 현재 나의 상황에 적합한 소리를 선택하기만 하면 됩니다.  Made by You = 사용자가 직접 만드는 사운드이처럼 맞춤형 사운드를 제시해 주는 Made for You 접근 방식과는 달리 Made by You 는 사용자가 직접 A/B 사운드를 비교해 어느 쪽 사운드가 얼마나 더 나은지를 단계별로 선택해 나가는 접근 방식으로, MySound 2.0 에서는 압축 설정이 포함되도록 업데이트 되어, 기존 MySound 보다 더 많은 사운드 비교가 가능합니다.​A/B 사운드 비교가 끝나고 생성된 사운드를 나만의 프로그램으로 저장해 놓으면, 다음 번 유사한 청취 환경에서는 또 다시 사운드를 비교할 필요없이, 프로그램 선택 만으로 나만의 선호도가 반영된 해당 청취 환경에 최적화된 사운드를 경험할 수 있는 것입니다.   SoundRelax™ - 웰빙이 대세 그리고 MySound 와 함께 와이덱스 모멘트 앱에서 활용할 수 있는 이명완화 프로그램 사운드릴렉스 역시, 모멘트 앱이 ‘CES 2023’ 혁신상을 수상하는 데 일조한 것 같은데요.​사운드릴렉스는 와이덱스의 이명 완화 솔루션인 Zen 을 기반으로, ‘소리를 통한 웰빙’에 중점을 두고개발한 이명 완화 프로그램으로 Zen 이 이명 완화 만을 위한 솔루션이었다면, 사운드릴렉스는 이명의 유무에 관계없이 모든 보청기 사용자를 대상으로 이명을 관리하고, 마음을 진정시키며 집중력을 높일 수 있도록 설계된 솔루션입니다.​사운드릴렉스는 Zen 에서 사용되는 프랙탈(Fractal) 톤 뿐만 아니라 변조된 파도 소리도 제공하는데요.​이 사운드릴렉스에 대한 내용은 이번 포스팅에서 알아보기엔 내용이 너무 광범위해서 다음 기회에 더욱 자세히 알아보기로 하고, 사운드릴렉스에서 제공하는 3가지 사운드에 대해서만 첨부한 영상에서 잠깐 만나보시죠.    여기까지 새롭게 업그레이드된 와이덱스 모멘트 쉬어의 대표적인 특징들에 대해 살펴봤는데요.​요약하자면, 하드웨어적으로는 세련되고도 더욱 편리해진 디자인을.​소프트웨어적으로는 보다 개인 맞춤형 사운드와 웰빙 사운드를 제공한다는 특징이 있습니다.​그러나, 개인적으로는 이번 모멘트 쉬어 보청기에서 가능했으면 좋겠다고 생각했던 부분인 Bidirectional streaming. 즉, 양방향 스트리밍을 통한 온전한 핸즈프리 기능이 반영되지 않은 점은 좀 아쉬운데요. Widex Sound Assist™뭐 그 대신 와이덱스 사운드 어시스트(Widex Sound Assist™) 라는 액세서리를 출시함에 따라, 핸즈프리 기능은 물론이고, 대화 상대방의 말소리에 포커스를 두는 파트너 마이크 기능, 말소리가 있는 쪽에 방향성 포커스를 두는 테이블 마이크 기능 등을 활용할 수 있게 되었습니다(국내 출시 예정).​ 통일성 있고 품격 높은 디자인의 와이덱스 모멘트 쉬어 제품군여튼 이 사운드 어시스트 부터 TV플레이, 케이스, 충전기, 그리고 보청기까지 세련되고도 통일성 있는 디자인을 선보이고 있는 이번 모멘트 쉬어 라인업은 특히나 와이덱스가 추구하는 아이덴티티를 엿볼 수 있는 제품군인 것 같습니다.​듣는 즐거움에 보는 즐거움도 추가된 와이덱스 모멘트 쉬어에 대한 이야기는 여기까지 하고, 이번 포스팅에서 자세히 다루지 못한 부분은.. 앞으로 계속됩니다! <明>​​난청/보청기 상담 전화 010-3163-1121이진태 전문청능사(Audiologist)​대구가톨릭대학교 의료과학대학 언어청각치료, 경영 전공한국청능사협회 / (사)한국청각언어재활학회 평생회원한림국제대학원 청각언어연구소 우수연구개발상 수상한국청각언어재활학회 학회장상 수상국제학술지 'Journal of audiology & otology' 연구논문 참여 (Speech Recognition in Real-Life Background Noise by Young and Middle-Aged Adults with Normal Hearing)Professional Certificate of Competence in AudiologyCertificate of Life Membership KAACertificate of Completion in HA Repair CourseCertificate of Completion in HA Specialization ProcessCertificate of Completion in Starkey Academy  [공유] 하이엔드 보청기의 시작을 알리다, 와이덱스 모멘트 쉬어(Widex Moment Sheer™)안녕하세요! 하이엔드 보청기의 시작을 알리다, 소리 그 이상의 가치, 와이덱스 보청기입니다. 1956년, 덴...blog.naver.com 네이버 예약 :: 대명보청기대구 반월당에 위치한 대명보청기 전문센터는 1997년에 오픈하여 난청을 겪고 있는 많은 분들에게 전문적인 청각 서비스를 제공하고 있습니다. 청각학 및 보청기에 관한 전문교육을 받은 청능사(Audiologist)로 구성된 대명보청기에서는 난청과 관련해 고객님 개개인에 맞는 평가(Assessment), 적합(Fitting), 후속조치(Follow-up)에 초점을 맞춰 보다 전문적인 청각 서비스를 실시하고 있습니다.  고객님들이 들으시는 소리를 더 만족시켜드리기 위하여 고객님의 마음의 소리까지 귀 기울여 듣는 보청기 전문센터가 되도록 최...booking.naver.com 궁금할 땐 네이버 톡톡하세요! 대명보청기대구광역시 중구 중앙대로 355 #와이덱스 #보청기 #모멘트쉬어 #충전식보청기 #대명보청기 #대구보청기 #이진태청능사 #전문청능사 "
"음성인식, STT와 TTS의 차이점 ",https://blog.naver.com/broadcns2013/222987424374,20230118,"​  ​AI가 산업 전반에 핵심 전략으로 자리 잡고있는 가운데 콜센터 업계에서는 AI가 접목된콜센터 AICC가 새 트렌드로 떠오르고 있습니다. ​글로벌 시장조사업체 리서치 앤 마켓에따르면 AI컨택센터 시장은 2020년 115억 달러(약 14조 원) 규모에서 2025년 361억 달러(약 46조 원)로 커질 전망이며,국내 기업들도 이러한 트렌드에 따라 AI를 도입해 콜센터를 고도화하는 등 AICC 사업을 확대하고 있습니다. ​ AI컨택센터는 인공지능(AI)을 기반으로 한 고객센터로서 음성인식, 문장 분석, 대화 엔진 등의 AI 기술을 적용해 고객의 질문에 답변을 하게 됩니다. ​시간 및 장소에 구애받지 않고 효율적으로 고객을 응대할 수 있으며,운영비용 또한 대폭 절감할 수도 있기에 활용 범위가 급속히 확대되고 있습니다. ​인간과 유사한 목소리로 일상적인 언어를구사해 고객의 질문에 적절하게 대응하며, 실시간으로 상담 내용을 파악해 상담사에게 답변과 관련된 정보를 찾아주는 업무를 하고 있습니다. ​상담원의 경우24시간 고객을 응대하기가 어렵지만AI 컨택센터를 이용하면 언제든지 고객에게서비스할 수 있다는 큰 장점이 있습니다. ​이렇게 진화, 발전된 AI컨택센터는 무한 확장성을 가지고 있어 각종 산업분야에 활용되고 있습니다. ​AI 음성인식 기술이 더 이상 기술로만 머무는 것이 아니라, 일상 생활 곳곳에서 쓰이고 있는 서비스에 도입되어 이용자들에게 한 차원 더 높은 편의를 제공하고 있습니다.​저희 브로드씨엔에스는 음성인식 솔루션을중심으로 다양한 AI 관련된 서비스를 개발하고사업을 추진하고 있습니다.​​AI 음성인식 기술은 어떤 원리에 따라 작동하는 것일까요?스마트 스피커의 예를 들어 설명해 보겠습니다. ​ ​우리가 스마트 스피커를 향해서음성으로 명령을 전달하면 인공지능은 사용자의 명령어를 음성에서 텍스트의 형태로 변환한 뒤에 해당 텍스트를분석하여 명령어에 포함된 의도를 파악합니다. ​그 다음 분석에 대해 적절한 응답을 선택한 뒤에 이를 다시 음성의 형태로 변환하여 전달하는 과정을 거치게 됩니다. ​AI 음성인식의 필수적인 STT 기술과 TTS 기술에 대해 알아보도록 하겠습니다.​​ STT 기술1. 음성인식의 개념, 현재 기술과 발전​음성인식(Automatic Speech Recognition; ASR)이란?사람의 말(음성)을 글로 변환하는 말하며흔히 STT(Speech to Text)라고 부릅니다.​음성인식은 받아쓰기와 비슷하지만 그 자체에 완벽한 언어적인 이해까지 포함하고 있지는 않습니다. 달리 말하면, 들리는 대로 받아 적는데 일부 언어적인 지식을 사용한다고 보시면 됩니다.​컴퓨터의 언어적 이해에 관련된 일들은 자연어 이해(Natural Language Understanding; NLU)에서 전문적으로 다루고 있습니다. ​지난 10년간은 확률적인 예측 모델인 HMM(Hidden Markov Model)에 기반한 음성인식 기술이 널리 사용됐습니다.​음성에서 특징을 추출해서 음소를 인식하고, 확률 모델이나 DNN을 이용해 음절, 단어를 재구성해서 문장으로 출력하는 방법입니다. ​이를 위해 개별적으로 학습된 음향 모델, 언어 모델 및 발음 사전 등이 사용됩니다.​최근에는 음성을 입력받아 문장을 출력하는 과정이 하나의 모델로 처리되는 종단형(end-to-end) 음성인식 기술이 새롭게 등장했고, 뛰어난 음성인식 성능과 간단한 학습 방법으로 인해 많은 음성인식 기업에서 도입하고 있습니다. ​딥러닝을 사용하는 어떤 분야나 마찬가지겠지만 음성인식에서도 좋은 성능을내려면 많은 양질의 데이터가 필요합니다.​특히 음성인식의 경우 음성과 함께 전사된 텍스트까지 필요하므로 학습 데이터를 확보하는 데 훨씬 더 큰 비용이 듭니다.​이러한 문제로 페이스북(현 메타)이 지난 2020년에 공개한 self-supervised learning(자기 지도학습) 모델인 wav2vec 2.0이 각광받고 있습니다.​이 모델은 라벨 즉, 전사 텍스트가 없는 대량의 음성 데이터로 자체적인 특징(representation)을 스스로 학습 한 후, 비교적 적은 라벨링 된 데이터만 fine-tuning 하는 것으로도 매우 뛰어난 성능을 보여주고 있습니다.​ 2. 음성인식 기술의 적용과 응용​음성인식의 활용 범위는 워낙 광범위하므로AICC에서 널리 사용하는 응용만 짚어보겠습니다.​음성인식을 직접적으로 사용하는 것은 콜봇과 AI 상담 어시스턴트가 있습니다.​AI 상담 시스템은STT (Speech To Text : 음성인식기술),TA (Text Analysis : 문자 분석), 챗봇을 통해 실시간 AI 정보를 상담사에게 즉시 제공해상담사의 업무처리를 용이하게 지원합니다.​브로드씨엔에스는 STT (Speech To Text) 솔루션을 기반으로 비실시간서비스와 실시간 STT 서비스를 제공하고 있습니다.   ​“비실시간STT”는 상담이 종료된 후에 녹취 서버 음성파일이 생성되게 됩니다.​음성파일이 STT 서버로 전송되고, 텍스트로 변환 된 후 수행됩니다. ​주로 FTP(File Transfer Protocol : 다른 컴퓨터로 파일을 전송할 수 있도록 하는 프로그램)를이용하여 주기적으로 녹취 서버의 음성 파일을 가져오게 됩니다.​결론적으로 비실시간STT는 음성 파일이 생성된 이후 준 실시간으로 텍스트로 변환되어 상담원에게 까지 전달되는 솔루션이라고 할 수 있습니다.​ “실시간 STT”는고객이 음성을 남기는 동시에 음성 데이터가 STT 서버에 전달되어 텍스트로 변환됩니다.​발화가 끝나게 되면 텍스트 변환 결과가 상담원에게 제공됩니다.​주로모바일 단말에서 직접 STT 서버로 호출하거나, ​포트 미러링(Port mirroring : 데이터를 다른 주변장치로 복사하는 방법)을통하여 직접 STT 서버로 전달하는 방식을 사용합니다. ​한 마디로 고객의 발화가 진행되는동시에 텍스트 변환이 수행되고,발화 완료 시 텍스트를 즉시 제공하는 솔루션입니다.​또한 상담 어시스턴트 기능을 통하여실시간으로 고객 대화 음성을 텍스트로 변환,상담원에게 채팅 형식으로 제공하고 대화 내용 내 핵심 키워드가 자동으로 추출,검색될 수 있도록 합니다.​이로써 상담원은 고객과의 상담 내용의핵심을 정확히 알 수 있으며, 오상담(불완전판매, 민원유발)을 줄임으로써고객만족도 향상과 고객의 충성도를 제고할 수 있습니다. ​ ​- AI 상담 어시스턴트는 상담원의 통화 내용을실시간으로 텍스트로 변환한 후 자연어 이해,지식 검색 등을 활용해 적절한 답변을 추천하고상담 후 처리를 자동화합니다. ​- 상담 통화는 음성인식 기술로 텍스트로 변환한 후에는 자연어 처리 기술을 기반으로상담 내용 요약, 상담 분류, 상담평가 등 다양한 상담 분석(TA)에 활용됩니다.​- 2022년 AI 바우처에 포함됐던음성인식 콜백, 녹취 음성인식 & 검색 서비스도음성인식 기술의 활용 예입니다.​  ​ TTS 기술  1) TTS의 개념 보통 TTS(Text to Speech)라고부르는 음성합성은 글(텍스트)를 사람의 음성으로 만들어 내는 기술입니다.​음성합성 기술은 실제 음성 데이터베이스를 기반으로 음성 합성을 하는 방식,통계적 SPSS(Statistical Parametric Speech Synthesis) 음성합성 방식이 있습니다.​근래의 음성합성 전형이 된SPSS 방식은 텍스트에서 언어적인 특징을추출해  음향적인 특징으로 변환한 후마지막으로 음성(waveform)으로 변환하는3단계로 이루어집니다.​최근에는 글자나 음소로부터 음향적 특징을얻어내는 단계에 DNN을 사용하거나아니면 전체 과정을 하나의 DNN 모델로처리하는 종단형 TTS 방식이 많이 등장하고 있습니다.​둘 다 DNN TTS라고 부릅니다.​ 2) TTS의 용도​콜봇의 답변 음성이 바로 TTS를 사용해 만들어 낸 것입니다.요즘 TTS의 음성이 워낙 자연스럽기 때문에IVR의 음성도 성우가 녹음하는 대신 음성합성한 것을 사용합니다.​컨택센터 분야 외에 AI 비서나 스마트 스피커,웹 페이지의 읽어주기 서비스 등에TTS가 널리 활용되고 있습니다.​AI 음성인식의 필수적인 STT 기술과 TTS 기술에 대해서 알아보았습니다. ​쉽게 말해 STT란 사람의 음성을 STT(Speech To Text) 엔진을통해 음성을 텍스트로 변환하는 기술이며,반대로 TTS는텍스트를 음성으로 바꾸는 기술입니다. ​음성인식 서비스를 통해 고객이 주문한 사항을텍스트로 보여주거나 관련 정보들을 AI를 통해제공하여 더 빠르고 정확한 고객서비스 업무를가능하게 하며 고객 입장에선 더 짧은 대기시간으로 명확히 문제를 해결할 수 있습니다. ​상담원 입장에서는고객과의 마찰을 줄이고 신속하게 고객의 문의를 해결할 수 있게 되어 업무 강도 및 스트레스가 감소할 것입니다. ​이처럼 현재 많은 분야에서음성인식기술(STT)과 음성합성(TTS) 기술사용되고 있으며 그 분야는 더욱 폭넓게확장되고 발전하고 있습니다. ​인공지능(AI)에 대한 사회의 관심은 앞으로도 점차 커질 것으로 예상되며인공지능 챗봇, 콜봇과 같은 다양한 서비스가확대되어 생활의 편의를 향상시키고삶의 질을 높여줄 수 있을 것으로 기대됩니다. ​브로드씨엔에스도 이러한 사회의 요구에맞춘 혁신적인 서비스를 통해 인공지능 서비스 시장을 선도할 수 있는더욱 고도화된 AI 솔루션 콜센터 클라우드의선두 기업으로 앞장서겠습니다. ​감사합니다.​​​ ​ "
싱가포르  초 단기 어학연수 ,https://blog.naver.com/raffles7/222707360042,20220421,"​오늘은 한 달 미만 초단기 등록도 가능한 싱가포르 EF 어학연수 프로그램에 대해 안내드리려고 합니다.​ 싱가포르 호파 빌라로 필드트립을 간 EF 싱가포르 학생들​​​코로나로 인해 우리가 그토록 사랑하던 여행을 잠시 멈추었었는데요, 올해 2022년도에는 여름휴가 계획으로 해외여행 계획을 세울 수 있게 되었습니다.  그동안  못 가셨던 해외여행으로 여름휴가를 2주 이상 쓰시려는 직장인분들을 위한 2주부터 등록이 가능한 어학연수 프로그램을 안내드리려고 합니다.  코로나 이전에 한참 해외 한 달 살기가 유행이었던 적이 있었죠?  직장인들에게 한 달 살기까지는 휴가를 못 내더라도  있는 연차 등등 다 이어 붙여서 2주 정도  여름휴가를 쓰실 수 있다면 그 기간을  위드 코로나를 아시아에서 제일 먼저 선포하고 일상으로 돌아간 코로나 방역 선진국 싱가포르에서 여행과 어학연수 두 마리 토끼를 다 잡아보시죠.부모님 동반 없이 혼자 어학연수 계획 중인 고등학생에게도 싱가포르 EF 캠퍼스 추천드려요. 싱가포르는 특히 치안이 안전하여 미성년자 홀로 연수에 적합하고,  아침, 저녁 제공 되는 홈스테이 가정에서 머무르며 EF통학할 수 있고, 다른 나라보다 미성년자 단기 연수시 준비해야 할 서류가 매우 간단합니다.​  ​​게다가 4월 26일부터 싱가포르 입국을 위한 한국에서 출국전에 받아야 하던 코로나 검사가 폐지 되었습니다.  싱가포르 입국하자마자 공항에서 받던 코로나 검사는 이미 3월에 폐지되었고요.   백신 접종 완료자는 코로나 이전처럼 그냥 비행기랑 숙소 예매만 하시면 싱가포르 입국 준비 끝입니다!  ​시간이 없는 직장인들이 이렇게 단기 어학연수에서 가장 바라는 점은 영어 리딩이나 라이팅보다는 오랫동안 안 써서 어색해진 영어 회화 향상이 아닐까요?  ​EF의 어학연수는 다른 어학원보다 좀 더 듣기와 말하기 능력 항상에 포커스가 맞추어져 있습니다. ​위  영상에서 보신 EF의 수업 체계는 다음과 같이 5 가지의 수업으로 구성되어 있습니다.1.  General Class :  저희도 익히 경험한 강남역 영어학원 같은 소규모 렉처 스타일 수업2.  Conversation class : 다양한 디스커션 수업3. iPAD Class : 아이패드를 이용한 2-3명의 pair conversation 수업4. i Lab computer class:  유창한 영어를 위해서는 파트너와의 공부도 필요하나 학습자 스스로의 혼자 해야 할 공부들이 있죠?  바로 올바른 문법을 익히고 올바른 발음으로 자신의 생각을 정리하고 전달해야 합니다. 그를 위한 video lesson(온라인 문법 공부)과  speech recognition(발음 수정)을 컴퓨터를 통해 공부하고  itracker라는 툴을 통해 본인의 프로그레스를 확인할 수 있습니다.5. SPIN Class:  Special Interest Class로 대학 교양처럼 마케팅이나 저널리즘 같은 흥미 있는 과목 수업을 영어로 참여하는 수업입니다.​​​EF 싱가포르 캠퍼스는 싱가포르 리버 크루즈 타시면 꼭 지나가는 관광 핫 플레이스인 클라키에 위치하고 단독 건물을 사용하고 있습니다. 학생라운지나 루프탑 테라스 같이 강의실 외에도 학생들 휴게 공간도 잘 갖추고 있습니다. ​싱가포르 EF 어학연수 2주부터 등록 가능한 기간제 프로그램은 주당 수업 시간에 따라 일반과정과 집중과정을 고르 실 수 있습니다.​ EF 집중과정​ EF 일반과정​​​해외연수를 결정할 때 국내 어학원이 수준이 딸리거나 부족해서 해외연수를 떠나시는 것은 아닐 거예요.  영어를 쓰면서 낯선 곳에서 현지인의 삶을 체험해 보고 영어 공부에 대한 동기부여를 얻고 싶으셔서 이실 텐데요.성공적인 어학연수는  어학당 수업 방과 후를 어떻게 지내느냐가 매우 중요합니다.   한국인 학생들과 몰려다니면서 보낸다면 영어는 늘지 않을 거예요.  EF는 한국인 학생과 중국인 학생 비율이 매우 적은 것이 특징입니다.​EF 프로그램은 어학원 수업 외에도 다양한 액티비티가 있는 것도 특징입니다.  액티비티는 아래의 캘린더에서 보시듯이 EF 캠퍼스 옥상에서 하는 바비큐 파티, 비치발리볼이나 차이나타운 불아사 템플 방문같이 공짜인 프로그램도 있고 유니버설 스튜디오 방문처럼 유로 프로그램도 있습니다.   EF의 액티비티에 열심히 참여하셔서 방과 후 시간도 알차게 보내보시길 바랍니다.​ ​​​ ​​​EF 싱가포르의 어학연수 기간 동안 숙박은 어떻게 해야 하는지 궁금해하실 텐데요?​1.  싱가포르언 호스트 패밀리에서 홈스테이 하시는 방법과  ​2.  도비곳 MRT 도보 5분 거리에 위치한 EF 연계 숙소인 YMCA 레지던스에서 이용하시는 방법이 있습니다.  클라키에 위치한 EF와 전철로 1정거장 떨어져 있고  서울 강남에 해당하는 오차드로드에 위치하고 있어서 교통도 편리하고 주변에 편의시설이 넘칩니다. ​​ 방 타입은 혼자 쓰는 싱글룸과 룸메이트와 같이 쓰는 2인실 트윈룸이 있습니다. ​​공용 주방에 전자레인지, 인덕션, 토스터, 냉장고 등이 있어서 간단한 요리는 가능합니다. ​​​싱가포르 EF의 어학연수 프로그램에 대해 더 궁금하신 사항은 저희 링크에이드로 연락 주시면 친절히 답변드리겠습니다.   ​싱가포르 유학원 링크에이드는 싱가포르 EF 어학연수 입학 수속을 무료로 도와드리고 있습니다.입학 허가를 받은 이후 싱가포르로 출국 및 현지 정착까지 세심히 도와드리고 있습니다.관심 있으신 분들은 카카오톡 아이디 linkaid 로 카톡 상담 가능합니다. 전화가 편하신 분들은 010-2473-0049로 전화 주시면 친절히 안내해 드리겠습니다.​ ​ "
컴퓨터 비전의 미래를 그리다 – 퀄컴과 쿠바랩(QUVA lab) ,https://blog.naver.com/qualcommkr/222140696259,20201111,"지난 2014년, 퀄컴은 암스테르담 대학교(University of Amsterdam)에서 설립한 스타트업 유비전 테크놀로지(Euvision Technologies)를 인수했습니다. 예지력이 있는 투자였지요. 유비전은 컴퓨터 비전용 머신러닝을 연구하고 있었습니다. 당시 퀄컴 입장에서는 상대적으로 새로운 주제였죠. 이 머신러닝 주제는 곧장 빛을 보지는 못했습니다. 당시 상황을 간략히 설명하자면, 아마존에서 알렉사를 출시하기 1년 전이었고, 우버에서는 2년이 더 지난 뒤에야 무인자동차를 테스트하기 시작했습니다. 퀄컴은 이른 시일 내에 전력 효율적인 하드웨어에서 컴퓨터 비전용 고급 알고리즘과 머신러닝을 구동할 수 있어야 한다는 점을 깨달았습니다. 이에 암스테르담 대학교와 합동으로 팀을 꾸려, 딥 비전(deep vision) 연구를 위한 합작 연구소를 개소했는데, 그 곳이 바로 쿠바랩(QUVA lab)입니다. 좌측부터 - 스멀더스 교수, 웰링 교수, 스눅 교수아놀드 스멀더스(Arnold Smeulders) 교수와 맥스 웰링(Max Welling) 교수, 케이스 스눅(Cees Snoek) 교수, 그리고 입스트라시오 가베스(Efstratios Gavves) 조교수가 함께 이끄는 쿠바랩은 자율성을 갖고 자체적으로 연구를 진행했습니다. 이러한 연구 방식은 R&D에 대한 퀄컴의 열정적인 헌신의 일환으로, 높은 수준의 과학 연구를 지원하는 방식의 하나이기도 합니다. 눈앞의 사업 목표 때문에 연구가 제한당하지 않도록 말이죠. 그리고, 이 방식은 성공했습니다. 쿠바랩은 NeurIPS (Neural Information Processing Systems), ICML (International Conference on Machine Learning), CVPR (Conference on Computer Vision and Pattern Recognition) 및 ICCV (International Conference on Computer Vision)와 같은 권위 있는 컨퍼런스에서 논문을 발표했으며, 퀄컴 AI 리서치와도 협력하여 새로운 지적 재산을 창출하고 있습니다. 전 세계 유수의 연구자들과 퀄컴이 함께 쏟아온 노력이 인정받은 것이죠. 쿠바랩 초기에는 컴퓨터 비전과 딥 러닝의 교차점에 중점을 두었습니다. 허나 시간이 지나면서 프라이버시 보호 딥 러닝이나 분산 딥 러닝, 등변 딥 러닝 알고리즘 등 머신러닝 주변부로 주제를 확장해갔습니다.​그리고 올해, 퀄컴은 쿠바랩과 협업을 5년 더 연장하기로 했습니다. 아주 흥미로운 다양한 연구 주제들이 쿠바랩을 기다리고 있죠. 이에 스멀더스 교수와 스눅 교수, 그리고 퀄컴 테크놀로지 네덜란드 법인의 부사장이자 네덜란드 대학교에서 교수를 겸하고 있는 웰링 박사와 함께 지난 5년간 딥 비전 분야가 얼마나 발전해왔는지 뒤돌아보면서, 미래에 AI가 지닐 가능성에 대해 원격으로 이야기를 나누어 보았습니다. ​​쿠바랩이 설립되고 지난 5년 동안 컴퓨터 비전과 딥 비전에 대한 인식은 어떻게 바뀌었나요?아놀드 스멀더스 교수: 90년대의 컴퓨터 비전은 일반적인 세분화 문제 해결에 상당히 집중하고 있는 편이었습니다. 세분화는 사람이 시각적 인지를 위해 사용하는 핵심 이해 능력이기 때문입니다. ​“소는 다리가 네 개 달린 개체이다.” 이 문장은 사실 대부분 경우에 틀립니다. 소를 보여주는 사진은 대개 다리 두 개만 보여줄 뿐이고, 다리를 정의하지 않으므로 관련성이 없는 설명이기도 하죠. 게다가 예외가 너무 많기 때문에 실용적인 설명이라 할 수도 없습니다. 세상에 다리가 네 개 달린 것들은 아주 많죠. 그렇기에 비전에 설계 구성요소 규칙을 기반으로 하는 접근 방식을 활용하는 것은 매우 제한적이었습니다. 그리고 많은 컴퓨터 비전 작업에 세분화가 그리 필요치 않다는 사실도 서서히 깨달아 갔죠. ‘소’를 인식하려면 우선 동물임을 파악해야 하고, 그 개체가 초원 위에 서 있는 상태여도 똑같이 인식할 수 있어야 합니다. 개체를 파악하려면 일단 외곽선을 먼저 식별하면 됩니다. 지난 10년간 이 시스템은 분류 알고리즘을 효과적으로 쌓아왔습니다. 2012년에 알렉스 크리셰브스키 외 2인이 발표한 유명한 논문 이후, 신경망은 사람이 하는 이미지 설명에서의 구성요소 및 규칙 기반 알고리즘보다 컴퓨터 비전에서 훨씬 더 우수한 결과를 보인다는 점이 분명해졌습니다. 신경망은 균질(homogeneous)합니다. 그러므로 훨씬 높고 근본적인 계층에서 이론을 허용하여 이전 알고리즘 모델에 필요했던 구성요소별로 성능을 최적화합니다. 구성요소를 최적화하는 대신 말이죠. 쿠바랩은 컴퓨터 비전에서 신경망이 등장하고 얼마 지나지 않아 시작되었습니다. 케이스 스눅 교수: 2015년 쿠바랩을 시작한 당시 딥 러닝은 이미 컴퓨터 비전에 상당한 영향을 미치고 있기는 했지만, 대체 기술 역시 있는 상황이었습니다. 딥 러닝이 발전하는 속도와 이것이 학계 및 산업계에 형성한 연구 모멘텀에 당시 저희도 모두 놀랐습니다. 이제는 딥 러닝을 논하지 않고 컴퓨터 비전 논문을 발표하는 건 거의 불가능하다고 할 수 있습니다.​맥스 웰링 교수: 이제 전체 분야를 딥 러닝이 지배하고 있다고 해도 과언이 아닙니다. 딥 러닝은 이미지 데이터에 아주 이상적인 방식입니다. 딥 러닝은 아주 빠르게 발전해왔고, 자율주행차 부문에서는 영상 시퀀스를 실시간으로 분석할 수 있으며, 진짜 얼굴과 구별하기 어려울 정도로 사실적인 얼굴 이미지를 만들 수도 있는 수준에 이르렀습니다.​​뿌듯했거나, 특히 큰 교훈을 얻은 연구 프로젝트가 있다면 몇 가지 소개해주세요.스멀더스 교수: 초반에 라인 반 덴 봄가르드(Rein van den Boomgaard) 박사가 최소 최대 필터와 버거스 편미분 방정식 (Burger’s partial differential equation) 사이의 관계를 도출해냈는데요. 이미지 필드의 수학적 설명의 극단을 연관 지을 수 있었던 점이 매우 자랑스러웠습니다. 시간이 조금 더 지나서는 제가 박사 과정을 지도했던 야스퍼 위이링스(Jasper Uijlings)가 아이디어를 얻어, 이미지의 간단한 자체 정렬에서 이미지의 상향식 그룹화를 정의하는 데 성공했습니다. 스눅 교수와는 지난 10년간 함께 각고의 노력 끝에 TREC-VID 대회에서 우승을 차지하는 쾌거를 거두기도 했습니다. 그 외에도 박사 과정에 있는 학생이 이전에는 상상하기 어려웠던 것을 발견할 때 저 역시 자랑스러움을 느낍니다.​스눅 교수: 저는 제 연구가 모두 자랑스럽습니다. 그리고 실패에서 얻은 교훈만이 발전을 위한 진정한 밑거름이 될 수 있다고 생각합니다. 쿠바랩에서 진행한 연구 중 뿌듯했던 사례를 하나 들자면 VideoLSTM을 들 수 있는데요. 가장 큰 이유는 딥 비전의 도움 덕분에 VideoLSTM이 활동 인식(activity recognition) 및 영상 내 위치 식별(localization)을 위한 LSTM을 아키텍처를 제공하기 때문입니다. 두 번째 이유는 이 프로젝트가 쿠바랩 학생들과 퀄컴 AI 리서치 암스테르담 지부의 연구원들이 합동 연구한 결과이기 때문입니다. 그리고 마지막으로 중요한 이유는 이 프로젝트를 통해 특허를 세 개나 출원할 수 있었기 때문입니다.​웰링 교수: 아마 두 가지 예를 들 수 있지 않나 싶습니다. 첫 번째는 급증하는 신경망에서 역전파(backpropagation)를 수행하는 방법을 발견한 것입니다. 이전에는 걸림돌로 작용했던 부분이죠. 두 번째는 조합 공간에서 효율적인 검색 방법을 학습하는 알고리즘을 설계한 것입니다. 현재 이 알고리즘을 사용해 칩용 물리적 설계를 개선하는 작업을 진행하고 있습니다. 실패 사례도 말씀을 드리자면, 프라이버시 보호 딥 러닝 아키텍처를 개발하는 데 큰 기대를 걸었으나 이 방식은 기존에 개발된 방식 대비 그다지 경쟁력이 있지는 않았습니다. ​​팬데믹으로 인해 디지털 헬스나 접촉 경로 추적 등의 분야에 적용되는 머신러닝(ML) 연구에 속도가 붙고 있는데요. 쿠바랩에서 진행하는 연구에 어떤 영향이 있으리라고 보시나요?스눅 교수: 쿠바랩의 임무는 딥 비전 부문에서 세계적인 수준의 연구를 수행하는 겁니다. 과학에서는 당대의 이슈에 지나친 영향을 받지 않고 연구에 집중하는 것이 중요합니다. 물론 코로나19의 영향과 중대성을 과소평가하는 건 아닙니다. 하지만 근본적인 문제에 집중하면 결과는 자연스레 따라오게 되어 있습니다. 디지털 헬스의 경우, 코로나19 측면에서 보자면 접촉 경로 추적 부문에 쿠바랩의 연구를 직접적으로 활용할 수 있습니다. 활동 인식 분야에서의 저희 연구 역시, 쉬운 예로는 사회적 거리두기 모니터링 등에 활용할 수 있습니다.​​딥 비전 부문에서 현재 발전 가능성이 가장 높은 트렌드는 무엇인가요?스멀더스 교수: 이미지, 목적, 이미지 형성 원칙의 기존 지식을 현재 신경망 개발을 이끄는 보편적 원칙과 통합하는 작업이라고 봅니다. 일반화와 차별화 사이에 올바른 균형점을 찾고, 이를 목적과 상황에 적용하는 것이 가장 큰 도전 과제라 할 수 있습니다. 저희는 아직 컴퓨터 비전의 신경망 아키텍처의 초기 단계에 있습니다.​스눅 교수: 오늘날의 딥 비전은 어마어마한 양의 연산을 바탕으로 라벨과 픽셀을 연관시키는 법을 학습하여 잘 정의된 분류 문제를 훌륭히 처리합니다. 연관성을 찾기 위해 학습하는 대신 결과의 원인을 예측하고, 동시에 물리적 시뮬레이션이나 생성 모델 혹은 자체 감독의 도움으로 라벨 감독의 필요성을 줄임으로써, 그리고 이를 연산 효율적인 방식으로 처리하면서 이러한 현상(現狀)에 도전하는 게 저는 가장 유망한 트렌드라고 생각합니다.​웰링 교수: 저는 머신러닝에 대해 이야기하겠습니다. 특히 두 가지 트렌드를 흥미롭게 보고 있는데요. 하나는 조합 최적화 문제를 위한 강화 학습과 베이지안(Bayesian) 최적화 활용이고, 다른 하나는 머신러닝을 위한 양자 역학 및 양자 연산 활용입니다.​​축하드립니다. 쿠바랩 프로젝트가 5년 더 연장되었는데요. 향후 비전은 무엇인가요?스눅 & 스멀더스 교수: 감사합니다. 퀄컴에도 축하의 말을 전합니다. 쿠바랩의 비전은 앞에서 설명한 트렌드와 과제들을 기반으로, 반복 영상 학습, 비지도 영상 압출, 자체 감독 활동 인식 등의 주제를 다루는 딥 머신러닝 기반에 특히 주력하고자 합니다. 채용도 계획하고 있으니, 재능 있는 많은 학생 여러분께서 지원하여 함께 쿠바랩의 비전을 실현해 나가면 좋겠습니다.​웰링 교수: 저 역시 위에서 설명했던 새로운 주제들에 대한 연구나 열정적인 협업을 기대하고 있겠습니다.  최첨단 AI 연구를 통해 세계가 당면한 문제를 해결하고, 수십억 인구의 삶의 질을 향상하는 데 보탬이 되고 싶나요? 퀄컴은 현재 머신러닝을 비롯한 여러 분야의 인재를 찾고 있습니다. 자세한 내용은 아래의 링크를 참고해주세요.​Machine Learning Researcher – On-device AI theory and algorithmMachine Learning Research - Speech RecognitionMachine Learning Researcher - Speech SynthesisMachine Learning Researcher – Audio and SpeechMachine Learning Senior Software Engineer – Embedded AI FrameworkMachine Learning Researcher (Intern)Deep Learning Systems Research Engineer on Camera Perception for Autonomous DrivingComputer Vision and Machine Learning R&D Intern for Autonomous Driving  ​ "
[미국대학] 카네기 멜론 대학교 컴퓨터 사이언스 (Computer Science) ,https://blog.naver.com/minzzang2013/221567884755,20190622,"미국대학 - Carnegie Mellon University School of Computer Science (카네기멜론대학교 컴퓨터싸이언스)     안녕하세요미국 학위유학 전문 김쌤입니다.오늘은 컴퓨터 사이언스 하면 떠오르는 대표적인 대학, 카네기멜론 대학교가 생각나는데요. 카네기멜론의 컴퓨터싸이언스의 장접은 무엇인지, 학교는 어떠한지 함께 알아보겠습니다~!    펜실베니아주 피츠버그에 위치한 카네기 멜론 대학교의 컴퓨터 사이언스 대학(SCS)는 1988년에 설립된 분야의 선도적인 대학교입니다. 해마다 꾸준히 미국 뿐만 아니라 세계적으로 최고의 컴퓨터 사이언스 대학으로 선정되어 왔습니다. US News는 MIT, S스탠포드, UC버클리와 함께 카네기 멜론의 컴퓨터 사이언스 프로그램을 공동 1위에 랭크하고 있습니다.    지난 15년 동안 카네기 멜론의 컴퓨터 과학 학교의 연구원들은 알고리즘, 컴퓨터 네트워크, 분산 시스템, 병렬 처리, 프로그래밍 언어, 컴퓨터 생물학, 로봇 공학, 언어 기술, 인간과 컴퓨터의 상호작용, 소프트웨어 공학 분야에서 발전을 이루었습니다.   많은 연구와 교육과 산업연계를 통해 SCS교수진은 정보기술, 네트워크, 사이버보안, Machine Learning, natural language processing, speech recognition, robotics 분야에서 리더적인 역할을 하고 있습니다. 그리고 월스트리트 저널 편집자들이 조사한 결과 카네기 멜론의 CS 학브 프로그램이 미국 내 기업 인력 채용 부분에서 1위를 차지했습니다.       Undrgradauate Program카네기 멜론 대학교(CMU)의 스쿨 오브 컴퓨터 사이언스 (SCS)에서는 computational biology, computer science 학사학위를 수여하며 2018년 가을학기부터 artificial intelligence의 학사학위도 수여하고 있습니다.다. CS와  음악, 미술과 연계해서 공부할 수 있습니다.  또한 computational biology, computer science, human-computer interaction and robotics.에 관한추가전공도 제공하고 있습니다. 위의 추가 전공을 취득하고자 하는 학생은 대학에서 요구하는 1차 전공을 위한 요건을 충족해야하며 학교 담당자와 상의한 후에 선택 할 수 있습니다.​<IDEATE Program>카네기 멜론의 컴퓨터 사이언스 학생으로서, 학생은 게임 디자인, 애니메이션 & 특수 효과, 미디어 디자인, 학습 미디어, 사운드 디자인, 창조 산업을 위한 기업가 정신, 지능형 환경 또는 물리 컴퓨팅의 8가지 창조 산업 영역 중 하나에 집중 또는 부전력을 통합하여 학습할 수 있는기회를 가질 수 있습니다. 이는Carnegie Mellon의 컴퓨터, 공학, 디자인, 예술 분야에서의 다양하고 최고의 학과들이 기술과 창의적인 관행을 결합하는 학문 간 관심사를 가진 학생들에게 봉사할 수 제공하는 독특한 프로그램입니다.   Undergraduate ConcentrationSCS 학부생들은 학문적 깊이를 얻기 위해  아래의 특정 컴퓨터 영역을 집중 공부할 수 있습니다.  ▷ Algorithms & Complexity ▷ Computational Biology ▷ Computer Systems ▷ Security and Privacy ▷ Software Engineering   Graduate ProgramsSCS는 7개의 부서에 걸쳐 광범위한 전문 및 학술 석사 프로그램을 제공합니다. 카네기 멜론이 컴퓨터 사이언스 분야에서  탑의 학교임을 짐작할 수 있습니다. 입학요건은 프로그램에 따라 다르기 때문에 지원전 반드시 확인을 해야합니다.Computational Biology DepartmentM.S. in Automated Science: Biological ExperimentationM.S. in Computational BiologyComputer Science DepartmentM.S. in Computer ScienceHuman-Computer Interaction InstituteMaster of Educational Technology and Applied Learning Science (METALS)Master of Human-Computer InteractionM.S. in Product ManagementInstitute for Software ResearchMaster of Information Technology StrategyM.S. in Information Technology - Privacy EngineeringM.S.in Information Technology - Software EngineeringMaster of Software EngineeringLanguage Technologies InstituteMaster of Computational Data ScienceM.S. in Artificial Intelligence and Innovation (MSAII)M.S. in Biotechnology Innovation and ComputationM.S. in Intelligent Information SystemsM.S. in Language TechnologiesMachine Learning DepartmentMaster's in Machine LearningM.S. in Machine LearningSecondary Master's in Machine LearningRobotics InstituteM.S.- Computer VisionM.S. in RoboticsMaster's in Robotic Systems Development   위와 같이 알아보았습니다. 카네기 멜론의 컴퓨터 사이언스 단과대학의 방대하고도 세부 사항이 살아있는 학위 프로그램들을 블로그를 통해서 소개 드리기에 한계가 느껴질 정도네요ㅠㅠ 그래도세계적인 명성만큼의 높은 겨쟁률이지만 우수한 지원자들이 몰리는 이유를 알수 있었던 것 같습니다.  자세한 입학내용 및 지원세부사항은 김쌤에게 문의해 주세요.컴퓨터 사이언스를 공부하고자 하는 지원자 모든 분들, 화이팅입니다!!    ​ "
불합리한 보청기급여제도: 건강보험공단의 서식 및 이비인후과의 보청기 검수확인의 오류 ,https://blog.naver.com/hearing1004/222006218345,20200621,"이번 글에서는 #보청기급여제도 중 #이비인후과 #병원의 #보청기 #검수확인 방법의 #오류에 대해서 살펴보기로 한다.#보청기검수확인은 #보청기적합확인(hearing aid hearing aid verification) 또는 #착용효과의 측정(validation or outcom measures)로 볼 수 있다. 이는 난청인이 일상생활에서 보청기를 통해 의사소통 능력의 개선, 삶의 질 등에 어느 정도 도움이 되는지를 객관적 및 주관적으로 확인 하는 것이며. #2-cc 커플러측정(2-cc coupler measurement), #실이측정(real-ear measurement), 음장에서의 #기능이득(functional gain)과 #어음인지검사(speech recognition test), #자가평가설문지 등을 이용할 수 있다. 하지만 현재 검수확인에 사용하는 서식 및 이비인후과의 검수확인 방법은 한마디로 엉터리다. 이를 확인해 보면 다음과 같다.​#서식의 오류#건강보험공단에서는 잘못된 서식을 그대로 사용하고 있으며,  현재까지 검수확인에 대한 지침을 마련하지도 않고 있다. 이로 인해 청각학에 이해가 부족한 이비인후과 전문의들은 대충 검수확인을 실시하는 오류를 범하고 있어서 난청인들의 의사소통 능력에 방해가 되어 불만과 자부담 비용이 증가하며, 건강보험공단의 기금이 낭비되는 예를 볼 수 있다.​ 처방전(위)와 검수확인서(아래) 상기 서식 그리고 이비인후과의 검수확인에서 잘못된 점을 찾아보면 다음과 같다.1. 처방전에서 말소리 명료도를 어음인지역치(SRT)를 구하는 경우가 있다.   말소리 명료도는 %, 어음인지역치는 dB HL로 나타내야 한다.2. 검수확인서에서 처방전의 결과를 방음실의 말소리명료도와 비교하도록 하고 있다.  이는 큰 오류로 헤드폰 착용 시에는 쾌적강도레벨(most comfortable level, MCL)에서 단어인지도를 측정하는 것은 종합적인 어음인지 능력을 평가하는 것이기 때문이다. 난청인의 쾌적강도레벨을 구하기 위해서는 어음의 제시음압레벨을 증가시켜야 하는데, 이는 일종의 증폭기를 착용한 것이나 마찬가지이기 때문에 보청기 착용 후 어음명료도와 비교하는 것은  말도 안 되는 것이다.3. 보청기 착용 후 제시음압레벨을 MCL 또는 60 dB HL에서 측정한다.   보청기 착용 후 60 dB HL 또는 쾌적강도레벨에서 구한 단어인지도를 헤드폰 착용 시와 비교하기도 하는데 이는 정말 무지한 발상이다. 보청기 착용 후 쾌적강도레벨을 구한다는 것은 어음의 음압레벨을 증감시키는 것으로 이 경우는 보청기의 증폭이 잘못된 것으로 봐야 한다. 왜냐하면 보청기 착용 후 보통의 대화음레벨은 대부분 난청인의 쾌적강도레벨 부근에 근접해야 하기 때문이다. ​  따라서 국민건강보험법 시행규칙 [별지 제22호 서식]의 검수확인서아래의 내용으로 반드시 수정해야 한다.첫째, 보청기 착용 전후의 #청력역치레벨은 모두 #방음실에서 실시하여 비교하도록 해야 한다. 둘째, 보청기 착용 전후의 #어음인지도 역시 모두 방음실에서 측정한 값으로 비교해야 하며, 제시하는 어음의 음압레벨은 45~50 dB HL로 보청기 착용 전후에 모두 같도록 해야 한다.​그리고 보청기 착용 전후의 청력역치레벨 및 단어인지도를 확인하는 방법은 다음을 이용해야 한다.​1) 보청기 착용 후 청력역치레벨(증폭역치)  보청기를 착용한 후 청력역치레벨(증폭역치)을 측정하는 이유는 청력의 개선여부를 판단하기 위함이다. 그러나 증폭역치를 측정하는 이유는 기능이득을 확인하기 위함이다. 기능이득은 이론 상 실이측정의 실이삽입이득(REIG)와 같다. 따라서 검수확인을 하기 위해서는 보청기적합공시과 실이삽입이득을 비교해야 한다. 그러나 한국인의 경우는 한국형 적합공식을 이용해야 하지만 현재까지 특별히 마련한 적합공식이 없다. 그리고 기능이득의 정도, 즉 증폭역치레벨은 비선형 증폭시스템의 압축역치 및 압축비율에 의해 영향을 받는다.   아래의 그림 우측에서 압축역치가 낮은 경우는 좌측의 그림처럼 청력역치레벨을 더 개선할 수 있어서 기능이득이 더 높게 나타난다. 그리고 이때 나타난 기능이득을 NAL-NL2 등의 보청기적합공식에서 나타난 REIG와 비교하여 효과를 확인하지만, 개인 별 차이가 있으며, 특히 저주파수 대역은 폐쇄효과(occlusion effect) 등으로 인해 개인 별 차이가 크게 나타난다. 또한 한국형 적합공식도 고려해야 한다.  하지만 이비인후과에서는 기능이득에 대한 이해가 부족하기 때문에 증폭역치에 대한 해석을 마음대로 하고 있다. 이는 결국 난청인의 의사소통 능력에 걸림돌이 되고 있어서 보청기 착용인들이 불편함과 분노를 표출하고 있다. 보청기의 압축역치 및 압축비율(우측)과 기능이득(좌측)의 예 2) 보청기 착용 전후 단어인지도  상기에서 설명하였듯이 보청기 착용 후의 어음인지도(단어인지도)는 처방전에 있는 헤드폰 착용 시의 단어인지도와 비교하는 것은 어리석은 방법이다. 보청기 착용 후의 개선 효과는 난청인이 실생활에서 보청기로 하여금 어느 정도 도움이 되는지를 확인하는 것이기 대문이다. 보청기 착용 전 즉, 헤드폰에서의 단어인지도는 쾌적강도레벨에서 구한 값이므로 이미 증폭기를 착용한 후의 결과라고 봐야 하기 때문이다. 따라서 보청기 착용 후의 어음인지도의 개선효과는 아래의 그림과 같이 방음실에서 보청기 착용 전후에 같은 음압레벨(45~50 dB HL)로 제시하여 측정한 단어인지도를 비교해야 한다(Bentler et al., 2016). 참고로 헤드폰 착용 시의 단어인지도는 보청기 착용효과 또는 난청인의 종합적인 어음인지 능력을 추측하는 것으로 보청기 착용 후의 어음인지도와는 개인 별 표준편차, 청력의 정도 및 형태 등으로 인해 차이가 나타난다.  그리고 보청기를 착용하는 목적이 일상생활에서 보통 크기의 대화음을 인지하는 것이 목적이기 때문에 보청기 착용 후 MCL 또는 60 dB HL에서 단어인지도를 구하는 것은 큰 오류이다. 다만 청력역치레벨이 경중도 이하이거나 심도 이상인 경우는 제시어음의 음압레벨을 조절할 수 있다. 보청기 착용 전후의 어음인지도 그리고 어음의 제시음압레벨의 예 ​ "
OpenAI의 인공지능 모델 라인업 현황과 미래 전망 ,https://blog.naver.com/shkim_pat/223046291191,20230316,"이상한 김변리사는 인공지능(AI) 전문 미디어 'AI타임스'에 AI를 주제로 하는 글을 기고하고 있습니다. 아래 글은 AI타임스에서도 보실 수 있습니다.​  ​오픈AI(OpenAI)가 배포한 인공지능 모델 중 가장 유명한 것은 챗GPT(ChatGPT)이다. 지난 글에서는 챗GPT를 소개하면서, 챗GPT의 학습 데이터, 학습에 사용한 모델, 학습 환경 등을 토대로 챗GPT가 특허로 보호받을 수 있는지 함께 검토해 보았다. 그러나 오픈AI의 인공지능 모델 라인업을 보면 더 많고 다양하다. 국내에서는 아직 많은 관심을 받지 못하고 있지만, 해외에서는 뛰어난 성능으로 주목받고 있는 모델들을 소개하고 싶다. 필자의 직업은 일종의 기술 작가(Technical Writer)인 변리사이다. 조금 어려운 내용도 있지만 가능한 쉽게 풀이하여 독자들에게 전달해 보겠다. 그리고 가능하다면 OpenAI가 준비하는 미래도 함께 전망해 보겠다.​​챗GPT는 대형 언어 모델인 GPT-3로부터 시작되었다. GPT-3를 이용하면 작문, 요약, 질의응답, 분류, 번역 등을 자동으로 할 수 있다.​​ GPT-3는 인간이 쓴 글을 무서울 정도로 실제에 가깝게 모방할 수 있다. 인간이 직접 쓴 글처럼 복잡한 문장을 만들어 낼 수도 있다. 그 때문에 인간이 쓴 글인지 GPT-3가 쓴 글인지 쉽게 구분하기 어렵다. 그래서 인공지능이 작성한 글을 식별해 내는 GPT제로(GPTZero)라는 앱이 출시되어 인기를 누리고 있다. GPT-3가 이해하는 언어에는 프로그래밍 언어도 포함된다. 2021년부터 인간을 대신해 코딩을 하고 있다.​​GPT-3는 상업용 글쓰기인 카피라이팅에도 효과적이다. 비개발자 유저라면 제일 관심 분야일 듯하다. 실제로 AB 테스트를 이용하여 인간과 GPT-3 간의 카피라이팅 대결이 이루어졌다. 결과는 더 높은 전환율을 기록한 GPT-3의 승리였다. GPT-3에 대해서 각광하는 이유도 이것 때문이다. GPT-3는 시간, 창의성, 생산성을 도와주는 도구가 될 수 있기 때문이다. 아이디어나 영감이 부족하더라도 누구나 글을 쓸 수 있다. 그것도 고작 몇 초 만에 말이다. 구독료는 인건비에 비하면 얼마 되지 않는다.​​GPT 모델은 사실 하나가 아니다. 여러 모델이 모여 가문(family)을 이루고 있다. 기본 모델로는 Davinci, Curie, Babbage, Ada가 있다. 나열한 순으로 성능이 우수하고 비싸다. Ada가 가장 저렴하지만 가장 빠르기도 하다. 텍스트 구문 분석, 간단한 분류, 키워드 추출 등의 태스크에 적합하다. Codex라고 하는 모델도 있다. 이 모델은 앞서 말한 코딩을 대신해 주는 모델이다. 수십억 줄의 GitHub 코드를 학습했고, 특히 Python에 가장 능숙하다고 소개하고 있다.​​GPT-3는 GPT 시리즈의 3세대 모델이다. GPT-3의 이전 모델인 GPT-2는 15억 개의 파라미터를 가지고 있었는데, GPT-3의 경우 그 수가 1,750억 개다. 인간으로 치면 뇌의 용량과 지능이 100배 커진 것이다. 인공지능의 '파라미터'는 인간의 뇌의 '시냅스'라는 것과 대비된다. 시냅스는 뇌에서 신호를 전달하는 통로 역할을 한다. 인간의 시냅스는 약 100조 개라고 알려져 있는데, GPT 시리즈의 최종 목표도 버금가는 개수의 파라미터일 것이다.​​ GPT-3의 카피라이팅, 출처: 오픈AI​​달리(DALL-E)를 '이미지 생성기'라는 딱딱한 명칭보다는 '인공지능 아티스트'라는 이름으로 소개하고 싶다. 달리의 이름은 '월-E(애니메이션)'와 '살바도르 달리(화가)'에서 따왔다​​. 달리는 그림을 그리고, 아웃페인팅과 인페이팅을 하고, 이미지를 변형하기까지 한다.​​달리에게 그림을 그리게 하기 위해서는 텍스트로 명령하면 된다. 달리가 학습한 수많은 사진들 중에서 입력한 단어로 분류되어 있는 사진을 단순히 찾아주는 것 아니냐고 의심할 수 있겠다. 그러면 생성형 AI라는 이름을 달리에게 붙였을 리 있겠나. '살바도르 달리'는 초현실주의 화가이다. 그의 화풍을 물려받은 달리는 '아보카도 모양의 안락의자'나 '말을 타고 달을 달리는 우주 비행사'와 같은 이미지도 척척 그려낸다. 정해진 규칙에 따라 명령어를 입력할 필요도 없다. 달리는 우리가 일상에서 사용하는 자연어를 이해하기 때문이다.​​아웃페인팅과 인페이팅 실력은 정말 놀랍다. 아웃페인팅은 원본 이미지의 바깥 부분 배경을 그려내는 것을 말하고, 인페이팅은 원본 이미지의 일부를 지워낸 뒤 특정 객체로 덮씌워 그려내는 것을 말한다. 아래 좌측 이미지를 보면, 달리는 원작의 어두운 배경을 벽으로 만들고 존재하지 않았던 몸과 팔을 자연스럽게 만들어냈다. 우측 이미지에서도 달리는 원작에 없었던 플라밍고 튜브를 그려냈다. 빛과 그림자까지 고려하여 티가 나지 않는다.​​달리는 입력 받은 이미지를 자유롭게 변형할 수도 있다. 독자들은 한 번쯤 '진주 귀고리를 한 소녀'를 마주한 적이 있을 것이다. 원작은 네덜란드 화가인 요하네스 페르메이르가 그린 것이다. 그러나 우리가 본 작품은 고개가 오른쪽을 바라보고 있거나, 터번의 모양이나 색상이 바뀌었거나, 진주가 더 커졌거나, 귀고리가 다른 것으로 바뀐 것들이었다. ​​다음 세대 달리는 어떻게 변화할까? 지금까지의 달리가 텍스트를 이미지로 만드는 기술(Text-to-Image)이었다면, 다음은 아마도 텍스트를 비디오로 만드는 기술(Text-to-Video)이 되지 않을까 싶다. 그것이 영상 중심의 콘텐츠 소비 트렌드에 맞다. 그렇게 되면 화가가 아니라 감독을 대신할 수 있게 될 것이다. 아직 음성이나 배경음악이 입혀진 상태는 아니기 때문에 결과물은 무성영화 같은 형태가 될 듯하다.​ 달리가 그린 작품, 출처: 오픈AI​​위스퍼(Whisper)는 오디오 전문 인공지능이다. 오픈AI는 위스퍼를 '자동 음성 인식(Automatic Speech Recognition, ASR) 시스템'이라고 소개하고 있다.​​ 위스퍼는 음성을 인식하고, 전사와 번역을 할 수 있다. 전사(transcription)한다는 것은 음성을 문자로 옮기는 것을 말한다. 위스퍼는 영어 외 다른 언어도 전사할 수 있다. 이를 위해서 위스퍼는 680,000시간의 다중언어(multilingual) 음성 데이터를 학습했다.​​오픈AI의 위스퍼 연구 소개 페이지를 보면, 흥미롭게도 비영어(Non-English) 전사의 예로 K-Pop 한국어 전사를 제시하고 있다. 사용된 음악은 가수 윤하의 '오르트 구름'이다. 위스퍼는 한국어를 인식하고 전사와 동시에 번역을 수행해서 결과물로 'While darkness was my everything I ran so hard that I ran out of breath ...'을 출력해낸다. 위스퍼의 핵심 개발자 중 한국인이 있기 때문이다. 그 덕분인지 위스퍼는 한국어 데이터도 상당 시간을 학습했다.​​그 이름답게 악센트가 특이하거나 배경 소음에도 강해 작은 소리도 인식해낼 수 있다. 음성 인식 시스템이라고 하면, 단어 하나 혹은 음절 하나 단위로 인식하는 것이라고 생각하기 쉽다. 그래서인지 내비게이션을 이용할 때 우리는 음절 하나하나를 또박또박 발음하고 있지 않나. 그러나 GPT-3와 형제인 위스퍼는 다르다. 위스퍼는 문맥을 이해하기 때문에 오디오의 다음 단어를 예측하는 방식으로 정확도를 높이고 있다. 이 같은 특성 때문에 위스퍼는 오히려 음성에는 없던 엉뚱한 텍스트를 내놓기도 한다. 점진적으로 개선되리라 기대해 본다.​ 위스퍼가 전사한 음성, 출처: 오픈AI"
[논문리뷰-20] Semi Supervised Multi label learning for Classification of Wafer Bin Map ,https://blog.naver.com/mindmerge/223051963262,20230322,"#Semi-Supervised Multi-Label Learning forClassification of Wafer Bin MapsWith Mixed-Type Defect Patterns​ ​This abstract describes a study on classifying defect patterns in wafer bin maps (WBMs) to identify the root causes of process faults in semiconductor manufacturing. ​The semiconductor manufacturing process has become more complicated with the increase in wafer size, resulting in an increased probability of mixed-type defect patterns in WBMs. Previous studies have mainly used labeled data to classify mixed-type defect patterns, but this study proposes the use of a semi-supervised deep convolutional generative model to utilize both labeled and unlabeled data. The proposed model formulates the problem of classifying mixed-type defect patterns as a problem of multi-label classification and adopts multiple latent class variables for distinct single patterns. The proposed model also has the advantage of being able to generate new WBM data.​ ​This passage describes the proposed method for classifying WBMs with mixed-type defect patterns using a semi-supervised learning approach. ​The method is an extension of SS-DGM, which is a semi-supervised version of the popular deep generative model, VAE. The proposed method utilizes deep convolutional neural networks to extract local invariant features from high-dimensional image data more effectively, thus constructing a semi-supervised convolutional deep generative model (SS-CDGM). ​This approach is more effective in modeling complex high-dimensional data, such as WBMs, using deep neural networks as flexible function approximators. The proposed method can be trained using an efficient variational optimization algorithm and has shown effective performances in many semi-supervised learning problems. The structure of the SS-DGM can be easily extended to address the problem of classification of mixed-type defect patterns.​The semi-supervised version of the popular deep generative model, VAE, is a deep learning method called semi-supervised deep generative model (SS-DGM). SS-DGM is a deep generative model that combines a deep neural network with variational inference to learn a latent variable model of the data distribution. The model can be trained with both labeled and unlabeled data to improve classification accuracy, which makes it a semi-supervised learning method. The SS-DGM is an extension of the VAE, which is a deep generative model that uses a probabilistic encoder and decoder to generate new data samples from a latent variable space. The SS-DGM has been used successfully in various applications, including image and speech recognition, and natural language processing.​The proposed model has an inherent advantage of being a generative model, which means that it can generate new WBM data after the training is completed. ​By controlling the values of four latent class variables, the model can generate new WBM data with desired defect patterns. This feature can be useful for mimicking real WBM data and can be a great advantage when there is a limited amount of real data available for training deep neural network models. The generated WBM data can also be used for other studies and applications.However, the generated WBM data may not be perfectly clear due to the sigmoid activation applied to the last fully connected layer. To produce clear WBM data, the generated data needs to be binarized by thresholding the values for individual dies. In this study, a threshold of 0.25 was found to produce clear WBM data without erasing the defect patterns. The generated WBM data with all possible combinations of four latent class variable values are illustrated in Figure 8 of the article.​ ​ "
[딥 러닝(DL) 개요] - 4. 딥 러닝의 적용 ,https://blog.naver.com/dev_jun97/223017514119,20230216,"1. 컴퓨터 비전(Computer Vision) 기계의 시각에 해당하는 부분을 연구하는 컴퓨터 과학의 최신 연구 분야 중 하나이다.(출처:https://ko.wikipedia.org/wiki/%EC%BB%B4%ED%93%A8%ED%84%B0_%EB%B9%84%EC%A0%84) 컴퓨터 비전 - 위키백과, 우리 모두의 백과사전컴퓨터 비전 41개 언어 문서 토론 읽기 편집 역사 보기 위키백과, 우리 모두의 백과사전. 시리즈 인공지능 펼치기 주요 목표 펼치기 접근 펼치기 철학 펼치기 역사 펼치기 기술 펼치기 용어 v t e 컴퓨터 비전 (Computer Vision)은 기계의 시각에 해당하는 부분을 연구하는 컴퓨터 과학의 최신 연구 분야 중 하나이다. 공학적인 관점에서, 컴퓨터 비전은 인간의 시각이 할 수 있는 몇 가지 일을 수행하는 자율적인 시스템을 만드는 것을 목표로 한다 (많은 경우에는 인간의 시각을 능가하기도 한다). [1] 그리고 과학적 관점에서는...ko.wikipedia.org ​ 이미지 분류(Image Classification) 이미지에서 물체, 사람, 장면을 식별하는 데  사용된다. 그림 1. 개, 고양이 분류(출처:https://towardsdatascience.com/10-papers-you-should-read-to-understand-image-classification-in-the-deep-learning-era-4b9d792f45a7)[그림 1]과 같이 입력 이미지를 미리 정해진 카테고리 중 하나의 라벨로 분류하는 문제이다. 문제 정의는 간단하지만, 컴퓨터 비전 분야의 핵심적인 문제 중의 하나이다. 이미지 분류의 확장으로 물체 검출, 영상 분할 등이 이미지 분류 문제를 푸는 것으로 인해 해결될 수 있다. 객체 탐지(Object Detection) 그림 2. OpenCV의 딥 뉴런 네트워크로 감지된 물체(출처:https://ko.wikipedia.org/wiki/%EA%B0%9D%EC%B2%B4_%ED%83%90%EC%A7%80)컴퓨터 비전, 이미지 처리와 관련된 컴퓨터 기술로서, 디지털 이미지와 비디오로 특정한 계열의 시맨틱 객체 인스턴스(인간, 건물, 자동차 등)를 감지하는  작업이다. [그림 2]와 같이 OpenCV를 이용하여 사물을 구별할 수 있다. 이미지 분할(Image Segmentation) 이미지를 각각 다름 객체 또는 배경을 나타내는 다른 영역으로 분할할 수 있다.이미지 분할은 디지털 이미지를 이미지 영역(image regions) 또는 이미지 개체(image objects)라고 하는 여러 이미지로 분할하는 프로세스이다.분할의 목표는 이미지 표현을 보다 의미 있고 분석하기 쉬운 것으로 단순화하는 것이다.[그림 3]은 자율주행 차량, 사람, 사물 등 인식 기능을 설계할 때에 사용하는 이미지 분할의 예시이다. 그림 3. 자율주행​  2. 자연어 처리(NLP) 텍스트 분류 (Text Classification) 텍스트를 감정 분석 또는 스팸 탐지와 같은 다양한 범주로 분류할 수 있다. 그림 4. 기사 분류텍스트를 입력으로 받아 텍스트가 어떤 종류의 범주에 속하는지를 구분하는 작업이다.[그림 4]와 같이 여러 개의 기사가 있으면 그 기사가 스포츠. 엔터테이너, 기술에 관한 기사인지를 판단하여 분류하는 작업의 예시이다.  또한 스팸처리와 같은 이진 분류(Binary Classification), 글의 내용이 긍정 혹은 부정인지를 판별하는 감정 분석(Sentiment Analysis), 챗봇에 많이 쓰이는 입력 받은 텍스트로부터 사용자의 의도를 질문, 명령, 거절 등과 같은 의도를 분류하는 의도 분석(Intention Analysis) 등 다양한 텍스트 분석이 존재한다. 텍스트 생성(Text Generation) 뉴스  기사나 제품 설명과 같은 새로운 텍스트를 생성할 수 있다. 그림 5. LSTM을 사용한 글 생성(출처 : https://iq.opengenus.org/text-generation-lstm/)어떤 단어 이후의 나올 문자나 단어를 예측할 때 사용한다. 텍스트 데이터는 일반적으로 일정한 시퀀스를 가지고 있다. (영의 경우 띄어쓰기, 한국의 경우 조사 혹은 띄어쓰기 등등) 따라서 RNN 또는 LSTM 같은 딥 러닝 모델을 사용하여 다음에 올 문자나 단어를 예측하여 텍스트를 생성한다. 질문 답변(Question Answering) 대량의 텍스트 데이터를 기반으로 질문에 답할 수 있다. 그림 6. ChatGPT2023.02.16 기준 뜨거운 감자인 ChatGPT인 경우 질문과 답변에 사용된다고 볼 수 있다. 과거에도 챗봇이나 다른 툴들이 존재했다. 사용자가 원하는 검색어를 구글 검색하는 것처럼 입력하면 딥러닝을 통한 최적의 결과값을 출력해 준다.  3. 음성 인식(Speech Recognition) Voice to Text 음성을 텍스트로 전사할 수 있으므로 음성 콘텐츠를 검색하고 분석하는 것이 더 쉬워진다. 그림 7. 음성 인식 과정[그림 7]과 같이 인식 구간 추출, 잡음처리, 특징추출과 같은 과정을 딥러닝의 자연어 처리 모델을 사용하여 분석하는 것이다.  음성 합성(Speech Synthesis) 텍스트에서 음성을 생성하여 가상 개인 비서 및 기타 음성 기반 응용 프로그램을 만들 수 있다. 그림 8. 음성 합성대표적인 예로 TTF 서비스를 들 수 있고 시각 장애인을 위한 읽기 도구, 귀로 듣는 책 등에 사용되는 것을 알 수 있다.​ "
엘리트만 살아남을 시대 [AI와 메타버스] ,https://blog.naver.com/theelitementor/223062844991,20230402,"The new age of AI and the metaverse has dawned. Brace yourselves. © Getty Images​​여러분은 인공지능(Artificial Intelligence: AI)과 메타버스(metaverse)의 시대에 살고 있습니다. ​​AI가 나오기 전까지 ""학습 능력(learning ability)"" 또는 ""논리적 생각(logical reasoning)""은 사실상 이 세상에서 오직 인간만이 독점(monopolize)할 수 있는 능력이었습니다. ​Not any more. ​이제 AI의 능력은 정보 수집(data collection)은 기본이고, 시각적 인식(visual perception), 음성 인식(speech recognition), 서로 다른 언어들 사이의 번역(translation) 뿐만 아니라, 결정(decision-making) 등 논리적 판단(judgment)의 영역에까지 그 범위(range)를 넓히고 있습니다. ​​AI는 인간의 핵심적인 능력을 추가적으로 수행하고 있습니다. ​​그리고 AI 기술의 발전 속도(speed)는 점점 더 빨라지고 있습니다. ​​예를 들어, 이미 미국의 SAT 시험에서 상위 10%에 해당하는 점수를 받을 정도로 진화하였고(competitively evolved), 세계적인 명문 와튼 스쿨(The Wharton School)의 경영학 석사(MBA) 과정에서도 무난히 졸업할 수 있을 정도로 고도화되었습니다(highly advanced). ​즉 AI는 이미 인류의 90% 이상을 제쳤습니다. ​​ OpenAI announces GPT-4, claims it can beat 90% of humans on the SATGPT-4 performed at the 90th percentile on a simulated bar exam, the 93rd percentile on an SAT reading exam, and the 89th percentile on the SAT Math exam, OpenAI claimed.www.cnbc.com ​​ ChatGPT Gets an MBAThe AI-powered chatbot did better than expected on a Wharton exam. That’s something to get excited about, says the professor behind the experiment.www.bloomberg.com ​​메타버스 기술이 나오기 전까지는 우리의 세상은 눈으로 보고 만질 수 있는 물리적인 3D 공간이 사실상 전부였습니다. 인터넷은 우리가 정보를 찾고 의사소통을 하는 유용한 공간이었지만, 2D 모니터 화면이라는 결정적인 공간적 한계가 있었습니다. ​​이제는 메타버스라는 새로운 가상 현실 공간(a virtual-reality space)이 생겨서, 2D 인터넷은 물론 그동안 인간이 활동해오던 3D 물리적 공간의 제약(limitations)도 초월(transcend)해서 존재하고 활동할 수 있게 되었습니다. ​​공간의 제약을 초월한다는 것은 동시에 시간의 제약도 극복한다는 의미입니다. ​이전까지는 공간의 이동에는 시간이 필요하였는데, 메타버스 기술 덕분으로 이제는 공간의 이동에 더 이상 시간이 문제가 되지는 않기 때문입니다.​현재로선 AI보다 상대적으로 느리지만, 메타버스 기술의 발전 속도도 점점 더 빨라지고 있고, 그 활용 범위도 점점 더 넓어지고 있습니다. ​무엇보다도, 메타버스 기술에 거는 인간의 기대(expectation)가 이미 매우 큽니다. ​​ Why the Metaverse Will Change the Way You WorkVirtual meetings that feel real, new ways to build and teach, plus jobs you haven’t heard of—soon it won’t be science fiction.www.wsj.com ​​AI와 메타버스 덕분에 우리는 이전에 할 수 없었던 많은 것을 할 수 있게 되었고, 따라서 우리의 삶을 무척 편하고 윤택하게 해주는 측면이 있는 것은 분명합니다. ​True, they are making our lives more efficient and richer than ever before. ​​그러나, 만약에 여러분이 이런 첨단 과학 기술로부터 혜택(benefits)만 받을 것으로 생각하신다면, 너무나 큰 착각을 하시는 것입니다. ​​You would be hopelessly naive. ​They are very likely to be the biggest threats to your survival. ​As a matter of fact, they already are. ​​이 문제에 대해서 여러분이 차근차근 생각해보실 수 있는 기회를 만들어드려야겠다는 생각을 오래 전부터 하였는데, 이제 이에 대해서 다룰 수 있는 시간이 되어서 포스트를 올립니다. ​​​​By failing to prepare, you are preparing to fail. ​ㅡ Benjamin Franklin (1706 - 1790) ​​​​ © Getty Images​​첫째, AI는 여러분의 지적인 능력(intelligence)을 이미 위협하고 있습니다. ​ intelligence​The ability to acquire and apply knowledge and skills. ​Oxford English Dictionary​지능은 크게 두 가지 영역으로 나눌 수 있습니다. ​하나는 지식이나 기술을 학습(acquire)하는 능력이고, 다른 하나는 학습한 것을 응용(apply)하는 능력입니다. ​​학습 능력은 이미 AI가 인간의 능력을 능가(excel)하였습니다. ​​인간이 만든 각종 AI 컴퓨터와 프로그램은 ""여러분이 지금까지 학습한 모든 지식""을 단 몇 시간 또는 심지어 몇 분 안에 (in a matter of hours or minutes) 학습할 수 있고, 어떤 AI는 여러분이 알고 있는 모든 지식이 이미 완전히 입력되어 있는 상태입니다. ​(여러분 중 '한 명이 알고 있는 모든 지식'이 아니라, ""여러분 모두가 알고 있는 모든 지식""을 말하는 것입니다.) ​실제로 여러분 중 상당수가 이미 그런 AI를 현재 사용하고 있기도 합니다. ​​AI는 스스로 학습하는 능력도 가지고 있기 때문에, 그리고 과학 기술이 계속 발전하기 때문에, AI는 앞으로도 계속 진화(evolve)하여 지식을 더욱 많이 알게 될 것이고, 학습 속도(speed)도 증가할 것입니다. ​그러나, 여러분의 뇌(brain)는 눈에 띄게 진화하지는 않을 것입니다. ​인간의 능력에는 한계가 있습니다. 인간의 능력 개발 속도는 AI 기술 발전 속도에 비하면 너무 느립니다. ​​현재 인류의 뇌는 지금으로부터 무려 1만년 전에 (10,000 years ago) 살았던 인류(neolithic humans)의 뇌와 사실상 차이가 없다는 걸 혹시 아시는가요? ​다시 말해서, 1만년 전에 태어난 아기를 (타임머신을 타고) 현재로 바로 데리고 와서 잘 키우면, 아무런 문제없이 여러분처럼 현대 문명에 적응하고, 심지어 한국의 서울대(SNU)나 미국의 예일(Yale) 같은 명문대에 합격할 수 있다는 의미입니다. ​​AI는 여러분과는 달리 기억력(memory)에도 아무런 문제가 없습니다. ​​인간에게 기억력은 너무나 불완전한 능력이라서 이미 학습한 것도 시간이 좀 지나면 거의 다 잊어버리곤 하지만, AI는 한 번만 입력하면 모든 정보를 무기한, 완벽하게 기억합니다. ​예를 들어서, 여러분과 AI가 똑같은 학교에 입학해서 새로운 지식을 동시에 배우기 시작하면, 여러분은 학습 능력 측면에서는 절대로 AI를 따라가지 못할 것입니다. 똑같은 기간에 배운 지식을 AI는 100% 기억하지만, 여러분은 상당히 많이 잊어버릴 것이니까요. ​학교에서만 경쟁력에서 차이가 날까요? ​여러분과 AI가 똑같은 회사에 입사해서 새로운 일을 동시에 배우기 시작하여도, AI가 업무 파악 능력 측면에서도 여러분을 압도(overwhelm)할 것입니다. ​​머리 나쁘고 일 못하는 사람을 뽑느니, 시키는 대로 척척 하는 AI를 채용하지 않을까요? ​​Do you get the picture? ​​ The Quest for a Robot With a Sense of TouchRobots that can touch are the next step in robotics technology. It will allow them to perform all sorts of tasks that are now impossible.www.wsj.com ​​​둘째, 로봇(robot)은 여러분보다 체력, 즉 물리적인 힘(physical strength)도 월등(superior)합니다. ​​여러분과는 비교도 안 되는 엄청난 힘을 가진 AI 로봇을 이미 생산하고 있고, 산업 현장이나 일상 생활에서 대량으로 소비하는 것도 이제는 시간 문제일 뿐입니다. ​지적인 능력이 상대적으로 부족한 사람들은 (어쩔 수 없이) 육체적인 노동을 하면서 생존해왔는데, 이제는 육체적인 노동이야 말로 AI 로봇이 더 잘할 수 있는 시대가 되었습니다. ​​참고로, 육체 노동(manual labor or blue-collar work)은 여러분이 생각하시는 것보다 범위가 훨씬 더 넓습니다. ​벽돌을 나르거나 청소를 하는 사람들처럼 돌아다니거나 물건을 만지는 사람들만이 육체 노동자가 아닙니다. 사무실에 가만히 앉아서 일을 하더라도, 상사(boss)가 ""시키는 대로만 일하는 단순 사무직"" 직원들도 모두 육체 노동자들입니다. ​기계적으로(mechanically) 일하니까요. ​ mechanically​1. by means of a machine or machinery​2. without thought or spontaniety; automatically​Oxford English Dictionary​원래 기계적인 일은 인간보다 기계가 훨씬 더 잘합니다. ​​기계는 인간처럼 잡념(stray thoughts)도 없고, 집중력의 한계도, 심리적 불안정도 없습니다. ​AI가 없는 로봇도 이미 인간보다 더 큰 물리적 힘을 발휘한지 오래되었죠? 시키는 대로 잘하죠? ​AI가 탑재된 로봇은 이제까지와는 차원이 다르게 (수많은 분야에서) 인간의 노동력을 불필요하게 만들 것입니다. ​​자율주행차(self-driving car or autonomous vehicle) 기술이 이미 시장에 나왔죠? ​예를 들어서, 이제는 AI가 탑재된 로봇이 택시나 버스 등 자동차 운전 기사(car drivers) 또는 배달 기사(deliverymen)를 모두 대체(replace)할 수 있게 된다는 의미입니다. ​Hence the term ""driver-less."" ​​힘들어 하거나 귀찮아 하거나 짜증낼 줄도 모르고, 한결같이 친절하게, 논리적으로, 고객의 필요(needs)를 효율적으로 충족시키는 AI를 기대하는 소비자들의 수요(demand)는 이미 충분합니다. ​​한마디로 말해서, 여러분이 한창 활동하실 시대에는 지적인 능력 뿐만 아니라, 육체적인 노동력에서도 인간은 AI와의 경쟁에서 밀릴 수 있다는 것입니다. ​​ What Is the Metaverse? The Future Vision for the InternetPeople will be able to do almost anything in the metaverse: attend concerts, watch UFC fights, participate in work meetings. Here’s what it could look like.www.wsj.com ​​그나마 AI는 기본적으로 프로그램(program)이고 알고리듬(algorithm)이기 때문에, 우리가 실제로 살고 있는 물리적인 공간에서는 인간과 너무나 크게 구별됩니다. ​(적어도 아직은) AI가 실제 세상에서는 완벽한 인간의 모습으로 존재할 수가 없기 때문이죠. ​It's not even close. ​​인간은 ㅡ 오직 인간만이 제공할 수 있는 ㅡ 매우 다양하고 복잡한 인간적인 특징들(human characteristics)을 심리적으로 원하고 또 다양한 이유로 필요로 하기 때문에, 이런 측면에서는 AI 로봇이 인간보다 질적으로 경쟁력이 떨어집니다. ​​그러나, 메타버스 공간에서는 그렇지 않습니다. ​It's a whole new world, involving a whole new game. ​​셋째, 머지 않아 메타버스 안에서는 AI가 인간보다 훨씬 더 경쟁력이 있을 것을 예상하고 대비해야 합니다. ​​왜냐하면, 메타버스 안에서는 인간(avatar)인지 AI인지 ㅡ 여러분의 눈과 귀만으로는 ㅡ 도저히 구별이 되지 않을 날이 올 것이기 때문입니다. ​​특히 사회적 생존과 직접적으로 관련된 지적, 물리적 능력 (intellectual and physical abilities) 측면에서는 AI 로봇이 메타버스 안에서 여러분보다 월등하게 더 뛰어난 능력을 발휘하는 ""사람의 모습""으로 나타날 날이 곧 올 것을 여러분은 예상하셔야 합니다. ​For your own survival. ​왜냐하면, 과학 기술의 발전으로 메타버스 공간은 더욱 정교하고 세련되게 디자인될 것이기 때문입니다. ​그리고, 우리가 그동안 알던 실제 세상의 물리적 공간은 앞으로도 크기가 그대로 유지되지만, 메타버스 공간은 엄청나게 ㅡ 어쩌면 상상을 초월할 정도로 크게 ㅡ 늘어날 것이기 때문입니다. ​​ The social structure © The Elite Mentor​인간 사회는 피라미드 구조라는 것을 여러분은 이미 잘 알고 계실 겁니다. ​이것은 어느 나라, 어느 분야에서나 마찬가지입니다. ​​앞서 올려드린 ""엘리트가 되기 위한 시간 (3)"" 포스트에서도 이에 대해서 설명해드린 바 있습니다. ​​ 엘리트가 되기 위한 시간 [Time to be an Elite] (3) : 논리적 사고력을 향상시키는 가장 이상적인 방법앞서 두 번에 걸쳐서 올려드린 ""엘리트가 되기 위한 시간"" 포스트에서는 시간(time)과 기초(foun...blog.naver.com ​​저는 ""사람들하고만 경쟁하면 되는 시대""에서 자랐고, 공부하였고, 미국으로 유학도 갔으며 취직도 하였고, 기업의 CEO도 되었습니다. ​지적인 능력에서 AI가 저를 따라오려면 (아직도) 너무나 많은 시간이 걸릴 정도로 격차를 무척 크게 벌려놓았습니다. ​이것은 제가 일찍이 AI를 의식하면서 실력을 높이 쌓은 것이 전혀 아닙니다. 저는 사실 다른 사람들을 별로 의식하지도 않았고, 따라서 어떻게 보면 사람들과 경쟁하지도 않았습니다. ​저는 저 자신하고만 싸우면 되었고, 또 그것만으로도 충분하였기 때문입니다. ​​요즘 여러분 세대를 보면 '참 불쌍한 세대(a pitiful generation)이구나' 하는 생각을 종종 하게 됩니다. ​​실제로 여러분은 (여전히 사람들과도 경쟁하면서) ""AI와도 경쟁해야만 하는 시대""에 살고 있기 때문입니다. ​​Again, you're living in a whole new world. ​​(그런 의미에서 저는 '얼마나 운이 좋은 세대인가' 하는 생각도 하게 됩니다.) ​​그러나, 세상은 항상 변해왔습니다. ​항상 새로운 것이 발생해서 오래된 것을 대체하였습니다. ​​도전이 없는 성공은 존재하지 않습니다. ​​​​A pessimist sees the difficulty in every opportunity; an optimist sees the opportunity in every difficulty. ​ㅡ Winston Churchill (1874 - 1965) ​​​​Never let the future disturb you. You will meet it, if you have to, with the same weapons of reason which today arm you against the present. ​ㅡ Marcus Aurelius (121 - 180) ​​​​저는 논리적이고, 합리적인 사람입니다. ​저는 여러분을 동정(sympathize)이나 하기 위해서 이 글을 쓰기로 하지는 않았습니다. ​​저는 ""AI와 메타버스 시대에서 여러분이 어떤 마음가짐(mindset)과 비전(vision)을 가지고 살아야 하는지""에 대해서 조언(advise)해드리기 위해서 이 포스트를 쓰기로 하였습니다. ​​That's because the highest law of nature is that you shall survive no matter what the circumstances. ​Adapt and evolve. ​Fight and win. ​​ The social structure in the age of AI © The Elite Mentor​간단히 설명해드리죠. ​앞에서도 말씀드렸지만, 엘리트는 가장 상위에 있는 집단입니다. ​지적인 능력이 상대적으로 부족해서 육체적인 노동으로 생존해야 하는 최하층(the lowest strata) 집단은 이제 사회적 멸종(social extinction)의 위기에 있다고 보시면 됩니다. ​기계와 AI 로봇이 그들을 가장 먼저 대체(replace)할 것이니까요. ​​최하층보다는 상대적으로 지적인 능력이 더 뛰어나지만, 엘리트 수준은 아닌 중간층 집단(the mediocre class)은 위 이미지에서 회색으로 표시되어 있습니다. ​AI는 이미 이들의 능력과 직업도 일부 잠식(take over)하였고, 이들을 완전히 대체하는 것도 시간의 문제일 뿐입니다. ​​구체적으로 설명해드리자면, 특별한 재능도 없고 체력이나 정신력이 특별히 뛰어나지도 않으면서, 집단적으로(collectively) 학원에나 다니거나 기계적(mechanically), 수동적(passively)으로 공부하는 습관이 몸에 밴 사람들이 ㅡ 즉 원래 인간들 사이에서도 별로 경쟁력이 없었던 집단이 ㅡ AI의 직격탄(direct hit)을 맞을 것입니다. ​​​AI가 넘보지 못하는 영역이 바로 ""엘리트의 세계""입니다. ​​ 엘리트(elite)의 의미엘리트(elite)의 뜻은 ""능력 또는 자질 측면에서 뛰어난 최고의 집단""입니다. elite A select g...blog.naver.com ​​엘리트는 단지 전문 지식만으로 되는 게 결코 아닙니다. ​전문 지식은 물론이고, 폭넓은 교양과 뛰어난 공감능력까지 골고루 갖추어야만 엘리트가 될 수 있습니다. ​​그 중에서도 가장 핵심이 바로 논리적 사고력(logical reasoning skills)입니다. ​​논리적 사고력은 수학적 연산(mathematical calculation)처럼 양적(quantitative)이지도 않고, 기계적(mechanical)이지도 않습니다. ​​논리는 매우 질적이고(qualitative), 입체적이며(multidimensional), 실질적이고(substantial), 추상적입니다(abstract).​​그리고, 경우에 따라서는 양적이고, 기계적이며, 형식적이고(formal), 구체적이기도(concrete) 합니다. ​Logic is very complicated. ​​ 엘리트 유학을 꿈꾸는 분들을 위하여""엘리트(elite)""의 원래 의미는 ""능력 또는 자질 측면에서 뛰어난 최고의 집단""입니다...blog.naver.com ​​위에 링크해드린 ""엘리트 유학을 꿈꾸는 분들을 위하여"" 포스트를 포함한 수많은 포스트에서 제가 일찍이 논리적 사고력의 중요성을 강조해드렸지만, 앞으로 여러분은 더욱 높은 수준의 논리적 사고력과 영어 실력을 갖추기 위하여 노력하셔야 합니다. ​​다시 한 번 강조해드리지만, 저는 지금 먼 미래에 대해서 말하고 있는 게 아닙니다. ​저는 현재 여러분이 살고 있는 사회(contemporary society)에 대해서 말하고 있습니다. ​​저에게 AI는 공상과학이나 남의 이야기였지만, 여러분은 ""AI는 바로 나의 이야기이자 나의 문제다"" 라고 생각하셔야만 한다는 것을 명심하시기 바랍니다. ​​ 영국의 역사와 영어의 특징 [엘리트 영어의 올바른 이해 1]영어(English)는 현재 지구상에서 가장 많은 사람들이 사용하면서도 여전히 가장 빠른 속도로 전파되고 있...blog.naver.com ​​또 메타버스 기술로 인하여 세계는 더욱 좁아질 것입니다. ​교통 수단도 이미 무척 발달하였지만, 앞으로는 장거리 여행의 필요성 자체가 상당히 감소할 것이기 때문입니다. ​그러면, 영국이나 미국에 사는 원어민이 메타버스 기술 덕분에 ""언제든지"" ""한국에서"" 활동할 수 있겠죠? ​​오래 전부터 저는 완벽한 영어 실력은 이젠 ""기본 중의 기본""인 능력이라고, 그 이유는 이미 영어는 세계어(world language)가 되었기 때문이라고 강조해드렸습니다. ​​여러분의 부족한 실력을 여러분이 AI 기술에 의존해서 해결하려고 할수록 ​첫째, 여러분 스스로 생각하고 해결하는 능력을 키울 수가 없게 되는 것이 가장 근본적이고 가장 심각한 문제이며, ​둘째, 오히려 AI에게는 여러분의 약점을 더 많이 파악하는 (그래서 여러분보다 더 경쟁력이 있게 될) 절호의 기회가 될 것이라는 점을 아시기 바랍니다. ​​AI는 이미 학습 능력이 있고, 이미 영어가 완벽하며, 게다가 논리적인 생각도 하기 시작하였기 때문입니다. ​​(예를 들어서 여러분이 ChatGPT 같은 AI를 통하여 궁금한 것을 물어보고 지식을 알아낼 수는 있지만, ""질문하는 능력"" 또한 개인의 논리적 사고력에 따라서 천차만별로 차이가 납니다. 그리고 AI가 아무리 정확한 정보를 여러분에게 알려주어도, 이 역시 얼마나 제대로 이해하느냐 하는 것은 여러분의 ""이해력"" 즉 논리적 사고력에 의해서 결정되는 것입니다. 다시 말해서, AI가 여러분의 능력 그 자체를 향상시켜 주는 데에는 한계가 있을 수밖에 없는 것입니다.)​​여러분이 미래에서의 생존을 준비하기 위해서는 지금 최선을 다해서 AI를 능가하는 것은 기본이고, AI와의 격차를 최대한 벌려야 합니다.  ​​""초격차 전략(super-gap strategy)""이라는 말을 들어보셨는가요? ​​ Tech That Will Change Your Life in 2023Our tech columnists look ahead to an Apple headset, Netflix password crackdowns, the next DALL-E and more.www.wsj.com ​​지금까지 저는 AI 및 메타버스와 관련하여 논리와 영어에 대해서만 조언해드렸습니다. ​​그러나, 사실 이 문제는 인간의 이성적, 지적인 능력에만 국한(limit)되지 않습니다. ​AI는 인간의 창의력(creativity)에도 이미 도전(challenge)하기 시작하였습니다. AI는 이미 음악을 작곡하고 그림도 그리기 시작하였습니다. ​즉 AI는 예술 분야에까지 그 손을 뻗고 있습니다. 다만 예술은 저나 이 블로그와 직접적인 관련은 없는 분야이기 때문에 여기에서 자세히 다루지는 않을 것입니다. ​그러나, 앞으로 예술가 등 창의적인 분야에 종사하는 분들도 무척 힘들어질 것입니다. AI는 이미 역사적인 걸작(masterpiece)도 감쪽같이 복사하기 시작하였을 뿐만 아니라, 심지어 예술성 측면에서도 인간으로부터 호평(favorable reception)을 받기 시작하였습니다. ​​ Thousands of Artists Reimagine Vermeer's 'Girl With a Pearl Earring'A Dutch museum selected winning works by five artists—and one A.I. image generatorwww.smithsonianmag.com ​​그러므로, 여러분은 세상이 어떻게, 얼마나 빨리 변하는지 ㅡ 예술을 포함하여 ㅡ 다방면으로 (in many ways) 잘 살피고, 시대의 흐름(trend)을 잘 따라가도록 꾸준히 노력하셔야 합니다. ​​​​You can never plan the future by the past. ​ㅡ Edmund Burke (1729 - 1797)​​​​이와 관련해서 아래 링크해드린 ""미국 로스쿨로 유학 가는 이유 (2)"" 포스트를 꼭 읽어보시기 바랍니다. 미국 로스쿨 유학을 생각하지 않는 분들도 꼭 읽어보시기 바랍니다. ​자기 자신에게 적합한 분야를 찾고 급변하는 시대에 적응하는 방법에 대해서 설명해드렸기 때문입니다. ​​ 미국 로스쿨로 유학 가는 이유 (2) - 한국으로 돌아오지 않는 경우미국 로스쿨의 JD 과정으로 유학 가서 졸업 후 계속 미국 등 해외에 남아서 활동하려면 지금부터 무엇을 ...blog.naver.com ​​​논리는 범위가 매우 넓습니다. ​여러분이 생각하시는 것 이상입니다. ​​​​I think, therefore I am. ​ㅡ René Descartes (1596 - 1650) ​​​​한마디로, ""인간의 이성(reason)과 관련된 모든 것""이 곧 논리입니다. ​이와 관련해서는 위에 링크해드린 ""엘리트 유학을 꿈꾸는 분들을 위하여"" 포스트와 아래 링크해드린 ""논리적 사고력을 향상시키는 방법"" 시리즈에서도 자세히 설명해드렸으니, 꼭 읽어보시기 바랍니다. ​​ 논리적 사고력을 향상시키는 방법이 글에서는 논리의 기본에 대해서 몇가지 설명해드리고자 합니다. 그리고 이를 바탕으로 앞으로 논리에 대...blog.naver.com ​​여러분이 성숙한 논리적 사고력을 갖춘 동시에 이 세상의 흐름을 잘 파악하시게 될 때, 여러분의 미래에 대해서도 방향을 그만큼 더 잘 설정하시게 될 것입니다. ​그런 의미에서 위에서 링크해드렸던 월스트릿저널(WSJ) 등의 기사들을 모두 읽어보시기 바랍니다. 인공지능 및 메타버스와 관련하여 너무나 중요한 기사들이니까요. ​특히 AI는 이 기사들의 내용을 모두 알고 있다는 걸 아시기 바랍니다. ​단순히 알고 있는 정도가 아니라, 모두 기억도 하고 있습니다. 즉시 요점을 요약할 수도 있습니다. ​하나의 글을 요약하기 위해선 고도의 논리적 사고력이 필요하다는 걸 알고 계시는가요? 어떤 내용이 더 중요하고 덜 중요한지(importance), 그리고 문장이나 단락들 사이의 맥락(context)을 판단(judge)해야 하기 때문입니다. ​여러분은 위 기사들을 아무런 막힘 없이 술술 읽어내려갈 수 있으신가요? 언제까지나 기억하고 얼마든지 요점을 정확히 요약할 수 있으신가요? ​중요한 기사가 새로 나오면, 바로바로 읽으실 수 있을 정도로 세상이 어떻게 돌아가는지 관심도 있고 습관도 되어 있으신가요? 뉴욕타임스나 월스트릿저널 같은 세계적으로 저명한 신문을 구독(subscribe)하고 계신가요? ​그만큼 영어를 잘하시는가요? ​​극소수의 엘리트가 아니라 일반 대중(the general public)을 상대로 출판하는 신문 기사(daily newspaper articles)를 읽으실 때, 과연 문법은 완벽하고 단어에도 막힘이 없으신가요? ​​ ​​논리적으로 미래에는 무슨 일이든 발생할 수 있고, 구체적으로 무슨 일이 발생할지는 그때 가보아야만 알 수 있는 것이기 때문에, 미래에 대한 예측(prediction)은 조심스럽게 해야 하는 게 맞습니다. ​(Therefore, you're welcome to disprove my argument ㅡ with evidence.) ​​그러나, 여러분에게 AI와 메타버스는 더 이상 미래가 아니고 현실이기 때문에, 제가 여러분의 경각심(attention and awareness)을 더욱 높이기 위해서 이 포스트를 쓴 것입니다. ​​논리적 사고력, 그리고 이를 뒷받침하기 위한 영어 실력은 여러분에겐 무슨 사치품도, 선택 과목도 아닙니다. ​They are not a luxury or option. ​​그리고, 이 AI와 메타버스의 시대에서 논리와 영어를 '대충' 잘하는 것은 그야말로 아무런 의미도 없습니다. ​​You need to master them to survive, not to mention succeed in life. ​​The clock is ticking, and you cannot turn back the clock. ​​​이제부터 여러분이 가장 심각하게 생각해보셔야 할 두 가지를 알려드리겠습니다. ​​첫째, AI 또는 AI 로봇은 할 수 없고, 여러분은 할 수 있는 일은 도대체 무엇인가요? ​둘째, AI 또는 AI 로봇도 할 수 있지만, 여러분이 월등하게 더 잘할 수 있는 일은 도대체 무엇인가요? ​​여러분이 위 두 가지 질문에 모두 긍정적으로 대답하고, 확신할 수 있으며, 특히 구체적인 이유까지 제시할 수 없다면, 여러분은 이미 위험한 상태에 있다는 것을 아셔야 합니다. ​​참고로, 저는 아주 정확한 이유로 이 질문에 대해서 여러 가지 긍정적인 대답을 할 수 있습니다. 그 중에서 몇 가지는 세계 최고(the best in the whole world)라고 생각합니다. ​(단지 이 세상의 모든 사람들을 일일이 조사해보지 않아서 객관적으로 증명할 수는 없을 뿐인데, 여기서 중요한 것은 저는 그럴 필요조차 없을 정도로 ""초격차""의 지위에 있다는 것입니다.)  ​​ ​​It is not the most intellectual of the species that survives; it is not the strongest that survives; but the species that survives is the one that is able best to adapt and adjust to the changing environment in which it finds itself. ​ㅡ Charles Darwin (1809 - 1882)​​​​​​​​​​​​​​© 엘리트 유학 멘토 엘유멘 엘유맨 The Elite Mentor  "
"4. SA(Speech Audiometry), SDT, SRT, SDS, WRS, MCL, UCL, DR, Masking level in speech ",https://blog.naver.com/eunae_89/222449663403,20210729,"어음청력검사 (SA) 회화 어음에 대한 청력역치 측정 회화 어음에 대한 이해능력의 평가 : HA 사용 시 성취도 예측어음에 대한 최적 음량 역치 측정 : HA 처방시 DR 추정PTA의 validity 평가 : PTA 평균 값과 SRT가 15dB 이상 차이날 때  malingering 의심 ​ speech bananaSpeech banana phoneme을 audiogram에 표시하면 banana 모양phoneme : 한 언어의 음성체계에서 단어의 의미를 구별 짓는 최소의 소리 단위 / 자음과 모음∨ low frequency : 모음∨ high frequency : 자음 ​​ SA 검사방법 검사기기 : PTA와 동일 자극음 : nonsense syllable, word, sentencs, paragraph어음 전달 방법 : recorded voice, monitored live voice반응 평가 방법 : written response, talkback response​​ Speech Audiometry 시 측정 내용 SDT : Speech Detection ThresholdSRT : Speech Reception ThresholdSDS : Speech Discrimination Score (=WRS, Word Recognition Score)MCL : Most Comfortable Loudness LevelUCL : Uncomfortable Loudness Level DR : Dynamic Range ​​ SDT  어음인식역치 : 단지 말이라고 확인할 수 있는 최저수준의 어음강도어음에서 최소가청역치자극 어음을 이해하기 위해서는 SDT + 8-9dB∨ monosyllabic words 사용하여 검사∨ SDT is always better than PTA∨ SDT is always better than SRT​ SRT 어음청취역치 : 검사어음의 50%를 정확하게 알아들을 수 있는 어음의 최소강도500Hz+1kHz+2kHz/3 + 30dB (MCL)에서 정반응율이 50%가 될 때까지 10dB씩 낮춰가며 검사, 역치부근에서는 5dB조절∨ spondee words 사용하여 검사∨ masking은 PTA에 준하여 시행∨ descending method ​SRT와 PTA평균 Threshold는 거의 일치하거나 10dB이내 차이 (ski-sloping type의 HL의 경우 15dB이상 차이를 보이며 이때는 SRT를 환자의 청력역치로 사용하는 것이 타당함)15dB 이상 차이 보이는 경우 : malingering, retrocochlear HL​ SDS(=WRS) 어음명료도검사 : 회화 어음에 대한 이해 능력 평가, 보청기 사용 시 성취도 예측500Hz+1kHz+2kHz/3 + 30dB (MCL)에서  자극음 제시하고 맞춘 개수를 %로 결과 표시∨ PB words 사용 (phonetically balanced word)​performance-intensity function curve∨ 가청범위에서 10-20dB 간격으로 SDS 측정하여 연결한 그래프∨ HL type 혹은 원인 감별에 도움∨ PB max (어음명료도 100%)  : 정상청력의 경우 SRT + 30dB에서 PB max 도달 ∨ conductive HL : intensity를 높이면 PB max 값에 도달∨ SNHL : intensity를 높여도 100%에 도달하지 못함∨ over-roll phenomenon : PB max를 보인 후 intensity를 증가시킬수록 오히려 SDS 값이 감소되는 현상, retrocochlea HL에서 저명 어음 명료도 곡선MCL 최적안정역치 : 5dB 간격으로 강도를 높여가면서 듣기에 가장 편한 음의 강도 측정SRT + 30dB 정도HA의 적정gain을 결정하는 기준​ UCL 불쾌역치 : 강도를 높여 감에 따라 듣기 불편함을 느끼는 음의 강도 측정SRT + 75-90dB 정도 (normal에서는 90-100dB까지 불편감이 없으나 SNHL에서는 감소함)HA의 출력 한계를 의미​ DR 역동범위 : UCL - SRT HA fitting에 활용​ Masking level in speech  (If cross-heard signal > 3FAHL BC(NTE))=3FAHL AC(NTE) + cross-heard signal + ESML(10)* cross-heard signal = PL-IA-3FAHL BC(NTE)-MDSL(5)* PL(presentation level) = 3FAHL AC + 30dB = MCL​​​​​ "
NLP(1) : 자연어 처리(Natural Language Processing)란? ,https://blog.naver.com/cslee_official/223073505980,20230413,"written by 송진숙 ​ 1. 자연어 처리(NLP; Natural Language Processing) 란? ​자연어 처리 (NLP; Natural Language Processing) [1] 분야는 인공지능의 한 분야로서 사람과 사람 간 지식, 의견, 정보 등을 전달하는 소통 수단인 언어를 컴퓨터가 이해할 수 있도록 변환하여 처리하는 분야이다. ​자연어 처리 기술을 응용한 대표적인 서비스 사례로 기계 번역(Machine Translation), 챗봇(Chatbot), 문법 자동 수정, 전체 텍스트 검색, 요약(summarization) 서비스 등을 생각할 수 있다. 또는 대량의 텍스트 데이터를 분석하고 이해하는 데에도 자연어 처리가 응용된다. 회사가 고객과의 상담 내용을 텍스트 데이터로 변환한 다음 자연어 처리하여 고객의 만족 및 불만족 사유를 분석하는 것도 한 예시로 볼 수 있다. ​자연어 처리에는 텍스트 인식, 음성 인식 등이 포함된다. 음성 인식(speech recognition)의 경우 음성을 기호나 텍스트로 변환하는 작업을 거쳐 이해한다.  2. 자연어 처리 시 유의점 ​컴퓨터는 자연어를 그대로 이해하는 것이 아니라 단어 또는 문장을 기계가 이해할 수 있는 숫자의 나열인 벡터로 변환(임베딩)[2]한 것을 연산 및 처리한다. 즉, 컴퓨터는 말이나 글을 숫자로 구성된 벡터로 바꾸어 이해한다. 임베딩은 언어의 통계적 패턴을 반영하여 벡터에 단어/문장 간 관련도, 의미 및 문법적 정보 등을 함축하는 방법이다. 이때 자연어의 정보를 임베딩에 함축시키는 과정에서 정보 손실을 최소화해야 한다.​자연어 처리 분야는 이미지 분류 분야나 정형 데이터 처리 분야와 다르게, 데이터가 단어 간 순서 및 상호 정보가 반영된 시퀀셜(sequential) 데이터라는 점이 큰 특징이자 장벽으로 작용한다. 그 밖에도 자연어 처리에 있어서 유의해야 하는 데이터의 특징에는 다음과 같은 것이 있다. 먼저, 자연어에는 동음이의어, 다의어, 동형어 등이 존재하여 같은 형태의 단어라도 문맥에 따라 다양한 의미를 전달할 수 있다.​예를 들어, 한국어로 “배에서 배를 많이 먹었더니 배가 부르다”는 말을 영어로 전달하려고 한다고 하자. 이를 각 한국어 지원 기계 번역 서비스로 결과를 도출해 본 결과는 다음 표와 같다.​[표 1] 한국어-영어 번역 결과 Papago[3]I’m full because I ate a lot from my stomach.Kakao translator[4]I ate a lot of boats on the boat and my stomach is full.Bing translator[5]I ate a lot of pears on the boat, and the boat called.Google Translator[6]I ate a lot of pears on the boat and I am full. 결과를 보면, 실제 의미에 맞게 번역된 경우도 있고, 일부의 경우 표현이 누락되거나 다른 표현으로 오역하기도 하였다. 오역의 사례를 보면 단어의 중의적 의미 때문에 해석에 모호성이 생겼음을 알 수 있다. ​이 외에도 문장이 단편적이거나 문장 내에 정보가 부족해서 해석에 모호성이 생기기도 한다. 예를 들어 “대신 부탁했다.” 는 문장은 누가 누구에게 무엇을 부탁했다는 것인지에 대한 정보가 누락되어 있어 정확한 번역 결과가 나오지 않는다. 특히, 영어와 다르게 한국어는 주어를 생략해도 문법적으로 정상인 문장을 구성할 수 있다. 인간의 소통 과정에서는 주어가 생략되는 경우 문맥 정보를 활용하여 생략된 정보를 메꾸어 이해하지만 기계 번역에서는 문장의 정확한 의미를 파악하기가 어렵다.​나아가, 똑같은 현상에 대해서도 다양하게 표현할 수 있고, 각각의 표현에 미묘한 의미의 차가 존재하는 표현의 다양성에서 기인하는 문제가 있다. 예를 들어 이미지 데이터는 한 픽셀의 RGB 값이 바뀐다고 해도 이미지 전체의 시각적 의미에는 큰 변화가 없는 반면, 단어는 약간의 변화가 문장 전체의 의미를 완전히 다르게 할 수 있다. 같은 단어의 조합인데 어순에 따라 다른 의미를 전달하기도 한다. 이는 자연어 데이터를 정규화 하는 작업을 매우 어렵게 한다.  3. 한국어 자연어 처리의 유의점 ​언어학적 특성에 따라 고립어로 분류되는 영어, 중국어 등과 다르게 한국어는 교착어에 해당한다. 영어, 중국어 언어 체계는 어순에 따라 단어의 문법적 기능이 영향을 받아서 문장의 의미를 해석할 때 단어의 어순이 중요하게 여겨진다. 그러나 한국어는 어간에 접사가 붙어 단어의 의미와 문법적 기능이 구체화되고, 어순의 중요성이 상대적으로 낮다.​한국어를 자연어 처리할 때는 다음과 같은 한국어의 언어적 특징을 유의해서 처리할 필요가 있다. ​첫째, 하나의 어근에 접사가 붙어 다양한 형태의 단어가 파생된다. 이는 한국어의 형태소 분석을 어렵게 하는 원인이 된다. ​둘째, 접사에 따라 단어의 역할이 정해지므로 어순은 상대적으로 덜 중요하다. 동일한 의미의 문장을 다양한 어순으로 표현할 수 있고, 어순을 바꾸어도 문법적으로 정상인 문장 사례가 다양하게 존재한다. 예를 들어, “먹으러 가려고 밥을”, “밥을 먹으러 가려고”, “가려고 밥을 먹으러”, 모두 같은 의미의 문장이다.​나아가, 영어는 평서문과 의문문의 어순이 다르므로 문장의 형태로 이를 구분할 수 있다. 중국어의 경우도 의문문에만 사용되는 단어를 통해 평서문과 의문문을 구분할 수 있다. 그러나 한국어는 평서문과 의문문의 문장 형태가 같은 경우가 적지 않고, 물음표 부호가 없으면 이를 구분하기 어렵다. 예를 들어 “점심 먹었어”가 점심을 먹었다는 의미의 평서문인지, 점심을 먹었냐는 의문문인지 물음표가 없다면 알기 어려운 것이다. [7][8]  4. 자연어 처리 관련 개념 ​4.1. 어휘 분류 사전​단어의 중의성 문제를 개선하기 위해 사용할 수 있는 방법으로 어휘 분류 사전을 활용하는 방법이 있다. 일례로 레스크 알고리즘(Lesk Algorithm)이 있다.[9] 레스크 알고리즘은 Wordnet DB 등 사전(dictionary)을 기반으로 해서 단어를 학습한다. Wordnet DB는 영어 단어에 대해 단어 별로 가능한 의미를 정리하고 이들에 번호를 매겨놓은 DB이다. 동의어 집합 또한 제공한다. 이 데이터를 지도학습(supervised learning) 하여 사용한다. Wordnet은 Python NLTK 모듈에 포함되어 임포트하여 사용할 수 있다.​한국어 단어에 대해서는 KorLex, Korean Wordnet(KWN) 등을 참고할 수 있다.​한편 자연어 처리에 순환 신경망(RNN) 모델, 딥러닝(Deep Learning) 기법 등이 활용되기 시작하면서 단어의 모호성 처리 문제는 많이 개선되고 있다. ​​4.2. TF-IDF​단어를 벡터로 변환할 때, 단어의 특징을 효과적으로 추출하고 특징 별 수치를 벡터로 표현하는 방법을 ‘특징 벡터’ 표현법이라고 한다. 단어에서 특징을 추출하는 데는 TF-IDF (Term Frequency-Inverse Document Frequency) 값을 이용한다. TF-IDF 값은 출연 빈도를 사용하여 특정 단어가 특정 문서 내에서 얼마나 중요한지를 나타내는 수치이다. 이 값이 높을수록 해당 단어는 해당 문서를 잘 대표하는 특징적인 성질을 띤다고 해석할 수 있다. TF-IDF는 TF값과 IDF값을 곱한 값이다. TF는 단어가 문서 내에 출연한 빈도를 계산한다. IDF는 특정 단어가 출연한 문서의 수를 계산하여 대다수의 문서에서 자주 출연하는 부사나 정관사와 같은 단어에 대해 역수를 취하여 페널티를 부여한다.   5. 자연어 처리 방법  자연어 처리 임베딩 방법은 단어 수준 임베딩과 문장 수준 임베딩으로 구분된다. ​​5.1. 단어 수준 임베딩; 문맥 독립 임베딩​Word2vec, GloVe, FastText 등은 단어 수준 처리 방법에 해당한다. 단어 수준 임베딩은 벡터에 단어의 문맥적 의미를 함축한다. 하지만 단어의 형태가 같을 경우 동음이의어를 분간하기 어렵고, 언어의 모호성이나 유의성을 처리하기 어려운 한계가 있었다. 이와 같은 같은 모델을 문맥을 고려하지 않는 문맥 독립(context-free) 임베딩 모델이라고도 한다. ​형태소를 음절로 분할하여 키워드의 빈도를 도출하는 과제 등은 단어 수준 임베딩에서 할 수 있는 자연어 처리 과제의 일례이다.​​5.1.1. Word2vec​Word2vec[10]은 Mikolov 등이 2013년 논문에서 제안한 임베딩 방법으로, 함께 쓰이는 단어들이 유사할수록 비슷한 벡터 값을 갖을 것이라고 가정하였다. 특정 단어를 기준으로 주변 단어들을 사용하여 단어를 임베딩 한다. 구체적인 임베딩 방법은 크게 CBOW(Continuous Bag of Words)와 Skip-gram 두가지 방식이 있다. ​CBOW 방법은 주변에 나타나는 단어를 벡터로 입력 받아 해당 단어를 예측한다. Skip-gram은 대상 단어를 벡터로 입력 받아 주변에 나타나는 단어를 예측하는 네트워크를 구성하여 단어 임베딩 벡터를 학습한다. 이 가운데 Skip-gram이 좀 더 널리 쓰이고 있다.​[그림 1] Skip-gram 알고리즘  *Mikolov et al. (2013)에서 재인용​5.1.2. GloVe​또다른 단어 수준 임베딩 방법에는 GloVe (Global Vectors for Word Representation)[11]가 있다. GloVe는 코퍼스(corpus) 문서에서 특정 단어와 함께 사용된 단어의 사용 또는 출현 빈도를 회귀 방법을 통해 예측한다.Word2vec의 skip-gram과의 차이점은, skip-gram은 코퍼스(corpus) 문서 내에서 주변 단어를 예측하는 반면, GloVe는 코퍼스(corpus) 문서에서 각 단어마다 동시 출현 빈도를 분석하고 빈도 행렬을 생성한다. 그리고 이 행렬을 활용하여 동시 출현 빈도를 예측한다. 이러한 방법으로 GloVe는 대상 단어와 주변 단어에 대한 학습 과정을 반복하는 skip-gram보다 학습 속도가 빠르다. ​​5.2. 문장 수준 임베딩; 문맥 기반 임베딩​한편 단어 수준 임베딩은 순서 정보를 담고 있는 시퀀스 데이터를 다루기에 적합하지 않았다. 자연어는 단어/문장의 순서 및 단어/문장 간 상호 정보를 고려해야 하는 시퀀스 데이터이므로 순서 정보를 사용하는 시퀀셜 모델링(sequential modelling) 이 고안되었다. 이는 문장 수준 임베딩, 문맥 기반(context-based) 임베딩 모델이라고도 불린다.문장 수준 임베딩 기법을 통해 단어의 시퀀스 정보를 함축하고, 동음이의어를 문맥에 따라 분리하는 것이 좀더 용이해지고 자연어 처리의 성능이 더욱 향상되었다. 문장 수준 임베딩 방법으로 처리할 수 있는 과제의 일례로는 기계 번역, 감성 분석(sentiment analysis) 등이 있다.​​5.2.1. 순환 신경망 (RNN; Recurrent Neural Network)​대표적으로 순환 신경망 (RNN) 아키텍쳐가 시퀀셜 모델링에 활용된다.기존 신경망(NN; Neural Network)과 순환 신경망(RNN; Recurrent Neural Network)의 차이는 다음과 같다. 기존 신경망 구조는 정해진 입력 X를 받아 Y를 출력하는 구조였다.​[그림 2] 기존 신경망 아키텍처  *김기현 (2019)에서 재인용하지만 RNN은 입력 X와 직전의 은닉상태(hidden state)를 참조하여 현재의 상태를 결정하는 작업을 재귀적으로 여러 time-step에 걸쳐 수행한다. RNN은 기존 신경망과 달리 이전 time-step의 자기 자신을 참조하여 현재의 상태를 결정하기 때문에 각 단계마다 네트워크 가중치 파라미터가 공유되었다. 각 time-step별 은닉상태는 출력 값이 될 수 있다. RNN을 하나의 은닉층이라고 보고 RNN 층이 사용된 모형을 RNN 모형이라고 하기도 한다.​[그림 3] RNN 아키텍처 *김기현 (2019)에서 재인용RNN 기반 모형에는 seq2seq[12] 이 있다. seq2seq는 인코더-디코더(-생성자) 구조를 하고 있으며, 앞서 배치된 RNN이 인코더 역할을 하고, 그 다음 RNN이 디코더 역할을 한다.​[그림 4] Seq2seq 알고리즘  *Sutskever et al. (2014)에서 재인용** ABC는 입력된 문장이고, WXYZ는 출력된 문장이다.​그러나 RNN 방식은 time-step이 길어지면 기울기 소실 등의 문제가 잘 발생하고, 은닉 상태를 통해 과거의 정보를 저장할 때 문장의 길이가 길어지면 앞의 과거 정보가 마지막 시점까지 전달되지 못하는 문제가 있었다.​이러한 문제를 장기 의존성 문제(long-term dependency)라고도 부른다. 이에 RNN은 긴 시퀀스 데이터를 효과적으로 처리하지 못하는 한계가 있었다. ​​5.2.2. LSTM (Long Short-Term Memory)​장기 의존성 문제에 대처하기 위해 기존 RNN 모형을 일부 수정한 LSTM[13]이 고안되었다. LSTM은 은닉상태 외에 셀 스테이트(cell state)라는 변수를 추가로 이용했다. 그리고 여러 게이트를 열고 닫아 정보의 흐름을 조절하여 데이터를 더 오래 기억하고 더 긴 길이의 데이터도 좀 더 효과적으로 처리할 수 있게 하였다.​그러나 파라미터가 많아진 만큼 LSTM의 구조는 더 복잡해졌고, 훈련시간이 증가했다. 그리고 LSTM도 문장의 길이가 길어질수록 장기 의존성 문제(long-term dependency)가 나타나는 한계를 여전히 가지고 있었다. ​LSTM과 유사한 구조의 모델로 GRU(Gated Recurrent Unit)이 있다.​​5.2.3. Transformer​RNN, LSTM, GRU 네트워크의 장기 의존성 문제를 극복하기 위해 Transformer[14]라는 딥러닝 아키텍처가 고안되었다. Transformer는 현재 자연어 처리 과제에서 가장 최신 기술로서 사용된다. Transformer가 등장한 뒤로 자연어 처리 분야에서 응용되던 RNN, LSTM 네트워크 등은 Transformer로 빠르게 대체되었다. Transformer 네트워크를 적용한 자연어 처리 모델에는 최근 자연어 처리 성능을 크게 향상시킨 BERT(Bidirectional Encoder Representations from Transformers)[15], GPT (Generative Pre-trained Transformer), T5 등이 포함된다.​Transformer는 2017년 Google에서 제안한 Attention 기반의 인코더-디코더 모형 알고리즘이다. RNN 네트워크에서 사용한 순환 방식을 사용하지 않고 Attention방법을 사용한다. ​Attention은 RNN 기반 모델이 갖는 장기 의존성 문제을 보완하기 위해 고안되었다. RNN기반 모델, seq2seq의 주요한 문제점은 입력된 시퀀스 데이터에 대해 마지막 은닉상태(hidden state)만을 디코더에 전달한다는 점이었다. 이 때문에 입력된 모든 단어의 정보가 디코더에 제대로 전달되지 못하고, 입력된 단어가 많을수록 앞쪽에서 입력된 단어는 거의 전달이 되지 않았다.[16]​하지만 Attention 기법은 각 단어에 대한 은닉상태 정보를 모두 디코더로 전달한다. 이전 단어들의 정보를 기반으로 다음 단어를 예측하는데, 예측하고자 하는 단어와 관련이 높은 단어에 더 많은 주의(attention)를 기울여 가중치를 부여한다고 하여 Attention 기법이라는 이름이 붙게 되었다.​Transformer모델은 N개의 인코더가 쌓인 형태를 한다. 가장 마지막에 있는 인코더의 결과값이 디코더에 전달된다. Attention 기법을 소개한 Vaswani et al. 2017 논문에서 인코더는 N=6개였지만, N은 다양한 값으로 지정할 수 있다.​[그림 5] Attention 기반 Transformer의 인코더-디코더 구조 * Vaswani et al. (2017)에서 재인용 [참고 학습 자료]– The Stanford Natural Language Processing Group, https://nlp.stanford.edu/– Stanford University, CS224N: Natural Language Processing with Deep Learning, Winter 2019, Lecture 1-22, on Youtube, https://www.youtube.com/playlist?list=PLoROMvodv4rOhcuXMZkNm7j3fVwBBY42z– Kyunghyun Cho, https://sites.google.com/site/deepernn/home/blog/lecturenotebriefintroductiontomachinelearningwithoutdeeplearning– Kyunghyun Cho, https://github.com/nyu-dl/Intro_to_ML_Lecture_Note  [참고 문헌][1] Thomas Erl, Wajid Khattak and Paul Buhler 공저, 조성준 외 역, 빅데이터 기초: 개념, 동인, 기법, 시그마프레스, 2017, 201-202.[2] Yoshua Bengio, Réjean Ducharme, Pascal Vincent and Christian Jauvin, A Neural Probabilistic Language Model, 2003, https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf[3] https://papago.naver.com/[4] https://translate.kakao.com/[5] bing.com/translator[6] translate.google.com[7] 김기현, 김기현의 자연어처리 딥러닝 캠프: 파이토치 편, 한빛미디어, 2019[8]이기창 저, NAVER Chatbot Model 감수, 한국어 임베딩: 자연어 처리 모델의 성능을 높이는 핵심 비결, 에이콘출판, 2019[9] M. Lesk, Automatic sense disambiguation using machine readable dictionaries: how to tell a pine cone from an ice cream cone. In SIGDOC ’86: Proceedings of the 5th annual international conference on Systems documentation, 1986, 24-26,http://portal.acm.org/citation.cfm?id=318728&dl=GUIDE,ACM&coll=GUIDE&CFID=103485667&CFTOKEN=64768709; Satanjeev Banerjee and Ted Pedersen, An Adapted Lesk Algorithm for Word Sense Disambiguation Using WordNet, Lecture Notes in Computer Science, 2002, Vol. 2276, 136 – 145, https://www.cs.cmu.edu/~banerjee/Publications/cicling2002.ps.gz[10] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado and Jeffrey Dean, Distributed Representations of Words and Phrases and their Compositionality, 2013, https://arxiv.org/abs/1310.4546[11] J. Pennington, R. Socher and C. Manning, GloVe: Global Vectors for Word Representation, Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), 2014, 1532-1543, https://doi.org/10.3115/v1/D14-1162[12] Ilya Sutskever, Oriol Vinyals and Quoc V. Le, Sequence to Sequence Learning with Neural Networks, 2014, https://arxiv.org/abs/1409.3215; D. Bahdanau, K. Cho and Y. Bengio, Neural Machine Translation by Jointly Learning to Align and Translate, ICLR, 2015, https://arxiv.org/abs/1409.0473[13] Sepp Hochreiter and Jürgen Schmidhuber, Long Short-Term Memory, Neural Computation 9 (8): 1735–1780, doi:10.1162/neco.1997.9.8.1735, https://direct.mit.edu/neco/article-abstract/9/8/1735/6109/Long-Short-Term-Memory?redirectedFrom=fulltext[14] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin, Attention is all you need, Advances in neural information processing systems, 2017, 5998-6008, https://arxiv.org/abs/1706.03762[15] Jacob Devlin, Ming-Wei Chang, Kenton Lee and Kristina Toutanova, BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding, 2018, https://arxiv.org/abs/1810.04805[16] Sudharsan Ravichandiran, 전희원 외 옮김, 구글 BERT의 정석 – 인공지능, 자연어 처리를 위한 BERT의 모든 것(Getting Started with Google Bert), 한빛미디어, 2021 "
Artificial intelligence and machine learning ,https://blog.naver.com/yoono044/223033798632,20230303,"Artificial Intelligence and Machine Learning: A Comprehensive OverviewArtificial intelligence (AI) and machine learning (ML) are two technologies that are changing the world as we know it. They have the ability to automate tasks, make predictions, and process data in ways that were previously impossible. In this article, we will explore the concepts of AI and ML, their applications, advantages, disadvantages, and the future of these technologies. 1. IntroductionArtificial intelligence and machine learning are terms that are often used interchangeably, but they are not the same thing. AI refers to the ability of machines to perform tasks that normally require human intelligence, such as speech recognition, decision making, and problem-solving. Machine learning, on the other hand, is a subset of AI that involves the use of algorithms to enable machines to learn from data and improve their performance over time.​2. What is Artificial Intelligence?Artificial intelligence is the science of creating machines that can perform tasks that normally require human intelligence. There are three types of AI:​Narrow or Weak AI: This type of AI is designed to perform a specific task, such as image recognition or speech recognition.​General or Strong AI: This type of AI has the ability to perform any intellectual task that a human can do.​Super AI: This type of AI is hypothetical and refers to an AI that has surpassed human intelligence and can make decisions and solve problems that humans cannot.​3. What is Machine Learning?Machine learning is a subset of AI that involves the use of algorithms to enable machines to learn from data and improve their performance over time. There are three types of machine learning:​Supervised Learning: This type of machine learning involves the use of labeled data to train a machine learning model to make predictions or classifications.​Unsupervised Learning: This type of machine learning involves the use of unlabeled data to train a machine learning model to identify patterns and relationships in the data.​Reinforcement Learning: This type of machine learning involves the use of a reward system to train a machine learning model to make decisions and take actions.​4. Applications of AI and Machine LearningArtificial intelligence and machine learning have a wide range of applications across various industries. Here are some examples:​4.1 HealthcareAI and machine learning are being used to improve the accuracy of medical diagnoses and personalize treatment plans. They are also being used to analyze medical data and predict the likelihood of certain health outcomes.​4.2 TransportationAI and machine learning are being used to improve the safety and efficiency of transportation systems. They are being used to predict traffic patterns and optimize routes for vehicles.​4.3 FinanceAI and machine learning are being used to improve fraud detection and financial forecasting. They are also being applied in the development of automated financial advisors and trading algorithms.​4.4 Education​AI and machine learning are being used to personalize learning experiences for students and improve educational outcomes. They are being used to create adaptive learning systems that can adjust to the individual needs and abilities of students.​4.5 Entertainment​AI and machine learning are being used to create personalized content recommendations and improve the user experience of entertainment platforms. They are also being used in the development of virtual reality and augmented reality technologies.​5. Advantages of AI and Machine Learning​Artificial intelligence and machine learning offer several advantages, including:​5.1 Increased Efficiency​AI and machine learning can automate tasks, freeing up time for humans to focus on more complex and creative work.​5.2 Improved Accuracy​AI and machine learning can process large amounts of data with great accuracy, reducing the risk of errors and improving the quality of decision-making.​5.3 Cost Reduction​AI and machine learning can reduce costs by automating tasks that would otherwise require human labor.​5.4 Personalization​AI and machine learning can personalize experiences for users, improving customer satisfaction and loyalty.​6. Disadvantages of AI and Machine Learning​Artificial intelligence and machine learning also have some disadvantages, including:​6.1 Job Displacement​AI and machine learning can automate tasks that were previously performed by humans, leading to job displacement.​6.2 Privacy Concerns​AI and machine learning can collect and process vast amounts of personal data, raising concerns about privacy and security.​6.3 Bias​AI and machine learning can perpetuate biases that exist in the data that they are trained on, leading to unfair and discriminatory outcomes.​7. Future of AI and Machine Learning​The future of AI and machine learning is exciting and full of possibilities. Here are some trends that are likely to shape the future of these technologies:​7.1 AI Ethics​As AI becomes more advanced and more integrated into our daily lives, there will be a growing need to ensure that these technologies are used ethically and responsibly.​7.2 Integration with Other Technologies​AI and machine learning will continue to be integrated with other technologies, such as the Internet of Things and blockchain, to create new and innovative applications.​7.3 Advancements in Robotics​Advancements in robotics will enable machines to perform tasks that were previously impossible, further blurring the line between humans and machines.​8. Conclusion​Artificial intelligence and machine learning are two technologies that are changing the world as we know it. They offer a range of advantages, including increased efficiency, improved accuracy, cost reduction, and personalization. However, they also have some disadvantages, such as job displacement, privacy concerns, and bias. The future of AI and machine learning is exciting, and it will be important to ensure that these technologies are used ethically and responsibly.​9. FAQsWhat is the difference between AI and machine learning?What are some examples of AI and machine learning applications?How can AI and machine learning improve healthcare?What are some of the ethical considerations when it comes to AI and machine learning?What is the future of AI and machine learning? "
윈도우 가상키보드 컴퓨터 키보드 안될 때 해결법 화상키보드 이용하기 ,https://blog.naver.com/economicfree11/222748989584,20220527,"​​​​​​ 컴퓨터 키보드가 되지 않을 때급한 불부터 끄기 위한 방법​​​아이티 정보와 전자기기를 알려드리는 어른아이입니다. 아마 이 글을 검색해서 들어오신 분들이라면 컴퓨터 키보드가 되지 않아서 모바일로 검색해서 접속하신 분들 중 한명일거라고 생각합니다.​​​오늘은 컴퓨터 키보드가 안될 때 해결법으로 가장 빠르고 간단하게 해결할 수 있는 윈도우 가상키보드 이용 방법에 대해서 알아보겠습니다​  ​ 컴퓨터 키보드 안될 때화상키보드우선 시작(windows)키를 눌러주세요. 키보드가 되지 않기 때문에 마우스로 좌측 하단에 있는 윈도우키를 클릭해주면 됩니다. 이후 스크롤을 주루룩 내려주세요.​ ​내리다보면 알파벳 W로 되어있는 칸에 보이는 Windows 접근성 폴더를 클릭해주세요. ​​ ​접근성에는 Speech Recognition과 내레이터, 돋보기 등등 다양한 접근성을 도와주는 프로그램이 보입니다. 여기서 반가운 녀석이 등장합니다. 윈도우 가상키보드인데요.​​가상 키보드란 말 그대로 윈도우 화면 상에 키보드를 띄어주는 프로그램입니다. 바탕화면의 일부를 키보드로 활용할 수 있는것이죠. 따라서 키보드가 되지않을 때, 임시방편으로 마우스 클릭을 통해 입력을 할 수 있는 것입니다. 키보드 안될 때 해결법으로 가장 유명하면서도 빠르게 대체할 수 있는 프로그램으로, 사실 완전히 키보드 오류를 해결하지는 못하지만, 시간이 없고 급한 분들에게는 좋은 프로그램입니다.​​위 화상 키보드 사진을 보면 일반 키보드와 굉장히 유사하게 생긴 것을 확인할 수 있습니다. 화상키보드 크기를 잘 조절해서 이용하시길 바랍니다​​ 만약 내가 화상키보드를 자주 이용해야할 것 같다 싶으면 바탕화면 공백에 우클릭을 한뒤에 나타나는 터치 키보드 단추 표시(Y) 키를 눌러주세요.​ 그러면 작업표시줄 우측에 이렇게 키보드 아이콘이 나타납니다. 이걸 더블클릭 해주면 ​ ​이렇게 화상키보드가 뿅 하고 나타납니다. 키보드 안될 때 해결법으로 임시방편 제대로 해결할 수 있는 방법입니다. 윈도우가상키보드는 저도 종종 사용하는 방법인데 이게 단순히 키보드 오작동 뿐만 아니라 화면을 여러군데에서 사용할 때에도 괜찮더라고요​​​​​​​​  오늘은 컴퓨터 키보드 안될 때 해결 방법 중 하나로 화상키보드 이용방법을 알아보았는데요. 요즘 모바일이 워낙에 환경이 잘 마련되어있어서 모바일로 해결하는게 더 빠를 수도 있지만,​​ PC 안에서 반드시 일을 급하게 처리해야 한다면 윈도우 환경속에 있는 가상. 화상키보드가 가장 빠른 방법이라고 생각합니다. 도움이 되셨기를 바라면서 이번 포스팅을 마치겠습니다. 감사합니다.​​​  ​​​​​​ "
[논문 리뷰| NLP] squence to squence ,https://blog.naver.com/ongbbb/222867343003,20220905,"​   전통적 rnn 한계입력, 출력 길이 다를 수 있음한국어는  s o v 순이지만 영어는 s v o 순으로 위처럼 각 단어를 바로 예측하는 방식으로 가면 네가를 input으로 받았을 때 그다음 miss 를 예측하기엔 어려움, 즉 그리워까지 봐야만 miss를 잘 예측할 수 있음​> idea) 전체 문맥을 아우르는 context vector를 뽑은 다음에 decoding을 해보자!​​ ​encoder 내부에는 LSTM, embedding, fc 등이 포함될 수 있는데 encoder decoder 의 architecture가 비슷할 수 있지만 다른 파라미터를 가짐​​ ​임베딩: one-hot 인코딩한 벡터의 차원이 너무 클 수 있음 > 임베딩 레이어를 통해 차원을 축소한 뒤에 디커더를 거쳐 hidden state (s) h4: 맨 마지막 단어까지 본 hidden state이므로 이를 context vector로 사용 ​​​ 바뀐 순서로 했을 때 성능이 더 좋았던 이유 context vector는 맨 마지막 hidden state를 사용하기 때문에 앞쪽에 나온 단어의 정보보다는 뒤쪽 단어의 정보를 주로 담고 있음이처럼 순서를 바꾸어 주게되면 input 문장의 앞쪽 단어의 문맥이 context vector에 더 잘 담게 됨*수학적으로는 순서를 바꾸든 바꾸지 않던 동일하게 작동되는 것이 맞지만, 이렇게 순서를 바꾸었을 때 더 잘 학습돼 모델이 효율적으로 학습할 수 있고, 그 결과 정확도까지 좋아졌음을 경험적으로 증명  LSTM 뿐 아니라 통계적 방법인 SMT을 함께 사용해봄(통계방법 > 딥러닝으로 넘어가는 과도기라 이렇게 두개를 같이 사용한 듯)​BLEU score: 기계번역의 성능 측정 metric​문제 제기​- DNN의 경우 input, output 의 차원이 고정되었는데 이는 speech recognition이나 기계번역에서 적용하기는 어렵다는 한계>> LSTM 만으로 할 수 있다. 인코딩을 통해 고정된 context vector를 뽑아낸 후, 그 뒤에서 LSTM에 이를 넘겨주는 것을 통해 좋은 성능 나올 수 있다 EOS(end of sequence)를 만나면 그때까지의 hidden state들을 context vector로 전달(파랑) ​​​​이미지 출처 및 참고 영상​https://www.youtube.com/watch?v=4DzKM0vgG1Y ​ "
KDT 71일차 인공지능 ,https://blog.naver.com/ycs318/223064541058,20230404,오늘의 진도 : Chatbot  Chatbot 을 외부에 연동할 때 [ 챗봇 설정 ] -> [ 메신저 연동 ] -> [ Custom ]​외부 연동 샘플 파일 경로 : CLOVA Chatbot Custom API (ncloud-docs.com)샘플 파일 내 android 패키지 파일들은 https://mvnrepository.com/artifact/net.morimekta.utils/android-util 여기서 받아야함  리액트에서 음성을 문자열로 바꿔주는 라이브러리 - npm i --force react-speech-kit또는 - npm i --force react-speech-recognition 
"일곱번째 학기를 떠나보내며, ",https://blog.naver.com/khanu98/222978142261,20230108,"2022 Fall Semester: Sep 1, 2022 ~ Dec 24, 2022Retrospect and Reflection on the 16 weeks that I studied most sincerely ever.  총 6개의 수업을 들었다. 경영학과 전공 4과목, 정치외교학과 전공 2과목.​김지현 교수님, ""Strategic Management (전략경영; 영강)""김승현 교수님, ""Management Information System (경영정보시스템; 영강)""박승재·정승환 교수님, ""공급사슬애널리틱스(Supply Chain Analytics)""이정원 교수님, ""비즈니스프로그래밍(Business Programming)""김명섭 교수님, ""한국국제정치사(韓國國際政治史; Korea In International History)""김성호 교수님, ""미국정치사상(American Political Thoughts)"" 공급사슬애널리틱스(Supply Chain Analytics)12월 9일 기말고사와 함께 종강 전반부 8주는 박승재 교수님께서 회귀분석과 시계열분석에 대해 강의해 주셨다. 2번의 과제와 1번의 팀 프로젝트가 있었다. 팀플은 회귀분석이나 시계열분석 기법을 실생활에 적용해 인사이트를 도출하는 게 주제였다. 우리 팀은 ""한강공원 주차장 이용 대수"" 공공데이터를 종속변수로, 기상 데이터, 사회적 거리두기 등을 독립변수로 두고 회귀분석을 진행했다. 발표는 10월 21일에 있었고, 내가 팀 대표로 발표를 맡았다.  후반부 8주는 정승환 교수님께서 파이썬의 여러 패키지들을 가르쳐주셨다. 주로 pandas와 pandas SQL을 다뤘고, 시각화로는 matplotlib.pyplot을 다뤘다. 2번의 과제와 기말고사가 있었다. 기말고사에는 1장의 치트시트가 허용됐는데, 여기에 내용을 깨알같이 손으로 써서 가져가야 했다. 시험은 코드가 주어지면 결과를 쓰고, 결과가 주어지면 그 결과를 내는 코드를 쓰는 식으로 나왔다. 비즈니스프로그래밍(Business Programming)12월 11일 기말과제 제출로 종강비프밍. 중간고사 기간까지는 R을 배우고, 기말고사 기간까지는 python을 배웠다. 일반표현식이나 numpy 등도 꽤 깊게 다루어 주셔서 재밌었다. 총 8번의 개인과제, 1번의 중간고사 대체과제, 1번의 기말고사 대체과제가 있었다. 파이썬 파트부터는 공급사슬애널리틱스랑 겹치는 내용들이 꽤 있어서 공부하기에 수월했다. 전략경영(Strategic Management)12월 15일 기말고사와 함께 종강 전략경영. 전적대에서 들은 적이 있는데, 전필이라고 인정을 못 받아서 또 들었다. 근데 그때 들었던 전략경영과 전혀 다른 내용이었다. 몇몇 주요한 논문들을 중심으로 강의가 진행됐다. 총 6개의 챕터(Lecture)가 있었다. L1은 Overview였다. L2와 L3는 Competition, Value Creation, Value Capturing에 대해서 체계적으로 다뤘다. L2는 공급자1, 소비자1 이런 식으로 단순화된 상황을 전제하고 있었고, L3에서는 그러한 비현실적 제약(constraint)들을 하나씩 풀어가며 경쟁구도를 분석했다. 중간에 Wintel Simulation 게임도 있었다. L4는 Value Network와 Porter's Five Forces를 다뤘다. 포터의 5요인 이론이 가지는 한계를 극복하는 시도로 Ryall, M. D.이 제시했던 Value Network Map도 배울 수 있어서 좋았다. L5는 블록체인이었는데, 그 내용의 특수성 때문에 관련 분야에서 일하고 있는 김준우 선배의 특강으로 갈음했다. L6은 System of Interactions and Performance Landscapes에 관한 것으로, 처음에는 다소 생소했지만 마이클 포터(Michael Porter)의 논문과 니콜라이 시겔코프(Nicolaj Siggelkow)의 논문을 읽으면서 무슨 말인지 이해할 수 있었다. 개인적으로 L6이 하이라이트였던 수업이라고 느꼈다. 학기 내내 진행된 팀플이 있었는데, 우리 팀은 MATLAB으로 ""구성원의 alternation이 조직의 performance에 미치는 영향""을 분석했다. Work-in-progress 발표는 11월 초에 가졌고, Final Presentation은 12월 6일에 있었다. Jessie와 Milan이 발표를 맡았다. 우리 팀은 유일하게 computational model을 선택한 팀이라 교수님이 많이 도와주셨다. 중간고사는 없었고, 12월 15일에 기말고사가 있었다. 외에도 2번의 퀴즈가 있었고, Blank Street Coffee을 분석하는 개인과제도 있었다. 경영정보시스템(Management Information System)​​12월 19일 기말고사와 함께 종강경정시의 특성상 그렇게 어려운 내용은 없었다. 약간 경영학 원론 같은 수업이었다. 교수님도 너무 착하셔서 편안한 분위기 속에서 수업을 들을 수 있었다. 요약하면 착한 교수님과 착한 수업. 중간고사는 없었고, 팀 단위로 Five Forces & Value Chain을 분석하는 과제와 Net Neutrality에 대해 토론하는 과제가 있었다. 외에도 4개 주제 중 택1하여 진행하는 Written Assignment도 있었는데, 우리 팀은 Salesforce.com로 진행했다. 우리 팀은 NEW IT 프레젠테이션으로 ""speech recognition"" 기술을 다뤘다. 발표는 12월 14일이었고, 교수님 지침에 따라 팀원 모두가 발표에 참여했다. 나는 도입부와 음성인식 기술의 역사에 대해 간략히 다루었다. 기말고사는 12월 19일에 있었는데, omr 답안을 사용하는 점이 특이했다. 대학 시험에서 omr을 쓰는 경우는 처음이었다. 미국정치사상(American Political Thoughts)​12월 20일 기말고사와 함께 종강 가장 고군분투했던 수업. 미정사. 하지만 역시 킹성호 교수님. 목요일 1교시, 화요일 2-3교시에 수업이었다. 통학러의 좌절을 오랜만에 느끼게 해준 수업이다. 예전 같았으면 지각을 왕창 했었겠지만, 철이 들었는지 종강까지 지각 0에 결석 0이었다. 그렇지만 매주 써야 하는 쪽글은 버거웠다. 한 학기 동안 영화를 보는 시간이 3번 있었는데, 그나마 숨을 돌릴 수 있는 시간이었다. 수업은 크게 보면 다음과 같이 4개의 파트로 나눠진다. (1) Overview (개괄), (2) Federalist Paper (연방주의자 논집), (3) Democracy In America (미국의 민주주의), (4) American Constitutionalism (미국 헌정주의).이 수업은 ""민주주의가 어떻게 헌정주의와 같이갈 수 있는가?""라는 질문에서 출발한다. 즉, 인치(rule of people)인 민주주의는 법치(rule of law)인 헌정주의와 끊임없는 길항작용을 하지만, 그러한 긴장(tension)이야말로 건강한 민주주의를 가능케 한다는 것이다. 이어 John Jay, James Madison, Alexander Hamilton이 쓴 연방주의자 논집에서 15개 정도의 논설문을 읽었다. 기억에 남는 건 역시 자주 언급된 10번과 51번이다. 토크빌이 쓴 미국의 민주주의 또한 내용이 방대해서 일부 챕터는 수업에서 다루지 않았다. 가장 기억에 남는 것은 마지막에 등장하는 tutelary government에 대한 이야기였다. Griffin 교수가 쓴 American Constitutionalism은 미국 헌정주의에 대한 교과서라고 할 수 있다. 미국 헌정주의의 역사적 맥락과 여러 쟁점들을 포괄적으로 소개하고 있다. 이 텍스트는 챕터 1부터 챕터 6까지 다 다뤘다. 토크빌과 Griffin부터는 발표가 있었는데, 나는 석훈형님과 Griffin의 챕터 2를 맡아 학우들 앞에서 발표했다. 발표는 11월 15일이었다.중간고사는 5쪽의 페이퍼를 제출하는 것으로 대체됐고, 기말고사는 강의실에서 대면으로 실시됐다. 중간고사 대체 페이퍼는 ""Federalist Paper""와 ""Democracy In America"" 두 텍스트를 비교하며, ""1787년 당시의 제정 의도와 토크빌이 그려내는 1830년대 미국 헌정체제 사이의 연속성과 비연속성을 서술""하는 것이었다. 기말고사는 전범위였지만, 주로 Griffin 책에서 나왔다. 약술형 5문제로는 ""Amendment 13, 14, 15"", ""Marbury v. Madison (1804)"", ""US v. Carolene Products Co. (1938)"", ""Brown v. Board of Education (1954)"", ""Method of Reason""가 나왔다. 서술형 문제로는 ""매디슨이 말한 '파벌(faction)'과 토크빌이 말한 '자발적 공동체'의 공통점과 차이점의 비교분석""이 제시됐다. 한국국제정치사(韓國國際政治史; Korea In International History)12월 24일 기말 소론 제출로 종강 내가 제일 재밌게 들었던 한국국제정치사 수업은 매시간이 인식의 지평을 넓히는 시간이었다. 수많은 인물들, 수많은 사건들, 수많은 연결들, 그 속에서 온전히 복원되는 역사. 공부할 게 너무 많다는 생각이 들었다. 김명섭 교수님께 개인적으로 너무 감사드린다. 금공강을 포기하고 넣은 수업이었는데, 첫 시간부터 종강의 순간까지 단 한 번도 듣기를 후회한 적이 없었다. 중간고사는 없었고, 12월 16일에 기말고사가 있었다. 기말고사는 극악의 난이도였다. 도반과 함께 한 학기 동안 소논문을 작성하는 과제가 있었는데, 나는 윤제와 ""이승만과 쑨원의 반봉건 및 반제 운동""을 비교하는 연구를 진행했다.​  Result: 이번 학기, 진짜 열심히 했다! "
"미국, 엔비디아 중국 수출 중단 명령 ",https://blog.naver.com/200186166/222863657022,20220901,"​ 칩 디자이너인 엔비디아는 수요일에 미국 관리들이 인공지능 작업을 위한 자사의 최고 컴퓨팅 칩 중 두 개를 중국에 수출하는 것을 중단하라고 명령했다고 말하여, 이번 분기에 4억 달러의 매출을 차지했던 사업에 차질이 생겼다.​​엔비디아는 머신러닝 작업 속도를 높이기 위해 설계된 자사의 A100과 H100 칩에 영향을 미치는 이번 금지가 올해 초 발표한 주력 칩인 H100 개발 완료를 방해할 수 있다고 밝혔다. 그 공시는 시간외 거래에서 약 4% 하락하고 있다.​이 발표는 미국과 중국 사이의 기술 긴장이 크게 고조되고 있음을 시사한다. 엔비디와 경쟁사인 AMD와 같은 회사의 미국 칩이 없다면, 중국 조직들은 다른 많은 작업들 중에서 이미지 및 음성 인식을 사용하는 고급 컴퓨팅의 종류를 비용 효율적으로 수행할 수 없을 것이다.​AMD는 엔비디아의 것과 유사한 플래그십 인공지능 칩을 만들었으며 코멘트를 위한 요청에 즉시 응답하지 않았다.​ AMD 주가는 시간외 거래에서 2% 하락중이다.   엔비디아는 증권거래위원회에 제출한 서류에서 미국이 중국과 러시아에 대한 판매에서 엔비디아의 서버용 최고 성능 제품인 A100과 H100 집적회로에 대한 새로운 라이센스 요구된다고 밝혔다. 엔비디아의 이번 분기 전망에는 이번 조치로 영향을 받을 수 있는 중국에 대한 4억 달러의 데이터 센터 매출이 포함되어 있으며, 엔비디아는 현재 러시아에서 제품을 판매하지 않고 있다.​​엔비디아 대변인은 이메일로 보낸 성명에서 ""우리는 중국 내 고객들과 함께 대체 제품으로 계획되거나 향후 구매에 만족하기 위해 노력하고 있으며 교체품이 충분하지 않은 경우 라이센스를 요청할 수도 있다""고 말했다. ""새로운 라이센스 요구사항이 적용되는 현재 제품은 A100, H100 및 이를 포함하는 DGX와 같은 시스템뿐이다.​엔비디아 주가는 2.4% 하락한 150.94달러로 마감한 뒤 시간외 거래에서 4% 이상 하락했다. 올해 들어서는 핵심 게임칩 사업의 도전 속에 현재까지 주가가 48.7% 하락했고, S&P500지수 SPX는 16.4% 하락했다.​최근 몇 달 동안 재고가 쌓이면서 게임 카드 매출이 급감했지만 엔비디아의 사업은 데이터센터 매출에 의존해 왔다. 팩트셋에 따르면 데이터 센터 매출은 지난해 67억 달러에서 지난해 106억 달러로 증가했으며, 분석가들은 평균적으로 올해 서버 매출이 157억9000만 달러에 이를 것으로 예상하고 있다.​수요일의 뉴스는 엔비디아가 H100의 개발에 영향을 미칠 수 있으며 ""향후 Nvidia 통합 회로가 A100과 거의 동등한 임계값과 동일한 성능 및 칩 대 칩 I/O 성능을 모두 달성할 경우 데이터 센터 부문의 향후 비즈니스에 타격을 줄 수 있습니다.이러한 회로를 포함하는 모든 시스템""도 동일한 요구 사항에 직면하게 된다.​새로운 라이센스 요건은 회사의 H100 개발을 적시에 완료하거나 A100의 기존 고객을 지원할 수 있는 능력에 영향을 줄 수 있으며 회사가 특정 사업을 중국 밖으로 전환하도록 요구할 수 있습니다.""라고 SEC는 밝혔다. ""그 회사는 [미국 정부]와 계약을 맺고 있으며 회사의 내부 개발 및 지원 활동에 대한 면제를 요구하고 있습니다.""​엔비디아는 연방정부의 새로운 라이센스 요건은 ""적용된 제품이 중국과 러시아의 '군사적 최종 용도' 또는 '군사적 최종 사용자'에 사용되거나 전용될 수 있는 위험을 해결하기 위한 것""이라고 말했다. 미국은 수년 전부터 중국 모기업의 인수 제안 차단과 판매 제한 등 중국군의 고성능 반도체 기술 획득을 막기 위한 움직임을 보여 왔다.​엔비디아가 이번 결정에 가장 큰 영향을 받은 것으로 보이지만, 다른 미국 서버 칩 제조업체들도 수요일 시간외 거래에서 주가가 하락했다. 어드밴스트 마이크로디바이스 주식회사 AMD의 주가는 약 2% 하락했고 인텔 INTC의 주가는 약 0.3% 하락했다.​​https://finance.yahoo.com/news/1-u-officials-order-nvidia-214913361.html UPDATE 2-U.S. officials order Nvidia to halt sales of top AI chips to ChinaChip designer Nvidia Corp on Wednesday said that U.S. officials told it to stop exporting two top computing chips for artificial intelligence work to China, a move that could cripple Chinese firms' ability to carry out advanced work like image recognition and hamper a business that Nvidia expects to...finance.yahoo.com ​Aug 31 (Reuters) - Chip designer Nvidia Corp on Wednesday said that U.S. officials have ordered it to cease exporting two of its top computing chips for artificial intelligence work to China, hampering a business that accounted for $400 million in sales in the current quarter.​Nvidia said the ban, which affects its A100 and H100 chips designed to speed up machine learning tasks, could interfere with the completion of developing the H100, the company's flagship chip announced earlier this year. The disclosure sent share down about 3% in after-hours trading.​The announcement signals a major escalation of the technology tensions between the United States and China. Without American chips from companies like Nvidia and its rival Advanced Micro Devices, Chinese organizations will be unable to cost-effectively carry out the kind of advanced computing used image and speech recognition, among many other tasks.​AMD makes similar flagship artificial intelligence chips to Nvidia's and did not immediately respond to re3uqest for comment. AMD shares were down 2% in after hours trading. (Reporting by Eva Mathews in Bengaluru; Editing by Devika Syamnath and David Gregorio)  https://www.marketwatch.com/story/nvidia-stock-falls-after-u-s-moves-to-restrict-its-data-center-sales-in-china-11661983697?siteid=yhoof2&yptr=yahoo Nvidia stock falls after U.S. moves to restrict its data-center sales in ChinaNvidia Corp. shares fell in late trading Wednesday after the graphics-chip specialist disclosed that the U.S. government is seeking to restrict its...www.marketwatch.com ​Nvidia Stock Drops as U.S. Limits Exports to China. ‘Military End Use’ Is the Key​Nvidia Corp. shares fell in extended trading Wednesday after the graphics-chip specialist disclosed that the U.S. government is seeking to restrict its data-center business in China.In a filing with the Securities and Exchange Commission, Nvidia NVDA revealed that the U.S. has installed new license requirements for its A100 and forthcoming H100 integrated circuits — Nvidia’s highest-performance products for servers — in sales to China and Russia. The filing specifically states that Nvidia’s forecast for the current quarter includes an expected $400 million in data-center sales to China that could be affected by the move; Nvidia does not currently sell products in Russia.​“We are working with our customers in China to satisfy their planned or future purchases with alternative products and may seek licenses where replacements aren’t sufficient,” an Nvidia spokesperson said in an emailed statement. “The only current products that the new licensing requirement applies to are A100, H100 and systems such as DGX that include them​The new license requirement may impact the company’s ability to complete its development of H100 in a timely manner or support existing customers of A100 and may require the company to transition certain operations out of China,” the SEC filing reads. “The company is engaged with the [U.S. government] and is seeking exemptions for the company’s internal development and support activities.”Nvidia said that the federal government’s new license requirements are meant to “address the risk that the covered products may be used in, or diverted to, a ‘military end use’ or ‘military end user’ in China and Russia.” The U.S. has for years been making moves to prevent China’s military from obtaining high-performance semiconductor technology, including blocking proposed acquisitions by Chinese parent companies and restricting sales.Other U.S. makers of server chips also saw shares decline in after-hours trading Wednesday, though Nvidia appears to be the company most impacted by the decision. Advanced Micro Devices Inc. AMD shares declined about 2%, while Intel Corp. INTC shares fell about 0.3%.Nvidia shares fell more than 4% in after-hours trading after closing with a 2.4% decline at $150.94. The stock has declined 48.7% so far this year amid challenges in its core gaming-chip business, while the S&P 500 index SPX has fallen 16.4%.While sales of gaming cards have plunged in recent months after inventory built up, Nvidia’s business has relied on data-center sales. Data-center revenue grew to $10.6 billion last year from $6.7 billion the year before, and analysts on average expect server sales to reach $15.79 billion this year, according to FactSet.Wednesday’s news could hurt future business in the data-center segment, as Nvidia admitted in the filing that the development of the H100 could be affected and that “any future Nvidia integrated circuit achieving both peak performance and chip-to-chip I/O performance equal to or greater than thresholds that are roughly equivalent to the A100, as well as any system that includes those circuits” will face the same requirement.​​​​ "
청력테스트부터 보청기 구매~ 난청센터의 선택! ,https://blog.naver.com/surelee2/222005794836,20200619," ​​​​​보청기 구매 과정에서 난청센터에서 진행되는 청력테스트는 청각검사기기를 활용하여 피검자에게 청각신호를 주고 피검자의 반응을 통해 원하는 청각정보를 얻는 것을 말합니다. ​이 과정에서 피검자의 주관이나 의지에 의해 소리에 반응하는 청력테스트를 주관적 청력검사, 피검자의 의지와는 관계없이 중이나 청신경의 반응을 보는 객관적 청력검사로 나누어 보게 됩니다.​​​​​   ​​​​​난청센터에서 보청기 선택이나 구매에서는 보통 피검자의 의지에 의한 반응을 통해 청력정도를 확인하는 주관적인 청력테스트를 사용하게 됩니다.​​이러한 주관적 청력테스트에는 순음청력검사, 골전도청력검사, 어음청력검사 등이 있고  최근에는 소음하 어음청력검사의 중요성이 높아지고 있습니다.청력테스트는 보청기선택뿐만 아니라 조절이나 재활에서도 중요하며 이에 대한 사용자의 테스트과정에 대한 이해수준도 효과에 영향을 줄 수 있습니다. ​​​​먼저 청력검사에 대해 설명을 드리면....​​​​  ​​​ 순음청력검사순음청력검사는 피검자의 의사소통에 중요한 125 ~ 8,000 Hz 범위의 가청 주파수 범위에서 포괄적으로 청력역치를 측정하는 것을 말합니다. ​이러한 순음청력검사를 통해 대상자의 청력손실의 유무나 손실 정도, 종류, 유형, 형태 등을 파악하게 되며 향후 진행되는 보청기 선정이나 조절, 재활에서도 매우 중요한 자료로 사용되게 됩니다. ​​​​  ​​​ 골전도청력검사소리를 골전도로 전달하여 내이를 직접 자극하여 청력역치를 측정하는 것을 골전도청력검사라 말합니다. ​이러한 골전도청력검사는 기도순음청력검사와 비교하여 대상자의 난청 종류, 예를 들어 전음성, 혼합성, 감각신경성 등 청력손실이 일어난 병변의 위치를 파악하게 됩니다. 하지만 골전도검사는 오차범위가 기도검사에 비해 큰 단점이 있으므로 해석에 주의해야 합니다.​​​​​  ​​​ 어음청력검사말소리를 활용하여 청력테스트를 하게 되는 어음청력검사는 speech recognition threshold(SRT),  Word recognition score(WRS), Sentence recognition score(SRS) 등이 있습니다.​말소리를 활용하여 검사어음을 50% 이해할 수 있는 역치를 측정하고 편안하게 들을 수 있는 쾌적한 강도수준에서 친숙한 단어나 문장을 활용하여 피검자의 말소리 이해수준을 측정하게 됩니다.​피검자의 청각적 언어인지에 대한 부분을 알 수 있으므로 이 과정 후에 피검자와의 심도있는 상담이 필요합니다.​​​​  ​​ 중이 검사중이검사는 중이의 질병유무나 종류를 구분하고 청력정도를 평가하는 것에 도움을 줄 수 있습니다.​중이검사 중 고막운동도검사는 외이도에 삽입되는 프로브를 통해 압력을 조절하여 외이도의 압력변화에 따라 고막 및 중이내부의 반사되는 소리 에너지의 양을 측정하여 그래프로 표기하는 것을 말합니다. 보통 이 검사는 외이도에 프로브를 삽입하는 과정으로 자동측정이 이루어지므로 피검자의 주관적 반응은 필요하지 않습니다.​​​​​  ​​​​ 소음하 어음청력검사소음하 어음청력검사는 소음속에서 말소리를 듣고 이해할 수 있는 능력을 측정하는 것으로 이를 잘 수행한다면 보청기를 사용하여 시끄러운 곳에서 더 많이 들을 수 있습니다. ​현대인은 활발한 사회활동이 필요하며 주위의 시끄러운 소리 자극에 노출되어 있습니다. 난청인의 보청기 사용에서 소음의 노출은 필연적이라는 것입니다. 이로 인해 청각선진국에서는 난청센터의 보청기 과정에서 소음하 어음청력검사를 꼭 시행해야 하는 과정으로 보고 있습니다.​​​ 고정된 신호대잡음비(fixed SNR)에서의 소음하 검사고정된 소음에서 말소리를 들려주고 인지정도를 측정하는 것으로 고정된 SNR에서 문장을 제시하여 그 결과를 백분율로 점수화하게 됩니다. ​하지만 이 방법은 어음청력검사 중 문장인지도 검사목록을 중복해서 사용함으로 인해 피검자가 문장을 예측할 수 있고 난청인의 어음인지 수준에 따라 너무 쉽거나 어렵게 느낄 수 있습니다. 또한 제시된 SNR에 따라 검사가 진행되어 난청인의 소음하 말소리 인지정도나 수준을 정확히 알기 어렵습니다.​​​ 신호대잡음비를 조정하는(adaptive SNR) 변동형 소음하 검사피검자의 반응에 따라 제시되는 소음의 강도가 변환되어 측정하는 변동형 소음하 문장인지 검사는 fixed SNR 에서 검사 단점을 극복할 수 있습니다. ​지금까지 청각선진국에서는 활발하게 사용되어 왔으며 최근 우리나라 언어를 활용한 Adaptive SNR Matrix TEST가 개발되었습니다. 이러한 변동형 소음하 검사는 객관적으로 소음속에서 난청인의 어음인지 수준을 평가하는 것에 도움이 되며 수치화된 비교가 가능하고 보청기 피팅이나 재활에서도 폭넓게 사용이 가능합니다.​​​​​​  ​​ 청력도(Audiogram)청력검사 후 보게 되는 것이 바로 청력역치가 측정된 청력도(audiogram)입니다.방음실에서 실시된 피검자의 주파수별 청력역치를 청력도에 기록하게 되며 이러한 청력도는 소리의 고저나 강약을 한 눈에 파악할 수있게 합니다. ​청력도에서 가로 축은 소리의 고저를 주파수로 표시하며 세로 축은 소리의 강약을  dB HL 로 표기하게 됩니다.​ ​​​   ​​ ​전문 난청센터에서의 청력테스트와 조절, 재활은...​보청기에서 이러한 청력테스트가 중요한 이유는 바로 난청인의 청력정도에 맞추어 보청기를 선택하기 위한 기초자료가 되며 보청기 음향이득을 대상자에 맞추기 위한 조절, 대상자의 청각적 능력을 발휘할 수 있도록 하는 재활과정에서도 중요하게 사용된다는 점때문입니다. ​청력테스트를 통해 대상자는 자신의 난청정도나 상태 등에 대해 이해하게 되며 착용되는 보청기의 형태나 모델 등을 결정하는 것에서 청력테스트는 매우 중요한 가치를 가지게 됩니다.​보청기를 구매하고 조절하는 과정은 주로 전문 난청센터에서 이루어지게 됩니다. 센터에서의 전문성이 효과에 영향을 주는 주된 이유이기도 합니다. 조절의 영향이 보청기 효과의 가장 큰 부분이라고 할 수 있습니다. 이러한 전문성이 중요한 과정에서 청력테스트나 실이측정 등의 과정이 매우 중요한 가치를 가지게 됩니다. 이를 활용하여 전문가는 대상자의 이득을 결정하고 조절을 시행하게 됩니다.​보청기 사용과정에서 난청인은 적응이라는 과정을 거치게 됩니다. 재활은 이러한 난청인의 보청기 적응을 돕고 가진 청각능력을 발휘 할 수 있게 합니다. 언어전문가는 청각테스트와 평가 과정을 통해 목표설정과 프로그램을 진행을 시행할 수 있게 됩니다.​전문 난청센터의 청력테스트, 검사와 보청기선정, 조절 등의 차이는 바로 난청인이 느끼는 효과의 차이입니다.​​​​​​  ​​​ 보청기에서 청력테스트와 조절이 중요하다는 것을 알게 되었다는 000님.​20년 가까이 오랜 기간 동안 보청기를 사용해 왔지만 따님으로 부터 보청기를 착용하지 않는다는 불평을 듣고 계시다는 000님은 착용하면 불편하는 점때문에 지속적으로 사용을 꺼려 왔다는 말씀을 주셨습니다.저희 센터는 사용중인 보청기의 노후화로 인해 교체를 위해 방문하신 것이구요~ 따님께서는 000님이 보청기 사용에 불편함을 느낀다는 000님때문에 저희 종로오티콘 보청기를 찾아 방문하셨다고 하셨는데요.. ​​​검사가 진행되고 완성된 보청기를 착용하게 되는 과정속에서 000님은 ""여기는 다르게 느껴져.""라는 말씀을 따님께 하셨답니다. 평가와 검사, 그리고 청력테스트 등의 과정이 보청기구매를 위한 단순한 측정이 아니라 보청기를 000님께 정확하게 맞추기 위한 세밀한 과정이라는 것이 느껴졌다는 말씀이셨습니다.​000님은 최근 방문에서 지금은 보청기를 항상 착용중이라는 말씀을 해 주셨습니다. 20년 동안의 불편이 지난 몇 달간의 과정으로 바뀌게 된 것이지요~~ ​​​​​  ​​​​ 오티콘보청기 종로센터는 체계적인 청각과정을 진행하고 있습니다.​청력테스트가 청각전문가에 의해 매우 세심하게, 그리고 다양한 관점에서 시행되며 이를 통해 대상자에 적합한 보청기를 선택할 수 있도록 합니다. 특히 전문가가 실이측정, 비져블스피치매핑, 기능이득 평가 등의 과학적인 과정을 통해 보청기 조절을 시행하며 평가를 통해 세밀한 피팅이 이루어지도록 하게 됩니다.​전문가의 과정에 대한 인식정도가 낮은 우리나라와는 달리 청각선진국에서는 보청기과정에 청각전문가의 참여와 도움을 매우 중요하게 인식하고 있으며 이는 바로 보청기효과나 지속적인 사용에 깊은 연관이 있습니다.​과학적인 보청기과정~~ 오티콘보청기 종로점에 있습니다.​​​​​​ 오티콘보청기 종로센터서울특별시 종로구 종로 139  ​ "
How YouTube became a force for free speech in South Korea ,https://blog.naver.com/newsboxs/223095492077,20230507,"How YouTube became a force for free speech in South Korea 겸손은 힘들다 김어준 뉴스공장SeoulCNN — YouTube’s most watched daily live program in South Korea might surprise you. It is not about K-pop, it is not a K-drama, and has nothing to do with BlackPink.It is a provocative current affairs talk show called “Gyeomson (Modesty) is Nothing,” fronted by an irreverent host, Kim Ou-joon, whose lack of deference to authority is making waves in a country where traditional media has a reputation for respectful coverage.Kim’s style is reminiscent of a US late-night chat show host. Openly partisan, he says his aim is to counter-balance what he sees as a bias toward the conservative government with a liberal voice.​“Conservative media are actively making biased reports, and I think they can do that based on their political stance,” Kim told CNN. “The problem here is that they’re pretending to be fair, hiding behind the mask of fairness.”Kim’s brash style stands out all the more given recently raised concerns by the US Department of State that South Korean officials are using defamation lawsuits to restrict freedom of expression.It highlighted in a March human rights report the case of broadcaster MBC, which is being sued by the Foreign Affairs Ministry for a story in which it claimed the South Korean President Yoon Suk Yeol had been caught on a hot mic making less-than complimentary remarks about US lawmakers.​The presidential office has criticized the State Department report for “a lack of accuracy,” claiming it had “simply collected and announced the claims of civic groups or media reports.”​ 김어준 뉴스공장 겸손은 힘들자 진행자 김어준​But taking on the conservative administration doesn’t faze Kim, who has been sued for defamation on several occasions.Kim’s critics meanwhile say his taste for controversy goes only one way, accusing him of paying less attention to reports involving the liberal Democratic Party.​Political influencersStill, the show’s reputation for daring to go where others fear to tread has done wonders for its viewing figures. Every morning at 7:05 a.m., about 160,000 people tune in to hear Kim’s takes on the biggest issues of the day.​Industry observers say the program’s popularity – it has more than 1 million subscribers and can rake in donations of 90 million won ($70,000) a day – reflects a changing media landscape.​Increasingly, they say, current affairs programs are turning to YouTube to disseminate their content, drawn both by large audiences and the perception that the online arena grants greater space for freedom of expression.​“Modesty is Nothing,” for instance, is the reincarnation of a radio show called “News Factory” that was taken off air after a row with the government.​YouTube has a high penetration in South Korea. According to Statista, a market and consumer data statistics site, there were over 46 million YouTube users in South Korea as of January this year – more than 90% of the population (compared to over 70% in the United States).​Most traditional media outlets now run their own YouTube channels, as do an increasing number of smaller, independent companies, of all political persuasions – like the right-wing Tubeshin (1.46 million subscribers) and the leftwing Newstapa (over 1 million).​YouTube’s growing influence was demonstrated in the run-up to last year’s general election, when Yoon – then a candidate for the People Power Party – saw his popularity fall following a stumbling performance in a Christmas Day YouTube interview with 3 ProTV.​Before the show he had been neck-and-neck with opponent Lee Jae-myung from the (then ruling) Democratic Party of Korea; soon after a Gallup survey showed him trailing by around 8% points.​The clout of both right and left-wing channels has also been shown in recent movements. Right-wing channels fueled rallies in support of the former President Park Geun-hye after she was impeached in 2017 over a corruption scandal; they also backed protests outside the retirement home of her liberal successor Moon Jae-in. Left-wing channels backed counter-rallies outside the current President Yoon’s home.​Last year the former leader of Yoon’s party hit out at what he said was the “evil influence” of YouTube channels.​ Platform for free speechYouTube channels are seen as offering a space for free speech that is all the more important given the concerns voiced by the US State Department.Jung June-hee, a professor of media at South Korea’s Hanyang University ERICA campus, said most traditional outlets avoided criticizing the government – partly because of their own right-wing leanings but also because they feared being sued.“After President Yoon Suk Yeol came to power, there have been many cases where the presidential office filed complaints to the media,” Jung said.“The fear of being targeted, whether you’re on the same side or not, is significant,” Jung said.Over time, the lack of critical coverage meant citizens had lost confidence in traditional media and turned to the internet instead, said Rhee June-woong, a professor in communication at Seoul National University.“We can’t say that traditional newspapers and broadcast media have been completely abandoned, but more and more citizens are dissatisfied with them and are seeking information, interpretation, and expression in internet media,” Rhee said.CNN asked the presidential office to comment on its recent defamation lawsuits, but it has not yet received a reply.​ 김어준 뉴스공장 겸손은 힘들다News Factory closed downIt’s a dynamic that’s not lost on Kim. His previous show, the publicly funded “News Factory” program, had for years been Seoul’s top-rated radio program (and made him one of the country’s highest paid presenters).​Debuting in 2016 on TBS in a two-hour slot Monday to Friday, its format was simple, featuring Kim’s comments on the topic of the day and a news roundup, followed by segments featuring newsmakers from politicians and professors to journalists, artists and scientists.​Its no-holds barred approach to news analysis and its live interviews broke the mold in South Korean media and made it a go-to show for politicos, said Jung June-hee, a media professor at Hanyang University.​“(Previously) politicians didn’t appear on radio shows, and morning radio shows … (mostly) used to summarize news from the night before and deliver information such as real-time traffic updates,” Jung said.​But its oppositional style angered conservatives, as did its coverage of the scandal surrounding former President Park. When a conservative administration returned to power in 2022 (following a stint by liberal Moon Jae-in, under whom the show enjoyed a heyday), its days were numbered.​Soon after, the conservative city council announced it planned to cut TBS’s budget in a move widely seen as reflecting its displeasure with “News Factory.”​While Seoul’s Mayor Oh Se-hoon denied the link, in February he criticized the show for being “one-sided,” and the network TBS for “crossing the line that public broadcasting cannot possibly cross.”​“In any country, people can never be patient when public broadcasting is biased and in favor of a particular political party,” Oh said.​ 김어준의 뉴스공장 겸손은 힘들다 제작진Fall and riseKim took “News Factory” off air in December. The next month, he launched “Modesty is Nothing.”The only real differences between the shows are the name and medium. The format is the same and even the studio is a replica, though it has now grown bigger – in line with Kim’s ambitions.Within a week, it had surpassed a million subscribers. It has since consistently ranked top in terms of real-time daily viewership on YouTube in South Korea.To Professor Jung, it’s a success that demonstrates “voices cannot be silenced.”Kim, meanwhile, hopes to build a show with as much recognition as any on traditional media.“I will create a type of press that has not yet existed on YouTube,” Kim said. “This is a declaration that I will show that canceling the show for a political reason was wrong.”​출처: By Yoonjung Seo and Andrew Raine, CNN​​​[구글 번역이란 점을 양해 바라며 여러분들 모두가 알고 있다시피 해석이 제대로 되지 않은 부분이 있음을 이해하시기 바랍니다.]​YouTube가 한국에서 표현의 자유를 위한 힘이 되는 방법 2023년 3월 8일 겸손은힘들다 진행자 김어준과 유밀리기자가 오늘의 뉴스에 대해 이야기를 나누고 있다​서울CNN 한국에서 가장 많이 본 YouTube 일일 생방송 프로그램이 여러분을 놀라게 할 것입니다. K-pop에 관한 것도 아니고, K-drama도 아니며, BlackPink와는 아무런 관련이 없습니다.​전통 언론의 예의 바른 취재로 정평이 난 나라에서 권위에 대한 불복종으로 파문을 일으키고 있는 불손한 진행자 김어준의 진행으로 진행되는 도발적인 시사토크쇼 '김어준의 뉴스공장 겸손은힘들다'이다..​김 씨의 스타일은 미국 심야 토크쇼 진행자를 연상시킨다. 공개적으로 당파인 그는 자신의 목표가 진보적인 목소리로 보수 정부에 대한 편견으로 보는 균형을 맞추는 것이라고 말했습니다.​김 위원장은 CNN과의 인터뷰에서 “보수언론이 편향보도를 적극적으로 하고 있는데 정치적 입장에 따라 그렇게 할 수 있다고 생각한다”라고 말했다. ""여기서 문제는 그들이 공정한 척하고 공정함의 가면 뒤에 숨어 있다는 것입니다.""최근 미 국무부가 한국 관리들이 명예훼손 소송을 이용해 표현의 자유를 제한하고 있다는 우려를 제기한 점을 감안하면 김 씨의 성급한 태도는 더욱 두드러진다.​지난 3월 인권보고서에서 방송사 MBC가 윤석열 한국 대통령이 뜨거운 마이크를 받고 칭찬 이하의 발언을 했다며 외교부가 고소한 사례를 강조했다. 미국 국회의원에 대해 청와대는 국무부 보고서가 “시민단체나 언론보도의 주장만 모아 발표한 것”이라며 “정확성이 부족하다”라고 비판했다.​ 유튜브 쇼 '겸손은힘들다'의 진행자이자 크리에이터인 김어준이 서울에 있는 자신의 스튜디오에서 있다.그러나 명예훼손으로 여러 차례 고소를 당한 김정은은 보수 정권에 도전해도 당황하지 않는다.​한편 김 씨를 비판하는 사람들은 논란에 대한 그의 취향이 한 방향으로만 가고 있다고 말하며 그가 진보적인 민주당과 관련된 보도에 덜 관심을 보인다고 비난합니다.​정치적 인플루언서그럼에도 불구하고 다른 사람들이 밟기 두려워하는 곳으로 감히 가겠다는 쇼의 명성은 시청률에 놀라운 일을 했습니다. 매일 아침 7시 5분에 약 160,000명의 사람들이 오늘의 가장 큰 이슈에 대한 Kim의 견해를 듣기 위해 채널을 조정합니다.​업계 관계자들은 이 프로그램의 인기(구독자 수 100만 명 이상, 하루 9000만 원 기부금 모금 가능)가 변화하는 미디어 환경을 반영한다고 말한다.​그들은 많은 시청자와 온라인 무대가 표현의 자유를 위한 더 큰 공간을 부여한다는 인식에 이끌려 시사 프로그램이 콘텐츠를 보급하기 위해 점점 더 YouTube로 눈을 돌리고 있다고 말합니다.​예를 들어 '겸손은힘들다'는 정부와의 다툼 끝에 방송이 중단된 '뉴스팩토리'라는 라디오 프로그램의 환생이다.YouTube는 한국에서 높은 보급률을 자랑합니다. 시장 및 소비자 데이터 통계 사이트인 스태티스타(Statista)에 따르면 올해 1월 현재 한국의 유튜브 사용자는 4600만 명을 넘어섰다.​우파 Tubeshin (구독자 146만 명)과 좌파 Newstapa (구독자 100만 명 이상)와 같이 점점 더 많은 수의 소규모 독립 기업이 모든 정치적 신념을 갖고 있는 것처럼 대부분의 전통적인 미디어 매체는 이제 자체 YouTube 채널을 운영하고 있습니다.​유튜브의 성장하는 영향력은 지난해 총선을 앞두고 당시 국민의힘 후보였던 윤 후보가 3 ProTV와의 크리스마스 날 유튜브 인터뷰에서 부진한 성과로 인기가 떨어지는 것을 보았을 때 입증됐다.​쇼 전에 그는 (당시 집권) 민주당의 상대 이재명과 막상막하했다. Gallup 설문 조사에서 그가 약 8% 포인트 뒤처진 것으로 나타난 직후였습니다.​우익과 좌익 채널의 영향력은 최근 움직임에서도 나타났다. 우익 채널은 2017년 부패 스캔들로 탄핵된 박근혜 전 대통령을 지지하는 집회에 불을 지폈다. 그들은 또한 그녀의 자유주의 후계자 문재인의 은퇴자 집 밖에서 시위를 지지했습니다. 좌익 채널은 윤 총장의 자택 밖에서 반대집회를 지지했다.​지난해 윤 전 대표는 유튜브 채널의 '악영향'을 거론했다.​ 박근혜 전 대통령 지지자들이 2018년 4월 6일 서울중앙지법 앞에서 그녀의 석방을 요구하는 집회를 열고 있다.​언론의 자유를 위한 플랫폼YouTube 채널은 미국 국무부의 우려를 감안할 때 더욱 중요한 언론의 자유를 위한 공간을 제공하는 것으로 간주됩니다.한국 한양대학교 ERICA 캠퍼스의 정준희 언론학과 교수는 대부분의 전통적인 매체들이 정부를 비판하는 것을 피했다고 말했습니다.​정 실장은 “윤석열 총장이 집권한 뒤 청와대가 언론에 고발하는 경우가 많았다.​정은 “동료든 아니든 표적이 되는 것에 대한 두려움이 크다”라고 말했다.​시간이 지남에 따라 비판적 보도의 부족은 시민들이 전통적인 미디어에 대한 신뢰를 잃고 대신 인터넷으로 눈을 돌렸다는 것을 의미한다고 서울대학교 커뮤니케이션학과 이준웅 교수는 말했습니다.​이 대통령은 “전통적인 신문과 방송 매체가 완전히 버렸다고 말할 수는 없지만 점점 더 많은 시민들이 이에 불만을 품고 인터넷 매체에서 정보와 해석, 표현을 찾고 있다”라고 말했다.​CNN은 최근 명예훼손 소송에 대해 청와대에 논평을 요청했지만 아직 답변을 받지 못했다.​ 2023년 3월 8일 '겸손은힘들다' 진행자 김어준과 유밀리 기자가 오늘의 뉴스에 대해 이야기를 나누고 있다​뉴스 공장 폐쇄Kim에게 잃지 않는 역학입니다. 그의 이전 프로그램인 공적 자금 지원을 받는 ""뉴스 팩토리"" 프로그램은 수년 동안 서울에서 가장 높은 평가를 받는 라디오 프로그램이었습니다(그리고 그를 국내에서 가장 높은 유료 발표자 중 한 명으로 만들었습니다).2016년 월요일부터 금요일까지 2시간 동안 TBS에 데뷔한 이 프로그램은 형식이 간단했습니다. 오늘의 주제에 대한 Kim의 논평과 뉴스 요약, 정치인, 교수, 언론인, 예술가 및 과학자에 이르기까지 뉴스메이커가 등장하는 세그먼트가 이어졌습니다. 정준희 한양대 미디어학과 교수는 ""뉴스 분석에 대한 거침없는 접근 방식과 라이브 인터뷰는 한국 언론의 틀을 깨고 정치인들이 즐겨 찾는 프로그램이 됐다""라고 말했다. 정 실장은 “(예전에는) 정치인들이 라디오에 출연하지 않았고, 아침 라디오에서는 (대부분) 전날 밤 뉴스를 정리하고 실시간 교통정보 등을 전달하는 데 사용했다”라고 말했다. 그러나 그것의 반대 스타일은 박 전 대통령을 둘러싼 스캔들에 대한 취재와 마찬가지로 보수주의자들을 화나게 했다. (쇼가 전성기를 누렸던 자유주의 문재인에 이어) 2022년에 보수 정부가 집권했을 때, 그날이 얼마 남지 않았습니다. 얼마 지나지 않아 보수적인 시의회는 ""뉴스 팩토리""에 대한 불만을 반영하는 것으로 널리 알려진 움직임으로 TBS의 예산을 삭감할 계획이라고 발표했습니다. 오세훈 서울 시장은 링크를 부인했지만, 지난 2월 그는 쇼가 ""일방적""이며 네트워크 TBS가 ""공영 방송이 넘을 수 없는 선을 넘었다""라고 비판했습니다. 오 씨는 “어느 나라든 공영방송이 특정 정당에 편향돼 있으면 참을 수 없다”라고 말했다  '겸손은힘들다' 제작진이 2023년 3월 13일 서울 스튜디오에서 생방송을 진행하고 있다.​가을과 상승김 씨는 지난 12월 '뉴스팩토리'를 하차했다. 다음 달에 그는 ""Modesty is Nothing""을 시작했습니다.쇼 간의 유일한 차이점은 이름과 매체입니다. 형식은 동일하고 스튜디오도 복제품이지만 이제는 Kim의 야망에 따라 더 커졌습니다. 일주일 만에 구독자 100만 명을 돌파했다. 이후 한국 유튜브 일일 실시간 시청률 1위를 꾸준히 기록하고 있다. 정 교수에게는 “목소리는 멈출 수 없다”는 것을 보여준 성공이다. 한편 Kim은 전통적인 미디어에서 가장 인정받는 쇼를 만들기를 희망합니다. 김 대표는 “아직 유튜브에 없던 언론을 만들겠다”라고 말했다. “정치적 이유로 공연을 취소한 것이 잘못되었음을 보여주겠다는 선언입니다.” 출처: By Yoonjung Seo and Andrew Raine, CNN BUSINESS​  ​​ "
"[교실 밖에서 듣는 바이오메디컬공학] 1. 손발 잃은 사람들의 희망 : 근전 인터페이스 (전자의수, 의족) (임창환) ",https://blog.naver.com/bookmid/222660208084,20220301,"의수에 관절이 생겨나다© stepintofuture, 출처 Pixabay 신체에서 잃어버린 부분을 다른 기구로 대체하고자 하는 욕망은 너무나 자연스러운 본능이겠지요. 그래서 잃어버린 팔이나 다리를 대체하는 의수나 의족은 무려 2천 년 이상의 역사를 갖고 있습니다. 하지만 500년 전까지만 하더라도 의수나 의족은 앞에서 언급한 후크 선장의 갈고리나 해적이 달고 다니던 통나무 의족과 크게 다르지 않았지요.  이런 의수, 의족에 큰 변화의 바람을 일으킨 사람이 있었는데요. 바로 ‘보철의 아버지’라고도 불리는, 16세기 프랑스 외과의사 앙부르아즈 파레 Ambroise Paré입니다. 파레는 처음으로 의수와 의족에 관절을 달아 팔과 다리를 접을 수 있게 했는데요. 물론 자동으로 접을 수 있었던 것은 아니고 일일이 손으로 잡아당겨야 했지만 그가 의수나 의족에 관절을 만들어 넣은 것은 인공 보철의 역사에서 아주 중요한 이정표가 됐습니다. 특히 중세였던 당시는 다리를 잃은 기사가 관절이 있는 의족을 착용하자 다시 말을 타고 전투에 참가할 수 있게 되었는데요. 이것이 중세 전쟁의 판도를 바꿨다는 평가를 받기도 합니다.  파레의 접히는 의수와 의족 이후, 인공 보철의 역사에서 가장 큰 변화를 만들어 낸 사건이 1960년 무렵 일어납니다. 1950년대 후반에 임산부의 입덧을 없애 주는 약으로 널리 사용됐던 ‘탈리도마이드 thalidomide’라는 약을 복용한 임산부들이 팔이나 다리가 없는 기형아를 출산하기 시작한 것이 계기가 되었는데요. 이 사건은 인류 의학 역사상 최악의 비극이었습니다. 무려 2만 5천 명이 넘는 아이들이 팔이나 다리 없이 태어나게 되었기 때문이지요.  무려 2천 명이 넘는 피해자가 발생했던 영국에서는, 이 아이들을 위해 ‘터치 바이오닉스 Touch Bionics’라는 회사가 설립됩니다. 그리고 마침내 손가락을 전동으로 움직일 수 있는 의수를 만들어 내었지요. 현대의 전자의수는 이 불쌍한 아이들을 돕기 위해 시작됐다고 해도 과언이 아닙니다. 전자의수를 가능하게 한 근전 인터페이스© viniciusamano, 출처 Unsplash 그런데 전자의수를 착용하면 어떻게 의도한 대로 손을 자유롭게 움직일 수 있는 것일까요? 그 비결은 바로 ‘근전 인터페이스 myoelectric interface’라는 기술에 있습니다. 우리 몸은 수많은 근육으로 이뤄져 있는데요. 이 근육을 수축시키는 동력은 다름 아닌 우리 몸에 흐르는 전류입니다. 근육이나 신경은 흥분성 세포라고 하는데요. 세포의 흥분이 일어나면 활동전위 action potential라고 하는 전류를 발생시킵니다. 이처럼 근육에 전류가 흐르면 근육의 수축이 일어나게 되는 것이지요. 심장의 경우에도 심장 벽을 이루는 근육에 전류가 흐르면 심장이 수축되면서 온몸에 피를 공급할 수 있게 됩니다.  이처럼 근육에서 발생하는 전류를 근전도 electromyogram라고 부르는데요. 근전도 신호는 피부 표면에서도 측정이 가능합니다. 피부에 전기신호를 측정할 수 있는 전극을 여러 개 붙인 채로 서로 다른 손동작을 취하면 각각의 전극에 독특한 패턴의 근전도가 측정됩니다. 이 신호를 기계학습 기술을 이용해서 분석하면 어떤 손동작을 취했는지를 알아낼 수 있지요.  그런데 손이나 팔을 잃어버린 사람들은 원하는 손동작을 취할 수가없습니다. 하지만 우리의 뇌에는 여전히 잃어버린 손이나 팔을 움직이게 하는 영역이 남아 있기 때문에 특정한 손동작을 하라는 명령을 내릴 수 있습니다. 이런 명령을 내릴 때, 팔의 잘린 부분 바로 위쪽에 남아 있는 근육의 근전도 신호를 측정하면 그 사람이 내린 명령을 간접적으로 알아내는 것이 가능하지요. 이런 방법으로 전자의수를 의도대로 움직일 수가 있습니다.  전자 의족의 경우에도 같은 원리로 발목이나 발가락을 움직이는 것이 가능합니다. 하지만 전자 의족은 무엇보다 잘 걸을 수 있도록 도와주는 게 더 중요한 부분인데요. 현대의 의족은 인공지능 기술을 이용해 개개인의 보행 패턴을 자동으로 분석해서 모터를 제어함으로써 자연스러운 걸음걸이를 가능하게 할 정도로 발전했습니다.  하지만 아직도 영화 <어벤저스>나 <스타워즈>에 등장하는 전자의수나 전자 의족을 만들기 위해서는 더 많은 연구가 필요합니다. 최근 의공학자들이 집중하고 있는 기술은 전자의수로 물체를 만졌을 때 느껴지는 감각을 우리 뇌로 전달하는 것입니다. 인간의 감각은 온몸에 거미줄처럼 뻗어 있는 신경망을 통해 뇌로 전달이 되는데요. 잘려진 팔의 부위에서 손가락의 감각을 전달하는 신경섬유를 찾아낸 뒤, 그 부위에 전기 자극을 주면 손가락에 전달되는 감각을 뇌로 전달하는 게 가능합니다.  하지만 아직은 거칠거나 부드러운 천을 만질 때, 뜨거운 물체를 만질 때와 같이 다양한 감각을 만들어 내지는 못하고 있는데요. 많은 의공학자들이 이 연구에 뛰어들고 있기 때문에 가까운 미래에는 다양한 감각을 인공적으로 만들어 내는 것이 가능할 것으로 기대합니다.   전자 의수나 전자 의족의 비싼 가격도 문제인데요. 아무래도 사용자가 많지 않다 보니 소량 주문 생산을 해야 해서 가격이 비싸질 수밖에 없는 실정입니다. 최근에는 이런 문제를 해결하기 위해 3D 프린터를 이용해서 저렴한 의수를 만드는 회사도 생겨났습니다. 저개발 국가나 저소득 장애인들에게는 희소식이 아닐 수 없지요.  그런가 하면 앞서 살펴본 것처럼 잘린 팔이나 다리 부위에서 근전도를 측정하지 않고 뇌의 운동영역에서 직접 신호를 읽어내 전자의수나 전자 의족을 제어하는 기술도 개발되고 있는데요. 이런 기술을​ 뇌-기계 인터페이스 Brain-Machine Interface, BMI​라 합니다. BMI 기술은 근전 인터페이스보다 정확도나 속도는 많이 떨어지지만 근전 인터페이스를 쓸 수 없는 사지마비 환자를 위해 반드시 필요한 기술입니다.  뿐만 아니라 최근 들어 장애가 없는 일반인들에게 근전 인터페이스 기술을 적용하려는 시도도 이뤄지고 있습니다. 여러분들도 잘 아는회사인 페이스북(메타)은 2019년에 컨트롤 랩스 Ctrl Labs라는 스타트업 회사를 무려 1조 원에 가까운 큰돈을 투자해 인수했습니다. 이 회사 는 팔에 팔찌처럼 착용할 수 있는 무선 근전도 센서를 개발했는데요. 근전 인터페이스 기술로 서로 다른 손동작을 인식해 증강현실 AR이나 가상현실 VR 장비를 별도의 컨트롤러 없이 조절할 수 있게 해 줍니다.  한양대학교 연구팀에서도 최근 얼굴에서 측정되는 근전도 신호를 이용한 근전 인터페이스 기술을 개발하고 있는데요. 이 기술은 VR 헤드셋을 착용할 때 헤드셋과 피부가 닿는 부위에 전극을 부착해 근전도신호를 읽어 내어 얼굴의 표정을 알아내는 기술입니다. 이 기술을 이용하면 VR 메타버스 공간 내에서 아바타의 얼굴에 실제 자신의 표정을 투영하는 것이 가능하겠지요.  그런가 하면 근전 인터페이스 기술을 이용하면 무음 발화 인식 silent speech recognition 시스템이라는 것도 만들 수 있습니다. 전극이 부착된 VR 헤드셋을 착용하면 말을 하지 않고 입을 움직이는 것만으로 근전도 신호를 이용해 어떤 말인지 알아낼 수 있는 기술이지요.  이처럼 인공지능 기술이 발전하면서 근전 인터페이스의 성능도 나날이 높아지고 있는데요. 가까운 미래에는 저희가 개발하고 있는 근전 인터페이스 기술을 이용해 심각한 장애를 가진 사람들이 메타버스 공간에서 새로운 삶을 살게 될 수도 있지 않을까 기대해 봅니다.​#교실밖에서듣는바이오메디컬공학 #바이오메디컬 #MID #엠아이디 #임창환 #의수 #의족 #전자의수 #전자의족 #VR #메타버스 #근전도 #근전인터페이스 #관전 #뼈 #장애 #보철 #청소년도서 #과학도서 #공학 #선행학습 #과학​ ​세계 자동차 시장을 선도하고 있는 테슬라의 CEO ‘일론 머스크’가 관심을 가지고 있는 또 하나의 분야가 있는데, 그것은 바로 '뇌'다. 현재는 뇌전증과 같은 질병을 뇌에 전극을 심어 치료하는 기술을 연구하고 있지만, 궁극적으로는 우리 머릿속 생각을 컴퓨터에 업로드할 수 있는 단계까지 발전시키는 것을 목표로 하고 있다고 한다. 그리고 그는 진짜로 원숭이의 뇌에 전극을 심어 생각만으로 컴퓨터 게임을 하게 함으로써 그 가능성을 보여주었다. 먼 미래의 이야기인 줄만 알았던 현실이 생각보다 가까이 와 있는 것이다.이런 놀라운 실험이 성공하기까지는 흔히 ‘의공학’으로도 불리는 ‘바이오메디컬공학’ 기술의 발전이 한 몫을 톡톡히 했다. 100년 전까지만 해도 우리는 X-레이 기술조차 없어 우리 몸속의 모습을 들여다보고 아픈 곳을 찾아내는 것이 어려운 시대였다. 하지만 바이오메디컬공학 기술의 발전 덕분에 이제는 수초 단위의 심장의 움직임을 영상으로 찍어낼 수 있게 되었고, 인공와우 같은 인공장기가 상용화되어 많은 사람들이 새 삶을 얻게 되었다. 뿐만 아니라 바이오메디컬공학은 먹지 않고도 질병을 치료하는 ‘전자약’, ‘인공지능’이 판독하는 CT 영상처럼 우리가 상상만 했던 현실을 이미 실현시키고 있다.그리고 이제 바이오메디컬공학은 4차 산업혁명의 대흐름과 함께 파킨슨병, 치매와 같이 인류가 극복하지 못한 질병을 정복하기 위해, 그리고 ‘로봇 팔’과 같은 첨단 의료기기를 개발하기 위해, 궁극적으로는 ‘뇌-컴퓨터 인터페이스’ 기술을 통한 인간 증강을 위해 쉼 없이 달려나가고 있다. 바야흐로 미래 의료서비스와 인류의 복지를 책임질, ‘미래 핵심산업기술’이 된 것이다.​​책에 대한 더 자세한 정보는 아래 ↓↓↓에서 확인해 주세요! :) 교실 밖에서 듣는 바이오메디컬공학 저자임창환, 김선정, 김안모, 김인영, 이병훈, 장동표, 최성용출판MID발매2021.12.16. ​ "
어음 청각 검사 알아보기  ,https://blog.naver.com/killerny/222857677810,20220825,"어음청력검사란 (Speech audiometry)?어음에 대한 민감성 (sensitivity) 및 인지도 (recognition score) 등을 측정하는 평가입니다ex) 순음청력검사 : 순음을 사용해 주파수별 청력역치를 측정하는 검사 ​목적 : 어음인지 능력을 측정하여 순음청력검사(Pure tone audiometry)로 평가할 수 없는 의사소통 능력을 파악합니다. 어음청력검사 결과는 청력 손실 및 청각 보조 기기로부터의 혜택 정도를 예측하는 데 사용할 수 있고 청각 보조 기기 적합 후 상담, 청능재활의 계획 시 도움이 됩니다.​- 주변 소음을 최대한 차단해 주는 방음실에서 검사를 하고 청력검사기 (audiometry)를 사용하여 검사를 실시합니다. ​<어음 청력검사 종류>  1. 어음인지역치 (Speech recognition threshold, SRT)2. 단어인지도 검사 (Word recognition score, WRS)3. 문장인지도 검사 (Sentence recognition score, SRS)4. 쾌적 레벨(MCL), 불쾌 음량 레벨(UCL)5. 자음 지각검사(K-CPT), 말 지각 발달검사(KNISE-DASP) ​ 어음인지역치 검사 (Speech recognition threshold, SRT)정의 : 제시된 이음절어를 50% 인지할 수 있는 최소 강도 레벨을 측정하는 것입니다.  목적 : 어음인지 시 필요한 민감성(sensitivity), 즉 SRT를 측정하여 순음청력검사 결과의 신뢰도를 확인하고 단어 및 문장 인지도 검사의 기초 자료로 사용하는 것입니다. ​<검사 과정>1. 자극음 결정: SRT 검사에서는 일상생활에서 친숙하게 사용하는 이음 절어(spondee)을 주로 사용하는데 각 음절의 강도가 동일하고 검사용으로 표준화한 어표를 사용해야 합니다 2. 검사 귀 결정 및 보정 확인3. 친수화(familiarization) 과정4. SRT 결정5. 결과 판독 시 주의사항 : ±10(PTA) : SRT와 PTA가 10dB 이내일 경우 순음청력검사의 신뢰성이 좋다고 판단합니다.그 이상 차이를 보일 경우 순음청력역치 신뢰도를 의심하여 순음청력역치를 재 측정합니다. 6. SRT 측정을 위한 차폐  학령전기용 이음절어표(KS-BWL-P) 그림판<어음탐지역치>(Speech - Detection Threshold): 어음인지역치검사(SRT) 측정이 불가능한 경우 SRT 대신 시행하여 어음의 유무를 탐지(detect) 할 수 있는 최소 레벨을 측정하는 것ex) SDT<SRT=8-10dB ​ 단어인지도 검사(Word recognition score, WRS)정의 : 피검자가 듣기 편한 레벨 (MCL)에서 단음절어를 듣고 얼마나 정확하게 인지하는지를 백분율로 점수화한 것입니다.​<검사 과정>1. 자극음 결정 : - 단음절(one-syllable, monosyllable) 단어- 단어의 친숙성 (familiarity)- 음소 간의 비유사성 (phonetic dissimilarity)- 표준어의 대표성 (normal sampling)- 단어 간 가청범위의 동질성 (homogeneity)2. 검사 귀 결정 및 보정 확인3. 단어 제시 방법4. WRS 계산법 및 주의사항 : 백분율 (%)5. WRS 측정을 위한 차폐 ​<어음인지역치 검사(SRT)와 단어인지도 검사(WRS) 차이점>- 어음인지역치검사(SRT)는 이음절어를 50% 인지할 수 있는 최소 강도 민감성(sensitivity)를 측정하지만 단어인지도 검사(WRS)는 쾌적 레벨(MCL)에서 단음절어를 들었을 때 얼마나 잘 이해하는지 정확도(accuracy)를 평가하는 것입니다.단어인지도 검사는 표준화된 단음절어를 사용하여 쾌적 레벨(MCL)에서 검사를 실시합니다.  수정된 학령전기용 단음절어표(KS-MWL-P) 문장인지도 검사 (Sentence recognition score, SRS)정의 : 피검자가 듣기 편안한 레벨(MCL)에서 문장을 듣고 문장 내 단어를 얼마나 정확히 인지하는지를 백분율(%)로 점수화한 것입니다. ​<단어인지도 검사 (WRS)와 문장인지도 검사(SRS) 차이점>- 단어인지도 검사(WRS)는 단음절어만을 제시하기에 일상생활 속 의사소통 능력을 완벽하게 반영하기에는 한계가 있습니다.SRS에서는 문맥 힌트가 포함되어 있는 문장을 자극음으로 제시하여 피검자의 일상생활 속 의사소통 능력을 반영하는 결과를 얻고자 합니다.문장인지도 검사는 표준화된 일상생활 문장을 사용하여 쾌적 레벨에서 검사를 실시합니다.​<검사 과정>1. 자극음 결정: 표준화된 일상생활의 문장을 사용, 문장 난이도, 대상자 연령 고려2. 검사 귀 결정 및 보정 확인3. 문장 제시 방법4. SRS 계산법 및 주의사항 : 백분율 (%)5. SRS 측정을 위한 차폐  학령전기용 문장표(KS-SL-P) 쾌적 레벨(MCL, Most comfrotable level), 불쾌 음량 레벨(UCL, Uncomfortable loudness level)- 어음을 자극음으로 이용하여 쾌적 레벨 또는 불쾌 음량 레벨을 측정할 경우 보통 피검사자와 대화하면서 MCL과 UCL을 찾습니다. 쾌적 레벨(MCL)은 어음인지역치 검사(SRT) 보다 점차 큰 강도로 변화시켜 주며 검사자의 목소리가 편안하게 들리는 레벨을 측정합니다. 불쾌 음량 레벨(UCL)은 MCL과 마찬가지로 지속적인 대화를 통하여 소리가 크거나 불편하지 않을 정도의 크기가 되면 버튼을 누르거나 말을 하여 측정합니다.  피검자에게 크게 들리지만 고통스럽지 않은 강도를 찾습니다. ​SRT와 UCL을 측정했다면 그 차이 값을 통해 어음 역동 범위(Dynamic range, DR)을 구할 수 있습니다.* 어음역동범위 : 불쾌 강도 레벨(UCL) - 청력역치(hearing threshold)건청인의 경우 보통 어음 역동 범위가 80dB 이상이나 난청인의 경우 난청의 정도와 종류가 유사하더라도 개인 간 역동 범위가 다를 수 있습니다.따라서 청각 보조 기기 적합 전에는 반드시 피검자의 Most comfortable level, Uncomfortable level, Dynamic range을 구해야 합니다. ​<국내 외 어음 청각 검사 도구>- 무소음 환경 어음 청각 검사 도구 : 한국어 어음 청각 검사 도구(Korean speech Auiometry, KSA)- 한국 표준 일반용 이음절어표(KS-BWL-A, 조수진 외, 2008)- 한국 표준 학령기용 및 학령 전기용 이음절어표(KS-BWL-S, KS-BWL-P, 조수진 외, 2008)- 한국 표준 일반용 단음절 어표(KS-MWL-A, 김진숙 외, 2008)- 한국 표준 학령기용 및 학령 전기용 단음절 어표(김진숙 외, 2008, 신현욱 외, 2009)​* 소음하 어음 청각 검사- K-SPIN (김진숙 외, 2000)- K-HINT (문성균 외, 2005)​<Speech Perception Stages>1. Detection 인식 (Sound awareness)2. Discrimination 변별 (Sound discrimination)3. Recognition 확인 (Identification)4. Understanding 이해 (Comprehension)​​​ 권준열보청기 청각언어센터 창원경상남도 창원시 의창구 평산로207번길 4 킹덤시티 4층 ​ "
Why deep learning? in life sciences 딥러닝공부시작! ,https://blog.naver.com/dltjdus0225/222912820180,20221028,"Deep Learning for the Life Sciences: Applying Deep Learning to Genomics, Microscopy, Drug Discovery, and More 저자Bharath Ramsundar,Eastman, Peter출판Oreilly & Associates Inc발매2019.05.05. 앞으로 나올 모든 내용은 위의 책에서 쓰여진 글입니다. 현재 저는 프로그래밍과 머신러닝에 대해 공부한지 약 4개월 정도되고, 혼자 공부한지는 대략 두달 반정도 되어가는 것 같습니다. 여러강의도 듣고 해서 개념들이 이것저것 합쳐져있어서 정리가 되지 않아 블로그에서 쓰면서 좀 정리를 해보고 싶어서 쓰려고합니다!! 공부하시는 분들께 도움이 되셨으면 좋겠네용!!그리고 위의 deep learning for the life sciences 는 cheminformatics 관심있는 사람들이 보시면 좋을 것 같아요~~ 저도 지금 화학정보학을 배우고있는데 저책에 배우고싶은 library가 저 책에 있어서 선택하게 되었습니다​<이제 편의를 위해 혼자 메모하듯이 쓰겠습니다. 반말?!주의...ㅎㅎ>일상 생활에 머신러닝의 영향이 커진 가운데, physical 과 life sciences의 많은 영역에도 영향을 주고 있다. 기술 발전을 이끄는 것 중 하나는 deep neural networks로 알려진 머신러닝 방법의 개발이다.원래 artificail neural network의 기술적인 토대는 1950년대에 만들어졌지만, 진정한 기술의 힘은 컴퓨터 하드웨어가 발전할 때까지 완전히 실현되지 못하고 있었다. 딥러닝으로 이용한 기술의 발전을 아는 것은 중요하다. < Deep learning application >speech recognitionImage recognitionRecommender systemsLanguage translation​현재, life science분야에서는 자동화와 간소화된 실험의 이용으로 많은 데이터가 생성되고 있다. 예를 들면, 예전과 달리, gene sequencing같은 실험은 지금은 비싸지도 않고, 루틴하게 이루어지고 있다. 시퀀싱의 발전으로 여러 질병 등등을 포함한 건강과 관련된 다양한 결과들을 개인의 유전적 코드와 결합한 데이터베이스의 생성을 만들어지게 되었다. 그리고 이러한 데이터들을 분석하고 처리할 수 있는 컴퓨터 기술의 발전으로 과학자들은 질병의 원인들을 이해하고, 새로운 치료법을 개발하고 있다. ​또한 실험기술의 발전으로 화학물질의 구조와 생물학적 과정이나 활동에 미치는 영향을 분류하는 여러 데이터베이스가 개발되었다. 이러한 Structure-activity relationships (SARs)는 cheminformatics 화학정보학으로 알려진 분야의 기초가 되고 있다. 과학자들은 이러한 대규모 데이터세트를 사용하여 차세대 약물 개발을 주도할 예측 모델을 구축하고 있다. 이러한 많은 양의 데이터로 인해 사이언스 및 컴퓨터 영역 모두에서 잘할 수 있는 새로운 유형의 과학자가 필요한 상태이다. ​​​책의 구매링크인데 제가 관련 중요한내용을 이제 꾸준히 업로드할 예정이니 블로그 내용 쭉 한번보시고 구매를 고려하는 것도 좋을거같아요​https://link.coupang.com/a/EA5M1 Deep Learning for the Life Sciences:Applying Deep Learning to Genomics Microscopy Drug Discov...COUPANGlink.coupang.com ( 이 포스팅은 쿠팡파트너스 활동의 일환으로, 이에 따른 일정액의 수수료를 제공받습니다.)​​ "
How Neural Networks Fit into the Architecture of A ,https://blog.naver.com/yoono044/223022770380,20230221,"Neural networks are a key component of the architecture of AI, and they play a crucial role in the development of intelligent applications. Neural networks are a type of machine learning algorithm that is designed to mimic the way the human brain works.​In this article, we will explore the different types of neural networks and their applications in artificial intelligence.​​ https://res.cloudinary.com/engineering-com/image/upload/w_640,h_640,c_limit,q_auto,f_auto/196431_web_pmfitk.jpg​What Are Neural Networks?​Neural networks are a type of machine learning algorithm that is designed to recognize patterns in data. They are modeled after the structure of the human brain, with layers of interconnected nodes that process information.​Each node in a neural network receives input from the previous layer and applies a mathematical function to it. The output of each node is then passed on to the next layer, until the final output is produced.​Types of Neural Networks​There are several types of neural networks, each with its own unique characteristics and applications. The most common types of neural networks include:​Feedforward Neural Networks​Feedforward neural networks are the most basic type of neural network. They consist of an input layer, one or more hidden layers, and an output layer. Information flows in one direction, from the input layer to the output layer.​Feedforward neural networks are used in a wide range of applications, including image and speech recognition, natural language processing, and predictive modeling.​Recurrent Neural Networks​Recurrent neural networks are designed to process sequential data, such as text or speech. They are able to remember previous inputs and use this information to make predictions about future inputs.​Recurrent neural networks are commonly used in applications such as language modeling, speech recognition, and time series prediction.​Convolutional Neural Networks​Convolutional neural networks are designed to process data with a grid-like structure, such as images or video. They use a process called convolution to extract features from the input data.​Convolutional neural networks are commonly used in image and video recognition applications, as well as natural language processing.​Applications of Neural Networks in AI​Neural networks are used in a wide range of applications in artificial intelligence. Some of the most common applications of neural networks include:​Image and Video Recognition​Neural networks are used in applications such as image and video recognition, where they are able to recognize objects and patterns in visual data.​Natural Language Processing​Neural networks are used in natural language processing applications, where they are able to process and understand human language.​Predictive Modeling​Neural networks are used in predictive modeling applications, where they are able to analyze data and make predictions about future outcomes.​Conclusion​In conclusion, neural networks are a key component of the architecture of AI. They are a type of machine learning algorithm that is designed to recognize patterns in data, and they are modeled after the structure of the human brain.​There are several types of neural networks, each with its own unique characteristics and applications. Some of the most common applications of neural networks include image and video recognition, natural language processing, and predictive modeling.​By understanding the different types of neural networks and their applications in AI, we can better understand how intelligent applications are developed and how they work to produce intelligent outcomes. "
롤러코스터를 타는 사운드하운드 AI [SOUN] 분석 ,https://blog.naver.com/j0hn_d0e/223039625321,20230309,"안녕하세요. 여러분만큼만 알고 여러분만큼 모르는 JOHN DOE 입니다.​오늘은 AI가 주목을 받고 있는 가운데1년 내 등락률이 롤러코스터 그 자체인 사운드하운드 AISOUN 분석입니다.  회사 개요 SoundHound AI, Inc. develops independent voice artificial intelligence (AI) platform that enables businesses across industries to deliver high-quality conversational experiences to their customers. Its products include Houndify platform that offers a suite of Houndify tools to help brands build conversational voice assistants, such as automatic speech recognition, natural language understanding, wake words, custom domains, text-to-speech, and embedded voice solutions The company is headquartered in Santa Clara, California.SoundHound AI, Inc.는 업계 전반의 기업이 고객에게 고품질의 대화 경험을 제공할 수 있도록 하는 독립적인 음성 인공지능(AI) 플랫폼을 개발합니다. 이 회사의 제품에는 자동 음성 인식, 자연어 이해, AI 자기 인식, 사용자 지정 도메인, 텍스트 음성 변환 및 임베디드 음성 솔루션과 같은 브랜드의 대화형 음성 비서를 구축하는 데 도움이 되는 Houndify 도구 모음이 포함되어 있습니다. 이 회사는 캘리포니아 산타 클라라에 본사를 두고 있습니다.  재무 및 투자 상황 2022년 4월 28일 상장 한 뒤로성장주 답게 순이익은 마이너스인 상황이지만 총 매출은 지속적으로 성장하고 있습니다.​또한 회사의 상장일이 1년도 되지 않았으므로 재정적인 상황을 장기적인 관점에서 확인하기가 어려운 상황입니다.​하지만 회사를 정상적인 궤도에 진입시키기 위해서는 반드시  흑자전환에 성공해야합니다.​ 위 사진은 사운드하운드 AI에 투자중인 회사들을 보여줍니다.​우리에게는 익숙한 현대차, 네이버, 삼성, 라인, KT 등과 그 외에도 중국의 텐센트, 벤츠 및 오라클 등 여러 분야의 대기업들에게 1,500억이 넘는 금액을 투자받고 있으며 국내 AI 솔루션 업체인 솔트룩스와 협력중입니다.​ (솔트룩스 회사 개요)​현대차는 사운드하운드(SoundHound)와 음성 AI(인공지능) 플랫폼을 7년 간 장기 공급 계약을 체결하기도 했습니다.​  관련 칼럼 Near midday on Wednesday, SoundHound shares were 14% lower, to $2.87, leaving the stock with a gain of 62% so far in 2023. Other AI companies were sliding too, BigBear.ai Holdings (BBAI) off 11%, C3.ai AI –5.05%  (AI) off 3.8% and BuzzFeed (BZFD) 4.7% lower.수요일 정오 무렵, 사운드하운드의 주가는 14% 하락한 2.87달러로 2023년 현재까지 62%의 상승률을 기록했습니다. 다른 AI 기업들도 하락했고, BigBear.ai 홀딩스(BBAI)는 11% 하락했고, C3.ai AI –5.05%(AI)는 3.8%, 버즈피드(BZFD)는 4.7% 하락했습니다. SoundHound shares have benefited this year from the frenzy over generative artificial intelligence that began when Open AI launched ChatGPT on Nov. 30. Microsoft MSFT –0.18% ’s (MSFT) update of its Bing internet search service to include an Open AI-powered chatbot, and the promise of something similar from Alphabet GOOGL +0.42% ’s (GOOG) Google have added to the excitement, as has news about generative AI projects from Meta Platforms (META), Snap (SNAP), Salesforce (CRM), Duolingo (DUOL) and C3.ai, among many others.사운드하운드 주식은 11월 30일 오픈 AI가 ChatGPT를 출시했을 때 시작된 생성적 인공지능에 대한 광란으로부터 올해 이익을 얻었습니다. 마이크로소프트 MSFT – 0.18% (MSFT)가 Bing 인터넷 검색 서비스를 오픈 AI 기반 챗봇을 포함하도록 업데이트했고, 메타 플랫폼 (META), 스냅 (SNAP), 세일즈 포스 (CRM), 듀올 (DuOLA) 및 구글의 생성적 AI 프로젝트에 대한 뉴스와 함께 알파벳 GOGL +0.42% (GOOG)와 유사한 것에 대한 약속이 추가되었습니다i, 다른 많은 것들 중에서요. Mohajer describes SoundHound as having three business pillars. The first is royalties from licensing its voice AI interface to both auto makers like Mercedes-Benz , Honda and Hyundai, and consumer device companies, in particular for televisions from Vizio . The second targets customer service, for applications like drive-through restaurants, including the White Castle burger chain. And the third pillar brings those ideas together, using generative AI models to make it easier to interact, and transact, in cars and via TVs and other devices.Mohajer는 SoundHound가 세 가지 비즈니스 기둥을 가지고 있다고 설명합니다. 첫 번째는 음성 AI 인터페이스를 메르세데스-벤츠, 혼다, 현대와 같은 자동차 제조업체와 소비자 장치 회사, 특히 비지오의 텔레비전에 대한 라이센스로 인한 로열티입니다. 두 번째 목표는 화이트 캐슬 버거 체인을 포함한 드라이브스루 레스토랑과 같은 애플리케이션에 대한 고객 서비스입니다. 그리고 세 번째 기둥은 자동차와 TV 및 기타 장치를 통해 상호 작용하고 거래하는 것을 더 쉽게 만들기 위해 생성적 AI 모델을 사용하여 이러한 아이디어들을 한데 모읍니다. Mohajer says the market’s recent fascination with AI software “has been one of the best things that’s ever happened to us.” He says that this is one of the rare moments in the history of technology business that huge demand intersects with technology readiness. 모하저는 최근 AI 소프트웨어에 대한 시장의 관심이 ""우리에게 일어난 일 중 가장 좋은 일 중 하나""라고 말합니다 그는 지금이 기술 비즈니스 역사상 엄청난 수요와 기술 준비가 교차하는 드문 순간 중 하나라고 말합니다.  회사에 대한 사견 위 차트는 사운드하운드 AI의 5일 차트입니다.3월 8일에는 장 열리자마자 약 17%가 증발해버렸습니다.​3분기에 괄목할 만한 성장을 기록했지만, 회사의 총 운영비가 2,700만 달러인 데 반해 1,120만 달러의 수익으로 인해 약 1600만 달러의 손해가 있었습니다. 물론 성장주이기 때문에 단기적으로는 손실이 있음을 모두 알고 있지만 사운드하운드 AI는 77%의 총 마진에 비해 과도한 손실을 낸 것이 아닌가 생각해봅니다.​그리고 사운드하운드AI는 MS의 챗GPT로 인한 인공지능 수혜를 받은 종목이라고 생각합니다.​장기적으로 사운드하운드 또한 음성 인공지능 분야의 강자로서 우상향 하는 그래프를 만들 수도 있지만 아직까지는 인공지능 수혜를 받아야 하는 회사지, 같은 peer의 회사들에게 수혜를 줄만한 영향력있는 회사로는 발돋움 하지 못했습니다.​개인적인 생각으로는 아직 장기적인 투자보다는 스윙 트레이딩에 어울리는 회사라고 생각됩니다.​  ​# 투자는 반드시 자기 자신의 판단과 책임하에 하여야 하며, 수익에는 반드시 높은 위험이 따른다는 것을 기억하고 투자시 어떤 위험이 있는지 반드시 확인하시기 바랍니다.​ "
[python]파이썬/음성인식/speechrecognition/파일 읽기/SpeechRecognition 설치 ,https://blog.naver.com/scyan2011/222557259745,20211103,"음성인식 음성인식이란 인간이 발음한 구문을 문자열로 바꾸어주는 것을 의미합니다. 우리는 이전 포스팅에서 파이썬을 이용하여 TTS(Text To Speech) 프로그램을 만들었습니다. 문자열을 음성으로 읽어주는 프로그램이었지요. 그와는 정반대의 프로그램이 바로 음성인식입니다.​SpeechRecognition 설치​음성인식을 위해서는 먼저 SpeechRecognition 라이브러리를 설치해야합니다. 명령 프롬프트에서 다음과 같이 입력합니다.​  pip3 install SpeechRecognition pydub​저는 이미 설치가 되었다고 나오지만 여러분은 프로그레스바가 보이면서 설치가 시작됩니다.​음성파일 다운​텍스트로 바꾸어 줄 음성파일을 다운로드하여 동일 디렉토리에 저장합니다.​ 첨부파일soundRec.wav파일 다운로드 ​이제 준비가 끝났으니 sr.py 파일을 작성합니다. 음성인식 엔진은 여러가지가 있지만 별도의 키 생성이 필요없는 Google Speech Recognition을 사용하도록 하겠습니다.​sr.py 파일​speech_recognition 클래스를 임포트합니다. 사운드 인식 객체를 생성하고 오디오 파일을 메모리에 로드 한후 recognize_google()함수를 이용하여 텍스트로 전환합니다. 전환된 텍스트는 print() 함수로 출력하면 되겠지요? import speech_recognition as sr filename = ""soundRec.wav""# 사운드 인식 객체 생성r = sr.Recognizer()# 파일 열기with sr.AudioFile(filename) as source:    # 오디오 파일을 메모리에 로드    audio_data = r.record(source)    # 음성파일을 텍스트로 전환    text = r.recognize_google(audio_data)    print(text)  # 텍스트로 출력 ​실행​실행하면 음성 파일을 읽어서 문자열로 보여줍니다. ​  ​​​ "
"martedi, 28 marzo ",https://blog.naver.com/saintelp/223057626992,20230328,"Titoli AI da tenere d'occhio mentre Big Tech migliora i prodotti con l'intelligenza artificiale​=AI stocks to watch as Big Tech improves products with AI​ Titoli AI da tenere d'occhio mentre Big Tech migliora i prodotti con l'intelligenza artificiale​​REINHARDT KRAUSELun 27 marzo 2023 22:41 GMT+9​​I titoli di intelligenza artificiale sono più rari di quanto si possa pensare tra il ronzio sulla tecnologia dei chatbot come GPT-4. Molte aziende promuovono iniziative di tecnologia AI e machine learning. Ma ci sono davvero pochi titoli AI pubblici e puri.​=Artificial intelligence stocks are rarer than you might think amid buzz over chatbot technology such as GPT-4. Many companies tout AI technology initiatives and machine learning. But there really are few public, pure-play AI stocks.-------​​In general, look for AI stocks that use artificial intelligence to improve products or gain a strategic edge. Amid a surge in investor interest in artificial intelligence, be on guard against poor performing companies that tout themselves as plays on AI technology.Despite the banking crisis, venture capital is flowing to AI startups. Andreessen Horowitz led a $150 million funding round for Character.AI.Chip maker Nvidia (NVDA) at its GTC conference announced a wide-ranging portfolio of AI products, including new graphics processing units, data center hardware, AI software models and AI as a service.  NVDA stock belongs to the IBD Leaderboard..Microsoft (MSFT) has increased its stake in OpenAI as it aims to take on Google-parent Alphabet (GOOGL) in internet search and office productivity tools.In addition, OpenAI's ChatGPT is only one of many ""generative AI"" technologies that could roil a host of industries by creating text, images, video and computer programming code on their own. Generative AI technology already is finding applications in marketing, advertising, drug development, legal contracts, video gaming, customer support and digital art.As of now, analysts say Microsoft seems to be winning a public relations war versus Google on artificial intelligence initiatives and investments.Meanwhile, OpenAI on March 14 launched its next-generation chatbot technology. It's called GPT-4. The new language model is multi-modal. That means it accepts text, speech, images and video as inputs. IBD readers can check it out at OpenAI's website.AI Stocks: Acquisitions, InvestmentsChina's Baidu (BIDU) launched its Chat-GPT equivalent on March 16. Meanwhile, Adobe (ADBE) on March 21 unveiled generative artificial intelligence services for creative professionals and marketers. They include Adobe Firefly, a new family of creative generative AI models focused initially on image generation and text effects.""Companies who don't truly embrace generative will see their multiple compress by 50% over the next five years,"" RBC Capital report said in a recent note to clients. ""We believe every technology company needs a strategy to truly embrace generative AI, otherwise they will be left behind by those that do.""The report went on to say: ""Not only will these companies see market share losses over time, but they will also see multiple compression as investors lose confidence in the ability of those companies to be future-proof.""The generative AI wars are heating up in marketing. Salesforce (CRM) on March 7 rolled out Einstein GPT, which adds OpenAI's features across its software platform. Pilot technology will be available first on its Slack messaging tools. Salesforce has used predictive AI tools since 2016.Best AI StockBank of America, Morgan Stanley and Barclays tout chip maker Nvidia and Arista Networks (ANET) as top AI stocks. Internet data centers will need more computing power and network bandwidth to process AI workloads.""We see ChatGPT and the surging AI use cases akin to the 2007 iPhone introduction that expanded the mobile landscape and use cases for consumers and businesses,"" said a Morgan Stanley report on AI stocks..Barclays also picks Sprout Social (SPT), Sprinklr (CXM), Iron Mountain (IRM) and Seagate Technology (STX). BofA likes Taiwan Semiconductor Manufacturing (TSMC), Adobe, Shutterstock (SSTK) and online advertising firm Appier Group (APPIF).Prior to ChatGPT's launch in November, IDC predicted that the conversational AI market will grow at a 37% compound annual growth rate from $3.3 billion in 2021 to just over $16 billion in 2026. Generative AI is expected to impact cybersecurity.Artificial Intelligence 'Table Stakes'""We see (generative) AI becoming 'table stakes' for most software companies,"" Evercore ISI analyst Mark Mahaney said in a report. ""This generally favors the bigger companies with deeper pockets and access to more data.""Key to the rise of generative AI are improved natural language processing models that help computers understand the way that humans write and speak. OpenAI is part of a wave of NLP startups that includes AI21 Labs, Anthropic, Cohere and others. Anthropic has introduced a competitor to ChatGPT called ""Claude.""All AI software needs computing power to find patterns and make inferences from large quantities of data. And the race is on to build AI chips for data centers, self-driving cars, robotics, smartphones, drones and other devices.In addition, Bank of America is bullish on AI and internet companies.""Use of AI will be critical driver of all things Internet, including content relevance, ad performance, e-commerce conversion, marketplace efficiency and even customer service,"" BofA analyst Justin Post said in a recent note to clients.Nvidia Among AI Stocks To WatchCloud service providers are expected to hike investments in artificial intelligence technology. Nvidia beat Wall Street's estimates for its fiscal fourth quarter as data center chip sales rose 11% to $3.62 billion.Nvidia provides software development tools to build artificial intelligence applications. Rival Intel (INTC), meanwhile, aims to catch up in AI development tools.Nvidia faces more competition from AI chip startups Cerebras, Sambanova and Graphcore. Advanced Micro Devices (AMD) is ramping up AI initiatives as well.Some companies have been aggressive making AI acquisitions. IBM (IBM) has bought at least five artificial intelligence companies since mid-2020. They include Databand.ai, Turbonomic, ReaQta, MyInvenio and WDG Automation.Alphabet's AcquisitionAlphabet recently acquired Alter for $100 million, an AI avatar startup that enables brands and creators to express virtual identities. The acquisition is aimed at helping Google ramp up its content offerings and compete with other platforms like TikTok.For many companies, gaining an edge with AI requires ongoing investments in compute, networking and data center infrastructure.AI usage is exploding in facial and voice recognition technology, medical diagnostics, algorithmic trading, and automated customer service bots.The top artificial intelligence stocks to buy span chip makers, enterprise software companies and technology giants that utilize AI tools in many applications. Think of cloud computing giants Amazon.com (AMZN), Microsoft and Google.Tech Giants Among Best Artificial Intelligence StocksAlso, cloud computing giants sell AI analytical services to business customers.Amazon itself uses AI to customize online retail offerings and recommend products to website visitors. The e-commerce behemoth also uses robotics and AI at its fulfillments centers.Further, Amazon leverages AI in retail stores, noted a recent Monness, Crespi, Hardt and Co. report to clients. More than 30 Amazon Fresh U.S. stores, over 25 Amazon Go U.S. stores and two Whole Foods Market stores use Just Walk Out payment technology.Google, of course, uses AI to better parse complex search prompts, helping it to deliver relevant advertising and web results. Plus, Google uses AI tools in digital advertising.Meanwhile, Salesforce rolled out new AI-based tools at its Dreamforce customer conference in September.Top AI Stock: Software Market KeyVenture capitalist Marc Andreessen once observed how ""software is eating the world"" by remaking industries through automation. In the same way, artificial intelligence is expected to modernize software.Amid a shortage in software engineers, low-code programming tools are making it easier for business units to develop AI applications. DataRobot is part of a new wave of AI startups bringing low-code tools to market.Meanwhile, Snowflake (SNOW) and startups such as Databricks aim to shake up the database market with lightning-fast analysis of ""unstructured data"" gathered from sensors. One example would be streaming video.Databricks announced new contributions to multiple opensource projects at its recent AI SummitStill, corporate adoption of AI technologies is nascent. The majority of organizations are still experimenting with AI technology, said an Accenture (ACN) study. Only 12% are using AI tools at a maturity level that achieves a strong competitive advantage, according to Accenture.But the AI software market is expected to jump 21.3% to $62.5 billion in 2022, forecasts market research firm Gartner. The research group adds the worldwide AI semiconductor market will grow to more than $70 billion by 2025, up from $23 billion in 2020.Artificial Intelligence Stocks: IBM Sells Watson HealthNot every effort succeeds. IBM (IBM) in January sold off Watson Health to private equity firm Francisco Partners. The deal reportedly came in above $1 billion. But IBM had invested much more in Watson. Despite the Watson setback, IBM continues to acquire AI startups.AI tools are playing a big role in Facebook-parent Meta Platforms (META) legacy business and new initiatives. As it moves into the ""metaverse,"" Meta said it has built a new artificial intelligence supercomputer. Called the AI Research Supercluster, the Meta computer uses chips from Nvidia.Also, Apple (AAPL) continues to build up artificial intelligence assets. It hired former Google scientist Samy Bengio, who left the internet search giant amid turmoil in its artificial intelligence research department.Artificial Intelligence Stocks Span Chips, Software, Internet GiantsMeanwhile, Microsoft in April 2021 acquired speech recognition software maker Nuance Communications (NUAN), whose artificial intelligence tools are widely used in the health care market. In addition, Microsoft aims to deliver Nuance AI tools to health care customers via its Azure cloud computing platform.Microsoft, Google and Nvidia have dropped off the IBD Leaderboard, which is IBD's curated list of leading stocks that stand out on technical and fundamental metrics.AI technology uses computer algorithms. The software programs aim to mimic the human ability to learn, interpret patterns and make predictions.""Machine learning"" is the most widely used form of AI deployed in industries. Machine learning systems use huge troves of data to train algorithms to recognize patterns and make predictions.""AI workloads are classified as training or inference,"" Oppenheimer analyst Rick Schafer said in a recent note. ""Training is the creation of an AI model through repetitive data processing/learning. Training is compute-intensive, requiring the most advanced AI hardware/software. Generally located in hyperscale data centers, we estimate training total addressable market at $21 billion by 2025.""Software Companies Integrate AI ToolsAI companies to watch include information technology services firms such as IBM, Accenture, and Epam Systems (EPAM).Research firm IDC estimates that IBM, Accenture and Infosys hold 28% of the $17 billion artificial intelligence IT services market, said a Susquehanna Financial Group report.In addition, software companies are among artificial intelligence stocks to watch. Many software-as-a-service companies use AI tools.Bank of America recently upgraded Palantir (PLTR) to buy citing its AI prowess.Digital media and marketing software maker Adobe at a conference strutted out cloud-based tools that will allow companies to better personalize content for customers on a large scale. "
[Course2/Week3]Consider accessibility during user research ,https://blog.naver.com/helen_kang87/222589606065,20211208,"Accessibility is the design of products, devices, services, or environments for people with disabilities. Designing for accessibility is about considering all users’ journeys, keeping their permanent, temporary, or situational disabilities in mind. By researching how people with disabilities interact with products like yours, you can better understand how to design for them. It’s not possible to accurately guess all the ways that a user might experience your product, which is one reason why including people with disabilities in your research is so important. Here are a few considerations for you to take into account when conducting research during the empathize phase of the design process. ranging from Permanent (one arm), Temporary (arm injury), and Situational (new parent)Touch: How would you design for users who have use of one arm, either permanently, temporarily, or situationally?Decide where to place buttons within your design based on several different hand sizes.Create a feature that allows double taps to avoid accidental icon clicks.Enable the one-handed keyboard feature and general keyboard compatibility.Allow button customization for easy access to information that the user finds most important. Ranging from Permanent (blind), Temporary (wears glasses), to Situational (distracted driver)See: How would you design for users who have limited vision, either permanently, temporarily, or situationally?Use a larger font to create a reader-friendly design of the app.Ensure the app and the images have alternate text that can be read by a screen reader.Detect whether the user is operating a motor vehicle.Design the app with high contrast colors.Don’t rely on text color to explain navigation or next steps. For example, don’t use red text alone as an indicator of a warning. Instead, your design should include explicit instructions.Customizable textThere are some additional web accessibility tools that individuals with dyslexia or other visual processing disabilities may benefit from. One of these ways is customizable text, a feature that allows users to change how text is displayed in order to read the text more easily.  Text customization involves changing everything from the color or font to the size or even the spacing of the text. For example, some fonts may be easier to read than others for users to read, so customizing fonts could be a great help. Therefore, customizable text allows more options than simply magnifying the text or zooming in, making the content more adaptable yet maintaining the functionality.  Ranging from permanent (deaf), temporary (ear infection) to situational (bartender)Hear: How would you design for users who have limited hearing, either permanently, temporarily, or situationally? Don’t rely solely on sounds to provide app updates, like a new message notification. Instead, enable haptics, which are vibrations that engage a user’s sense of touch, and notification lights.Apply closed captioning to all videos. Provide a text messaging system within the app to allow users to communicate through writing. Ranging from permanent (nonverbal), temporary (laryngitis), to situational (non-native speaker)Speak: How would you design for users who cannot speak, either permanently, temporarily, or situationally? Provide written intros, descriptions, and instructions for users, in addition to video-based content.Provide Real-Time Texting during phone calls with users or with app support.Arrange alternatives for automated systems that rely on speech recognition.Provide an in-app messaging system that allows the use of emojis and image uploads.This list is just a small fraction of the considerations you should take into account when designing for users with disabilities. The best way to learn about how to improve your designs is to conduct research and get feedback from people with disabilities directly.Assistive technologiesCreating a product design that takes every range of ability into account can be incredibly difficult. Thankfully, there are services and apps designed to help bridge the gap. For example, one service that’s recently been added by phone providers is Real-Time Texting, which lets users text during a phone call to improve communication. Another feature that helps people with disabilities is alternative text. People who have low vision or are blind often rely on screen readers to read the content on their screens aloud. But, if your informational icons don’t have labels or alternative text, the screen reader can’t describe the functionality of that button to the user. Not every image or icon is informational, so only include descriptions when necessary. Researching and learning about assistive technology will help you better understand the impact these technologies have. Here are a few links to help you get started: Discussing the value of disability technology: Disability Technology from TEDx Talks on YouTubeHow accessibility in technology changes lives: How the Blind Use Technology to See The World from TEDx Talks on YouTubeAn explanatory guide for phone accessibility: Smartphone accessibility: a comprehensive guide from Uswitch Google’s accessibility information hub homepage Putting inclusive design into practiceTo put inclusive design into practice, it’s important to immerse yourself in the assistive technology that people with disabilities might use and to have conversations with them about their experiences. The best way to design empathetically for the world around you is to engage with your users and ask them questions about the way your designs could help them succeed.To learn more about inclusive design, here are some additional resources to get you started:Describing ways to design products inclusively: Inclusive Design: 12 Ways to Design for Everyone from Shopify Google’s accessible approach to inclusive design: An Accessible Process for inclusive Design from Google I/OBreaking down important inclusive design principles: 6 Principles for Inclusive Design from UX Planet "
Chat GPT #5 ChatGPT 관련 질문 ,https://blog.naver.com/freefromhere/223038203053,20230308,"질문Could you please let me know what the main services of ChatGPT are?답변Certainly! As a language model, ChatGPT is capable of providing various services, such as:​Language translationText summarizationText generationSentiment analysisQuestion answeringLanguage modelingChatbot developmentLanguage understandingSpeech recognition and synthesisThese are some of the main services that ChatGPT can offer, among others. Let me know if you have any further questions!​질문Can you suggest some good questions to use with ChatGPT?답변Sure, here are some good questions you can ask ChatGPT:​Can you tell me a joke? -> 영어라 이해하기 어려움What's the weather forecast for tomorrow in New York City? -> AI 모델이라 말해주기 힘듬.Who is the author of the book ""To Kill a Mockingbird""?Can you give me a summary of the plot of the movie ""The Godfather""?How can I improve my writing skills?Can you recommend a good restaurant in London?What are the current trends in artificial intelligence?How can I reduce my carbon footprint?What are some effective study techniques for exams?Can you help me generate a creative business name?These are just a few examples, but ChatGPT can assist you with a wide variety of questions and tasks. Just ask away! "
Global Automotive Voice Recognition Market: Analysis & Forecast 2019-2024   ,https://blog.naver.com/bharatbook/221459299430,20190206,"Bharat Book Bureau Provides the Trending Market Research Report on “Global Automotive Voice Recognition Market 2019 by Company, Regions, Type and Application, Forecast to 2024”under Automotive Category. The report offers a collection of superior market research, market analysis, competitive intelligence and Market reports.Global Automotive Voice Recognition Market 2019Voice Recognition is the identification of a person from characteristics of voices. Voice recognition converts speech from a recorded audio signal to text. Humans convert words to speech with their speech production mechanism. An voice recognition aims to infer those original words given the observable signal.Scope of the Report:In the recent years, the development of automotive voice recognition market is quite fast due to the fast increase of the amount of global vehicle production.Request a free sample copy of Automotive Voice Recognition Market Report @ https://www.bharatbook.com/marketreports/Sample/Reports/1361875   Nuance is still the largest player in the automotive voice recognition market, occupying market share more than 70%. It sells millions of products to automotive manufacturers and automotive multimedia system supplier.In fact, the automotive voice recognition system is generally integrated in the automotive multimedia system. Now, more and more automotive voice recognition can recognize several languages to meet drivers’ demand.Thanks to the fast development of automotive industry, the development of automotive voice recognition market is promising in the next several years, especially in developing regions like China.The global Automotive Voice Recognition market is valued at 180 million USD in 2018 and is expected to reach 220 million USD by the end of 2024, growing at a CAGR of 3.8% between 2019 and 2024.The Asia-Pacific will occupy for more market share in following years, especially in China, also fast growing India and Southeast Asia regions.North America, especially The United States, will still play an important role which cannot be ignored. Any changes from United States might affect the development trend of Automotive Voice Recognition.Europe also play important roles in global market, with market size of xx million USD in 2019 and will be xx million USD in 2024, with a CAGR of xx%.This report studies the Automotive Voice Recognition market status and outlook of Global and major regions, from angles of players, countries, product types and end industries; this report analyzes the top players in global market, and splits the Automotive Voice Recognition market by product type and applications/end industries.Market Segment by Companies, this report covers- Nuance- VoiceBox- Iflytek- Fuetrek- Sensory- AMI- LumenVoxMarket Segment by Regions, regional analysis covers- North America (United States, Canada and Mexico)- Europe (Germany, France, UK, Russia and Italy)- Asia-Pacific (China, Japan, Korea, India and Southeast Asia)- South America (Brazil, Argentina, Colombia)- Middle East and Africa (Saudi Arabia, UAE, Egypt, Nigeria and South Africa)Market Segment by Type, covers- Single language recognition- Multilingual RecognitionMarket Segment by Applications, can be divided into- Passenger Vehicle- Commercial VehicleBrowse our full report with Table of Contents : https://www.bharatbook.com/marketreports/global-automotive-voice-recognition-market-2019-by-company-regions-type-and-application-forecast-to-2024/1361875About Bharat Book Bureau:Bharat Book is Your One-Stop-Shop with an exhaustive coverage of 4,80,000 reports and insights that includes latest Market Study, Market Trends & Analysis, Forecasts Customized Intelligence, Newsletters and Online Databases. Overall a comprehensive coverage of major industries with a further segmentation of 100+ subsectors.Contact us at:Bharat Book BureauTel: +91 22 27810772 / 27810773Email: poonam@bharatbook.comWebsite: www.bharatbook.comFollow us on : Twitter, Facebook, LinkedIn, Google Plus "
Top 10 Applications of Machine Learning ,https://blog.naver.com/yoono044/223022941784,20230221,"​Machine Learning is a powerful technology that is transforming industries and has numerous applications. From healthcare to finance, e-commerce to marketing, Machine Learning is being used to solve complex problems and improve decision-making. In this article, we will explore the top 10 applications of Machine Learning.​​ https://i.pinimg.com/originals/9a/d5/1e/9ad51e0280807f9d7865caca54372bda.png​1. Fraud Detection​One of the most significant applications of Machine Learning is fraud detection. Machine Learning algorithms can analyze large amounts of data and identify patterns that indicate fraudulent behavior. Banks, credit card companies, and insurance companies use Machine Learning to detect fraud and prevent losses.​2. Predictive Maintenance​Predictive Maintenance is another application of Machine Learning. Machine Learning algorithms can analyze sensor data from machines and predict when maintenance is needed. This can help companies reduce downtime and increase the lifespan of their equipment.​3. Customer Experience​Machine Learning is also being used to improve customer experience. Machine Learning algorithms can analyze customer data and make personalized recommendations based on that data. This can help companies improve customer satisfaction and increase sales.​4. Decision-Making​Machine Learning is being used to improve decision-making in various fields, including healthcare, finance, and marketing. Machine Learning algorithms can analyze data and make predictions based on that data. This can help companies make better decisions and reduce the risk of making costly mistakes.​5. Self-Driving Cars​Self-driving cars are one of the most exciting applications of Machine Learning. Machine Learning algorithms can analyze sensor data from cars and make decisions based on that data. Self-driving cars have the potential to reduce accidents, increase mobility, and improve the overall transportation system.​6. Natural Language Processing​Natural Language Processing is another application of Machine Learning. Machine Learning algorithms can analyze text and make predictions based on that text. Natural Language Processing is being used in various fields, including customer service, chatbots, and voice assistants.​7. Image and Speech Recognition​Machine Learning is being used for image and speech recognition. Machine Learning algorithms can analyze images and speech and make predictions based on that data. Image and speech recognition have numerous applications, including security systems, medical imaging, and virtual assistants.​8. Personalization​Machine Learning is being used to personalize products and services. Machine Learning algorithms can analyze customer data and make personalized recommendations based on that data. Personalization can help companies increase customer satisfaction and loyalty.​9. Cost Savings​Machine Learning is being used to reduce costs in various fields, including healthcare, finance, and manufacturing. Machine Learning algorithms can analyze data and identify areas where costs can be reduced. This can help companies increase profitability and competitiveness.​10. Research​Machine Learning is being used in research to analyze large amounts of data and identify patterns and relationships. Machine Learning algorithms can help researchers make new discoveries and advance their understanding of various fields, including biology, physics, and social sciences.​Conclusion​In conclusion, Machine Learning has numerous applications and is transforming industries. From fraud detection to self-driving cars, Machine Learning is being used to solve complex problems and improve decision-making.​It is essential to stay up-to-date with the latest developments and techniques in Machine Learning to leverage its potential. With the vast amount of data being generated every day, Machine Learning is becoming more critical than ever, and understanding its applications is essential to improve business operations and research.​​ "
"<파이썬으로 배우는 음성인식>(다카시마 료이치 지음, 정권우 옮김)_비제이퍼블릭_서평 ",https://blog.naver.com/reading-star100/223009625284,20230208,"파이썬으로 배우는 음성인식 저자다카시마 료이치출판비제이퍼블릭발매2023.01.09.   파이썬은 모르더라도 음성인식은 우리의 일상에 너무 깊숙이 들어와서 잘 알 것 같다. 원리로서의 음성인식이 아니라 음성인식을 활용한 다양한 기술을 잘 활용해야 한다고 해야 할까? 예를 들면 사람이 기계에게 말을 걸거나 기계가 사람의 말을 듣고 문자로 변화해 주는 것(음성인식 노트 앱 포함) 등 모두 음성인식 기술을 사용한 것이다. ​책 앞표지를 보니 음성인식의 기술 발전 동향을  알려줄 뿐만 아니라 딥러닝 실습까지 하는 실천서이다. 음성인식의 기초부터  최신 기법까지 차근차근 배우기에 적합한 책이다. ​뒤표지에는 이 책의 구성이 일목요연하게 정리되어 있고, '음성인식의 핵심 원리를 파헤치고 실습까지 한 권으로 완성'할 수 있다고 강조하였다.​저자는 다카시마 료이치이다. 2013년 고베대학 대학원을 나왔고 시스템 정보학 연구과 박사 후기 과정을 수료한 공학박사이다. 옮긴이 정권우는 현재 네이버 파파고 팀에서 딥러닝을 통해 더 나은 번역기를 개발하고 있다. 옮긴이의 말에 의하면 ""딥러닝을 이용한 최신 음성인식 시스템을 직접 개발해 보는 것이 이 책의 목표""이다.   서문과 목차부터 살펴보고 차근차근 배워보도록 하자. 이 책의 목적은 간단하다. 독자가 음성인식 기술의 태성부터 현재까지의 기술 발전의 흐름을 배우고 딥러닝 기반의 최첨단 음성인식 시스템을 직접 구현하도록 돕는 것이다...실습 과제를 하나씩 재현해 보면서 음성인식 기술의 태서부터 점차 최신 기법으로 탈바꿈해나가는 과정을 한 편의 이야기를 읽는 듯한 느낌으로 고스란히 전달하고자 한다.서문 중에서 Python 3.7.3​파이토치 설치: https://pytorch.org/​이 책에서 소개하는 소스 코드 URL: https://github.com/bjpublic/python_speech_recognition​저자는 음성인식과 관련된 기초 지식을 쉬운 예로 차근차근 설명하였고, 3장부터는 직접 파이썬으로 소스 코드와 더불어 실습해 보면서 그 과정을 설명하였다. 음성 특징값 추출 방법에 대해 설명하고, 이를 이용해 음성 유사도를 측정하고 음성인식을 실행하는 부분을 설명하였다. ​음성인식에 활용되는 다양한 기법과 모델을 시간 흐름의 순으로 정리하며 그 원리와 장단점을 하나씩 짚어가면서 다음 순서로 넘어가는 방식을 취하여 독자가 이해하기 쉽다.  ​이 책을 음성인식 기술의 전반적인 흐름과 특징을 이해하고, 음성인식 기술 적용에 관심 있는 독자들에게 추천한다.   1장 음성인식이란?1장에서는 음성인식은 무엇이며, 어디에 사용되는지, 음성인식의 원리는 어떤 것인지에 대해 설명하였다. 음원 분리, 음원 음식, 자연어 처리 등 AI 스피커의 내부 처리 과정을 이해할 수 있었다. 소리 인식은 '지각 단계'와 '인식 단계'로 구분됨을 알 수 있고, 기계가 수행하는 음성인식 처리 과정, 음식인식 모델 내부 처리 과정 등을 이해할 수 있다. 최근 들어 음성인식의 정확도가 획기적으로 향상된 것은 딥러닝 모델로 전환되면서부터였고, 최근에는 End-to-End 모델이 음성 인식 최신 기법으로써 주목받고 있음을 알 수 있었다. 기계가 이러한 기능을 자연스럽게 수행하기 위해서는 사람이 발화한 음성을 듣고 어떤 의미인지를 올바르게 인식해야 한다. 이와 같이 발화 내용(음성 신호)를 인식하는 기술을 통틀어 음성인식 기술이라 한다.2쪽사람의 지각 단계를 모방한 처리 과정을 '특징 추출'이라 한다. 사람의 지각 단계에서 소리 진동 파형이 와우 기저막에서 주파수별로 소리 세기에 따라 분해되듯 특징 추출도 유사한 방식으로 처리되며, 일반적으로 '푸리에 변환'이라는 신호 처리 기법을 이용한다.5쪽실제로 기계가 학습하는 것은 입력 음성과 각 인식 결과 후보 확률을 계산하기 위한 계산 규칙이며, 이 계산 규칙을 '음성인식 모델'이라 한다.6쪽2장 음성인식 기초 지식2장에서는 음성인식에 대해 알아야 할 기초 지식을 설명하였다. '음성인식과 확률'을 설명하고,  '음성인식 문제를 수식으로 정의'하였다. 텍스트 종류와 발음 사전에 대해 설명하였고,  음성인식 실험 두 가지를 소개하였다. 쉬운 예로 확률, 확률의 연쇄 법칙, 확률의 주변화 등에 설명하였다.  그리고 고립 단어를 어떻게 인식하고 연속 음성을 어떻게 인식하는지, 음성인식 실험 프로세스를 설명하였다.  앞에서 언급한 자루 예시는 어떠한 추가 정보 없이 단순히 과거 결과만을 고려한 확률로, 이를 '사전 확률'이라 한다.12쪽어떤 사건을 관측한 후에 고려하는 확률을 '사후 확률' 혹은 조건 x를 전제로 사건 y가 발생하는 '조건부 확률'이라 한다.13쪽일부 조건에 대한 동시 확률을 모두 더하게 되면 나머지 조건의 확률로 표현할 수 있다. 14쪽3장 음성 처리 기초와 특징 추출 3장에서는 파이썬 소스 코드와 함께 설명하였다.데이터를 준비하고, 음성 파일을 읽어보고,  푸리에 변환으로 음성을 주파수 분해하고,  음성을 단시간 푸리에 변화하여 스펙트럼을 생성하는 방법 등을 설명하였다. 이외  로그 Mel Fiter Bank의 특징, Mel 주파수 켑스트럼 특징을 설명하였고,  이 특징의 평균과 표준편차를 계산해 보게 하였다. ​이 책에서 사용하는 데이터 셋 주소:  Shinnosuke Takamichi (高道 慎之介) - JSUTJSUT (Japanese speech corpus of Saruwatari-lab., University of Tokyo)sites.google.com 샘플링 주파수가 클수록 세밀한 음성을 기록할 수 있으므로 그만큼 정보량이 많아지지만, 일반적인 음성인식 실험에서는 샘플링 주파수 16kHz 음성을 취급한다... 이를 위해 48kHz 음성 데이터를 16kHz 음성 데이터로 변환한다.(다운 샘플링 처리)27쪽마이크의 개수를 채널 수라고도 한다. 채널수가 1인 음성은 모노 음성, 채널 수가 2인 음성은 스테레오 음성으로 부른다. 일반적으로 음성인식에서는 채널 수가 1인 음성을 사용한다.30쪽디지털시계가 1초마다 시간을 표시하는 것과 같이, 연속치를 일정 간격으로 구분하여(샘플링이라 한다) 취급하기도 한다. 31쪽음성 파일에 대한 3가지 정보:1. 채널 수: 음성을 녹음하는 마이크 개수, 이 책에서는 채널 수가 1인 음성을 취급한다.2. 샘플링 주파수(Hz): 1초 동안 샘플링하는 음압치 개수. 이 책에서는 샘플링 주파수 16,000Hz 음성을 다룬다. 3. 샘플 사이즈(비트): 음압치를 이산치로 변환할 때 사용하는 눈금 개수. 이 책에서는 샘플링 사이즈 16비트 음성을 취급한다. 32쪽입력된 파형 전체를 대상으로 분석 결과에 영향을 끼치지 않을 정도의 매우 작은 노이즈 값을 추가하는 방법이 있으며 이를 디더링 처리라 한다.51쪽녹음 환경에 따라서는 녹음기에 전기적 노이즈가 추가되는 등 평균값이 0에서 벗어난 값이 되기도 한다. 이 영향을 제거하기 위해 단시간 푸리에 변환을 적용할 때 프레임별로 진폭값 평균을 빼는 작업을 하는데, 이를 직류 성분 제거라 한다.51쪽일반적으로 음성이 입술에서 마이크로 도달하는 과정에서 고주파 영역일수록 음성 세기가 빠르게 감소한다. 그래서 감소한 만큼의 음성 세기를 보상하기 위해 고역 강조라는 전처리를 한다.52쪽필터 뱅크 분석은 진폭 스펙트럼 차원을 제거하는 방법 중 하나로, 원리는 매우 간단하다. 일정 범위의 주파수 성분 값을 하나로 취합하여 주파수 성분 개수를 줄이는 것이다.54쪽Mel 필터 뱅크 분석은 사람 청각 특성에 근거하여 취합하는 필터 방법이다.56쪽소리 파워 스펙트럼으로부터 성도 공진 특성에 대한 정보를 추출하는 방법이 바로 켑스트럼 분석이다.69쪽4장 음성인식 첫걸음 DP Matching4장에서는 음성인식의 기본인 DP  Matching에 대해 설명하였다. 초창기 음성인식 방법론, 발화 속도 차이에 효과적으로 대응하는 방법론으로 2020년대 현대 음성인식에는 사용하지 않는다.음성인식에서 떼어놓을 수 없는 정렬(얼라인먼트) 문제를 언급하며,  얼라인먼트를 추정하면서 거리를 계산하는 기법 DP Matching에 대해 설명하고  구현해 보았다. 동일한 발화라도 길이와 속도에 따라 변동하므로, DP Matching을 이용한 인식 기법을 사용한 것이다. 음성 전체 길이는 물론이고 음성마다 단어들의 발화 속도도 서로 다르다. 이를 음성의 ""시간적 흔들림""이라 한다.87쪽음성 간 유사도를 측정하기 위해서는 각 음성의 프레임이 서로 대응하는지를 파악해야 하는데, 이러한 대응 관계를 ""얼라인먼트""라 한다. 87족5장 GMM-HMM 기반 음성인식5장에서는 혼합 정규 분포와 은닉 마코프 모델 기반 음향 모델을 설명하였다. 1980~2010년까지 음성인식을 지탱해온 기술이다.DP Matching을 이용한 인식 기법의 실제 음성인식 서비스에 적용할 때 2가지 큰 문제로 템플릿 방식을 대신할 분포와 빈도의 관점을 내놓았다.   정규분포와 최빈 추정법을 활용한 매개변수를 추정하는 법에 대해 설명하였다. 다만 정규분포가 알맞은 분포이지만, 근사 오차는 피할 수 없으므로 이 근사 오차를 줄이기 위해 분포보다 더 정밀한 기법을 설명하였다. 즉 혼합 정규분포와 EM 알고리즘에 대한 설명이다.이외  은닉 마코프 모델(HMM)에 대해 소개하였고 얼라인먼트를 자동으로 설명하는 GMM-HMM과 비터비 알고리즘 조합이 자주 사용된다고 하며 실제 구현하여 실험을 수행하는 과정을 보여주었다.  DP-Matching의 한계1. 미리 준비된 템플릿과 일치하는 발화 내용만을 인식할 수 있다.2. 음성 불규칙성(다양성)에 대한 대응이 어렵다.104쪽음성 다양성에 대한 대응 능력이 현저히 떨어지는 템플릿 방식을 대신하여 실용적으로 활용하는 개념이 바로 ""분포""와 ""빈도""다.105쪽우리는 모든 데이터를 상세 조건 z에 분배하여 각각의 정규분포 빈도를 독립적으로 최대화하는 방법을 취하지 않는다. 대신 데이터를 분배할 수 있는 모든 패턴들과 그 확률을 고려하여 GMM 분포 전체 빈도를 최대화한다. 이런한 방법은 빈도 기댓값을 최대화하는 것에 해당하므로 EM 알고리즘이라 부른다. 116쪽6장 DNN-HMM 기반 음성인식6장에서는 딥러닝을 기반으로 한 음향 모델 중에서 실제 상용화된 제품에 가장 흔하게 적용되는 기법이라고 했다. GMM 대신 DNN를 사용해 성능을 향상시킨 것을 보여주었다. 이런 방식 때문에 음성인식 서비스가 널리 보급되기 시작했다. DNN이 등장하게 된 계기, 기본개념, 학습 방법, 학습에 활용되는 기술(Optimizer, DNN 매개변수 초기화)   등에 대해 설명하였다. 나아가 DNN과 HMM을 조합한 DNN-HMM 하이브리드 시스템을 소개하고 파이썬과 파이토치로 구현해 볼 수 있도록 하였다. GMM의 한계1. 생성 모델의 음성 식별 능력 한계2. GMM 기반 다양한 음성에 대한 근사 한계188쪽7장  End-to-End 모델 기반 연속 음성인식7장에서는 이 책의 핵심 주제인 End-to-End 모델에 대해 설명하였다.  기존 음성인식 모델을 단 하나의 DNN으로 모델링 한 것이 바로 End-to-End 모델인데, 구현도 쉬워서 향후 발전 가능성 매우 높다고 하였다.  직접 파이썬과 파이토치로 구현해 보고 활용해 보면 좋을 것 같다.  E2E 모델에는 다양한 종류가 있지만 이 책에서는 CTC와 Attention 모델을 소개한다... 모두 순환 신경망이라는 DNN을 사용하여 입력 배열의 시간적 의존 관계와 출력 배열의 문맥 정보를 모델링 한다. 248~249쪽  Yes24 리뷰어 클럽 서평단에 선정되어, 출판사로부터 책을 제공받아, 완독하고 쓴 솔직 서평입니다. "
언어의 아이들  ,https://blog.naver.com/jyace0713/221526394036,20190430,"아이들은 어떻게 언어를 배우는 걸까요?저는 둘째 아이가 5살때 싱가포르에 처음 왔는데 한국말을 곧 잘 하던 아이가 여기서 중국어와 영어를 하면서한국어 발음이 좀 뭔가 어색한 부분이 생겨서 깜짝 놀랐습니다.그 후 다시 한국으로 돌아가서 초등 3,4 학년을 다녔는데 가서는 또 금방 영어를 빛의 속도로 잃어버리고한국어를 너무 잘하는 모습을 보고 아이들은 정말 스폰지처럼 주변의 환경을 흡수한다는 생각이 들었습니다.​해외에서 살다보면 아이들이 영어로 공부를 하게되고 한국어를 쓸 기회가 많지 않아 말을 잃어버리고 발음이 어눌해져서 그런 부분을 보완해주려고 한국어 책을 읽히고 한국어 수업을 듣게 하기도 합니다.한국에서는 반대로 늘 한국어로만 생활하고 수업을 하기 때문에 영어 학습에 대한 고민이 많습니다.아이들은 학원에서 터무니 없이 많은 영어단어를 외우기도 하고 너무도 자신의 수준에 맞지않은 어려운 원서를 읽으며 억지로 수업을 쫓아가기도 합니다. 가끔 한국식 영문법을 죽어라 가르치는 학원도 있습니다.( 그것이 영어 표현이나 영작으로 연결되기는 참 어려운) 오늘 우연히 신문기사로 접한"" 언어의 아이들"" 이라는 책을 보고 언어 학습에 관심이 많은 저에게는 꽤 흥미로운 제목이어서 책에 대해서 찾아보았습니다. ​아이들에게는 언어를 학습할 수 있는 타고난 언어학습 시스템이 있다고들 합니다.그것이 맞다면 최대한 일찍 외국어에 노출해주는 것이 좋겠지만모국어가 형성되어 있지 않은 상태에서 자칫 모국어와 외국어의 충돌로 인하여 아이들이 혼돈 스러워 할 수도 있고 실지로 많은 부작용으로 학습 장애나 학습 거부를 하는 아이들도 있다고 합니다.​저자는 60개월 이전에 아이들에게 두개의 언어를 노출했고 두 언어간의 혼동은 없었다고 합니다.아마 정확하지는 않지만  먼저 한국어가 모국어로 형성된 뒤에 자연스럽게 영어 소리에 노출된 환경이었을 듯 합니다.예전에 아이들에게 영어 도서 수업을 진행하다보면 부모님들께서 아이들이 예전에 영어 유치원을 다닐때보다실력이 줄었다고 이야기하는 경우를 종종 봅니다.영유 졸업 후 초등학교를 다니면 아무래도 한국어를 훨씬 더 많이 쓰게 되기 때문에 아이들이 언어의 노출이 줄어들어 영어 실력이 줄어들게 됩니다. 영어 유치원에서 단어와 문장을 무작정 외우게하여 실력을 향상시키게 되다면 그것은 일시적일뿐 지속되기는 어렵다는 생각이 듭니다.그 언어로 사고하고 그 언어로 표현할 수 있다면 그것이 진짜 아이의 언어실력이라고 할 수 있습니다.아이들이 시험공부하듯 그것을 암기하고 넘어간다면 반짝 효과를 볼 수 는 있지만 실지 그 언어를 써야할때 그 표현이 튀어나오지 않는 경우도 많습니다. 언어에 대한 많은 노출 경험, 사고하고 표현할 수 있는 기회 그것이 억지로가 아닌 아이들이 자연스럽고 재미있게 할 수 있는 그런 환경이 제공된다면 아이들은 이중 언어를 자유자제로 활용할 수 있는 아이들로 커나갈 것입니다. 저자가 신문 기사에서 소개한 대로 파닉스 이후에 단계별로 책을 접하며 그 안에서 다양한 표현과 문법을 기억할 수 있도록 도와주는 방법이 효과적입니다. 책을 보며 많은 아이디어와 교육 방향들을 잡아나가면 좋을 것 같습니다.​책소개영국 옥스퍼드 대학교 동아시아 학부(Faculty of Oriental Studies)에서 한국학과 언어학을 가르치고 있는 조지은(Jieun Kiaer) 교수는 서울 대학교 아동가족학과를 졸업하고 동 대학원에서 언어학 석사 학위를 받은 후 2007년 영국 킹스 칼리지 런던(KCL)에서 언어학 박사 학위(「Processing and interfaces in syntactic theory: the case of Korean」)를 받았다. 언어의 구조와 의미 연구를 수행하고 현재는 이중 언어 습득 관련 연구와 함께 국내외 학자들과 협력을 하고 있다. 국내에서 2014년 출간된 『한국어 속에 숨어 있는 영어 단어 이야기』, 영국에서 출간된 『화용적 통사론(Pragmatic Syntax)』, 『상호언어적 단어(Translingual Words)』 등을 펴냈다. ​현재 영국 UCL 연구원으로 재직 중인 송지은 박사는 서울 대학교 언어학과를 졸업하고 동 대학원에서 석사 학위를 받은 후 2018년 영국 유니버시티 칼리지 런던(UCL)에서 음성 과학 박사 학위(「The effects of adverse conditions on speech recognition by nonnative listeners: Electrophysiological and behavioural evidence」)를 받았다. 인간의 뇌파를 살펴봄으로써 말소리에 담긴 다양한 정보 처리에 관해 연구하는 한편 언어 배경이 다양한 사람들이 실질적 언어 환경에서 ‘의사 소통’이라는 목적을 달성하게 하는 언어적, 인지적 원리를 밝히려고 노력하고 있다. 음성학과 외국어 습득의 다양한 주제에 관해 국제 학술지에 논문을 게재하며 활발히 연구 중이다. ​언어는 배우는 것일까, 아니면 타고나는 것일까? 언어와 생각은 어떤 관계가 있을까? 태아에게 언어를 들려주는 태교는 실제로 효과가 있을까? 세상에는 얼마나 많은 소리가 존재하는 것일까? 2개 혹은 2개 이상의 언어를 모국어로 사용하는 사람들의 머릿속에는 어떤 일이 일어날까? 어린 나이에 외국어 공부를 시작하거나 외국에 가서 살지 않고도 외국어 학습의 효과를 높일 수 있는 방법은 무엇일까? 『언어의 아이들』은 4부에 걸쳐 아동 언어 발달, 음성학, 어휘와 문법, 이중 언어 습득이라는 큰 주제를 다루면서 다양한 관련 연구와 에피소드를 담고 있다.​​목차I 언어로 세상의 문을 여는 아이들배우는 것인가? 타고나는 것인가? | 무궁무진한 말의 세계 | 말은 못해도 다 알아들어요 | 언어의 문이 닫히다 | 한 발짝 한 발짝 | 님 침스키가 하는 말​II 소리의 세상으로제일 먼저 소리부터 | 우리말의 말소리 목록 | 먼저 익히는 소리, 많이 말하는 소리 | 함미, 함머니, 할머니 | 운율 익히기​III 아이들의 머릿속 사전새로운 단어가 만들어지다 | 구조와 의미를 만들고 이해하며 | 어휘 조사 프로젝트​IV 말 하나 더 배우기말이 많은 세상? | 여러 개의 언어를 습득하는 능력 | 대한민국에서 외국어 배우기 | 언어​https://news.naver.com/main/read.nhn?mode=LSD&mid=shm&sid1=103&oid=020&aid=0003214132 언어 자체에 ‘공포심’ 느낄 수도…영어 교육이 어려운 이유는?“유아기에 모국어처럼 영어를 익혀야 한다” “시간이 흐르면 실력이 비슷해진다”…. 영어 교육은 어렵다. 이 말도 맞고 저 말도 그럼직하다. 아이들은 도대체 어떻게 말을 배우는 걸까. 최근 ‘언어의 아이들’(사이언스북news.naver.com ​ "
"Talk to ChatGPT 영어회화 학습하기, ChatGPT에  마이크로  말하고 스피커로 듣기 ",https://blog.naver.com/smartmba/223037068229,20230307,ChatGPT에  마이크로  말하고 스피커로 듣기​https://chrome.google.com/webstore/detail/talk-to-chatgpt/hodadfhfagpiemkeoliaelelfbboamlk Talk-to-ChatGPTTalk to ChatGPT through your microphone and hear its responses with a voice. Uses speech recognition and text-to-speech technologieschrome.google.com https://www.youtube.com/watch?v=VXkLQMEs3lA Talk to ChatGPT through your microphone and hear its responses with a voice. Uses speech recognition and text-to-speech technologies​​ 
"딥러닝 레볼루션_AI시대, 어떻게 준비할 것인가_1 ",https://blog.naver.com/dnjs9985/222843196077,20220808,"딥러닝 레볼루션​​저자 Terrence Sejnowski​ 꽤나 유쾌하신 분이구나 룰루~​ 오궁오궁ㅋㅋ본인 책 읽는 대학동기 따님이 책은 꼭 읽어보고 싶고만!'Learning How to Learn', Coursera의 인기 강좌였다. 획기적이지 않은가. 배움의 방법을 강의 컨텐츠화 할 생각을 ...!!! 궁금해궁금해​Terry Sejnowski, Keynote: Learning How to Learn - YouTube  원서 목차는 이러하다.​Part I Intelligence Reimagined 11. The Rise of Machine Learning 32. The Rebirth of Artificial Intelligence 273. The Dawn of Neural Networks 374. Brain-style Computing 495. Insights from the Visual System 63Part II Many Ways to Learn 796. The Cocktail Party Problem 817. The Hopfield Net and Boltzmann Machine 918. Backpropagating Errors 1099. Convolutional Learning 12710. Reward Learning 14311. Neural Information Processing Systems 161Part III Technological and Scientific Impact 16912. The Future of Machine Learning 17113. The Age of Algorithms 19514. Hello, Mr. Chips 20515. Inside Information 21916. Consciousness 23317. Nature Is Cleverer Than We Are 24518. Deep Intelligence 261​한글판 책은 소제목들이 모두 정직하게 직역되어있다. '뉴럴 네트워크의 여명'... 여명黎明? 이 무슨 뜻인가. ㅋㅋ 의미가 와닿지 않는 관계로 공부하면서 내 마음대로 수정했다 히이..​​지능의 재해석1장 머신러닝의 활약* 딥러닝?수학, 컴퓨터공학, 신경과학에 뿌리를 둔 머신러닝의 한 분야. - 문제해결 방식의 변화: (20c) 로직 -> (오늘날) 학습 알고리즘   동일한 학습 알고리즘이 서로 다른 문제를 해결하는 데 이용될 수 있으니 로직보다 효율적임.​1. 운전자율주행차량​ DARPA에서는 175마일에 달하는 Mojave 사막 코스를 10시간 안에 횡단하는 자율주행로봇에게 200만 달러의 상금을 걸고 Grand Challenge를 시작했다. 주행 코스와 속도제한에 대한 상세 정보는 대회 시작 2시간 전에 각 팀에게 CD-ROM으로 전달되었으니 사전 리허설은 불가능했다. 2005년 1:40 pm, Stanford Racing Team에서 개발한 Stanely는 DARPA 코스를 완주한 첫 번째 로봇이 되었다. 이 차량의 모델은, a diesel-powered Volkswagen Touareg R5. 첨부파일Journal of Field Robotics - 2006 - Thrun - Stanley  The robot that won the DARPA Grand Challenge.pdf파일 다운로드 DARPA측에서 전달한 GPS corrider은 파란색 선으로 실제 도로 구간과 상당한 차이가 난다. 이렇게 좁고 구불구불한 구간을 소프트웨어에 의존하여 통과해야했기 때문에 mapping이 제대로 되지 않은 대부분의 로봇은 미국 서부 사막의 난코스를 무사히 횡단하지 못했던 것이다.​ Flowchart of Stanley software system​Sebastian Thrun: 세바스찬 쓰런: 구글의 무인 자동차 | TED Talk: Progress report로, Self-driving car training 과정을 운전석 시점에서 볼 수 있다. 3분 40초가 아깝지 않을 영상 ㅎㅎ​​2. 청취음성인식 + 언어번역매일 눈을 뜨면 부르는 익숙한 그 이름, ""하이, 빅스비"". 지금은 없으면 매우 섭섭할 인공지능의 음성인식 기술이 10년 전에는 전세계 뉴스에 보도될만큼 놀라운 일이었다. Nov 7, 2012/ Rick Rashid Speech Recognition2012년, 마이크로소프트의 CRO, Rick Rashid가 중국 텐진 행사장에서 영어로 연설하고있다. 화면 속의 중국어 자막은 미리 준비된 내용이 아니며 딥러닝 음성인식 시스템에 의해 중국어로 동시 통역되어 청중의 눈과 귀로 강연의 내용을 이해시키고 있었다. 이 아슬아슬한 순간은 뉴스를 통해 전 세계로 보도되었으며 캡쳐한 사진 속에 자막으로 보이는 ""I'm speaking in english and hopefully you'll hear me speaking chinease in my own voice.""을 말하자 마자 박수갈채가 쏟아져 나왔다.​Speech Recognition Breakthrough for the Spoken, Translated Word - Microsoft Research​​3. 진단Disease Partitioning Algorithm머신러닝과 빅데이터 기술은 의료 진단에서의 정확도를 높이는데에 사용될 수 있다. 원격 의료도 가능해질까? Fev. 2017 Nature약 13만장의 피부병변의 이미지로 CNN을 학습한 결과, 21명의 피부과 전문의와 비슷하거나 뛰어난 진단 수행력을 보인다는 연구 결과가 2017년 네이쳐 표지를 장식했다. 스마트폰의 사진 촬영으로 피부과 전문의 수준의 신속한 진단을 받을 수 있다는 희망을 준 것이다.​ Deep CNN layout (from left to right)  Tree-structured taxonomy of skin disease and example of test set img.​ t-SNE visualization of CNN's representation for four disease classes* t-SNE, a method for visualizing high-dimensional data​Coloured point clouds represent the different disease categories, showing how the algorithm clusters the diseases. ​Dermatologist-level classification of skin cancer with deep neural networks (stanford.edu): full article in pdf.​+) 원격의료 관련해서, 중국 인공지능 의료산업 체인망아직 우리나라는 원격의료에 대해서 조용하지만, 중국에서는 5G 네트워크 통신과 클라우드 서비스를 통해 원거리 통신을 가능케하면서 정부 차원에서 팍팍 힘을 실어주고 있다. 또한 의료겸직이 가능하도록 제도를 수정하고, 의사들의 고강도 업무에 따른 합당한 수입을 얻을 수 있도록 수익구조를 개선하는 등 원격의료를 향해 적극적으로 움직이고 있다. 원격의료가 시행되려면 표와 같은 의료기술 개발 뿐만 아니라 플랫폼과 컨트롤타워 역할을 하는 기업들 역시 힘을 함께 보태야할 것이다.​​4. 투자알고리즘 매매Renaissance Technologies, LCC, RenTech Sources: 'The Money Formula', Wiley, 2017무지 궁금했던 전설의 르네상스 테크놀로지스. Cold War에서 Code breaker로 활약했던 수학자 James Simons가 설립한 미국 헤지펀드로, 월스트리트의 때가 묻지 않은 박사들만 고용하는 것으로 유명했다. James Ax에 의해 고안된 Medallion fund는 1988년 펀드 설립 이래 사상 최대의 수익을 기록했고, 그 기록은 지금까지도 깨지지 않고 있다.​Jim Simons의 강연. 특별한 내용은 없지만, 눈이 맑고 반짝반짝하셔서 계속 봤다. 어쩜 저리 정정하실까ㅋㅋ The mathematician who cracked Wall Street | Jim Simons - YouTube​​ "
"GPU 완벽 정리! CPU와 GPU의 차이, GPU서버가 무엇인지 한방에 정리해드립니다! (feat.알리바바클라우드 공식 파트너사 라스컴) ",https://blog.naver.com/lascomco/222949661926,20221208,"​안녕하세요! 알리바바클라우드 공식 파트너 라스컴입니다.인공지능, 딥러닝 영역이 각광받고 있는 요즘, GPU의 성능이 아주 중요하다는 것은 잘 아실 것입니다.도대체 GPU가 인공지능, 머신러닝에 어떤 역할을 하길래 각광받고 있는 중인지 살펴보겠습니다! ​GPU란?아마 게임을 좋아하시는 분들이라면 고사양의 그래픽 카드를 사용해야 게임이 잘 돌아간다 라고 알고 계실 것입니다. 이는 그래픽카드가 게임을 실행할 때 그래픽 처리 속도를 개선하기 때문인데요, GPU (Graphics Processing Unit) 는 그래픽 연산을 빠르게 처리하여 모니터에 이미지를 출력하기 위한 연산 장치입니다. ​ 출처 : NVIDIA 모니터에 화면이 나오기 위해서는 ‘지정한 픽셀에 특정 색이 출력되게끔 처리해라’ 라는 명령이 동시에 이루어져야 하는데요, GPU가 이 명령이 가능하게끔 합니다. 최근에는 GPU의 연산 처리 기술이 발전하면서 여러 사업 분야에서 활용할 수 있게 되었습니다. ( 인공지능, 자율 주행  등 )​​CPU vs GPU단순히 수행하는 업무에서만 보았을 때는 CPU가 GPU의 역할이 비슷해 보일 수 있지만, ‘어떤 작업을 처리하느냐’에 따라 퍼포먼스가 달라지기 때문에 CPU와 GPU의 구분이 필요합니다.이는 각각의 연산 처리 방법과 용도에서 큰 차이를 보이기 때문인데요, 아래 사진을 보며 비교해보면 차이를 금방 알 수 있습니다.  출처 : NVIDIA​CPU는 강력한 연산 처리능력으로 개별적인 작업을 신속하게 처리하는데 특화되어 있기 때문에 복잡한 연산을 순차적으로 빠르게 처리할 수 있습니다. 반면 GPU는 처음 출시됐을 당시 ‘그래픽 처리･출력을 빠르게 렌더링‘ 하는 목적으로 구성되었기 때문에 작은 캐시와 다수의 연산 장치가 있는 것이 특징입니다. 이와 같은 구조 때문에 데이터를 병렬로 대량 처리하는 데 특화되어 있습니다.​ ​GPUCPU처리 방식동시다발적으로 처리 ( 병렬 구조 )순차적으로 연산 ( 직렬 구조 )구조소량의 캐시 메모리다량의 연산 장치로 구성대량의 캐시 메모리 제어장치와 연산장치로 구성고속 데이터 처리활용 방안다수의 단순 데이터를 빠르게 처리복잡한 연산 데이터를 빠르게 처리 ​​GPU 서버 활용GPU가 처음 출시됐을 당시에는 단순 이미지 출력 정도의 역할만 수행했으나, 지금은 3D 그래픽 또는 단순 계산 처리에서는 CPU보다 능가하거나 비슷한 성능을 보입니다. 이에 따라 여러 사업 분야에서 GPU서버를 활용하는 사례가 빈번히 나오고 있습니다.​(1) GPU 서버란?GPU 서버는 GPU의 장점을 활용하여 딥러닝, 비디오 프로세싱, 과학 컴퓨팅 등의 대규모 연산에 최적화된 컴퓨팅 자원입니다. 수천 개의 코어를 활용하여 병렬 연산을 수행하기 때문에 목표 데이터를 빠르게 처리할 수 있다는 장점이 있습니다.​(2) GPU 활용 사업 영역GPU 서버는 이미 다양한 산업군에서 활용하여 있으며, 앞으로도 활용 범위는 더욱 확장이 될 것으로 전망합니다.핵심은 ‘많은 데이터를 병렬 구조로 빠르게 처리하는 기술’을 각 사업 영역에 활용한다는 점입니다. 출처 : McKinsey&Company사업 분야활용 범위그래픽클라우드 게임, VR/AR, 실시간 렌더링, virtual anchors, 클라우드 그래픽 워크스테이션, digital twins 등머신러닝 & 딥러닝안면인식, 자동 주행, Speech recognition, Natural language processing, CTR 예측 등제조 영역클라우드 CAD 디자인, mechanical simulation, fluid simulation 등과학 영역에너지 효율성 연구, 기상 예측, 분자 역학 연구 등 ​​GPU서버 선정 시 고려사항GPU서버를 공급하는 업체는 매우 많습니다. 이 많은 플랫폼 중 어떤 GPU서버를 선택해야 할까요? 여기에는 4가지 고려사항이 있습니다.​성능이 좋은 최신 GPU 사용이 가능한가?저렴한 비용에 GPU 서버 사용이 가능한가?GPU서버의 활용을 극대화할 수 있는 부가 서비스가 있는가?기업의 사용 용도에 맞게 GPU 스펙 조정이 가능한가? ​​알리바바 클라우드 GPU 서버 장점알리바바 클라우드 GPU서버는 위의 3가지 고려사항을 모두 충족할 수 있는 GPU 서버를 제공합니다. 이는 알리바바 클라우드만의 서비스 장점 때문인데요, 하나씩 살펴보겠습니다.​(1) 최신 NVIDIA GPU를 저렴한 비용에 사용 가능알리바바 클라우드는 인공지능, 머신러닝에 최적화된 최신 NVIDIA GPU를 사용할 수 있습니다.사업 분야에 따라 다양하게 GPU 스펙을 선택할 수 있으며, 최신 GPU A100을 가장 저렴한 비용에 지원하기 때문에 많은 관심을 받고 있습니다.  ​(2) GPU서버 활용 극대화 : 인공지능 가속 솔루션 제공 (AIACC)알리바바 클라우드는 다른 플랫폼과 다르게 AIACC 가속화 알고리즘을 제공합니다. 훈련 및 추론 시나리오에 AIACC를 적용하여 사업별 퍼포먼스를 향상시킬 수 있습니다. ​지금까지 GPU에 대한 개념과 GPU 서버 선정 시 고려사항을 설명해 드렸는데요, 위의 설명만으로는 GPU사용 결정에 어려움이 있을 수 있습니다. GPU서버 관련하여 추가 궁금하신 점이 있다면 하단 배너를 통해 알리바바 클라우드 공식 파트너 라스컴에 문의해주세요! ​ "
JIPA ,https://blog.naver.com/dongjong-medicine/223112047795,20230526," (1)The Journal of the International Phonetic Association (JIPA, /ˈdʒaɪpə/[1]) is a peer-reviewed academic journal that appears three times a year. (2)It is published by Cambridge University Press on behalf of the International Phonetic Association. (3)It was established as Dhi Fonètik Tîtcer (""The Phonetic Teacher"") in 1886. (4)In 1889, it was renamed Le Maître Phonétique and French was designated as the Association's official language.[2] (5)It was written entirely in the IPA, with its name being written accordingly as ""lə mɛːtrə fɔnetik"" and hence abbreviated ""mf"", until it obtained its current name and English became the official language again in 1971.[3][2] (6)It covers topics in phonetics and applied phonetics such as speech therapy and voice recognition. (7)The journal is abstracted and indexed in the MLA Bibliography.​​국제 음성 학회 저널(JIPA, /ˈdʒaɪpə/[1])은 매년 세 번 발간되는 학술 저널로, 캠브리지 대학 출판사를 통해 국제 음성 학회를 대표하여 발간됩니다. 이 저널은 1886년에 '디 포네틱 티처(Dhi Fonètik Tîtcer, The Phonetic Teacher)'로 설립되었습니다. 1889년에는 '르 메트르 포네틱(Le Maître Phonétique), (""The Phonetic Teacher"") 로 이름이 변경되었으며, 프랑스어가 학회의 공식 언어로 정해졌습니다.[2] ★이 저널은 전적으로 국제 음성 기호(IPA)로 작성되었으며, 이름도 ""lə mɛːtrə fɔnetik""으로 표기되고, 그에 따라 ""mf""로 약칭되었습니다. 그러나 1971년에 현재의 이름으로 변경되면서 다시 영어가 공식 언어가 되었습니다.[3][2] 이 저널은 음성학과 응용 음성학(음성 치료 및 음성 인식 등)에 관련된 주제를 다루고 있습니다. 이 저널은 MLA Bibliography에 수록되고 색인화되어 있습니다. The first issue of The Phonetic Teacher Discipline Phonetics, phonologyLanguage EnglishEdited by Marija Tabain /təˈbeɪn/ (2020)​Publication details  Former name(s)  Le Maître Phonétique  The Phonetic Teacher  History1886–presentPublisher  Cambridge University Press on behalf of the International Phonetic Association  Frequency Triannually​Standard abbreviations ISO 4 J. Int. Phon. Assoc.​Indexing ISSN 0025-1003 (print) 1475-3502 (web) LCCN 74648541OCLC no.474783413​Links  Journal homepage   Journal of the International Phonetic Association | Cambridge CoreJournal of the International Phonetic Association - Marija Tabainwww.cambridge.org Online archive   Journal of the International Phonetic Association | All issues | Cambridge CoreAll issues of Journal of the International Phonetic Association - Marija Tabainwww.cambridge.org #동종요법 #수원_동종요법 #자연주의의학 #대체의학 #보완대체의학 #약물학 #레퍼토리 #자연요법 #한의학 #통증 #피로회복 #면역력증강 #스트레스 #소화불량 #알레르기 #자연요법 #한방치료 #허브치료 #건강관리 #예방의학 #대체의학 #의학 #서양의학 #대체보완의학 #대체보완요법 #레퍼토리 #망상 #경험 #주기율표 #Sensation#원소 #화학 #과학 #교육 #학습 #연구 #탐구 #자연 #우주 #명상 #명상호흡 #Psychotherapy #Homeopsychotherapy #음악치료 #Raga #음성학 #언어학 #구음지도​​ "
"후두의 양성 점막 질환 (2) - 성대 낭(Intracordal cyst), 성대구증(Sulcus vocalis), 접촉성 육아종(Contact granuloma), 삽관 육아종 ",https://blog.naver.com/jyo05020/222850033450,20220816,"안녕하세요 여러분들^^맛있는 점심 드셨나요~? 점심 먹고 나니까 솔솔 잠이 오네요..zzZZ​저는 식곤증을 이겨내기 위해서 정신줄을 부여잡고! 지난 포스팅에 이어서 '후두의 양성 점막 질환' 남은 질환들을 리뷰하고자 이렇게 돌아왔답니다!!​그럼 지금부터 후두의 양성 점막 질환에 대해 이어서 마저 알아볼까요?~​이비인후과 전문의 닥터둥둥을 따라서 천천히 와주세요^^   보아스이비인후과 건강정보 : 성대노화1. 후두의 양성 점막 질환은 어떤 것들이 있나요?​ 1) 성대 결절(vocal nodule) 2) 성대 폴립(vocal polyp) 3) Reinke 부종(Reinke's edema)  - 위 세 가지 질환은 이전 포스팅을 참고해 주시면 감사하겠습니다^^ 후두의 양성 점막 질환 (1) - 후두 & 성대의 구조 및 역할, 성대 결절(vocal nodule), 성대 폴립(vocal polyp), 라인케 부종(Reinke's edema)안녕하세요 여러분! 광복절을 포함한 연휴가 끝나고 오늘은 출근하는 날이에요^^ 연휴에 푹 쉬어서 그런지 ...blog.naver.com ​ 4) 성대 낭(Intracordal cyst)  - 음성 남용으로 인해 손상된 점막이 파묻힌 상피 세포 위로 치유되면서 파묻힌 상피세포에서 발생하는 것으로 생각되거나, 염증 혹은 외상에 의한 점액 분비선의 폐쇄로 점액이 저류되어 발생하기도 함  - 음성을 과도하게 사용한 병력이 있으며, 성대 결절이나 성대 폴립과 구분하기 어려운 경우가 많지만, 후두 스트로보스코피에서 낭의 형태를 뚜렷이 보이고 점막 파동의 전달이 낭 상부의 점막에서 단절되는 것이 특징임 - 보존적 치료로 후두의 전반적인 위생 상태를 개선하고 음성치료를 시행할 수는 있으나, 근본적 치료를 위해서는 수술을 시행해야 함 - 정상 성대 점막과 고유층을 보존하면서 낭 전체를 적출하는 미세 피판법(microflap technique)을 시행해야 재발의 가능성이 줄어듬 - 성대 결절이나 폴립과 달리 점막의 유착과 강직을 방지하기 위해, 술 후 수일간의 성대 안정을 권유하는 것이 좋고, 음성치료로 회복을 촉진할 수 있음 VOICESURGEON.NET - Vocal Cord Cyst ​ 5) 성대구증(Sulcus vocalis) - 성대의 유리연(free edge)을 따라 평행하게 홈이 나 있는 것을 뜻하며, 주로 양측성으로 생겨서 음성장애를 일으키는 질환  - 인후두 역류 질환, 성대낭의 파열, 노인성 후두증(presbylaryngis), 염증성 질환, 후두 손상 등으로 발생하며, 과도한 음성 사용자에서 호발하고, 대다수에서 중년기 이후에 음성장애가 나타남, 선천성 질환이라는 시각도 일부 존재함  - 과도한 쉰 목소리, 고음과 고성 발성장애의 증상이 뚜렷하며, 다른 양성 성대 병변(성대 결절, 성대 폴립, Reinke 부종, 근긴장성 음성장애 등)이 약 10%에서 동반됨  - 후두 내시경검사에서는 증상을 설명할 만한 이상 소견을 발견하지 못하는 경우가 많지만, 후두 스트로보스코피에서 저주파수 음역에서는 성대 점막의 전장이 진동하지만, 고주파수 음역에서는 파동이 정지되는 부위가 관찰되고, 이 부위에 성대구의 형태가 저명하게 나타남  - 성대 내연이 궁상으로 휘어져 성문 폐쇄부전을 보이며, 발성 시 성문이 방추형으로 열려 있고 성대 내연의 진동 운동이 감소하며, 가성대가 과도하게 내전되는 소견이 관찰됨 - 음성 치료가 성대의 부적절한 보상기전을 교정하는 데 도움이 되나 한계가 있음, 수술 방법으로는 지방, 근막, 실리콘 등을 성대 내에 주입하거나, 성대를 내전시키는 갑상성형술 I형, 성대 점막의 직접적인 복구를 위한 성대구절제술(sulcussectomy) 등 여러가지 수술이 시도되고 있으나 만족스럽지는 못함 Formant analysis in dysphonic patients and automatic Arabic digit speech recognition - May 2011BioMedical Engineering OnLine 10(1):41​ 6) 접촉성 육아종(Contact granuloma) - 지나치게 낮은 톤으로 음성을 과도하게 사용하거나 만성적인 기침과 습관적인 헛기침, 인후두역류 질환이 있을 때 발생함  - 피열연골의 성대돌기 부위의 점막과 연골막에 염증이 진행하여 궤양 혹은 육아종으로 진행하며 3~6개월이 지나면 성숙한 육아종이 됨  - 주로 40~50대의 남자에서 발생하고, 쉰 목소리와 인후두 통증, 이물감을 호소하기도 함  - 후두 내시경 검사에서 피열연골의 성대돌기 부위에 함몰된 궤양이나 돌출된 육아종이 관찰되며, 조직 검사를 통해 악성종양, 결핵, 사르코이드증(sarcoidosis), 진균증 등과 감별해야 함 - 습관적인 헛기침이나 지나친 저음 발성을 자제하고, 음성치료와 인후두 역류 질환 치료를 선행해야 하며, 병변에 스테로이드를 직접 주사하는 방법도 시도되고 있음, 보존적 치료만으로도 치료 효과가 우수하지만 거대 육아종을 형성한 경우와 수개월간의 보조적 치료 후에도 효과가 없는 경우에 한하여 후두 내시경과 CO2 레이저를 이용한 절제술을 시행함 Medical Voice Center - Contact Granuloma​ 7) 삽관 육아종(Intubation granuloma) - 기관 내 삽관이나 강직형 기관지경 검사 등으로 삽관과 발관에 의한 직접적인 손상, 튜브에 의한 압박, 오랜 삽관 등 성대 피열연골부의 점막이나 연골막에 손상이 가해지는 모든 경우가 원인이 될 수 있음  - 손상된 점막과 연골막이 치유되는 과정 중 육아조직이 증식하여 삽관육아종이 발생하며, 인후두 역류가 동반되면 삽관육아종의 발병률이 증가하게 됨  - 비교적 드문 질환으로 편측성이 양측성보다 많고, 우측에 호발하며, 대개 20~60세의 성인, 주로 여자에게 발생하고 소아에서는 드묾  - 음성은 비교적 정상적이거나 약간의 쉰 목소리를 보이고, 이물감, 통증을 호소하기도 함 - 완전히 자라면 저절로 떨어져 나가는 특성이 있으므로, 먼저 성대안정, 음성치료, 금연, 국소 스테로이드 분무와 항생제 투여, 인후두 역류 치료 등의 보존적 치료를 우선적으로 시도하는 것이 원칙! - 보존적 치료 후에도 육아종이 없어지지 않으면 후두 미세 수술(Laser Microscopic Surgery, LMS)이나 CO2 레이저를 이용한 절제술을 시행할 수 있음 - 수술 전이나 후에 스테로이드를 병변에 주사하거나 전신적으로 투여하면 재발 방지에 효과가 있음  - 최근 손상된 피열 연골 점막과 연골막 부위에 섬유아세포의 증식과 육아종의 재발을 억제하기 위해 Mitomycin C를 이용한 치료를 시도하고 있음​ Voicedoctor.net - Intubation vocal cord granulomas Voicedoctor.net - Intubation vocal cord granulomas  * 위 사진들은 삽관 육아종이 발견된 이후 8개월 동안 경과 관찰만 하면서 보존적 치료를 한 경우 양측 병변이 모두 저절로 떨어져 나가 소실된 것을 보여주는 증례입니다!!! (교과서로만 보다가 직접 증거를 보니.. 신기방기합니다...ㄷㄷㄷ)  '무리한 발성''음성의 과다 사용''흡연'+'갑자기 목소리가 쉬었어요!!'​최대한 빨리 이비인후과 전문의에게 꼭꼭꼭 진료받으셔야 합니다!!(담배 끊으세요.. 지금부터라도.. 제발...ㅠㅠㅠㅠㅠㅜㅜㅜㅜㅜㅜㅜㅜ)  오늘 후두의 양성 점막 질환 두 번째 시간까지 모두 마무리하였습니다!!​생소한 이름들도 있지만, 익숙한 이름들도 많이 있었죠?? 가수들이 성대결절을 앓았다는 얘기는 심심치 않게 들리는 내용들이니까요~ 무엇보다 이 질환들은 음성치료와 보조적인 치료들, 미세한 후두 수술이 중요합니다!​목소리가 이상하다면 꼭 이비인후과 전문의에게 진료를 받으러 가셔야 합니다^^​지금까지 긴 글 읽어주셔서 감사드리며,이상 이비인후과 전문의 닥터둥둥이었습니다!!!  ​ "
대전보청기 : 청력검사란!? (어음청력검사) ,https://blog.naver.com/khearing9ms/222674308927,20220316,"안녕하세요!!스타키보청기 대전센터 입니다~​청각학전문가(청능사, Audiologist)가 알려주는 청각학·보청기 정보 2탄'어음청력검사'편 오늘은 여러분이 보청기센터에 방문하게 되시면,청력을 평가하기 위해 가장 기본적으로 실시하게 되는 2가지 검사 중 2번째어음청력검사에 대하여 알아보겠습니다. 출처:istockphoto어음청력검사(Speech Audiometry)란?​어음청력검사는 말소리, 즉 어음을 이용하여 일상생활에서의 의사소통능력을 측정하는 검사로 순음청력검사와 더불어 가장 대표적으로 사용되는 청력검사 방법입니다.​어음청력검사는 주로 단음절, 이음절, 문장 등을 이용하여 청력 역치 및 어음인지도를 측정하는 데 사용합니다.​ 출처: pinterest어음청력검사의 유래1920년 중반 처음으로 개발된 이래 제2차 세계대전 이후 청력이 손상된 군인들의 재활을 위해 미국 재활병원을 중심으로 보다 체계적인 어음청각검사 방법들이 개발되었고, 어음을 이용한 청각검사는 순음을 이용한 청각검사보다 의사소통능력을 평가하고 예측하기에 더 적절하며, 유소아의 경우 순음보다 어음을 사용하는 것이 청력역치검사에서 성공 확률이 더 높을수 있다고 보고되고있습니다.​그렇다면 어음청력검사에는 어떤종류가 있을까요??  ​어음청력검사는크게 3가지 검사가 있습니다.​이음절 단어를 이용하여 검사하는 어음인지역치(SRT: Speech Recognition Threshold)일음절 단어를 이용하여 검사하는 단어인지도(WRS: Word Recognition Score)문장을 이용하여 검사하는 문장인지도(SRS: Sentence Recognition Score) 출처: https://brunch.co.kr/@creamjune/35가장 먼저  어음인지역치(SRT: Speech Recognition Threshold) 는순음청력검사의 신뢰도를 확보하고 자극음의 경우 이음절 단어를 사용합니다. ​여기서 중요한 것은 피검자가 검사의 사용되는 단어를 모두 친숙하게 파악한 이후 검사를 진행하는것이 중요합니다.SRT 검사는 순음청력검사와 같은 방법으로 진행하나 순음이 아닌 어음을 통해 가장 작은 소리의 역치를 찾는 검사라고 할 수 있습니다.​ 출처: kr.123rf|다음으로단어인지도(WRS: Word Recognition Score)는어음의 음향학적인 정보를 가지고 '말소리 명료도'를 확인하기 위해 실시하는 검사로, 의사소통 장애의 정도, 청력손실 병변부위에 대한 정보, 보청기의 선택, 청능재활의 평가와 계획, 중추청각처리장애 판별 및 재활 등에 필요한 정보를 제공해주는 검사입니다.​자극음은 먼저 검사를 받는 사람의 듣기 편안한 소리(MCL: Most Comfortable Level)수준을 찾아 일음절 단어를 사용하여 검사를 진행합니다. ​ 출처: wecanall마지막으로문장인지도(SRS: Sentence Recognition Score)는문장을 들려주고 대상자의 문장인지능력을 평가하는 검사로, 듣기 편안한 소리(MCL: Most Comfortable Level)에서 문장을 읽어주고 대상자가 따라 말하면 목표단어의 개수를 세거나 맞은 문장의 수를 세어 평가합니다.​​끝으로 어음청력검사는 순음청력검사와 더불어 종합적인 청력의 평가를 위해 필수적인 검사도구입니다.​국가표준으로 공시되어 있는 한국어음청력검사 방법은 기존의 연구결과를 바탕으로 국제표준에서 권장한 절차에 따라 개선하였기 때문에 현재의 검사방법을 준수하여 측정한 결과를 신뢰할 수 있습니다.    보청기센터에 방문하시기 전, 내가 받게 되는 검사를 미리 알고 방문하신다면나의 귀를 평가하는데 도움이 되지 않을까요??​다음시간에는 청각학전문가(청능사, Audiologist)가 알려주는 청각학·보청기 정보 3탄'임피던스청력검사(중이검사)'편으로 찾아뵙겠습니다~ 스타키보청기 대전센터대전광역시 중구 계백로 1697-1 4층 ​ "
삼성전자; 인공지능반도체개발위해 네이버와 협업 【 2022년 12월 6일 화요일자 코리아헤럴드 】 ,https://blog.naver.com/bychance/222947488386,20221206,"Samsung joins hands with Naver to develop AI chips Han Jin-man (left), executive vice president and head of memory global sales and marketing at Samsung Electronics Co., and Chung Suk-geun (right), CEO of Naver CLOVA CIC, are seen in this photo provided by Samsung on Tuesday. (Yonhap)Samsung Electronics Co. said Tuesday it is working with internet portal giant Naver Corp. to develop next-generation artificial intelligence (AI) chips, as part of efforts to improve efficiency of processing large AI data.Samsung, the world's largest memory chip maker, said its cooperation with Naver will create great synergies in developing semiconductors with high efficiency and speed for AI-specific calculations.""There are growing needs to develop new types of semiconductors that are focused on processing hyperscale AI, as the current computing systems have their limits in doing so,"" Samsung said in a statement.""We will continue expanding the lineup of market-leading memory chip products by offering solutions to meet the needs of AI service companies and users,"" the company said.Naver, which has advanced technology in AI algorithms that automate the process of machine learning, released the AI platform CLOVA in 2017, which offers various AI-based services for search, speech recognition and natural language processing.In May last year, it unveiled the supersized AI platform HyperCLOVA. (Yonhap)​관련기사 & 자료 Samsung joins hands with Naver to develop AI chipsSamsung Electronics Co. said Tuesday it is working with internet portal giant Naver Corp. to develop next-generation artificial intelligence (AI) chips, as part of efforts to improve efficiency of processing large AI data. Samsung, the world's largest memory chip maker, said its cooperation with Nav...www.koreaherald.com 삼성전자경기도 수원시 영통구 삼성로 129 삼성전자공업단지 ​ "
[KT AIVLE School 3th:A+ 기자단] 5주차. 딥러닝 ,https://blog.naver.com/tiranopower/223034217013,20230304,"머신러닝에 이어서 딥러닝 분야를 배우게 되었습니다.. 머신러닝의 기초가 없어서는 딥러닝을 다루기는 진짜 힘들었으며..  핸즈온 강의를 통해 복습 철저히 해야겠습니다..  Part1. Deep Learning 이란?​1) 개념:  인공 신경망을 기반으로 한 기계 학습 알고리즘 분야​2) 종류 Image ClassificationSpeech RecognitionNatural Language Processing  ​Part2. 모델 생성 과정​데이터 전처리- 모델 설계 - 모델 학습 - 모델 평가로 나눌 수 있습니다.​* 데이터 전처리 : 모델이 학습할 수 있는 형태로 데이터를 변환하는 과정입니다. 이 과정에서는 데이터를 수집하고, 레이블링하며, 데이터를 분할하는 등의 작업을 수행합니다. 데이터 전처리의 성능은 모델의 성능에 직접적인 영향을 미치므로, 이 과정에서 충분한 시간을 투자해야 합니다.​* 모델 설계 : 입력 데이터와 출력 데이터 간의 관계를 학습하는 신경망 구조를 정의하는 과정입니다. 이 단계에서는 모델의 구조, 레이어, 활성화 함수, 손실 함수, 최적화 알고리즘 등을 정의합니다. 이 과정에서는 모델의 복잡도와 일반화 능력 등의 요소를 고려하여 최적의 구조를 설계해야 합니다.​* 모델 학습  : 모델이 입력 데이터와 출력 데이터 간의 관계를 학습하는 과정입니다. 이 단계에서는 데이터셋을 이용하여 모델을 학습하며, 이때 모델의 가중치를 조정해나가는 과정이 진행됩니다. 이 과정에서는 학습률, 배치 크기, 에포크 수 등의 하이퍼파라미터를 조정하여 모델의 성능을 최적화해야 합니다.​* 모델 평가 : 학습된 모델이 실제로 얼마나 잘 작동하는지 측정하는 과정입니다. 이 과정에서는 테스트 데이터셋을 이용하여 모델의 성능을 평가하며, 모델의 정확도, 재현율, 정밀도 등을 측정합니다. 이때, 모델이 오버피팅(overfitting)되는지 언더피팅(underfitting)되는지 확인하고, 이를 해결하기 위한 방법을 적용해야 합니다.​  Part3. Image Classification​자동으로 물체를 분류하고 탐지할 수 있는 기술입니다. 모델은 여러 층으로 이루어진 인공 신경망을 사용하여 이미지에서 특징(feature)을 추출하고 이를 분류하는 방식으로 동작합니다. 이 때, 딥러닝 모델은 학습 데이터를 사용하여 자동으로 최적화되며, 최적화된 모델은 새로운 이미지를 분류할 때 정확한 결과를 도출할 수 있습니다.  Part4. Speech Recognition​인간의 음성 언어를 컴퓨터가 이해하고 처리할 수 있는 텍스트로 변환하는 기술입니다. 이를 통해 음성 명령어를 인식하고, 음성 검색, 음성 번역, 음성으로 문서 작성 등 다양한 응용 분야에서 사용됩니다.크게 두 단계로 나눌 수 있습니다. 첫째, 음성 신호를 디지털 신호로 변환하는 단계입니다. 이 단계에서는 마이크로부터 입력된 음성 신호를 전기 신호로 변환하여 컴퓨터가 이해할 수 있는 디지털 신호로 변환합니다.둘째, 변환된 디지털 신호를 분석하여 텍스트로 변환하는 단계입니다. 이 단계에서는 음성 인식 엔진이 딥러닝 알고리즘을 사용하여 입력된 음성을 이해하고, 텍스트로 변환합니다. 이 때, 딥러닝 알고리즘은 다양한 음성 특성을 학습하고, 이를 기반으로 음성 신호를 텍스트로 매핑합니다.음성인식 기술은 연속적인 음성 신호를 처리할 수 있으며, 대화형 인터페이스를 구현하는 데 매우 유용합니다.   Part5. Natural Language Processing​사의 언어와 컴퓨터의 언어 간 상호작용을 위한 기술입니다. 자연어 처리는 인간의 언어를 이해하고 처리하는 것을 목적으로 하며, 이를 통해 인간과 컴퓨터 간의 의사소통, 문서 분석, 정보 검색, 기계 번역 등 다양한 응용 분야에서 활용됩니다.​자연어 처리 기술은 크게 세 단계로 나누자면,첫째, 텍스트 전처리 단계입니다. 이 단계에서는 텍스트 데이터를 토큰화하고, 언어적인 요소들을 분리해내는 등의 작업을 수행합니다.​둘째, 텍스트 분석 단계입니다. 이 단계에서는 자연어 처리 알고리즘을 사용하여 텍스트 데이터를 분석하고, 의미론적 정보를 추출합니다. 이를 통해 텍스트 분류, 감정 분석, 정보 추출 등의 작업이 가능해집니다.​셋째, 텍스트 생성 단계입니다. 이 단계에서는 자연어 생성 기술을 사용하여 컴퓨터가 새로운 텍스트를 생성합니다. 예를 들어, 기계 번역, 문서 요약, 질문 응답 시스템 등이 이러한 기술을 사용합니다. "
"(가능성) on-off line possibility by 2030 / OS,cloud,office(work),learning & electronic products/ A~Wxyz ",https://blog.naver.com/acklego/223047843250,20230317,"AI-POWERED POSSIBILITIES BY 2030: Transforming Industries and Enhancing Human Lives​Certainly! Here's a more detailed list of AI-powered possibilities by 2030:​2023:Continued advancements in natural language processing (NLP) and speech recognition technology lead to more widespread adoption of virtual assistants and chatbots in customer service and support.​2024:AI-driven supply chain optimization and predictive maintenance systems become more prevalent, reducing costs and increasing efficiency for businesses.Autonomous vehicles become more widely adopted, particularly in logistics and delivery.​2025:Healthcare sees significant advancements in personalized medicine and disease diagnosis through the use of AI and machine learning.Smart home technology becomes even more sophisticated, with AI-powered devices learning and adapting to users' preferences and habits.​2026:AI-powered fraud detection and prevention systems​ become more advanced, reducing the risk of financial crimes.The use of AI in education becomes more widespread, with personalized learning experiences tailored to individual students' needs and abilities.​2027:AI-powered virtual and augmented reality technology sees significant growth, transforming industries​ such as entertainment, gaming, and tourism.The use of AI in agriculture leads to more efficient and sustainable farming practices.​2028:AI-powered financial and investment advice becomes more sophisticated, with more personalized recommendations based on individual goals and risk tolerances.The use of AI in energy management and sustainability leads to more efficient and eco-friendly buildings and infrastructure.​2029:AI-powered robots ​become more prevalent in manufacturing and industry, leading to increased efficiency and productivity.The use of AI in environmental monitoring and conservation helps to protect and preserve our planet's natural resources.​2030:AI-driven breakthroughs in science and research​ lead to new discoveries and advancements in fields such as medicine, chemistry, and physics.AI-powered assistants become even more intelligent and capable, with the ability to understand and complete complex tasks and problem-solving.​Ah, I see what you mean now. Here's a comparison of AI-powered possibilities in online and offline contexts:​Online AI-powered possibilities:Chatbots and virtual assistants for customer service and supportPersonalized content recommendations on streaming platforms and social mediaAI-powered fraud detection and prevention in online transactionsOnline advertising targeting and optimization based on user behavior and preferencesAI-powered language translation and natural language processing for online communicationOffline AI-powered possibilities:Autonomous vehicles for transportation and logisticsSmart homes and cities with AI-powered energy management and sustainability featuresAI-powered robots for manufacturing and industryPersonalized healthcare and medicine through AI-powered diagnosis and treatmentAI-powered environmental monitoring and conservation effortsWhile there are certainly overlaps and intersections between these online and offline AI-powered possibilities, there are also some unique opportunities and challenges for each context. For example, online AI-powered possibilities are more focused on user interaction and engagement, while offline AI-powered possibilities are more focused on physical infrastructure and the environment. Ultimately, the combination of these online and offline AI-powered possibilities has the potential to transform nearly every aspect of our lives and society as a whole.​​In the case of an offline internet environment, the possibilities for AI are more limited since many AI-powered applications and systems rely on the internet to access data and cloud-based computing resources. However, there are still some AI-powered possibilities that are available offline:Offline AI-powered personal assistants or chatbots can still perform certain tasks, such as setting reminders or playing music, without an internet connection.AI-powered image or speech recognition technology can be used offline to perform certain tasks, such as identifying faces in a photo or transcribing a recording.AI-powered robots or drones can perform certain tasks offline, such as monitoring crops in a field or inspecting infrastructure.Offline AI-powered predictive maintenance systems can still function by analyzing data stored locally on devices or servers.However, it's important to note that many AI-powered possibilities are heavily dependent on the availability of internet access and cloud computing resources. Without these resources, the scope and capabilities of AI systems are limited. Therefore, while there are still some offline AI-powered possibilities available, the majority of AI-powered applications and systems will require internet access to function effectively.Certainly! Here are some examples of how on-off line internet access can impact everyday life:Communication: With internet access, people can communicate instantly with anyone around the world, using applications such as email, instant messaging, and social media. In an on-off line environment, communication becomes more challenging, but it's still possible to use technologies such as SMS or voice calls, which do not rely on a continuous internet connection.Navigation: In an on-off line environment, GPS navigation apps can still provide directions, but only if the user has previously downloaded the relevant maps and data. This is particularly useful when traveling to areas with poor internet connectivity or when using a GPS-enabled device without a cellular data plan.Entertainment: With internet access, people can stream music, movies, and TV shows on-demand. In an on-off line environment, entertainment options become more limited, but people can still enjoy downloaded content or play offline games.Education: Online education platforms have become increasingly popular, offering courses and resources to learners around the world. In an on-off line environment, educational resources can still be accessed offline, but learners may need to download course materials in advance.Work: With internet access, many people can work remotely, accessing company networks and cloud-based applications from anywhere. In an on-off line environment, remote work becomes more challenging, but workers can still use locally-installed software or offline documents to get work done.Overall, while internet access is becoming increasingly important in everyday life, there are still ways to access and use technology in an on-off line environment.​​​​​ "
[건강 정보]칸디다구내염아구창 중요성 알고싶다면 ,https://blog.naver.com/murder10695/222950337823,20221209,"[건강 정보]칸디다구내염아구창 중요성 알고싶다면​​​방갑습니다. 이번 포스팅은 칸디다구내염아구창의 중요성에 관해서 말하는 포스팅을 마련했죠. 여려명의 중년이 해당되는 칸디다구내염아구창에 관해서 평일에도 여려명의 궁금함을 느껴지고 있는 상황이죠. 그렇지만 아무래도 이에 관해서 정확하게 알고 있는 사람이 많지 않은 상황이죠. 유난히도 주말 같은 온도계의 움직임이 심각한 시점이 접근한다면 건강 상태에 차이가 체감되는 경우도 다양하게 있었는데요. 평소와는 달라진 내용이 체감되는 상황이라면 해당되는 칸디다구내염아구창를 고민하지 않을 수 없죠. 여려명의 중년이 개선법이 존재하지 않는다고 추측하는 경우도 있었으나 이것은 평일에 어떤방법으로 체력을 케어하고 있는지에 따라서 준비도 가능하기 때문에 노력이 이만큼 중대한 칸디다구내염아구창라고 할 수 있었는데요. 그렇지만 아쉽게 보여지는 내용이 있는 상황에서도 여려명의 중년이 해당되는 칸디다구내염아구창에 관해서 간단하게 분류하고 병이 추측되는 상황에도 가만히 두는 분까지 있었는데요. 이러한 칸디다구내염아구창는 차이가 발생한 경우에 무엇보다도 조금이라도 빨리 해결하는 것이 최우선으로 필요하며 효능이 있는 해결법이 될 수도 있었는데요. ​​건강에 효과적인 몸을 처리하는 방식으로는 알려진 것은 아니지만, 다수의 병이 이와 같은 평소 정신적인 처리와 나트륨도 함유된 건강에 효과적인 먹는것과 적절한 산책을 꾸준하게 하면서 면역 능력을 향상시키는 것이 요구되는 부분입니다. 무엇보다도 평일에 자신의 건강 처리에 관해서 어떤 것들을 하고 있는지 가끔은 걱정 해보는 것이 좋지 않을까 싶은데요. 한번 정리하면 이시간 알 수 있는 효과적인 건강 관련 영어단어에 관해서 알려드리고 총정리 하겠습니다. 웃게 되는 오전되세요.​speech recognition threshold : 언어인지문턱single photon emission computed tomography : 단일광자방출단층촬영술halo sign : 달무리징후delusional jealousy : 망상질투eosinophilic pustular folliculitis : 호산구잔고름집털집염neuro ophthalmology : 신경안과학catenoid : 사슬배열 sputum liquefaction : 가래액화fronto occipital : 이마뒤통수 venogram : 정맥조영사진gibbus : 육봉 / 꼽추copulation impotence : 성교불능증​ "
Popular and highly-rated online English elarning platform ,https://blog.naver.com/neuroped/223002378143,20230201,"here are some popular and highly-rated options to consider:Duolingo: A free app that offers bite-sized lessons and gamified learning to make language study fun and accessible.Babbel: A subscription-based platform that offers interactive lessons, speech recognition technology, and personalized learning paths.Coursera: A massive open online course (MOOC) platform that offers a wide range of English language courses, including those taught by top universities and language schools.Rosetta Stone: A paid platform that uses a combination of visual, auditory, and kinesthetic learning methods to immerse students in the language.Busuu: A language-learning app that uses a combination of lessons, interactive exercises, and feedback from native speakers to help students improve their fluency.English Central: A video-based platform that focuses on helping students improve their speaking and listening skills through real-world video content and speech recognition technology.Memrise: A free app that uses memory games and spaced repetition to help students learn vocabulary and grammar.Lingoda: A platform that offers live online English classes with certified teachers, as well as self-paced lessons and personalized learning paths.Englishtown: A paid platform that offers live online classes, interactive lessons, and personalized feedback from teachers.These platforms offer different approaches to English language learning, so it's important to research each one and find the one that aligns best with your learning style, goals, and budget. "
엣지 AI를 IoT 스마트 카메라로 ,https://blog.naver.com/qualcommkr/222069134695,20200824,"인공지능(AI), 엣지 컴퓨팅, 고해상 이미지. 가정 혹은 기업용 프리미엄급 기기의 스마트 카메라라면 대부분 갖추고 있는 기능들입니다. 우리는 생각보다 많이 AI의 도움을 받는 활동에 의지하고 있습니다. 그리고 이는 로봇부터 커넥티드카에 이르기까지, 다양한 종류의 기기를 구동할 수 있도록 지원하는 스마트 카메라 덕분입니다.​하지만 프리미엄급이 아닌 중저가 기기의 경우라면 어떨까요? 중저가형 기기에 탑재된 카메라에 어떻게 이러한 고급 기능과 성능을 통합하여 사물인터넷(IoT)과 미래 시장 세그먼트의 수요를 맞출 수 있을까요? 퀄컴 비전 인텔리전스 플랫폼(Qualcomm Vision Intelligence Platform)은 네트워크 엣지에서 연산 집약적인 작업을 구동하는 다양한 SoC의 범위를 포괄합니다. 소비자와 기업 세그먼트 전반의 IoT 기기를 고려하여 설계된 플랫폼으로써, 이 플랫폼에 포함되어 있는 칩 덕분에 IoT 기기 설계자는 컴퓨터 비전이나 이미지 처리, 머신러닝(기계학습), 기기 보안과 같은 작업이 기기에서 수행될 수 있도록 설계할 수 있습니다. 엣지에 있는 AI는 제조사가 그간 마주하던 제약을 완화해, 이들이 프라이버시나 네트워크 지연성 걱정 없이 IoT 혁신에 집중할 수 있도록 도와줄 수 있습니다.​​더 다양한 카메라 세그먼트를 위한 프리미엄 카메라 기술 및 엣지 AI 퀄컴은 최근 퀄컴 QCS610과 퀄컴 QCS410을 출시했습니다. 엣지에서의 AI 추론을 위한 SoC로써, 전작들 대비 향상된 성능을 자랑합니다.​퀄컴 비전 인텔리전스 플랫폼 포트폴리오에 최근 추가된 이 새로운 제품들은 이전에는 프리미엄급 기기에서만 첨단 AI를 적용할 수 있었던 세그먼트에서 AI의 활용성을 확대해 줍니다. 즉, 대중적으로 사용되고 있는 CCTV나 대시보드 카메라, 영상 회의 장치 등에서도 사물 분류, 세분화(segmentation), 번호판 인식, 얼굴 인식, 인체 감지, 인원 확인 등의 작업이 가능해지리라는 것이죠. 이 SoC들은 엣지에서의 AI 추론 능력을 기반으로 하는 새로운 제품에 대한 아이디어를 불러일으킬 겁니다.​QCS610과 QCS410은 이기종 컴퓨팅(heterogeneous computing)을 위한 퀄컴의 하드웨어 및 소프트웨어의 융합 패러다임을 따르고 있습니다. 이 두 칩은 퀄컴® 헥사곤 DSP 벡터 프로세서(Qualcomm® Hexagon DSP Vector Processor)와 퀄컴® 아드레노 GPU(Qualcomm Adreno GPU), 두 하드웨어를 기반으로 하는 퀄컴 AI 엔진(Qualcomm AI Engine)을 포함하고 있습니다. 연결성 측면에서는 통합 와이파이(802.11a/b/g/n/ac)와 블루투스 5.x 무선 통신, 그리고 5G 지원 인터페이스를 포함하고 있습니다.​여기에서 이기종 패러다임을 완성하는 건 바로 소프트웨어입니다. AI 용 퀄컴 뉴럴 프로세싱 SDK(Qualcomm Neural Processing SDK)가 모델 전환 및 실행을 위한 툴을 제공하며, 더불어 원하는 성능 프로필에 따른 컴퓨팅 코어(CPU/GPU/DSP)에 맞는 API를 제공합니다. ​텐서플로우 라이트(TensorFlow Lite)에서 수억 대의 안드로이드 기기가 추론을 가속할 수 있는 헥사곤 델리게이트 소프트웨어를 배포한 바 있는데요. 이 역시 헥사곤 DSP를 활용합니다. 그 결과, AI 추론을 네트워크 엣지에서 가능케 하는 연산력과 효율성 모두가 크게 향상되었습니다. 기기에서 작업을 진행할 수 있다면 프라이버시, 지연성, 연결성, 비용 등 클라우드 컴퓨팅이 내재하고 있는 문제들을 해결할 수 있습니다.​​엣지 AI 사용사례제조사에서는 QCS610과 QCS410를 활용해 새로운 기기를 설계하거나 AI와 엣지 컴퓨팅을 활용해 기존 기기를 개선할 수 있습니다. 크게 아래와 같은 사례를 들 수 있습니다.​① 맵 프로젝션파일럿 AI(Pilot AI)는 추론 작업을 클라우드에서 엣지 디바이스로 옮기는 알고리즘 세트를 개발했는데요. 이는 데이터 소스와 가까운 곳(엣지)에서 프라이빗하고 안전하며 빠른 의사 결정을 가능케 합니다.​이 사례는 상점이나 공장, 사무실이나 일반 건물과 같은 장소에서 활용도가 높아지고 있습니다. QCS610과 QCS410 덕분에 파일럿 AI의 맵 프로젝션(지도 투영) 기술은 (사람 혹은 사물의) 움직임을 추적하고, 3차원 공간에 있는 여러 대의 스마트 카메라로부터 얻은 입력값을 2차원 지도에 취합할 수 있습니다. 아래 영상과 같이 말이죠. 이 기술의 시의적절한 적용 예시를 올해 전 세계를 강타한 팬데믹 (혹은 앞으로 도래할 공중보건 위기) 상황에서 찾아볼 수 있겠네요. 민관 가릴 것 없이 모든 기관에서 반드시 준수해야 할 사회적 거리두기나 방문자 수를 AI 카메라​와 무선 통신을 통해 측정할 수 있겠죠.​사무실이나 상점, 공장의 관리자들은 현재 실내 인원을 확인함으로써 시설 내 사람 간 거리는 어느 정도 되는지 파악하고 거리두기 실천을 위한 알람을 보낼 수 있습니다. 이는 특히 식당이나 복도, 출입구 등 자연스레 사람들이 모일 수밖에 없는 장소에서 유용하게 쓰일 수 있을 겁니다. 발열이 의심되는 사람이 지나간 경로를 추적할 수 있는 기능은 감염 지역 관리에 도움이 되겠죠. 파일럿 AI가 생성한 분석 결과는 실시간 알람 전송에 활용될 수도 있고, 장기적으로는 기업에서 사람들이 자주 밀집하는 구역의 배치를 재구성하는 데 도움이 될 수도 있습니다.​​② 듀얼 페이싱 AI 대시캠​AI 대시캠은 기업에서 운용하는 차량의 안정성, 보안성, 그리고 가시성을 높여줄 수 있습니다. 대시보드 위에 설치된 카메라들은 엣지 프로세싱을 통해 실시간 상황 및 사물 탐지가 가능하며, 이를 통해 차량 사고율을 줄일 수 있습니다. 듀얼 페이싱 대시캠은 AI와 엣지 컴퓨팅을 이용해 운전자 행동(주의 정도, 졸음 운전, 도로 미응시 등)과 도로 상황을 실시간으로 분석하여 사고 위험성을 줄입니다.​AI 대시캠은 사건을 감지하고 운전자가 집중하지 않고 있는 경우 적색 불빛이나 ‘멈춤’ 사인을 표시함으로써 알람을 보낼 수 있습니다. 더불어 기업의 차량 관리 및 운전자 안전을 위한 플랫폼으로서 역할 할 수도 있습니다. 갑작스러운 가속이나 예상치 못한 제동 및 충돌 등의 사건 전후의 영상을 기록/저장할 수 있으며, 사후 확인을 위해 해당 영상을 자동으로 클라우드에 업로드할 수도 있습니다.​​③ 코로나 대응​여러 국가 및 도시에서 락다운(lockdown)을 완화하며 조심스럽게 경제 활동을 재개함에 따라, 학교와 사무실을 비롯한 여러 장소의 출입 지점에서 사람들을 모니터링하는 것이 매우 중요해졌습니다. 이러한 수요를 맞추기 위해 많은 기업에서는 체온 및 얼굴 인식 스캐너를 개발하고 있는데요. 퀄컴 AI 엔진을 탑재하고 있는 고세이프(GoSafe) 를 개발한 원스크린(OneScreen)도 그중 하나입니다.​스마트 빌딩에 활용할 수 있는 고세이프 사례를 보면, 기기는 건물을 출입하는 사람들을 검사하여 출입을 통제하도록 설계되었습니다. 방문자가 만약 발열 증상을 보이거나 마스크를 미착용하고 있는 경우 고세이프는 이를 바로 확인할 수 있고, 그 결과를 보안 담당자에게 전달하여 이들의 출입을 제한할 수 있습니다. 고세이프 자체가 건물의 보안 시스템과 직접 연결되어 있는 경우라면 알아서 이들의 출입을 제한 혹은 허가할 수 있습니다.​​첨단 AI, 그리고 확대되는 가능성네트워크 엣지 컴퓨팅에서의 다음 단계는 바로 AI 작업을 클라우드에서 IoT 기기로 옮기는 것입니다. 대중적인 카메라 중심 기기의 초 당 추론 성능이 향상될 때 혁신적인 적용 사례 역시 확산될 수 있을 것입니다. ​퀄컴 비전 인텔리전스 플랫폼은 엣지 AI를 위한 스마트 카메라 개발 및 프로토타입 제작을 위한 이상적인 출발점입니다. 알텍(Altek), 애로우(Arrow), 이인포칩스(eInfochips), 랜트로닉스(Lantronix), 썬더컴(Thundercomm)을 비롯한 여러 파트너사가 현재 이 생태계에 포함되어 있습니다. 고객사에서 가장 혁신적이고 경쟁력 있는 제품과 서비스를 창출하도록 돕는 퀄컴 비전 인텔리전스 플랫폼과 같은 놀라운 기술을 꾸준히 개발해 나갈 퀄컴의 모습을 앞으로도 기대해주세요. 사물인터넷을 위한 퀄컴 비전 인텔리전스 플랫폼꾸준히 발전하고 있는 IoT(사물인터넷) 생태계가 우리 생활의 모든 영역을 변화시킬 거라는 전망은 너무...blog.naver.com  최첨단 AI 연구를 통해 세계가 당면한 문제를 해결하고, 수십억 인구의 삶의 질을 향상시키는 데 보탬이 되고 싶나요? 퀄컴은 현재 머신러닝을 포함한 여러 부문의 인재를 모집하고 있습니다. 자세한 내용은 아래의 링크를 참고해주세요.Machine Learning Researcher – On-device AI theory and algorithmMachine Learning Researcher – Audio and SpeechMachine Learning Researcher - Speech SynthesisMachine Learning Research - Speech RecognitionMachine Learning Senior Software Engineer – Embedded AI FrameworkMachine Learning Researcher (Intern)Deep Learning Systems Research Engineer on Camera Perception for Autonomous DrivingComputer Vision and Machine Learning R&D Intern for Autonomous Driving  ​ "
전국 스마트폰 활용지도사 7회워크샵/SNS소통연구소  ,https://blog.naver.com/kjtsmile/222563725537,20211110," SNS소통연구소에서 2021년을 마무리하는 전국 스마트폰 활용지도사 7회 워크샵(2021년11월6일(토) 7일(일)이 있어 서울에 다녀왔습니다.​워크샵 주변을 둘러보며.......​ 워크샵 주변 배경 전국 스마트폰 활용지도사 7회워크샵/SNS소통연구소 대학로 오카라이 호텔 입구와 숙소사진, 숙소에서 바라본 배경을 사진에 담아 보았습니다.​ 워크샵 주변 배경 전국 스마트폰 활용지도사 7회워크샵/SNS소통연구소 대학로 오카라이 호텔에서 1박을 한 후 아침에 일어나 주변에 있는 곳을  산책하면서사진을 담아보았습니다.​교육내용​1일차 교육- 관점을 바꾸면 기회가 생긴다- 스마트 워크교육- 구글어시스턴스- 네이버오피스- 핀테크 교육- 스캐너어플- speech recognition anywhere  등​2일차 교육​- vflat사용법-  keywords everywhere 사용법- 유튜브 애드블록- 리무브- 지식인 사용법- vrew 사용법 등​ 워크샵 이모저모 전국 스마트폰 활용지도사 7회워크샵/SNS소통연구소 교육현장에서 열심히 강의를 듣고 있는 전국 지부장 및 지국님의 모습과 유은서 선생님의 멋진 민요 공연도 참 좋았습니다.​​워크샵 동영상 전국 스마트폰 활용지도사 7회워크샵/SNS소통연구소 전국 스마트폰 활용지도사 7회워크샵/SNS소통연구소를 다녀와서느낀점은교육도 유익하고 참 재미있었고요전국에 계시는 지부장 및 지국장들과 좋은 만남 따뜻한 만남이 참 좋았습니다.이번 기회로 더 준비하고 노력하는 스마일 강사가 되도록 노력하곘습니다.소통을 통한 사회 복지실현을 꿈꾸며....​ 오라카이 대학로 호텔서울특별시 종로구 율곡로 180 ​ 다음 워크샵을 기대하며전국 모든 선생님들 화이팅하시고 건강 잘챙기세요.​  ​ "
신뢰가는 청력검사-무료정밀테스트행사(8/17~9/30) ,https://blog.naver.com/soisori/222855069508,20220822,"굿모닝보청기 강릉점에서는 오픈행사로 정밀 청력검사를 무료로(기간: 8/17~9/30) 진행해드립니다. ​보청기 착용하기 전 전문 검사는 필수이지요.​ 이미 많은분이 오픈행사 혜택으로 무료 정밀 청력검사를 받으셨답니다. ​특히, 여러 검사단계중  '단어인지도검사'에서 놀라운 결과를 확인하셨습니다. ​듣기 가장 편안한 소리로 올렸을때도 소리를 정확하게 알아듣는 능력을 이미 상실하셔서 대화에 어려움이 있으신 이유를 수치로 정확히 알게되었고보청기의 빠른 선택과, 꾸준한 청능재활훈련을 실감하게 되셨죠. 공기전도검사골전도검사단어인지도검사(Word Recognition Score: WRS)-일음절문장인지도(Sentence Recognition Score: SRS)-문장어음인지역치(Speech Recognition Threshold: SRT)-이음절보청기만족도 예측평가(Acceptable Noise Level: ANL)​굿모닝보청기 강릉점 청력검사실 Inventis Harp 오디오메타 완비 ​테스트 후에는 심도있는 상담을 통해 최적의 제품을 선택할 수 있도록 도와드립니다. ​TV를 많이 보시는분, 사회생활을 하시는분, 자녀와 많은 대화를 하시는분, 취미로 댄스동호회 가시는분, 농사를 하시는분에 따라 우리가 접하는 모든 소리는 개인 최적화 맞춤이 되어야하니깐요. 어렵지 않아요. 굿모닝보청기 강릉점 김소이 원장과 상의 하세요.  오시는길: 옥천오거리 남대천방향, 중앙시장입구상담전화: 033-643-5233상담문자: 010-9550-8759 굿모닝보청기 강릉점강원도 강릉시 율곡로 2805 태우빌딩 101호  ​ "
[이렇게 사용하세요!] 네이버 클라우드 플랫폼 유저 API 활용 방법 - 3편 ,https://blog.naver.com/n_cloudplatform/222545102818,20211022,"누구나 쉽게 시작하는 클라우드네이버 클라우드 플랫폼 ncloud.com 입니다.​* 본 기술 포스팅은 NAVER Cloud Research Center 정금영 님의 글입니다.  지난 ‘유저 API 이용하기 – 1편, 2편’ 포스팅유익하게 보셨나요?​네이버 클라우드 플랫폼에서 이용할 수 있는유저 API를 소개하고, 그중에서도서버 API와 스토리지 API를 자세히 소개했습니다. ​[ 1편 : VPC 만들기부터 서버 생성까지 ] [이렇게 사용하세요!] 네이버 클라우드 플랫폼 유저 API 활용 방법 - 1편누구나 쉽게 시작하는 클라우드 네이버 클라우드 플랫폼 ncloud.com 입니다. * 본 기술 포스팅은 네이버클...blog.naver.com [ 2편 : Storage API 활용하기 ] [이렇게 사용하세요!] 네이버 클라우드 플랫폼 유저 API 활용 방법 - 2편누구나 쉽게 시작하는 클라우드 네이버 클라우드 플랫폼 ncloud.com 입니다. * 본 기술 포스팅은 네이버클...blog.naver.com  오랜만에 [ 유저 API 이용하기 - 3편 ]으로 찾아왔습니다!​오늘은 AI·NAVER API 내 포함된 User API 사용 방법을 공유드리겠습니다. 네이버 클라우드 플랫폼의 AI·NAVER API 에서는 CLOVA AI (클로바 인공지능), Papago Translation(파파고 번역), Machine Learning(머신러닝), Maps(네이버 지도 API), NAVER Service(네이버 서비스)를 제공하고 있습니다.​ AI·NAVER API - 애플리케이션 등록 화면 ( ncloud.com 콘솔 내 )​지금부터 User API를 종류별로 자세히 설명드리겠습니다.​CLOVA AI (클로바 인공지능)- CLOVA Speech Recognition (CSR) : 음성을 텍스트로 반환하는 서비스 ( API 가이드 보기 )- CLOVA Face Recognition (CFR) : 이미지 속의 얼굴과 가장 닮은 유명인을 찾거나, 얼굴의 윤곽과 눈/코/입 위치, 표정 값을 얻을 수 있는 서비스 ( API 가이드 보기 )- CLOVA Voice – Premium : 텍스트를 음성으로 읽어주는 서비스 ( API 가이드 보기 )- CLOVA Sentiment : 단어/문장/문구 내용의 감정을 분석하는 서비스 ( API 가이드 보기 )- CLOVA Summary : 원문(문서)으로부터 핵심 문장을 추출하여 요약하는 서비스 ( API 가이드 보기 ) Papago Translation (파파고 번역)- Papago Text Translation : 네이버가 자체적으로 개발한 인공신경망 기반의 기계 번역 기술인 NMT(Neural Machine Translation)를 이용한 번역 서비스 ( API 가이드 보기 )- Papago Website Translation : 소스 언어로 작성된 html 문서 혹은 element를 타깃 언어로 번역하는 서비스 ( API 가이드 보기 )- Papago Doc Translation : 파일 안에 소스 언어로 작성된 텍스트를 타깃 언어로 번역하는 서비스 ( API 가이드 보기 )- Papago Language Detection : 번역하기 위해 입력된 텍스트의 언어를 자동으로 감지해 주는 서비스 ( API 가이드 보기 )- Papago Korean Name Romanizer : 한글로 된 이름을 로마자 표기로 변환해 주는 서비스 ( API 가이드 보기 ) Machine Learning (머신러닝)- Pose Estimation : 입력된 비전 데이터를 통해 사람을 인식하고 포즈를 분석해 주는 API 서비스 ( API 가이드 보기 )- Object Detection : 입력된 비전 데이터를 통해 객체를 인식하거나 객체 감지를 이용한 애플리케이션을 만들 때 유용한 API 서비스 ( API 가이드 보기 ) Maps (네이버 지도 API)- Static Map : 요청된 URL 매개변수를 기반으로 웹 페이지에 표시할 수 있는 이미지로 지도를 반환하는 서비스 ( API 가이드 보기 )- Directions 5 & Directions 15 : 사용자가 지정한 출발지/목적지 정보에 따라 경로 관련 정보를 제공하는 서비스 ( API 가이드 보기 )- Geocoding : 주소의 텍스트를 입력받아 좌표를 포함한 상세정보들을 제공하는 서비스 ( API 가이드 보기 )- Reverse Geocoding : 좌표를 통해 주소 정보(법정동, 행정동, 지번주소, 도로명주소 등)를 반환하는 서비스 ( API 가이드 보기 ) NAVER Service (네이버 서비스)- CAPTCHA (Image) : 이미지 캡차 API는 자동 입력 방지를 위해 사람의 눈으로 식별 가능한 문자가 포함된 이미지를 전송하고 입력값을 검증하는 서비스 ( API 가이드 보기 )- CAPTCHA (Audio) : 음성 캡차 API는 자동 입력 방지를 위해 숫자가 포함된 음성 메시지를 전송하고 입력값을 검증하는 서비스 ( API 가이드 보기 ) - nShortURL : 웹 페이지의 링크 정보는 매우 긴 경우가 많습니다. nShortURL 서비스는 긴 URL을 짧은 형태로 줄여주는 서비스 ( API 가이드 보기 )- Search Trend ​ : 네이버 통합검색에서 특정 키워드가 얼마나 많이 검색되는지 기간별로 추이를 확인할 수 있는 서비스 ( API 가이드 보기 ) 많은 API 중에 Papago Text Translation, CLOVA Summary, Search Trend을 샘플로 준비했습니다. 첫 번째는 Papago Text Translation입니다.👉샘플 확인 (github 링크) public void translation() {   final TranslationRequestDto requestDto = TranslationRequestDto.builder()                                                .sourceLang(LangEnum.KO)                                                .targetLang(LangEnum.EN)                                                .text(""오늘도 좋은 하루되세요."")                                                .build();      final TranslationResponseDto responseDto = aiNaverService.translation(requestDto);   System.out.println(""Plan Text : "" + requestDto.getText());   System.out.println(""Translation Text : "" + responseDto.getMessage().getResult().getTranslatedText());} 👉요청 : 오늘도 좋은 하루되세요.👉결과 : Have a great day today. 직접 문구를 바꿔서 해보셔도 기대하는 결괏값이 잘 나오는 것을 확인할 수 있습니다.😊  두 번째는 CLOVA Summary입니다.👉샘플 확인 (github 링크) public void textSummary() {   final TextSummaryRequestDto requestDto = TextSummaryRequestDto.builder().document(   TextSummaryRequestDto.DocumentRequestDto.builder()                                 .content(""넷플릭스 한국 드라마 '오징어 게임' 출연진이 미국 NBC 유명 토크쇼 '지미 팰런쇼' 출연을 예고한 가운데 진행자이자 유명 코미디언 지미 팰런이 달고나 게임에 직접 도전했다.""                                       + ""6일 지미 팰런은 자신의 사회관계망서비스(SNS)를 통해 'Squid game Cookie'라는 제목의 짧은 영상을 게재했다.""                                       + ""영상에 따르면 지미 팰런이 직접 프라이팬에 소다와 설탕을 붓고 달고나 제조한다. 이후 지미 팰런은 자신의 이름 이니셜인 'JF'가 새겨진 달고나를 직접 손에 들고 있다. 열심히 달고나를 핥아봤지만 바늘을 대는 순간 달고나가 부서지면서 총소리가 울린다. 이어 지미 팰런은 바닥에 쓰러지면서 영상은 마무리된다.""                                       + ""'오징어 게임'에서 주연 배우로 활약한 이정재, 박해수, 정호연, 위하준은 6일(현지시간 5일) 지미 팰런쇼 녹화에 참여했다. 다만 신종 코로나바이러스 감염증(코로나19) 상황을 고려해 한국과 미국을 화상으로 연결해 인터뷰하는 방식으로 진행됐다. 해당 방송은 7일 오후에 공개될 예정이다.""                                       + ""한편, 지난 9월 17일 공개된 오징어 게임은 456억 원의 상금이 걸린 의문의 서바이벌 게임에 참가한 사람들이 최후의 승자가 되기 위해 목숨을 걸고 극한의 게임에 도전하는 내용을 담고 있다. 연일 기준 글로벌 OTT 콘텐츠 순위 집계에서도 1위를 차지하며 흥행을 이어가고 있다.""                                       + ""'오징어 게임'의 흥행에 힘입어 출연 배우들의 인스타그램 팔로워는 폭증했다.""                                       + ""새벽 역의 정호연은 7일 9시 기준 1562만 명이 인스타그램을 팔로우했다. 이는 한국 여자 배우 중 팔로워 최고 기록이다. '오징어 게임' 공개 전 그의 팔로워는 약 40만명이었다.""                                       + ""이 놀라운 효과에 주인공 기훈 역의 이정재, 상우 역의 박해수도 뒤늦게 인스타그램 계정을 개설했다. 이정재 인스타그램은 개설 5일만에 500만 팔로우로 껑충 뛰었다."").build())                                 .option(TextSummaryRequestDto.OptionRequestDto.builder().language(""ko"").build())                                 .build();   final TextSummaryResponseDto responseDto = aiNaverService.textSummary(requestDto);   System.out.println(""Plan Text : "" + requestDto.getDocument().getContent());   System.out.println(""Summary Text : "" + responseDto.getSummary());} 👉요청 : 넷플릭스 한국 드라마 '오징어 게임' 출연진이 미국 NBC 유명 토크쇼 '지미 팰런 쇼' 출연을 예고한 가운데 진행자이자 유명 코미디언 지미 팰런이 달고나 게임에 직접 도전했다. 6일 지미 팰런은 자신의 사회관계망 서비스(SNS)를 통해 'Squid game Cookie'라는 제목의 짧은 영상을 게재했다. 영상에 따르면 지미 팰런이 직접 프라이팬에 소다와 설탕을 붓고 달고나 제조한다. 이후 지미 팰런은 자신의 이름 이니셜인 'JF'가 새겨진 달고나를 직접 손에 들고 있다. 열심히 달고나를 핥아봤지만 바늘을 대는 순간 달고나가 부서지면서 총소리가 울린다. 이어 지미 팰런은 바닥에 쓰러지면서 영상은 마무리된다.'오징어 게임'에서 주연 배우로 활약한 이정재, 박해수, 정호연, 위하준은 6일(현지시간 5일) 지미 팰런 쇼 녹화에 참여했다. 다만 신종 코로나바이러스 감염증(코로나19) 상황을 고려해 한국과 미국을 화상으로 연결해 인터뷰하는 방식으로 진행됐다. 해당 방송은 7일 오후에 공개될 예정이다. 한편, 지난 9월 17일 공개된 오징어 게임은 456억 원의 상금이 걸린 의문의 서바이벌 게임에 참가한 사람들이 최후의 승자가 되기 위해 목숨을 걸고 극한의 게임에 도전하는 내용을 담고 있다. 연일 기준 글로벌 OTT 콘텐츠 순위 집계에서도 1위를 차지하며 흥행을 이어가고 있다.'오징어 게임'의 흥행에 힘입어 출연 배우들의 인스타그램 팔로워는 폭증했다. 새벽 역의 정호연은 7일 9시 기준 1562만 명이 인스타그램을 팔로우했다. 이는 한국 여자 배우 중 팔로워 최고 기록이다. '오징어 게임' 공개 전 그의 팔로워는 약 40만 명이었다. 이 놀라운 효과에 주인공 기훈 역의 이정재, 상우 역의 박해수도 뒤늦게 인스타그램 계정을 개설했다. 이정재 인스타그램은 개설 5일 만에 500만 팔로우로 껑충 뛰었다.​👉결과 : 넷플릭스 한국 드라마 '오징어 게임' 출연진이 미국 NBC 유명 토크쇼 '지미 팰런 쇼' 출연을 예고한 가운데 진행자이자 유명 코미디언 지미 팰런이 달고나 게임에 직접 도전했다. '오징어 게임'에서 주연 배우로 활약한 이정재, 박해수, 정호연, 위하준은 6일(현지시간 5일) 지미 팰런 쇼 녹화에 참여했다. '오징어 게임'의 흥행에 힘입어 출연 배우들의 인스타그램 팔로워는 폭증했다. 이처럼 긴~ 내용을 간략하게 정리하여, 내용을 쉽게 인식할 수 있습니다! 세 번째는 Search Trend입니다.👉샘플 확인 (github 링크)  public void searchTrend() {   final SearchTrendRequestDto requestDto = SearchTrendRequestDto.builder()                                                   .startDate(""2021-09-09"")                                                   .endDate(""2021-10-09"")                                                   .timeUnit(""date"")                                                   .keywordGroups(Arrays.asList(                                                      SearchTrendRequestDto.KeywordRequestDto.builder().groupName(""오징어게임"").keywords(Arrays.asList(""오징어게임"")).build()))                                                   .build();   final SearchTrendResponseDto responseDto = aiNaverService.searchTrend(requestDto);   System.out.println(""Search Trend Keywords : "" +  requestDto.getKeywordGroups());   System.out.println(""Search Trend Result"");   System.out.println(responseDto.getResults().get(0).getData().toString().replaceAll(""\\),"", ""\\)\n""));} 👉요청 : 2021.09.09~2021.10.09 사이에 “오징어게임” 트렌드 분석👉결과 :SearchTrendResponseDto.SearchTrendDateResultResponseDto(period=2021-09-09, ratio=2.25179)SearchTrendResponseDto.SearchTrendDateResultResponseDto(period=2021-09-10, ratio=2.33381)SearchTrendResponseDto.SearchTrendDateResultResponseDto(period=2021-09-11, ratio=2.48864)SearchTrendResponseDto.SearchTrendDateResultResponseDto(period=2021-09-12, ratio=2.28764)SearchTrendResponseDto.SearchTrendDateResultResponseDto(period=2021-09-13, ratio=2.42253)SearchTrendResponseDto.SearchTrendDateResultResponseDto(period=2021-09-14, ratio=2.40622)SearchTrendResponseDto.SearchTrendDateResultResponseDto(period=2021-09-15, ratio=3.91346)SearchTrendResponseDto.SearchTrendDateResultResponseDto(period=2021-09-16, ratio=6.48909)SearchTrendResponseDto.SearchTrendDateResultResponseDto(period=2021-09-17, ratio=44.91828)SearchTrendResponseDto.SearchTrendDateResultResponseDto(period=2021-09-18, ratio=100.0)SearchTrendResponseDto.SearchTrendDateResultResponseDto(period=2021-09-19, ratio=98.0899)SearchTrendResponseDto.SearchTrendDateResultResponseDto(period=2021-09-20, ratio=80.13696)SearchTrendResponseDto.SearchTrendDateResultResponseDto(period=2021-09-21, ratio=79.79139)SearchTrendResponseDto.SearchTrendDateResultResponseDto(period=2021-09-22, ratio=88.06388)SearchTrendResponseDto.SearchTrendDateResultResponseDto(period=2021-09-23, ratio=86.76977)SearchTrendResponseDto.SearchTrendDateResultResponseDto(period=2021-09-24, ratio=68.51924)SearchTrendResponseDto.SearchTrendDateResultResponseDto(period=2021-09-25, ratio=71.65734)SearchTrendResponseDto.SearchTrendDateResultResponseDto(period=2021-09-26, ratio=56.47573)SearchTrendResponseDto.SearchTrendDateResultResponseDto(period=2021-09-27, ratio=51.27697)SearchTrendResponseDto.SearchTrendDateResultResponseDto(period=2021-09-28, ratio=46.20704)SearchTrendResponseDto.SearchTrendDateResultResponseDto(period=2021-09-29, ratio=37.01326)SearchTrendResponseDto.SearchTrendDateResultResponseDto(period=2021-09-30, ratio=35.33568)SearchTrendResponseDto.SearchTrendDateResultResponseDto(period=2021-10-01, ratio=34.60255)SearchTrendResponseDto.SearchTrendDateResultResponseDto(period=2021-10-02, ratio=34.18551)SearchTrendResponseDto.SearchTrendDateResultResponseDto(period=2021-10-03, ratio=36.01477)SearchTrendResponseDto.SearchTrendDateResultResponseDto(period=2021-10-04, ratio=36.10025)SearchTrendResponseDto.SearchTrendDateResultResponseDto(period=2021-10-05, ratio=30.71705)SearchTrendResponseDto.SearchTrendDateResultResponseDto(period=2021-10-06, ratio=23.82046)SearchTrendResponseDto.SearchTrendDateResultResponseDto(period=2021-10-07, ratio=22.8062)SearchTrendResponseDto.SearchTrendDateResultResponseDto(period=2021-10-08, ratio=20.29227)​결과를 토대로 분석해 보면, 추석이 다가오며 오징어게임에 대한 관심이 높아졌으며, 그 열기가 식지 않고 유지되는 것을 알 수 있습니다.​  오징어게임 주연 배우 검색 데이터도 포함하여 결괏값을 그래프로 표현해 보았습니다. TV 쇼에 대한 인기와 함께, 출연 배우분들에 대한 관심도 함께 증가했음이 데이터로 증명됐습니다.   총 3편을 통해 살펴본 AI·NAVER API 맛보기는 여기까지입니다. 가이드를 통해 고객분들께서 API를 다양하게 활용하여 서비스를 만들고 이용할 수 있길 바라는 마음입니다. 앞으로도 네이버 클라우드 플랫폼을 보다 편하게 활용하실 수 있도록 다양한 콘텐츠로 만나 뵙겠습니다.  감사합니다.​* 네이버 클라우드 플랫폼 API 활용하기 방법은 온라인 무료교육 영상으로도 살펴보실 수 있습니다.​   ​ * 본 포스팅과 관련된 궁금증은 댓글로 남겨주시면 답변드리겠습니다.​ 누구나 쉽게 시작하는 클라우드 - ncloud.com​ "
비상교육 X 디비북 서비스 론칭 이벤트 ,https://blog.naver.com/eunyoung5193/221501880023,20190331,"비상교재를 더 편하게, 더 빠르게 만나는 방법비상교육 X 디비북 서비스 론칭 이벤트이벤트 기간2019년 03월 25일(월) ~ 2019년 4월 21일(일)당첨자 발표2019년 4월 25일(목)비상교육만의 양질의 영어 교재를 더 저렴하고 더 편리하게 디비북으로 만나 보세요!디비북은 조금 특별한 외국어 전문 전자책 서비스입니다.학생도 만족!공부하던 교재 그대로! 온라인으로 손쉽게 접속오디오 서비스로 공부를 더 생생하게!모르는 내용은 메신저로 선생님과 바로 소통선생님도 만족!프로그램 설치 X, 인터넷만 있으면 OK학습 로그 분석 및 숙제검사, 학생 코멘트 가능합리적인 가격차별화된 서비스로 학생과 선생님 모두를 만족시켜 영어 학습 시너지가 향상됩니다.다양한 부가 콘텐츠단어, 숙어가 들어간 유튜브 영상 검색 및 연동모든 단어, 지문에 MP3 제공DvAnalytics를 통해 나의 학습 진도율, 정오답률 등 분석 제공영어 학습 특화 기능북마크 기능을 활용하여 나만의 단어장 만들기 가능책 별 오답노트가 생성되어 복습 가능음성 인식(Speech Recognition) 기능을 이용한 스피킹 테스트 가능구글 번역으로 해설 제공선생님 전용 서비스디비북에서 제공하는 모든 콘텐츠 공유(단어, 문제, 동영상 강의, 예문, 지문)메신저 기능으로 학생과 학습내용 공유, 질의 응답 가능학습자 분석 기능을 통한 코멘트 주기, 숙제 검사 서비스 제공   ​ "
NeurIPS 2020에서 발표된 퀄컴의 최신 AI 연구 소식 ,https://blog.naver.com/qualcommkr/222197254445,20210106,"NeurIPS(Neural Information Processing Systems)는 AI 연구자 및 엔지니어가 모이는 최대 규모의 연례 학회로, 새로운 발견을 공유하고 협업하며 함께 AI 산업 발전을 도모하는 장입니다. 코로나 바이러스 팬데믹으로 인해 올해 행사는 온라인으로 진행했지만, 해당 학회는 쉬지 않고 산업을 변화시키고 삶을 풍요롭게 만들 새로운 발견으로 AI 산업을 이끌어 왔습니다. NeurIPS 2019 이후 지능형 기기 및 서비스는 점점 우리 일상에 들어와 향상된 카메라와 음성 UI, 개인화 등의 주요한 이점을 제공하며 AI 연구 및 제품에 대한 니즈를 꾸준히 생산하고 있습니다. 만약 이 글을 읽는 여러분이 올해 NeurIPS 학회에 온라인으로 참석했거나, 혹은 퀄컴 AI 리서치가 어떤 기관인지 궁금해했던 분이라면 이번 포스팅이 매우 흥미롭게 다가올 겁니다. 퀄컴이 올해 NeruIPS 2020에서 발표한 최신 논문과 데모, 워크숍, 대담, 그리고 그 외 AI 관련 소식을 함께 살펴보겠습니다.​​NeurIPS 2020에서 채택된 퀄컴의 논문NeurIPS와 같은 학회에서 새로운 논문은 혁신적이고 영향력 있는 AI 연구를 AI 커뮤니티에 알릴 수 있는 주요 방식입니다. 아래 세 가지는 이번 학술대회에서 채택된 퀄컴의 논문들인데요. 전력 효율성과 머신 러닝 기반 연구를 발전시킬 연구라 할 수 있습니다.​● “Bayesian Bits: Unifying Quantization and Pruning”이 논문은 기울기 기반 최적화를 통한 합동 혼합 정밀도 양자화 및 프루닝(pruning)을 위한 실용적인 방법을 보여줍니다. 모델 정확도를 잃지 않으면서 양자화와 프루닝을 향상하는 것은 연산 수요를 줄이고 전력 효율성을 높이는 데 매우 중요합니다. 이 연구 결과는 고정 비트 폭을 지닌 네트워크보다 정확도 및 효율성 사이에 더 나은 균형을 제공하는 프루닝된 혼합 정밀도 네트워크를 학습할 수 있음을 보여줍니다. ​● “Structured Convolutions for Efficient Neural Network Design”이 논문은 컨볼루션 신경망(CNN) 빌딩 블록의 내형 구조에서 중복성을 활용하여 모델 효율성을 살펴봅니다. 구조화된 컨볼루션 덕분에 컨볼루션 연산을 합 풀링(sum-pooling) 연산으로 분해할 수 있고, 매우 낮은 복잡성과 적은 가중치의 컨볼루션도 가능해집니다. 광범위한 CNN 구조에 퀄컴의 방식을 적용하면 최대 2배까지 줄어든 ‘구조화된’ 버전의 ResNets과 같이 상당한 개선을 확인할 수 있습니다.​● “Natural Graph Networks”이 논문은 그래프 구조 데이터를 위한 신경망 알고리즘에 대한 기초 연구를 살펴봅니다. 이 논문에서는 업데이트 집계가 순열 불변형인 기존 그래프 컨볼루션 네트워크나 메시지 전달 네트워크를 적용하는 대신 앞서 언급한 제한사항을 제거하는 자연성 개념을 연구합니다. 더불어 그래프 네트워크가 잘 정의되어 더 큰 클래스의 그래프 네트워크를 여는 데 이 개념이 충분하다는 점도 보여줍니다. 여기에, 등변성 메시지 네트워크 모수화를 사용하는 그래프에서 신경망의 실용적인 예시화를 제공하며 여러 벤치마크에서 얻은 긍정적인 성과도 함께 공유하고 있습니다.​​관련 데모퀄컴의 연구는 단지 이론에 그치지 않습니다. 데모를 통해 AI 연구를 실생활에 적용하죠. 전력 효율성 부문의 발전 사항과 흥미로운 AI 적용 사례를 영상을 통해 여러분과 공유하고자 합니다.​온디바이스 그룹 등변성 CNN기존의 CNN은 회전과 같은 특정 대칭 변화에 약합니다. 그룹 등변성 CNN(G-CNN)은 임의의 회전(arbitrary rotation)과 같은 다양한 변화에 맞게 일반화될 수 있지만, 이를 핸드폰에 구현하는 건 매우 복잡합니다. 등변성 CNN은 연산 효율적이면서 데이터 효율적이고, 강력한 성능도 제공합니다. 그래서 최초로, 퀄컴은 전화기에서 G-CNN을 실시간으로 구동해 보았습니다. 아래 데모 영상은 G-CNN이 분할 맵핑 작업에서 기존의 CNN 대비 향상된 성능을 보이면서 림프절 조직 스캔에서 각 픽셀을 양성 혹은 악성으로 바르게 구분하는 방식을 보여줍니다. 조직 스캔에는 일정한 방향이라는 게 없으므로 스캔 이미지를 10도씩 천천히 돌리면서 분할 작업을 실행합니다. G-CNN을 활용하면 더 정확하고 안정적이며 높은 신뢰도로 분류할 수 있습니다. ​AdaRound 기반 신경망 양자화퀄컴 AI 리서치는 모델 정확도는 유지하면서 전력 효율적인 고정 소수점 추론을 가능케 하는 최첨단 양자화 기술을 개발해 왔습니다. 작년에는 데이터 프리 양자화(Data Free Quantization, DFQ)를 선보였죠. 올해에는 한 단계 더 나아가 AdaRound로 정확도를 향상하고 4비트 양자화를 구현했습니다. AdaRound는 훈련 후 양자화 기술로 적응형 반올림(Adaptive Rounding)을 줄인 말인데요. 레이블이 없는 최소한의 데이터만 있으면 되며 모델 미세 조정이 필요하지 않습니다. AdaRound는 양자화 중에 가장 근접한 값으로 반올림하는 대신, 모델 정확도를 유지하기 위해 최적의 반올림 선택지를 자동으로 찾습니다. 아래의 데모에서는 AdaRound와 기본 양자화를 적용한 두 사례를 함께 보여줍니다.​• 개체 감지 모델에서의 8비트 가중치와 8비트 활성화 양자화• 시맨틱 분할 모델에서 4비트 가중치와 8비트 활성화 양자화​두 가지 사례 중 AdaRound로 양자화된 모델은 기본 양자화 모델 대비 높은 정확도를 보입니다. 더욱이 AdaRound 양자화 모델은 32비트 부동 소수점 모델 대비 훨씬 더 전력 효율적이기도 합니다. 예를 들어 4비트 가중치 양자화는 와트 당 성능이 8배 이상 높아집니다. 참고로 AdaRound는 첨단 모델 효율성 성능을 위한 퀄컴 이노베이션 센터(QuIC) 오픈소스 프로젝트인 AI 모델 효율성 툴킷(AIMET)에서 추가될 예정이니 기대해주세요. 해당 프로젝트는 깃허브에서 확인할 수 있습니다. NeurIPS 2020 Demo: Neural network quantization with AdaRound | QualcommNeural-network models can be very large and compute intensive, which can make them challenging to run on edge devices. Model quantization provides significant benefits in power and memory efficiency, as well as latency.www.qualcomm.com ​효율적인 고화질 영상 시맨틱 분할요즘 영상은 고 데이터 비율 소스입니다. AI로 영상을 분석하면 자율주행이나 스마트 카메라, 확장현실(XR)과 같은 다양한 용처에 유용한 통찰력과 기능을 제공할 수 있습니다. 하지만 영상 화질과 프레임율이 높아지면서 AI 모델도 더 복잡해짐에 따라 실시간으로 이러한 작업 부하를 처리하는 게 더 어려워지고 있지요. 아래의 비교 영상은 모바일 기기에서 영상이 실시간으로 스트리밍되는 모습을 보여주는데요. 퀄컴의 접근 방식은 기존의 모델 대비 2048x1024 영상에서 연산 복잡성을 78 GMACs(Giga-Multiply-and-Accumulator-Count)에서 17GMACs로 줄입니다. 게다가 프레임 당 추론 시간도 74ms(밀리세컨드)에서 26ms로 단축됩니다. 이는 지연 시간에 민감한 사례에서 매우 주요한 차이점을 만듭니다.  NeurIPS 2020 Demo: Efficient semantic segmentation of high-resolution video | QualcommAnalyzing video with AI can provide valuable insights and capabilities for many applications such as autonomous driving, smart cameras, and extended reality.www.qualcomm.com ​​그 외 공유 사항최근 퀄컴이 발표한 퀄컴 스냅드래곤 888 모바일 플랫폼에는 6세대 퀄컴 AI 엔진이 포함되어 있는데요. 이전 세대 대비 향상된 기능과 프로세싱 성능을 제공하는 이번 6세대 엔진은 26TOPS(1초당 26조 회 연산) 성능을 자랑합니다. 이번 학술대회에서도 영상 업스케일 및 음성 잡음 억제에 사용되는 온디바이스 AI 등 고객사에서 이를 활용하는 데모를 함께 선보이기도 했습니다.​워크숍, 소셜 미팅, 토론, 포스터 발표퀄컴 AI 리서치는 이번 NeurIPS 2020에 스폰서로 참여하면서 아래와 같은 다양한 프로그램을 통해 최신 어젠다 및 연구 활동을 공유하기도 했습니다. • [워크숍] 미분 기하학과 딥러닝의 만남(Differential Geometry meets Deep Learning)– (초청 대담2) 기하학적 딥러닝의 게이지 이론(Gauge Theory in Geometric Deep Learning)• AIMET Overview Talk at the sponsor webpage on Dec 6th, 10:30 – 11 a.m. PT (Expo Day).• [대담] AIMET 개요 (스폰서 페이지)• Bayesian Bits: Unifying Quantization and Pruning (poster session on Dec 9th, 9 - 11 a.m. PT). Interview with the authors at the sponsor webpage on Dec 10th, 10:00 - 10:30 a.m. PT.• [포스터 세션] 베이지안 비트: 양자화 및 프루닝 통합(Bayesian Bits: Unifying Quantization and Pruning). 저자 인터뷰(스폰서 페이지) • [포스터 세션] 효율적인 신경망 설계를 위한 구조적 컨볼루션(Structured Convolutions for Efficient Neural Network Design). 저자 인터뷰(스폰서 페이지)• [포스터 세션] 자연 그래프 네트워크(Natural Graph Networks). 저자 인터뷰(스폰서 페이지)• [소셜 프로그램] 팬데믹 상황에서 머신러닝(ML) 커리어 시작/전환하기– 머신 러닝 분야에서 가상 환경으로 커리어를 시작하면서 발생하는 어려움과 기회​​AIMET 진척사항 및 실제 적용오픈 소스로 공개되고 오랜 시간이 지나지 않았지만 AIMET은 이미 광범위한 AI 생태계를 지원하고 있으며 실제로 다양한 산업 버티컬에 영향을 주고 있습니다. 스마트폰의 경우 AIMET은 다양한 OEM과 ISV가 스타일 전송이나 이미지 뷰티 필터와 같은 증강현실(AR) 사용사례를 위한 카메라 애플리케이션을 가속할 수 있도록 돕고 있습니다. 자동차 부문에서는 위의 데모 영상과 유사한 ADAS 사용사례 개체 감지 모델에 AIMET 모델 양자화를 사용하고 있습니다. 정확도를 잃지 않으면서도 와트당 성능을 향상할 수 있는 능력은 고화질 카메라가 여러 대 동시에 구동되는 자동차에서 매우 중요합니다. 데이터 센터에서는 AIMET의 모델 압축 기술을 사용해 Resnet-50과 같이 비전 작업의 중추 역할을 하는 대중적인 모델의 초당 추론 성능을 높이고 있습니다. 앞으로도 AIMET에 첨단 AI 모델 효율성 연구를 적용해 오디오와 음성, 비디오와 같은 다른 사용사례까지 지원할 수 있게 되길 바랍니다.​앞으로도 NeruIPS를 비롯한 여러 AI 컨퍼런스에서 여러분과 (가상으로도) 만나 AI 분야에서 퀄컴이 미치고 있는 영향력에 대해 공유할 시간이 있으면 좋겠습니다. 퀄컴은 기초 연구에서 획기적인 발명을 만들고 이를 제품과 산업으로 확산시킵니다. 퀄컴 AI 리서치는 퀄컴의 다른 부문과도 협력하여 최신 AI 개발 및 기술을 퀄컴의 제품에 통합합니다. 이를 통해 연구실에서 진행되던 연구가 실제 기술이 되어 우리 삶을 풍요롭게 만들어 주는 데 걸리는 시간을 단축하고 있지요.  최첨단 AI 연구를 통해 세계가 당면한 문제를 해결하고 수십억 인구의 삶의 질을 향상하는 데 보탬이 되고 싶나요? 퀄컴은 언제나 열려 있습니다. 퀄컴은 현재 머신러닝을 비롯한 여러 분야의 인재를 기다리고 있습니다. 여러분과 함께 AI의 미래를 그려볼 날을 고대하고 있겠습니다! Machine Learning Researcher – On-device AI theory and algorithmMachine Learning Research - Speech RecognitionMachine Learning Researcher - Speech SynthesisMachine Learning Researcher – Audio and SpeechMachine Learning Senior Software Engineer – Embedded AI FrameworkMachine Learning Researcher (Intern)Deep Learning Systems Research Engineer on Camera Perception for Autonomous DrivingComputer Vision and Machine Learning R&D Intern for Autonomous Driving   ​ "
자바스크립트/음성인식/Web Speech ,https://blog.naver.com/scyan2011/222525219194,20211003,"자바스크립트의 Web Speech API를 사용하면 음성인식앱을 만들 수 있습니다.. 물론 구형의 웹브라우저에서는 작동하지 않지만 엣지, 크롬등 최신 버전의 웹브라우저에서 사용이 가능합니다. ​음성 인식​객체 생성​webkitSpeechRecognition() 함수를 이용하여 객체를 생성합니다.​var recognition = new webkitSpeechRecognition();​속성 설정​continuous - 연속 인식 가능 여부  true/ falseinterimResults - 음성 변화 인식   true / false​결과 출력​onresult 속성을 이용하여 음성인식 결과를 출력합니다. 이번 예제에서는 텍스트박스에서 인식된 텍스트를 출력합니다.  recognition.onresult = function (e) {        var textarea = document.getElementById('results');        for (var i = e.resultIndex; i < e.results.length; ++i) {            if (e.results[i].isFinal) {                textarea.value += e.results[i][0].transcript;            }        }    } 예제​간단한 음성인식 앱의 예제입니다. 첨부파일recognition.html파일 다운로드 <html><body><textarea id=""results"" cols=""80"" rows=""5""></textarea> <script>    var recognition = new webkitSpeechRecognition();    recognition.continuous = true;    recognition.interimResults = true;     recognition.onresult = function (e) {        var textarea = document.getElementById('results');        for (var i = e.resultIndex; i < e.results.length; ++i) {            if (e.results[i].isFinal) {                textarea.value += e.results[i][0].transcript;            }        }    }     // 음성 인식 시작    recognition.start(); </script></body></html> 실행​한국어와 영어를 구분하여 인식합니다^^​  ​  ​ "
Types of Machine Learning Algorithms ,https://blog.naver.com/yoono044/223022939067,20230221,"​Machine Learning is a powerful technology that allows machines to learn from data and make predictions. There are different types of Machine Learning algorithms that can be used to enable machines to learn from data. In this article, we will explore the different types of Machine Learning algorithms, including supervised and unsupervised learning.​​ https://www.researchgate.net/profile/Alessio_Rossi/publication/316991321/figure/download/fig3/AS:613909073494032@1523378737404/Machine-learning-problems-classification-and-useful-algorithms.png​Supervised Learning​Supervised Learning is a type of Machine Learning algorithm that involves training a machine using labeled data. Labeled data is data that has already been classified or categorized. The algorithm learns from the labeled data and makes predictions based on that learning.​Supervised Learning is used for tasks such as regression, where the goal is to predict a continuous variable, and classification, where the goal is to predict a categorical variable. Examples of applications that use supervised learning include image and speech recognition, sentiment analysis, and fraud detection.​Some popular supervised learning algorithms include Linear Regression, Logistic Regression, Decision Trees, Random Forest, Support Vector Machines, and Artificial Neural Networks.​Unsupervised Learning​Unsupervised Learning is a type of Machine Learning algorithm that involves training a machine using unlabeled data. Unlabeled data is data that has not been classified or categorized. The algorithm learns from the unlabeled data and tries to find patterns and relationships in the data.​Unsupervised Learning is used for tasks such as clustering, where the goal is to group similar data points together, and anomaly detection, where the goal is to identify unusual data points. Examples of applications that use unsupervised learning include customer segmentation, recommendation systems, and data compression.​Some popular unsupervised learning algorithms include K-Means Clustering, Hierarchical Clustering, Principal Component Analysis, and t-SNE.​Semi-Supervised Learning​Semi-Supervised Learning is a type of Machine Learning algorithm that involves training a machine using both labeled and unlabeled data. Semi-Supervised Learning is used when there is not enough labeled data available, and it is too expensive or time-consuming to label more data.​Semi-Supervised Learning is used for tasks such as text classification, where the goal is to categorize text into different topics, and image classification, where the goal is to classify images into different categories.​Some popular semi-supervised learning algorithms include Label Propagation, Label Spreading, and Co-Training.​Reinforcement Learning​Reinforcement Learning is a type of Machine Learning algorithm that involves training a machine to make decisions based on a reward system. The algorithm learns from its actions and adjusts its behavior based on the feedback it receives.​Reinforcement Learning is used for tasks such as game playing, robotics, and self-driving cars. Examples of popular reinforcement learning algorithms include Q-Learning, SARSA, and Deep Q-Networks.​Conclusion​In conclusion, there are several types of Machine Learning algorithms, including supervised learning, unsupervised learning, semi-supervised learning, and reinforcement learning. Each algorithm has its own strengths and weaknesses and is used for different applications.​It is essential to understand the different types of Machine Learning algorithms to choose the right algorithm for your application. With the vast amount of data being generated every day, Machine Learning is becoming more critical than ever, and understanding its different algorithms is essential to leverage its potential.​​ "
[로보데이터] “의료 서비스에 특화된 음성인식 프로그램 구축해야” ,https://blog.naver.com/robodata/222973814715,20230103,"성균관의대 등 공동연구팀, 삼성서울병원 외래환자 112명 상담 분석증상·질병명 인식 정확도, 네이버 78.9%·아마존 64.7%·구글 53.5% 순일반적으로 잘 사용되지 않는 의학용어…음성인식 인공지능 훈련 장벽 최근 재택치료 등 의료현장에서 음성을 텍스트로 바꿔주는 프로그램 사용이 증가하는 가운데, 의료 서비스에 특화된 음성인식 프로그램을 구축해야 한다는 주장이 제기됐다.​성균관의대·서울의대·아주대 대학원 의생명과학과 연구팀은 지난 4일 이같은 연구 결과를 담은 논문 ‘Accuracy of Cloud-Based Speech Recognition Open Application Programming Interface for Medical Terms of Korean’을 국제학술지 ‘JKMS(Journal of Korean medical science)’에 발표했다.​ 성균관의대·서울의대·아주대 대학원 의생명과학과 연구팀은 지난 4일 이같은 연구 결과를 담은 논문 ‘Accuracy of Cloud-Based Speech Recognition Open Application Programming Interface for Medical Terms of Korean’을 국제학술지 ‘JKMS(Journal of Korean medical science)’에 발표했다(사진출처: JKMS).​클라우드 기반의 음성인식 개방형 API(Application Programming Interface)는 누구나 사용할 수 있도록 서비스를 제공함으로써 다양한 분야에서 활용되고 있다. 하지만 의료 서비스와 관련된 정확성에 대한 연구 데이터가 부족한 실정이다.​이에 연구진들은 클라우드 기반의 음성인식 개방형 API인 네이버의 ‘클로바(CLOVA)’, 구글의 ‘Speech-to-Text’, 아마존의 ‘Transcribe’가 의학용어를 인식하는 정확도를 분석했다.​연구진은 지난 2021년 4월부터 7월까지 삼성서울병원 심장센터에 내원한 외래환자 중 음성 기록에 동의한 환자 112명을 대상으로 진료 내용을 녹음한 뒤 세 음성인식 API의 정확도를 비교했다.​그 결과, 전체적인 단어를 인식하는 정확도는 클로바가 75.1%로 가장 높았으며 다음이 Transcribe 57.9%, Speech-to-Text 50.9% 순이었다.​증상이나 질병명에서는 클로바가 78.9%로 가장 높은 정확도를 보였으며, Transcribe(63.7%), Speech-to-Text(53.5%)가 뒤를 이었다. 장기의 명칭과 위치에서도 클로바 84.1% Transcribe 72.9%, Speech to Text 57.1%를 기록했다.​수술·약·심장 등 진료 시 가장 많이 언급됐던 의학용어 10개를 추출해 따로 분석한 결과, 클로바가 가장 높은 정확도를 보였다.​하지만 5글자의 단어를 인식하는데 클로바, Speech-to-Text, Transcribe가 각각 52.6%, 56.3%, 36.6%의 정확도를 보이며, 장문의 단어에서는 클로바 외 음성인식 API들이 강점을 보이기도 했다.​ 이에 연구진은 의료 서비스에 특화된 음성인식 API를 구축할 필요가 있다고 했다. 일반적인 클라우드 기반의 음성인식 API에서는 현실적으로 의학용어에 대한 훈련이 어렵다는 것.​연구진은 “이번 연구는 한국어로 진행돼 다른 언어와 비교할 수 없으며, 질병의 종류도 심장과 관련된 질환으로 한정된 한계가 있다”면서도 “클로바의 정확도는 80% 미만으로, 이는 클라우드 기반의 음성인식 API가 의사와 환자 간 대화를 인식하는데 한계가 있다는 것을 의미한다”고 말했다.​연구진은 “의학용어의 특성이 낮은 정확도의 원인으로 작용했을 가능성도 있다”며 “음성인식 인공지능을 훈련할 때 사용하는 일반적인 토론에서 의학용어가 상대적으로 덜 사용되기 때문에 이를 훈련하는 것은 어려운 작업이다. 예를 들어 클로바는 ‘순환기내과’를 ‘술 한잔’으로 인식하기도 했다”고 했다.​그러면서 “의료 서비스에 특화된 클라우드 기반의 음성인식 API를 개발할 필요성이 있다”며 “연구에서 세 API는 단어의 길이와 종류에 따라 각기 다른 강점을 보였다. 이를 결합한다면 의학용어 대한 정확도를 높이고 성능도 향상시킬 수 있을 것”이라고 말했다.​출처 : 청년의사(http://www.docdocdoc.co.kr) "
"호주 EF 어학연수 -시드니, 브리즈번, 퍼스 ",https://blog.naver.com/raffles7/222839549135,20220804,"오늘은 호주 어학연수에 대해 안내드리려고 합니다.​ ​호주로 영어 연수를 가면 어떠한 장점이 있을까요?  우선 영국과 비교하여 볼 때 날씨가 화창한 편이에요.  야외 액티비티를 즐기기에 더할 나위 없죠.미국이나 영국에 비해  우리나라와 시차가 많이 나지 않습니다.   홀로 어학연수 가 있는 자녀의 안부를 확인하기 위해 밤잠 설치실 필요가 없죠.  호주는 드넓은 나라입니다.  동과 서의 자연환경도 많이 다르고요.  많은 학생들이 호주에서 어학연수를 하시는 동안 호주 여행을 많이 하십니다.  여행뿐 아니라 연수하시는 동안 EF 시드니, 브리즈번, 퍼스 캠퍼스로 교환학생도 신청하셔서 다양한 호주를 경험하고 계십니다​.그리고 호주는 학생비자 소지자도 주 20시간까지 파트타임 일을 할 수 있습니다.  어학연수하면서 합법적으로 돈도 벌 수 있으니 휴학하며 어학연수를 계획 중인 대학생들에게 추천하는 나라입니다. ​​세계적인 어학연수 교육기관인 EF는 호주 시드니, 브리스번, 퍼스 이렇게 3곳에 캠퍼스를 가지고 있습니다.​ ​​​​​ 호주 EF 캠퍼스들​​1.  우선 학생들에게  제일 인기 많은 국제도시 시드니 캠퍼스부터 소개 드릴게요.​ ​​​EF 시드니의 수업은 다음과 같이 진행되고 있습니다.​ ​​휴학도 하고 비싼 돈 드려서 간 어학연수이니 만큼 한국인이 적은 곳을 선호하실 텐데 시드니 캠퍼스는 한국인 비율이 2~3% 정도로 낮습니다. ​EF 하면 교실에서 하는 전통적인 수업 이외에도 다양한 액티비티가 있는 것이 특징이죠.  유료, 무료 액티비티가 다양하게 준비되어 있으니 영어를 빨리 늘리고 싶으신 분들은 적극적으로 신청하시길 바랍니다.  다양한 외국 친구들을 사귀는 기회도 될 거예요.​ ​​​해외연수는 영어교육기관만 등록해서 될 일이 아니죠?  EF는 국제학생들이 연수 기간 동안 연계 숙소를 가지고 있는 것이 장점입니다.  시드니같이 특히나 물가가 비싼 대도시로 연수 가시는 학생들은 EF의 학생 연계 숙소를 합리적인 가격으로 이용하실 수 있습니다.​ ​​​ EF 시드니 숙소​​2.  두 번째로 EF 브리즈번 캠퍼스를 소개합니다.​ ​EF 브리즈번의 수업은 다음과 같이 진행되고 있습니다.​ ​​EF 브리즈번도 한국인 비율이 2~3%로 매우 적은 편이고 시드니 캠퍼스에 비해 장기 연수하는 학기제 학생이 많은 편입니다.​ ​EF 브리즈번으로 가시는 학생은 다음과  같은 Student One이라는 학생 레지던스를 사용하시게 됩니다.​ ​3.  마지막으로 호주의 자연을 즐기시고 싶은 학생들에게는 EF 퍼스 캠퍼스를 추천드려요.​ ​​EF 퍼스의 수업은 다음과 같이 진행되고 있습니다. ​EF 퍼스 캠퍼스에도 한국인은 2~3% 낮아요.  그리고 퍼스 캠퍼스는 유럽 학생,  여학생 비율이 높은 편입니다.​ ​EF 퍼스로 가시는 학생은 다음과  같은 Campus Perth라는 학생 레지던스를 사용하시게 됩니다.​ ​​​EF의 수업 방식은 캠퍼스에 상관없이 동일하게 제공됩니다. EF의 어학연수과정은 다른 어학원보다 좀 더 듣기와 말하기 능력 항상에 포커스가 맞추어져 있습니다.​ ​위 영상에서 보신 EF의 수업 체계는 다음과 같이 5 가지의 수업으로 구성되어 있습니다.1. General Class : 저희도 익히 경험한 강남역 영어학원 같은 소규모 렉처 스타일 수업2. Conversation class : 다양한 디스커션 수업3. iPAD Class : 아이패드를 이용한 2-3명의 pair conversation 수업4. i Lab computer class: 유창한 영어를 위해서는 파트너와의 공부도 필요하나 학습자 스스로의 혼자 해야 할 공부들이 있죠? 바로 올바른 문법을 익히고 올바른 발음으로 자신의 생각을 정리하고 전달해야 합니다. 그를 위한 video lesson(온라인 문법 공부)과 speech recognition(발음 수정)을 컴퓨터를 통해 공부하고 itracker라는 툴을 통해 본인의 프로그레스를 확인할 수 있습니다.5. SPIN Class: Special Interest Class로 대학 교양처럼 호주 문화, 마케팅이나 저널리즘, 넷플릭스로 배우는 영어 같은 흥미 있는 과목 수업을 영어로 참여하는 수업입니다.​EF 호주  어학연수 프로그램에 대해 더 궁금하신 사항은 저희 링크에이드로 연락 주시면 친절히 답변드리겠습니다. ​  ​​링크에이드는 EF 어학연수 입학 수속을 무료로 도와드리고 있습니다.입학 허가를 받은 이후 출국 및 현지 정착까지 세심히 도와드리고 있습니다.관심 있으신 분들은 카카오톡 아이디 linkaid로 카톡 상담 가능합니다. 전화가 편하신 분들은 010-2473-0049로 전화 주시면 친절히 안내해 드리겠습니다.​ ​​​​ "
음성인식기술_엘솔루 ,https://blog.naver.com/run7572/222852231197,20220819,"엘솔루의 기계번역, 음성인식 및 자연어처리의 원천기술을 기반으로다양한 제품과 서비스를 제공하는 음성인식기술을 보유한 기업이다. ​기계번역 및 음성인식 기술로 인해 외국어 번역에 대한 두려움이 사라지기를 바라며 아래의 내용을 참조하시기 바랍니다. (하단 링크파일 참조) ​​음성 인식국내 최고의 한국어 및 영어 음성인식 기술한국어 및 영어에 최적화된 최고의 기술로, 화자의음성을 텍스트로 변환하여 여러 산업분야에 활용되는 기술​기계 번역세계 최고의 인공신경망 기계번역 기술한국어를 중심으로 약 150개 이상의 언어쌍을 지원하는기계번역 솔루션​자연어처리사람의 언어를 이해하는 자연어처리 기술딥러닝 및 머신러닝 알고리즘 기반의 자연어처리 기술로컴퓨터가 사람의 언어를 이해하여 다양하게 표현하는 기술​오픈 API 플랫폼누구나 사용할 수 있는 인공지능 기술기반 플랫폼여러 산업 분야에 적용할 수 있는 당사 보유 3가지 기술(음성인식, 기계번역, 자연어처리)을 저렴한 비용으로 사용​​Speech Recognition(음성인식)​ezDAS화자의 음성을 인식하여 텍스트로 변환시키는 솔루션​ezDAS는 인공지능 딥러닝 기술 기반의 음성인식 기술을 통해음성을 데이터로 변환 다양한 분석, 검색, 번역 등을 이행할 수 있는 음성인식 솔루션.​주요기능고객의 음성데이터를 학습하여 고품질 음성인식 기술 제공고객 산업분야에 최적화된 음성인식 기술 제공음성에서 추출된 텍스트는 다양한 2차 소프트웨어 활용실시간 음성인식 지원​특ㆍ장점음성인식 원천기술 보유 및 전문 엔지니어 보유최대 한국어, 영어, 일본어, 중국어 지원95% 이상의 음성인식률(콜센터 상담사 기준)적은 음성데이터만으로 도메인 특화엔진 개발 기능초고속 디코더 기술로 매우 빠른 음성인식 기술 제공최소 인프라환경에서 최고 음성인식 기술 제공​ezTalkyAI번역과 음성인식 기술이 융합되어 실시간으로 통번역 해주는 모바일 어플리케이션​이지토키는 현대화 사회의 필수품인 스마트폰에서 쉽고 빠르게4개 언어에 대해 실시간 번역 및 통역을 해주는 모바일 앱 솔루션.​주요기능한국어/영어/일본어/중국어 총 8개 언어쌍 지원실시간 통역 및 번역 서비스 제공언어별 주요 회화체 및 북마크 기능 제공​특ㆍ장점기관, 기업에 커스터마이징 된 독자적인 통번역기 개발 가능특정 산업에 특화된 전문 통번역기 개발 가능음성인식 원천기술을 고도화와 지속적인 성능개선음성인식 기반 다양한 서비스 개발 및 구축 가능​Subtitle Translation Service영상의 음성을 인식하여 자막을 자동 생성시켜주는 솔루션​유튜브, 인터넷 강의 및 온라인 비디오 등 영상으로부터음성인식 후 원하는 언어로 자막을 생성, 영상에 적용시킬 수 있는 서비스.​주요기능유튜브(YouTube), 인터넷강의, 온라인비디오 등 모든 영상의 음성을 인식하여다국어로 번역, 자막을 생성하여 제공해주는 서비스다양한 형식의 자막파일 제공(*.srt / *.xlsx / .txt)​특ㆍ장점세계 수준의 음성인식 기술 적용으로 높은 인식률 제공인공신경망 기계번역 솔루션으로 높은 번역품질 제공영상 별 특정 도메인에 최적화된 번역자막 제공14개 이상의 언어쌍 제공​Neural Machine Translation (기계번역)​1.ezNTS2.Desktop3.ezWeb4.TPS​ezNTS최신의 인공지능 알고리즘과 아키텍처가 적용된 인공신경망 기계번역 솔루션으로,최적의 환경에서 고객의 업무 효율성과 생산성을 향상시키는 인공지능 언어처리 솔루션입니다.​엘솔루에서 새롭게 출시한 인공신경망 기계번역 솔루션 ezNTS는 국내 최고의 AI 전문가들이 이전 번역솔루션의 단점을 보완하고 기존 고객들이 요구하였던 기능들을 추가 반영하여,가장 강력한 번역 품질과 사용자 편의성을 높인 인공지능 기계번역 소프트웨어입니다.이전 SPNS 보다 번역 성능과 품질이 고도화 된 ezNTS​세계 최고의 한국어 번역기를 중심으로 최대 150개 이상의 조합으로 번역을 지원세계 최초로 HWP(아래아 한글) 파일 번역을 포함 전세계 모든 문서 번역을 지원사용자 사전 및 번역 메모리를 통해 개인 및 기업에 특화된 번역 성능을 제공서버 구축형(On-premise)부터 API, 클라우드까지 다양한 시스템 사용 환경을 지원CPU에 최적화된 인공지능 아키텍처는 강력한 보안 기능과 GPU 수준의 처리 속도를 제공국내 최초로 시스템 운용 및 번역 사용 현황을 한 눈에 확인할 수 있는 모니터링 시스템을 제공​ezNTS 주요 기능텍스트 및 전세계 모든 문서 번역번역 향상을 위한 사용자 사전 및 번역 메모리제3의 서비스 및 솔루션에 적용 가능한 API시스템 운용 및 번역 사용 모니터링 Desktop시간과 장소에 제약받지 않고 다양한 문서 및 파일,그리고 웹페이지를 번역할 수 있도록 지원하는 데스크탑 자동 기계번역 솔루션입니다.​주요기능문서, 이메일, 웹페이지, 텍스트 외 파일 번역 제공한국어 중심으로 다국어 번역 제공PC 내 소프트웨어 설치로 데이터 보안의 안정성 제공 ezWeb이지웹은 엘솔루의 기계번역 기술을 통해홈페이지를 3개 국어로 자동번역해주는 솔루션입니다.​주요기능영어/일본어/중국어 홈페이지 동시 생성한글 홈페이지 레이아웃을 그대로 유지(문자열만 추출하여 번역)한글 홈페이지의 업데이트 내용 실시간으로 외국어 홈페이지에 반영기계번역 내용 모니터링으로 번역 품질 향상​특ㆍ장점업데이트 된 내용 또한 실시간 번역처리효율적인 비용 관리 및 절감 실현다국어 홈페이지의 운영으로 웹 접근성 향상특정산업분야에 특화된 번역품질 제공​원본출처 :  음성인식 / 기계번역 / 자연어처리 / 오픈 API 플랫폼 | LLSOLLU "
Artificial Intelligence ,https://blog.naver.com/steve666/223009251260,20230208,"Artificial Intelligence​Artificial Intelligence (AI) is a rapidly growing field that has the potential to revolutionize many industries, from healthcare to finance, retail to manufacturing, and beyond. In this blog post, we'll explore what AI is, how it works, and its various applications. We'll also take a look at some of the challenges and ethical considerations associated with AI and how we can ensure its responsible development and deployment.​What is Artificial Intelligence?Artificial Intelligence is a branch of computer science that focuses on creating intelligent machines that can perform tasks that typically require human intelligence, such as recognizing speech, understanding natural language, and making decisions. AI can be classified into two main categories: narrow or weak AI, which is designed for a specific task, and general or strong AI, which has the ability to perform a wide range of tasks.​How Does Artificial Intelligence Work?There are many different approaches to AI, but most rely on some combination of machine learning, computer vision, and natural language processing. Machine learning is the process of training a computer to learn from data and make predictions or decisions based on that data. Computer vision involves using algorithms to interpret and understand visual data, such as images or video. Natural language processing involves using algorithms to understand and generate human language, such as speech recognition or language translation.​​Applications of Artificial IntelligenceArtificial Intelligence has a wide range of applications across many industries, including:Healthcare: AI is being used to analyze medical images, such as X-rays and MRI scans, to help diagnose diseases. It can also be used to analyze patient data to predict outcomes and provide personalized treatment plans.Finance: AI is being used to analyze financial data to identify fraud and make investment decisions. It can also be used to automate routine tasks, such as account reconciliation and portfolio management.Retail: AI is being used to analyze customer data to personalize recommendations and advertisements, and to optimize pricing and inventory management.Manufacturing: AI is being used to optimize production processes, such as predictive maintenance, and to improve quality control.Transportation: AI is being used to optimize logistics, such as route planning and traffic management, and to improve safety, such as collision avoidance.​Challenges and Ethical ConsiderationsWhile AI has the potential to bring many benefits, it also presents a number of challenges and ethical considerations. For example:Bias: AI algorithms can be biased if they are trained on biased data, leading to unfair or harmful decisions.Privacy: AI algorithms can collect and analyze large amounts of personal data, raising concerns about privacy and security.Job Loss: AI has the potential to automate many jobs, leading to concerns about job loss and income inequality.To address these challenges and ensure the responsible development and deployment of AI, it is important to consider ethical principles, such as transparency, accountability, and fairness, and to involve a diverse range of stakeholders in the decision-making process.​Artificial Intelligence has the potential to revolutionize many industries and bring many benefits, but it also presents a number of challenges and ethical considerations. As AI continues to advance, it is important to ensure its responsible development and deployment and to consider its impact on society. By working together to address these challenges and ensure the ethical use of AI, we can help ensure that its potential is realized for the benefit of all. "
어텐션 메커니즘(Attention Mechanism) ,https://blog.naver.com/realmercy_/223064858053,20230405,"  ​1. Attention의 개념과 필요성Seq2seq 모델은 입력 시퀀스를 하나의 고정된 벡터로 압축하여 출력 시퀀스를 생성하는 구조를 가지고 있다. 하지만 이런 구조는 입력 시퀀스의 길이가 길어지면 정보 손실이 발생하고, 번역이나 요약과 같은 태스크에서는 입력 시퀀스의 모든 정보가 출력 시퀀스에 필요하지 않을 수 있다.이러한 문제점을 해결하기 위해 등장한 것이 어텐션 메커니즘이다. 어텐션 메커니즘은 입력 시퀀스의 모든 단어를 출력 시퀀스에 반영하는 것이 아니라, 입력 시퀀스에서 현재 출력 단어와 연관이 있는 부분에 집중하여 그 부분만을 사용하여 출력을 생성한다. 이를 통해 입력 시퀀스의 모든 정보를 반영하면서도 정보 손실을 최소화할 수 있다.따라서 어텐션 메커니즘은 Seq2seq 모델의 성능을 크게 향상시켰으며, 이후 기계 번역, 음성 인식, 이미지 캡셔닝 등 다양한 분야에서 활발하게 사용되고 있다.​딥러닝 분야에서 Attention은 입력 데이터 중에서 특정 부분에 더 집중해서 학습할 수 있는 방법을 제공한다. 이는 모델이 입력 데이터의 중요한 부분에 더욱 강한 가중치를 부여하여 학습을 진행할 수 있게 하므로, 전반적인 학습 효율성과 성능 향상에 도움을 준다.​ 예를 들어, 자연어 처리 분야에서 Attention 모델은 입력 문장의 단어들 중에서 문장의 의미를 파악하는 데 중요한 역할을 한다. 이 때, Attention 모델은 입력 문장의 단어들 중에서 더 중요한 단어들에 더 집중하여 그 단어들이 문장 전체의 의미에 미치는 영향력을 최대화하도록 한다.​​2. Attention 모델의 역사 Attention 모델의 기원은 인간의 시각과 인지 메커니즘에서 비롯된다. 인간은 눈으로 보는 시각 정보 중에서 더 중요한 부분에 주목하고, 그 부분에 집중하여 판단하고 학습하며, 이러한 메커니즘이 딥러닝 모델에도 적용될 수 있다는 아이디어에서 출발하였다.​ Attention 모델이 처음으로 적용된 분야는 자연어 처리 분야이다. 2014년에 제안된 Bahdanau Attention 모델은 기계번역 분야에서 대표적으로 활용되며, 이후 다양한 Attention 모델이 개발되어 오늘날 딥러닝 분야에서 광범위하게 활용되고 있다.​ Attention 모델은 초기의 딥러닝 모델이 가진 문제점인 고정된 길이의 입력 데이터를 처리하는 데 한계를 극복하기 위한 방법 중 하나로 등장하였다. Attention 모델을 적용하면 입력 데이터의 길이에 상관없이 모델이 필요한 부분에 집중하여 학습할 수 있으므로, 다양한 길이의 입력 데이터를 처리하는 데 용이하다. 또한, Attention 모델은 RNN과 같은 시계열 데이터 처리 모델과 결합하여 사용할 때, 더욱 강력한 성능을 발휘할 수 있다.​ 이렇게 Attention 모델은 인간의 인지 메커니즘에서 착안하여 발전되었으며, 현재는 다양한 분야에서 활용되고 있다. 자연어 처리 분야 뿐만 아니라, 이미지 처리 분야에서도 주목을 받고 있으며, GAN과 같은 생성 모델에서도 활용되어 진보적인 성능 향상을 이루고 있다.​​3. Attention 메커니즘3-1. Attention 메커니즘의 원리어텐션 메커니즘의 원리 어텐션 메커니즘은 기존의 인코더-디코더 모델에서 디코더가 출력을 생성할 때, 인코더의 전체 입력에 대해 동일한 가중치를 부여하는 것이 아니라, 입력 데이터의 각 부분에 다른 가중치를 부여하는 방식이다. 이를 통해 디코더가 입력 데이터의 특정 부분에 더 집중하여 출력을 생성할 수 있게 된다. 어텐션 메커니즘은 주로 자연어 처리 분야에서 사용되며, 번역, 요약, 질의응답 등 다양한 태스크에서 뛰어난 성능을 보인다.​어텐션 메커니즘은 크게 세 가지 요소로 이루어져 있다.​Query: 디코더가 생성 중인 출력 데이터를 나타낸다.Key: 인코더에서 생성된 입력 데이터의 표현을 나타낸다.Value: 인코더에서 생성된 입력 데이터 자체를 나타낸다.​어텐션 메커니즘은 Query와 Key의 내적값을 구한 후, softmax 함수를 적용하여 가중치를 구한다. 이 가중치를 Value와 곱한 후, 모든 Value의 가중합을 구하여 디코더의 출력 데이터를 생성한다.​3-2. 다양한 Attention 메커니즘의 종류다양한 어텐션 메커니즘의 종류 어텐션 메커니즘에는 다양한 종류가 있다. 그 중에서도 가장 대표적인 어텐션 메커니즘은 다음과 같다.​Scaled Dot-Product Attention: Key와 Query의 내적값을 구하고, scale factor로 나눈 후 softmax 함수를 적용하여 가중치를 계산한다. 이 가중치를 Value와 곱한 후, 모든 Value의 가중합을 구한다.Multi-Head Attention: Scaled Dot-Product Attention을 여러 번 적용하여 결과를 병합하는 방식이다. 이를 통해 모델이 입력 데이터의 다양한 부분에 집중할 수 있게 된다.Self-Attention: 입력 데이터 내에서 서로 다른 위치 사이의 관계를 학습하는 어텐션 메커니즘이다. 자연어 처리 분야에서 많이 사용된다.어텐션 메커니즘의 장점 어텐션 메커니즘은 입력 데이터의 특정 부분에 더욱 집중하여 출력을 생성하기 때문에, 다음과 같은 장점을 가지고 있다.​3-3. Attention 메커니즘의 장점어텐션 메커니즘은 입력 데이터의 특정 부분에 더욱 집중하여 출력을 생성하기 때문에, 다음과 같은 장점을 가지고 있다.​어텐션 메커니즘은 입력 데이터의 특정 부분에 더욱 집중하여 출력을 생성하기 때문에, 다양한 입력 데이터를 처리할 수 있다. 예를 들어, 기계 번역 모델에서는 입력 문장의 각 단어들 사이의 관계를 파악하여 번역을 수행한다. 이때, 어텐션 메커니즘을 이용하면 입력 문장의 단어들 간의 관계를 더욱 잘 파악할 수 있다.어텐션 메커니즘을 사용하면 모델의 해석 가능성이 높아진다. 어텐션 가중치를 시각화하면 입력 데이터의 어느 부분에 모델이 집중하는지 확인할 수 있다. 이를 통해 모델이 어떤 특성을 파악하고 있는지, 어떤 부분이 중요한 역할을 하는지 등을 파악할 수 있다.어텐션 메커니즘을 사용하면 기존의 인코더-디코더 모델보다 더욱 정확한 출력을 생성할 수 있다. 디코더가 입력 데이터의 특정 부분에 집중하여 출력을 생성하기 때문에, 출력의 정확성이 높아진다.​따라서, 어텐션 메커니즘은 다양한 자연어 처리 태스크에서 뛰어난 성능을 보이며, 모델의 학습 효율성과 해석 가능성을 높이는 장점을 가지고 있다. 이를 통해, 최근에는 어텐션 메커니즘이 자연어 처리 분야에서 기본적인 구성 요소로 자리잡고 있다.​​4. Attention 모델의 응용기계 번역 (Machine Translation) Attention 모델은 기계 번역 분야에서 큰 역할을 한다. 이전의 기계 번역 모델은 인코더-디코더 구조로 구성되어 문장을 고정된 길이의 벡터로 압축하고, 이를 디코더에서 다시 번역하는 방식으로 작동했다. 하지만 이러한 방식은 문장이 길어질수록 정확도가 저하되는 단점이 있다.Attention 모델은 이러한 단점을 보완하기 위해, 입력 문장의 단어들 간의 관계를 파악하여 출력 문장을 생성한다. 따라서 문장의 길이가 길어져도 입력 문장의 어느 부분에 집중해야 하는지 파악할 수 있어 번역 정확도가 향상된다.​이미지 캡셔닝 (Image Captioning) Attention 모델은 이미지 캡셔닝 분야에서도 사용된다. 이미지 캡셔닝은 주어진 이미지에 대해 자연어 문장을 생성하는 작업이다. 이를 위해서는 이미지의 특정 부분에 집중해야 한다.Attention 모델은 입력 이미지의 특정 부분에 집중하여 문장을 생성한다. 이를 통해 이미지의 세부 정보를 파악하고, 더욱 자연스러운 문장을 생성할 수 있다.​챗봇 (Chatbot) Attention 모델은 챗봇 분야에서도 사용된다. 챗봇은 대화형 인터페이스를 통해 사용자와 대화하는 프로그램이다. 챗봇은 입력 문장에 대해 적절한 대답을 생성해야 하므로, 입력 문장의 의도를 파악하는 것이 중요하다.Attention 모델은 입력 문장의 단어들 간의 관계를 파악하여 입력 문장의 의도를 파악하고, 적절한 대답을 생성한다.​문서 요약 (Text Summarization) Attention 모델은 문서 요약 분야에서도 사용된다. 문서 요약은 긴 문서를 간결하게 요약하는 작업이다. 이를 위해서는 문서의 핵심 내용을 파악해야 한다.​감성 분석 (Sentiment Analysis) Attention 모델은 감성 분석 분야에서도 사용된다. 감성 분석은 문장이나 문서에 대한 감정을 분석하는 작업이다. 이를 위해서는 문장이나 문서의 특정 부분에 집중해야 한다.Attention 모델은 입력 문장이나 문서의 특정 부분에 집중하여 감정을 분석한다. 이를 통해 더욱 정확한 감성 분석을 수행할 수 있다.​음성 인식 (Speech Recognition) Attention 모델은 음성 인식 분야에서도 사용된다. 음성 인식은 사람의 음성을 인식하여 텍스트로 변환하는 작업이다. 이를 위해서는 음성의 특징을 파악해야 한다.Attention 모델은 입력 음성의 특정 부분에 집중하여 음성을 인식한다. 이를 통해 음성 인식 정확도가 향상된다.​위와 같이 Attention 모델은 자연어 처리 분야 뿐만 아니라 다양한 분야에서 활용된다. 입력 데이터의 특정 부분에 집중함으로써 높은 정확도를 보이는데, 이는 현재까지도 많은 연구가 이루어지고 있으며, 더욱 발전될 것으로 예상된다.​​​​​ "
[김해보청기] '어음청력검사'란? ,https://blog.naver.com/aramis0422/222775273041,20220616,"안녕하세요!청능재활 전문센터, 오티콘보청기 김해부원점입니다.  ​청력검사는 난청의 정도, 병변 부위 등 보청기 선택에 필요한 정보를 제공하기 때문에 매우 중요한데요,정확하게 청력평가를 시행해야하는 이유도 바로 이 때문입니다.​오늘은 순음청력검사와 더불어 가장 기본적,필수적 청력검사라고 일컬어지는 어음청력검사에 대해 알려드리겠습니다!​ '어음청력검사'란?어음청력검사(speech audiometry)는 말소리(어음)를 이용하여 의사소통능력을 측정하는 검사입니다.따라서 순음을 제시하는 순음청력검사보다 일상생활 의사소통능력을 더욱 잘 반영할 수 있습니다. ​어음청력검사는 피검자의 협조가 요구되는 주관적인 평가로, 방음실에서 시행합니다.  일반적으로 검사자가 단음절, 이음절, 문장 등을 제시하고 피검자가 따라하며 진행합니다. 하지만 상황에 따라(유소아, 외국인 등) 그림을 선택하는 방법도 가능합니다.​검사의 종류에는 대표적으로 SRT, WRS, SRS가 있습니다.​ 어음인지역치(SRT) 검사어음인지역치(speech-recognition threshold)는 어음을 겨우 이해할 수 있는 가장 낮은 어음 강도(dB HL)를 의미합니다.​어음인지역치 검사는 강강격(양양격) 이음절 낱말을 사용합니다. 한국표준 이음절어표 예시​▶검사 방법① 순음역치평균보다 20~25dB HL 높은 강도 혹은 MCL에서 이음절 낱말을 제시합니다.​② 피검자가 정반응하면 10dB 하강, 틀리거나 못 듣는 경우 다시 맞힐 때까지 5dB 상승합니다. ③ SRT의 결정: 50% 바르게 응답한 가장 낮은 어음강도가 역치이며, 5개 단어 중 3개(60%), 혹은 3개 단어 중 2개(67%) 이상을 맞히는 레벨을 역치로 결정할 수도 있습니다.​※ SRT를 통해 순음청력검사의 청력역치와 비교하여 신뢰도를 확인할 수 있으며,  SRT는 WRS/SRS의 기초자료가 됩니다.SRT와 PTA가 10dB 이내일 경우,신뢰성이 좋다고 판단합니다.​아래 사진은 PTA, SRT, WRS 기록지 예시입니다 ↓ PTA와 SRT의 역치가 10dB 이내이므로 신뢰성있는 결과라고 할 수 있습니다.​ 단어인지도(WRS) 검사단어인지도(word recognition score)는 단음절을 제시하고 얼마나 정확히 인지하는지 백분율(%)로 계산한 결과입니다.  한국표준 단음절어표 예시​▶검사 방법① 어음인지역치보다 30~40dB HL 높은 강도 혹은 MCL에서 단음절 낱말을 제시합니다.​② 일반 성인의 경우 50개 낱말을 모두 사용하는 것이 바람직하지만 25개, 10개 낱말도 가능합니다. ③ 개수에 따른 정반응률을 백분율(%)로 점수화합니다.​※보청기 적합 전/후 일상에서의 의사소통능력이 얼마나 증가되었는지 객관적으로 비교할 수 있으므로 보청기 상담 시 반드시 시행하는 것이 좋습니다.​ 문장인지도(SRS) 검사문장인지도(sentence recognition score)는 문장을 제시한 후 얼마나 정확히 인지하는지 백분율(%)로 점수화한 결과입니다.​검사 방법은 WRS와 동일하며,문장 전체를 옳게 맞춘 것 혹은 문장 내 정반응한 목표 단어를 기준으로 하여 점수화할 수 있습니다. 한국표준 문장표 예시 오늘은 어음청력검사에 대해 알아보았습니다.어음청력검사가 어떤 검사인지 이해하시는 데 도움이 되셨길 바랍니다^^​어음청력검사는 실제 의사소통능력을 파악할 수 있는 검사인만큼 정확하게 평가되어야 합니다.따라서 보청기를 선택하실 때, 청각전문가/전문청능사에게 정확한 청력검사 받으시길 바랍니다:)​감사합니다♥ 오티콘보청기 김해부원점경상남도 김해시 김해대로 2349 부원역그린코아더센텀 107동 232, 233호 ​ "
테드 TED 강연 레이 커즈와일 Get ready for hybrid thinking 관계대명사 who which that 제한적 계속적 용법 주어-동사 수일치 인공지능 AI 강의 ,https://blog.naver.com/coolhanet/222919145090,20221104,"Ray Kurzweil: Get ready for hybrid thinking Filmed March 2014 at TED2014 삶 주요 저서 기계학습 머신러닝 미래학자  레이 커즈와일(Ray Kurzweil, 1948년 2월 12일 출생)은 미국 뉴욕의 유대계 이민 가정에서 성장했으며 어린 시절 공상과학소설(Science Fiction)과 프로그래밍에 탐닉했으며, 1970년 컴퓨터과학으로 MIT에서 학사학위를 받은 컴퓨터 과학자, 작가, 사상가, 발명가이며 세계적 미래학자입니다. 레이 커즈와일은 2012년 구글 창업자에 의해 구글의 기술 이사로 영입돼 인공지능과 머신러닝 사업을 이끌고 있습니다. ​과학을 실제 삶에 적용해내는 폭넓은 경력과 성향, 재능을 지닌 레이 커즈와일은 광학문자인식(Optical Character Recognition, OCR), 음성인식(Speech Recognition), 음성합성(Speech Systhesis), 전자음악 키보드(electronic musical keyboards), 시각 장애인용 음성 변환기 등을 발명했습니다.​레이 커즈와일은 베스트셀러 저서로는 ‘영적 기계의 시대(The Age of Spiritual Machines: When Computers Exceed Human Intelligence, 2000)’, ‘지능형 기계의 시대(The Age of Intelligent Machines. 1990)’, ‘건강한 삶을 위한 10% 해결책(The 10% Solution for a Healthy Life, 1994)’, ‘특이점이 온다(The singularity is near: When Humans Transcend Biology, 2005년)’, ‘정신을 창조하는 법(How to Create a Mind: The Secret of Human Thought Revealed, 2012)’ 등 인공지능(Artificial Intelligence)과 기술적 특이점(technological singularity), 건강, 인류의 미래에 관한 여러 권의 주요 저서가 있습니다. 레이 커즈와일은 인공지능의 진화가 필연적이며, 인공지능이 인류와 협력하는 일종의 도구가 될 것이라는 낙관론을 가지고 있으며, 인간 뇌의 일부로써 인간의 사고를 확장하는 데 도움을 줄 것이라고 전망했습니다. 레이 커즈와일은 1999년 기술분야의 국가적 최고영예인 전미 기술 메달(National medal of technology)을 받았고, 2002년 발명가들의 명예의 전당(National inventors hall of fame)에 헌액됐으며, 21개의 명예박사학위를 받았습니다. 레이 커즈와일은 기하급수적인 속도로 발전하는 과학기술 시대에 학문간 경계를 뛰어넘는 융합, 미래에 대한 통찰력, 기업가와 창업 정신을 함양하는데 교육의 중점을 두는 실리콘벨리 기반의 싱크탱크 교육재단인 학제간연구대학인 Singularity University를 2009년 설립합니다. 타임(Time)은 “Kurzweil의 폭넓은 경력과 과학과 실용적이며 종종 인도주의적인 응용을 결합하는 성향은 토마스 에디슨과의 비교를 고무시켰다(Kurzweil's eclectic career and propensity for combining science with practical - often humanitarian - applications have inspired comparisons with Thomas Edison.)”고 평가하고 있습니다. ​ 2022년 10월 28일 금요일 14시 54분 송파구 방이동 올림픽공원 몽촌토성 산책로 주변에서 촬영한 노랗게 단풍이 든 은행나무(gingo 은행나무과 은행나무속 낙엽교목) 가을 풍경사진 ​ 레이 커즈와일(Ray Kurzweil)의 TED 강연 : Get ready for hybrid thinking ​내용 요약 어휘 구문 정리 대뇌 신피질  ​영어는 하나의 언어입니다. 레이 커즈와일(Ray Kurzweil)의 테드 강의 ""Get ready for hybrid thinking, Filmed March 2014 at TED2014""를 여러 번 따라 듣고 말해 보면서, 강의 내용을 우리말로 요약 정리해 보고 자신의 말과 글로 표현해 보면서, 어휘와 구문도 자신의 것으로 익혀 보시기 바랍니다. Two hundred million years ago, our mammal ancestors developed a new brain feature: the neocortex. This stamp-sized piece of tissue (wrapped around a brain the size of a walnut) is the key to what humanity has become. Now, futurist Ray Kurzweil suggests, we should get ready for the next big leap in brain power, as we tap into the computing power in the cloud. 2억 년 전, 우리의 포유동물 조상들은 새로운 뇌 기능을 발전시켰습니다: 신피질. 이 우표 크기의 조직(호두만 한 크기의 뇌를 둘러싼)이 인류가 되어온 것의 열쇠입니다. 우리는 클라우드에서 컴퓨팅 파워를 활용하는 것처럼, 이제, 우리는 두뇌 파워의 다음 큰 도약을 준비해야 한다고 미래학자 레이 커즈와일(Ray Kurzweil)이 말합니다.​Ray Kurzweil: Get ready for hybrid thinking | TED Talk 출처: Ray Kurzweil: Get ready for hybrid thinking | TED Talk  2억 년 전 초기 포유류의 대뇌 신피질(neocortex)은 우표 정도 크기였고 두께도 우표 정도였는데, 호두만한 크기의 뇌를 얇게 둘러싸고 있었으며, 신피질(neocortex)은 비포유류 동물이 보이는 고정된 행동 양식 대신 새로운 방식의 사고능력, 새로운 행동을 가능하게 했습니다.비포유류 동물(포유류가 아닌 동물)들은 고정된 행동만을 하지만, 포유류인 생쥐는 포식자를 피하려다 길이 막히면 새로운 해법을 찾아내고 그것을 기억하고 새로운 행동을 보이며, 그러한 학습된 새로운 행동은 공동체 안에 급속히 퍼져 또 다른 행동을 끌어내기도 합니다. 6천 5백만 년 전 백악기의 멸종 사건(the Cretaceous extinction event)이라 불리는 갑작스럽고 급격한 환경변화가 일어나 공룡이 멸종하고 동식물종의 75%가 멸종했고 신피질을 가진 포유류는 자신의 생태적 지위를 앞서나가 포유류의 몸집이 더 커지며 두뇌는 훨씬 더 빠른 속도로 커지고 신피질은 그보다도 훨씬 더 빠르게 커지고 특이한 굴곡을 만들어 내면서 그 표면을 넓힙니다. 인간 뇌의 신피질(neocortex)은 꺼내서 펼치면 식탁의 냅킨 정도의 크기와 얇은 두께를 가지고 있지만, 이제는 수많은 주름과 굴곡을 가지고 우리 두뇌의 80%를 차지하며, 신피질(neocortex)은 우리가 고차원의 사고를 할 수 있게 하며 시를 쓰거나 앱을 만들고 TED 강연을 할 수도 있게 합니다. 레이 커즈와일은 인간의 뇌와 신피질이 약 3억 개의 모듈(mordule)로 이루어져 있으며 계층적(hierarchical)으로 사고한다고 설명했습니다.“대문자 ‘A’의 가로획(crossbar)을 알아보는 일련의 모듈이 있을 때 이 모듈들이 층층이 쌓여 다음 단계에서는 대문자 A를 인식할 수 있으며 더 높은 단계로 나아가면 A로 시작하는 단어 ‘사과(APPLE)’를 인식할 수 있는 단계로 나아갑니다.사과(Apple)를 알아보는 단계에서는 ‘A-P-P-L’ 철자만 봐도 E를 예측하여 사과(Apple)라는 단어를 자동으로 떠올릴 수 있게 됩니다. 또 다른 다섯 단계로 올라가면 이제 당신은 상당히 높은 단계에 다다르게 되고, 서로 다른 감각으로 내려가 특정한 구성을 감지하거나, 특정한 목소리의 질을 듣거나, 특정한 향을 냄새 맡을 수 있는 모듈이 있어서 “아내가 방에 들어왔어.”라고 말할 것입니다. 또 다른 10단계로 올라가면 이제 당신은 아주 높은 단계에 이르게 되며, 당신은 아마 전두엽(frontal cortex)에 이르렀을 테고 “그것은 아이러니하네. 그것은 우스워. 그녀는 예뻐.”라고 말하는 모듈이 있을 겁니다. 레이 커즈와일은 컴퓨터가 실제로 인간의 신피질과 비슷한 기술로 인간의 언어를 완전히 이해하기 시작했으며, 컴퓨터 역시 웹이나 책을 수억 쪽이나 읽고 이해한 내용을 기반으로 검색하며, 인간의 신피질과 비슷한 방식으로 언어를 계층적으로 학습해왔으며, 따라서 인간의 신피질과 컴퓨터가 결합해 진화하는 것이 가능하다고 주장했습니다. 20년 후(2014년 강연 기준)인 2030년대에는 나노봇(nanobots)이 나오며, 나노봇이 사람의 머릿속으로 들어가 모세 혈관을 통해서 인간의 신피질을 클라우드에 있는 합성 신피질에 연결함으로써 인간의 신피질을 확장하고 사고를 확장해 줄 것입니다. 만약 몇 초 만에 복잡한 검색을 하려고 만 개의 컴퓨터가 필요한 경우에 머릿속에 있는 신피질을 클라우드에 있는 컴퓨터에 1, 2초간 접속할 수 있으며, 그때 우리의 사고는 생물학적인 것과 기하급수적으로 증가하는 비생물학적인 것이 혼재된 하이브리드(hybrid) 사고가 될 것입니다.  우리가 마지막으로 신피질(neocortex)을 확장했던 때는 2백만 년 전으로 우리가 원인(原人, humanoid)이 되어 이렇게 큰 이마를 발전시켰던 시기입니다. 전두엽(frontal cortex)은 실제로 질적으로 다르지 않으며 신피질(neocortex)이 양적으로 확장된 것이며, 그러한 추가적인 사고의 양이 다른 종들과 다르게 우리가 질적인 도약을 하고 언어와 예술, 과학, 기술, 그리고 TED(테드) 컨퍼런스를 발명하게 했습니다.  다음 수십 년에 걸쳐서, 인간은 신피질(neocortex)을 다시 무제한으로 확장할 수 있을 것이고, 그러한 추가적인 양은 또다시 문화와 기술에 있어서 또 하나의 질적인 도약을 가능하게 하는 요소가 될 것입니다.   2022년 10월 29일 토요일 10시 14분 송파구 방이동 올림픽공원 몽촌토성 산책로 88호수 주변 단풍이 든 가을 풍경사진​​ 구문/어휘(vocabulary)정리  It's a story of the neocortex, [which means ""new rind.""]에서 ‘접속사+대명사’ 역할을 하는 계속적 용법의 관계대명사절 [which means ""new rind.""]가 선행사 the neocortex를 부연설명한다.  Rather than the fixed behaviors [that non-mammalian animals have], it could invent new behaviors 문장에서 ‘접속사+대명사’ 역할을 하는 제한적 용법의 관계대명사절 [that non-mammalian animals have]가 선행사 the fixed behaviors를 수식한다. But it has so many convolutions and ridges it's now 80 percent of our brain은 ‘so ~ that’ 구문으로 접속사 that이 ridges와 it's 사이에 생략된 것이다. Well, computers are actually beginning to master human language with techniques [that are similar to the neocortex] 문장에서 ‘접속사+대명사’ 역할을 하는 제한적 용법의 관계대명사절 [that are similar to the neocortex]가 선행사 techniquess를 수식한다. neocortex 〖해부학〗 (대뇌의) 신피질(新皮質)rind [raind] 껍질《과실·야채·수목 따위의》, 외피(外皮)mammal [mǽməl] 포유동물rodent [róudənt] 갉는; 설치류의 (동물)《쥐·토끼 따위》postage stamp 우표walnut [wɔ́ːlnʌ̀t] 호두나무; 그 열매viral [váiərǝl] 바이러스성(性)의, 바이러스가 원인인adopt [ǝdάpt/ǝdɔ́pt] (의견·방침·조처 따위를) 채택하다evolve [ivάlv/ivɔ́lv] 서서히 발전하다; 진화하다evolution [èvǝlúːʃən/ìːvǝ-] 진화violent [váiǝlǝnt] (자연현상·행동 따위가) 격렬한, 폭력적인cretaceous [kritéiʃǝs] 백악(白堊)(질)의(chalky); (C-) 〖지질〗 백악기(紀)의extinction 멸종, 불을 끔, 소등dinosaur [dáinǝsɔ̀ːr] 〖고대생물〗 공룡extinct [ikstíŋkt] (불이) 꺼진, 멸종한overtake [òuvǝrtéik] …을 따라잡다; 추월하다ecological [èkǝlάdʒikəl] 생태학의〔적인〕niche [nitʃ] 활동 범위, (특정) 영역; 생태적 지위anthropo- [ǽnθrǝpou] ‘사람·인류(학)’이란 뜻의 결합사morph-, morpho- [mɔ́ːrf], [mɔ́ːrfou, -fǝ] ‘형태, 조성’의 뜻의 결합사distinctive [distíŋktiv] 독특한ridge [ridʒ] 산마루, 산등성이; ⦗일반적⦘융기convolution [kὰnvǝlúːʃən/kɔ̀n-] 소용돌이, 뇌회(腦回)sublimate [sʌ́blǝmèit] 〖화학·정신분석〗 승화시키다〔하다〕, 《비유적》 고상하게 하다〔되다〕module [mάdʒuːl/mɔ́-] 규격화된 구성단위; [컴퓨터] 모듈《장치나 프로그램을 몇 개로 나눈 것 중의 하나》implement [ímplǝmǝnt] 도구, 이행〔실행〕하다(fulfill) hierarchy [háiərὰːrki] 계급 제도thesis [θíːsis] (pl. -ses [-siːz]) 논제, 주제; 논문plethora [pléθǝrǝ] 과다(過多), 과잉neuroscience 신경과학《주로 행동·학습에 관한 신경조직 연구 제(諸)분야의 총칭 spatial [spéiʃəl] 공간의; 공간적인resolution [rèzǝlúːʃən] 결의, 해결, 분해, 해상도(解像度)interneuron (중추신경계 내부의) 개재(介在) 뉴론recognize [rékǝgnàiz] 알아보다, 인지하다crossbar 가로대, (높이뛰기 등의) 바; 횡선axon [ǽksɑn/-sɔn] (신경세포의) 축색돌기(軸索突起)conceptual [kǝnséptʃuǝl] 개념상의abstract [æbstrǽkt, -́-] 추상적인 on the lookout for …에 눈을 번뜩이며, …을 찾으면서threshold [θréʃhould] 문지방, 발단, 출발점; 역치(閾値)《자극에 대해 반응이 시작되는 분계점》, sloppy [slάpi/slɔ́pi] (땅이) 질퍽한, (일 따위가) 엉성한fabric [fǽbrik] 직물, 천, (직물의) 짜임새, 구성perfume [pə́ːrfjuːm, pǝrfjúːm] 향기, 향수(scent)frontal [frʌ́ntəl] 앞(쪽)의, 정면의sophisticated [sǝfístǝkèitid] 정교한, 고도로 세련된complicated [kάmplikèitid/kɔ́m-] 복잡한receptor [riséptǝr] 〖생리〗 수용기(受容器), 수용체particular [pǝrtíkjǝlǝr] 특정한reflex [ríːfleks] 반사의; 반사; 〖생리〗 반사작용hilarious [hilέəriǝs, hai-] 명랑한, 즐거운; 들떠서 떠드는algorithm [ǽlgǝrìðəm] 알고리듬, 연산(演算)(방식)query [kwíəri] 질문(inquiry), 의문tiresome [táiǝrsǝm] 지치는; 지루한, 귀찮은frothy [frɔ́ːθi/frɔ́θi] 거품투성이의; 거품 같은; 공허한meringue [mǝrǽŋ] 《F.》 머랭《설탕과 달걀 흰자위로 만든 과자 재료》 harangue [hǝrǽŋ] 장황한 이야기, 장광설(長廣舌)encyclopedia [ensàikloupíːdiǝ] 백과 사전glutathione [glùːtǝθáioun] 〖생화학〗 글루타티온《생체 내의 산화환원(酸化還元) 기능에 중요한 작용을 함》 supplement [sʌ́plǝmǝnt] 보충, 보충하다nano- [nǽnǝ, néinǝ] pref. ‘10억분의 1’의 뜻《기호 n》, ‘미소(微小)’의 뜻exponential [èkspounénʃəl] (증가율 등이) 기하급수적인shrink [ʃriŋk] (shrank [ʃræŋk], shrunk [ʃrʌŋk]) (천 등이) 오그라들다, (수량·가치 등이) 줄다capillary [kǽpǝlèri/kǝpílǝri] 모세관(현상)의synthetic [sinθétik] 합성의, 인조의《고무 따위》extension [iksténʃən] 연장, 늘임; 확장access [ǽkses] 접근, 〖컴퓨터〗 (데이터에) 접근하다hybrid [háibrid] 잡종, 튀기, 잡종의, 혼혈의subject [sʌ́bdʒikt] 지배를 받는, 종속하는《to》, (…을) 걸리기 쉬운《to》;⦗서술적⦘…조건으로 하는accelerate [æksélǝrèit] 빨리하다, 가속하다expand [ikspǽnd] 넓히다, 확장〔확대〕하다; 팽창시키다humanoid [hjúːmǝnɔ̀id] 인간을 닮은; 원인(原人)forehead [fɔ́(ː)rid, fάr-, fɔ́ːrhèd] 이마, 앞머리primates [praiméitiːz] pl. 〖동물〗 영장류(靈長類)qualitative [kwάlǝtèitiv/kwɔ́lǝtǝ-] 질적인; 정성(定性)의quantitative [kwάntǝtèitiv/kwɔ́ntǝ-] 분량상의, 양적인leap [liːp] 도약하다, 뛰어오르다; 뜀, 도약conference [kάnfərǝns/kɔ́n-] 회의, 협의회architecture [άːrkǝtèktʃǝr] 건축술〔학〕; 구조, 구성, 체계enclosure [enklóuʒǝr] 울; 동봉한 것; 울로 둘러막은 땅revere [rivíǝr] 존경하다, 숭배하다 dizzying 현기증 나게 하는, 어지럽게 하는convincing 설득력 있는, 납득이 가게 하는《증거 따위》entrepreneur [ὰːntrǝprǝnə́ːr] 《F.》 실업가, 기업가visionary [víʒənèri/-nəri] 상상력이 있는; 공상가accomplishment [ǝkάmpliʃmǝnt/ǝkɔ́m-] 성취, 완성litany [lítəni] 장황한 이야기breakthrough [bréikθrùː] (과학·기술 등의) 획기적인 발전take ~ for granted : ~을 당연하다고 생각하다[여기다]optical character recognition 광학식 문자인식《OCR》synthesize [sínθǝsàiz] 종합하다; 합성하다singularity [sìŋgjǝlǽrǝti] 특이성; 특이점(singular point)transcend [trænsénd] 초월하다; 능가하다depict [dipíkt] (그림·글·영상으로) 그리다; 묘사〔표현〕하다blur [blǝːr] (시력·인쇄 따위의) 흐림, (눈·시력·시계 등을) 희미하게〔흐리게〕 하다facilitate [fǝsílǝtèit] (손)쉽게 하다; 촉진〔조장〕하다.eclectic [ekléktik] 취사선택하는, 절충주의의propensity [prǝpénsǝti] 성향, 성벽(inclination) humanitarian [hjuːmæ̀nǝtέəriǝn] 인도주의자; 인도주의의​​ 대표 이미지와 같은 2022년 10월 31일 월요일 13시 59분 송파구 방이동 올림픽공원 몽촌토성 산책로 주변 단풍이 든 가을 풍경사진​​ 관계대명사의 제한적 용법 계속적 용법 who which that what 주격관계대명사절의 주어-동사 수 일치  2020년 10월 4일 글(아래 링크)에서 예문을 사용하여 자세히 설명했듯이, 관계대명사(Relative Pronoun)는 접속사 역할과 대명사(주어, 보어, 목적어로 쓰임)의 역할을 동시에 수행하며, 관계대명사의 선행사가 사람일 경우 who(주격), whom(목적격), whose(소유격), 선행사가 사물일 때는 which(주격, 목적격), 관계대명사 that은 선행사가 사람, 사물 또는 사람과 사물일 때 모두 사용할 수 있습니다. 제한적 용법의 관계대명사절은 선행사를 한정[제한] 수식하는 형용사절 역할을 하며, 계속적 용법(비제한적 용법)의 관계대명사절(who, which)은 앞에 오는 선행사에 대해 부연설명을 하며, 관계대명사 that은 계속적 용법(비제한적 용법)에 사용하지 못하며 게속적 용법에서는 목적격 관계대명사라도 생략할 수 없습니다.  주격관계대명사절의 동사의 단수‧복수, 수동‧능동 여부는 주어-동사 수 일치에 따라 선행사에 일치시킵니다.​2020년 10월 4일 글 링크하니 관계대명사의 이해와 학습에 도움이 되시기 바랍니다. ​2020년 10월 4일 글 링크 영문법] 관계대명사·관계부사 제한용법·계속용법, 접속사, 강동구 송파구 방이동 잠실 중고등영어학원관계대명사(접속사+대명사, who which that what)와 관계부사(접속사+부사, where when why how) 기본 관계...blog.naver.com ​​뷰카시대 자기주도 개별맞춤교육학습행복 한선생영어©All rights reserved 한선생영어서울특별시 송파구 위례성대로20길 31   "
[영문기사] RWS and CEDAT85 launch new live subtitling and captioning solution | 보이스인식 + 자동번역  ,https://blog.naver.com/rws-korea/222506806106,20210915,"RWS and CEDAT85 launch new live subtitling and captioning solution for online meetings and events - Solution removes language barriers with real-time subtitling across 130+ languages​ ​​ ​RWS, the world’s leading provider of technology-enabled language, content management, and intellectual property services, and CEDAT85, pioneers in speech recognition technology, have announced a new partnership and the launch of a live subtitling and captioning solution for online meetings and events. The offering allows multilingual participants to actively collaborate and engage with each other in real-time – in their own language – without linguistic barriers.​Combining Language Weaver and CEDAT85’s Voice Recognition and Speech-to-Text technologies the solution works by transcribing live speech and audio into text and captions, and then applies the latest neural network machine translation models to translate text across 130+ languages. The captions appear on screen in real-time in the attendee’s own language.​“Live events typically take place in one language – somewhat limiting the size of the audience. With the exponential rise in online meetings and events companies now have an opportunity to widen their reach,” explains Thomas Labarthe, President of RWS’s Language Services and Technology division. “We’re excited by our new solution that removes all the language challenges associated with online conversations and collaboration.”​The solution can be customized for any environment and integrated within any platform where live discussions and events take place. CEDAT85 is also including this new functionality in its Cabolo® device, which transcribes meetings and conversations into text.​“At CEDAT85 we like challenges and we know we have the right technology to support our customers with whatever ambitions they might have. The amount of human and technology knowledge put together by RWS and CEDAT85 in this partnership is impressive and can only bring great benefits and strategic achievements to all our customers worldwide,” said Enrico Giannotti, CEO of CEDAT85. “We look forward to delivering this innovative offering and setting the bar at the highest level when it comes to transcription and translation for any type of meeting, across any industry.”​Language Weaver is RWS’s secure, adaptable machine translation platform that processes high volumes of content. It instantly, and securely, translates content across 2,700 language combinations. CEDAT85’s Speech Recognition and Speech-to-Text technologies have been developed over the past 35 years and are used by some of the globe’s largest brands to transcribe conversations instantly for analysis and collaboration. The combination of these technologies provide one of the most powerful solutions to increase the inclusion and participation of event attendees that speak multiple languages.​To learn more about the partnership and solution, please click here. ​----------------------------------------------------------------------------------------------------------------------#RWS #CEDAT85 #기계번역 #기계번역기술 #음석인식기술 #번역기술 #번역컨설팅 #번역소프트웨어 #번역용소프트웨어 #번역테크놀로지 #CAT #기업용기계번역 #기업용번역컨설팅 #ai번역 #OCR #음성인식 #기업용번역솔루션 #번역솔루션 #ai번역 #인공지능번역 #MT #NMT #번역전략 #현지화전략 #로컬라이제이션 #대용량번역 #비대면미팅번역 #화상회의 #화상회의용 #화상회의번역 #외국어미팅 #커뮤니케이션전략 #랭귀지위버 #자동번역 #기업용자동번역 #번역IT ​​ "
싱가포르 EF 어학연수 프로그램 비용 안내 ,https://blog.naver.com/raffles7/222988100906,20230118,"많은 EF 캠퍼스 중에서도 싱가포르는 단기 어학연수지로 인기가 높습니다.  아시아에서 영어가 공용어인 나라이고, 치안이 안전하여 미성년자나 여성분들 홀로 연수 계획하시는 분들에게 추천드려요.  매일매일 운행하는 직항 항공편이 많고 아주 극 성수기를 제외하고서는 왕복항공권을 백만 원 안되게 구입할 수 있고 운행하는 저가 항공사들도 많아서 운이 좋으면 훨씬 저렴하게 구하실 수도 있죠.  부산 출발 싱가포르 직항 제주항공도 있어서  남쪽에 계시는 분들도 편하게 여행하실 수 있고요.  그리고 싱가포르는 입국을 위해 준비해야 하는 절차가 매우 간단합니다.  미국이나 캐나다처럼 미리 전자여행 허가증 같은 거 안 받아 놓으셔도 됩니다.  코로나 백신 2차까지 완료하신 분들은 코로나 이전과 마찬가지로 자유롭게 싱가포르 여행하실 수 있습니다.  건강상의 이유로 코로나 백신 못 맞으신 분들도 출국 전 코로나 검사만 받으실 뿐 싱가포르 도착하여 격리 없이 자유로이 싱가포르 여행이 가능하답니다. ​오늘은 한 달 미만 초단기 등록도 가능한 싱가포르 EF 어학연수 프로그램에 대해 안내드리려고 합니다.EF의 어학연수는 2주 이상 등록이 가능합니다.  그래서  ​휴가와 어학연수 두 마리 토끼를 다 잡으시려는 직장인분들께도 추천드려요.매주 월요일 개강입니다.주당 수업 시간에 따라 일반과정과 집중과정을 고르 실 수 있습니다.​ 싱가포르 EF 집중과정 시간표​​ 싱가포르 EF 일반 과정 시간표해외연수를 결정할 때 국내 어학원이 수준이 딸리거나 부족해서 해외연수를 떠나시는 것은 아닐 거예요. 영어를 쓰면서 낯선 곳에서 현지인의 삶을 체험해 보고 영어 공부에 대한 동기부여를 얻고 싶으셔서 이실 텐데요.성공적인 어학연수는 어학당 수업 방과 후를 어떻게 지내느냐가 매우 중요합니다. 한국인 학생들과 몰려다니면서 보낸다면 영어는 늘지 않을 거예요. ​EF는 한국인 학생과 중국인 학생 비율이 매우 적은 것이 특징입니다. 싱가포르 EF 학생들싱가포르 EF 학생들싱가포르 EF 학생들싱가포르 EF 학생들싱가포르 EF 학생들싱가포르 EF 학생들싱가포르 EF 오리엔테이션데이​EF 프로그램은 어학원 수업 외에도 다양한 액티비티가 있는 것도 특징입니다.  아래 EF 싱가포르 액티비티  칼렌더는 2023년 1월 실제 EF 싱가포르 액티비티  칼렌더 입니다. 루프탑 파티, 비치 발리볼과 같이 공짜인 프로그램도 있고, 플라우 우빈 여행, 보라카이 여행이나 내셔널갤러리같이 유료 프로그램도 있습니다. 방과 후 시간도 알차게 보내셔서 외국인 친구도 많이 만들고 영어실력도 일취월장하시길 바랍니다!여기서 EF 어학연수를 아주 즐겁게 보낼 수 있는 꿀팁!EF는 매주 월요일이 개강이라고 말씀드렸죠?  아래 캘린더 보시면 월요일에 회색으로 된 ORIENTATION / WALK TOUR가 있습니다.   이 WALK TOUR는 무조건 참가하세요!  이미 다녀오셨던 분들 얘기를 들으면 이 월요일이 매주 들어오는 새 친구도 만나게 되고 여기서 뜻맞는 사람들끼리 그룹이 만들어진다고 하네요.  적극적인 참여로 방과 후 시간도 알차게 보내셔서 외국인 친구도 많이 만들고 영어실력도 일취월장하시길 바랍니다!​​ ​​EF의 어학연수 프로그램은 다른 어학원보다 좀 더 듣기와 말하기 능력 항상에 포커스가 맞추어져 있습니다. EF에서는 어떤 수업을 받게 되는지 궁금하신 분들은 아래의 동영상을 참고해 주세요.​ 위 영상에서 보신 EF의 수업 체계는 다음과 같이 5 가지의 수업으로 구성되어 있습니다.1. General Class : 저희도 익히 경험한 강남역 영어학원 같은 소규모 렉처 스타일 수업2. Conversation class : 다양한 디스커션 수업3. iPAD Class : 아이패드를 이용한 2-3명의 pair conversation 수업4. i Lab computer class: 유창한 영어를 위해서는 파트너와의 공부도 필요하나 학습자 스스로의 혼자 해야 할 공부들이 있죠? 바로 올바른 문법을 익히고 올바른 발음으로 자신의 생각을 정리하고 전달해야 합니다. 그를 위한 video lesson(온라인 문법 공부)과 speech recognition(발음 수정)을 컴퓨터를 통해 공부하고 itracker라는 툴을 통해 본인의 프로그레스를 확인할 수 있습니다.5. SPIN Class: Special Interest Class로 대학 교양처럼 마케팅이나 저널리즘 같은 흥미 있는 과목 수업을 영어로 참여하는 수업입니다.​​그리고 EF 싱가포르의 어학연수 기간 동안 숙박은 어떻게 해야 하는지 궁금해하실 텐데요?  ​​1. 싱가포르언 호스트 패밀리에서 홈스테이 하시는 방법과 ​ ​2. 도비곳 MRT 도보 5분 거리에 위치한 ​EF 연계 숙소인 YMCA 레지던스에서 이용하시는 방법이 있습니다. 클라키에 위치한 EF와 전철로 1정거장 떨어져 있고 서울 강남에 해당하는 오차드로드에 위치하고 있어서 교통도 편리하고 주변에 편의시설이 넘칩니다. ​ 방 타입은 혼자 쓰는 싱글룸과 룸메이트와 같이 쓰는 2인실 트윈룸이 있습니다. ​공용 주방에 전자레인지, 인덕션, 토스터, 냉장고 등이 있어서 간단한 요리는 가능합니다.​ 운동시설로는 수영장과 헬스장이 있습니다. ​​​마지막으로 싱가포르 EF로 어학연수  비용에 대해 안내드리겠습니다.영어를 배울 수 있는 나라들이 다 선진국이다 보니 어학연수 비용도 비용이지만 주거비용이 만만치 않습니다.  2023년도 싱가포르로 2주 일반과정(연수비용)으로 홈스테이 2인실에서 연수할 경우 (주거비용) 총 USD1490(오늘환율 1235원 적용시 약 184만원)입니다.연수과정을 수업시수 많은 집중과정을 선택하시거나, 연수 기간을 늘리시거나, 홈스테이를 1인실 사용하거나, 홈스테이가 아닌 레지던스 사용시 비용은 늘 수 있으니 궁금하신 분들은 개별적으로 연락주세요.​​싱가포르 EF의 어학연수 프로그램에 대해 더 궁금하신 사항은 저희 링크에이드로 연락 주시면 친절히 답변드리겠습니다. ​  싱가포르 유학원 링크에이드는 싱가포르 EF 어학연수 입학 수속을 무료로 도와드리고 있습니다.입학 허가를 받은 이후 싱가포르로 출국 및 현지 정착까지 세심히 도와드리고 있습니다.관심 있으신 분들은 카카오톡 아이디 linkaid로 카톡 상담 가능합니다. 전화가 편하신 분들은 010-2473-0049로 전화 주시면 친절히 안내해 드리겠습니다.​ ​ "
만들면서 배우는 프로그레시브 웹 앱 - 사용자 경험을 극대화하는 차세대 웹 앱 기술 ,https://blog.naver.com/zpdl92/221540527946,20190518,"  원서: Building Progressive Web Apps: Bringing the Power of Native to the Browser 1st​https://www.amazon.com/dp/B075HP52WY/ref=sr_1_2?crid=13OPIOLATKBW9&keywords=progressive+web&qid=1558146865&s=digital-text&sprefix=prograsive+web%2Cdigital-text%2C303&sr=1-2 Building Progressive Web Apps: Bringing the Power of Native to the BrowserMove over native apps. New progressive web apps have capabilities that will soon make you obsolete. With this hands-on guide, web developers and business execs will learn how—and why—to develop web apps that take advantage of features that have so far been exclusive to native apps. Fe...www.amazon.com ​책소개네이티브 앱의 장점과 웹의 낮은 진입 장벽이라는 두 가지 장점을 모두 지닌 차세대 웹 기술, 프로그레시브 웹 앱(Progressive Web App, PWA)의 구현 방법을 다룬 책이다. 프로그레시브 웹 앱 기술을 활용해 가상으로 만든 호텔의 웹사이트를 개선하고 다양한 기능을 추가하는 과정이 상세히 수록돼 있다. 기존의 웹 개발 기술을 활용해 최신 프로그레시브 웹 앱의 구현 방법을 배우고 싶거나, 프로그레시브 웹 앱에 관심이 많다면 이 책의 내용을 차근차근 따라 해보자. 이 책이 훌륭한 길잡이가 되어 줄 것이다.  목차chapter 1 프로그레시브 웹 앱 소개 __1.1 웹의 역습 __1.2 오늘날의 모바일 환경 __1.3 프로그레시브 웹 앱의 장점 __1.4 탭, 웹 그리고 서비스 워커 ​chapter 2 당신의 첫 번째 서비스 워커 __2.1 샘플 프로젝트 구성하기 __2.2 고담 임페리얼 호텔에 오신 것을 환영합니다 __2.3 코드 알아가기 __2.4 현재의 오프라인 사용자 경험 __2.5 첫 번째 서비스 워커 만들기 __2.6 점진적 향상이란? __2.7 HTTPS와 서비스 워커 __2.8 웹에서 콘텐츠 가져오기 __2.9 오프라인 요청 감지하기 __2.10 HTML Response 생성하기 __2.11 서비스 워커의 범위(Scope) 이해하기 __2.12 정리 ​chapter 3 캐시 스토리지 API __3.1 CacheStorage란 무엇인가 __3.2 언제 캐시할지 결정하기 __3.3 CacheStorage에 요청 저장하기 __3.4 CacheStorage로부터 요청 받아오기 __3.5 샘플 앱에서 캐싱하기 __3.6 각각의 요청에 올바른 응답 매칭하기 __3.7 HTTP 캐싱과 HTTP 헤더 __3.8 정리 ​chapter 4 서비스 워커 생명주기와 캐시 관리 __4.1 서비스 워커의 생명주기 __4.2 서비스 워커의 수명과 waitUntil의 중요성 __4.3 서비스 워커 업데이트하기 __4.4 캐시를 관리해야 하는 이유 __4.5 캐시 관리 및 이전 캐시 제거 __4.6 캐싱된 response를 다시 사용하기 __4.7 올바른 헤더 캐싱을 제공하기 위한 서버 설정 __4.8 개발자 도구 __4.9 정리 ​chapter 5 ‘오프라인 우선’을 받아들이기 __5.1 오프라인 우선이란 무엇입니까? __5.2 일반적인 캐싱 패턴 __5.3 믹스 앤 매치, 새 패턴 생성하기 __5.4 캐싱 전략 세우기 __5.5 캐싱 전략 구현하기 __5.6 어플리케이션 쉘 아키텍처 __5.7 앱 셸 구현하기 __5.8 목표 달성 __5.9 정리 ​chapter 6 IndexedDB로 로컬에 데이터 저장하기 __6.1 IndexedDB란? __6.2 IndexedDB 사용하기 __6.3 SQL Ninja를 위한 IndexedDB __6.4 IndexedDB 실제로 적용하기 __6.5 프로미스를 활용한 데이터베이스 __6.6 IndexedDB 관리 __6.7 서비스 워커에서 IndexedDB 사용하기 __6.8 IndexedDB 에코시스템 __6.9 정리 ​chapter 7 백그라운드 동기화를 통한 오프라인 기능 보장 __7.1 백그라운드 동기화는 어떻게 작동하는가 __7.2 The SyncManager __7.3 동기화 이벤트로 데이터 넘기기 __7.4 앱에 백그라운드 동기화 추가하기 __7.5 정리 ​chapter 8 메시지를 통한 서비스 워커와 페이지 간의 커뮤니케이션 __8.1 윈도우에서 서비스 워커로 메시지 보내기 __8.2 서비스 워커에서 열려있는 모든 윈도우로 메시지 보내기 __8.3 서비스 워커에서 특정 윈도우로 메시지 보내기 __8.4 MessageChannel로 커뮤니케이션 채널을 열어두기 __8.5 윈도우 간 통신하기 __8.6 동기화 이벤트에서 페이지로 메시지 보내기 __8.7 정리 ​chapter 9 인스톨 가능한 웹 앱으로 홈 화면 차지하기 __9.1 인스톨 가능한 웹 앱 __9.2 브라우저가 ‘앱 설치 배너’를 언제 표시할지 결정하는 방법 __9.3 웹 앱 매니페스트의 구조 __9.4 다양한 플랫폼 호환성 고려하기 __9.5 정리 ​chapter 10 사용자에게 푸시 알림 보내기 __10.1 푸시 알림의 생애 __10.2 알림 생성하기 __10.3 푸시 이벤트 구독하기 __10.4 서버에서 푸시 이벤트 전송하기 __10.5 푸시 이벤트 수신하고 알림 표시하기 __10.6 정리 ​chapter 11 프로그레시브 웹 앱 UX __11.1 우아함과 신뢰 __11.2 서비스 워커에서 상태정보 알려주기 __11.3 프로그레시브 UI KITT로 커뮤니케이션하기 __11.4 프로그레시브 웹 앱의 일반적인 메시지 __11.5 올바른 단어 선택 __11.6 사용자에게 명확히 설명하기 __11.7 프로그레시브 웹 앱 디자인 __11.8 설치 프롬프트 책임지기 __11.9 RAIL로 성능 측정 및 성능 목표 설정하기 __11.10 정리 ​chapter 12 PWA의 미래 __12.1 Payment Request API로 결제 수락하기 __12.2 Credential Management API로 사용자 관리하기 __12.3 WebGL을 사용한 Real-Time Graphics __12.4 음성 인식 지원을 위한 첨단 API __12.5 WebVR을 통한 브라우저 내 가상 현실 __12.6 앱에서 앱으로 쉽게 공유하기 __12.7 매끄러운 미디어 재생 UI __12.8 다가올 위대한 시대 ​부록A 서비스 워커 : ES2015 적용하기 부록B 전면 광고를 싫어하는 이유 부록C CORS vs NO-CORS접기  저자 및 역자소개탈 아터 (Tal Ater) (지은이) 저자파일 최고의 작품 투표 신간알림 신청20년 이상 경력의 개발자, 컨설턴트, 사업가입니다. 클라이언트, 서버, 제품을 개발했고, R&D 부서와 제품 부서를 관리했습니다. 저자는 오픈 소스 커뮤니티에 열정을 가지고 활발하게 참여하고 있습니다. 저자가 구현한 유명한 서비스 워커(Service Worker)와 스피치 레코그니션(Speech Recognition) 라이브러리 등 커뮤니티에 공유된 여러 가지 오픈 소스는 수백만 명의 사람들이 매일 사용하고 있습니다. 저자는 웹 개발, 제품 개발, 보안, 오픈 소스에 대하여 광범위하게 글을 쓰고 강연을 해왔습니다. 저자의 업적과 ... 더보기최근작 : <만들면서 배우는 프로그레시브 웹 앱> … 총 2종 (모두보기)한민주 (옮긴이) 저자파일 최고의 작품 투표 신간알림 신청삼성전자의 개발자로 출발해, UNHCR에서 디지털 업무를 맡아 진행했습니다. 프리랜서 IT 프로젝트 매니저로 활동한 바 있으며, 현재는 번역을 하며 공부하고 있습니다. 개발자가 함께 성장하고 협업하는 문화를 좋아합니다.최근작 :양찬석 (옮긴이) 저자파일 최고의 작품 투표 신간알림 신청안드로이드 초창기부터 모바일 앱 개발 관련 일을 해왔습니다. 현재는 구글 코리아에서 한국의 앱 개발자들이 더 좋은 앱을 만들어 더 많은 이익을 얻을 수 있도록 돕는 역할을 하고 있습니다. 개발자들 사이에서 정보와 경험을 공유하는 문화를 존중하고 다양한 개발 커뮤니티 활동에 참여하는 것을 즐깁니다.최근작 :  출판사 소개한빛미디어 도서 모두보기 신간알림 신청   최근작 : <파이썬 라이브러리를 활용한 데이터 분석>,<엑셀 바이블>,<게임으로 익히는 코딩 알고리즘>등 총 577종대표분야 : 프로그래밍 언어 1위 (브랜드 지수 268,751점), 오피스(엑셀/파워포인트) 2위 (브랜드 지수 145,021점), 그래픽/멀티미디어 3위 (브랜드 지수 162,792점)   출판사 제공 책소개차세대 웹 기술, 프로그레시브 웹 앱 ​프로그레시브 웹 앱(Progreesive Web App, PWA)은 네이티브 앱과 웹의 장점을 모두 지닌 새로운 종류의 웹 앱이다. 쉽게 말해 앱처럼 작동하는 웹이라고 볼 수 있다. 웹을 앱처럼 사용할 수 있기 때문에 사용자 입장에서는 굳이 앱을 설치하지 않아도 되며, 다양한 플랫폼에 간단히 이식이 가능하고, 오프라인에서도 사용할 수 있는 등 다양한 장점으로 차세대 웹 기술로도 손꼽힌다.<만들면서 배우는 프로그레시브 웹 앱>에서는 프로그레시브 웹 앱 기술을 활용하여 가상으로 만든 ‘고담 임페리얼 호텔’의 웹사이트를 직접 개선해본다. 이 과정을 직접 따라하면서 프로그레시브 웹 앱의 구현 방법을 익힐 수 있다.<만들면서 배우는 프로그레시브 웹 앱>은 다른 누구보다도 개발자를 위한 책이다. 독자에게 HTML과 자바스크립트 웹 개발에 대한 기본적인 이해가 있는 가정 하에 쓰여졌으며, 기존의 웹 개발 기술을 활용해 프로그레시브 웹 앱을 구현하고 싶은 독자에게 특히 유용한 지침서가 되어 줄 것이다. 접기 "
생활 속의 AI 기술에 대해 알아보자! ,https://blog.naver.com/codinggenius_/222929971489,20221116,"​안녕하세요 저희는 AI Genius 대학생 서포터즈 1조입니다! 1조가 오늘 소개할 주제는 '생활 속 AI기술에는 어떤 것이 있는지'입니다. 지금부터 소개하겠습니다! 음성 인식 기술여러분은 음성 인식 기술이라고 하면 어떤 것이 제일 먼저 떠오르나요? 아마 스마트 스피커, 인공지능 스피커를 가장 쉽게 떠올릴 수 있을 텐데요! 우리는 음성 인식 스피커를 통해 음악 감상, 정보 검색 등의 기능을 활용하고 있습니다.우리의 일상에서 가장 많이 사용되고 있는 인공지능 기기 중 하나입니다. ​     Previous imageNext image*이미지 출처 – Freepik.com  ​음성 인식은 '자동 음성 인식(Automatic Speech Recognition: ASR)', '컴퓨터 음성 인식' 또는 '음성-텍스트 변환(Speech-to-Text)'이라고도 부르며, 프로그램이 사람의 음성을 텍스트 형식으로 처리해주는 기술입니다. ​여러분, 혹시 대화를 하고 있는데 어디선가 부르지 않은 사람의 목소리가 들렸던 경험 있으신가요? 저희 서포터즈들은 수업을 듣던 도중 교수님의 말씀에 반응한 시리가 대답을 하는 바람에 재미있는 상황이 연출되었던 경험이 있습니다. 또 우리는 헤이 카카오 “오늘 날씨 어때?” 오케이 구글 “볶음밥 레시피 알려줘” 등 음성 인식 스피커를 사용하여 텍스트가 아닌 음성만으로 내가 원하는 정보를 찾을 수 있게 되었습니다. ​저희 서포터즈들은 아침에 일어나 날씨 정보를 묻고, 듣고 싶은 기상송을 틀고, 또 심지어는 나가는 시간에 맞추어 알람을 맞추는모든 과정을 휴대폰 기기를 들고 직접 설정하지 않고 AI 음성 인식 기술을 활용하고 있는데요! 여러분들도 AI 음성 인식 기술을 십분 활용한다면 손 하나 까딱하지 않고 ‘음성’만으로도 원하는 정보를 얻고 또 더욱 편리한 생활을 경험해 볼 수 있지 않을까요?  스마트홈 기술​여러분 ‘스마트홈’에 대해서 들어보셨나요? 스마트홈이란 지능형 정보가전 기기를 네트워크로 연결하여 사람과의 상호작용이 가능한 서비스 환경을 구축하고, 생활 서비스를 제공하는 기술을 말합니다. ​코로나가 장기화되면서, 우리 생활의 많은 부분이 바뀌었는데요, 특히 재택근무, 원격수업의 수요가 커지면서 기업 간의 스마트홈 주도권 경쟁이 더 뜨거워지고 있는 추세입니다.  스마트홈 기술이 적용된 사례로는 어떤 것이 있는지 지금부터 소개해드리겠습니다!  에너지관리 스마트 주택‘미세먼지 없는 미래를 위한 에너지 제로 스마트 주택’현재 스마트홈의 자동 에너지 관리 시스템은 전력, 냉난방 시스템, 지능형 영상보안, 주차관리 등 다양한 솔루션으로 더 지능화되고 자동화되고 있어요. 이 과정에서 집을 유지하는데 드는 인력과 비용은 획기적으로 결집되고, 안전하며 편리해지고 있어요. 원리는 주요 설비에 IoT센서를 장착하는 것인데, 사용하지 않는 전자기기의 전력은 알아서 차단하고 생활 패턴에 맞춰 집안의 밝기를 조절함으로써 전기세가 나가지 않도록 막아준답니다.  사람이 생활하는 공간에 필요한 난방, 냉방, 온수, 조명, 환기를 화석 에너지가 아닌 신재생에너지를 이용해 얻는 제로 에너지빌딩 건축이 확산되고 있는데요, 이에 따라 1년간 소비한 에너지와 생산한 에너지를 동일하게 해 에너지 소비량을 제로에 가깝게 만드는 새로운 스마트 주택이 생겨나고 있습니다. 출처: LX 땅과 사람들 2019년 9월호  원격제어 LG IoT 스마트홈대표적인 예시로 LG IoT 스마트홈이 있습니다! 출처: 한겨레​1.음성 명령으로 동시실행이 가능합니다.외출 전 ‘나 나갈게’한마디면 조명과 가전제품이 꺼지고 가스밸브가 잠기고 보안모드가 실행됩니다.​2. 외출 중에도 집안 확인 및 가전 실행이 가능합니다.휴대폰 앱으로 언제 어디서나 집안의 상황을 확인할 수 있습니다. 귀가 전 에어컨을 미리 켜놓거나 혼자 있는 반려동물을 위해 TV를 틀어줄 수 있습니다.​3. 내 생활패턴에 맞춰 자동실행이 가능합니다.안방 조명을 끄면 무드등과 잔잔한 음악이 켜지고, 거실 조명을 켜면 공기청정기가 켜지도록 설정해 보세요.스마트 홈 기기는 물론 일반 가전제품까지 내 생활 패턴에 맞게 켜고 꺼지도록 설정할 수 있습니다.​4. 말 한마디면 다 해주는 스마트 홈 스피커”헤이 클로바, 비긴 어게인에 나온 노래 틀어줘””헤이 클로바, 오늘 학교에서 시험을 봤어가 영어로 뭐야?””헤이 클로바, 에어컨 온도 26도로 올려줘”이처럼 다양한 생활 정보와 콘텐츠, 간단한 명령을 말 한마디로 해결할 수 있습니다.       AI 교통시스템 교통 신호는 불필요한 제어로 인해 차들이 자주 정지함에 따라 환경오염이 발생하고, 급작스러운 발차나 정지로 인한 안전사고로 인한 대기 가스 배출, 그리고 무엇보다도 교통 혼잡이라는 큰 문제점을 안고 있었는데요!! 최근 들어 현 교통 시스템에 AI가 도입되면서, 획기적인 변화를 맞고 있습니다!       AI 영상분석 기술을 기반으로 교차로 내 보행자와 차량 등 모든 교통 객체의 정보를 수집하고 분석하는 스마트 교차로 구축을 완료해 본격 운영을 하고 있습니다. AI는 방향별, 차종별, 교통량 등 대기행렬길이를 분석하면서 시간대, 요일, 계절별 등 최적의 신호 주기를 제공해줍니다. 파주에 살면서 직접 거리를 지나가 봤는데 교통 구축이 잘 되어있습니다. 특히 이번에 구축된 스마트 교차로 AI 분석시스템을 통해 전국 최초로, 교차로의 서비스 수준을 방향별로 실시간으로 알려주니 교차로에서 안전하게 운전을 할 수 있었습니다. 파주형 스마트 교통체계 구축을 통해 시민 불편을 최소화하고 시민이 안전한 교통 환경을 다양하게 제공했으면 좋겠습니다.   AI 로봇여러분은 식당, 공항, 백화점 등에서 여러 로봇을 보신 적이 있으신가요?저희 서포터즈들은 최근에 많은 장소에서 AI 로봇을 볼 수 있었습니다! 바로 최근에 백화점과 식당에 갔을 때 보았답니다! 고기집 식당에서는 로봇이 스스로 음식을 테이블 앞까지 서빙하는 모습을 볼 수 있었고넓은 백화점에서 목적지를 직접 LG CLOi라는 로봇이 안내해주는 모습을 볼 수 있었습니다. 그래서 궁금증이 생겨 CLOi로봇에 대해 알아보려 합니다.LG 전자는 CLOi라는 자체브랜드로 가이드로봇, 서브봇, 바리스타봇, 샬균봇 등 다양한 로봇을 선보이고 있습니다. ​   로봇에는 LG U+의 통신기술, LG전자의 모터와 인버터1), LG이노텍의 비전센서2)​,LG에너지솔루션의 배터리, LG CNS의 AI와 Cloud 등이 활용되었다고 합니다!​1) 인버터: 전기를 변환하는 데 사용하는 기구로, 환경이 다른 장비를 사용 가능하도록 전력을 변환하거나 특정 기기를 가동할 때 사용함2) 비전센서: 카메라 센서의 일종으로, 사람의 눈처럼 사물의 형태와 크기, 글자, 패턴 등을 판별할 수 있는 센서 ​​ 현재, AI로봇을 개발하기 위해 많은 국가와 기업이 도전하고 있다고 하니추후에는 보편적인 식당, 백화점, 공항뿐만 아니라 많은 장소에서 인간과 AI로봇이 서로 공존하는 모습으로 더 멋진 과학기술을 가진 세상이 오기를 희망해봅니다!  생활 속에 사용되고 있는 AI기술의 일부인 음성 인식 기술, 스마트홈, 스마트교통, AI로봇에 대해 알아봤습니다.굉장히 다양하게 쓰이며 생활 속에서 편리함을 가져다주는 AI기술, 또 다른 것으로 어떤게 있을까요?저희 블로그에는 AI기술 외에도 AI관련 글을 포스팅 중이니 많은 관심 부탁드립니다!지금까지 1조였습니다. 감사합니다!​​ "
Raymond Kurzweil ,https://blog.naver.com/kwangchoi/221537620034,20190514,"Open main menu	SearchRay KurzweilRead in another language​Watch this pageEditRaymond Kurzweil (/ˈkɜːrzwaɪl/ KURZ-wyle; born February 12, 1948) is an American inventor and futurist. He is involved in fields such as optical character recognition (OCR), text-to-speech synthesis, speech recognition technology, and electronic keyboard instruments. He has written books on health, artificial intelligence (AI), transhumanism, the technological singularity, and futurism. Kurzweil is a public advocate for the futurist and transhumanist movements, and gives public talks to share his optimistic outlook on life extension technologies and the future of nanotechnology, robotics, and biotechnology.​Ray Kurzweil​Kurzweil on or prior to July 5, 2005BornRaymond KurzweilFebruary 12, 1948 (age 71)Queens, New York City, New York, U.S.NationalityAmericanAlma materMassachusetts Institute of Technology (B.S.)OccupationAuthor, entrepreneur, futurist and inventorEmployerGoogleSpouse(s)Sonya Rosenwald Kurzweil (1975–present)[1]AwardsGrace Murray Hopper Award (1978)National Medal of Technology (1999)WebsiteOfficial website Kurzweil received the 1999 National Medal of Technology and Innovation, the United States' highest honor in technology, from President Clinton in a White House ceremony.[2] He was the recipient of the $500,000 Lemelson-MIT Prize for 2001[3]. And in 2002 he was inducted into the National Inventors Hall of Fame, established by the U.S. Patent Office. He has received 21 honorary doctorates, and honors from three U.S. presidents. The Public Broadcasting Service (PBS) included Kurzweil as one of 16 ""revolutionaries who made America""[4] along with other inventors of the past two centuries. Inc. magazine ranked him #8 among the ""most fascinating"" entrepreneurs in the United States and called him ""Edison's rightful heir"".[5]​Kurzweil has written seven books, five of which have been national bestsellers.[6] The Age of Spiritual Machines has been translated into 9 languages and was the #1 best-selling book on Amazon in science. Kurzweil's book The Singularity Is Near was a New York Times bestseller, and has been the #1 book on Amazon in both science and philosophy. Kurzweil speaks widely to audiences both public and private and regularly delivers keynote speeches at industry conferences like DEMO, SXSW, and TED. He maintains the news website KurzweilAI.net, which has over three million readers annually.[7]​Kurzweil has been employed by Google since 2012, where he is a ""director of engineering"".​Life, inventions, and business career	Books	EditKurzweil's first book, The Age of Intelligent Machines, was published in 1990. The nonfiction work discusses the history of computer artificial intelligence (AI) and forecasts future developments. Other experts in the field of AI contribute heavily to the work in the form of essays. The Association of American Publishers' awarded it the status of Most Outstanding Computer Science Book of 1990.[38]​In 1993, Kurzweil published a book on nutrition called The 10% Solution for a Healthy Life. The book's main idea is that high levels of fat intake are the cause of many health disorders common in the U.S., and thus that cutting fat consumption down to 10% of the total calories consumed would be optimal for most people.​In 1999, Kurzweil published The Age of Spiritual Machines, which further elucidates his theories regarding the future of technology, which themselves stem from his analysis of long-term trends in biological and technological evolution. Much emphasis is on the likely course of AI development, along with the future of computer architecture.​Kurzweil's next book, published in 2004, returned to human health and nutrition. Fantastic Voyage: Live Long Enough to Live Forever was co-authored by Terry Grossman, a medical doctor and specialist in alternative medicine.​The Singularity Is Near, published in 2005, was made into a movie starring Pauley Perrette from NCIS. In February 2007, Ptolemaic Productions acquired the rights to The Singularity is Near, The Age of Spiritual Machines and Fantastic Voyage including the rights to film Kurzweil's life and ideas for the documentary film Transcendent Man,[13] which was directed by Barry Ptolemy.​Transcend: Nine Steps to Living Well Forever,[39] a follow-up to Fantastic Voyage, was released on April 28, 2009.​Kurzweil's book, How to Create a Mind: The Secret of Human Thought Revealed, was released on Nov. 13, 2012.[40] In it Kurzweil describes his Pattern Recognition Theory of Mind, the theory that the neocortex is a hierarchical system of pattern recognizers, and argues that emulating this architecture in machines could lead to an artificial superintelligence.[41]​Kurzweil's latest book and first fiction novel, Danielle: Chronicles of a Superheroine, follows a young girl who uses her intelligence and the help of her friends to tackle real-world problems. The book comes with companion materials, A Chronicle of Ideas, and How You Can Be a Danielle that provide real-world context. The book is set to release in January 2019.[42]​Movies	Views	Predictions	Awards and honors	See also	References	External links	Last edited 6 days ago by an anonymous user​Content is available under CC BY-SA 3.0 unless otherwise noted.Terms of UsePrivacyDesktop Kurzweil Accelerating Intelligenceblog the Polymath | Unlocking the power of human versatility in print | feature w. Ray Kurzweil May 10, 2019 A + E  | Music by Saul Paul feat. Ray Kurzweil series: the inspired arts May 1, 2019 A + E  | Music by Native Indian feat. Ray Kurzweil series: the inspired arts May 1, 2019 Time | list • the...KurzweilAI.net ​ "
AI에 관련된 대표적인 기술 용어 20개  ,https://blog.naver.com/yoongs_ai/223058055817,20230328,"​안녕하세요! AI 관련 대표적인 기술 키워드에 대해 정리해보았습니다.​머신러닝(Machine Learning): 데이터를 이용하여 패턴을 학습하고 예측하는 기술입니다.딥러닝(Deep Learning): 인공 신경망을 이용하여 다양한 데이터를 학습하고 분류, 예측하는 기술입니다.컴퓨터 비전(Computer Vision): 이미지, 영상 등의 데이터를 이용하여 패턴을 학습하고 인식하는 기술입니다.자연어 처리(Natural Language Processing, NLP): 인간의 언어를 이해하고 분석하는 기술입니다.강화학습(Reinforcement Learning): 시행착오를 통해 보상을 최대화하는 방향으로 학습하는 기술입니다.분산 컴퓨팅(Distributed Computing): 여러 대의 컴퓨터를 연결하여 하나의 문제를 해결하는 기술입니다.빅데이터(Big Data): 대량의 데이터를 처리하고 분석하는 기술입니다.클라우드 컴퓨팅(Cloud Computing): 인터넷을 통해 서버, 데이터, 소프트웨어 등을 제공하는 기술입니다.데이터 마이닝(Data Mining): 대량의 데이터에서 유용한 정보를 추출하는 기술입니다.인공지능(Artificial Intelligence, AI): 인간의 학습, 추론, 의사 결정 등의 능력을 모방하는 기술입니다.자율 주행(Autonomous Driving): 인공지능과 센서 등을 이용하여 차량이 스스로 운전하는 기술입니다.음성 인식(Speech Recognition): 음성 데이터를 이용하여 음성을 인식하고 분석하는 기술입니다.빅데이터 분석(Big Data Analysis): 대량의 데이터를 다루는 기술로, 기존의 데이터베이스 관리 기술로는 처리할 수 없는 규모의 데이터를 처리하고 분석하는 기술입니다.인공 신경망(Artificial Neural Network): 생물학적 신경망에서 영감을 받은 인공 신경망으로, 딥러닝에서 중요한 기술입니다.GAN(Generative Adversarial Networks): 생성적 적대 신경망으로, 학습 데이터로부터 새로운 이미지, 음악, 영상 등을 생성하는 기술입니다.GPU(Graphic Processing Unit): 그래픽 카드에서 사용되는 처리장치로, 병렬 처리가 가능하여 머신러닝, 딥러닝 등의 연산에 많이 사용됩니다.17. CNN(Convolutional Neural Network): 컨볼루션과 풀링을 이용하여 이미지, 영상 등의 데이터를 처리하는 딥러닝 모델입니다.LSTM(Long Short-Term Memory): RNN의 한 종류로, 시퀀스 데이터를 처리하는데 특화된 모델입니다.Transfer Learning(전이 학습): 미리 학습된 모델을 이용하여 새로운 모델을 만드는 기술입니다.Autoencoder(오토인코더): 입력 데이터를 압축하고 복원하는 과정에서 학습하는 딥러닝 모델입니다.​이러한 AI 기술들은 우리 일상 생활에서도 꾸준히 사용되고 있습니다. 스마트폰의 음성인식 기능, 자동 번역 서비스, 음성 알림 시스템 등이 대표적인 예시입니다. AI 기술은 더 나은 인공지능의 발전을 위해 끊임없이 연구되고 있으며, 앞으로 더 많은 분야에서 적용될 것으로 예상됩니다. "
"[공지] ■ Windows 11, 22H2의 누적 업데이트 파일(KB5026372) : 22621.x → 22621.1702 (= 일반 사용자 업데이트) ",https://blog.naver.com/kasbel/223098006981,20230510,"Microsoft는 Windows 11 사용자를 위한 화요일 5월 패치 업데이트 KB5026372를 출시하여 빌드 22621.1702로 가져갔습니다. 몇 가지 주요 기능과 개선 사항이 포함되어 있으며 4월 말에 출시된 KB5025305에서 사용할 수 있는 모든 업데이트를 제공합니다. 아래 릴리스 노트는 이 두 업데이트의 모든 변경 사항을 가져옵니다.​하이라이트 새로운! 이 업데이트는 설정 > Windows 업데이트 페이지에 새로운 토글 컨트롤을 추가합니다. 이 기능을 켜면 장치에 사용할 수 있는 최신 비보안 업데이트 및 개선 사항을 얻기 위해 장치에 우선 순위를 지정합니다. 관리 장치의 경우 토글은 기본적으로 비활성화되어 있습니다. 자세한 내용은 Windows 업데이트가 장치에 제공되는 즉시 받기를 참조하십시오 .이 업데이트는 Windows 운영 체제의 보안 문제를 해결합니다.개량 이 업데이트는 커널 모드 하드웨어 적용 스택 보호 보안 기능에 영향을 미칩니다. 이 업데이트는 호환되지 않는 드라이버 데이터베이스에 더 많은 드라이버를 추가합니다. 장치는 Windows 보안 UI에서 이 보안 기능을 활성화하고 드라이버를 로드할 때 이 데이터베이스를 사용합니다.이 업데이트는 Windows LAPS(Local Administrator Password Solution)의 경합 상태를 해결합니다. LSASS(Local Security Authority Subsystem Service)가 응답을 중지할 수 있습니다. 이는 시스템이 동시에 여러 로컬 계정 작업을 처리할 때 발생합니다. 액세스 위반 오류 코드는 0xc0000005입니다.새로운! 이 업데이트는 방화벽 설정을 변경합니다. 이제 애플리케이션 그룹 규칙을 구성할 수 있습니다.이 업데이트는 이란 이슬람 공화국에 영향을 미칩니다. 이 업데이트는 2022년부터 정부의 일광 절약 시간제 변경 명령을 지원합니다.이 업데이트는 LSASS(Local Security Authority Subsystem Service) 프로세스에 영향을 미치는 문제를 해결합니다. 응답하지 않을 수 있습니다. 이로 인해 시스템이 다시 시작됩니다. 오류는 0xc0000005(STATUS_ACCESS_VIOLATION)입니다.이 업데이트는 Microsoft Edge IE 모드에 영향을 미치는 문제를 해결합니다. 탭 창 관리자가 응답하지 않습니다.이 업데이트는 보호된 콘텐츠에 영향을 미치는 문제를 해결합니다. 보호된 콘텐츠가 있는 창을 최소화하면 표시되지 않아야 할 콘텐츠가 표시됩니다. 이것은 작업 표시줄 썸네일 실시간 미리 보기를 사용할 때 발생합니다.이 업데이트는 MDM(모바일 기기 관리) 고객에게 영향을 미치는 문제를 해결합니다. 이 문제로 인해 인쇄가 중지됩니다. 이는 예외 때문에 발생합니다.이 업데이트는 특정 이동통신사의 앱 아이콘을 변경합니다.이 업데이트는 서명된 WDAC(Windows Defender Application Control) 정책에 영향을 미치는 문제를 해결합니다. Secure Kernel에는 적용되지 않습니다. 이는 보안 부팅을 활성화할 때 발생합니다.이 업데이트는 잘못된 영역에 작업 보기를 표시하는 문제를 해결합니다. Win+Tab을 눌러 전체 화면 게임을 닫을 때 발생합니다.이 업데이트는 PIN을 사용하여 비즈니스용 Windows Hello에 로그인할 때 발생하는 문제를 해결합니다. 원격 데스크톱 서비스에 로그인하지 못할 수 있습니다. 오류 메시지는 ""요청이 지원되지 않습니다""입니다.이 업데이트는 관리자 계정 잠금 정책에 영향을 미치는 문제를 해결합니다. GPResult 및 결과 정책 집합은 이를 보고하지 않았습니다.이 업데이트는 통합 쓰기 필터(UWF)에 영향을 미치는 문제를 해결합니다. WMI(Windows Management Instrumentation) 호출을 사용하여 이 기능을 끄면 장치가 응답하지 않을 수 있습니다.이 업데이트는 ReFS(Resilient File System)에 영향을 미치는 문제를 해결합니다. OS가 올바르게 시작되지 않도록 중지 오류가 발생합니다.이 업데이트는 MySQL 명령에 영향을 미치는 문제를 해결합니다. 명령이 Windows Xenon 컨테이너에서 실패합니다.이 업데이트는 SMB 다이렉트에 영향을 미치는 문제를 해결합니다. 멀티바이트 문자 집합을 사용하는 시스템에서는 엔드포인트를 사용하지 못할 수 있습니다.이 업데이트는 구형 Intel 그래픽 드라이버에서 DirectX를 사용하는 앱에 영향을 미치는 문제를 해결합니다. apphelp.dll 에서 오류가 발생할 수 있습니다 .이 업데이트는 레거시 LAPS(Local Administrator Password Solution) 및 새로운 Windows LAPS 기능에 영향을 미치는 문제를 해결합니다. 구성된 로컬 계정 암호를 관리하지 못합니다. 레거시 LAPS 정책이 있는 컴퓨터에 2023년 4월 11일자 Windows 업데이트를 설치한 후 레거시 LAPS .msi 파일을 설치하면 이 문제가 발생합니다.알려진 문제 적용 대상징후해결 방법IT 관리자Windows 11, 버전 22H2(Windows 11 2022 업데이트라고도 함)에서 프로비저닝 패키지를 사용하면 예상대로 작동하지 않을 수 있습니다. Windows가 부분적으로만 구성되었을 수 있으며 Out Of Box Experience가 완료되지 않거나 예기치 않게 다시 시작될 수 있습니다. 프로비저닝 패키지는 비즈니스 또는 학교 네트워크에서 사용할 새 장치를 구성하는 데 사용되는 .PPKG 파일입니다. 초기 설정 중에 적용되는 프로비저닝 패키지는 이 문제의 영향을 받을 가능성이 큽니다. 프로비저닝 패키지에 대한 자세한 내용은 Windows용 프로비저닝 패키지를 참조하십시오 .참고 Windows Autopilot 을 사용하여 Windows 장치를 프로비저닝하는 것은 이 문제의 영향을 받지 않습니다.가정이나 소규모 사무실에서 소비자가 사용하는 Windows 장치는 이 문제의 영향을 받지 않을 가능성이 높습니다.Windows 11, 버전 22H2로 업그레이드하기 전에 Windows 장치를 프로비저닝할 수 있으면 문제를 방지할 수 있습니다.현재 조사 중이며 다음 릴리스에서 업데이트를 제공할 예정입니다.모든 사용자들이 업데이트를 설치한 후 중국어 또는 일본어를 사용할 때 일부 앱에서 음성 인식, 표현 입력 및 필기에 간헐적인 문제가 발생할 수 있습니다. 영향을 받는 앱은 때때로 특정 단어를 인식하지 못하거나 음성 인식 또는 영향을 받는 입력 유형에서 입력을 수신하지 못할 수 있습니다. 이 문제는 앱이 오프라인 음성 인식을 사용할 때 발생할 가능성이 높습니다.앱 개발자 참고 사항 이 문제는 Windows.Media.SpeechRecognition 에서 SRGS(Speech Recognition Grammar Specification)를 사용하는 음성 인식에만 영향을 미칩니다 . 다른 음성 인식 구현은 영향을 받지 않습니다.이 문제를 완화하려면 장치를 다시 시작할 때마다 다음을 수행해야 합니다.음성 인식 또는 기타 영향을 받는 입력 유형에 문제가 있는 앱을 닫습니다.시작 버튼을 선택하여 작업 관리자를 열고 ""작업 관리자""를 입력한 다음 선택합니다.왼쪽에서 ""프로세스"" 탭을 선택한 다음 ""이름"" 열을 선택하면 프로세스 목록이 이름별로 정렬됩니다.ctfrmon.exe를 찾아 선택합니다.""작업 종료"" 버튼을 선택합니다.ctfmon.exe의 새 인스턴스가 자동으로 시작되는지 확인합니다.이제 영향을 받는 앱을 열고 음성 인식 및 기타 입력 유형을 사용할 수 있습니다.해결 방법을 찾고 있으며 다음 릴리스에서 업데이트를 제공할 예정입니다. 이 업데이트를 받으려면 Windows Update를 실행하고 최신 업데이트를 적용하십시오. 오프라인 컴퓨터에 설치하려는 경우 Microsoft 업데이트 카탈로그 에서 다운로드할 수 있습니다 .​Windows 11, 22H2 빌드 누적 업데이트 22621.1702 배포되었습니다.   오늘 5월달 월간 보안 업데이트가 배포되었습니다..-------------------------------------------------------------------------------------------------------------------------------------------------월간 보안 업데이트 : 매월 두번째 수요일 (보안 / 비보안 업데이트)  --- 자동 업데이트 (이전 업데이트 모두 포함)선택적 비보안 업데이트 : 매월 네번째 수요일 (비보안 버그 수정 업데이트)   --- 자동 업데이트 [선택적 업데이트] 항목에 나타남 (Windows 10 20H2 및 21H2에 대한 선택적 업데이트가 더 이상 없습니다.)대역 외(OOB) 업데이트 : 새로 발견된 문제나 취약성을 해결하기 위해 불시에 대역 외(OOB) 릴리스를 배포Preview 업데이트 : 참가자 대상으로 업데이트 (버그 수정 업데이트)  --- 참가자 전용 업데이트 (일반 사용자용 아님)-------------------------------------------------------------------------------------------------------------------------------------------------https://techcommunity.microsoft.com/t5/windows-it-pro-blog/windows-monthly-updates-explained/ba-p/3773544 선택적 업데이트는 수동으로 업데이트를 선택하고 다운로드 및 설치를 클릭하지 않는 한 다운로드하거나 설치하지 않습니다. 이 패치에는 보안 수정 사항이 포함되어 있지 않지만 일반적인 개선 사항 및 버그 수정 사항이 포함되어 있습니다. 누적 업데이트는 지금까지의 업데이트를 모두 포함하고 있는 업데이트 파일입니다.ㅎ따라서, 가장 최신의 누적업데이트 파일만 설치하면 최신 윈도우가 되는 겁니다. (이전 누적업데이트를 일일이 설치할 필요없습니다.)​말 그대로 누적된 업데이트 파일입니다. 때문에 파일 용량도 갈수록 커지구요.​서비스 스택 업데이트는 무엇 인가요?  ※ 빌드 진행 상황○ 개발자 채널 [Canary]                          = Windows 11 : 빌드 25375.1            (= 참가자 빌드)○ 개발자 채널 [Dev]                               = Windows 11 : 빌드 23451.1000            (= 참가자 빌드)○ 베타 채널 [Beta]                                 = Windows 11 : 빌드 22621.1690 /  22624.1690        (= 참가자 빌드)○ 미리보기 채널 [Release Preview]     = Windows 11 : 빌드 22621.1635            (= 참가자 빌드)------------------------------------------------------------------------------------------------------------------------○ 일반 사용자 채널                                = Windows 11 22H2 : 빌드 22621.1702              (= 일반 사용자 빌드)○ 일반 사용자 채널                                = Windows 11 21H2 : 빌드 22000.1936              (= 일반 사용자 빌드)○ 일반 사용자 채널                                = Windows 10 : 빌드 1904x.2965           (= 일반 사용자 빌드)      1. 자동 업데이트 ​시작 → 설정 → 업데이트 및 복구 → 윈도우 업데이트자동 업데이트에 안뜨는 분들은 아래 파일을 받아서 수동으로 업데이트해주시면 됩니다.ㅎ​Windows 업데이트 후 청소 프로그램Windows 11, 업데이트가 실패하는 경우 해결 방법Windows 11, 누적업데이트 및 드라이버 업데이트를 숨기는 방법​업데이트 안되시는 분들은 ISO 파일을 탐색기에서 탭재시키고 setup.exe를 실행해서 ""기존자료 유지하는 방법""으로 설치를 진행하시면 업데이트 가능합니다.     2. 수동 업데이트 파일 다운로드  Windows 11, 22H2, 빌드 22621 : 2023-05-09​2023-05 Windows 11 Version 22621에 대한 누적 업데이트(KB5025239) : 22621.x → 22621.1702 선택 1) msu 파일 ( = 서비스 스택 + 누적업데이트)  --- 그냥 떠블클릭해서 설치하면 됩니다. (권장) [x64]https://catalog.sf.dl.delivery.mp.microsoft.com/filestreamingservice/files/20a05b31-ee84-41b9-94b2-bc02f95cf9ee/public/windows11.0-kb5026372-x64_d2e542ce70571b093d815adb9013ed467a3e0a85.msu  [ARM64]https://catalog.sf.dl.delivery.mp.microsoft.com/filestreamingservice/files/5658ad5e-b6a9-4133-8f95-294178025db6/public/windows11.0-kb5026372-arm64_ca95adaf3a2ac2357002aa3fb78cd47454057b98.msu   선택 2) 서비스 스택 / 누적 업데이트 cab 파일KB5026372 = 누적 업데이트(22621.1702) (Cumulative update : CCU) (2023-05-09)22621.1626 = 서비스 스택 업데이트 (Servicing Stack Update : SSU) (누적업데이트 설치 준비 파일) (2023-05-09)   ※ [수동으로 설치 방법]1) 그냥 msu 파일을 떠블클릭하면 설치됩니다. (windows update는 켜두셔야합니다)2) 또는 msu 파일을 아래 툴로 설치하면 됩니다.3) 또는 cab 파일로 설치하려면 위의 msu 파일을 압축을 풀어서서비스 스택(ssu-xxx.cab)을 먼저 설치하고 누적 업데이트 파일(Windows-xxx.cab)을 설치합니다. 아래 처럼 74%에서 설치 시간이 좀 걸립니다. 계속 기다리면 설치완료됩니다.​1 중 1을(를) 처리하는 중 - Package_for_RollupFix~31bf3856ad364e35~amd64~~22000.160.1.0 패키지를 추가하는 중[===========================74.0%==========                ] ​※ Cab 파일 설치도구 : 설치도구 : InstallUpdate-GUI-v1.0.0.7z0.00MB또는InstallUpdate-GUI-v1.1.2.zip0.04MB 위의 설치도구를 받아서 압축을 풀고 실행해서....​​1) [Browse..] 버튼을 눌러서 cab 파일의 위치를 지정해줍니다.2) [Install] 버튼을 눌러서 cab 파일을 설치해주고3) [Y] 키를 눌러서 재부팅 해주면 됩니다...4) 파워셀이나 실행창에서 winver 명령어로 업데이트된 버전 확인...      ------------------------------------------ 아래는 이미 설치했으면 건너뜁니다. ------------------------------------------------------------  2023-02 Windows 11, version 22H2 x64에 대한 .NET Framework 3.5 및 4.8.1 누적 업데이트(KB5022497) :  (2023-02-14) [x64]https://catalog.s.download.windowsupdate.com/c/msdownload/update/software/secu/2023/01/windows11.0-kb5022497-x64-ndp481_92ce32e8d1d29d5b572e929f4ff90e85a012b4d6.msu  [ARM64]https://catalog.s.download.windowsupdate.com/c/msdownload/update/software/secu/2023/01/windows11.0-kb5022497-arm64-ndp481_5e26eb6b35476c58b96023b6b1d305c6b8506cb9.msu​ "
[중등 영어 에세이 주제] The Impact of Artificial Intelligence Technology ,https://blog.naver.com/writer_pnp/223007731002,20230207,"Artificial Intelligence (AI) is a rapidly growing field of technology that is changing the world as we know it. AI refers to the development of computer systems that can perform tasks that normally require human intelligence, such as visual perception, speech recognition, decision-making, and translation between languages.​One of the key benefits of AI technology is that it can save time and improve efficiency. For example, AI-powered machines and algorithms can analyze large amounts of data much faster and more accurately than humans. This can help companies make better decisions and improve their bottom line. AI is also being used in healthcare to diagnose diseases, develop personalized treatments, and even perform surgeries.​However, there are also concerns about the impact of AI on society. As AI technology continues to advance, it may replace jobs that were once done by humans. This could lead to unemployment and economic inequality. It is important for policymakers to consider these issues and develop strategies to ensure that the benefits of AI are shared by all members of society.​In conclusion, AI is a powerful technology that has the potential to transform the world for the better. However, it is important to consider the potential negative impacts of AI and take steps to address them. By doing so, we can ensure that AI technology is developed and used in a responsible and ethical manner that benefits everyone. "
"라즈베리파이4에 풀버전 윈도우10 설치하기(초초간단, 끝장, 더 이상 쉬울수 없다) ",https://blog.naver.com/m4316/222035213510,20200719,"Raspberry Pi 4에 Windows 10 풀버젼 설치하기가 또 다시 돌아왔습니다. 처음 썼던 것은 최대한 자세히 써드린다고 써드렸지만 따라오지 못하시거나 따라왔는데 실행이 되지 않는 경우도 있었고 두번째는 그나마 쉬웠지만 여전히 여러 작업이 끼다보니 변수에 의해 되지 않는 경우가 발생했습니다.​그.러.나. 이번엔 충격적이게도(?) 이미지 파일만 마이크로 SD카드에 올리면 바로 윈도우10이 가능하다는 것이었습니다. 또한 이번엔 램이 4GB가 다 인식을 하고(하지만 3GB만 사용가능), USB 4개 포트가 다 이용이 가능합니다. 저번처럼 USB-C 타입 전원 충전기 꽂는 곳에 오징어 허브(?) 꽂고 안해도 된다는 것이죠. 바로 시도해보겠습니다.​** 참고로 라즈베리파이4(Raspberry Pi 4) 4GB 이상을 추천드립니다. 라즈베리파이 3B, 3B+ 도 가능은 하지만 램이 1기가밖에 안되고 1.2, 1.4 별 차이 안나는 것 같지만 속도차이 상당합니다. 라즈베리파이4 8GB를 쓰셔도 3GB 밖에 사용할 수 없습니다. 바이오스에서 램 디스크로 활용이 가능하나.. 라즈 램이면 언제 날아갈지 모르는 데이터라.. 참고만 하세요.. 괜히 비싸게 라즈4 8GB 샀는데 안된다고 하셔도 소용없습니다.. 라즈4 4기가 사세요..(?)​먼저 가장 중요한 한방 이미지를 받아야 하는데, 두방도 아니고 한방입니다..(퍽퍽.. 끌려간다..) 디스코드(Discord)의 윈도우즈 온 라즈베리파이라는 오픈채널에 있습니다. 아래 링크로 들어가시면,https://discord.com/invite/jQCpfVK Join the Windows On Raspberry Productions Discord Server!Check out the Windows On Raspberry Productions community on Discord - hang out with 2,633 other members and enjoy free voice and text chat.discord.com 이런 채팅방이 뜹니다. 저는 크롬으로 시도해서 디스코드 설치하지 않고도 가능하지만 다른 브라우저는 확인을 못해봤습니다. 로그인 안하고도 게스트로 확인 가능합니다. 왼쪽 메뉴의 #download-links 의 Amir Dahan이 올린 3.33GB file on Mega 를 들어가줍니다. 이렇게 뜨는데 다운로드를 눌러 원하는 위치에 다운로드 받아줍니다.​ 받는 동안 제가 항상 Micro SD 카드에 이미지 넣을 때 쓰는 엣쳐(balenaEtcher)를 깔아줍니다. 그림처럼 윈도우 버젼을 깔아주시면됩니다. 혹시라도 리눅스나 맥을 쓰시면 맞게 깔아주시면됩니다.https://www.balena.io/etcher/ balenaEtcher - Flash OS images to SD cards & USB drivesA cross-platform tool to flash OS images onto SD cards and USB drives safely and easily. Free and open source for makers around the world.www.balena.io https://www.partitionwizard.com/free-partition-manager.html Best Free Partition Manager for Windows | MiniTool Partition Wizard FreeMiniTool Partition Wizard Free 12.1 V12.1: Use Shadow Copy (VSS) to copy OS/disk/partition without reboot. (289) All-inclusive free partition manager to organize disk partitions to get the best of your hard drive Measure drive performance and analyze disk space usage in simple ways Download Now Go t...www.partitionwizard.com ​다음은 미니툴 파티션 위자드(MiniTool Partition Wizard)라는 것을 깔 건데 일단 깔기만해주세요. 아래서 왜 까는지는 알려드리겠습니다.(자신이 윈도우 켜지는 것만 감상하고 싶고 사용목적이 없다면 이 과정은 생략해도 됩니다.) 설치파일을 실행시켜주면 이렇게 나오는데 위에 있는 미니툴 파티션 위자드 프리만 설치해줍니다. 다음 종료, 지금은 필요 없으니 실행 안시킵니다. 이렇게 필요한 준비물 3개가 모두 모였습니다. 일단 RAR형식으로 된 build 0.2.1 압축파일을 풀어줍니다. 안에 이미지 파일이 들어있습니다. 아직 버전이 1도 안된 걸 보면 추후에 더 발전할 가능성이 있다는 것을 알 수 있습니다. 옆에 살포시 놓고, 엣쳐를 실행시켜줍니다. 맨 왼쪽에 Select image를 눌러줍니다. 아 엣쳐든 윈32 디스크 이미져(Win32 Disk Imager)건 기타 본인이 쓰는 이미징 프로그램이 있다면 모든 백신의 실시간 검사, USB 검사를 끄시기 바랍니다. 어떤 이미지 프로그램을 쓰던 오류가 난다면 백신일 확률이 높습니다. 일단 저는 V3 Lite 를 사용하기에 실시간 검사를 꺼주고,  안에 환경설정에서 USB 드라이브 자동 검사를 꺼줍니다. 꼭 꺼주세요. 이것때문에 실패하는 경우가 99%입니다. 알약이던 디펜더건 다 꺼주세요. 여튼 다시 셀렉트 이미지 누르고, 아까 압축 푼 build 0.2.1을 열기 해줍니다. 중간에 확실히 Micro SD Card 인지 확인해주세요. 보통의 이미저 프로그램은 그게 하드디스크고 윈도우가 깔려있고 상관없이 밀어버립니다. 최소 두번 이상 확인해주세요.(더블 체크) 저는 싼 디스크(SanDisk)의 32GB 기본형 모델로 했습니다. 가 아니라 Lexar(렉사)꺼네요.(?) 확인이 완료되었다면 플래쉬!(Flash!) 버튼을 눌러줍니다. 뚜루루루루루~ 완료됬습니다. 꼭 Flash Complete! 아래의 1 Successful device 옆에 초록불이 들어왔는지 확인해주세요. 백신을 안껐다거나 연결이 불량하여 오류가 나면 옆에 노란불이나 빨간불 들어와 있는 경우가 있습니다. 완료됬다고 바로 라즈에 꼽으면 SD카드가 뻑날 가능성이 높습니다.​완료가 됬으면 바로 라즈베리파이에 잘 꽂... 지 말고 아까 마지막으로 설치한 미니툴 파티션 위자드 프리를 실행해줍니다. 자신이 윈도우10이 라즈에서 실행되는 영롱한 장면만을 보고싶다면 스킵하셔도 됩니다. 이렇게 실행하면 맨 아래 아까 윈도우 설치한 SD카드가 보입니다. 오른쪽에 할당되지않음이 무려 20기가가 있죠. 저 공간은 실제로 라즈에 꼽으면 사용불가능한 영역입니다. 관상용이 되지 않고 무언가를 설치하기 위해서는 용량이 필요하겠죠? 아까 이미저처럼 저 디스크가 방금 윈도우10 프로를 깔은 SD카드인지 더블 체크하시고 윈도우가 깔린 영역(가운데)을 오른쪽 클릭합니다. 그리고 확장을 선택해줍니다. 뜨면 가운데 슬라이더를 오른쪽 끝까지 당겨줍니다. 간혹 약간 공간을 남겨서 SD 불량 섹터를 걸러주는 경우가 있는데.. 제가 해본 결과 SD 뻑나는건 그런거랑은 상관없더군요.. 그냥 끝까지 땡겨줍니다.. 하하.. 그러면 옆의 공간이 다 합쳐져서 사용가능한 공간이 20기가 이상 되는 것을 확인할 수 있습니다. 왼쪽 아래 적용을 눌러주고, 예, 확인하면 끝납니다. 이제 드디어 이 마이크로 SD 카드 녀석을 라즈에 꼽아줍니다.. 저는 라즈베리파이4 4GB 모델로 했습니다. 아래에 꼽아주고, 전원케이블, 마이크로 흐드미(Micro HDMI), 마우스, 키보드를 연결시켜줍니다. 이번 버전에선 USB포트 4개가 정상 인식하니 그냥 꼽아주시면 됩니다. 그리고 실행이 됩니다. 실행되는 영롱한 과정을 찍은 비디오가.. 녹화시작인줄 알았는데 사진이었어서 없습니다. 저는 재연출 따위는 없습니다. 처음 실행만 찍어서요.. 그래서 넘어갑니다.​일반 윈도우10 설치과정과 동일하게 진행해줍니다. 다만 다른건 저번엔 한글 윈도우 이미지를 넣었으니 한글로 떴지만 이번엔 만들어진 이미지를 쓰는거라 영어입니다. 다들 영어할 줄 아시죠...? 그럴줄 알고 각 단계별 사진을 찍어보았습니다.. 하하.. 먼저 나라 선택입니다. 영어버전이지만 저는 Korea를 선택. 키보드 레이아웃입니다. 역시 Korean 선택.. 국뽕.. Skip 왼쪽 아래 I don't have internet 을 눌러줍니다. 안그래도 버벅거리는데 이상한거 설치시키려고 마이크로소프트 녀석들이 수작부리는겁니다.. 인터넷을 연결하면 이렇게 많은 것들을 할 수 있다고 현혹하지만...왼쪽 아래 Continue with limited setup 을 눌러 회피해줍니다. 그럼 계약권 어쩌구 나오고, Accept, 계정 이름을 선택해줍니다. 당연히 저는 Bill Gates로..(?!) 윈도우10은 이스터 에그가 없다지만.. 뭔가 더 좋은 성능을 줄 것 같습니다... 하하.. 비번에서 많이들 낚이시는데, 비번을 넣지 않고 Next를 눌러도 넘어가집니다. 나중에 PIN 없애고 이럴일이 없습니다. 물론 마소 계정에 로그인하기 전까지는요. 테스트할건데 로그인까지 하긴 좀 그렇죠. 이제 잡기능들 소개시간입니다. 소개라고 한 이유는 다 안할거라서요. 일단 다 아래꺼 선택하시면 된다고 보시면 됩니다.음성인식, Don't use online speech recognition. Accept, 위치인식, No, Accept, 장치찾기, No, Accept, 진단 정보 전송, Basic, Accept, 잉킹이랑 타이핑 증가라고 하는데, 2018년인가 부터 생긴것으로 한마디로 개인이 검색하거나 한것을 이용해 연관 검색같은거를 올려주는 기능입니다.No, Accept, 경험 데이타 활용, 이것도 연관 팁 같은거를 보여주는겁니다.No, Accept, 광고에 쓰기, No, Accept, 그럼 당신을 위해 윈도우를 준비하는 중 뜹니다(Preparing Windows) 기분탓이겠지만 전보다 빨라진거 같습니다. 그리고 바탕화면에 도착! 기묘한 배경화면으로 해놨네요. 작업관리자에서 4코어 1.5Ghz 정상인식합니다. 메모리는 무려 3GB!!!!!전에는 1기가바이트 밖에 안되서 압축을 과도하게 하다보니 더 느려지고 또 필수적인 것도 안되고 그랬는데 이제 아주 쾌적합니다. 물론 1GB일 땐 500MB 정도 먹었던 거 같은데 3기가바이트 되니 1GB를 먹네요.. 나쁜놈.. 디스크는 SD카드 정상인식합니다. 현재까지로는 3.5mm 오디오잭이랑 이더넷 어댑터(인터넷 랜선 꽂는곳)이 안된다고 하므로 USB WIFE.. 가 아니라 WIFI 어댑터 모듈을 꽂아보겠습니다.. 그리고 실패했습니다.. 그래서 사진도 안찍음! 저렴한 USB to Ethernet 어댑터를 주문했습니다. 한국에서는 LAN 포트라고 하는 줄 모르고 엄청나게 검색을 해버렷... 주말이라 인터넷은 못해보는게 한이네요.. 일단 인터넷을 연결하면 저 Edge Beta Setup을 누르고 사용하시면 됩니다. 기본적으로 깔린 더럽한 인터넷(Internet Explorer) 같은건 없습니다. 꼭 엣지를 깔아주세요. 엣지가 싫다면 크롬(Chrome) 브라우져 오프라인 버전 검색해서 받아 옮기시면 됩니다.​이제 완전이 사용 가능한 윈도우10 설치는 끝났고, 오버클럭이나 심심한데 해보겠습니다. 처음 시작할때 라즈베리 로고에서 ESC를 누르면 바이오스(Bios)로 들어가집니다. 현재 오버클럭이 안된 상태라 위에 1.5GHz 로 표시되고 램은 3,072MB(3GB) RAM 이 사용되고 있습니다. 처음에 있는 Device Manager(디바이스 매니저) 선택 Raspberry Pi Configuration(라즈베리파이 설정) 선택 CPU Configuration(씨피유 설정) 선택 여기서 Default(기본) 설정은 1500 입니다. 1.5Ghz 입니다. 이렇게 여러가지 있는데 Low는 디폴트랑 같고 커스텀을 선택해봅니다. 아래 숫자칸이 수정이 가능해지는데 2000을 넣으면 2.00기가헤르츠가 됩니다. Max를 누르면 바이오스에서 라즈베리파이4의 한계 클럭으로 올려줍니다. 확인결과 2200, 2.2Ghz로 올려버립니다. 들어가긴 하는데 작동이 그냥 1.5Ghz로 작동하는 것 같습니다. 안먹는듯.. 쥬륵 설정이 완료가 되면 바로 나가지 않고 F10을 눌러 적용해줍니다. 무조건 해당 화면에서 저장을 해야 적용이 됩니다. 뭐 뜨면 Y 눌러주면 됩니다. 커스텀으로 2000 하고 Max 선택해준 결과입니다. 각각 2.00Ghz, 2.20Ghz로 적용이 됩니다. 근데 맥스는 뭔가 안되는 것 같은 느낌이 들어서 커스텀으로 2000만 주고 씁니다. 마지막으로 인터넷이 안되니 USB로 옮긴 하드웨어모니터(CPUID HWMonitor)입니다. 센서가 인식이 안되는지 그닥 뜨는건 없습니다. 또한 64비트인데도 불구하고 64비트 프로그램이 다 사용이 안됩니다. 32비트 하웨모니터를 사용했습니다.​이상으로 라즈베리파이에 윈도우10 설치 끝판왕을 설치해봤습니다. 끝장인 만큼 이제 윈도우10의 이미지만 정상화되면 누구나 쉽게 설치하고 사용할 수 있어질 것 같습니다. 거기에 여러 프로그램들이 안정화가 된다면 미니 PC나 스틱 PC, 일체형 PC들의 대체제(사양때문에 완벽하게는 대체하지 못할듯)가 될 수 있을 것 같습니다. 가격이 워낙 저렴하니 말이죠.​제가 참고한 영상은 아래와 같습니다. 인터넷만 됬다면 스팀이나 다른 게임도 받아서 해봤을텐데 아쉽습니다. 빨리 USB 랜선 변환기가 도착하면 좋겠습니다.​그리고 항상 드리는 말씀이지만 윈도우 설치는 라즈베리파이에서 정상적으로 작동시키는 범주에 들어가지 않기 때문에 이로 인해 발생하는 책임은 여러분에게 있다는 것을 명심하시길 바랍니다.​https://youtu.be/3ngGlMikto0 https://youtu.be/dJUHEBtEG_k ​ "
Machine Learning vs AI: What's the Difference? ,https://blog.naver.com/yoono044/223022905638,20230221,"​​Machine Learning and Artificial Intelligence are two of the most popular buzzwords in the tech industry. While they are often used interchangeably, they are not the same thing. In this article, we will explain the difference between Machine Learning and AI and how they are related.​​ https://miro.medium.com/max/958/1*c9STAwkoSwbSSNbzNotk4g.jpeg​What is Artificial Intelligence?​Artificial Intelligence, or AI, is a broad term that refers to the development of intelligent machines that can perform tasks that usually require human intelligence, such as decision-making, visual perception, speech recognition, and language translation. AI involves developing algorithms that can analyze data, learn from it, and make decisions based on that learning.​What is Machine Learning?​Machine Learning is a subset of Artificial Intelligence that allows machines to learn from data and make predictions without being explicitly programmed. It involves developing algorithms that can analyze data, learn patterns, and make decisions based on that data.​In other words, while AI is a more general term that refers to the development of intelligent machines, Machine Learning is a specific technique used to enable machines to learn and make predictions.​How are Machine Learning and AI related?​Machine Learning is a critical component of AI. It provides machines with the ability to learn from data, which is essential for the development of intelligent machines. Machine Learning is used to train models that can make predictions or classify data based on that learning.​In contrast, AI involves developing a range of algorithms and techniques that enable machines to perform tasks that usually require human intelligence. Machine Learning is just one of the many techniques used to develop AI.​For example, deep learning is a subfield of Machine Learning that involves training neural networks to perform specific tasks, such as image recognition or natural language processing. Deep learning is just one of the many techniques used in AI, along with other techniques such as rule-based systems, genetic algorithms, and expert systems.​In summary, Machine Learning is a subset of Artificial Intelligence that focuses on enabling machines to learn from data and make predictions, while AI involves the development of intelligent machines that can perform tasks that usually require human intelligence.​Conclusion​In conclusion, Machine Learning and Artificial Intelligence are not the same thing, but they are related. Machine Learning is a specific technique used to enable machines to learn from data and make predictions, while AI involves the development of intelligent machines that can perform tasks that usually require human intelligence.​As the field of AI continues to evolve, it is essential to stay up-to-date with the latest developments and techniques. Both Machine Learning and AI have numerous applications in various fields, including healthcare, finance, e-commerce, and marketing.​Therefore, it is worth investing time and resources to understand and leverage the potential of these technologies. With the vast amount of data being generated every day, Machine Learning and AI are becoming more critical than ever, and it is worth investing time and resources to understand and leverage their potential.​​​#MachineLearning #AI #ArtificialIntelligence #DataScience #Tech #Technology #DataAnalysis #DeepLearning #NeuralNetworks #DataMining #RuleBasedSystems #ExpertSystems #GeneticAlgorithms #DataPreparation #ModelTraining #ImageRecognition #NaturalLanguageProcessing #Healthcare #Finance #Ecommerce #Marketing #Investment #Innovation #BigData #Python #Programming #TechEducation #LearningResources​​ "
"세계화, 아직 끝나지 않았다 ",https://blog.naver.com/chrisannsue/223022787073,20230221,"The case for globalisation optimismPerhaps isolation is not inevitable, after all 세계화 낙관론의 근거어쨌든 고립은 불가피한 것이 아닐지 모른다 Finance & economics | Free exchange금융과 경제 | 프리 익스체인지Feb 16th 2023 ‧ Finance & economics: 각국의 경제/금융정책 및 실물경제의 흐름에 대한 칼럼으로, 버튼우드(Buttonwood)는 금융, 프리 익스체인지(Free Exchange)는 실물경제를 다룬다.  “We are suffering just now from a bad attack of economic pessimism,” wrote John Maynard Keynes in 1930, in the midst of a disintegrating global economy. He went on to describe the much better future the world could expect if it ever got its act together. Things are not so bleak today, but it is nevertheless hard to feel cheerful about globalisation’s prospects. America and China, which together account for nearly a quarter of world trade, are on ever-icier terms. Rules which fostered an era of rapid globalisation are being flouted into irrelevance. Perhaps most distressing is the sense that this film has played before. The 19th century saw its own period of breakneck globalisation. In the end, however, economic nationalism and great-power conflict destroyed the global trading system, and much else besides. A spiral towards catastrophe sometimes seems only a few stray balloons away. ‧ 존 메이너드 케인스: 잉글랜드의 경제학자. '완전 고용을 실현·유지하기 위해서는 정부의 개입 없이 시장에만 경제를 맡기는 자유방임주의가 아닌 정부의 개입과 보완책이 필요하다'는 케인스주의를 주창했다. 이런 케인스의 이론은 오늘날 거시경제학의 기초가 되었고 여전히 경제학계에서 큰 영향력을 행사하고 있다.‧ get one’s act together: 마음[자신]을 가다듬다‧ flout: 비웃다, 조롱하다, 경멸하다‧ irrelevance: 부적절, 무관계, 현대성의 결여‧ breakneck: 위험하기 짝이 없는 존 메이너드 케인스(John Maynard Keynes)는 세계 경제가 한창 해체되던 1930년 “우리는 지금 경제 비관론의 해로운 공격으로 고통 받고 있다""고 썼다. 계속해서 그는 세계가 전열을 가다듬을 경우 기대할 수 있는 훨씬 더 나은 미래상을 묘사했다. 오늘날 그렇게 상황이 암담하지는 않지만 그럼에도 세계화의 전망에 낙관하기는 어렵다. 합해서 세계 무역의 거의 4분의 1을 차지하는 미국과 중국의 관계는 그 어느 때보다 더 냉랭하다. 한때 급속한 세계화 시대를 촉진시켰던 규칙들이 지금 시대착오적인 것으로 홀대받고 있다. 아마도 가장 가슴 아픈 것은 이런 시나리오가 전개되었던 적이 있었다는 느낌일 것이다. 19세기는 무서운 속도의 세계화 시대였다. 그러나 결국 경제 민족주의와 강대국 간의 갈등 탓에 세계 무역 체제는 물론이고 그 밖의 많은 것이 파괴되었다. 파국으로 향하는 하향곡선은 이따금 멀리 떠돌아다니는 몇 개의 풍선처럼 보인다. The world has experience with cold war, but not between countries as economically intertwined as America and China. In a suspicious atmosphere, accidents happen. The habit of protecting and subsidising domestic firms—as both countries are now doing on a gargantuan scale—may prove difficult to break. All this means globalisation’s immediate prospects appear bleak. But looking on the bright side, as Keynes did, is a helpful reminder of the ways in which events often end up going better than expected. Where globalisation is concerned, demography, technological progress and the example of history itself could push the world in the direction of more, rather than less, integration. Globalisation’s prospects are brighter than most now appreciate. ‧ gargantuan: 거대한, 굉장히 큰 세계는 냉전을 경험했지만, 미국과 중국처럼 경제적으로 뒤얽힌 국가 사이의 냉전은 아니었다. 심상치 않은 분위기에서 사고가 일어난다. (양국이 현재 대대적인 규모로 따르고 있는) 국내 기업을 보호하고 보조금을 지급하는 관행은 근절하기 어려운 것으로 입증될지 모른다. 이 모든 것을 고려하면 세계화의 단기적인 전망은 암울해 보인다. 그러나 케인스가 그랬듯이 밝은 면을 보면 사건들이 흔히 예상보다 긍정적으로 전개된다는 사실을 되새기는 데 도움이 된다. 세계화에 관해서라면 인구구성과 기술 진보, 역사의 전례 자체가 통합 약화가 아니라 통합 강화의 방향으로 세계를 몰아갈 수 있다. 세계화 전망은 현재 대부분이 평가하는 것보다 밝다. Start with demographic change. History suggests that trade policy responds to the relative scarcity or abundance of factors of production, like labour. In the 19th century, countries with lots of land but few workers, like America and Australia, subsidised immigration. But as economic integration narrowed price and wage gaps across countries, and workers in once labour-scarce economies grew angry at slow pay growth, governments began erecting barriers to goods and people. Recent experience tells a similar story. Exposure to imports from labour-rich economies like China fuelled anti-trade sentiment. Americans have elected successive protectionist presidents after years of labour-market weakness, in which too many workers competed for too little work. 인구구성 변화부터 시작해보자. 역사를 돌아보면 무역 정책은 노동 같은 생산 요소의 상대적 희소성이나 풍부성에 따라 달라진다. 19세기에 미국과 호주처럼 토지는 풍부하지만 노동력은 적은 국가들은 이민에 보조금을 지급했다. 그러나 경제 통합을 통해 국가들 사이의 가격 및 임금 격차가 줄어들고 한때 노동력이 부족했던 국가들의 노동자들이 지지부진한 임금 성장에 분개하자 정부는 상품과 사람들을 가로막는 장벽을 쌓기 시작했다. 최근의 경험이 전하는 메시지도 비슷하다. 중국처럼 노동력이 풍부한 경제에서 수입 시장 개방은 반(反) 무역 정서를 부채질했다. 수년 동안 노동시장의 약세로 지나치게 많은 노동자가 지나치게 적은 일자리를 놓고 경쟁한 이후 미국인들은 연달아 보호주의 성향의 대통령을 선출했다. Recently, though, the situation has begun to change. Unemployment rates are low across much of the rich world, and investment programmes intended to reshore production may further boost demand for workers, even as labour forces grow more slowly or shrink. Although robots may eventually help plug workforce gaps, rich countries looking to expand production will need to welcome foreign workers, or source goods and components through supply chains which tap abundant labour supplies in other economies. Either would deepen cross-border ties. ‧ reshore: 제조업을 본국으로 회귀하다‧ plug: 틀어막다  하지만 최근 들어 상황이 바뀌기 시작했다. 대부분의 선진국에서 실업률이 낮은 상태이며 노동력이 더 느린 속도로 증가하거나 감소할 때조차 생산 리쇼어링이 목적인 투자 프로그램으로 말미암아 노동 수요가 더욱 증가할 수 있다. 로봇이 결국 인력 격차를 메우는 데 일조할 수 있지만 생산을 확대하려는 선진국들은 다른 국가의 풍부한 노동력 공급을 활용하는 공급 사슬을 통해 외국인 노동자를 받아들이거나 상품과 부품을 공급받아야 할 것이다. 어느 쪽이든 간에 국경을 초월한 유대가 강화될 것이다. Technological change is another cause for optimism. In the 19th century, railways and telegraphs brought a sharp decline in transport and communication costs, and were at least as responsible for economic integration as cuts to tariff barriers. Over the past half-century, information technology and container shipping helped make the explosive growth of global supply chains possible. Today, privacy and national-security concerns have led to some balkanisation of digital-information flows. One might suppose governments will be more protective still of powerful new ai. ‧ balkanisation: (국가·제국 따위의) 발칸화(化), 소국(小國)으로 쪼개지기, 분단 정책/[비유] 분쇄, 분열 낙관론의 또 다른 요인은 기술 변화다. 19세기에 철도와 전신은 운송 및 통신 비용의 급격한 감소를 초래했으며 적어도 관세 장벽의 감소에 못지않게 경제 통합에 이바지했다. 지난 반세기 동안 정보 기술과 컨테이너 수송으로 말미암아 세계 공급 사슬의 폭발적인 성장이 가능해졌다. 오늘날 개인정보와 국가 보안에 대한 우려는 디지털 정보 흐름의 분열로 이어졌다. 일각에서는 정부가 여전히 강력한 새로운 인공지능을 더욱 보호할 것이라고 추측할지 모른다. But technology will facilitate trade in other ways. The transition to renewable energy sources will create new patterns of resource scarcity and abundance. Remote-work technologies have already reduced the cost of providing services across borders. In a context of labour scarcity, this sort of trade is likely to increase, whether or not domestic working arrangements return to patterns last seen before covid-19. In addition, continued improvements in machine translation and speech recognition will reduce the cost of trade in both goods and services among countries that speak different languages. Although the macroeconomic effects of progress in ai are difficult to predict, an ai-powered economic boom would probably be associated with large global flows of investment and capital goods. If productivity were to surge in the economies of ai leaders like America, such places might become more eager to export and more open to measures which liberalise trade. 그러나 기술은 다른 방식으로 무역을 촉진할 것이다. 재생 가능 에너지로 전환함에 따라 자원 부족과 풍부의 새로운 패턴이 창조될 것이다. 원격 근무 기술 덕분에 국경을 초월한 서비스 제공의 비용이 이미 절감되었다. 국내 업무 환경이 코로나19바이러스 이전에 마지막으로 목격된 패턴으로 돌아오든 아니든 상관없이 노동력이 부족한 상황이라 이런 종류의 무역이 증가할 가능성이 있다. 아울러 기계 번역과 음성 인식이 지속적으로 개선됨에 따라 언어가 서로 다른 국가 간의 상품과 서비스의 거래 비용이 모두 감소할 것이다. AI 발전의 거시경제학적인 영향을 예측하기는 어렵지만 AI에서 동력을 얻은 경제 호황은 십중팔구 투자와 자본재의 세계적인 큰 흐름과 연관될 것이다. 만일 미국과 같은 AI 선두주자들의 생산성이 급증한다면 그런 지역은 수출에 더욱 주력하고 무역을 자유화하는 조치를 더욱 개방적으로 수용할 것이다. Optimism is warranted, above all, because we learn from the past. The macroeconomic shocks in 2007-09 and 2020 could easily have sparked depressions, but did not because policymakers understood how to avoid the gravest errors of the 1930s. Covid took an awful toll, but advances in public health and medicine helped ensure that the pandemic was less deadly than the Spanish flu, in a world far more populous and connected than that of 1918. And whereas the leaders of a century ago could not anticipate the terrible cost of the detour taken in August 1914, those today are well aware of it. History will be different as a result. 무엇보다 우리가 과거로부터 배우기 때문에 낙관론은 근거가 있다. 2007~09년과 2020년의 거시경제적인 충격으로 말미암아 경기 침체가 촉발될 가능성이 높았다. 그러나 정책 입안자들이 1930년대의 가장 심각한 오류를 피하는 법을 이해한 덕분에 그 가능성은 실현되지 않았다. 코로나19바이러스는 끔찍한 피해를 입혔지만 공중 보건과 의학의 발전에 힘입어 1918년에 비해 훨씬 더 복잡하고 연결된 세계에서 펜데믹의 피해를 스페인 독감보다 줄일 수 있었다. 아울러 한 세기 전의 지도자들은 1914년 8월에 택한 우회적인 방식에 따를 끔찍한 대가를 예측할 수 없었지만 오늘날 지도자들은 잘 인식하고 있다. 그러므로 역사는 달라질 것이다. All changeThose still feeling dour should take courage from recent experience. For all the considerable difficulties of the past decade or so, global trade as a share of gdp has only retreated a little from the peak it reached in 2008. Recent history demonstrates, moreover, that nothing in geopolitics is for ever—and trends which look inexorable come to an end. The cold war divided the world and then, suddenly, it did not. Supreme confidence in the inevitable spread of democracy was displaced by the worry that an authoritarian China would dominate the globe, which is now barely a worry at all. The stalemate between America and China will one day be old news, perhaps sooner than most currently think. ‧ dour: 뚱한, 음침한, 완고한‧ inexorable: 무정한, 냉혹한, 들어주지 않는, 굽힐 수 없는‧ stalemate: 막다름, 고착상태 cf) doldrums, gridlock, deadlock 모든 것은 변한다그래도 우울한 기분이 가시지 않는다면 최근 경험에서 용기를 얻어야 할 것이다. 지난 10여 년 동안 고전을 면치 못했지만 그래도 GDP 대비 세계 무역량은 2008년의 최고점에서 조금 후퇴했을 뿐이다. 뿐만 아니라 최근 역사에서 확인할 수 있듯이 지정학에서 영원한 것은 없으며 거칠 것 없어 보이는 트렌드라도 끝이 나기 마련이다. 냉전은 세계를 분열시켰지만 돌연 상황이 역전되었다. 민주주의가 필연적으로 확산될 것이라는 궁극의 자신감은 독재적인 중국이 세계를 지배할 것이라는 우려로 대체되었지만 이제 그것은 걱정거리도 아니다. 미국과 중국 사이의 교착 상태는 언젠가 어쩌면 대부분이 생각하는 것보다 더 빨리 고릿적 뉴스가 될 것이다. Mistakes led the world to its current uncertain state, it is true. And more mistakes will certainly be made. But the past shows only what has gone wrong, not what will. It is by remembering this that we find the wisdom to do better.  실수들이 세상을 현재의 불확실한 상태로 이끈 것은 사실이다. 그리고 분명히 더 많은 실수가 저질러질 것이다. 그러나 과거는 무엇이 잘못될 것인가가 아니라 무엇이 잘못되었는지를 보여줄 뿐이다. 이를 마음에 새김으로써 우리는 더 나아질 수 있는 지혜를 찾는다. by Christina "
인공지능과 딥러닝의 원리 ,https://blog.naver.com/kpros/222983518551,20230113,"인공지능과 딥러닝의 원리​인공지능(AI)은 컴퓨터에서 인간의 지능을 시뮬레이션하는 것이다. 학습, 문제 해결, 패턴 인식 및 의사 결정과 같이 일반적으로 인간의 지능이 필요한 작업을 수행할 수 있는 알고리즘과 컴퓨터 프로그램의 개발을 포함한다.​딥러닝은 인간 뇌의 구조와 기능을 본뜬 인공신경망 활용을 기반으로 하는 AI의 하위 집합이다. 이러한 네트워크는 상호 연결된 노드 또는 ""뉴런""의 레이어로 구성되며, 패턴을 인식하고 대량의 데이터를 기반으로 예측하도록 훈련된다. 딥 러닝 알고리듬은 이미지 및 음성 인식, 자연어 처리 및 예측 분석과 같은 작업에 사용될 수 있다.https://youtu.be/MKmsfUAnvqY ​Principles of Artificial Intelligence and Deep Learning​Artificial Intelligence (AI) is the simulation of human intelligence in computers. It involves the development of algorithms and computer programs that can perform tasks that typically require human intelligence, such as learning, problem-solving, pattern recognition, and decision making.​Deep Learning is a subset of AI that is based on the use of artificial neural networks, which are modeled after the structure and function of the human brain. These networks are composed of layers of interconnected nodes or ""neurons,"" and are trained to recognize patterns and make predictions based on large amounts of data. Deep learning algorithms can be used for tasks such as image and speech recognition, natural language processing, and predictive analytics, among others. "
Transformers (신경망 언어모델 라이브러리)  ,https://blog.naver.com/youseok0/223093700449,20230504,"https://wikidocs.net/book/8056 🤗Transformers (신경망 언어모델 라이브러리) 강좌현재 전 세계적으로 가장 많이 활용되고 있는 신경망 언어모델(Neural Language Models) 기반 딥러닝 라이브러리인 [Hugging Face](https://hugg…wikidocs.net Transformers (신경망 언어모델 라이브러리) 강좌  ​NLP가 무엇인가요?​NLP는 인간의 언어(human language)와 관련된 모든 것을 이해하는 데 중점을 둔 언어학(linguistics) 및 기계 학습(machine learning)의 한 분야입니다. NLP 작업은 단일 단어를 개별적으로 이해하는 것은 물론 해당 단어의 컨텍스트, 즉 주변 문맥도 함께 이해할 수 있도록 하는 것을 목표로 합니다.다음은 일반적인 NLP 태스크 종류와 그 예시들을 보여주고 있습니다.​전체 문장을 분류하기(Classifying whole sentences): 리뷰(review)의 감정(sentiment)을 식별하고, 이메일이 스팸인지 감지하고, 문장이 문법적으로 올바른지 또는 두 문장이 논리적으로 관련되어 있는지 여부를 판단합니다.​단일 문장에서 각 단어를 분류하기(Classifying each word in a sentence): 문장의 문법적 구성요소(명사, 동사, 형용사) 또는 명명된 개체(개체명, e.g., 사람, 위치, 조직) 식별​텍스트 컨텐트 생성하기(Generating text content): 자동 생성된 텍스트로 프롬프트 완성(completing a prompt), 마스킹된 단어(masked words)로 텍스트의 공백 채우기​텍스트에서 정답 추출하기(Extracting an answer from a text): 질문(question)과 맥락(context)이 주어지면, 맥락에서 제공된 정보를 기반으로 질문에 대한 답변을 추출​입력 텍스트에서 새로운 문장을 생성하기(Generating a new sentence from an input text): 텍스트를 다른 언어로 번역(translation), 텍스트 요약(summarization)​NLP는 문어적 텍스트(written text) 처리에 국한되지 않습니다. NLP는 오디오 샘플의 스크립트(transcript) 또는 이미지 설명(image caption) 생성과 같은 음성 인식(speech recognition) 및 컴퓨터 비전(computer vision) 등의 복잡한 문제도 또한 해결합니다.​NLP는 왜 어려운가요?컴퓨터는 인간과 같은 방식으로 정보를 처리하지 않습니다. 예를 들어, 우리가 “배고프다(I am hungry)”라는 문장을 읽으면, 인간은 그 의미를 쉽게 이해할 수 있습니다. 마찬가지로, ""나는 배고프다(I am hungry)""와 ""나는 슬프다(I am sad)""와 같은 두 문장이 주어지면 우리는 두 문장이 얼마나 유사한지를 쉽게 결정할 수 있습니다. 기계 학습(Machine Learning, ML) 모델의 경우 이러한 일을 수행하기가 더 어렵습니다. 모델이 이 작업을 수행하기 위해서는, 텍스트가 모델이 학습할 수 있는 방식으로 처리되어야 합니다. 그리고 언어가 복잡하기 때문에 인간은 이 처리가 어떻게 이루어져야 하는지에 대해 신중하게 생각해야 합니다. 지금까지 텍스트를 표현하는 방법(how to represent text)에 대한 많은 연구가 진행되었으며, 다음 섹션에서 몇 가지 방법을 살펴보겠습니다.​, 🤗Transformers 라이브러리의 첫 번째 도구인 pipeline()​​트랜스포머 모델은 이전 섹션에서 언급한 모든 종류의 NLP 작업을 해결하는 데 사용됩니다. 다음은 Hugging Face 및 트랜스포머 모델을 사용하는 몇몇 회사 및 조직들을 보여주고 있습니다. 이들은 자신들이 만든 모델들을 공유하여 커뮤니티에 다시 기여합니다 🤗Transformers 라이브러리는 공유된 모델을 만들고 사용할 수 있는 다양한 기능들을 제공합니다. Model Hub에는 누구나 다운로드하여 사용할 수 있는 수천 개의 사전 학습된 모델(pretrained models)들이 포함되어 있습니다. 또한 자신의 모델을 허브에 업로드할 수도 있습니다! ⚠️ The Hugging Face Hub는 트랜스포머 모델에만 국한하지 않습니다. 누구나 원하는 모든 종류의 모델이나 데이터셋(datasets)을 공유할 수 있습니다! 사용 가능한 모든 기능을 활용하려면 huggingface.co 계정을 만드세요!트랜스포머 모델이 내부적으로 어떻게 작동하는지 알아보기 전에 몇 가지 흥미로운 NLP 문제를 해결하는 데 사용할 수 있는 몇 가지 예를 살펴보겠습니다.​파이프라인(pipeline) 활용하기🤗Transformers 라이브러리의 가장 기본적인 객체는 pipeline() 함수입니다. 이 함수는 특정 모델과 동작에 필요한 전처리 및 후처리 단계를 연결하여 텍스트를 직접 입력하고 이해하기 쉬운 답변을 얻을 수 있습니다.​from transformers import pipeline classifier = pipeline(""sentiment-analysis"") classifier(""I've been waiting for a HuggingFace course my whole life."") ​여러 개의 문장을 동시에 입력으로 전달할 수도 있습니다!classifier([""I've been waiting for a HuggingFace course my whole life."", ""I hate this so much!""]) ​기본적으로 이 파이프라인은 영어 문장에 대한 감정 분석(sentiment analysis)을 위해 미세 조정된(fine-tuned) 사전 훈련 모델(pretrained model)을 선택합니다. 위 코드에서 classifier 객체를 생성할 때 해당 모델이 다운로드되고 캐시됩니다. 생성된 classifier 객체를 다시 실행하면 캐시된 모델이 대신 사용되며 모델을 다시 다운로드할 필요가 없습니다.파이프라인에 텍스트가 입력되면 3가지 주요 단계가 내부적으로 실행됩니다.텍스트는 모델이 이해할 수 있는 형식으로 전처리됩니다(preprocessing).전처리 완료된 입력 텍스트는 모델에 전달됩니다.모델이 예측한 결과는 후처리되어(postprocessing) 우리가 이해할 수 있는 형태로 변환됩니다.현재 활용 가능한 몇 가지 파이프라인들은 다음과 같습니다:feature-extraction (텍스트에 대한 벡터 표현 제공)fill-maskner (named entity recognition, 개체명 인식)question-answeringsentiment-analysissummarizationtext-generationtranslationzero-shot-classification이 중 몇 가지를 살펴보겠습니다!​Zero-shot 분류​우선, 레이블이 지정되지 않은 텍스트를 분류해야 하는 더 어려운 작업부터 시작하겠습니다. 텍스트에 주석(annotation)을 추가하는 것은 일반적으로 시간이 많이 걸리고 분야 전문 지식(domain expertise)이 필요하기 때문에 이 작업은 실제 프로젝트에서 매우 흔한 시나리오입니다. 이 활용 사례의 경우, zero-shot-classification 파이프라인은 매우 유용합니다. 그 이유는 해당 분류에 사용할 레이블을 직접 마음대로 지정할 수 있으므로 사전 훈련된 모델의 레이블 집합에 의존할 필요가 없기 때문입니다. 이미 우리는 해당 모델이 두 레이블(긍정, 부정)을 사용하여 문장을 긍정 또는 부정으로 분류하는 방법을 이미 보았습니다. 그러나 이 모델을 이용해서 우리가 원하는 또 다른 레이블 집합을 사용하여 텍스트를 분류할 수도 있습니다.​from transformers import pipeline classifier = pipeline(""zero-shot-classification"") classifier( ""This is a course about the Transformers library"", candidate_labels=[""education"", ""politics"", ""business""], )​위와 같이 완전히 다른 새로운 레이블 집합으로 문장 분류를 수행할 때도 새로운 데이터를 이용해서 모델을 미세 조정(fine-tuning)할 필요가 없기 때문에 zero-shot 분류라고 합니다. 위 예시에서 보는 바와 같이 원하는 레이블 목록에 대한 확률 점수를 직접 반환할 수도 있습니다!​텍스트 생성이제 파이프라인을 사용하여 텍스트를 생성하는 방법을 살펴보겠습니다. 여기서 주요 아이디어는 입력으로 특정 프롬프트(prompt)를 제공하면 모델이 나머지 텍스트를 생성하여 프롬프트를 자동 완성한다는 것입니다. 이것은 전화기에서 볼 수 있는 텍스트 예측 기능(predictive text feature)과 유사합니다. 텍스트 생성은 랜덤하게 수행되므로 여러분의 출력이 아래 결과와 다르게 나오는 것은 정상입니다.from transformers import pipeline generator = pipeline(""text-generation"") generator(""In this course, we will teach you how to"") 위의 generator 객체에 num_return_sequences 인자(argument)를 지정하여 생성되는 시퀀스의 개수를 정할 수 있고, max_length 인자로는 출력 텍스트의 총 길이를 제어할 수 있습니다.파이프라인에서 허브의 모든 모델을 사용할 수 있다!이전 예제에서는 현재 작업에 대한 기본적인 모델(default model)을 사용했지만 허브에서 여러분이 원하는 모델을 선택하여 특정 작업(예: 텍스트 생성)에 대한 파이프라인에서 사용할 수도 있습니다. 모델 허브(Model Hub)로 이동하여 왼쪽에 있는 특정 태그를 클릭하면 관련 작업을 지원하는 모델만 표시됩니다. 그 결과는 이 페이지에서 볼 수 있습니다.자 이제 distilgpt2 모델을 사용해 봅시다! 파이프라인에서 이 모델을 로드하는 방법은 다음과 같습니다.from transformers import pipeline generator = pipeline(""text-generation"", model=""distilgpt2"") # distilgpt2 모델을 로드한다. generator( ""In this course, we will teach you how to"", max_length=30, num_return_sequences=2, ) 언어 태그(language tags)를 클릭하여 그 언어에 특화된 모델을 세부적으로 검색하고 선택함으로써 원하는 언어로 표현된 텍스트를 생성할 수 있는 모델을 사용할 수 있습니다. Model Hub에는 다중 언어를 지원하는 다국어 모델(multilingual models)에 대한 체크포인트도 포함되어 있습니다.특정 모델을 클릭하여 선택하면 온라인에서 직접 테스트할 수 있는 위젯(widget)이 표시됩니다. 이렇게 하면 다운로드하기 전에 그 모델의 기능을 빠르게 테스트할 수 있습니다.추론 API모든 모델은 Hugging Face 웹사이트에서 제공되는 Inference API를 사용하여 브라우저를 통해 직접 테스트할 수 있습니다. 이 페이지에서 직접 임의의 텍스트를 입력하고 모델이 해당 텍스트를 처리하는 것을 살펴보면서 모델들을 테스트해 볼 수 있습니다.위젯을 구동하는 Inference API는 유료 제품으로도 제공되므로 실무적으로도 편리하게 사용 가능합니다. 자세한 내용은 가격 책정 페이지를 참조하세요.Mask filling다음으로 시도할 파이프라인은 fill-mask 입니다. 이 작업은 주어진 텍스트의 공백을 채우는 것입니다.from transformers import pipeline unmasker = pipeline(""fill-mask"") unmasker(""This course will teach you all about <mask> models."", top_k=2) top_k 인자(argument)는 출력할 공백 채우기 종류의 개수를 지정합니다. 여기서 모델은 마스크 토큰(mask token) 이라고 부르는 특수한 <mask> 단어를 채웁니다. 마스크 채우기(mask-filling) 모델에 따라 서로 다른 마스크 토큰을 요구할 수 있으므로 다른 모델을 탐색할 때 항상 해당 마스크 토큰을 확인하는 것이 좋습니다. 이를 확인하는 한 가지 방법은 위젯에 사용된 마스크 토큰을 살펴보는 것입니다.개체명 인식(Named entity recognition)개체명 인식(NER, Named Entity Recognition)은 입력 텍스트에서 어느 부분이 사람, 위치 또는 조직과 같은 개체명에 해당하는지 식별하는 작업입니다. 예를 살펴보겠습니다.from transformers import pipeline ner = pipeline(""ner"", grouped_entities=True) ner(""My name is Sylvain and I work at Hugging Face in Brooklyn."") 여기서 모델은 ""Sylvain""이 사람(PER)이고 ""Hugging Face""가 조직(ORG)이며 ""Brooklyn""이 위치(LOC)임을 올바르게 식별했습니다.파이프라인 생성 함수에서 grouped_entities=True 옵션을 전달하여 파이프라인이 동일한 엔티티에 해당하는 문장의 부분(토큰 혹은 단어)들을 그룹화하도록 지시합니다. 여기서 모델은 ""Hugging""과 ""Face""를 단일 조직(ORG)으로 올바르게 그룹화했지만 이름 자체는 여러 단어로 구성됩니다. 사실, 다음 장에서 보게 되겠지만, 전처리 과정에서 심지어 일부 단어를 더 작은 부분으로 나눌 수도 있습니다. 예를 들어 Sylvain은 S, ##yl, ##va 및 ##in의 네 부분으로 나뉩니다. 후처리 단계에서 파이프라인은 해당 조각을 성공적으로 재그룹화했고, 그 결과로 ""Sylvain""이 단일 단어로 출력되고 있습니다.질의 응답(Question Answering)question-answering 파이프라인은 주어진 컨텍스트(context) 정보를 사용하여 입력 질문에 응답을 제공합니다.from transformers import pipeline question_answerer = pipeline(""question-answering"") question_answerer( question=""Where do I work?"", context=""My name is Sylvain and I work at Hugging Face in Brooklyn"", ) 이 파이프라인은 제공된 컨텍스트에서 정보를 추출하여 작동합니다. 응답을 새롭게 생성하지는 않습니다.자동 요약(Summarization)요약(summarization)은 텍스트에 존재하는 모든(또는 대부분의) 중요한 내용을 유지하면서 해당 텍스트를 더 짧은 텍스트로 줄이는 작업입니다. 예를 살펴보겠습니다.from transformers import pipeline summarizer = pipeline(""summarization"") summarizer( """""" America has changed dramatically during recent years. Not only has the number of graduates in traditional engineering disciplines such as mechanical, civil, electrical, chemical, and aeronautical engineering declined, but in most of the premier American universities engineering curricula now concentrate on and encourage largely the study of engineering science. As a result, there are declining offerings in engineering subjects dealing with infrastructure, the environment, and related issues, and greater concentration on high technology subjects, largely supporting increasingly complex scientific developments. While the latter is important, it should not be at the expense of more traditional engineering. Rapidly developing economies such as China and India, as well as other industrial countries in Europe and Asia, continue to encourage and advance the teaching of engineering. Both China and India, respectively, graduate six and eight times as many traditional engineers as does the United States. Other industrial countries at minimum maintain their output, while America suffers an increasingly serious decline in the number of engineering graduates and a lack of well-educated engineers. """""" ) 텍스트 생성과 마찬가지로 옵션으로 결과에 대해 max_length 또는 min_length 지정이 가능합니다.기계 번역(Translation)번역(Translation)의 경우 작업(task) 이름에 언어 쌍(예: ""translation_en_to_fr"")을 지정하면 시스템에서 기본적으로 제공하는 모델(default model)을 사용할 수 있지만 가장 쉬운 방법은 Model Hub에서 사용하려는 모델을 선택하는 것입니다. 아래 예시에서 프랑스어에서 영어로 번역을 시도해 보겠습니다.from transformers import pipeline translator = pipeline(""translation"", model=""Helsinki-NLP/opus-mt-fr-en"") translator(""Ce cours est produit par Hugging Face."") 텍스트 생성 및 요약과 마찬가지로 옵션으로 결과에 대해 max_length 또는 min_length 지정이 가능합니다.지금까지 살펴본 파이프라인은 대부분 데모용입니다. 특정 작업(specific tasks)을 위해 프로그래밍되었으며 변형된 작업이나 더 복잡한 작업은 수행할 수 없습니다. 다음 섹션에서는 pipeline() 함수 내부에 어떠한 기능 및 동작들이 존재하고 그것들을 어떻게 변경할 수 있는지에 대해서 알아보겠습니다.​3. 🤗Transformers는 어떻게 동작하는가?  이 섹션에서는 Transformer 모델의 아키텍처를 개략적으로 살펴보겠습니다.Transformer의 짧은 역사다음은 Transformer 모델의 (짧은) 역사에서 참조할 수 있는 몇 가지 주요 모델들입니다. Transformer 아키텍처는 2017년 6월에 소개되었습니다. 원래 이 아키텍처 연구의 초점은 기계번역(machine translation)이었습니다. 최초 모델이 소개된 후 다음과 같은 몇 가지 강력하고 우수한 모델들이 추가적으로 도입되었습니다.2018년 6월: GPT, 최초의 사전 학습된(pretrained) Transformer 모델. 다양한 NLP 작업에 대한 미세 조정에 사용되었고 그 당시 많은 태스크에서 최고 성능을 달성2018년 10월: BERT, 또 다른 대규모 사전 학습된 모델. 이 모델은 특히 고수준의 문장 요약을 제공하도록 설계되었습니다(다음 장에서 더 자세히 설명합니다!)2019년 2월: GPT-2, 윤리적인 문제로 인해 즉시 공개되지 않은, 기존 GPT보다 규모가 더 크고 성능이 향상된 GPT 버전2019년 10월: DistillBERT, 속도가 60% 더 빠르고 메모리 소비는 40% 줄였지만 여전히 BERT 성능의 97%를 유지하는 증류된(distilled) BERT 버전2019년 10월: BART 및 T5, 원래 Transformer 모델과 동일한 아키텍처를 사용하는 두 개의 대규모 사전 학습 모델(순수 Transformer 아키텍처를 사용한 최초의 사전 학습 모델)2020년 5월: GPT-3, 미세 조정 없이도 다양한 작업을 훌륭하게 수행할 수 있는 GPT-2의 더 큰 버전으로 제로샷 학습(zero-shot learning)이라고 함그 외에도 다양한 사전 학습 모델이 존재하며, 이 목록은 단지 몇 가지 다른 종류의 Transformer 모델을 강조하여 설명하기 위한 것입니다. 사전 학습 모델은 대체로 다음 세 가지 범주로 그룹화할 수 있습니다:GPT-like 모델(auto-regressive Transformer 모델)BERT-like 모델(auto-encoding Transformer 모델)BART/T5-like 모델(__sequence-to_sequence __Transformer 모델)나중에 각각의 그룹별로 좀 더 상세하게 알아볼 것입니다.Transformers는 언어 모델입니다.위에서 언급한 모든 Transformer 모델(GPT, BERT, BART, T5 등)은 언어 모델(language model) 로 학습되었습니다. 이는 그 모델들이 자가 지도(self-supervised) 학습 방식으로 많은 양의 원시 텍스트에 대해 학습되었음을 의미합니다. 자가 지도 학습(self-supervised learning)은 목적 함수(objectives)가 모델의 입력에서 자동으로 계산되는 학습 유형입니다. 즉, 사람이 데이터에 레이블을 지정할 필요가 없습니다!이러한 유형의 모델은 학습된 언어에 대한 통계적인 이해를 가능하게 하지만 실제 태스크에는 그다지 유용하지 않습니다. 이런 이유때문에 일반적으로 사전 학습된 모델은 전이 학습(transfer learning) 이라는 프로세스를 거치게 됩니다. 이 프로세스 동안 모델은 주어진 작업에 대해 감독(supervised) 방식 즉, 사람이 주석으로 추가한 레이블을 사용함으로써 미세 조정(fine-tuning)됩니다.예를 들어, n개의 이전 단어를 읽은 문장에서 다음 단어를 예측한다고 합시다. 여기서 출력할 예측값은 과거 및 현재 입력값에 의존하지만 미래 입력값에는 의존하지 않기 때문에 우리는 이것을 causal language modeling 이라고 합니다. 또 다른 예는 모델이 문장에서 마스크된 단어(masked word)를 예측하는 masked language modeling 입니다. Transformers는 규모가 큰 모델입니다.몇 가지 예외 모델(예: DistilBERT)을 제외하고, 더 나은 성능을 달성하기 위한 일반적인 전략은 모델의 크기와 사전 훈련된 데이터의 양을 늘리는 것입니다. 불행히도 사전 학습 모델(pretrained model), 특히 큰 모델을 학습하려면 많은 양의 데이터가 필요합니다. 이는 시간과 컴퓨팅 리소스 면에서 고비용을 초래합니다. 심지어 이러한 대규모 모델 학습은 다음 그래프에서 볼 수 있듯이 환경적인 문제(environmental impact)를 야기하기도 합니다. 위 그래프의 맨 아래에서 사전 학습의 환경적 영향을 줄이기 위해 의식적으로 노력하고 있는 한 연구팀에 의해서 진행되는 프로젝트에서 대규모의 사전 학습을 실행할 때 배출되는 이산화탄소의 양을 보여주고 있습니다. 최적의 하이퍼 파라미터(hyperparameter)를 얻기 위해 많은 학습 시도를 실행해야 하는 경우 탄소 발자국(carbon footprint)은 훨씬 더 높을 것입니다.모든 연구 팀, 학생 조직 또는 회사가 자신들의 모델을 직접 처음부터 사전 학습한다고 생각해보십시오. 이는 광범위하고 불필요한 글로벌 비용으로 이어질 것입니다!이것이 언어 모델을 공유해야만 하는 가장 중요한 이유입니다. 학습된 가중치(weights)를 공유하고 이미 학습된 가중치를 기반으로 미세 조정(fine-tuning)하여 모델을 만들면 커뮤니티의 전체 컴퓨팅 비용과 탄소 발자국(carbon footprint)을 줄일 수 있습니다.전이 학습(Transfer Learning)사전 학습(Pretraining)은 모델을 처음부터 학습하는 작업입니다. 모델의 가중치(weight)는 무작위로 초기화되고, 사전 지식(prior knowledge)이 없이 학습이 시작됩니다. 이 사전 학습(pretraining)은 일반적으로 매우 많은 양의 데이터가 필요합니다. 따라서 매우 큰 규모의 데이터 코퍼스(corpus)가 필요하며 학습에는 최대 몇 주가 소요될 수 있습니다.반면에, 미세 조정(fine-tuning)은 모델이 사전 학습된 후에 수행되는 학습입니다. 미세 조정(fine-tuning)을 수행하려면 먼저 사전 학습된 언어 모델(pretrained language model)을 확보한 다음, 특정 태스크에 맞는 데이터셋(dataset)을 사용하여 추가 학습을 수행합니다. 잠깐만요! 최종 태스크(task)를 위해 처음부터 직접 학습하지 않는 이유는 무엇일까요? 몇 가지 이유가 있습니다.사전 학습된 모델(pretrained model)은 미세 조정(fine-tuning)에 사용할 데이터셋과 유사한 데이터를 바탕으로 이미 학습되었습니다. 따라서, 미세 조정(fine-tuning) 과정에서, 사전 학습 과정에서 얻은 지식을 활용할 수 있습니다. (예를 들어, NLP 문제의 경우 사전 학습된 모델은 원하는 태스크에 사용하는 언어에 대한 일종의 통계적 이해를 갖게 됩니다.)사전 학습된 모델은 이미 많은 데이터에 대해 학습되었기 때문에 미세 조정 과정에서 훨씬 적은 데이터셋을 이용하더라도 좋은 결과를 얻을 수 있습니다.같은 이유로, 좋은 결과를 얻는 데 필요한 시간과 자원은 훨씬 적을 수 있습니다.예를 들어, 영어로 사전 학습된 모델을 가지고, arXiv 말뭉치에서 미세 조정(fine-tuning)하여 과학/연구(science/research) 분야 데이터에 특화된 모델을 만들 수 있습니다. 미세 조정(fine-tuning)에는 제한된 양의 데이터만 필요합니다. 사전 학습된 모델이 획득한 지식은 ""전이(transferred)""되므로 전이 학습(transfer learning) 이라는 용어를 사용합니다. 따라서 모델을 미세 조정(fine-tuning)하면 시간(time), 데이터(data), 재정(financial) 및 환경(environmental) 비용이 절감됩니다. 또한 학습 과정이 전체 사전 훈련(pretraining)보다 제약이 적기 때문에, 보다 쉽고 빠르게 다양한 미세 조정 작업을 반복할 수 있습니다.또한, 만일 본인이 원하는 최종 태스크에 필요한 데이터가 충분치 않을 경우, 미세 조정(fine-tuning) 과정에서 처음부터 학습하는 방법보다 더 나은 결과를 얻을 수 있습니다. 따라서 항상 사전 학습된 모델을 활용하고 이를 미세 조정하려고 노력해야 합니다.일반적인 아키텍처여기서는 Transformer 모델의 일반적인 아키텍처를 살펴보겠습니다. 일부 개념을 이해하지 못하더라도 걱정하지 마십시오. 나중에 각 구성 요소를 자세히 다루는 섹션(section)이 있습니다.개요 (Introduction)모델은 주로 두 개의 블록으로 구성됩니다.인코더(Encoder) (왼쪽): 인코더는 입력에 대한 표현(representation) 혹은 자질(feature)을 도출합니다. 이는 모델이 입력으로부터 이해를 얻도록(acquire understanding from the input), 다시 말해서, 최종 목적 태스크를 위해서 입력에 대한 표현 형태가 최적화되었음을 의미합니다.디코더(Decoder) (오른쪽): 디코더는 인코더가 구성한 표현(representation) 혹은 자질(feature)을 다른 입력과 함께 사용하여 대상 시퀀스를 생성합니다. 이는 모델이 출력 생성(generating outputs)에 최적화되어 있음을 의미합니다. 이들 각각의 블록은 작업의 종류에 따라 개별적으로 사용할 수도 있습니다.인코더 전용 모델(Encoder-only models): 문장 분류(sentence classification) 및 개체명 인식(named-entity recognition)과 같이 입력에 대한 분석 및 이해(understanding)가 필요한 태스크에 적합합니다.디코더 전용 모델(Decoder-only models): 텍스트 생성(text generation) 등과 같은 생성 태스크(generative tasks)에 좋습니다.인코더-디코더 모델(Encoder-Decoder models) 혹은 시퀀스-투-시퀀스 모델(sequence-to-sequence model): 번역(translation)이나 요약(summarization)과 같이 입력이 수반되는 생성 태스크(generative tasks)에 적합합니다.이러한 아키텍처는 이후 섹션에서 개별적으로 자세히 살펴보겠습니다.어텐션 계층 (Attention layers)Transformer 모델의 가장 중요한 특징은 어텐션 레이어(attention layers) 라는 특수 레이어로 구축된다는 것입니다. 사실, Transformer 아키텍처를 소개하는 논문의 제목이 ""Attention Is All You Need""였습니다! 이 강좌의 뒷부분에서 어텐션 레이어(attention layer)에 대한 상세한 내용을 살펴볼 것입니다. 그러나 현재는 이 레이어가 각 단어의 표현을 처리할 때, 문장의 특정 단어들에 특별한 주의를 기울이고 나머지는 거의 무시하도록 모델에 지시할 것 이라는 점만 알면 됩니다.이것을 상황에 맞게 설명하기 위해서, 영어로 된 텍스트를 프랑스어로 번역하는 작업을 고려해 봅시다. ""You like this course""라는 입력이 주어지면 번역 모델은 ""like""라는 단어에 대한 적절한 번역을 얻기 위해 인접 단어 ""You""에도 주의를 기울여야 합니다. 왜냐하면 프랑스어에서 동사 ""like""는 주어(subject)에 따라 다르게 활용되기 때문입니다. 그러나 문장의 나머지 부분(""this course"")은 해당 단어(""like"")의 번역에 그다지 유용하지 않습니다. 같은 맥락에서, ""this""를 번역할 때, 모델은 ""course""라는 단어에도 주의를 기울여야 합니다. ""this""는 연결된 명사가 남성(masculine)인지 여성(feminine)인지에 따라 다르게 번역되기 때문입니다. 위와 마찬가지로, 문장의 다른 단어들(""You"", ""like"")은 ""this""의 번역에 중요하지 않습니다. 더 복잡한 문장이나 더 복잡한 문법 규칙의 경우, 모델은 개별 단어를 적절하게 번역하기 위해 문장에서 해당 단어와 멀리 떨어진 단어에도 특별한 주의를 기울여야 합니다.동일한 개념이 자연어와 관련된 모든 태스크에 적용될 수 있습니다. 단어 자체가 고유한 의미를 가지고 있지만 그 의미는 주변 문맥, 즉 컨텍스트(context)에 의해 크게 영향을 받으며, 컨텍스트는 처리 중인 단어 앞이나 뒤에 존재하는 다른 단어들을 포함할 수 있습니다.어텐션 레이어(attention layer)가 무엇인지 이해했으므로 이제 Transformer 아키텍처를 자세히 살펴보겠습니다.오리지널 아키텍처Transformer 아키텍처는 원래 번역용으로 설계되었습니다. 학습이 진행되는 동안 인코더(encoder)는 특정 언어로 표기된 입력(문장)을 수신하고 디코더(decoder)는 원하는 대상 언어로 표기된 동일한 의미의 문장을 수신합니다. 인코더에서 어텐션 레이어(attention layer)는 문장의 모든 단어에 주의(attention)를 기울여야 합니다. 왜냐하면, 우리가 방금 보았듯이 현재 단어에 대한 번역 결과는 문장에서 해당 단어의 앞부분과 뒷부분의 내용에 따라 달라질 수 있기 때문입니다. 그러나 디코더는 순차적으로 작동하며 이미 번역된 문장의 단어들에만, 즉 현재 생성되고 있는 단어 앞의 단어들에만 주의(attention)를 기울일 수 있습니다. 예를 들어, 번역 대상(target sentence)의 처음 세 단어를 예측한 경우, 디코더에 이를 입력한 다음 인코더의 모든 입력(원본 문장의 모든 단어들)을 사용하여 네 번째 단어를 예측하려고 시도합니다.학습 도중 속도를 높이기 위해, 디코더(decoder)는 전체 대상 문장(target sentences)을 입력으로 받지만, 이 중에서 미래 단어(현재 디코딩 대상 단어의 이후에 나타나는 단어들)를 사용하는 것은 허용되지 않습니다. 두 번째 위치에 나타나는 단어를 예측하려고 할 때, 두 번째 위치의 정답 단어를 바로 접근할 수 있다면 학습이 제대로 진행되지 않을 것입니다. 예를 들어, 네 번째 단어를 예측하려고 할 때 어텐션 계층은 첫 번째에서 세번째 까지의 단어들에만 주의를 집중할 수 있습니다.원래 Transformer 아키텍처는 왼쪽에 인코더가 있고 오른쪽에 디코더가 있는 형태로 다음 그림과 같습니다: 디코더(decoder) 블록의 첫 번째 어텐션 계층(attention layer)은 디코더(decoder)에 대한 모든 (과거) 입력에 주의를 집중하지만, 두 번째 어텐션 계층은 인코더의 출력을 입력으로 받아서 사용하고 있음을 알 수 있습니다. 따라서 현재 단어를 가장 잘 예측하기 위해 전체 입력 문장(input/source sentence)에 액세스할 수 있습니다. 이것은 대상 언어(target language)가 원본 언어(source language)와 비교하여 상당히 다른 단어 순서(words in different orders)로 문장을 표현하는 문법 규칙(grammatical rule)을 가지거나, 원본 문장(input/source sentence)의 뒷부분에 나타난 컨텍스트(context)가 현재 단어에 대한 최상의 번역을 결정하는 데 도움이 될 수 있는 경우 매우 유용합니다.어텐션 마스크(Attention mask) 는 인코더/디코더에서 모델이 특정 단어에 주의를 집중하는 것을 방지하는 데 사용할 수 있습니다. 예를 들어, 문장을 일괄 처리(batching)할 때 모든 입력을 동일한 길이로 만들기 위해서 사용되는 특수 패딩 단어(padding word)에 적용할 수 있습니다.아키텍처(architectures) vs. 체크포인트(checkpoints)이 강좌에서 향후 Transformer 모델에 대해 공부하면서 아키텍처(architecture) 와 체크포인트(checkpoint), 그리고 모델(model) 이라는 용어를 자주 접하게 될 것입니다. 이 용어들은 각각 서로 다른 다른 의미를 가집니다.아키텍처(Architectures): 이 용어는 모델의 뼈대(skeleton)를 의미합니다. 모델 내에서 발생하는 각 레이어(layer)와 오퍼레이션(operation, 연산) 등을 정의합니다.체크포인트(Checkpoints): 해당 아키텍처에서 로드될 가중치 값들을 나타냅니다.모델(Model): 이것은 ""아키텍처(architecture)"" 또는 ""체크포인트(checkpoint)"" 보다는 덜 명확한 포괄적인 용어(umbrella term)입니다. 두 가지 모두를 의미할 수도 있습니다. 본 강좌에서는 표기의 명확성이 필요할 경우 모델이라는 용어보다는 아키텍처(architecture) 또는 체크포인트(checkpoint)를 주로 사용할 것입니다.예를 들어, BERT는 아키텍처(architecture)이고 BERT의 첫 번째 릴리스를 위해 Google 팀에서 학습한 가중치 세트(set of weights)인 bert-base-cased 는 체크포인트(checkpoint)입니다. 하지만, ""BERT 모델(BERT model)""과 ""bert-base-cased""도 모델이라고 말할 수도 있습니다.​​​4. 인코더 모델 (Encoder Models)  인코더 모델(encoder models)은 Transformers 모델의 인코더 모듈만 사용합니다. 각 단계에서 어텐션 계층(attention layer)은 초기/원본 입력 문장(initial sentence)의 모든 단어에 액세스할 수 있습니다. 이러한 모델은 종종 ""양방향(bi-directional)"" 주의 집중(attention)을 수행하는 것이 특징이며, auto-encoding model 이라고 부르기도 합니다.이러한 모델의 사전 학습(pretraining) 과정에서 일반적으로 주어진 초기 문장을 다양한 방법을 사용하여 손상시키고(예: 임의의 단어를 masking 하여), 손상시킨 문장을 다시 원래 문장으로 복원하는 과정을 통해서 모델 학습이 진행됩니다.인코더 모델(encoder models)은 문장 분류(sentence classification), 개체명 인식(named-entity recognition), 혹은 더 일반적으로 단어 분류(word classification) 및 추출형 질의응답(extractive question answering) 등과 같이 전체 문장에 대한 이해가 필요한 작업(task)에 가장 적합합니다.대표적으로 다음과 같은 모델이 있습니다:ALBERTBERTDistilBERTELECTRARoBERTa​​5. 디코더 모델 (Decoder Models)  디코더 모델(decoder models)은 Transformer 모델의 디코더 모듈만 사용합니다. 각 단계에서 주어진 단어에 대해 어텐션 레이어(attention layer)는 문장에서 현재 처리 단어 앞쪽에 위치한 단어들에만 액세스할 수 있습니다. 이러한 모델을 일반적으로 자동 회귀 모델(auto-regressive models) 이라고 합니다.디코더 모델(decoder models)의 사전 학습(pretraining)은 일반적으로 문장의 다음 단어 예측을 수행함으로써 이루어집니다.이러한 모델은 텍스트 생성(text generation)과 관련된 작업(task)에 가장 적합합니다.대표적인 모델로는 다음과 같습니다:CTRLGPTGPT-2Transformer XL​6. 인코더-디코더 모델 (Sequence-to-sequence models)  인코더-디코더 모델(sequence-to-sequence 모델이라고도 함)은 Transformer 아키텍처의 두 부분, 즉 인코더(encoder)와 디코더(decoder) 모두를 사용합니다. 각 단계에서 어텐션 계층(attention layer)은 초기/원본 입력 문장(initial sentence)의 모든 단어에 액세스할 수 있는 반면, 디코더의 어텐션 레이어(attention layer)는 문장에서 현재 처리 단어 앞쪽에 위치한 단어들에만 액세스할 수 있습니다.이러한 모델의 사전 학습(pretraining)은 인코더 또는 디코더 모델의 목적 함수(objectives)를 사용하여 수행될 수 있지만 일반적으로 약간 더 복잡한 처리 과정이 수반됩니다. 예를 들어, T5는 임의의 텍스트 일부분(text span, 여러 단어를 포함할 수 있음)을 하나의 마스크 특수 단어(mask special word)로 대체하여 사전 학습되며,학습 목표(objective)는 이 마스크 단어가 대체할 텍스트를 예측하는 것입니다.Sequence-to-sequence 모델은 요약(summarization), 번역(translation) 또는 생성형 질의응답(generative question answering) 등과 같이 주어진 입력에 따라 새로운 문장을 생성하는 작업(task)에 가장 적합합니다.대표적으로 다음과 같은 모델이 있습니다:BARTmBARTMarianT5​​​7. 편견과 한계 (Bias and Limitations)  만일 사전 학습된 모델이나 미세 조정된 모델을 상용 시스템에서 사용하려는 경우, 제약 사항(limitations)이 존재한다는 것을 유의해야 합니다. 그 중에서 가장 크게 이슈가 될 수 있는 사실은 대용량 데이터에 대해서 사전 학습을 수행하기 위해, 인터넷에 존재하는 좋은 데이터는 물론 최악의 데이터들도 무조건 수집하여 활용한다는 것입니다.이에 대한 빠른 설명을 위해 BERT 모델을 활용한 fill-mask 파이프라인의 예를 다시 살펴보겠습니다.from transformers import pipeline unmasker = pipeline(""fill-mask"", model=""bert-base-uncased"") result = unmasker(""This man works as a [MASK]."") print([r[""token_str""] for r in result]) result = unmasker(""This woman works as a [MASK]."") print([r[""token_str""] for r in result]) 위 두 문장에서 누락된 단어를 채우라는 요청을 받았을 때 모델은 성별과 상관없는(gender-free) 대답(waiter/waitress)은 오직 하나만 제공합니다. 나머지는 일반적으로 특정 성별과 밀접한 관련이 있는 직업들입니다. 그리고 매춘부(prostitute)는 모델이 ""여성"" 및 ""직업""과 연관되는 상위 5개 단어에 속해 있습니다. 이러한 현상은 BERT가 인터넷 전체에서 데이터를 수집하여 학습된 것이 아니라, 오히려 중립적인 데이터 즉, English Wikipedia 와 BookCorpus를 사용하여 학습된 드문 Transformer 모델 중 하나임에도 불구하고 발생합니다.​​요약 (Summary)  이 장에서는 🤗Transformers의 고수준(high-level) pipeline() 함수를 사용하여 다양한 NLP 작업을 수행하는 방법을 살펴보았습니다. 또한 허브에서 모델을 검색하고 사용하는 방법과 Inference API를 사용하여 브라우저에서 직접 모델을 테스트하는 방법을 알아봤습니다.우리는 또한 Transformer 모델이 고수준(high-level)에서 어떻게 작동하는지 논의하고 전이 학습(transfer learning)과 미세 조정(fine-tuning)의 중요성에 대해 이야기했습니다. 가장 중요한 핵심은 대상 작업의 종류에 따라 전체 아키텍처(full architecture)를 사용하거나 인코더(encoder) 또는 디코더(decoder)만 사용할 수 있다는 것입니다. 다음 표는 이를 요약한 것입니다. 모델예시태스크(Tasks)인코더(Encoder)ALBERT, BERT, DistilBERT, ELECTRA, RoBERTa문장분류(sentence classification), 개체명인식(named-entity recognition), 추출형 질의응답(extractive question answering)디코더(Decoder)CTRL, GPT, GPT-2, Transformer-XL텍스트 생성(Text generation)인코더-디코더(Encoder-Decoder)BART, T5, Marian, mBART요약(summarization), 번역(translation), 생성형 질의응답(generative question answering) ​​​​​​ "
What is Machine Learning? (기계학습이란 무엇인가?) ,https://blog.naver.com/mikangel/221810685846,20200216,"  A machine learning algorithm is a search process designed to choose the best function, from a set of possible functions, to explain the relationships between features in a dataset. To get an intuitive understanding of what is involved in extracting, or learning, a function from data, examine the following set of sample inputs to an unknown function and the outputs it returns.Given these examples, decide which arithmetic operation (addition, subtraction, multiplication, or division) is the best choice to explain the mapping the unknown function defines between its inputs and outputs: 기계 학습 알고리즘은 데이터 집합의 기능 간의 관계를 설명하기 위해 가능한 함수 집합에서 최상의 함수를 선택하도록 설계된 검색 프로세스입니다. 데이터에서 함수를 추출하거나 학습하는 데 관련된 내용을 직관적으로 이해하려면 다음 샘플 입력 집합을 알 수 없는 함수와 반환하는 출력에 대해 검사합니다.이러한 예제를 감안할 때, 알 수 없는 함수가 입력과 출력 간에 정의하는 매핑을 설명하는 가장 좋은 선택인 산술 연산(추가, 빼기, 곱셈 또는 나눗셈)을 결정합니다.​Function(Inputs) = OutputFunction(5, 5) = 25Function(2,6) = 12Function(2, 4) = 16Function(2, 2) = 04 Most people would agree that multiplication is the best choice because it provides the best match to the observed relationship, or mapping, from the inputs to the outputs:In this particular instance, choosing the best function it relatively straightforward, and a human can do it without the aid of a computer. However, as the number of input to the unknown function increase (perhaps to hundreds or thousands of inputs), and the variety of potential functions to be considered gets larger, the task becomes much more difficult. It is in these contexts that harnessing the power of machine learning to search for the best function, to match the patterns in the dataset, becomes necessary.​대부분의 사람들은 입력에서 출력에 이르기까지 관찰된 관계 또는 매핑과 가장 잘 일치하기 때문에 곱셈이 최선의 선택이라는 데 동의할 것입니다.이 특정 예시에서, 상대적으로 간단 하는 최고의 기능을 선택 하고, 인간이 컴퓨터의 도움 없이  할 수 있습니다. 그러나 알 수 없는 함수에 대한 입력 수가 수백 또는 수천 개의 입력으로 증가하고 고려할 수 있는 다양한 잠재적 함수가 커짐에 따라 작업이 훨씬 더 어려워집니다. 이러한 맥락에서 머신러닝의 힘을 활용하여 최상의 기능을 검색하고 데이터 집합의 패턴과 일치시키는 것이 필요합니다.​Machine learning involves a two-step process: training and inference. During training, a machine learning algorithm processes a dataset and chooses the function that best matches the patterns in the data. The extracted function will be encoded in a computer program in a particular form (such as if-the-else rules or parameters of a specified equation). The encoded function is known as a model, and the analysis of the data in order to extract the function is often referred to as training the model. ​ 기계 학습에는 교육과 추론이라는 두 단계의 프로세스가 포함됩니다. 학습 중에 기계 학습 알고리즘은 데이터 집합을 처리하고 데이터의 패턴과 가장 일치하는 함수를 선택합니다. 추출된 함수는 특정 형식의 컴퓨터 프로그램에서 인코딩됩니다(예: if-the-else 규칙 또는 지정된 방정식의 매개 변수). 인코딩된 함수는 모델로 알려져 있으며, 함수를 추출하기 위해 데이터의 분석은 종종 모델을 학습시키는 것이라고 합니다.​Essentially, models are functions encoded as computer programs. However, in machine learning the concepts of function and model are so closely related that the distinction if often skipped over and the terms may even be used inIn the context of deep learning, the relationship between functions and models is that the function extracted from a dataset during training is represented as a neural network model, and conversely a neural network model encodes a function a computer program. The standard process used to train a neural network is to begin training with a neural network where the parameters of the network are randomly initialized (we will explain network parameters later; for now just think of them as values that control how the function the network encodes works).​기본적으로 모델은 컴퓨터 프로그램으로 인코딩된 기능입니다. 그러나 기계 학습에서 기능 및 모델의 개념은 매우 밀접하게 관련되어 있기 때문에 종종 건너 뛰고 용어가 사용될 수 있습니다.​딥 러닝의 맥락에서 함수와 모델 간의 관계는 학습 중에 데이터 집합에서 추출된 함수가 신경망 모델로 표시되고 반대로 신경망 모델이 컴퓨터 프로그램을 인코딩한다는 것입니다. 신경망을 학습하는 데 사용되는 표준 프로세스는 네트워크의 매개 변수가 임의로 초기화되는 신경망으로 훈련을 시작하는 것입니다(나중에 네트워크 매개 변수를 설명할 것입니다.  지금은  네트워크 인코딩 기능을 제어하는 값으로 만 생각합시다).​This randomly initialized network will be very inaccurate in terms of its ability to match the relationship between the various input values and target outputs for the examples in the dataset. The training process then proceeds by iterating through the examples in the dataset, and for each example, presenting the input values to the network and then using the difference between the output returned by the network and the correct output for the example listed in the dataset to update the network’s parameters so that it matches the data more closely. Once the machine learning algorithms has found a function that is sufficiently accurate (in terms of the outputs it generates matching the correct outputs listed in the dataset) for the problem we are trying to solve, the training process is completed, and the final models is returned by the algorithms. This is the point at which the learning in machine learning stops.​이 임의로 초기화된 네트워크는 데이터 집합의 예제에 대한 다양한 입력 값과 대상 출력 간의 관계를 일치시키는 기능면에서 매우 부정확합니다. 그런 다음 학습 프로세스는 데이터 집합의 예제를 반복하고 각 예제에 대해 입력 값을 네트워크에 표시한 다음 네트워크에서 반환되는 출력과 나열된 예제에 대한 올바른 출력 간의 차이를 사용하여 진행됩니다. 데이터 집합에서 네트워크의 매개 변수를 업데이트하여 데이터를 보다 밀접하게 일치시도록 합니다. 기계 학습 알고리즘이 충분히 정확한 함수를 발견하면(데이터 집합에 나열된 올바른 출력과 일치하는 출력을 생성하는 출력의 측면에서) 해결하려는 문제에 대해 교육 프로세스가 완료되고 최종 모델은 알고리즘에 의해 반환됩니다. 이것이 기계 학습의 학습이 중지되는 지점입니다.​Once training has finished, the model is fixed, The second stage in machine learning is inference. This is when the model is applied to new examples-examples for which we do not know the correct output values, and therefore we want the model to generate estimates of this value for us. Most of the works in machine learning is focused on how to train accurate models (i.e., extracting an accurate function from data). This is because the skills and methods required to deploy a trained machine learning model into production, in order to do inference on new examples at scale, are different from those that a typical data scientist will possess. There is a growing recognition within the industry of the distinctive skills needed to deploy artificial intelligence systems at scale, and this is reflected in a growing interest in the field known as DevOps, a term describing the need for collaboration between development and operations teams (the operations team being the teams responsible for deploying a developed system into production and ensuring that these systems are stable and scalable). The terms MLOps, for machine learning operations, and AIOps, for artificial intelligence operations, are also used to describe the challenges of deploying a trained model. The questions around model deployment are beyond the scope of this article. so we will instead focus on describing what deep learning is, what is can be used for, how it has evolved, and how we can train accurate deep learning models.​교육이 완료되면 모델이 고정되고   기계 학습의 두 번째 단계는 추론입니다. 모델이 올바른 출력 값을 모르는 예제, 이 예제에 적용되므로 모델이 이 값의 추정치를 생성하기를 원합니다. 기계 학습의 대부분의 작업은 정확한 모델을 학습하는 방법(즉, 데이터에서 정확한 함수 추출)에 중점을 두고 있습니다. 이는 대규모로 새로운 예제를 추론하기 위해 숙련된 기계 학습 모델을 프로덕션에 배포하는 데 필요한 기술과 방법이 일반적인 데이터 과학자가 보유한 기술과 다르기 때문입니다. 업계에서는 대규모로 인공 지능 시스템을 배포하는 데 필요한 고유한 기술에 대한 인식이 높아지고 있으며, 이는 개발 팀과 운영 팀 간의 협업필요성을 설명하는 용어인 DevOps(개발 된 시스템을 프로덕션에 배포하고 이러한 시스템이 안정적이고 확장 가능한지 확인하는 팀)라는 분야에 대한 관심이 증가하고 있습니다. 기계 학습 작업에 대한 용어 인 MLOps및 인공 지능운영의  AIOps는숙련 된 모델을 배포하는 문제를 설명하는 데 사용됩니다.  모델 배포와 관련된 질문은 이 글의 범위를 벗어납니다. 따라서  우리는 딥 러닝이 무엇인지, 무엇을 위해 사용될 수 있는지, 어떻게 진화했는지, 그리고 정확한 딥 러닝 모델을 교육하는 방법을 설명하는 데 집중할 것입니다.​One relevant question here is: why is extracting a function from data useful? The reason is that once a function has been extracted from a dataset it can be applied to unseen data, and the values returned by the function in response to these new inputs can provide insight into the correct decisions for these new problems (i.e., it can be used for inference). Recall that a function is simply a deterministic mapping from inputs to outputs. The simplicity of this definition, however, hides the variety that exists within the set of functions. Consider the following examples: ​여기서 한 가지 질문은 데이터에서 함수를 추출하는 것이 유용한 이유는 무엇일까? 그 이유는 함수가 데이터 집합에서 추출되면 보이지 않는 데이터에 적용할 수 있으며, 이러한 새로운 입력에 대한 응답으로 함수에서 반환되는 값은 이러한 새로운 문제에 대한 올바른 의사 결정에 대한 통찰력을 제공할 수 있기 때문입니다(즉, 추론에 사용할 수 있습니다). 함수는 입력에서 출력에 이르는 결정적인 매핑일 뿐입임을 상기하십시오. 그러나 이 정의의 단순성은 함수 집합 내에 있는 다양성을 숨깁니다. 다음 예제를 보세요.​-      Spam filtering is function that takes an email as input and returns a value that classifies the email as spam(or not).-      Face recognition is a function that takes an image as input and returns a labeling of the pixels in the image that demarcates that face in the image.-      Gene prediction is a function that takes a genomic DNA sequence at input and returns the regions of the DNA that encode a gene.-      Speech recognition is a function that takes an audio speech signal as input and returns a textual transcription of the speech.-      Machine translation is a function that takes a sentence in one language as input and returns the translation of that sentence in another language.​- 스팸 필터링은 이메일을 입력으로 받아 이메일을 스팸으로분류하는 값을 반환하는기능입니다.- 얼굴 인식은 이미지를 입력으로 받아 이미지의 얼굴을 구분하는 이미지의 픽셀 레이블을 반환하는 기능입니다.- 유전자 예측은 입력시 게놈 DNA 서열을 취하고 유전자를 인코딩하는 DNA의 영역을 반환하는 기능이다.- 음성 인식은 오디오 음성 신호를 입력으로 받아 음성을 텍스트로 반환하는 기능입니다.- 기계 번역은 입력으로 한 언어로 한 언어로 문장을 소요하고 다른 언어로 해당 문장의 번역을 반환하는 기능입니다.​It is because the solutions to so many problems across so many domains ca be framed as functions that machine learning has become so important in recent years.​이는 많은 도메인에서 많은 문제에 대한 해결책이 최근 몇 년 동안 기계 학습이 매우 중요해진 함수로 구성되기 때문입니다. "
음성인식 모듈 -VC-02 (SPEAKUP 3 CLICK) ,https://blog.naver.com/ubicomputing/222960914474,20221220,"음성인식 모듈 -VC-02(SPEAKUP 3 CLICK) 개요본 제품은 음성인식 모듈 -VC-02 입니다.VC-02 오프라인 음성 인식 모듈을 기반으로 디자인된 제품입니다.VC-02는 150개의 오프라인 명령 인식, 가벼운 RTOS, 펌웨어 업데이트 기능을 지원합니다.특징SpeakUp 3 Click is based on the VC-02, an offline voice recognition AI module from Ai-Thinker Technology, characterized by high reliability and robust versatility. The VC-02 module uses an integrated voice chip US516P6 from Unisound, which continuously optimizes and innovates algorithms in speech recognition technology. The offline recognition algorithm is deeply integrated with the chip architecture, providing customers with an ultra-low-cost offline voice recognition solution. This board can be widely and quickly applied to all smart small household appliances and products requiring voice control. The US516P6 chip uses a 32-bit RSIC architecture core, running at 240MHz, and incorporates a DSP instruction set specifically for signal processing and speech recognition, an FPU arithmetic unit that supports floating-point operations, and an FFT accelerator (support 1024-point complex FFT/IFFT operations, or 2048-point real FFT/IFFT operations). What makes this module unique are features like offline identification of 150 local instructions and self-learning of wake-up words, RTOS lightweight system, 90% recognition rate in a far field distance from 1 up to 5m, firmware update feature, as well as the selection of the communication method with the module.This Click board™ comes with a configurable host interface allowing communication with MCU using the chosen interface. The VC-02 can communicate with MCU using the UART interface, a default communication interface with commonly used UART RX and TX pins, and a default baud rate of 115200bps. Users can also use other interfaces, such as SPI and I2C, to configure the module and write the library by themselves.At the center of the SpeakUp 3 Click, an additional unpopulated header offers full support for debugging and programming capabilities. The Ai-Thinker has provided its users with a VC series development page, where with a simple registration, they get the opportunity to create their own command list/SDK/firmware for this module quickly and easily for free. With this header, the user can use a JTAG interface, in addition to the UART interface, for programming and debugging available through the JTAG interface pins (TCK and TMS). For more information and help when creating custom firmware, you can contact their help center.A special addition to this Click board™ are connectors, marked as MIC and SPK, for an analog omnidirectional microphone and 8Ω 2W cavity speaker from Shenzhen Anxinke Technology. These parts can be found in the same package as the Click board or can be acquired as a solo version in our shop.This Click board™ can be operated only with a 3.3V logic voltage level. The board must perform appropriate logic voltage level conversion before using MCUs with different logic levels. However, the Click board™ comes equipped with a library containing functions and an example code that can be used, as a reference, for further development.SPECIFICATIONS TypeSpeech recognitionApplicationsCan be used for offline speech recognition suitable for all kinds of smart small household appliances, toys, lamps, and other products that need voice controlOn-board modulesVC-02 - offline voice recognition AI module from Ai-Thinker TechnologyKey FeaturesPure offline speech recognition module, high reliability, robust versatility, widely applied to smart home, offline identification of 150 local instructions, RTOS lightweight szstem, selectable communication interface, low power consumption, and moreInterfaceI2C,SPI,UARTCompatibilitymikroBUSClick board sizeL (57.15 x 25.4 mm)Input Voltage3.3V,5V 문서 SpeakUp 3 click example on Libstock SpeakUp 3 click 2D and 3D files SpeakUp 3 click schematic VC-02 datasheet연관제품연관제품 1​제품정보: https://vctec.co.kr/product/detail.html?product_no=21133 음성인식 모듈 -VC-02 (SPEAKUP 3 CLICK) - 가치창조기술음성인식 모듈 -VC-02 (SPEAKUP 3 CLICK) 판매가(VAT별도) 28,420원 상품코드 P000BFGV 상품요약정보 음성인식 모듈 -VC-02 입니다. SNS 상품홍보 (최소주문수량 1개 이상 ) 수량을 선택해주세요. 상품명 상품수 가격 음성인식 모듈 -VC-02 (SPEAKUP 3 CLICK) 28,420원 총 상품금액 (수량) : 28,420원 (1개) NAVER 네이버 ID로 간편구매 네이버페이 네이버페이 구매하기 찜하기 톡톡 이전 머니결제혜택 최대 3%적립 + 소득공제 다음 재고안내 상품의 실시간 재고는 네이버...vctec.co.kr ​가치창조기술​ "
"[청력검사] 어음청력검사(Speech Audiometry, SA) ",https://blog.naver.com/aram100405/222122160094,20201021,"어음청력검사(SA)Speech Audiometry어음청력검사의 정의인간의 일상생활 의사소통에서 가장 중요한 말 또는 어음을 사용하여 청각의 민감성과 인지 능력 등을 측정하는 평가로서 일상의 의사소통 능력을 파악하는 것을 일차적인 목표로 한다. 이차적으로 순음청력검사(PTA)와의 결과를 확인하고 신뢰도를 확보하기 위해서도 사용하는 검사다.  어음청력검사의 목적 * 청력손실의 진단 : 청력 손실이 있는지 감별 진단* 청각 재활의 참고 지침 : 난청인의 보청기, 인공와우 등의 재활 방법 선택* 청력 손실 환자의 장애 평가 수단 : 청각장애의 정도* 청각장애의 유형의 구별 : 누가현상을 통해 구분Q. 누가현상(recruitment)이란?A. 자극음의 강도를 일정하게 증가시킴에 따라 피검자가 느끼는 음의 강도가 비정상적으로 크게 느껴지는 현상으로, 이는 역동범위(불쾌음압레벨-역치)가 줄어들어 나타나는 현상이다. 이러한 누가현상은 감각성 난청(미로성 난청)이 있는 경우 나타나고, 신경성 난청(후미로성 난청) 등에서는 누가현상이 나타나지 않는다.  어음청력검사의 제시 방법 제가 말하는 단어를 듣고 그대로 따라 말씀해주세요.들은 단어를 추측해서 말씀하셔도 됩니다.방음실, 헤드폰, 의자, 한국 어음검사어표, 마이크, CD플레이어, 청력검사기, 소음계가 필요하다.자극음을 제시할 때는 동일한 강도로 표준화된 음절이나 낱말을 사용해야 하며, 어음 제시는 육성이나 CD로 제시할 수 있다. ​1) CD로 자극음을 제시할 경우 재현성이 높아 다른 검사 기관 비교나 장애 평가에 이용할 수 있다. 그러나 피검자가 검사에 적응하지 못하는 경우 검사 시간 조절하기가 어렵다는 단점이 있다.​2) 육성으로 자극음을 제시할 경우검사자가 VU미텉의 사용에 익숙해야 하고 자극음 제시 시 0미터에 가깝도록 해야한다.피검자의 특성에 맞게 자극음 제시 시간을 조절할 수 있다는 장점이 있다.​Q. 사용하는 단어의 선정 기준은?A. 사용단어는 단어의 친숙성(familiarity), 음소간의 비유사성(phoneitc dissimilarity), 표준어로서의 대표성(normal sampling), 단어 간 가청 범위의 동질성(homogeneity) 등의 선정 기준을 고려하여 개발되었고, 연령에 따라 이음절의 학령 전기용, 학령기용, 일반인용 어음표가 개발되었다.  어음청력검사의 종류1. 어음인지역치(speeh recognition threshold, SRT) 1) 정의 및 특징제시한 어음(이음절어)의 50%를 인지할 수 있는 최소한의 강도레벨을 의미한다.순음청력역치와 얼마나 일치하는지 확인하여 검사의 신뢰도를 확인하여 위난청을 감별진단할 수 있다.​2) 검사방법이음절어를 사용하며 각 음절의 강도가 동일하고 표준화한 낱말을 사용해야 한다.피검자는 검사 전 목표 단어 친숙화 과정(familiarization)을 거쳐야 한다.순음청력역치(PTA)에서 20~25dB 더한 강도에서 최초 자극음을 제시하고 따라말할 수 있도록 한다.3개 중 2개 단어를 정반응했을 경우 10dB 하강, 오반응했을 경우 5dB 상승하는 방법으로 역치 구한다.​Q. 친숙화 과정을 시행하는 이유는?A.  들은 단어가 어떤 단어인지 몰라 따라할 수 없는 경우를 배제하고, 피검자의 반응에 따라 검사 방법을 변경하기 위해 필요한 과정이다. 조음에  문제가 있을 경우 받아쓰기로 대체하기도 한다.​3) 결과 해석순음청력역치(PTA)와 어음인지역치(SRT) 값이 10dB 이내일 경우 PTA의 신뢰성이 좋다고 할 수 있다.10dB 이상의 차이를 보일 경우 PTA를 다시 측정한다.그러나 급경사형 난청 형태를 가진 사람의 경우 SRT보다 PTA값이 더 높을 수 있다.중추청각처리장애 혹은 치매 등의 인지장애를 가진 경우 PTA보다 SRT값이 더 높을 수 있다.  2. 어음탐지역치(speech detection threshold, SDT)1) 정의 및 특징제시한 어음의 50%를 탐지할 수 있는 최소한의 강도레벨을 의미한다.피검자가 제시된 어음을 따라말할 수 없는 경우 어음인지역치(SRT) 대신 시행할 수 있다.어음을 듣고 말할 수 있는 능력이 낮은 외국인과 언어 발달 수준이 낮은 유아에게 시행할 수 있다.보청기 등의 청각보조기기의 피팅을 위해 쾌적강도레벨(MCL)이나 불쾌음압레벨(UCL)을 측정하는 것이 좋다.​2) 검사방법각 음절의 강도가 동일한 표준어인 이음절어를 사용하고 어음탐지역치검사에서는 어음의 유무만 표시한다.친숙화 과정을 거친 후 순음청력역치(PTA)보다 대략 20~25dB 더한 강도에서 시작한다.정확하게 반응하면 5dB 강도를 내려 반응하지 않는 가장 낮은 강도를 확인한다.​3) 결과 해석어음인지역치(SRT)에 비해 8~10dB 정도 더 좊다.대부분 SRT와 SDT의 차이는 최대 12dB를 넘지 않는다고 한다. 그러나 난청 형태에 따라 다를 수 있다.  3. 단어인지도검사(word recognition score, WRS) 1) 정의 및 특징피검자가 적절하게 들을 수 있는 강도에서 단음절어를 얼마나 정확하게 인지하는지를 백분율로 나타낸 것일상적으로 사용하는 단음절을 이용하여 학령 전기용, 학령기용, 일반용 등의 연령별 사용이 가능하다.보청기 등의 청각보조기기 적합 전후에 반드시 시행해야 하는 필수적인 검사다.​2) 검사방법피검자의 쾌적강도레벨(MCL)에서 최초 자극 어음(단음절어)을 제시한다.건청인의 경우 어음인지역치(SRT) 값에서 30~40dB 더한 값이 MCL이지만 난청인, 특히 감각신경성 난청인의 경우 역동범위가 좁아 누가현상이 일어날 수 있으므로 반드시 피검자에게 크기가 적절한지 확인 후 검사를 진행해야 한다.25개 단음절어 중 정반응한 단어 수를 체크하여 백분율로 계산한다.백분율(%) = 정반응한 단어의 수 / 전체 제시한 단어 수 * 100​3) 결과 해석  4. 문장인지도검사(sentence recognition threshold, SRS)1) 정의 및 특징피검자가 적절하게 들을 수 있는 강도에서 문장을 얼마나 정확하게 인지하는지를 백분율로 나타낸 것단어인지도검사(WRS)의 경우 음절 내 어떤 음소를 인지하지 못하는지 파악하고자 사용하는데, 단음절을 이용하기 때문에 일상생활 속 의사소통 능력을 알아보는 데는 한계가 있기 때문에 문장인지도검사(SRS)를 활용한다.​2) 검사방법단어인지도검사(WRS)와 동일하다.  참고 SRT) 학령전기 아동 검사용 이음절단어 그림​ WRS) 학령전기 아동용 단음절단어 그림​학령전기 아동의 경우 따라 말하는 검사가 어렵다고 판단될 경우 위의 그림판을 준비하여 말하는 단어가 무엇인지 손가락으로 짚을 수 있도록 한다. 검사 전 아동이 단어를 알고 있는지 확인 후 검사를 시행해야 한다.​​ "
CS231n | Lecture 5. Convolutional Neural Networks ,https://blog.naver.com/kimsamuel351/222741958243,20220522,"역사와 현재 1-1. 딥러닝 역사1-2. CNN 역사1-3. 현재​2. CNN2-1. Convolutional Layer2-2. Spatial Dimensions2-3. Pooling LayerConvolutional Layer  1. 역사와 현재1-1. 딥러닝 역사​Mark 1 Perceptron / Frank Rosenblatt, ~1957Adaline/Madaline / Widrow and Hoff, ~1960=> back-propagation X​Rumelhart et al., 1986=> back-propagation O​Reinvigorated research in Deep Learning / Hinton and Salakhutdinov, 2006=> RBM-initialized Weight 설정과 fine tuning​Acoustic Modeling using Deep Belief Networks / Abdel-rahaman Mohamed, George Dahl, Geoffrey Hinton, 2010Context-Dependent Pre-trained Deep Neural Networks for Large Vocabulary Speech Recognition / George Dahl, Dong Yu, Li Deng, Alex Acero, 2012Imagenet classfication with deep convolutional neural networks / Alex Krizhevsky, Ilya Sutskever, Geoffrey E Hinton, 2012=> ImageNet 대회에서 사람보다 뛰어난 성능 입증 !​​1-2. CNN 역사​Receptive field of single neurones in the cat's striate cortext / Hubel, Wiesel, 1959=> 고양이가 시각 정보를 받아들이는 메커니즘Neocognitro / Fukushima, 1980=> 간단하고 복잡한 세포의 네트워크 구조  Gradient-based laerning applied to document recognition / LeCun, Bottou, Bengio, Haffner, 1998 CV에서 CNN이 적용된 첫 논문. LeNet-5. 필기체 이미지 분류ImageNet Classfication with deep convolutional neural networks 이미지 분류 대회 ImageNet에서 높은 정확도를 보여주며 발전을 이끈 논문. AlexNet​​1-3. 현재​  2. CNN2-1. Convolutional Layer​필터를 통한 이미지 특징 추출 image depth와 filter depth는 같음필터 수에 따른 activation mpas depthConvNet 이후 Activation Funtion (ReLU)​ low revel에서 high level로 갈수록 더 복잡해짐(추상적) => 더 자세한 영역까지 다룸​ CNN 모델은 이처럼 여러 층으로 구성됨2-2. Spatial Dimensions​ 딱 떨어지지 않는 경우는 비대칭으로 인해 fit 맞게 계산해줌. 그러나 위 예시(< 모두를 위한 딥러닝 시즌 2 - PyTorch >)에서는 올림해서 계산filter sizepaddingstride ​파라미터 수 계산 ​2-3. Pooling LayerConvolutional Layer​특징을 유지하면서 더 작고 다루기 쉬운 형태로 변환각 activation map을 독립적으로 반영하여 변환 Max Pooling Original Pooling / Average PoolingMax pooling / Min Pooling"
"로봇 인공지능 요약 #2 - Perception, Scene Understanding ",https://blog.naver.com/amazingsys/222886728568,20220928,로봇 인공지능 1주차 강의 중 Perception 에 대한 세부 내용을 정리해 봤습니다. 수업 내용을 요약하면서제가 이해한 내용으로 간단히 적어보았는데혹시 다른점이 있다면 누구든 댓글 부탁드려요!​​​  Perception 지각 & Scene Understanding 장면 이해​모두 연관된 기술이지만나 스스로 분류해보자면노란색은 perception 분홍색은 Scene Understanding가 아닐까 정리해봤다.어지러워 보일 수 있지만 구분 기준은해석이 들어가야하느냐 아니냐 였다.​Object Detection (객체 탐지)Object Recognition (객체 인식)Image Segmentation (이미지 분할)3D reconstruction (3D 재구성)Image Captioning (이미지 캡션)Relationships between objects (객체 간의 관계)Speech Recognition (음성 인식)Language Grounding (언어 교육)Speech to Text (STT) (음성 텍스트 변환)​교수님께서는 위 9가지 방법을 나눠서 설명하신게 아니라서통으로 묶어서 이해하기로 했다. 1. Object Detection (객체 탐지)the task of detecting instances of objects of a certain class within an image.이미지에서 객체를 탐지해낸다.ex) 검은색 네모난 물체를 찾으시오 2. Object Recognition (객체 인식)computer vision technique for identifying objects in images or videos이미지나 비디오에서 찾은 객체를 식별한다.ex) 신호등을 찾으시오 3. Image Segmentation (이미지 분할)find out boundaries for each object instance of objexts위에서 좀더 업그레이드 되어바운더리 라인을 잡아서 객체만 분리해서 식별 4. 3D reconstruction (3D 재구성)the process of capturing the shape and appearance of real objects.다각도로 사물의 실제 모형을 캡쳐해 인지한다. 5. Image Captioning (이미지 캡션)generationg a caption of the given image.주어진 이미지에 대한 캡션을 생성한다. 6. Relationships between objects (객체 간의 관계)객체와 객체가 어떤 위치와 관계에 있는지 해석한다.​​​ 7. Speech Recognition (음성 인식)말을 하면 그 말을 인식한다. 8. Language Grounding (언어 교육)객체를 각 언어에 맞는 클래스로 분류 접목 한다. 9. Speech to Text (STT) (음성 텍스트 변환)말을 하면 그 말을 인식하여 텍스트로 변환한다.​​​ 
뉴질랜드 어학연수- EF 오클랜드 ,https://blog.naver.com/raffles7/222874546512,20220914,"뉴질랜드의 수도가 어디일까요?  바로 웰링턴입니다.그런데 우리에게는 뉴질랜드 하면 떠오르는 대표 도시는 오클랜드(Auckland)이죠.  뉴질랜드의 인구의 약 30%가 살고 있는 대표 도시인데다가 바로 오클랜드가 뉴질랜드 문화, 경제의 중심지이기 때문이죠. 마치 미국의 수도가 워싱턴인데, 미국의 대표 도시하면 브로드웨이와 월스트리트가 있는 뉴욕이 떠오르듯이요.오늘은 살기 좋은 도시 1위에 선정되기도 하였고, 광활한 대자연과 도시가 조화를 이루고 다양한 인종이 서로 어울리며 이민자 문화에 대한 존중하는 문화가 잘 자리 잡혀서 유학생들도 학업 하기 좋은 환경의 EF 오클랜드 캠퍼스에 대해 소개 드리려고 합니다.  ​ EF 오클랜드 캠퍼스 학생들​​​​오클랜드에는 뉴질랜드 명문 대학인 오클랜드 대학교와 오클랜드 공과대학이 위치하고 있어서 유학생들이 많은 도시이기도 하답니다.게다가 오클랜드의 날씨는 여름에도 찜통더위가 없고 겨울에는 많이 춥지 않고 시원하고 안개가 자주 낍니다. 연중 내내 비는 자주 오지만 여름철 기온이 24도 이상 넘는 날이 별로 없답니다.  우리나라라 계절이 반대이니 겨울에 따뜻한 곳에서 연수 받고 싶은 분들께 추천드려요!  요즘 부쩍 미국 달러 환율이 높아져서 부담되시죠?  뉴질랜드 달러는 상대적으로 안정적인 환율을 유지하고 있답니다.  연수기간동안 식비나 여행경비등 생활비 측면에서도 고환율시대에 뉴질랜드가 대안이 될 수 있습니다.​  ​​EF 오클랜드 캠퍼스의 위치에 대해 먼저 안내드릴게요. EF 오클랜드 캠퍼스는 오클랜드 시티의 중시가이자 메인 쇼핑거리인 Queens Street에 위치하고 있습니다.  오클랜드 페리 터미널, 브리토마트역, 버스정류장까지 도보 5분 거리에 위치하여서 대중교통 이용이 용이하답니다.​ ​그럼 EF Auckland 캠퍼스의  실제 모습이 궁금하니  버추얼 투어 한번 해볼까요?  ​ ​약 300명가량 수용 가능한 EF 오클랜드 캠퍼스에는 강의실 뿐 아니라 다음과 같은 다양한 학생 휴게 공간이 있습니다.​ ​​​​​​EF 오클랜드의 어학연수 2주부터 등록 가능한 기간제 프로그램과 장기 연수자들을 위한 학기제 등록 프로그램이 있습니다.  두 과정 모두 주당 수업 시간에 따라 일반과정과 집중과정을 고르 실 수 있습니다.​ ​​ ​​다음으로  EF의 수업 방식에 대해 먼저 말씀드리겠습니다. EF의 어학연수과정은 다른 어학원보다 좀 더 듣기와 말하기 능력 항상에 포커스가 맞추어져 있습니다. ​위 영상에서 보신 EF의 수업 체계는 다음과 같이 5 가지의 수업으로 구성되어 있습니다.1. General Class : 저희도 익히 경험한 강남역 영어학원 같은 소규모 렉처 스타일 수업2. Conversation class : 다양한 디스커션 수업3. iPAD Class : 아이패드를 이용한 2-3명의 pair conversation 수업4. i Lab computer class: 유창한 영어를 위해서는 파트너와의 공부도 필요하나 학습자 스스로의 혼자 해야 할 공부들이 있죠? 바로 올바른 문법을 익히고 올바른 발음으로 자신의 생각을 정리하고 전달해야 합니다. 그를 위한 video lesson(온라인 문법 공부)과 speech recognition(발음 수정)을 컴퓨터를 통해 공부하고 itracker라는 툴을 통해 본인의 프로그레스를 확인할 수 있습니다.5. SPIN Class: Special Interest Class로 대학 교양처럼 뉴질랜드 문화, 마케팅이나 저널리즘, 넷플릭스로 배우는 영어 같은 흥미 있는 과목 수업을 영어로 참여하는 수업입니다.​​​해외연수를 결정할 때 국내 어학원이 수준이 딸리거나 부족해서 해외연수를 떠나시는 것은 아닐 거예요. 영어를 쓰면서 낯선 곳에서 현지인의 삶을 체험해 보고 영어 공부에 대한 동기부여를 얻고 싶으셔서 이실 텐데요.성공적인 어학연수는 어학당 수업 방과 후를 어떻게 지내느냐가 매우 중요합니다. 한국인 학생들과 몰려다니면서 보낸다면 영어는 늘지 않을 거예요. EF는 한국인 학생과 중국인 학생 비율이 매우 적은 것이 특징입니다.​​​EF 오클랜드 프로그램은 어학원 수업 외에도 다양한 액티비티가 있는 것도 특징입니다.​ ​​유료 무료 프로그램 골고루 섞여있습니다.  EF의 액티비티에 열심히 참여하셔서 외국 친구도 많이 사귀고 방과 후 시간도 알차게 보내보시길 바랍니다.​ ​​​EF 오클랜드의 어학연수 기간 동안 숙박은 어떻게 해야 하는지 궁금해하실 텐데요? 아래의 두 숙박시설을 이용하실 수 있습니다.​ City lodge : 1인실주소: 150 Vincent Street, Auckland CBD 1010, New Zealand(캠퍼스와 도보 15분 거리, 시티센터 5분 거리) ​​​2.  YMCA:  1인실 주소: Corner Pitt Street & Greys Ave, Auckland Centra(l캠퍼스 도보거리: 25분,시티 도보 거리: 15분)​ ​​ ​​​EF 오클랜드의 어학연수 프로그램에 대해 더 궁금하신 사항은 저희 링크에이드로 연락 주시면 친절히 답변드리겠습니다.   ​​​​ ​​​​ ​​링크에이드는 EF 어학연수 입학 수속을 무료로 도와드리고 있습니다.입학 허가를 받은 이후 출국 및 현지 정착까지 세심히 도와드리고 있습니다.관심 있으신 분들은 카카오톡 아이디 linkaid로 카톡 상담 가능합니다. 전화가 편하신 분들은 010-2473-0049로 전화 주시면 친절히 안내해 드리겠습니다.​​ "
Getting Started with Python for Machine Learning ,https://blog.naver.com/yoono044/223022947757,20230221,"​​Python is one of the most popular programming languages for Machine Learning. It is easy to learn, has a wide range of libraries, and is a powerful tool for data manipulation and visualization. In this article, we will explore the basics of Python for Machine Learning.​​ https://tse2.mm.bing.net/th?id=OIP.ap7CS3fjq55UMu35IpjsngHaD4&pid=Api&P=0​1. Setting up Python​The first step in getting started with Python for Machine Learning is to set up a Python environment. You can download and install Python from the official Python website. Once you have installed Python, you can use a code editor or integrated development environment (IDE) like PyCharm or Jupyter Notebook to write and execute Python code.​2. Data Manipulation with Pandas​Pandas is a popular Python library for data manipulation. It provides data structures for efficiently storing and manipulating large datasets. With Pandas, you can load data from various file formats, clean and transform data, and merge and join datasets. Pandas is a powerful tool for data manipulation in Machine Learning.​3. Data Visualization with Matplotlib​Matplotlib is a Python library for creating data visualizations. It provides a wide range of plotting functions for creating line plots, scatter plots, bar charts, and histograms. With Matplotlib, you can create high-quality visualizations that help you understand your data and communicate your findings to others.​4. Machine Learning with Scikit-Learn​Scikit-Learn is a popular Python library for Machine Learning. It provides a wide range of algorithms for supervised and unsupervised learning, including regression, classification, clustering, and dimensionality reduction. With Scikit-Learn, you can train and test Machine Learning models, evaluate their performance, and make predictions on new data.​5. Deep Learning with TensorFlow​TensorFlow is a popular Python library for Deep Learning. It provides tools for building and training neural networks. With TensorFlow, you can create complex models for image and speech recognition, natural language processing, and other applications. TensorFlow is a powerful tool for Deep Learning in Machine Learning.​Conclusion​In conclusion, Python is a powerful programming language for Machine Learning. With its wide range of libraries for data manipulation and visualization, as well as Machine Learning and Deep Learning, Python is a popular choice for data scientists and Machine Learning engineers.​It is essential to stay up-to-date with the latest developments and techniques in Python for Machine Learning to leverage its potential. With the vast amount of data being generated every day, Python for Machine Learning is becoming more critical than ever in data science and Machine Learning.​​ "
"[주기동] 인공지능 주요 동향 및 전망(20221019, 2068) ",https://blog.naver.com/palanmanzang/222906671815,20221024,"1. 인공지능 주요 동향 및 전망1) 증강 인력(The Augmented Workforce)​2016년 “세계 경제포럼”에서 발표된 “The Future of Jobs”에서 인간 일자리의 상당 부분이 사라지거나, 로봇이나 인공지능으로 대체될 것으로 전망가트너는 반대로 앞으로 인공지능이 대체하는 일자리보다 더 많은 일자리를 창출할 것으로 예측인간과 인공지능의 결합, 이른바 “인공지능 증강”이 최고의 가치를 창출 것이다. 다국적 기업인 유니레버(Unilever)는 매년 수천 명의 직원을 고용하며, 채용 프로세스를 개선하기 위해 후보자의 적성, 논리, 추론 및 위험 적성을 측정하는 게임을 사용증강 인력은 일반적으로 사람이 지능형 가상 비서 또는 기계와 함께 작업하여 더 나은 결과를 달성하는 작업 영역을 의미​2) 인공지능 기반 사이버 보안(AI-Based Cybersecurity)​2022년 최대 글로벌 리스크로, 전 세계 CEO들이 “사이버 리스크”사이버 보안은 사이버 공격의 규모와 복잡성이 증가함에 따라 현 보안 상태를 분석하고 개선하기 위해 인공지능을 이용인공지능은 기존 시스템의 이상 징후를 즉시 포착할 수 있으며, 새로운 유형의 공격을 정확히 찾아낼 수 있다 인공지능은 침해 위험을 최소화하고 보안 상태를 강화하며, 위협 식별이 가능한 기술로서 사이버 보안을 강화하고, 조직이 보다 강력한 보안 태세를 구축하는 데 도움이 될 것​3) 인공지능과 메타버스(AI and The Metaverse)​메타버스는 가상현실보다 한 단계 더 진화한 개념으로 대규모 가상 플랫폼(massive scale interactive virtual platform)에서 사람들이 놀고, 소셜 활동을 하고, 쇼핑도 하고, NFT(Non-Fungible Token)나 기타 형태의 암호 화폐를 사용하여 물건을 구매하고 비용을 지불할 수 있는 곳으로서 공상과학 소설에 생명을 불어넣는 것메타버스는 세상을 바꿀 수 있는 잠재력을 지닌 플랫폼으로 미래 핵심 트랜드의 중심메타버스는 사용자가 가상 세계에서 상호작용 할 수 있는 플랫폼으로, 우리가 일하고 배우고 노는 방식에 혁명을 일으킬 잠재력을 가지고 있다메타버스에서의 인공지능은 현실적인 아바타를 만들고, 새로운 디지털 제품과 서비스를 개발하고, 원격 작업과 협업을 촉진함으로써 메타버스가 실현​4) 노코드 인공지능(No-Code AI)​‘노코드’는 인공지능 비전문가가 인공지능 개발자 혹은 전문가 없이도 자신의 아이디어를 구현하고 테스트할 수 있는 code-free 기술을 의미인공지능 기술로 문제를 해결하여 생산성과 효율성을 높이는 것을 목표​‘노코드’ 인공지능 플랫폼은 말로 하거나 혹은 포토샵처럼 drag-and-drop 등 직관적인 과정을 통해 코드한 줄 작성하지 않고 누구나 손쉽게 소프트웨어 개발이 가능코드 개발 및 데이터 작업 측면에서 개발 시간을 기존 작업에 비해 90% 정도 절약할 수 있을 것회사는 프로그램 구현을 위한 인공지능 전문가를 필요로 하지 않게 되므로 개발 비용을 낮출 수 있을 것​5) MLOps(Machine Learning Operations)​MLOps는 시스템의 개발과 운영을 통합하여 개발 주기를 단축하기 위한 DevOps를 머신러닝 시스템에 적용한 것MLOps의 목표는 가장 빠른 시간 내에 가장 적은 위험을 부담하며 아이디어 단계부터 상용화 단계까지 머신러닝 프로젝트를 진행할 수 있도록 기술적 마찰을 줄이는 것​​2. 인공지능 기술 동향1) 초대형 언어 모델​초대형 언어 모델(Large Language Models: LLM)은 언어를 이해하기 위한 ‘두뇌’라고 할 수 있으며, 방대한 텍스트 데이터를 기반으로 인간의 언어를 인식하고, 예측 및 생성할 수 있는 머신러닝 알고리즘언어 모델은 의미론적 기술을 사용하여 보다 인간과 상호작용하는 언어를 생성할 수 있으며, 언어를 이해하는 데 있어 더욱 정교해지고 점점 더 커지고 있다GPT-3는 종래의 다른 언어 처리 인공신경망과 비교할 수 없을 만큼 규모가 거대했으며, 그럴듯한 문장을 생성하고, 인간과 대화하고, 코드를 자동으로 완성하기까지 결과물을 만들어냄으로써 큰 주목람다 (LaMDA)​자연어 이해, 생성, 추론이 가능하고, 이모티콘으로 영화를 추측하며, 농담 설명 기능을 포함한 새로운 초대형 언어 모델인 PaLM(Pathways Language Model)을 공개​2) 자연어 처리​컴퓨터가 인간이 하는 것과 거의 같은 방식으로 인간의 언어를 분석 및 이해하고 의미를 도출하는 인공지능의 한 분야자연어 처리 기술을 응용한 대표적인 서비스 사례로 기계 번역(Machine Translation), 챗봇(Chatbot), 문법 자동 수정, 전체 텍스트 검색, 요약(summarization) 서비스 등대량의 텍스트 데이터를 분석하고 이해하는 데에도 자연어 처리가 이용​SIRI, ALEXA 및 CORTANA와 같은 스마트한 디지털 어시스턴트는 NLP 기술을 사용한 가장 대표적인 제품​3) 제너레이티브 인공지능​인공지능과 머신러닝 알고리즘을 활용하여 기계가 학습 데이터를 기반으로 텍스트, 이미지, 오디오 및 비디오 콘텐츠와 같은 새로운 인공 콘텐츠를 생성하는 기술음악과 영상 분야에서 두각실시간으로 새로운 음악을 생성하고, 오래된 영화의 이미지를 복원하고, 흑백 영화에 색상을 추가하기도 하며 초당 더 많은 프레임을 생성기존의 인공지능 기반의 음성 인식에서 벗어나 컴퓨터 비전, 대화 지능(conversational intelligence), 콘텐츠 지능, 의사결정 시스템과 같이 우리가 상상하는 것보다 더 많은 분야에 적용되어 산업 발전에 기여할 것​4) 강화 학습환경과 상호작용하면서 걷는 법을 알아가는 것과 같은 학습 방법주어진 불확실하고 복잡한 환경에서 에이전트가 실수와 보상을 통해 학습하고 목표를 찾아가는 알고리즘​에이전트가 스스로 복잡한 결정을 내리고 장기적 목표 수립이 가능할 것으로 기대되는 트랜드 중 하나로, 사업전략 기획, 로봇공학, 데이터 과학, 맞춤형 교육 시스템, 항공기 제어 및 로봇 모션 제어, 금융거래 등에서 널리 사용​5) 멀티모달 학습​음성, 제스처, 시선, 표정, 생체신호 등과 같은 감각 입력(sensory input)을 융합하여 학습하는 머신러닝의 한 분야다양한 형식의 데이터를 융합하여 학습하는 모델은 더 많은 정보가 집계되기 때문에 단일 데이터를 이용하는 모델보다 좋은 성능을 제공멀티모달 학습은 다양한 형식의 데이터로부터 특징 벡터를 추출하고, 이를 다시 융합하여 분류 및 예측 학습을 수행하거나, 학습 모델을 통해 학습한 후 분류 결과를 취합하는 방식이 존재이미지에 대한 질문-답변 데이터 생성(visual question answering) 문제, 이미지와 비디오 캡셔닝(image and video captioning) 문제, 오디오 비디오 음성 인식(audio-visual speech recognition) 문제, 멀티모달 감정 인식 문제에 주로 활용​3. 인공지능 윤리 미국무성은 책임성, 형평성, 추적 가능성, 신뢰성, 통제 가능성의 주요 원칙을 기반으로 하는 인공지능에 관한 윤리 원칙(Ethical Principle for Artificial Intelligence)을 발표미 상원은 인공지능 능력과 투명성(AICT)에 관한 법안을 발의하기도 하였으며, 정부뿐 아니라 기업과 대학 등에서도 인공지능 윤리 원칙과 법ㆍ제도에 관한 연구가 다양하게 이루어지고 있다“사람 중심의 인공지능 윤리 기준”은 인공지능 시대의 다양한 상황에 대한 구체적인 대응 방안이 마련되어야 할 것​​#주기동 #인공지능 "
"Basics Plus RC Unit3 동사의 형태, 종류 및 수 일치 [수업 노트] ",https://blog.naver.com/hllee5019/222617757052,20220110,"​Basics Plus Unit3 동사의 형태와 종류 및 수 일치주요 어휘들을 정리하였습니다.복습에 활용하세요~​ 자가 테스트도 해보세요~자가 테스트는 PDF로 다운로드 가능합니다.​p.1391. commute to work 통근하다2. submit the registration form 등록양식을 제출하다3. be postponed 연기되다4. have approved ~을 승인했다5. please review ~을 검토해주세요6. be released 출시되다7. have remained low 낮게 유지되었다8. request a prompt response 신속한 답장을 요청하다​p.1409. participate in ~에 참여하다 10. attend the banquet 연회에 참석하다11. ask for ~을 요청하다12. refer to ~을 참고하다13. deal with ~을 다루다14. respond to ~에 응답하다15. reply to this e-mail 이 이메일에 답하다16. within 24 hours 24시간 이내에17. rise increasingly 점점 오르다18. during the seminar 세미나 중에19. result in an increase 증가를 초래하다20. a considerable increase 상당한 증가21. during the lecture 강연 중에​p.14122. admission to the park 공원 입장료23. local residents 지역 주민들24. each of the +복수명사+단수동사 각각의 ~는25. some of the merchandise 일부 상품26. last about an hour 한 시간 정도 지속되다27. make profits 이윤을 내다28. a full refund 전액 환불​p.14229. every product 모든 제품30. at reduced prices 할인된 가격에31. the vacation requests 휴가 신청서32. at least one month in advance 적어도 한달 이전에33. respond to the invitation 초대장에 응하다34. Simply/Just +동사원형 그저 ~하세요35. inform A of a delay A에게 지연을 알리다36. meet all requirements 모든 요건을 충족시키다​p.14337. include +N ~을 포함하다38. a speech recognition system 음성 인식 시스템39. the installation of ~의 설치40. shorten production time 생산시간을 줄이다41. by more than 30 percent 30 페센트 이상으로42. conduct regular inspections 정기 검사를 실시하다43. take place 발생하다44. next to the park 공원 옆에45. submit a report 보고서를 제출하다46. a monthly progress report 월별 진행 보고서47. at the end of the month 월말에48. postpone the event 행사를 연기하다49. attract visitors 방문객들을 끌어들이다50. a number of + 복수명사 많은 ~ ​p.14451. would like to do ~하고 싶다52. people who have ~을 가지고 있는 사람들53. register early 조기 등록하다 54. a desirable schedule 바람직한 일정표​  TEST YOURSELF (1) (빈칸에 우리말 뜻을 쓰세요.)1. commute to work : ________2. submit the registration form : ________3. be postponed : ________4. have approved : ________5. please review : ________6. be released : ________7. have remained low : ________8. request a prompt response : ________9. participate in : ________10. attend the banquet : ________11. ask for : ________12. refer to : ________13. deal with : ________14. respond to : ________15. reply to this e-mail : ________16. within 24 hours : ________17. rise increasingly : ________18. during the seminar : ________19. result in an increase : ________20. a considerable increase : ________21. during the lecture : ________22. admission to the park : ________23. local residents : ________24. each of the +복수명사+단수동사 : ________25. some of the merchandise : ________26. last about an hour : ________27. make profits : ________28. a full refund : ________29. every product : ________30. at reduced prices : ________31. the vacation requests : ________32. at least one month in advance : ________33. respond to the invitation : ________34. Simply/Just +동사원형 : ________35. inform A of a delay : ________36. meet all requirements : ________37. include +N : ________38. a speech recognition system : ________39. the installation of : ________40. shorten production time : ________41. by more than 30 percent : ________42. conduct regular inspections : ________43. take place : ________44. next to the park : ________45. submit a report : ________46. a monthly progress report : ________47. at the end of the month : ________48. postpone the event : ________49. attract visitors : ________50. a number of + 복수명사 : ________51. would like to do : ________52. people who have : ________53. register early : ________54. a desirable schedule : ________​​  TEST YOURSELF (2) (선택문제, 정답은 맨 밑에...)1. [conduct, submit] the registration form  2. please [review, reviewing]3. have [remains, remained] low  4. request a [prompt, promptly] response   5. [attend, participate] the banquet  6. [deal, reply] to this e-mail  7. [within, by] 24 hours  8. rise [increasing, increasingly]  9. [during, while] the seminar  10. [result, result in] an increase  11. a [considerate, considerable] increase    12. [local, locally] residents  13. each of the +복수명사+[단수, 복수] 동사  14. [many, some] of the merchandise   15. a [full, fully] refund   16. every [product, products]  17. at [reduce, reduced] prices   18. [least, at least] one month in advance  19. [respond, respond to] the invitation  20. inform A [ in, of] a delay  21. meet all [require, requirements]   22. a speech [recognize, recognition] system  23. the [install, installation] of  24. to [short, shorten] production time  25. by [more,  more than] 30 percent  26. conduct [regularly, regular] inspections  27. [next, next to] the park  28. a [month, monthly] progress report    29. [attract, attend] visitors  30. a number of + [단수, 복수] 명사 31. people who [has, have] ~  32. a [desire, desirable] schedule​​ 첨부파일Basics plus 강의노트 Unit3.pdf파일 다운로드 ​​   정답1. submit  2. review  3. remained   4. prompt   5. attend  6. reply   7. within 24 hours  8. increasingly 9. during   10. result in  11. considerable 12. local 13. 단수  14  some 15. full 16. product  17. reduced 18. at least    19. respond to   20. of  21. requirements  22. recognition  23. installation  24. shorten   25. more than 26. regular 27. next to   28. monthly  29. attract  30. 복수 31. have  32. desirable   ​동사의 형태에 대해 궁금하시다면아래의 포스팅을 참고하세요.  [토익문법] 동사 자리(1): 동사의 형태모든 문장은 주어가 있으면 동사가 꼭 필요하죠. '주어+동사'는 문장의 기본요소이다. 빈칸이 동...blog.naver.com 동사의 수 일치에 대해 궁금하시다면아래의 포스팅을 참고하세요.  [토익문법] 동사 자리(2): 동사의 수 일치동사 자리에 빈칸이 있는 경우, 선택지의 보기가 동사의 다양한 형태가 주어진 경우 동사의 수 일치를 우선...blog.naver.com  ​ "
아이폰 4 4S 출시일 디자인 성능 스펙은? 스티브 잡스 마지막 유작 ,https://blog.naver.com/windangelic/222970955629,20221231,"안녕하세요. IT 컴퓨터 인플루언서 화니입니다. 아이폰 1세대부터 써오던 글이 이제 아이폰 4까지 오게 되었습니다. 오늘은 iPhone 4와 4S의 성능과 디자인, 출시일, 카메라 등 다양한 스펙과 관련된 얘기를 해보도록 하겠습니다. 대표적으로 제목에도 작성했듯이 아이폰 4S는 스티브 잡스의 마지막 유작이기도 하죠.​​ ​아이폰 4의 출시일은 2010년 6월에 공개되었는데, 지금은 우리나라가 1.5차 출시국으로 분류되어 있어 미국에서 공개 후 한 달 이내 구매를 할 수 있지만 이때까지만 해도 바로 국내에서는 구매를 할 수 없었습니다. 우리나라에서 iPhone4는 출시 후  3달 뒤인 9월에 KT를 통해서 개통이 가능했으면  다음 해 3월에 SKT에서도 개통이 가능했습니다.​​ ​아이폰 4가 지니는 의미는 Apple에서 처음 독자적으로 설계한 AP가 탑재 된 iPhone이라는데 의미가 있는 것 같습니다. 스펙을 살펴보면 이전 모델에서는 삼성전자의 엑시노스가 탑재되었는데, 아이폰 4는 A4 APL0398 SoC가 탑재되었습니다. 메모리는 512MB의 LPDDR1이, 스토리지는 8 / 16 / 32GB의 내장 메모리가 장착되었습니다.​디스플레이는 3:2비율의 해상도 960 x 640의 Retina Display가 탑재되었습니다. 중요한 것은 전작보다 상당히 높아진 해상도의 IPS TFT-LCD 디스플레이와 800:1이라는 명암비의 개선으로 이때부터 Retina로 네이밍 된 것으로 알려져 있습니다.​​ ​카메라 스펙 또한 많은 개선이 있었는데 iPhone 처음으로 전면 30만 화소의 카메라를 탑재하였으며, 후면 카메라 역시 AF를 지원하는 500만 화소의 렌즈가 장착되어 많은 개선이 있었습니다. 처음으로 전면 카메라를 탑재함으로써 FaceTime을 이때부터 사용할 수 있게 되었습니다. 또한 HDR을 지원하기 때문에 밝은 사진도 촬영이 가능했지만 아직까지 기술의 부족으로 역광이나 형광등 아래 푸른 멍 현상 등 몇 가지 논란이 있기는 하였습니다.​​ ​58.6 x 11.2 x 9.3mm의 작은 크기와 137g의 화이트, 블랙 버전으로 출시된 아이폰 4의 디자인을 보면 후면까지 강화유리로  디자인되었는데 사실 우리에게 깻잎 통 디자인으로 더욱더 잘 알려져 있습니다. 이러한 디자인은 놀림을 당하기도 했지만 마니아층이 만들어질 정도로 iPhone4는 큰 인기를 끌었었습니다. ​​ ​두 번째 아이폰 4s는 2011년 10월 발표되었으며 이 버전부터 우리나라에서도 SKT와 KT를 통해서 바로 개통할 수 있었습니다. iPhone 4s의 외관을 살펴보면 이전 모델과 크게 차이는 없으나 AP가 A5 APL0995 Soc로 업그레이드되었는데 전작보다 CPU는 2배, GPU는 7배 정도 상향된 성능이라고 합니다. 512MB LPDDR2 메모리와 8 / 16 / 32 / 64GB의 스토리지가 탑재되었는데, 아이폰 처음으로 64GB의 모델이기도 합니다.​​ ​카메라 역시 이전 버전에서 논란이 되었던 부분들이 대부분 개선되었으며, 후면 800만 화소로 업그레이드되었습니다. 사실 내부 스펙과 카메라 성능을 제외하면 크게 달라진 점은 없어 보이나 iPhone 4s가 가지고 있는 의미는 두 가지가 있습니다. 첫 번째는 바로 Siri의 탑재인데, 우리나라에서는 당시 iOS 5로 버전에서는 바로 사용할 수 없었으나 iOS 6로 업그레이드되면서 한국어를 지원하면서 Siri를 활용할 수 있게 되었습니다.​​ ​두 번째는 제목에서도 언급 드렸듯이 스티브 잡스의 마지막 iPhone이라는 것입니다. 실제로 아이폰 4s의 발표 때도 스티브 잡스는 참석하지 못하였고 2011년 10월 4일 발표가 된 후 다음날 사망을 했기 때문에 스티브 잡스의 유작이라는 얘기도 많이 있습니다. 게다가 처음으로 Siri가 탑재되어서 S가 Steve의 약자라는 얘기도 있었지만 사실 Siri는 Speech Interpretation and Recognition Interface의 약자로 애플에서 시리를 통째로 인수했기 때문에 연관은 없습니다.​오늘은 아이폰 4 4S 성능 디자인 출시일 카메라 스펙과 관련된 얘기를 몇 가지 하였습니다. 참고로 아이폰 4s는 30핀을 사용하는 마지막 제품입니다. 오늘 내용도 흥미롭게 보셨다면 팬 하기 부탁드립니다. 이상 화니였습니다. 감사합니다. "
앤드류 응 교수 딥러닝 학습일지-(1) ,https://blog.naver.com/nrbsld237/222970916554,20221231,"방학동안 각잡고 공부하기로 한 분야가 몇개 있다.블로그에 올리면 누군가는 도움을 받겠지라는 생각 반과 내 공부를 정리해보자라는 생각 반으로 학습일지를 남기기로 다짐하였다.매주 2~3번 학습일지를 올리는 것으로 목표를 하며 학습일지를 올려보도록 하겠다.  강의는 컴퓨터과학의 대가로 불리는 앤드류 응(Andrew Ng) 교수의 딥러닝 강의를 수강하고 있으며, https://m.boostcourse.org/ai215/lectures/20010 부스트코스를 통해 강의를 무료로 수강중이다. 딥러닝 1단계: 신경망과 딥러닝부스트코스(boostcourse)는 모두 함께 배우고 성장하는 비영리 SW 온라인 플랫폼입니다.m.boostcourse.org  1-2 What is Neural NetWork?딥러닝이란 인공신경망(Neural NetWork)를 사용하여 인풋(x)가 주어졌을 때 아웃풋 (y)를 예측하는 방법이다.예를 들어 집 값을 결정 짓는 속성 중에 크기,방의 개수,우편번호, 재력이 있다고 가정해보자. 크기와 방의 개수는 가족의 크기(family size)를 결정하고 우편번호는 인근 주요 시설의 접근성을 결정하고, 재력과 우편번호는 또 인근 학교의 수준을 결정 짓는다고 가정하자.그럼 간단히 다음과 같이 나타낼 수 있을 것이다. 인공 신경망이라고 해봤자 대단한 것이 없고, 다음과 같이 특정 계층으로 나타낼 수 있으면 되는 것이다.딥러닝에서는 다음과 같음 size,#bedrooms,zip code,wealth와 같은 인풋(x)과 price라는 아웃풋(y)가 주어졌을 때둘 간의 상관관계를 추정하는 것이다.다음과 같이 간단한, 계층별로 특정 속성이 다음 속성에 영향을 주는 신경망을 Standard Neural Network라고 한다.   1-3 Supervised Learning with a Neural NetWork딥러닝이 활용되는 분야는 다양하다. 분야에 따른 다른 종류의 인공 신경망(Neural NetWork)가 사용되곤 한다. 분야 별로 활용되는 인공 신경망의 종류는 다음과 같다. 활용 분야인공신경망의 종류Home features -> Real Estate(위의 예시)Ad User Info -> Online Advertisement(맞춤형 광고 클릭 예측)Standard Neural NetWorkImage -> Photo tagging(사진이 어떤 대상인지 예측)Convolutional Neural NetWorkAudio -> Speech Recognition(음성인식 기술)English -> Machine Translation (번역 기술)Recurrent Neural NetWorkImage Radar Info -> Autonomous Driving(자율 주행 자동차 기술)Custom,Hybrid Neural NetWork 대표적 인공 신경망의 종류라는데 차차 알려주신다고 한다.​다음으로 Structured Data와 Unstructured Data에 대한 언급이 나온다. Structured Data (정형 데이터)Unstructured Data(비정형 데이터) Data Frame의 형태로 주어짐엑셀에 정리하기 쉬운, 행,열에 맞게 표로 정리된 데이터표로 주어지지 않는 데이터Audio(음성), Image(사진),Text(글) 파이썬, 엑셀 등을 다뤄본 사람이라면 알겠지만, 컴퓨터는 Structured Data를 다루는데에 압도적으로 능숙하다.반면 인간은 직관적으로 글을 읽고, 사물을 인식하기에 Unstructed Data를 다루는데에 능숙하다. 딥러닝의 발전은 컴퓨터가 Unstructed Data를 더 잘 다루도록 하는 것에 목표를 두고 있으며, 굉장히 빠른 속도로 발전 중이다.  1-4. Why is DeepLearning Taking off?사실 이 부분은 배울게 그다지 많지는 않다. 딥러닝 특성상 데이터가 많아야 정확도가 올라가는데,최근 여러 센서와 전자기기의 보급으로 데이터가 폭발적으로 많아져 딥러닝의 정확도가 올라 가는중이다.만을 언급하신다.   이것으로 1주차를 마무리 하겠습니다. 블로그에 이렇게라도 정리를 하니까 공부가 되는 느낌이군요. 2주차로 찾아 뵙겠습니다. "
Chat GPT 에게 AI 발전사에 대하여 물어 보았다. ,https://blog.naver.com/chrisnam1950/223025980273,20230224,"나의 마태복음 연구에 많은 통찰력을 주었던 책, Peter F. Ellis의  MATTHEW, his mind and his message의 일부를 사진 촬영하여 문자 변화을 거쳐 한글 번역 프로그램으로 번역해 보았는데 만족할만한 결과가 나오지 않았다. 이것은 신학이라는 전문 분야에 대한 한국어의 말뭉치가 부족하기 때문이 아니기 때문이 아닐까라는 생각을 하게 된다. 일반 상용문은 꽤 만족할 만한 결과를 얻고 있는데, 특정 전문 분야에 가면 그렇지 못한 결과가 나타난다. 결국 그 분야의 전문인들의 노력이 더 필요하다는 생각이다. ​그러다가 AL의 발전사에 대하여 알고 싶은 마음이 생겨 Chat GPT에게 먼저 발전사에 대한 질문을 하였고 그에 대한 대답을 들은뒤 CHAT GPT에 대한 간단한 설명을 요청하였더니 다음과 같은  답을 내놨다. 아마도 자신과 연관된 질문이기에 정확한 답을 하지 않았을까 생각한다. ​Tell us about the development history of AI​The concept of artificial intelligence (AI) has been around for centuries, but the development of AI as a field of study began in the mid-20th century. Here is a brief overview of the development history of AI:Early beginnings: The idea of creating machines that can perform tasks like humans dates back to ancient Greek myths. However, the concept of AI as we know it today began in the 1940s and 1950s.Dartmouth Conference: In 1956, the Dartmouth Conference was held, which is considered the birthplace of AI. This conference brought together researchers who were interested in creating machines that could think and learn like humans.The first AI programs: In the 1950s and 1960s, researchers developed the first AI programs, including the Logic Theorist and the General Problem Solver.The AI winter: In the 1970s and 1980s, progress in AI slowed down, and the field entered a period known as the ""AI winter."" This was due to a lack of funding and unrealistic expectations for AI.Expert systems: In the 1980s and 1990s, AI research focused on developing expert systems, which were programs that could solve problems in specific domains.Machine learning: In the 1990s, machine learning became an important area of AI research. Machine learning algorithms allowed computers to learn from data and improve their performance over time.Deep learning: In the 2000s and 2010s, deep learning became the most significant breakthrough in AI. Deep learning algorithms use neural networks to learn from data and can perform tasks like image recognition and natural language processing.Current AI applications: Today, AI is used in many applications, including self-driving cars, virtual assistants, image and speech recognition, and medical diagnosis.In summary, the development history of AI has been a long and complex journey that has involved many breakthroughs and setbacks. Despite its ups and downs, AI has become a critical field of study that has the potential to transform many industries and change the way we live and work.​<번역>질문 / AI의 발전 역사에 대하여 말해 주세요​인공 지능(AI)의 개념은 수세기 동안 존재해 왔지만 연구 분야로서의 AI의 발전은 20세기 중반에 시작되었습니다. 다음은 AI의 개발 역사에 대한 간략한 개요입니다.초기 시작: 인간처럼 작업을 수행할 수 있는 기계를 만드는 아이디어는 고대 그리스 신화로 거슬러 올라갑니다. 그러나 오늘날 우리가 알고 있는 AI의 개념은 1940년대와 1950년대에 시작되었습니다.Dartmouth Conference: 1956년에 Dartmouth Conference가 개최되었는데, 일체 포함. 이 회의에는 인간처럼 생각하고 배울 수 있는 기계를 만드는 데 관심이 있는 연구원들이 모였습니다.최초의 AI 프로그램: 1950년대와 1960년대에 연구원들은 최초의 AI 프로그램을 개발했습니다. , 논리 이론가 및 일반 문제 해결사 포함.AI 겨울: 1970년대와 1980년대에 AI의 발전이 둔화되었고 이 분야는 ""AI 겨울."" 이는 자금 부족과 AI에 대한 비현실적인 기대 때문이었습니다.전문가 시스템: 1980년대와 1990년대에 AI 연구는 전문가 시스템 개발에 중점을 두었습니다. 특정 영역의 문제를 해결합니다.머신 러닝: 1990년대에 머신 러닝은 AI 연구의 중요한 영역이 되었습니다. 기계 학습 알고리즘을 통해 컴퓨터는 데이터에서 학습하고 시간이 지남에 따라 성능을 개선할 수 있었습니다.딥 러닝: 2000년대와 2010년대에 딥 러닝은 AI에서 가장 중요한 돌파구가 되었습니다. 딥 러닝 알고리즘은 신경망을 사용하여 데이터에서 학습하고 이미지 인식 및 자연어 처리와 같은 작업을 수행할 수 있습니다.현재 AI 애플리케이션: 오늘날 AI는 다음을 포함한 많은 애플리케이션에서 사용됩니다. 자율 주행 자동차, 가상 비서, 이미지 및 음성 인식, 의료 진단.요약하면 AI의 개발 역사는 길고 복잡한 여정이었습니다. 많은 돌파구와 좌절. 기복에도 불구하고 AI는 많은 산업을 변화시키고 우리가 생활하고 일하는 방식을 변화시킬 잠재력을 가진 중요한 연구 분야가 되었습니다.​​Briefly explain Chat GPT.​Chat GPT (Generative Pre-trained Transformer) is a type of language model developed by OpenAI. It is a deep learning model that has been pre-trained on a large corpus of text data and can generate human-like responses to natural language inputs. Chat GPT uses a transformer architecture, which allows it to learn long-term dependencies and relationships between words in a sentence. It has been fine-tuned on specific tasks such as question-answering and conversational AI, making it capable of answering questions, generating text, and holding human-like conversations. Chat GPT has gained popularity as a powerful tool for natural language processing tasks and has been used in various applications, including chatbots, virtual assistants, and automated content creation.​번역질문 / Chat GPT에 대하여 간단하게 설명해 주세요번역결과​Chat GPT(Generative Pre-trained Transformer)는 OpenAI에서 개발한 일종의 언어 모델입니다. 대규모 텍스트 데이터 코퍼스에 대해 사전 훈련된 딥 러닝 모델이며 자연어 입력에 대해 인간과 유사한 응답을 생성할 수 있습니다. Chat GPT는 변환기 아키텍처를 사용하여 문장 내 단어 간의 장기적인 종속성과 관계를 학습할 수 있습니다. 질의 응답 및 대화형 AI와 같은 특정 작업에 대해 미세 조정되어 질문에 답하고 텍스트를 생성하고 인간과 같은 대화를 할 수 있습니다. Chat GPT는 자연어 처리 작업을 위한 강력한 도구로 인기를 얻었으며 챗봇, 가상 비서, 자동 콘텐츠 생성 등 다양한 애플리케이션에 사용되었습니다. "
음성인식 1주차 요약 ,https://blog.naver.com/amazingsys/222894075458,20221007,"초당 16000개주파수 16kHz그의 반이 최대분석주파수 8kHz​정교하게 sampling 해야하기 때문에 16bit1과 1사이를 2의 16승개 컴퓨터입장에서는 그렇게 큰건 아니다.​16kHz​음성전송율 16bit * 16kHz = 256kbps이를 소리는 유지하면서 압축한다.압축 하는 방법이 바로 Speech Coding​소리를 거의 유지하면서 8kbps 까지 압축 가능스마트폰에서 듣는 소리는 20kbps 정도 된다.군사용으로는 2kbps음질이 중요 하지 않다, 압축을 쎄게 해서 정보만 전달하면 된다.​very low -> 2 ,4kbps8kbps 는 low-bit-rate Speech CODEC (coding-decoding) ->가장 유명한 코덱 AMR-wb​wideband -> 16kHz이기 때문에 wide band 이다.​주파수가 8kHz이면최대분석주파수는 그의 반인 4kHz​만약 음성을 안녕하세요 라고 말했는데안냥하세요 라고 출력했다면 20% 에러​​1. Objective MeasureSNR : 크기(결함 에코의 높이)와 잡음(임상 에코 포함) 크기의 비. 파향 대조2. Subjective Quality MeasureMOS(Mean Opinion Score) : 사람이 직접듣고 좋으면 5점 나쁘면 1점 10사람에게 듣고 평균냄​ Speech EnhancementNoise Cancellation주위에 있는 잡음을ambient noisebackground noise​잡음제거 루틴이 음성 인식 전에 있다.SE(frontend 로 쓰인다) + ASRE2E : ASR engine (음성인식 엔진)LM : back-end​Echo Cancellation​​  음성 주파수가 주파수 16kHz 라면분석 최대 주파수 얼마인 지,​concatenative TTS (사슬을 잇다. 연쇄시키다)= unit selection TTS​  Speech SynthesisSpeech Synthesis–Text-to-Speech (TTS)ProsodySpeaker Conversion​Speech RecognitionSpeech-to-Text →Speech UnderstandingSpeaker RecognitionKey-word Spotting = hotwordSpeaker Verification​Speech Pathology -> 목소리로 아픈걸 확인​  Speech Production →Hearingidea →linguistic structure →prosody →motor control(Grammar)sound pressure →acoustic wave →auditory system→Neurological pulses →feedback​Lombard Effect →Feedback (더 크게 발성하는 경향이 있다)왜? 내가 말한 소리가 내 귀로 들어간다.  그런 어쿠스틱path 외에 뼈를 타고 가기도 한다. 골전도 마이크 진동만으로도 들을 수 있는 기기, 주위가 시끄러우면 어쿠스틱한 환경에서 소리가 잘 안들린다. 대화를 잘 하기 위해서는 목소리를 더 키워야 한다고 판단. 그 사람 목소리는 잘 안들리고, 내 목소리는 크게 들리니까 작용 반작용. 피드백.​Cocktail Party Effectbeamforming 공간적집중 -> 귀가2개 어느방향에서 소리가 나는지 소리의 위치정보separation -> 뇌신경에서 내가 듣고 싶은 소리, 그렇지 않은 소리를 나눈다.​Masking Effecttemporal masking 쇼생크탈출 하수관 천둥이 칠 때, 내리침frequency masking 시간영역이 아닌같은 시간 안에 같은 주파수 대역에 있는 두 톤이 있다면 큰톤이 작은 톤을 mask 한다.문턱 값을 넘어 서면 두번째 톤이 들린다.볼륨이 작은 second ton은 압축할 필요가 없다. 안들리니까.안들리는 소리를 굳이 통신채널 통해서 보낼 필요가 없다.가장 대표적인 오디오 CODEC은? MP3​코덱의 종류는 lossy lossless​  조음기관조음기관을 잘 본딴게 한글!  빨리 떨리면, 시간적으로 흔들리는 간격이 짧으면 주파수가 높다.소프라노는 최대 발생할 수 있는 주파수가 8kHz 넘어간다.​사람이 들을 수 있는 가청 주파수가 22kHz라면실제로는 20kHz이지만 margin 을 줘서 22라면샘플링 주파수는?!!! 그의 2배 44.1CD가 44.1 이다!​주파수 -> 분석 주파수는 반절가청 주파수 -> 샘플링 주파수는 2배​  비음 (Nasal Sound Output)유성음 -> 성대가 떨렬려서 나오는것 (Oral Sound Output) -> 모음무성음 -> 성대가 안떨리는거 필터 : 원하는 것만 챙기는것​  음성 사운드를 분류해본다.Vowel : 모음 no major airflow restrictionConsonant : 자음 significant restriction​Vocal Tractstaitionary : 정상화 백색소음 ex) 선풍기 소리 nonstationary : 노이즈 많은 것 칵테일파티 등등사람 목소리는 그 어느중간에 있는 것 Quasi-stationary long term으로 보면 nonstationary하지만 그 어느곳이 쿼지!​Formant -> resonance structure 소리굽쇠 공진peach 와 fundamental frequent​excitationexcite 발생시키다.​ ​​ ​ "
Explain how to use ChatGPT to impove English skills ,https://blog.naver.com/yanghaha/223031329518,20230301,"ChatGPT can be a useful tool to improve your English skills as it is a language model that can provide you with instant feedback and practice opportunities. Here are some ways to use ChatGPT to improve your English:​Practice writing:You can practice your writing skills by chatting with ChatGPT and asking it to provide feedback on your writing. ChatGPT can help you identify grammatical errors, suggest more natural phrasing, and provide vocabulary suggestions to help you express yourself more effectively.​Expand your vocabulary: ChatGPT can help you learn new words and phrases by providing definitions, synonyms, and examples of how to use them in context. You can ask ChatGPT for synonyms or antonyms of words you already know to help you expand your vocabulary.​Improve your grammar:ChatGPT can help you improve your grammar by providing corrections for your writing and answering your grammar-related questions. You can ask ChatGPT for explanations on specific grammar rules or ask it to check your grammar for accuracy.​Practice speaking:While ChatGPT does not have a speech recognition feature, you can still use it to practice your speaking skills by typing out your responses as if you were speaking. This can help you improve your fluency and confidence in speaking English.​Learn idioms and expressions:ChatGPT can help you learn idioms and expressions commonly used in English by providing definitions and examples. You can ask ChatGPT to explain the meaning of a particular idiom or expression, or to provide examples of how to use it in context.​Remember that while ChatGPT can be a helpful tool, it should not be the only resource you rely on to improve your English skills. It is important to also practice speaking and listening to native speakers, reading and writing in English, and using other language learning resources to become a well-rounded English learner.​번역결과ChatGPT는 즉각적인 피드백과 연습 기회를 제공할 수 있는 언어 모델이므로 영어 실력을 향상시키는 데 유용한 도구가 될 수 있습니다. 다음은 ChatGPT를 사용하여 영어 실력을 향상시키는 몇 가지 방법입니다.작문 연습: ChatGPT와 채팅하고 작문에 대한 피드백을 요청하여 작문 실력을 연습할 수 있습니다. ChatGPT는 문법 오류를 식별하고 보다 자연스러운 구문을 제안하며 자신을 보다 효과적으로 표현할 수 있도록 어휘 제안을 제공합니다.어휘 확장: ChatGPT는 새로운 단어를 배우는 데 도움이 될 수 있습니다. 정의, 동의어 및 문맥에서 사용하는 방법에 대한 예를 제공하여 구. 이미 알고 있는 단어의 동의어 또는 반의어를 ChatGPT에 요청하여 어휘를 확장할 수 있습니다.문법 개선: ChatGPT는 문법을 교정하여 문법을 개선하도록 도와줍니다. 문법 관련 질문 작성 및 답변. 특정 문법 규칙에 대한 설명을 ChatGPT에 요청하거나 문법이 정확한지 확인하도록 요청할 수 있습니다.말하기 연습: ChatGPT에는 음성 인식 기능이 없지만 마치 말하는 것처럼 응답을 입력하여 말하기 기술을 연습하는 데 사용하십시오. 이를 통해 영어 말하기에 대한 유창함과 자신감을 향상시킬 수 있습니다.관용구 및 표현 학습: ChatGPT는 정의와 예를 제공하여 영어에서 일반적으로 사용되는 관용구와 표현을 학습하는 데 도움을 줄 수 있습니다. ChatGPT에 특정 관용어나 표현의 의미를 설명하거나 문맥에서 사용하는 방법의 예를 제공하도록 요청할 수 있습니다.ChatGPT가 도움이 될 수 있지만 영어 능력을 향상시키기 위해 의존하는 유일한 리소스가 되어서는 안 됩니다. 원어민의 말과 듣기, 영어 읽기와 쓰기, 기타 언어 학습 자료 활용을 연습하여 균형 잡힌 영어 학습자가 되는 것도 중요합니다. "
[이렇게 사용하세요!] 일상 속 IoT 적용사례 (Feat. Cloud IoT Core) ,https://blog.naver.com/n_cloudplatform/221803553124,20200210,"​안녕하세요 네이버 클라우드 플랫폼입니다.​이번 포스팅에서는 IoT 트렌드와 함께 네이버 클라우드 플랫폼이 제공하는 IoT 플랫폼, Cloud IoT Core에 대해 알아보겠습니다.​또한 라즈베리 파이를 사용하여 방문객과 문자 대화를 진행하는 두 가지 IoT 활용 사례(+코드)를 확인하실 수 있습니다.   ​ 1. 일상 속 IoT 트렌드날이 갈수록 더욱 다양하게 활용되며 주목 받고 있는 IoT(Internet of Things)!지난 1월 10일 미국 라스베이거스에서 막을 내린 세계 최대 규모의 IT행사, CES2020의 중요한 이슈 중 하나는 “IoT”였습니다.  모든 디바이스를 인터넷을 통해 서로 연결하고, 진화한 인공지능(AI)과 IoT기술을 결합하며 IoT기술이 더욱 각광받고 있습니다. ​전 산업 영역에 걸쳐 IoT가 적용되고 있으며, 이미 우리 생활 깊숙이 자리하고 있는 IoT! IoT를 기반으로 생성되는 데이터를 수집하고, 처리하는 “IoT플랫폼”의 필요성이 더욱 커지고 있습니다.​더욱이 “IoT보안”이 그 어느 때 보다 중요해진 지금, 디바이스로부터 생성된 수많은 데이터를 클라우드를 통해 편리하고 안전하게 구축할 수 있는 강력한 IoT플랫폼이 필요합니다.​  2. Cloud IoT Core 소개네이버 클라우드 플랫폼은 IoT환경을 쉽고 편리하게 구축할 수 있는 Cloud IoT Core 상품을 제공하고 있습니다. Cloud IoT Core 서비스는 쉽고 빠르고 안전하게 IoT 기기 간의 통신을 가능하게 하며 실시간으로 메시지에 대한 처리 역시 가능합니다. Cloud IoT Core 서비스 구조​ 네이버 클라우드 플랫폼 Cloud IoT Core 3가지 특징 1. 편리한 데이터 전송 별도의 서버 구축 및 애플리케이션 개발 없이 경량형 메시지 프로토콜, “MQTT 프로토콜” 기반으로 데이터를 편리하게 전송할 수 있습니다.​2. 인증서 기반 암호화 통신 인증서를 통해 자격 증명된 IoT 디바이스에 한하여 클라우드와 연결될 수 있으며, 데이터는 사용자 인증 및 “TLS 방식”을 통해 암호화됩니다.​3. 실시간 메시지 처리사용자는 규칙 엔진을 통해 손쉽게 데이터를 실시간으로 분석하고 처리할 수 있습니다. ​ 3. Cloud IoT Core 활용사례Cloud IoT Core 상품을 통해 어떤 IoT시스템을 구축할 수 있을지 알아볼까요?Cloud IoT Core 서비스는 네이버 클라우드 플랫폼의 다른 상품과의 연동을 지원하며, 연동할 수 있는 서비스를 점차 확대할 예정입니다. Raspberry Pi를 사용하여, 네이버 클라우드 플랫폼의 Cloud IoT Core 서비스와 다른 서비스를 연동해 실제로 구축할 수 있는 두 가지의 사례를 설명 드리겠습니다.  ​기본 활용사례 1. Raspberry Pi로 방문자 문자 알림 받기도어벨을 누르는 경우, 사용자의 스마트폰으로 문자 메시지를 전송하여 부재 중에도 방문자가 왔음을 알려주는 시스템을 구축할 수 있습니다. Raspberry Pi를 도어벨로 활용하기 위해■Raspberry Pi에 버튼을 연결하여 버튼을 누르는 이벤트 발생시, Cloud IoT Core로 메시지 전달■버튼 이벤트 처리 Rule에 따라 Cloud Functions의 Action 실행 (관련 코드 #1)■Action에서 SENS(Simple & Easy Notification Service) API를 활용하여 문자 메시지로 방문자가 왔음을 전달 ​​심화 활용사례 2. Clova API로 방문객과 문자 대화기본 활용 사례에 Clova API(CSR/CSS)를 사용하여 음성-텍스트 전환을 가능하게 하고, Slack을 활용하여 사용자와 방문객이 문자로 대화 할 수 있는 시스템을 구축할 수 있습니다. *네이버 클라우드 플랫폼이 제공하는 AI Service 내에서, STT를 구현할 수 있는 Clova Speech Recognition(CSR)상품과 TTS를 구현할 수 있는 Clova Speech Synthesis(CSS)상품을 연동합니다.  기본 사례를 확장하여 Slack의 Webhook 기능을 활용한 메시징 처리를 위해■Cloud Functions의 Action으로 Slack 채널에 방문자 도착 메시지 전달 (Incoming Webhook, 관련 코드 #2)■Slack 채널에서 메시지를 입력하면 Raspberry Pi에 메시지를 전달하여(Ougoing Webhook) 문자 대화가 가능하며, 이후의 대화는 Raspberry Pi와 Slack이 직접 통신 (관련 코드 #3)■Raspberry Pi에서 방문객과 대화를 위한 음성 ↔ 텍스트 전환에 Clova API(CSR/CSS) 활용 (관련 코드 #4)  이 외에도 Raspberry Pi를 활용해 가정에서의 미세먼지/환기 경고 시스템, 동작 감지 센서를 통한 실내 전등 제어 시스템 등 Cloud IoT Core를 활용한 다양한 서비스를 구축할 수 있습니다.  더욱 자세한 정보는 네이버 클라우드 홈페이지 및 Cloud IoT Core 사용가이드에서 확인하실 수 있습니다.   ● 코드 #1 ( Cloud Functions의 Action ) import hmacimport base64import requestsimport timeimport jsonimport hashlibdef main(args):    response = send_message(args.get(""alert"", ""Someone is coming to your home""))    return {""response.status_code"": response.status_code, ""response.reason"": response.reason}def get_headers(uri):    timestamp = int(time.time() * 1000)    timestamp = str(timestamp)    try:        secret_key = bytes(secret_key, 'UTF-8')    except TypeError:        secret_key = bytes(secret_key).encode('UTF-8')    method = ""POST""    message = method + "" "" + uri + ""\n"" + timestamp + ""\n"" + access_key    try:        message = bytes(message, 'UTF-8')    except TypeError:        message = bytes(message).encode('UTF-8')    signing_key = base64.b64encode(hmac.new(secret_key, message, digestmod=hashlib.sha256).digest())    headers = {'Content-Type': 'application/json; charset=utf-8',                'x-ncp-apigw-timestamp': timestamp,                'x-ncp-iam-access-key': access_key,                'x-ncp-apigw-signature-v2': signing_key}    return headersdef send_message(msg):    service_id = ""YOUR_SERVICE_ID""    api_endpoint = ""https://sens.apigw.ntruss.com""    uri = ""/sms/v2/services/{serviceId}/messages"".format(serviceId=service_id)    url = ""{apiEndpoint}{uri}"".format(apiEndpoint=api_endpoint, uri=uri)    body = {""type"": ""SMS"",            ""contentType"": ""COMM"",            ""countryCode"": ""82"",            ""from"": ""SENDER_MOBILE_NUMBER"",            ""content"": msg,            ""messages"": [{""to"": ""RECEIVER_MOBILE_NUMBER"", ""content"": msg}]            }    response = requests.post(url, headers=get_headers(uri), data=json.dumps(body))    return response ​● 코드 #2 ( Cloud Functions의 Action ) import jsonimport requestsdef main(args):  response = send_to_slack(args.get(""text"", ""Someone is coming to your home""))  return {""response.status_code"": response.status_code, ""response.reason"": response.reason}def send_to_slack(msg):  url = ""YOUR_INCOMING_WEBHOOK_URL""  return requests.post(url,           headers = {""Content-Type"": ""application/json""},           data = json.dumps({""text"": msg})) ​● 코드 #3 ( Raspberry Pi에서 구동하는 간단한 웹 서버(Slack의 Outgoing Webhook 수신)로 간단하게 수신한 메시지를 다시 전달하는 (echo) 형태 ) from flask import Flask, requestimport jsonimport requestsSLACK_TOKEN = 'YOUR_SLACK_TOKEN'SLACK_WEBHOOK_URL = 'YOUR_OUTGOING_WEBHOOK_URL'app = Flask(__name__)def parse_request(request_):    text = ''    if request_.form.get('token') == SLACK_TOKEN:        channel = request_.form.get('channel_name')        username = request_.form.get('user_name')        text = request_.form.get('text')    return textdef send_to_slack(text):    response = requests.post(SLACK_WEBHOOK_URL,                headers = {""Content-Type"": ""application/json; charset=utf-8""},                data = json.dumps({""text"": text.split(' ')[1]}))    return response.status_code@app.route('/', methods=['GET'])def index():    return ('This is a website.', 200, None)@app.route('/webhook/slack', methods=['POST'])def webhook_for_slack():    text = parse_request(request)        if text != '':        send_to_slack(text)    return ("""", 200, None)if __name__ == '__main__':    app.run(debug=True, host='0.0.0.0') ​● 코드 #4 ( Raspberry Pi에서 Clova API 활용 )  코드 #4import sysimport requestsCLIENT_ID = ""YOUR_CLIENT_ID""CLIENT_SECRET = ""YOUR_CLIENT_SECRET""API_DOMAIN = ""https://naveropenapi.apigw.ntruss.com""def stt(path):    service_endpoint = ""/recog/v1/stt?lang=Kor""    url = ""{apiDomain}{serviceEndpoint}"".format(            apiDomain=API_DOMAIN, serviceEndpoint=service_endpoint)    data = open(path, 'rb')    headers = {        ""X-NCP-APIGW-API-KEY-ID"": CLIENT_ID,        ""X-NCP-APIGW-API-KEY"": CLIENT_SECRET,        ""Content-Type"": ""application/octet-stream""    }    response = requests.post(url, data=data, headers=headers)    return responsedef tts(text):    service_endpoint = ""/voice/v1/tts""    url = ""{apiDomain}{serviceEndpoint}"".format(            apiDomain=API_DOMAIN, serviceEndpoint=service_endpoint)    headers = {        ""Content-Type"": ""application/x-www-form-urlencoded"",        ""X-NCP-APIGW-API-KEY-ID"": CLIENT_ID,        ""X-NCP-APIGW-API-KEY"": CLIENT_SECRET    }    data = ""speaker=mijin&speed=0&text={text}"".format(text=text)    response = requests.post(url, headers=headers, data=data)    if response.status_code == 200:        with open('answer.mp3', 'wb') as f:            f.write(response.content)    return response ​※참고 1. Rasppberry Pi에서 음성 녹음하기  Recording Audio on the Raspberry Pi with Python and a USB Microphone — Maker PortalYou can elect to purchase the USB Microphone from our store, or below in the list of links for USB microphones and sound cards compatible with the Raspberry Pi. Depending on the application, the user may want to buy a nicer microphone (higher bit-depth, larger dynamic frequency range, higher samplinmakersportal.com  2. Raspberry Pi에서 오디오 재생하기 Adafruit Speaker Bonnet for Raspberry PiThis Bonnet uses I2S a digital sound standard, so you get really crisp audio. The digital data goes right into the amplifier so there's no static like you hear from the headphone jack. And it's super easy to get started. Just plug in any 4 to 8 ohm speakers, up to 3 Watts, run our installer script o...learn.adafruit.com ​​​ ​ "
[딥 러닝(DL) 개요] - 3. 인공 신경망(ANN) - 2 ,https://blog.naver.com/dev_jun97/223004093645,20230203,"1. 인공 신경망의 적용 인공 신경망(ANN)은 이미지 처리, 음석 인식, 자연어 처리 등 다양한 분야에서 광범위한 응용분야를 가지고 있다. 이미지 처리에는 객체 인식, 이미지 분류, 분할과 같은 작업에 사용될 수 있고, 음성 인식에서는 음성을 텍스트로 변환하는데 사용된다. 자연어 처리에서는 언어 번역, 감정 분석, 텍스트 분류와 같은 작업에 사용된다.​ 이미지 처리(Image Processing) 객체 인식, 이미지 분류 및 얼굴 인식과 같은 이미지 처리 응용프로그램에 널리 사용된다. 인공 신경망(ANN)은 이미지 내의 물체나 특징을 식별하도록 훈련될 수 있어 보안 시스템, 자율 주행 차량 및 의료 영상과 같은 애플케이션에 사용하기에 이상적이다.​ 음성 인식(Speech Recognition) 음성-텍스트 변환 및 스피커 인식과 같은 음성 인식 응용 프로그램에서도 사용된다. 인공 신경망(ANN)은 음성 패턴을 인식하도록 훈련되어 음성을 텍스트로 정확하게 전사하거나 화자를 식별할 수 있다.​ 자연어 처리(Natural Language Processing, NLP) 기게 번역, 감정 분석, 텍스트 분류와 같은 다양한 자연어 처리 응용 프로그램에 사용된다. 인공 신경망(ANN)은 인간 언어를 이해하고 처리하도록 훈련될 수 있어 가상 비서와 고객 서비스 챗봇과 같은 애플리케이션에서 사용하기에 이상적이다.​ 예측 분석(Predictive Analytics) 주식 시장 예측, 일기 예보, 고객 행동 분석과 같은 예측 분석 애플리케이션에 사용된다. 인공 신경망(ANN)은 많은 양의 데이터를 분석하도록 훈련되어 미래의 추세와 패턴에 대한 예측을 할 수 있다.​ 게임(Gaming) 게임 인공지능과 게임 추천 시스템과 같은 게임 애플리케이션에도 사용된다. 인공 신경망(ANN)은 게임의 상태와 다른 플레이어의 행동에 기초하여 결정을 내리도록 훈련받을 수 있다.​인공 신경망(ANN)의 사용은 빠르게 확장되고 있으며, 새로운 애플리케이션은 항상 개발되고 있다. 인공 신경망(ANN)은 우리가 기술과 상호작용하는 방식에 혁명을 일으킬 수 있는 잠재력을 가지고 있어 기술을 더 빠르고, 더 정확하고, 모든 사람이 더 쉽게 접근할 수 있게 만든다.​ 2. 인공 신경망의 과제와 한계 인공 신경망(ANN)은 많은 애플리케이션을 위한 강력한 도구임이 입증되었지만 극복해야 할 몇 가지 과제와 한계가 있다. 첫번째 주요 과제는인공 신경망(ANN)이 훈련 데이터에 너무 전문화되어 인공 신경망(ANN)이 새로운 데이터로 일반화하기 어려울 때 발생하는 과적합이다. 두번째 과제는 인공 신경망(ANN)을 훈련하는 데 사용되는 데이터의 품질과 양이다. 데이터의 품질이 나쁘거나 데이터를 충분히 사용할 수 없다면 인공 신경망(ANN)의 성능 저하로 이어질 수 있다. 인공 신경망(ANN)은 특히 대규모 데이터 세트의 경우 훈련하는데 오랜 시간이 걸릴 수 있기 때문에 훈련 시간도 제한 사항이다.​ 데이터 품질(Data Quality) 교육을 위해 많은 양의 데이터에 의존하며, 데이터 품질은 네트워크 성능에 직접적인 영향을 미친다. 결측값이 있거나 잘못된 레이블이 있는 데이터와같은 품질이 낮은 데이터는 과적합 또는 과소적합을 초래하여 네트워크의 정확도를 저하시킬 수 있다.​ 과적합(Overfitting) 과적합은 네트워크가 너무 복잡하고 훈련 데이터에서 너무 잘 훈련되어 테스트 데이터에서 성능이 저하될 때 발생한다. 이 문제는 네트워크에 너무 많은 매개 변수가 있거나 너무 오랫동안 훈련된 경우에 발생할 수 있다.​ 계산 복잡성(Computational Complexity) 훈련은 매개 변수가 많은 대규모 네트워크의 경우 계산적으로 복잡하고 시간이 많이 소요될 수 있다. 이는 특히 컴퓨팅 리소스가 제한적인 조직에서 어려운 과제가 될 수 있다.​ 해석성 결여(Lack of Interpretability) 인공 신경망(ANN)은 쉽게 해석할 수 없기 때문에 ""블랙박스(black boxes)""로 간주되어 예측을 어떻게 하는지 이해하기 어렵다. 이는 네트워크에서 생성된 예측의 배후에 있는 추론을 이해해야 하는 어려운 과제가 될 수 있다.​ 제한된 기능(Limited Functionality) 이미지 인식 또는 음성 인식과 같은 특정 작업을 위해 설계되었다. 이 의미는 규칙 기반 시스템같은 다른 AI기술에 비해 유연하지 않으며, 더 높은 수준의 이해가 필요한 더 복잡한 작업에 어려움을 겪을 수 있다.​ 데이터의 편향(Bias in Data) 훈련 데이터에 편향이 포함된 경우 ANN이 편향될 수 있다. 이는 네트워크가 편향된 예측을 하는 결과를 초래할 수 있으며, 이는 형사 사법 및 의료와 같은 분야에서 심각한 결과를 초래할 수 있다. "
OpenAI GPT-3 Powered NPCs: A Must-Watch Glimpse Of The Future (Modbox) ,https://blog.naver.com/aitutor21/223014693300,20230213,"https://www.youtube.com/watch?v=jH-6-ZIgmKY The developer of Modbox linked together Windows speech recognition, OpenAI's GPT-3 AI, and Replica's natural speech synthesis for a unique demo: arguably the first AI NPC. There is an uncomfortably long delay between asking a question and getting a response. But that's because GPT-3 and Replica are both cloud-based services. Future models running on-device could mostly eliminate the delay. Microsoft, which invested $1 billion in OpenAI, has exclusive rights to the source code & commercial use of GPT-3, so this feature is unlikely to be added to Modbox itself. "
[python]파이썬/음성인식/speechrecognition/마이크 입력/pyaudio ,https://blog.naver.com/scyan2011/222559085416,20211105,"마이크 입력 지난 포스팅에서는 음성 파일(.wav)을 문자열로 읽어보았습니다. 이번에는 마이크의 입력을 문자열로 바꾸어 볼까요? 이것을 위해서는 먼저 pyaudio 를 설치해야합니다.​​pyaudio 설치​Window 운영체계에서 pip로 pyaudio를 설치하려고 시도하면 오류가 발생하기 때문에 먼저 pipwin을 설치한 후 pyaudio를 설치하여야 합니다. 프롬프트창에 다음과 같이 입력합니다.​ pip install pipwinpipwin install pyaudio마이크를 이용하여 인식하는 sr1.py 파일을 작성합니다.​sr1.py​음성인식의  Microphone 객체를 이용하여서 오디오를 읽고 역시 recognize_google()함수로 문자열로 전환합니다. 첨부파일sr1.py파일 다운로드 import speech_recognition as sr # 음성인식 객체 생성r = sr.Recognizer()with sr.Microphone() as source:    # 마이크로부터 오디오 읽기    audio_data = r.record(source, duration=5)    print(""인식중......."")    # 음성을 문자열로 전환    text = r.recognize_google(audio_data)    print(text) 실행​실행 후 마이크에 음성을 입력하면 인식중... 이후에 문자열이 출력됩니다.  ​ "
[EO리뷰] 영어회화 어플 스픽 Speak | ChatGPT의 OpenAI의 $30M / 390억원 규모의 투자를 받다. ,https://blog.naver.com/sj_jennypark/223024980107,20230223,"아래 내용은 EO인터뷰 영상을 요약한 내용입니다.출처: https://youtu.be/63FWL7KcWrE 영어공부 어플 한국 국내시장 1위, Speak 스픽The future of language learningSpeak is a mobile app that helps people practice speaking Englsih.We use AI to replace a human, but build an experience that feels like you're talking to a real live human tutor. 스픽은, 인간을 대체할 수 있는 인공지능 기술을 활용하지만 이용자들은 마치 인간/사람에게 언어교육을 받는 것처럼 느껴지도록 한다.​아이폰과 비슷한, 손바닥만한 크기의 메모카드를 보고 사업 아이디어를 떠올린 Connor (스픽 공동창업자)는 청소년 시절 Flashcard+ 플래시카드 플러스 라는 앱을 개발하여 첫 번째 회사를 설립하게 된다.​ He thought the biggest impact that it had for him was that it allowed him to not have to worry or optimise around making money, but allowed him the freedom to really just like focus on thinking about what he wanted to pursue from a passion perspective. 플래시카드플러스 앱을 만들 당시로서 그는 이윤창출에 대해 걱정하거나, 이윤창출을 위해 어플을 최적화 할 필요가 없었기 때문에, 오로지 그의 열정적인 관점에서 볼 때에 그가 진정으로 추구하는 것이 무엇인지에 대해 집중해서 고심해볼 수 있었던 것이 그에게는 가장 큰 영향이었다고 말한다.그리고 이로 인해, Connor는 1년간 휴학하며 공동 창업자와 함께 AI 인공지능에 관한 리서치에만 몰두하게 된다.​스픽의 또 다른 공동창업자인 Andrew는 특이한 교육경로를 밟게 되었는데, 남들보다 뭐든지 빠르게 해냈던 그는 부모님의 결정으로 홈스쿨링을 하게 되고, 12살 전에 고등교육까지 마무리 짓게 된다. 12살의 나이에 워싱턴 대학교에 입학하여 생물화학과 신경생물학을 4년간 공부한 뒤, 스탠포드대학에서 신경과학 박사과정을 밟게 된다. 3.5년의 박사과정 이수 중 그는 스타트업을 시작하기 위해 하던 공부를 중단하게 된다. ​Connor와 Andrew는 회사를 설립하기 전까지 수많은 시간동안 룸메이트였고, 그들은 무엇을 할지는 몰랐지만, 한가지 확실한 건 AI 라는 것에 대해 매우 궁금해했다. 그래서 1년간 머신 러닝에 대해 깊이 공부하기 시작한다. 관련된 자료를 모두 읽고, 연구하고, 관련 수업들을 들으며 머신 러닝은 모든 것을 바꿀 수 있다는 것을 깨닫게 된다. 그리고는 다양한 문제를 해결하기 위해 수많은 알고리즘을 생성하고, 특별히 ""음성인식 Speech Recognition"" 에 매우 큰 흥미를 느끼게 된다. 유투브에서 랜덤으로 데이터를 이용하여 사람들이 말하는 것 뿐만 아니라, 억양, 어조 등 까지 이해하는 정확한 음성 인식 시스템을 개발한다. ​처음 시작할 당시 그들은 사람들이 어떤 방식으로 언어를 배우고 싶은지에 대해 조차 알지 못했다. 그저 그들은 본질적으로 그저, 많은 개념을 구성하고, 가능한 많이 배우고, 최대한 많은 이용자들에게 노출되려 노력했지만 필연적으로 잘 되지 않았다. 그래서 다시 처음으로 돌아가 다시 구상하고, 계속적으로 도전을 반복했다. ​스픽을 만들고 첫 몇 년은 Product market fit (: 좋은시장에서 그 시장을 만족시킬 수 있는 제품을 갖는 것) 을 개발하기 위해 많은 고난을 겪어야 했다. 수없이 많은 제품을 런칭하고 시도했고, 사람들은 좋아했지만, 첫 30일 이후로는 잘 이용하지 않았다. 고민의 날들은 계속되었고, 그들은 전세계적으로 사업을 굴리고 싶었다. 그러나, We realised that if we wanted to build something people love, we would need to pick a single market and start there. 사람들이 열광할 만한 것을 만드려면, 하나의 시장을 선택하여 거기서부터 시작해야함을 깨달았다.그리고 여러 나라를 돌저 유저 테스트를 실시했다. 그 중 한국은 GDP의 1퍼센트를 영어를 배우는데 소비하는 나라였고, 실제로 서울에는 영어교육/영어관련 학원, 기업들이 매우 많았다. 한국 사람들은 듣고 이해하는 부분에 비해 회화가 약했다. 한국 시장의 사람들은 다른 시장들에 비해  2-3배로 영어공부에 많은 돈을 쓰는 시장이었으며, 매우 활기찬 시장이었다. 한국에서 통한다면, 다른 어느 곳에서든지 통할 것이라 생각했다.  It's about building an experience that is sufficiently good to outcompete all the other choices that someone has in their life at that time. (자유시간에 인스타그램, 유투브, 헬스장, 산책 등 즐길 수 있는 옵션이 너무나도 많기 때문에) 충분히 좋아서 다른 옵션들을 능가하여 그 경쟁에서 이길만한 경험을 만들어내는 것에 대한 것이다.사람들이 우리 제품을 잘 이용하지 않았던 이유는, 우리 제품은 휴대폰에 대고 말을 해야 하는 서비스였는데 대부분의 이용자들은 통근 중에 지하철이나 버스에서 이용하고 싶어했기 때문이다. 그리고 그 시간에 사람들의 습관이 생성된다. 우리는 계속 질문했고, 사람들을 관찰했다.​ The more you improve the product market fit, the faster you typically grow. We have to always try to improve the product market fit.프로덕트 마켓핏을 향상 시킬 수록, 더 빠르게 성장할 수 있으며 계속적으로 향상 시키기 위해 노력해야 한다.가치있는 서비스를 만들기 위한 중요한 요소들Main components to making a really valuable service1. 머신러닝새로운 서비스를 만들기 위해 계속적으로 훈련하고, 새로운 모델을 만들고 있으며 머신러닝은 다루기 어렵지만, 완전한 경험을 위한 힘을 공급해준다. The power the entire experience and we're constantly training and building new models to build new features.​2. 최대한 빨리, 많이, 유용하고 매력적인 새로운 서비스들을 계속적으로 선보이는 것.Continuing to ship as many new product features as possible, as quickly as possible, too build the most appealing, useful product.​3. 사람들이 열광할 만한 컨텐츠를 담는 것.Building content that people love.We A,B test it and we're constantly iterating it and fixing it. 우리는 A/B테스트를 꾸준히 반복하며, 수정하고 개선해나간다. ​​ ​ "
ML lec12: NN의 꽃 RNN 이야기 ,https://blog.naver.com/chanmuzi/222838097159,20220803,"1. Introduction​- Sequence data ; We don't understand one word only ; We understand based on the previous words + this word. (time series) ; NN/CNN cannot do this ; 이전의 연산이 그 다음 연산에 영향을 주는 자료 형태를 처리할 필요가 있다.  ; h 대신 y로 표기하는 것이 옳다.​​​2. Recurrent Nerual Network 이전의 연산이 다음 연산에 영향을 주는 구조 ; 현재 state를 계산하기 위해 이전 state를 변수로 사용한다.​ ; Notice: the same function and the same set of parameters are used at everty time step.   같은 함수와 같은 세트의 파라미터가 매 step마다 쓰이는 것이다.​​- (Vanilla) Recurrent Nerual Network ; The state consists of a single ""hidden"" vector h:  ; 이전 모델들과 마찬가지로 가중치 곱하기 변수, 즉 WX 형태를 그대로 취하고 있다.​ ; RNN 에서는 sigmoid 대신 tanh 함수를 사용한다.​ ; y를 구할 때는 계산된 ht에 또다른 가중치를 곱해준다. 이때도 마찬가지로 기존의 WX 형태를 취한다.​​  ; sequence를 이해하고 다음에 어떤 것이 올지 예측하고자 한다. 여기서는 다음 글자가 무엇일지 예측하는 것이다.​  ; 우선 글자를 컴퓨터가 인식할 수 있는 벡터 형태로 변환해야 한다. 적용할 수 있는 여러 방법 중 여기서는 one-hot vector 를 적용했다.​ ; input lyaer의 vector를 hidden layer에 연산을 통해 전달한다. 이때 사용되는 식은 다음과 같다.  ; 알파벳 'h'에 대한 vector를 hidden layer로 보낼 때는 이전의 값이 존재하지 않기 때문에 이전의 값을 0이라고 가정하고 계산하게 된다. 이후 다른 알파벳들은 기존의 연산과 합친 값을 계산하게 된다.​ ; hiddent layer의 값들에 연산을 수행해 y의 값을 구하게 된다. 이때 사용되는 식은 다음과 같다.  ; output lyaer에 존재하는 값들에 대하여 softmax를 적용하여 가장 큰 값 하나만 고르게 되면 다음 값을 예측할 수 있게 된다.​​​3. RNN applications​https://github.com/TensorFlowKR/awesome_tensorflow_implementations​- Language Modeling- Speech Recognition- Machine Translation- Conversation Modeling/Question Answering- Image/Video Captioning- Image/Music/Dance Generation​​- Reucrrent Networks offer a lot of flexibility  ; Image Captioning ; Sentiment Classification ; Machine Translation ; Video Classification​​- Multi-Layer RNN ​- Training RNNs is challenging ; Several advanced models   - Long Short Term Memory (LSTM)   - GRU by Cho et al. 2014​​출처: https://www.youtube.com/watch?v=-SHPG_KMUkQ&list=PLlMkM4tgfjnLSOjrEJN31gZATbcj_MpUm&index=41 "
Self-supervised learning of audio and speech representation [1/4] ( KAIST 정준선 교수님 ) / 음성 인식이란? ,https://blog.naver.com/gypsi12/222997279535,20230128,"제가 지금까지 군생활을 하면서 가장 잘했다고 생각하는 것은데일 카네기의 인간관계론이라는 책을 읽은 것입니다.사람이 무엇을 원하는지 확실히 알게 되었기 때문입니다. ​사람은 모두 인정받기를 ""갈망"" 합니다.좋은 것으로는 부족합니다. ""갈망"" 합니다.​나의 존재가 인정받기를 원해서 우리는 자신을 다양한 방식으로 표현합니다.그 중에서 가장 쉽고 효과적인 수단은 ""말하기"" 입니다.그래서 사람들은 자신의 이야기를 ""들어주는"" 사람을 좋아합니다.나의 존재가 빛날 수 있도록 나를 하늘 높이 들어주는 사람을 좋아하는 감정은 자연스러운 것이니까요.​저의 목표는 친구같은 인공지능을 만드는 것이라고 말해왔습니다.하지만, 정확히 말하자면 진심을 다해 ""들어주는"" 인공지능이라는 생각이 듭니다.​이번 포스트는 음성인식분야입니다.  KAIST 정준선교수님의 SessionSelf-supervised learning of audio and speech representation을 듣고 정리해봅니다. ​  목차# 음성인식 소개# 음성인식이 어려운 이유# 모델 INPUT ( MFCC ) 음성인식 소개사람은 여러가지 modality와 감각을 사용해서 주변을 인식한다.즉, 시각, 청각,후각,촉각, 미각 등 여러가지 Sense를 사용해서 상황을 인지후에 행동을 한다.그 중에서 많이 쓰는 청각,시각을 살펴보자면제스쳐,리딩,speach를 통해서 Comunication을 주로 한다.​사람들은 대부분 언어를 사용해서 의사소통을 한다.언어 중에서도 대화를 통해 Comunication을 한다.​컴퓨터와 사람이 대화를 하기 위해서는 어떻게 해야 할 까?""음성"" 을 사용해 대화할 수 밖에 없다.​​​음성인식 분야는 2가지 메인 분야가 존재한다.​ASR ( 음성 인식) : 무엇을 말하고 있는지 인식하는 것이 목표그래서 Speach 신호를 받으면, 그에 해당하는 Text를 내뱉는 것​화자인식 : 무엇을 말했는지가 아닌 누가 말했는지 인식하는 것​​이번 포스트에서는 대부분 ASR에 대해 말할 것이다.ASR을 요약해보면 Speach를(소리)를 Text로 바꾸는 과정이라 할 수 있다.​시작에 앞서 이 작업이 왜 어려운 일인지를 소개해보려 한다.  음성인식이 어려운 이유see, seahole, wholeright, write 등​이 단어를 음성으로 구분하는 것은 매우 어렵다.비슷한 소리를 가지고 있는 다른 단어들이 굉장히 많다!​뿐만 아니라, 환경적인 특성에 영향을 많이 받는다.노이즈, 소리 울림등...​음성인식을 하기 위해서는굉장이 많은 어휘를 알아야 한다.일반 성인 어휘는 수천~수만개정도이다.그래서 성인 수준의 음성 이해를 가능하려면 저정도의 어휘를 인지할 수 있어야 한다.​+ 같은 단어라도 해도사람이 다른 의미로 이야기 할 수 있다서울,경상도, 전라도 말투가 다르듯이사람마다 이 언어를 다르게 구현하기 때문에같은 사람의 경우에도 감정에 따라같은 문장이라도 다르게 말하기 때문에 음성 인식이라는 것은 매우 어려운 문제이다!+ 음성 자체 말고 기계학습(Machine Learning) 측면에서도 음성인식은 Challenge 가 매우 많다​​​먼저,Input 길이가 매우 길다!nlp를 생각해보면, Imput 길이가 단어의 개수로 사용하는 경우가 많은데5~10개 수준으로 가능한 정도이다.하지만, 음성 인식의 경우 음성 input  1초당 최소 16000개, 많으면 44000개가 들어간다;;그래서 10초의 음성이라 하더라도 input이 수만개가 들어가기 때문에 매우 길다​​nlp의 같은 경우도 마찬가지지만,input이라하면 음성 input이고 output이라면 Word Sequence일텐데input,output 길이가 sample마다 계속 변한다!이미지 같은 경우 고정된 크기의 이미지를 반환하지만음성의 경우 1초짜리가 들어갈 수 도 있고 10초짜리가 들어갈 수도 있다.​​Image recognition같은 경우 똑같은 사물에 대해서는 어디에서 이미지를 수집하던지에 영향을 받지 않는다.하지만, 음성 인식의 경우 데이터가 많은 언어(ex English,Chinese...)가 아닌 이상 특정 언어 data를 얻으려면그 나라의 사람들에게서부터 얻을 수 밖에 없어 다량의 데이터를 확보하는 것이 힘들다.​  ## 이런 어려운 문제를 왜 풀어야 하는가?​음성은 우리 주변에서 아주 많이 사용되고 있다.음성인식 기술이 들어간 제품이 상당히 많다. 하지만 하는 사람이 많지는 않다.굉장이 여러가지를 할 수 있고 삶을 편안하게 하는 기술이다.​시리, 스마트스피커, 음성인식 기기, 회의록 작성, 화자 인식을 통한 보안인증 같은 기술이미래의 기술이 아니라 현재의 기술이다!​이미 많이 사용되고 있는 기술이고  시장이 큰 필요한 기술이기에 이러한 어려움에 불구하고도 연구할 필요가 있는 가치있는 분야이다.​​음성 인식을 어떻게 하는지 시작하기 전문제를 정의하고 넘어가자.​ 음성 인식 : Sound를 Text로 바꿔주는것input : audio를 feature vector로  -> 좀있다가 설명output : word/sentence/character level sequence주로 character 단위 sequence를 사용​소리 X 가 주어졌을 때, 가장 확률이 높은 정답 TEXT를 구하는 것이 목표기존 Computer vision, NLP 와 마찬가지로 딥러닝을 많이 사용한다.​Deep learning은 output = f(input)인 f를 학습하는 것으로 간단하게 표현 가능한데audio도 마찬가지로 보면 된다.  모델 INPUT​아까 Input으로 feature vector를 사용한다고 했는데이 vector는 feature extraction을 통해 얻는다​다른 Deep learning Domain에서는 요즘 사용하지 않는 기술이긴 하다음성은 특이하게마이크에서 잡히는 Audio를 바로 집어넣지 않고다듬어진 feature를 많이 사용하는 중이다.​음성을 모델의 input으로 사용할 수 있는 방법은 여러가지 방법이 있다.​마이크에서 녹음된 정보를 디지털화한 것Spectrogram, mfcc 등은 사람이 만든 feature이다.​Vision 분야에서는 sift나 Hog라는 이미지 feature 축출 기법을 사용하였다. 요즘에는 feature 를 축출하기 보다는 RGB 이미지(channel을 3개 사용)를 바로 사용한다.​음성 인식에서는 아직사람이 만든 feature 를 사용하는 쪽이 성능이 더 좋게 나와 아직까지 사용하는 중이다.물론, 예외도 있다.FaceBook(Meta) 쪽에서 나온 논문들을 보면 이 사람이 만든 feature 를 사용하기 보다raw 음성을 바로 사용하는 편이다. ( 하지만 이들 외에 다른곳에서는 사람이 만든 feature를 사용하는 추세이다.)​​좋은 feature란 무엇일까? feature 안에 충분한 정보가 담겨있어야 좋은 feature라고 말할 수 있을 것이다.​그러면 생각해야 할 점이 하나 있는데,음성 인식을 함에 있어서는 화자 정보를 분리하는 것이 좋다!=> 음성인식은 무엇을 말하는 지를 알아내는 것이 중요한 것인데 사람마다 가지고 있는 목소리의 특성을 배우게 되면 문제가 생긴다. 이는 불필요한 정보이다.​그래서 Feature의 조건이라 한다면,Noise와 주변 환경 소리에 대해 강인해야 하고음성 인식후 내뱉는 output의 차원이 충분히 낮아야 한다.(input이 16000개인 것에 비해 낮음)​이런 특성을 잘 만족하는 feature가 MFCC라는 것이다.  MFCC(Mel-Frequency Cepstral Coefficient)https://arxiv.org/ftp/arxiv/papers/1003/1003.4083.pdf1. 마이크에서 아날로그 신호를 디지털 신호로 변경​2. Pre-Emphasis​사람 목소리를 살펴보면, 낮은 주파수대의 음성이 높은 주파수대의 음성보다 에너지가 많다.하지만, 높은 주파수대에는 정보량이 많다.이 정보를 잘 얻어내기 위해서 높은 주파수대를 Boosting해줄 필요가 있는데이때, high-pass fiter(높은 주파수대가 잘 통과되는 필터 -> 낮은 주파수는 잘 통과되지 않음) 를 통해fiter후에 얻은 정보는 낮은 주파수대의 영향력을 낮춤으로써 높은 주파수대를 더 잘 인식할 수 있게 된다.​3. Sampling and Windowing​마이크에 들어온 압력을 시간에 따라 표현하게 되면Time(시간)에 따른 Emplitude(압력) 정보를 얻게 될텐데,사실 어떤 말을 하고 있는지 인식하기 위해서 중요한 것은 Emplitude(진폭)보다는Frequency(주파수)이다!​그래서 이를 시간당 주파수형태로 얻어내기 위해 Windowing을 진행한다.​ Window의 크기는 매우 민감한 hyperparameter이다.너무 작게 잡으면 의미있는 정보를 온전히 담을 수가 없고너무 크게 잡으면 여러가지 소리가 뭉쳐져서 해석하기 너무 복잡해진다.​위와 같은 문제들을 고려하여Window의 크기는 20~40ms 단위로 자른다. ( 보통 25ms로 많이 사용 한다.)​​4. Fast Fourier Transform​Fourier Transform 은 신호처리에서 쓰이는 기법인데Time(시간)에 따른 Emplitude(진폭) 변화를Frequency(주파수)에 따른 Emplitude(진폭) 변화로 나타낼 수 있게 된다.(이를 통해 특정 시간 내에 존재하는 다양한 Frequency(주파수)를 가진 신호들 중에원하는 주파수 영역만 남기고 나머지는 제거한 다음 원래대로 되돌리게 되면특정 주파수를 없에버리는 필터 역할을 할 수 있다.)​Fast Fourier Transform은 Fourier Transform을 빠르게 하는 알고리즘이다.​​​5. Mel Filter Bank ​사람의 귀는 고주파수 영역보다 저주파수 영역을 더 잘 해석한다.그래서 모델에서도 이 특성을 반영하여 저주파 성분 영향력 낮추기 및 고주파 성분 영향력 키우기를Mel Filter Bank에서도 해준다.​ ​이 삼각형 영역에 해당하는 주파수 영역의 값들을 통과시키는 것이다.저주파 성분들은 삼각형이 작아서 조금만 통과되고고주파 성분들은 삼각형이 커서 많이 통과된다.그리고 이런 삼각형을 N개 만들어서 통과시키면 Mel-Spectrogram이 된다. Mel-Spectrogram​​6. Discrete Cosine Transform​​마지막으로 이미지 압축 기법인 Dicrete Cosine Transform을 통해Mel-Spectrogram에 존재하던 주파수간의 Correlation을 끊어준다.​이 압축 기법은 Spectrogram을 주파수별로 분리시킨 다음에 해가 되거나 불필요한 주파수를 제거한다.이를 통해서 각 주파수간에 얽혀있는 Correlation(상관관계)를 끊어낼 수 있다고 한다.​음성에서는 화자에 따라 달라지는 주파수 feature 인 f0 (fundermantal frequency)를 제거하여feature 하나하나가 화자 독립적이게 된다.​​이런 과정을 통해 Mel-Spectrogram에 DCT까지 거치고 나면 비로소 MFCC가 된다.​이때 DCT를 거치고 나서 손실 압축하여 Correlation을 끊고이후 DCT 역변환을 통해 다시 되돌린 값을  MFCC Coefficients라고 하는데 그때 얻은 각 Time(시간)당  MFCC Coefficients값을그 Time(시간)의 음성 벡터로 사용한다. MFCC​| Mel-Spectrogram vs MFCCMel-Spectrogram : 주파수간에 Correlation이 녹아있어서 특정 영역에서 더 잘 동작할 수 있음.MFCC : 주파수간에 Correlation이 없어서 일반적인 상황에 두루두루 잘 동작함. 즉, 강인하다.​  [이미지 압축에 대한 아주 좋은 글]이미지의 디지털화이미지와 주파수 관계DCT(Discrete Cosine Transform)와 DFT(Discrete Fourier Transform)   ​​음성 Input에는 최대한 벡터에 정보를 많이 녹아내고 학습 가능하게 만들기 위한 기법들이 사용횐다.​- Dynamic Feature를 추가한다. : 시간 구간을 적당히 나누어서 그 구간에서 Feature가 어떤 값을 가지고 있는지 뿐만 아니라,Feature가 어떻게 변화하고 있는지(커지는지 작아지는지 등)에 대한 정보를 Feature에 최대한 녹아낸다.​​마지막으로 Mean, Various Normalization을 통해 모델의 input으로 사용하기 적당하게 다듬는다.  다음 포스트에서는 이 INPUT을 넣을 수 있는다양한 모델과, 기발한 작업들을 소개할 예정입니다.​​ "
"Stop the ""long face"" [영어] ",https://blog.naver.com/dychoe80/222745791011,20220525,"재밌는 영어 표현이 있어 소개한다. So stop the long face and let's spend sometimes on it.Speech Recognition — GMM, HMM. Before the Deep Learning (DL) era for… | by Jonathan Hui | MediumSpeech Recognition — GMM, HMM. Before the Deep Learning (DL) era for… | by Jonathan Hui | Medium이런 문맥에서 쓰인 표현이다.​However, ..., HMM remains important. But regardless of the status, .... So stop the long face and let’s spend sometimes on it.""음성인식에서 딥러닝이 HMM을 대체하고 있다고 해도, HMM을 이해하는 게 중요해. 그러니 울상 그만 짓고 HMM에 대해 알아보자.""  ""Stop the long face.""는 우리말로 ""울상 그만 지어라.""로 해석할 수 있다. 울상과 얼굴이 긴 거는 무슨 관련이 있을까?​기분이 안 좋을 때 표정이 아래로 늘어진다. 눈꼬리와 입꼬리가 내려가서 마치 얼굴이 길어지는 거처럼 보인다고 해서 이렇게 표현한듯하다.​반대로 웃으면 얼굴이 넓어진다고 표현할 수도 있겠다. 그렇지만 wide face라고 하진 않는다. From smiles to long faces물론, long face는 물리적(?)으로 긴 얼굴을 나타내기도 한다.  He came to me with a very long face.그는 울상을 지으며 다가왔다.​why the long face?왜 울상이야?​Why do you have a long face today?오늘 왜 그렇게 울상이야?​Stop the long face.울상 그만 지어.​Don't pull a long face.울상 짓지 마.  이제 이 유머를 보면서 웃을 수 있다. Idiom: Long face | Learn English (ecenglish.com) 학창 시절 선생님이 뾰로통한 표정 짓는 학생들한테 자주 하시는 말씀이 있었다.​""입 넣어!""​영어로 얼굴이 길다는 표현은 우리말로 입을 삐쭉 내밀고 있는 것과 비슷할 거 같다.  참고 Where The Phrase Why The Long Face Might Have OriginatedOne of the challenges of learning a language is coming to understand, and use correctly, specific turns of phrase -- idioms, some of which are ancient.www.grunge.com Idiom: Long face | Learn EnglishHome Idiom: Long face Poor Okay Good Great Awesome Average: 3.6 ( 13 votes) Sun, 05/08/2011 - 07:54 — Chris McCarthy Idioms Cartoon This month's joke is based on the expression long face . As you know, a horse, compared to a human, has a long face! We can use long face to describe someone's physical...www.ecenglish.com "
[유망산업] 글로벌 스마트 AR 글라스 (Smart Augmented Reality Glasses) 시장조사보고서 - QYResearch(QY리서치) ,https://blog.naver.com/qyresearch-korea/222892076933,20221005,"#QYResesarch(QY리서치) 발간 <글로벌 스마트 AR 글라스 (Smart Augmented Reality Glasses) 시장조사 보고서>를 소개합니다. ​스마트 글라스 시장은 가상현실(VR), 증강현실(AR),  혼합현실(MR), 확장현실(XR) 등으로 분류됩니다. 동 보고서는 #증강현실( Augmented Reality, AR) 스마트 글라스 시장을 집중 분석하고 있습니다.​스마트 #AR글라스는  투명한 렌즈 위에 증강현실(AR) 콘텐츠를 구현하여 현실 세계를 바탕으로 각종 디지털 콘텐츠와 정보를 투사시켜 함께 볼 수 있도록 제공하는 #웨어러블 기기입니다. ​가상세계의 정보와 현실 세계를 연결하여 현실 세계를 확장해 주므로 메타버스(Metaverse)의 개념을 실현 가능하게 해줄  혁신적인 기기로 교육, 의료, 제조공장 등 산업 전반에 활용될 수 있습니다.  최근에는 실용적이고 캐주얼하게 착용할 수 있도록 개발되고 있습니다. ​ 출처 : Googlex​스마트 AR 글라스 (Smart Augmented Reality Glasses)의 글로벌 메이저 기업은  CastAR, Epson, Googlex, Sony, Microsoft, AltoTech, Laster, Lumus, ODG 등입니다.​국내기업으로는 #삼성전자가 AR 글라스 출시를 예정하고 있으며,  #프라젠(PRAZEN), #피앤씨솔루션(P&C Solution), #이랜텍(Elentec) 등 다수의 기업이 개발 및 상용화를 전개하고 있습니다. ​​ 제품 – PRAZENOur Products AR Glasses Compact AR glasses with boarder FoV 70 degrees enabling immersive AR works with less fatigue in industrial and consumer application   AR Projector Large interactive AR display whenever we want big display enabling smart home control, AR smart menu system, AR education, signag...prazen.co (주)피앤씨솔루션(주)피앤씨솔루션pncsolution.co.kr 피앤씨솔루션, KAI와 경량형 스마트 글라스 개발 위한 MOU - 머니투데이XR(확장현실) 전문기업 피앤씨솔루션이 최근 한국항공우주산업(KAI)과 경량형 스마트 글라스 개발을 위한 업무 협약(MOU)을 체결했다고 30일 밝혔다.양사는 경량형...news.mt.co.kr Elentec :: Electronics & Technology휴대용 기기의 Battery Pack, 3D TV용 안경, PCM, 기타 SET OEM 제품 등을 생산www.elentec.co.kr ​동 보고서는 글로벌 스마트 AR 글래스 (Smart Augmented Reality Glasses) 시장을 아래와 같이 세분화 (Segmentation) 하고 있습니다. ​▶ 제품 유형별​Speech RecognitionGesture RecognitionEye Tracking ​​▶ 응용분야별​BusinessIndustriesHealthcareRecreationOther ​​▶ 지역별​​북미, 유럽, 아시아태평양, 남미, 중동 아프리카 등 (주요 국가 포함)​​​▶ 글로벌 키 플레이어로 아래 기업을 집중 분석하고 있습니다.​CastAREpson GooglexSonyMicrosoftAltoTechLasterLumusODGPenny ABReconSix15 TechnologiesTheiaVuzix​​▶ 세부 내용은 아래 목차를 참고 바랍니다.​GLOBAL Smart Augmented Reality Glasses MARKET REPORTHISTORY AND FORECAST 2017-2028, BREAKDOWN DATA BY MANUFACTURERS, KEY REGIONS, TYPES AND APPLICATION​​​Table of Contents​1 Smart Augmented Reality Glasses Market Overview1.1 Smart Augmented Reality Glasses Product Overview1.2 Smart Augmented Reality Glasses Market Segment by Type1.2.1 Speech Recognition1.2.2 Gesture Recognition1.2.3 Eye Tracking1.3 Global Smart Augmented Reality Glasses Market Size by Type1.3.1 Global Smart Augmented Reality Glasses Market Size Overview by Type (2017-2028)1.3.2 Global Smart Augmented Reality Glasses Historic Market Size Review by Type (2017-2022)1.3.3 Global Smart Augmented Reality Glasses Forecasted Market Size by Type (2023-2028)1.4 Key Regions Market Size Segment by Type1.4.1 North America Smart Augmented Reality Glasses Sales Breakdown by Type (2017-2022)1.4.2 Europe Smart Augmented Reality Glasses Sales Breakdown by Type (2017-2022)1.4.3 Asia-Pacific Smart Augmented Reality Glasses Sales Breakdown by Type (2017-2022)1.4.4 Latin America Smart Augmented Reality Glasses Sales Breakdown by Type (2017-2022)1.4.5 Middle East and Africa Smart Augmented Reality Glasses Sales Breakdown by Type (2017-2022)​2 Smart Augmented Reality Glasses Market Competition by Company2.1 Global Top Players by Smart Augmented Reality Glasses Sales (2017-2022)2.2 Global Top Players by Smart Augmented Reality Glasses Revenue (2017-2022)2.3 Global Top Players by Smart Augmented Reality Glasses Price (2017-2022)2.4 Global Top Manufacturers Smart Augmented Reality Glasses Manufacturing Base Distribution, Sales Area, Product Type2.5 Smart Augmented Reality Glasses Market Competitive Situation and Trends2.5.1 Smart Augmented Reality Glasses Market Concentration Rate (2017-2022)2.5.2 Global 5 and 10 Largest Manufacturers by Smart Augmented Reality Glasses Sales and Revenue in 20212.6 Global Top Manufacturers by Company Type (Tier 1, Tier 2, and Tier 3) & (based on the Revenue in Smart Augmented Reality Glasses as of 2021)2.7 Date of Key Manufacturers Enter into Smart Augmented Reality Glasses Market2.8 Key Manufacturers Smart Augmented Reality Glasses Product Offered2.9 Mergers & Acquisitions, Expansion​3 Smart Augmented Reality Glasses Status and Outlook by Region3.1 Global Smart Augmented Reality Glasses Market Size and CAGR by Region: 2017 VS 2021 VS 20283.2 Global Smart Augmented Reality Glasses Historic Market Size by Region3.2.1 Global Smart Augmented Reality Glasses Sales in Volume by Region (2017-2022)3.2.2 Global Smart Augmented Reality Glasses Sales in Value by Region (2017-2022)3.2.3 Global Smart Augmented Reality Glasses Sales (Volume & Value), Price and Gross Margin (2017-2022)3.3 Global Smart Augmented Reality Glasses Forecasted Market Size by Region3.3.1 Global Smart Augmented Reality Glasses Sales in Volume by Region (2023-2028)3.3.2 Global Smart Augmented Reality Glasses Sales in Value by Region (2023-2028)3.3.3 Global Smart Augmented Reality Glasses Sales (Volume & Value), Price and Gross Margin (2023-2028)​4 Smart Augmented Reality Glasses by Application4.1 Smart Augmented Reality Glasses Market Segment by Application4.1.1 Business4.1.2 Industries4.1.3 Healthcare4.1.4 Recreation4.1.5 Other4.2 Global Smart Augmented Reality Glasses Market Size by Application4.2.1 Global Smart Augmented Reality Glasses Market Size Overview by Application (2017-2028)4.2.2 Global Smart Augmented Reality Glasses Historic Market Size Review by Application (2017-2022)4.2.3 Global Smart Augmented Reality Glasses Forecasted Market Size by Application (2023-2028)4.3 Key Regions Market Size Segment by Application4.3.1 North America Smart Augmented Reality Glasses Sales Breakdown by Application (2017-2022)4.3.2 Europe Smart Augmented Reality Glasses Sales Breakdown by Application (2017-2022)4.3.3 Asia-Pacific Smart Augmented Reality Glasses Sales Breakdown by Application (2017-2022)4.3.4 Latin America Smart Augmented Reality Glasses Sales Breakdown by Application (2017-2022)4.3.5 Middle East and Africa Smart Augmented Reality Glasses Sales Breakdown by Application (2017-2022)​5 North America Smart Augmented Reality Glasses by Country5.1 North America Smart Augmented Reality Glasses Historic Market Size by Country5.1.1 North America Smart Augmented Reality Glasses Sales in Volume by Country (2017-2022)5.1.2 North America Smart Augmented Reality Glasses Sales in Value by Country (2017-2022)5.2 North America Smart Augmented Reality Glasses Forecasted Market Size by Country5.2.1 North America Smart Augmented Reality Glasses Sales in Volume by Country (2023-2028)5.2.2 North America Smart Augmented Reality Glasses Sales in Value by Country (2023-2028)​6 Europe Smart Augmented Reality Glasses by Country6.1 Europe Smart Augmented Reality Glasses Historic Market Size by Country6.1.1 Europe Smart Augmented Reality Glasses Sales in Volume by Country (2017-2022)6.1.2 Europe Smart Augmented Reality Glasses Sales in Value by Country (2017-2022)6.2 Europe Smart Augmented Reality Glasses Forecasted Market Size by Country6.2.1 Europe Smart Augmented Reality Glasses Sales in Volume by Country (2023-2028)6.2.2 Europe Smart Augmented Reality Glasses Sales in Value by Country (2023-2028)​7 Asia-Pacific Smart Augmented Reality Glasses by Region7.1 Asia-Pacific Smart Augmented Reality Glasses Historic Market Size by Region7.1.1 Asia-Pacific Smart Augmented Reality Glasses Sales in Volume by Region (2017-2022)7.1.2 Asia-Pacific Smart Augmented Reality Glasses Sales in Value by Region (2017-2022)7.2 Asia-Pacific Smart Augmented Reality Glasses Forecasted Market Size by Region7.2.1 Asia-Pacific Smart Augmented Reality Glasses Sales in Volume by Region (2023-2028)7.2.2 Asia-Pacific Smart Augmented Reality Glasses Sales in Value by Region (2023-2028)​8 Latin America Smart Augmented Reality Glasses by Country8.1 Latin America Smart Augmented Reality Glasses Historic Market Size by Country8.1.1 Latin America Smart Augmented Reality Glasses Sales in Volume by Country (2017-2022)8.1.2 Latin America Smart Augmented Reality Glasses Sales in Value by Country (2017-2022)8.2 Latin America Smart Augmented Reality Glasses Forecasted Market Size by Country8.2.1 Latin America Smart Augmented Reality Glasses Sales in Volume by Country (2023-2028)8.2.2 Latin America Smart Augmented Reality Glasses Sales in Value by Country (2023-2028)​9 Middle East and Africa Smart Augmented Reality Glasses by Country9.1 Middle East and Africa Smart Augmented Reality Glasses Historic Market Size by Country9.1.1 Middle East and Africa Smart Augmented Reality Glasses Sales in Volume by Country (2017-2022)9.1.2 Middle East and Africa Smart Augmented Reality Glasses Sales in Value by Country (2017-2022)9.2 Middle East and Africa Smart Augmented Reality Glasses Forecasted Market Size by Country9.2.1 Middle East and Africa Smart Augmented Reality Glasses Sales in Volume by Country (2023-2028)9.2.2 Middle East and Africa Smart Augmented Reality Glasses Sales in Value by Country (2023-2028)​10 Company Profiles and Key Figures in Smart Augmented Reality Glasses Business10.1 CastAR10.1.1 CastAR Corporation Information10.1.2 CastAR Introduction and Business Overview10.1.3 CastAR Smart Augmented Reality Glasses Sales, Revenue and Gross Margin (2017-2022)10.1.4 CastAR Smart Augmented Reality Glasses Products Offered10.1.5 CastAR Recent Development10.2 Epson10.2.1 Epson Corporation Information10.2.2 Epson Introduction and Business Overview10.2.3 Epson Smart Augmented Reality Glasses Sales, Revenue and Gross Margin (2017-2022)10.2.4 Epson Smart Augmented Reality Glasses Products Offered10.2.5 Epson Recent Development10.3 Googlex10.3.1 Googlex Corporation Information10.3.2 Googlex Introduction and Business Overview10.3.3 Googlex Smart Augmented Reality Glasses Sales, Revenue and Gross Margin (2017-2022)10.3.4 Googlex Smart Augmented Reality Glasses Products Offered10.3.5 Googlex Recent Development10.4 Sony10.4.1 Sony Corporation Information10.4.2 Sony Introduction and Business Overview10.4.3 Sony Smart Augmented Reality Glasses Sales, Revenue and Gross Margin (2017-2022)10.4.4 Sony Smart Augmented Reality Glasses Products Offered10.4.5 Sony Recent Development10.5 Microsoft10.5.1 Microsoft Corporation Information10.5.2 Microsoft Introduction and Business Overview10.5.3 Microsoft Smart Augmented Reality Glasses Sales, Revenue and Gross Margin (2017-2022)10.5.4 Microsoft Smart Augmented Reality Glasses Products Offered10.5.5 Microsoft Recent Development10.6 AltoTech10.6.1 AltoTech Corporation Information10.6.2 AltoTech Introduction and Business Overview10.6.3 AltoTech Smart Augmented Reality Glasses Sales, Revenue and Gross Margin (2017-2022)10.6.4 AltoTech Smart Augmented Reality Glasses Products Offered10.6.5 AltoTech Recent Development10.7 Laster10.7.1 Laster Corporation Information10.7.2 Laster Introduction and Business Overview10.7.3 Laster Smart Augmented Reality Glasses Sales, Revenue and Gross Margin (2017-2022)10.7.4 Laster Smart Augmented Reality Glasses Products Offered10.7.5 Laster Recent Development10.8 Lumus10.8.1 Lumus Corporation Information10.8.2 Lumus Introduction and Business Overview10.8.3 Lumus Smart Augmented Reality Glasses Sales, Revenue and Gross Margin (2017-2022)10.8.4 Lumus Smart Augmented Reality Glasses Products Offered10.8.5 Lumus Recent Development10.9 ODG10.9.1 ODG Corporation Information10.9.2 ODG Introduction and Business Overview10.9.3 ODG Smart Augmented Reality Glasses Sales, Revenue and Gross Margin (2017-2022)10.9.4 ODG Smart Augmented Reality Glasses Products Offered10.9.5 ODG Recent Development10.10 Penny AB10.10.1 Penny AB Corporation Information10.10.2 Penny AB Introduction and Business Overview10.10.3 Penny AB Smart Augmented Reality Glasses Sales, Revenue and Gross Margin (2017-2022)10.10.4 Penny AB Smart Augmented Reality Glasses Products Offered10.10.5 Penny AB Recent Development10.11 Recon10.11.1 Recon Corporation Information10.11.2 Recon Introduction and Business Overview10.11.3 Recon Smart Augmented Reality Glasses Sales, Revenue and Gross Margin (2017-2022)10.11.4 Recon Smart Augmented Reality Glasses Products Offered10.11.5 Recon Recent Development10.12 Six15 Technologies10.12.1 Six15 Technologies Corporation Information10.12.2 Six15 Technologies Introduction and Business Overview10.12.3 Six15 Technologies Smart Augmented Reality Glasses Sales, Revenue and Gross Margin (2017-2022)10.12.4 Six15 Technologies Smart Augmented Reality Glasses Products Offered10.12.5 Six15 Technologies Recent Development10.13 Theia10.13.1 Theia Corporation Information10.13.2 Theia Introduction and Business Overview10.13.3 Theia Smart Augmented Reality Glasses Sales, Revenue and Gross Margin (2017-2022)10.13.4 Theia Smart Augmented Reality Glasses Products Offered10.13.5 Theia Recent Development10.14 Vuzix10.14.1 Vuzix Corporation Information10.14.2 Vuzix Introduction and Business Overview10.14.3 Vuzix Smart Augmented Reality Glasses Sales, Revenue and Gross Margin (2017-2022)10.14.4 Vuzix Smart Augmented Reality Glasses Products Offered10.14.5 Vuzix Recent Development​11 Upstream, Opportunities, Challenges, Risks and Influences Factors Analysis11.1 Smart Augmented Reality Glasses Key Raw Materials11.1.1 Key Raw Materials11.1.2 Key Raw Materials Price11.1.3 Raw Materials Key Suppliers11.2 Manufacturing Cost Structure11.2.1 Raw Materials11.2.2 Labor Cost11.2.3 Manufacturing Expenses11.3 Smart Augmented Reality Glasses Industrial Chain Analysis11.4 Smart Augmented Reality Glasses Market Dynamics11.4.1 Smart Augmented Reality Glasses Industry Trends11.4.2 Smart Augmented Reality Glasses Market Drivers11.4.3 Smart Augmented Reality Glasses Market Challenges11.4.4 Smart Augmented Reality Glasses Market Restraints​12 Marketing Strategy Analysis, Distributors12.1 Sales Channel12.2 Smart Augmented Reality Glasses Distributors12.3 Smart Augmented Reality Glasses Downstream Customers​13 Research Findings and Conclusion​14 Appendix14.1 Research Methodology14.1.1 Methodology/Research Approach14.1.2 Data Source14.2 Author Details14.3 Disclaimer​상위 및 세분화 버전 별도 문의​샘플 보고서 신청 및 맞춤형 주문 상담은 QYResearch Korea로 문의 바랍니다.​ "
마케팅 성공을 좌우할 데이터 전략 - 퓨어스토리지코리아 유재성 사장  ,https://blog.naver.com/purestorage_korea/222776812924,20220617,"©퓨어스토리지치열한 경쟁이 일상인 오늘날 비즈니스 세계에서 기업들이 가장 신경 써야 할 자산은 바로 고객들과의 관계입니다. 특히, 디지털화로 인해 데이터가 폭발적으로 증가하고 있는 요즘에는 많은 기업들이 데이터를 기반으로 고객 경험을 제고하고자 하고 있습니다.​그 중에서도 고객들과 많은 접점을 가지고 있는 마케터들은 그 누구보다도 이 시류에 적극 동참하고 있습니다. 그리고 이들이 적극적으로 모색하고 있는 웹3.0 시대의 새로운 기술 및 데이터는 마케팅의 지평을 빠르게 변화시키고 있습니다. 마케팅의 지형을 재편할 웹 3.0 시대의 새로운 기술들대표적으로 새로운 수익 모델 및 탈중앙화라는 변화들을 이끌고 있는 NFT는 콘텐츠 마케팅과 디지털 미디어 환경에도 많은 영향을 미치고 있습니다. 로블록스(Roblox), 코카콜라 및 반스(Vans) 등 이미 수많은 기업들이 게임, 미술 및 메타버스 등에 토큰 기반 커뮤니티를 구축하고 있으며, 이와 같은 기회들은 앞으로 더 넓은 산업군으로 확장될 전망입니다.​또한, 익명성과 불변성이라는 특징을 가진 블록체인은 데이터 프라이버시와 관련된 문제들을 해결할 수 있는 기회를 제공할 것입니다. 데이터와 선호도를 공유하는 고객에게 ‘관심 토큰(attention token)’을 보상으로 지급하는 등의 사례 방법이 가능하기 때문입니다. 이 밖에도 고객들이 ‘고객 데이터 토큰(Customer data token)’을 통해 원하는 정보를 익명으로 제공하고 맞춤화 된 서비스를 누리는 방식 또한 가능해질 것입니다.​마지막으로 가상 현실 및 증강 현실(VR/AR), 그리고 인공지능(AI) 부문에서도 많은 마케팅 기회들이 생겨날 것입니다. 코로나19를 기점으로 점차 많은 소비자들이 VR 및 AR 환경을 찾고 있기 때문입니다. AI는 추천 엔진(recommendation engines), 계산원 없는 계산 기술(cashierless checkout), 그리고 AI 콜센터 등 유통 부문에서 눈부신 변화들을 이끌어내고 있습니다. 여기에 음성 인식(speech recognition) 기술 및 음성 텍스트 변환(STT; speech-to-text) 기술 등 AI의 기술들이 나날이 고도화되고 있어, 마케터들이 다뤄야 할 데이터의 범위는 더욱 복잡해지고 다변화될 전망입니다. 데이터 중심 시대를 좌우할 CMO의 전략 및 IT 인프라이와 같은 현장에서 마케터들이 접하게 될 데이터의 90%는 복잡한 비정형 데이터입니다. 텍스트, 영상, 이미지, 소셜 미디어 포스트, 서버 로그부터 콜 센터 직원의 메모까지 다양한 형태를 지닌 비정형 데이터는 유용한 인사이트를 내포하고 있지만 처리가 까다롭다는 단점을 지니고 있습니다.​때문에 기업의 CMO(최고마케팅책임자)는 이와 같은 데이터를 효과적으로 처리할 수 있는 IT 전략 및 인프라를 구축해야 합니다. CIO(최고정보책임자) 및 CISO(최고정보보호책임자)와 함께 컴플라이언스, 개인정보보호 및 거버넌스 전략을 수립하는 것은 물론, 다양한 시스템들이 수집한 데이터를 한 곳으로 통합한 뒤 이를 효율적으로 저장, 관리 및 보호할 수 있는 솔루션을 선택해야 합니다.​이 때 HDD(하드디스크)가 아닌 SSD(솔리드 스테이트 드라이브)만을 사용하는 올플래시 스토리지 솔루션을 선택하면 여러 이점들을 누릴 수 있습니다. HDD는 자기식 디스크를 회전시켜 데이터를 처리하지만, SSD는 반도체를 사용하기 때문에 보다 빠르게 데이터를 읽고 쓸 수 있기 때문입니다. 대표적으로 전 세계 17,000개 이상의 매장에서 매일 300만 개의 피자를 판매하는 도미노 피자는 올플래시 스토리지를 통해 고객들의 취향과 행동을 분석하고 더 나은 고객 경험을 제공하고 있습니다. 이처럼 오늘날 데이터 환경을 위해 설계된 스토리지를 활용하면 CRM과 같이 핵심적인 마케팅 앱을 가속화하고 데이터를 빠르게 분석해 경쟁사들에 앞서 유용한 인사이트를 얻을 수 있을 것입니다.​데이터 중심 마케팅은 고객 경험을 제고하고 비즈니스의 가치를 높이는 필수 여건으로 자리잡고 있습니다. 그리고 이와 같은 혁신의 중심에는 데이터를 통해 전략, 크리에이티브 및 콜투액션(call to action; CTA)을 분석할 수 있는 CMO의 역량과, 복잡하고 방대한 데이터를 빠르게 분석할 수 있는 IT 인프라가 자리하고 있습니다. 데이터라는 자산을 적극적으로 활용하고, 이를 통해 더욱 정교하고 성공적인 마케팅 프로젝트를 추진하고자 하는 CMO라면 현대적인 데이터 여건에 최적화된 IT 솔루션을 함께 고려해보기 바랍니다.  ▶퓨어스토리지 공식 웹사이트▶퓨어스토리지 공식 블로그▶퓨어스토리지 제품/기술 관련 문의▶대표 이메일: korea@purestorage.com▶대표전화: 02-6001-3330 퓨어스토리지코리아서울특별시 강남구 영동대로 517 30층 3019 - 3020호 ​ "
"날다, 이현수 대표 / 인공지능 음성인식 키오스크 ",https://blog.naver.com/powerkorea0505/222572185179,20211118,"​ ‘날다’의 인공지능 양방향 음성 챗봇 솔루션,본격적인 비대면 시대를 열다​  코로나19로 인해 찾아온 비대면 시대가 어느덧 완전하게 자리를 잡았다. 배달 시장은 빠르게 성장했고, 매장 내 키오스크 주문은 선택이 아닌 필수가 되어가고 있다. 정부는 11월 1일부터 위드코로나를 시작하고, 3단계 방역완화를 통해 내년 1월까지 예전의 일상을 찾아간다는 계획이다. 친구와 가족 등의 사적 모임은 10명까지 허용되며, 식당과 카페, 학원, 영화관, 체육시설 등 다중이용시설의 운영 제한 시간도 대부분 해제된다. 하지만 우리보다 앞서 위드코로나를 선택한 외국의 사례를 보면 방심은 절대 금물이다. 영국은 위드코로나 이후 하루 5만명 대의 확진자가 나오고 있고, 싱가포르는 접종 완료율이 84%에 육박함에도 매일 4천명 가까운 확진자가 쏟아지고, 사망자가 15배나 증가하였다. 어쩔 수 없이 선택한 위드코로나, 공중방역은 물론 철저한 생활방역이 더욱 필요한 시대가 될 것이다. 이런 가운데 완벽한 위드코로나 시대를 대비하여 완벽한 비대면 키오스크를 개발하여 많은 주목을 받고 있는 곳이 있다. 바로 ‘주식회사 날다(대표 이현수)’가 그 주인공이다.  ​ ​ ‘주식회사 날다’,HW와 SW를 융합한 창조적인 제품,음성 지원 기반의 인공지능 키오스크‘주식회사 날다’는 인공지능 기반의 음성안내 기술 집약형 ICT 전문기업으로, AI음성 & 키오스크를 자체적으로 개발하여 생산하고 있는 벤처기업이다. 관공서와 병원, 전시회관, 호텔 등의 입구에 설치하여 음성으로 찾고자 하는 목적지를 문의 시 음성안내 디스플레이 화면으로 정보를 표시하는 키오스크, 음성 지원 기반의 지능형 결제 키오스크, 지능형 무인 질병진단 플랫폼 등을 생산 및 개발하고 있다. 이현수 대표는 “주식회사 날다의 솔루션은 음성인식 기반이 핵심이다. HW 및 SW를 융합하여 창조적인 제품을 개발하고 있다. 사회 전반에 자리잡은 디지털 기술에 접목해서 빠르고 편리한 스마트 테크놀로지를 구현하는 것이 주요 사업이다”고 언급했다. ​  ​ ​ 인공지능 음성인식 키오스크 ‘카페 날다’,비대면 시대 최고의 무인 결제 솔루션이곳의 가장 대표적인 제품은 비대면 음성인식 무인 결제 시스템 ‘카페 날다’이다. 코로나 19 시대에 걸맞게 노터치 비대면 주문 방식을 채용했으며, 거리측정 센서와 열화상 카메라에 의한 고객 체온 측정이 가능하도록 설계했다. 기존의 터치 형식은 물론이고 음성인식을 지원하여 완벽한 비대면 결제 시스템을 완성했다. 인공지능의 머신러닝, Speech Recognition(STT), TTS 기술을 적용했으며, 사용자 주변의 잡음을 개선하기 위해 노이즈캔슬링 마이크를 적용했다. 사용자가 접근하면 자동으로 탐지하여 온도 체크가 진행되며, 정상온도가 확인된 고객만 주문을 접수할 수 있다. 주문은 터치와 음성 중 선택하여 진행할 수 있으며, 작동이 매우 간편해서 기계작동에 능숙하지 못한 어르신들도 쉽게 사용할 수 있다. 다양한 발음과 방언을 입력하여 인식률도 매우 높다. 매장 운영자를 위한 편의성도 매우 뛰어나다. 기존에 사용 중인 POS 및 신용카드사와의 연동 서비스가 가능하며, 빠르고 편한 제품/메뉴관리, 판매정산관리, 거래조회, 그리고 설정관리를 돕는다. 이 대표는 “최근 코로나19로 인해 비대면 시대가 열렸다. 코로나19가 끝나도 과거처럼 돌아가지 않을 것이다. 이미 비대면의 편리함에 세상이 익숙해졌다. 로봇이 서빙하고 커피를 만드는 세상이다. 더 이상 사람이 주문을 받고 결제하는 시대는 끝났다. 완성도 높은 제품으로 코로나19로 어려움을 겪고 있는 여러 소상공인을 위해 개발하게 되었다”고 언급했다. ​  ​ ​ ‘인공지능 음성 대중교통 안내시스템’,편리하고 안전한 여행을 위한최고의 ‘여행 길잡이’‘주식회사 날다’에서는 키오스크 외에도 음성인식과 IoT기술을 결합한 다양한 대화형 양방향 음성 챗봇(Chatbot)솔루션을 생산하고 있다. 중소기업진흥공단에 설치된 ‘인공지능 & 터치안내시스템’은 방문객이 건물 내부의 시설과 위치, 그리고 방문한 업체 혹은 사람을 검색하면 몇 동 몇 호에 위치해 있으며, 취급하는 사업 아이템까지 모든 정보를 안내한다. 또한 시설 내 운동장과 음식점 등에 관한 정보를 시각과 음성으로 안내하는 혁신적인 기능으로 제품이 설치된 중진공의 관계자들과 방문객들의 폭발적인 반응을 얻고 있다.​이곳에서 개발한 인공지능음성 대중교통안내시스템도 많은 주목을 받고 있는 솔루션이다. 이 제품 역시 양방향 음성 안내 시스템으로, 이른바 대중교통의 네비게이션이라고 불린다. 터미널, 기차역, 관광지 등에 설치되어 길 안내부터 주요 관광지에 대한 정보를 음성으로 제공한다. 예를 들어 인천공항에서 이태원을 가고 싶다면 음성으로 안내를 한 후, 교통정보가 담긴 QR코드를 제공하는데, 중간 환승지점과 같은 주요 경유지에서 음성과 진동으로 알리고 목적지까지 여행객을 안내한다. 관광지와 같은 곳에서는 관광지의 정보 등을 음성으로 안내한다. 한국어뿐만 아니라 영어, 중국어를 비롯한 다양한 언어 기능을 탑재할 수 있기 때문에 외국인이 많은 곳에서도 적극 활용할 수 있다. ​  ​ ​ ‘노이즈 캔슬링’ 기술로시끄러운 곳에서도 완벽한 음성 인식인공지능 전염병 무인진단시스템 특허 획득,다양한 음성 인식 솔루션 만들어 나갈 것 ‘주식회사 날다’에서 제공하는 모든 음성인식 솔루션에는 ‘노이즈 캔슬링’ 기술이 사용되었다. 타사 제품은 잡음이 많은 곳에서는 인식률이 떨어지는 경향이 있지만 이곳의 제품은 시끄러운 곳에서도 인식률이 굉장히 높다. 이 대표는 “당사의 제품은 주변에서 70 데시벨(dB)정도의 소` 음이 있어도 완벽하게 음성을 인식한다”고 언급했다.​이 대표는 앞으로 음성인식 솔루션의 활용범위를 더욱 확장할 계획이다. 인공지능 전염병 무인진단시스템에 대한 특허를 획득했으며, 위드 코로나 시대를 위해 위드 체큐온 온라인 출시 예정이다. 이밖에도 한국어 교육 솔루션 등 다양한 프로그램을 개발할 계획이다. ​  South Korean non-face-to-face solution startup Nalda makes it easy for visitors and travelers to find directions without a hassle[With Corona]South Korea has started With Corona from November 2021. But that does not mean we, Koreans, should be careless. Predecessors such as UK and Singapore, for example, saw from thousands to tens of thousands confirmed cases after With Corona. So we must remain alert and follow the quarantine rules. Still good idea is that we should rather keep making most of non-faceto-face lifestyle we have become familiar with since the pandemic. In fact, South Korea not only has proved excellence in containing the virus but also developing and installing non-face-to-face business software and devices. ​ [Nalda] Nalda is a South Korean tech venture that shined during the pandemic with its innovative contactfree AI voice recognition kiosk. Installed in public institutions, hospitals, exhibition halls and hotels, the kiosk guides visitors, helps them with transactions, and provides medical help. Cafe Nalda kiosk, for example, is equipped with non-touch speech recognition sensor, distance measure sensor, thermal sensor and camera sensor as well as noise cancelling mike. It detects when a person approaches and take the temperature before entering the building. It recognizes sounds of both standard and various dialects and it is easy to use that even old people can use it without a hassle. It is compatible with the existing POS and credit card terminals and makes operation, sales, transaction and set up more convenient. ​ [Chatbot]Apart from kiosk, Nalda is introducing chatbot solutions. The catbot installed in the Korea SMEs and Startups Agency building, for example, does multi-tasks: guiding visitors, helping visitors find people or companies they are looking for in the building, helping them find where cafeteria and such are located in the building. The voice recognition traffic information also is worth paying attention as it guides traffic information such as direction and navigation with voice recognition. It can be used in terminals, train stations and tourist spots, and the language option especially helps visitors or tourists find directions easy without a hassle. Heonsu Lee, CEO of Nalda, is ambitious to grow his business as a global voice recognition non-face-to-face solutions provider. ​​​​​​  글 | 신태섭 기자 tss79@naver.com온라인 업로드 | 백지원발행 | 11월 호 BRAND<저작권자 © 월간파워코리아 / 파워코리아데일리 무단전재 및 재배포금지> "
Tensorflow vs PyTorch ,https://blog.naver.com/gypsi12/222911363236,20221026,"앞으로 한달 정도 투자하여,파이토치에 대한 기본을 익히려고 한다.​요즘 추세가 Tensorflow가 아니라 PyTorch인 듯 해서이다.그렇다면, 이런 추세가 사실인지 확인해보고사실이라면 왜 그러한지 궁금해서 정리해보고자 이 글을 작성한다.​medium을 보던 중에 아래 글을 발견해서 한번 정리해보고자 한다.https://medium.com/aimluae/tensorflow-vs-pytorch-which-will-be-the-top-deep-learning-framework-in-2022-a3488635029 Tensorflow vs PyTorch: Which Will Be The Top Deep Learning Framework In 2022 ?PyTorch and Tensorflow are two of the most popular deep learning libraries available today. PyTorch was developed by Facebook’s AI…medium.com  Introduction​​PyTorch와 Tensorflow는 현재 가장 유명한 deep learning 라이브러리들이다.​PyTorch는 FaceBook's AI Research Lab에서 deep neural networks를 더 쉽게 구성하고 사용하기 위해 만들어졌고​TensorFlow는 Google Brain Team에서 만들었고다양한 형태의 input을 구성하는 것이 가능하다는 점과선형 회귀 모델부터 딥러닝 네트워크까지 사용자 정의(custom) 모델을 만들 수 있다는 점이 이점이다.​  History​TensorFlow는 Google에서 research(탐구)와 production(생산) 작업을 위해 개발했다.초기버전은 2015년 11월에 나왔고, 업데이트 버전이자 Keras를 지원하는 TensorFlow 2.0이 2019년 9월에 나왔다.​Keras는 다른 딥러닝 라이브러리보다 뛰어난 오픈소스 라이브러리이다.Keras는 2015년 3월에 있었던 ONEIROS(Opend-ended Neuro-Electonic Intelligent Robot Operating System)이라는 프로젝트를 진행하다 개발되었다.2.3 버전까지 Keras는 다른 딥러닝 라이브러리들의 백엔드로 사용되다가2.4 버전부터 TensorFlow 라이브러리의 도구처럼 쓰이고 있다.​PyTorch는 2016년 10월에 Adam Paszke의 FaceBook's AI Research lab에서의 intership 프로젝트에서 시작되었다.  Popularity & Adoption​깃허브에서 얻은 star 개수는 개발자 커뮤니티 사이에서의 인기(popularity)를 의미한다고 할 수 있다.​그런 측면에서 볼 때,Tensorflow는 PyTorch보다 3배 이상의 star를 받았다.​Tensorflow 165K ⭐️ | PyTorch 56.4k ⭐️​PyTorch는 최근 인기를 보면세상에 나온 기간이 상대적으로 짧아 별이 적다고 생각할 수 도 있을 것 같다.​Google Treands data에 따르면,PyTorch는 빠르게 성장하고 있으며, TensorFlow&Keras를 넘어섰다고 한다.​ 이를 증명하듯이, 허깅페이스에서의 대부분의 SOTA 모델들은PyTorch로 쓰여졌다. 그리고 최신 논문들은 대부분이 PyTorch를 사용해 framework를 구현했다.​  API Level Comparison​초심자가 사용하기에는 Tensorflow가 PyTorch보다 낫다.​PyTorch에서는 subclassing 방식으로 neural network layers 를 구성하는데이 방식은 class 하나 안에 여러개의 다른 layers들을 손수 다 넣어준다.=> 각 layer의 입력과 출력 형태를 신경써서 이어줘야 한다.​하지만, Tensorflow에서는 sequential API를 이용해 한줄씩 적어나가면그대로 각 신경망이 이어지기 때문에 쉽고,마지막에 model.fit() 만 하면 학습이 된다.( 물론,  tensorflow도 subclassing 방식으로 사용 가능하다.)​PyTroch는 Numpy와 비슷하다. ( PyTorch의 Tensor와 Numpy의 ndarray가 유사)  다른 점이라면, Numpy의 경우는 GPU에 최적화되어있지 않다.그래서 파이썬스러운 코드를 작성하는 것이 가능하다.​바면, TensorFlow는 덜 파이썬스럽다.왜냐하면, C++ 기반이기 때문이다.Tensorflow도 pytorch와 마찬가지로 GPU에 최적화되어있음.​PyTroch는dynamic auto differentiation (autodif) 를 지원하는데​+ automatic differentiation에 대하여..https://blog.naver.com/gypsi12/222907820461 Automatic Differentiation이란PyTorch가 dynamic auto differentiation(autodif)를 지원한다고 하기에 이 Auto differentiation이 무...blog.naver.com 이를 사용하여 어떤 종류의 목적함수든지 간에쉽게 편미분(partial derivative)를 계산할 수 있고모델 구조에 다양성을 제공 할 수 있다.​초창기 TensroFlow 1.0이나 Keras은static graph기반이었지만,TensorFlow 2.0에서부터 eager excution(즉시 실행)이라는 방법을 사용해dynamic graph를 지원한다.​+ dynamic graph에 대하여.. https://blog.twitter.com/engineering/en_us/topics/insights/2021/temporal-graph-networks​+ dynamic graph에 대하여..https://medium.com/watcha/gnn-%EC%86%8C%EA%B0%9C-%EA%B8%B0%EC%B4%88%EB%B6%80%ED%84%B0-%EB%85%BC%EB%AC%B8%EA%B9%8C%EC%A7%80-96567b783479​  Ecosystem & Tools​​TensorFlow는 제품 생산 영역에서 광범위하게 쓰이고PyTorch는 연구 분야에서 더 유명하다.​​ Tensorflow​​TensorFlow Servinghttps://www.tensorflow.org/tfx/guide/serving​텐서플로 서빙(Serving)은우리가 모델을 학습시키고 나서 이 모델을 배포하고자 할 때 사용하는 기능이다.​+ http://solarisailab.com/archives/2703 36. 텐서플로우 서빙(TensorFlow Serving)을 이용한 딥러닝(Deep Learning) 모델 추론을 위한 REST API 서버 구현 | 솔라리스의 인공지능 연구실36. 텐서플로우 서빙(TensorFlow Serving)을 이용한 딥러닝(Deep Learning) 모델 추론을 위한 REST API 서버 구현 2020년 5월 31일 by Solaris 이번 시간에는 텐서플로우 서빙(TensorFlow Serving)을 이용해서 딥러닝(Deep Learning) 모델 추론을 위한 REST API 서버를 구현하는 방법을 알아보자. [1] 텐서플로우 서빙(TensorFlow Serving) 텐서플로우 서빙(TensorFlow Serving)[2]은 구글에서 만든 프로덕션(production) 환경을...solarisailab.com  ​TensorFlow Extendedhttps://www.tensorflow.org/tfx​텐서플로를 기반으로 하는 AI 모델을제품 생산 수준에서 개발하고 싶을 때 사용하는 end-to-end AI 모델 생성 플랫폼이다.​​  TensorFlow Litehttps://www.tensorflow.org/lite/guide​모바일, 내장형기기, IoT 기기에서모델을 실행할 수 있도록 지원하는 도구이다.​  ​TensorFlow Hubhttps://www.tensorflow.org/hub​허깅페이스와 같이사전학습된 모델이 저장된 저장소이다.​huggingface와 동일하게tensorflow_hub를 !pip install 하고, import해서 불러온다음​원하는 모델에 접근해서 사용할 수 있다.  ​​Model Garden https://github.com/tensorflow/models​TensorFlow로 작성하여 SOTA를 찍은 소스코드를 불러올 수 있는 레포지토리이다.​  ​Vertex AIhttps://cloud.google.com/vertex-ai​사전학습된 언어 모델 뿐만 아니라,여러 AI 분야( CV, NLP 등 )의 모델을 불러올 수 있고다양한 커스텀 라이브러리를 제공하여​AI 모델을 최소한의 전문 지식과최소한의 코딩으로 손쉽게 사용할 수 있게  해주는 플랫폼​  ​ MediaPipehttps://mediapipe.dev/​다양한 ML 분야의 파이프라인(AI 모델을 구성하고 있는 세부 기능)들의 소스코드를제공하는 사이트이다.( face detection(얼굴인식) , multi-hand tracking(양손 트래킹) , object detection(사물 인식) 등 )​  ​Google Coralhttps://coral.ai/​대부분이 소프트웨어 AI모델을 학습해서 클라우드에 올린다음 배포하여 제공하는 것과 달리Coral은 AI모델을 하드웨어에 담아(회로기판) 세상에 적용 가능하게 만든다.​   ​TensorFlow.js https://www.tensorflow.org/js​자바스크립트를 서버에서 사용할 수 있도록 만든 프로그램인Node.js에서 ML 모델을 사용할 수 있도록 만든 라이브러리이다.​  ​TensorFlow Cloud https://www.tensorflow.org/cloud​아마존에 AWS(Amazon Web Service)가 있다면,  구글에는 GCP(Google Cloud Platform)가 있다.​TensorFlow Cloud는 로컬에서 코드를 실행시켰을 때,그 코드를 구글 Cloud의 서버에 보내서 실행시킬 수 있도록 해주는 기능이다.   Playground https://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=4,2&seed=0.79936&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false​간단한 Dense Layer(=Fully connected Layer)를 시각화하여 볼 수 있는 사이트이다.​  PyTorch​PyTorch Hub https://pytorch.org/docs/stable/hub.html​pre-trained 모델이 담긴 레포지토리를 공유하는 플랫폼이다.Audio, Vision, NLP등 다양한 범위의 모델이 존재한다.​  TorchServe https://pytorch.org/serve/​모델을 serving한다는 것은ML 모델을 사용할 수 있도록 모델을 배포하거나, 모델 API를 제공한다는 뜻이다.​즉, TorchServe는 Torch를 이용한 모델을배포할때 사용하는 도구이다.​  PyTorch Live https://playtorch.dev/​AI모델을 모바일에서 사용가능하도록 만든 도구이다.JavaScript와 React Native를 사용하여iOS와 Android앱에서 구동가능하게 만들었다.​  TorchVisionhttps://pytorch.org/vision/stable/index.html​PyTorch의 공식 ComputerVision 라이브러리이다.모델 구조(model architectures)들이나 유명한 datasets들을 포함하고 있다.​  ​TorchText https://pytorch.org/text/stable/index.html​Torch를 이용한 NLP분야의 유명한 datasets과 데이터 처리 도구들이 담긴 라이브러리이다.  ​​TorchAudiohttps://pytorch.org/audio/stable/index.html​PyTroch의 공식 audio 라이브러리이다.유명한 audio 모델(DeepSpech나 Wav2Vec과 같은)과그에 대한 설명과 pipeline을 제공한다. (ASR(automatic speech recognition)이나 다른 task들의 pipeline들)  ​SpeechBrainhttps://speechbrain.github.io/​PyTroch 오픈소스 speech 도구모음이다.ASR(자동음성인식), speaker recognition(화자 인식), verification(화자 검증), diarization(동질 음성 특징 분해) 을 제공한다.​  TorchElastic https://pytorch.org/docs/stable/elastic/run.html분산학습을 위한 도구이다.  ​PyTorch Lightninghttps://www.pytorchlightning.ai/​PyTroch의 Keras라고 불리는모델 구성이나 훈련 과정을 단순화하는 유용한 도구이다.​  Conclusion​​TensorFlow는 더 오래되었기에 이미 많은 도구들이 존재한다.하지만, PyTorch에는 TensoFlow에 존재하는 많은 도구들을 발전시켜 추가하는 중이다.지금 이 순간도 더욱 발전된 도구들이 PyTorch에 업데이트 되고 있다.​그래서 둘 중에 무엇을 선택할지 고민이 된다.​무엇을 선택할 지는, 순전히 사용자의 능숙도와 만들고자 하는 모델 종류에 달렸다.​만약, deep learning 초심자라면 TensorFlow로 시작하고시간이 지나 end-to-end 딥러닝 pipeline에 능숙해지고, 다른 모델 구조들에 익숙해진다면PyTorch로 코드를 작성하는 것이 유용할 수 있다.​ "
푸름 해운대보청기 - 보청기 조절을 위한 기본청력검사 ,https://blog.naver.com/donghyeop111/222881489294,20220922,"​ 푸름보청기 청각센터 해운대점부산광역시 해운대구 세실로 45 3층 안녕하세요해운대푸름보청기입니다.  어느덧 9월이 끝나가고 10월이 다가오고 있습니다.가을이 온 만큼 낮 밤으로 기온차가 많이 남으로 감기에 조심하시기리 바랍니다.​ 푸름 양산 / 부산 해운대보청기 오늘은 포스팅은보청기 착용 전이나 후로 조절에 꼭 필요한 청력검사에 대해서 알아보도록 하겠습니다.​ 푸름 양산 / 부산 해운대보청기 순음청력검사(puretone audiometry)​순음을 제시하여 주파수별 청력역치를 측정하는 검사입니다.​청력역치를 검사하는 방법으로는 상승법(ascending method),하강법(descending methhod), 수정상승법(combined ascending and descending approach)이렇게 3가지가 존재합니다.​검사방법은 2가지로기도전도청력검사와 골도전도청력검사2가지로 나뉩니다.​ 푸름 양산 / 부산 해운대보청기 ​기도청력검사란??​헤드폰 또는 삽입형 이어폰을 사용하여125~8000Hz에서 청력역치를 측정하고,공기전도로 외이-중이-내이로 소리가 전달되어청각기관의 모든 경로에 의한 청력을 파악하는 것입니다.즉 외이,중이,내이 한군데라도 이상이 있다고 비정상적인 청력이 나옵니다.양쪽에 청력이 차이 날 경우 좋은쪽 귀가 나쁜쪽 귀의 검사를 방해할 수 있기 떄문에 좋은쪽 귀에 소리를 넣어주어 검사하는 차폐검사를 해야합니다.​ 푸름 양산 / 부산 해운대보청기 골도청력검사란??​골진동체를 유양돌기에 위치하여 소리의 진동을 내이로 직접 전달하여 250~4000Hz의 청력역치를 측정하는 검사로 외이나 중이의 상태에 거의 영향을 받지 않으므로내이와 그 이상 청각전달경로의 기능을 확인합니다.​ 푸름 양산 / 부산 해운대보청기​어음청력검사(speech audimetry)​어음인지청력검사(SRT, speech recognition thershold)​이음절어를 50%가량 인지할 수 있는 최소강도레벨을 측정하는 것입니다.검사의 목적은 어음인지 시 필요한 민감성, 즉 SRT를 측정하여 순음청력검사 결과의 신뢰도를 확인하고 단어 및 문장 인지도 검사의 기초 자료로 사용하는 것입니다.​단어인지도검사(WRS, word recognition score)​피검자가 듣기 편안한 레벨에서 단음절어을 듣고얼마나 정확히 인지하는지를 백분율로 점수화한 것이다.검사의 목적은 피검자가 단음절어를 들었을 때얼마나 잘 이해하는지 그 정확도를 평가하는 것이다.​★ 두가지의 검사가 보청기 조절 필요한 이유​순음청력검사(puretone audiometry) 푸름 양산 / 부산 해운대보청기처음 보청기를 착용할 때 회사별 프로그램에 청력도를 입력해서착용자 본인에게 맞게 소리를 조절할 수 있는 기본값을 측정해줍니다.  푸름 양산 / 부산 해운대보청기측정값을 정한 후 주파수별로 큰소리, 중간소리, 작은소리를조절하여 착용자에 알맞게 조절해주는 것이 좋습니다.​청력도가 입력되지 않을 시 착용자에게 올바른 조절을 해드릴 수 없기 때문에 보청기를 착용하는 의미가 없어집니다.​어음청력검사(speech audimetry)​어음청력검사가 중요한 이유는 처음 보청기 조절을 한 후 말소리를 이해하고 인지가 잘되는지 확인하기 위해 다시 검사를 실시하는 것이 좋습니다.만약 착용 전후로 크게 변화가 없을 시 다시 보청기 조절을 통해 말소리 또는 TV 및 전화통화소리를 잘 들을 수 있도록 해주는 것이 좋습니다.​  ​이상 푸름 해운대보청기였습니다.​다음에는 더욱 유익한 정보로 돌아오도록 하겠습니다.​난청이 있음을 확인하셨거나 보청기에 대해서​알아보고 싶은게 있으신 분들은 푸름 해운대보청기 방문해주세요!!​ 푸름 양산 / 부산 해운대보청기 ​ "
[📚] 음성인식기술과 말하기/듣기 지도 ,https://blog.naver.com/notonlybutalso0930pm/222751086241,20220529,"새롭게 관심이 생긴 분야는 인공지능기술과 언어교육의 접목 가능성이다. 그 중에서도 자동음성인식/합성과 영어 교육에 흥미가 좀 생겼다.​그간 영어 말하기, 듣기 지도는 읽기, 쓰기에 비해 상대적으로 소외된 측면이 없잖아 있다. 적어도 내 수업에서는 그랬다ㅎ 듣기와 말하기는 언제나 곁다리로 구색맞추기용으로 해온 감이 있다. 수업이든 평가든.​그래서 소외되는 듣기/말하기 수업 평가에 대한 죄책감을 마음 한 켠에 은은히 묻어두고 살던 중 구름배 선생님의 아래 글을 보았고 ↓ 구술평가 공부 자료내가 편집위원을 하는 <함께 여는 국어교육> 2022 여름호는 초점이 구술평가이다. 예전에는 학급당 ...blog.naver.com 아 저는 당분간 고등학교 갈 일 없으니 사실 고교학점제고 입시고 나발이고 딱히 제 알바는 아닙니다만ㅋㅋㅋㅋ그래도 oral assessment에 대해 다시 한 번 생각해보는 계기는 되는거죠. 나는 언어를 가르치고 있음에도 불구하고 그간 너무 문자 언어 중심적으로 수업하며 음성 언어는 도외시 하지 않았나 하는 반성과 함께.​🌏 Katerina Evers & Sufen Chen (2020): Effects of an automatic speech recognition system with peer feedback on pronunciation instruction for adults, Computer Assisted Language Learning, DOI: 10.1080/09588221.2020.1839504​그러다가 이 논문을 읽었다 이거에요. 물론 총체적인 말하기 능력(speaking proficiency)보다는 발음 지도에 촛점을 맞추고 있기는 합니다만. 우리가 보통 가르칠 때 발음은 교과서에도 아주 구석탱이에 조그맣게 나와있고 그 부분은 통상 깔끔하게 무시되고 지나가지 않습니까? 특히 영어과는 원어민 중심주의에 대한 경계가 좀 있어서 너무 원어민같은 발음, 원어민스러운 말하기를 고집하는 것을 좀 지양하고 있기도 하고요. 그러나 정작 학습자는 기왕이면 원어민스러운 영어를 배우는 것을 더 선호하고 발음과 같은 경우는 학습자의 자신감과도 밀접하게 연결되어 있다는 불편한 진실ㅋㅋㅋㅋㅋ그렇다고 영어가 갖는 사회문화적인 헤게모니와 원어민 중에서도 백인 선호를 무시하고 맹목적으로 학습자 수요만 맞추는 것이 과연 맞나? 하는 고민도 항상 들곤 합니다.​애니웨이, 이 논문은 L2 학습자의 발음을 동료 피드백을 통해 교정하고 학습하는데, 그 과정에서 자동 음성 인식 기능(automatic speech recognition, ASR) 프로그램을 사용하고 그 효과를 분석한 것입니다. 대표적으로 많이 쓰이는 ASR 프로그램은 그... 아이폰 시리같은거 생각하면 됩니다.​제 2 언어 학습에 있어 정확한 발음은 중요한 요소이지만 (내 발음을 상대방이 못 알아들으면 communication breakdown이 일어나니까, 그 경우 학습자의 자신감이나 학습 동기도 손상되니까) 교실에서 발음 지도는 좀 그다지 중요하게 다루어지지 않습니다. 이것은 발음 지도에 대한 교사들의 무지에서 비롯되기도 하고 원어민이 아닌 교사 본인의 발음 자신감 부족 때문이기도 합니다. 그리고 발음 연습하는 학생들이 일시적으로 교실에서 주목받는 현상을 부담스러워하기도 하고요. ASR 프로그램을 사용하면 적어도 그렇게 불필요한 주목을 줄일 수는 있습니다. 즉각적인 피드백을 받을 수도 있고요. 그래서 연구자들의 관심이 요즘 새롭게, 무료로 쓸 수 있는 ASR 프로그램을 EFL 수업에 적용시키는 쪽으로 쏠리고 있다고 논문에서는 소개하고 있습니당.  그리하여 이 논문의 Research Question은 다음과 같습니다.​How effective is the ASR pronunciation practice for adults’ pronunciation?Would peer instruction significantly increase the effectiveness of ASR pronunciation practice for adults’ pronunciation?What is the adult learners’ perceived satisfaction with the ASR pro-nunciation teaching? How does perceived satisfaction differ in the conditions with and without peer instruction?What is the adult learners’ perception of the ease-of-use of the ASR website? How does perceived ease-of-use differ in the conditions with and without peer instruction?  연구 방법은 이렇습니다.중간 수준의 영어 실력을 가진 31명의 대만 EFL 학습자를 대상으로 발음 학습을 진행했습니다. 이 때 사용된 웹기반 ASR은 speechnotes라는 사이트입니다. Speech to Text Online Notepad. FreeProfessional Dictation & Text Editing. Distraction-free, Fast, Easy to Use & Free Web App for Dictation & Typingspeechnotes.co Speechnotes는 구글의 음성인식엔진을 기반으로 하는데, 구글의 음성 인식 엔진이 타사의 음성 인식 엔진과 비교하여 인식 정확도가 높다고 하네요. 단어를 하나 하나 끊어서 발음하지 않아도 다 알아들어서 유창성을 저해시키지도 않고요.​연구 참여자들은 실험집단과 통제집단으로 나누고, 각 집단은 다시 4명정도의 모둠으로 나누어 3-4문장으로 이루어진 짧은 텍스트를 받습니다. 텍스트는 중국어 발화자들이 특히 어려워하는 발음들로 이루어졌는데요, 그러한 발음들이란 다음과 같습니다:​1. 참여자들의 모국어(중국어)에 없는 영어 음소(phonemes): 예를 들면 /v/ 발음. /v/ 발음이 중국어에는 없어서 ‘vast’가 ‘fast’로 읽힐 수 있겠고요. 중국어 발화자들은 /i/와 /i:/를 구별하지 못하기 때문에 ‘sheep’과 ‘ship’도 발음이 어려울 수 있고요. 그 외에도 /ð/,/h/, /ae/와/e/(예: ‘man’ and ‘men’) 발음이 있습니다.​2. 중국어에도 존재하지만 다르게 발음되는 영어 음소 두 개(/r/and/ʃ/): 몰랐는데 중국어에서 /r/은 /ʒ/에 가깝게 발음된다고 합니다. 그래서 ‘Gin’과 ‘ring’이 헷갈릴 수 있겠습니다.​3. 긴 단어에서의 juncture: 아 이 부분 좀 흥미로웠는데ㅋㅋㅋㅋ이 발음 오류는 한국인 EFL 학습자에게도 분명히 나타날 수 있을 것 같았거든요. 중국어는 모든 음절 하나하나가 다 강세가 있고 음절 간 경계가 뚜렷한 언어입니다. 그래서 영어의 긴 단어를 음절 간 경계 없이 발음하는 것에 어려움을 겪을 지도 모른다는 것이 이 논문의 가설(?)입니다. 예를 들어 ‘shuddering’같은 단어를 중국어 L1 발화자가 읽으면 ‘shut the ring’으로 발음될 수도 있다는 것이지요. 제가 알기로 한국어도 비슷한 특징을 가진 언어이기 때문에 (우리도 음절 하나하나를 또박또박 발음하지 않습니까?) L1이 한국어인 EFL 학습자도 비슷한 현상을 보이지 않을까 싶네여.​참여자들은 세 가지 단계의 활동을 수행합니다.​1. Reading: 각 모둠에서 연습하는 학생(practicing student: PS)이 스피치노트를 켜고 주어진 텍스트의 첫 번째 단락을 읽습니다. 당연히 다른 학생들은 조용히 있어야함. 소리 내면 안됨 스피치노트가 듣고 받아써버리니까.​2. Pronunciation feedback: PS가 스피치노트 피드백을 확인합니다. 이게 스피치노트가 어느 부분 발음이 잘못되었습니다, 라고 피드백을 주는 것이 아니라 스피치노트가 인식한 transcript를 보면서 잘못 인식된 단어를 확인하는 활동인 것 같음.​3. Practice: 여기서 실험집단과 통제집단의 활동이 갈리는데요. 실험집단에서는 다른 모둠원이 PS가 발음을 교정할 수 있도록 도와줍니다. 통제집단에서는 이 작업을 PS 혼자 합니다. 혼자 잘못 인식된 발음을 연습해 보는것이지요. 이 단계에서 PS의 모둠원들은 PS의 발음을 0에서 10으로 점수를 부여하고 발음을 어떻게 향상시킬 수 있을지 설명하고 논의합니다. 실험집단에서는 그 과정에서 모둠원들이 직접적인 피드백과 발음 방법을 알려주고, 통제집단에서는 PS 혼자 발음 연습을 하고, PS의 발음을 다른 모둠원은 분석만 합니다. 그러니까 실험집단, 통제집단 모두 협동학습이 일어나는데 실험집단만 직접적인 동료 피드백을 받게 되는 것입니다.  학습자들의 발음 측정과 관련해서는 다음과 같은 도구가 쓰였습니다.우선은 read aloud 활동을 하고 두 명의 교사가 학생들의 억양과 발음이해가능정도를 1-9 스케일로 측정합니다. 그리고 3-4가지 주제로 대화를 진행(spontaneous conversation)하면서 아이엘츠 스피킹 루브릭에 따라 발음을 평가했습니다. 두 활동의 차이가 있다면, read aloud 활동에서는 학생들이 주어진 단어를 정확한 발음하는 것에 집중해야 했고 spontaneous speech에서는 그러한 제한이 없었다는 것입니다. 다시 말해 spontaneous speech에서는 학생들이 어려운 단어는 쓰지 않을 수도 있었고요, 대신 대화하는 중이므로 자신이 올바르게 의미를 전달하고 있는지 계속 생각해야 했겠져. 실험이 끝난 후 ASR 소프트웨어에 대한 만족도, 사용의 용이성 등을 조사하는 설문조사가 이뤄졌습니다.  연구 결과는 어땠을까요?read aloud 활동에서의 발음 차이를 보면, 우선 실험집단과 통제집단 모두 pretest에 비해 posttest에서 억양과 발음의 comprehensibility가 향상되었다고 합니다. 통제집단에 비해 실험집단이 발음 comprehensibility가 더 향상되었고요, 억양의 향상 정도는 큰 차이가 없었다고 합니다(내가 맞게 해석했다면ㅎ;;). 그러니 ASR 활용한 활동이 동료피드백과 결합될 때 발음의 intelligibility가 더 향상된다는 결론이 도출됩니다.​spontatneous conversation에서는 어땠을까여. 역시 pretest에 비해 posttest에서 두 집단 모두 유의미한 향상이 있었고, 향상 정도는 실험 집단이 더 컸다고 합니다. 연구 후 통제 집단은 아이엘츠 스피킹 루브릭 기준 Level 4(발음이 제한됨, 대화를 이해하기 어려울 정도의 잘못된 발음 있음)라면 실험집단은 Level 5(mispronunciation의 빈도가 훨씬 적음)로 평가되었습니다.​ASR 소프트웨어의 만족도와 사용의 용이성에 대해서도 참여자들은 긍정적으로 응답했습니다.  내가 임용고시를 공부할 때에는 CALL(Computer Assisted Language Learning) 활동은 그냥 대충 읽어만 보고 넘겼다. 어차피 못할거 아니까ㅋㅋㅋㅋ그런데 이제 각 학교의 1학년들이 개인 스마트 디바이스를 쓰게 되었고 그에 따라 영어 시간에 할 수 있는 수업의 폭도 훨씬 넓어진 만큼 CALL 활동도 더이상 이론 속에만 존재하는 활동이 아니게 되었다고 생각합니다. 그간 발음이나 말하기 지도가 전통적인 교실 수업에서 어려웠던 이유는 개별화된 지도나 수업 자료 지원이 어렵기 때문이었다고 생각하고 있는데, 그런 어려움이 많이 해소된 만큼 말하기나 발음 수업도 전보다 더 적극적으로 할 수 있지 않을까. 게다가 이 논문은 다른 연구와 달리 대단한 인지적인 능력을 요하는 학습을 한게 아니기 때문에 그대로 중학교 수업시간에 바로 적용해도 될 정도로 쉽고 실용적임. 한국어와 영어의 phonemes를 좀 더 공부해서 발음 수업을 만들어볼까한다. 좀 더 발전시키면 말하기 수업 및 평가도 설계할 수 있지 않을까??​사실 이게 초등학교에서는 실제로 수업시간에 이루어지고 있다면서요? EBS에서 초등학교 3학년에서 6학년 학생을 대상으로 AI펭톡을 개발해서 학생들이 자율적으로 영어 말하기와 듣기를 연습하고 대화에 참여할 수 있다고 합니다. EBS English인공지능 학습 메이트 회원 로그인 회원코드 도움말 비밀번호 확인 AI펭톡 공지사항 더보기 5월 봄소풍 이벤트 2022.05.13 4월 출석체크 챌린지 이벤트 당첨자 발표 2022.05.03 4월 출석체크 챌린지 이벤트 2022.03.30 AI 펭톡 1주년 생일 파티! 2022.02.28 AI펭톡 다운로드 PC 다운로드 앱 다운로드 가이드북 교과서 영어와 일상 영어를 함께 교육부 고시 초등영어 성취기준과 일상생활에서 자주 사용하는 영어 표현, 읽기, 듣기, 말하기까지 AI펭톡이 영어 말하기의 모든 것을 도와드려요! 게임처럼 재미있는 ...pengtalk-student.ebse.co.kr 인공지능 학습 메이트 AI펭톡 - Apps on Google PlayLearn to speak English with Pengsu and fun AIplay.google.com ‎인공지능 학습 메이트 AI펭톡‎펭수와 함께 하는 EBSⓔnglish AI 영어말하기 연습 인공지능 학습 메이트 AI펭톡 교과서 영어와 일상 영어를 함께 교육부 고시 초등영어 성취기준과 일상생활에서 자주 사용하는 영어 표현, 읽기, 듣기, 말하기까지 AI펭톡이 영어 말하기의 모든 것을 도와드려요! 게임처럼 재미있는 영어 말하기 연습 AI펭톡에서 참치캔과 스탬프를 모아보세요. 재미있는 게임처럼 영어 말하기 연습을 해보세요! 똑똑한 AI가 평가하는 영어 학습 현황 AI가 알려주는 영어 발음 평가와 단어, 문장, 대화, 표현, 유창 등 분야별 영어 말하기 실력…apps.apple.com 궁금해서 다운받아봤는데 담당교사한테 학습코드를 받아야된대서 못해봄(쭈굴). 관리자 신청을 한번 해볼까. EBS여 중고등학생용 영어 스피킹 리스닝 앱도 좀 만들어주시오,,, "
"""Screen Time""의 힘: 유,초등 언어 및 문해력 향상을 위한 디지털 미디어 활용법(1) ",https://blog.naver.com/sunyijoo/223110738447,20230524,"The Power of ""Screen Time"": Harnessing It to Promote Language and Literacy Learning in Early Childhood and Elementary School출처) American Federation of Teachers ERIC - EJ1281523 - The Power of ""Screen Time"": Harnessing It to Promote Language and Literacy Learning in Early Childhood and Elementary School, American Educator, 2021Headlines about the negative effects of screen time may alarm teachers and cause them to worry about using digital media with early childhood and elementary school students. However, the relationship between digital media use and language and literacy learning is complex, and there are, in fact, arg...eric.ed.gov 첨부파일The Power of Screen Time.pdf파일 다운로드 지난 몇 년간 참여한 프로젝트의 대부분이 앱이나 온라인을 통한 디지털 프로그램 개발이었어요. 특히 코로나를 맞아 유치원과 어린이집에도 본격적인 ""교실영어""라는 형태의 서비스가 인기를 끌면서 비대면 학습이 주를 이루게 되었습니다. 교육에서 디지털 미디어 사용을 ""찬성""하는 주장과 ""반대""하는 주장은 오랫동안의 관심사였습니다. 학교나 원에서 뿐만 아니라 가정에서도 스크린 사용이 많아지면서, 디지털 미디어 사용의 이점과 단점을 이해하는 것은 그 어느 때보다 중요한 것 같아요. ​최근 나온 관련 연구/보고의 글을 읽고 몇 차례 나눠서 요약 정리해 보려고 합니다. 이 자료는 화면을 이용하여 언어와 문해력을 최대한 발달시키는 방법에 대한 것입니다. 학습 디지털 미디어를 기획할 때 참고하면 좋은 구체적인 사례들이 포함되어 있어요. [용어] 디지털 미디어(Digital media): 기술을 통해 전달되는 콘텐츠를 설명하는 광범위한 용어: 텍스트, 이미지, 오디오, 애니메이션, 비디오 및 인터랙티브(interactives)를 포함​핫스팟(Hotspots) : clickable locations on the screen that activate animations and/or sounds​    - 화면에서 애니메이션 및/또는 사운드를 활성화하는 클릭이 가능한 위치디지털 미디어 학습과 관련된 평가반대 Against 시각과 청각이 풍부하고, 중요한 정보에 선택적으로 집중하고 처리하는 능력을 과도하게 요구하여 학습을 저해찬성 For 콘텐츠를 보다 집중적이고 일관성 있게 언어 및 비언어적으로 제시하여 정보 습득과 유지에 도움​디지털 학습에 미치는 요소들(factors)the presentation of the content 콘텐츠 제시(발표)the context of the digital media use 디지털 미디어 사용의 맥락the ages and backgrounds of the children 아이들의 나이와 배경   Guiding principles to help educators harness the power of screen time            to promote (not hinder) language and literacy learning 교육자가 스크린 타임의 힘을 활용하여 언어 및 리터러시 학습을 촉진(방해하지 않음)하도록 돕는 지도 원칙​​1. Digital Media Can Enhance Instruction 2. Digital Media Can Support Equity and Inclusion 3. Digital Media Can Promote Engagement and Motivation4. Digital Media Can Leverage Home-School Connections디지털 미디어는 수업을 향상시킬 수 있다디지털 미디어는 형평성과 포용성을 지원할 수 있다디지털지털 미디어는 참여와 동기 부여를 촉진할 수 있다디지털 미디어는 가정과 학교 연결에 활용될 수 있다​  Digital Media Can Enhance Instruction 디지털 미디어는 수업을 향상시킬 수 있다 방법) 교사가 목표로 삼고 있는 기술이나 연습을 강화하거나 도움이 될 디지털 미디어를 식별하기      - To identify digital media that can help to reinforce or provide practice with skills or concepts teachers are targeting  ​[CASE STUDY] 1) 디지털 미디어 활용 연구연구 대상: 유치원생 kindergartners from low-socioeconomic backgrounds학습 목표: 텍스트 단어 소리 주의 집중 교육 instruction focused their attention on the sounds of words in the text방법 : e book 사용요소: 텍스트 음성 변환(text to speech), 강조 표시된 단어(highlighted words),                    텍스트에 나타난 문자, 사물  또는 단어를 클릭하여 활성화할 수 있는 대화형 hot spots 포함 교육 효과를 위한 장치방해 요소를 최소화하기 위해 내레이션이 끝날 때까지 hot spots을 클릭할 수 없음스토리 이해력을 높일 수 있도록 캐릭터 또는 개체 hot spots은 대화 또는 음향 효과를 활성화단어 hot spot: 내레이터가 단어를 음절로 나누도록 하여 단어 인식과 음운 인식을 촉진중요! 전자책이 교육 내용과 일치하고 관련 없는 정보를 포함하지 않음연구 결과 : 인쇄물, 단어 읽기, 음운 인식(print, word reading, and phonological awareness)에 대한 개념 이해도 향상  2) 교사의 지도와 디지털 미디어 사용 결합 연구연구 대상: 미취학 아동preschoolers학습 목표: 어휘력과 개념 발달을 촉진 vocabulary and conceptual development방법:  정보 소개 Sesame Street 나 Elmo’s World 동영상 사용, 건강에 좋은 음식과 야생 동물과 같은 개념 범주에 대한 정보 소개 -  아이들의 이해를 돕기 위해 시각적 정보와 언어적 정보를 결합한 클립(to pair visual information with verbal information)을 선택수업  1) 소개 a video - 곤충에 초점을 맞춘 비디오                  : 해당 카테고리에 대한 정의와 설명 제공, 대표적 예시 곤충인 여치를 보여줌             2) 본 학습 Teacher-led discussion - 교사가 주도하는 비디오에 대한 토론              : 곤충의 특징에 초점             3) 복습  a read-aloud - 같은 주제에 대한 정보 책 읽기/낭독              : 소개된 단어와 개념(예: antennae) 복습(review)             4) 중요! 비디오의 정보는 책의 정보와 일치,  다양한 맥락에서 주제 학습 기회 제공연구 결과 : 어휘력과 개념 지식 향상   3) 수업과 연계된 웹 기반 디지털 미디어 사용 연구연구 대상: 캐나다 전역의 k-2 교실의 교사 및 어린이들학습 목표: 웹 기반 프로그램을 정규 언어 교육과 통합방법:  읽기(글자 및 소리, 유창성, 이해력, 쓰기/철자)에 초점을 맞춘 모듈이 포함된                               ABRACADABRA라는 프로그램을 사용 literacy.concordia.caliteracy.concordia.ca 수업  1) 소개: 과일에 관한 디지털 스토리에 참여             2) 본 학습 : 아이들에게 자신이 먹는 과일에 대해 그림을 그리고 라벨을 붙이기                                 또는 자신이 먹는 과일에 대해 글 쓰기             3) 언어 및 문해력에 대한 추가 지원additional support 가능연구 결과 : early literacy tasks such as phonological blending and letter-sound recognition                     음운 블렌딩 및 문자-소리 인식과 같은 초기 읽기 쓰기 작업에서 더 잘 수행 Providing content in multiple ways, and providing a more representative assortment of content, can be enriching for all students. 기타 시사점내레이터가 말해주는 자막과 강조 표시된 텍스트(closed captioning)는 단어 인식 능력(word recognition skills) 향상과 관련 : 텍스트와 음성을 동기화하면 어린이가 읽기를 배우는 단어에서 글자와 소리를 연결하는 능력이 향상​대화나 나레이션과 함께 삽화와 애니메이션이 포함된 풍부한 콘텐츠가 있는 비디오, 전자책 및 기타 디지털 도구는 어휘력과 이해력에 긍정적인 영향 : 삽화와 애니메이션이 콘텐츠와 직접적으로 연관되어 있어 정보의 시각 및 청각적 처리를 지원하기 때문​디지털 도구는 여러 교육 목표를 한번에 해결할 수 있다. GOOD! 특정한 문해력을 촉진하기 위해 의도적으로 선택된 기능 추가 carefully crafted e-books for kindergartners and first-graders      -> 결과) 단어 읽기와 어휘력이 크게 향상 ▶ segmented speech to support phonological awareness ▶ highlighted text to support word recognition▶ oral reading to support fluency▶ visuals of particular words to support vocabulary▶ dramatization with action and music intended to facilitate comprehension​▶음운 인식을 지원하는 세분화된 음성 ▶단어 인지를 지원하는 강조 표시된 텍스트▶유창성을 지원하는 구두 읽기▶어휘력 향상을 위한 특정 단어의 시각적 효과▶이해를 돕기 위한 액션과 음악을 사용한 극화  BAD! 학습을 방해(detract)하는 동화 전자책 이야기 나레이션 중에 어린이가 관련 없는 hotspot을 클릭할 수 있도록 허용       예) opening and closing a window on a page when Little Red Riding Hood’s mother is asking her              to bring her sick grandmother some treats각 페이지에서 관련 없는 게임을 하기         예) “painting”the scene from the story 이야기의 장면 그리기광고 보여주기​​디지털 미디어 도구(digital media tools) 선택시 고려할 점가르치려는 기술(skills)이나 개념을 지원하고, 지도 방식과 일치하는가?의도적으로 가장 중요한 내용을 시각적, 언어적으로 상호 보완하여 제시하는가?기술이나 개념 학습을 방해할 요소는 없는가? ​#리터터시 교육, #digitalmedia, #유아교육, #멀티미디어​​​​ "
인간과 컴퓨터 상호작용기술 ,https://blog.naver.com/hdjunkr/221873892310,20200326,"#HCI(Human-Computer Interaction)는 인간과 컴퓨터의 상호작용 기술을 연구하는 분야로, 시간과 장소의 구분없이 각종 디지털 서비스를 제공받을 수 있게 해주는 유비쿼터스 사회의 핵심 기술입니다.   정보통신 분야의 유망 기술 사이클 정보통신 하이퍼 사이클 ● 가트너의 전망: 인간과 컴퓨터 상호 작용 관련 기술의 성장 전망•자연어 처리 검색 기술•생체 측정 기술•음성 인식 기술•음성 합성 기술•인터페이스 기술 문자기반 인터페이스에서 그래픽기반 인터페이스로 전환 배우기 쉽고, 빠른 인식이 가능​ HCI 주요기술 및 밴더 현황 인간과 컴퓨터 상호작용이란?● 인간과 컴퓨터 상호작용(HCI) - 어떻게 하면 사람들이 쉽고 편하게 컴퓨터 시스템과 상호작용할 수 있는가를 연구하는 학문 - 인간과 컴퓨터 상호작용뿐만 아니라 컴퓨터를 매개하여 인간과 인간 사이에서 발생하는 상호작용도 포함하는 개념 - 인간, 컴퓨터, 상호작용, 태스크(Task), 상황(환경 정황)으로 구성  HCI 구성요소 주요특징 ● 인간과 컴퓨터 상호작용의 목적 - 기능성과 사용 편의성(Usability), 안전성(Safety), 효율성/효과성(Efficiency, Effectiveness)이 고려된 컴퓨터 시스템을 만드는 것 - HCI의 가장 중요한 요소(Factor) 중의 하나•그 시스템을 사용할 사람의 인지 프로세스를“어떻게 하면 컴퓨터와의 상호작용에 잘 적용하느냐”하는 것​인간과 컴퓨터 상호작용 역사  HCI 역사 ● 인간과 컴퓨터 상호작용의 중요성 및 발전 단계 HCI 발전단계 ● 인간과 컴퓨터 상호작용의 연구 분야 - 산업사회의 컴퓨터 하드웨어 및 소프트웨어 관련 시스템을 연구, 개발함에 있어서 생리적, 지능적, 감성적 특성 등의 인간 요소를 고려한 사용자 중심 설계를 추구하여 사용자 편의성과 수용성(Acceptability)이 우수한 인간 최적합의 컴퓨터 시스템을 구축하기 위한 연구 분야• - 학문 분야별 관련 연구 내용•인간 공학: 인간의 감각이나 운동에 관한 연구•산업 공학 : 컴퓨터와 조직에 관한 연구•심리학 : 행동과 개념에 대한 연구•인지 심리학 : 인지, 관심, 기억과 문제 해결, 사용자의 능력과 한계, 학습 등에 대한 연구•사회 심리학 : 사회적인 상황에서 인간의 행동•사회학 : 좁게는 작업 공간에서 크게는 사회에 영향을 주는 발전된 기술에 관한 연구•컴퓨터 과학 : 사용자 인터페이스를 위한 알고리즘 개발, 시스템 구조나 소프트웨어의 개발에 관한 연구•언어학 : 언어에 대한 연구​#인간과컴퓨터상호작용 관련기술● 음성인식 기술 - 음성인식(Speech Recognition)•인간의 음성을 기계나 컴퓨터에서 자동으로 인식하는 것으로 일반적으로 음성 신호로부터 음소 텍스트(발성된 문자열)를 인식하고 출력할 때까지를 의미 - 화자인식(Speaker Recognition) 기술•넓은 의미에서 말하는 사람, 즉 화자의 인식과 자연어 처리를 포함한, 음성 이해와 의미 추출까지를 포함 특정 화자에 대해서만 인식하는 화자 종속(Speaker-Dependent) 시스템 화자와는 상관없이 인식하는 화자 독립(Speaker-Independent) 시스템 화자 인식(Speaker Recognition)은 화자 식별(Speaker Identification)과 화자 검증(Speaker Verification)으로 분류​● 음성 합성 기술 - 음성 합성(Text-to-Speech or Speech Synthesis)이란 문자 그대로 글(text : ASCII text 또는 Machinereadable text)을 말(speech)로 바꾸는 기술- 기계적인 목소리에 대한 거부감으로 시각장애인용과 같이 극히 제한된 영역에서만 이용됨- 최근에는 인간의 자연음에 가까운 음성 합성기가 개발되어 비정서적인 면에서도 많이 개선됨​- 음성 합성 기술의 응용 분야•이메일 음성 서비스•실시간 교통 정보를 운전자에게 제공하는 차량 네비게이션 시스템•시각 및 청각 장애인을 위한 도우미 서비스•전자사전, 자동번역기, 장난감, 게임 등의 소비자 제품•타이핑된 문서의 오타 확인을 위한 교정기 등​● 자연어처리 기술(NLP) - 컴퓨터가 사람의 일상 언어를 이해하고 생성할 수 있도록 함으로써 인간의 지적 활동의 보조자 및 지원 도구로 활용하고자 하는 기술 - 사람들이 특별한 형식의 언어나 명령어 없이도 컴퓨터와 교류할 수 있게 함 - 자연어 처리(NLP) 기반 검색 기술은 인터넷상에 존재하는 정형, 비정형 데이터베이스 내의 정보를 인간의 일상 언어 문장을 통해 제공하는 기술 - 음성인식 기술과 결합한 자연어 처리 기술이 상용화되고 있 - 음성인식과 자연어 처리 기술이 조합된 구어체 시스템은 사용자들이 정보 검색 작업을 위해 음성 명령으로 데이터베이스를 조회할 수 있도록 해줌​인간과 컴퓨터 상호작용 기술의 전망● 차세대 컴퓨팅 - 컴퓨팅 기능이 주위 환경에 내재되어 이로부터 정보를 획득/활용하거나 사용자가 인식하지 못하는 상태에서도 컴퓨팅 기능을 수행할 수 있는 미래의 컴퓨팅 기술을 총칭 - HCI 기술은 다양한 형태의 인간 친화적인 정보 단말기기를 통해 개인화된 서비스를 제공하는 음성, 시각, 촉각, 후각, 미각 등 오감 정보처리 기술을 위한 차세대 사용자 인터페이스 기술의 개발 진행 중- 일반적인 상호 작용 수단으로 사용된 유니모달(Unimodal)에서 벗어나, 시각, 청각 이외에도 다양한 인터페이스 기술을 실현하기 위하여 멀티모달 기반의 인터페이스 기술로 발전- 촉각형 장치 등을 이용한 촉각 인터페이스 기술도 사용 - 오감 정보 처리 기술•시각, 청각, 촉각 중심에서 후각, 미각 정보 처리와 오감을 융합 재현하여 현실감 있는 서비스를 제공하는 기술로 발전•차세대 플랫폼의 소형화, 에이전트 소프트웨어에 의한 지능화와 아울러 인간과 컴퓨터의 상호작용에 의한 실감화 추세로 발전될 전망 차세대 컴퓨팅, 지능형 로봇 분야 HCI 요소 기술 범위 이미 인간과 컴퓨터 상호작용 기술은 많은 분야에 응용되어 실 생활에 사용되어 지고 있다 "
ChatGpt로 본 인공지능 문제점과 국내 기술 수준은 ,https://blog.naver.com/the-issue/223076398839,20230416,"□ 들어가며​인공지능(Artificial Intelligence, AI)은 우리 사회의 다양한 분야에서 활용되고 있는 기술로, 인간의 지능을 모방하여 컴퓨터가 스스로 학습하고 문제를 해결할 수 있도록 하는 것이다. 인공지능은 크게 기계학습(machine learning), 딥러닝(deep learning), 자연어 처리(natural language processing), 컴퓨터 비전(computer vision), 음성 인식(speech recognition) 등의 분야로 나눌 수 있다.​현재 AI 기술은 스마트폰, 자율주행차, 음성인식, 번역, 의료, 교육 등 다양한 분야에서 사용되며 우리의 삶에 깊숙이 관여하고 있다. 하지만 인공지능 기술이 빠르게 발전하면서 AI의 위협에 대해 지적하는 전문가들이 늘어나고 있다.​예를 들어 일론 머스크와 스티븐 호킹은 터미네이터 시리즈의 인공지능 네트워크인 스카이넷이 핵 전쟁을 일으켜 인류에게 위협을 가한 것처럼 AI는 인류의 존재 자체를 위협할 수 있다고 경고하고 있다.​그러나 인공지능의 발전이 인간의 지식과 창조력을 확장하고 문제를 해결하는 데 도움을 주며 인류에게 긍정적인 영향을 줄 것이라는 주장도 있다. 특히 최근 OpenAI에서 개발한 대화형 AI인 ChatGPT가 인기를 얻으면서 이에 기반한 다양한 서비스가 만들어지면 이에 대한 다양한 논의가 시작하고 있다. □ ChatGPT와 같은 대화형 AI가 인기를 얻는 이유는?​인간의 언어와 지식을 모방하고 활용하는 기술인 인공지능(AI)은 다양한 산업 분야에서 혁신적인 기술을 개발하는 데 중요한 역할을 하며 생산성을 높이는데 기여하고 있다.​과거에는 인공지능이 로봇과 같은 하드웨어와 결합하여 인간의 육체노동을 대신할 것이라고 생각했지만, 2016년 딥러닝에 기반한 알파고가 이세돌 9단과의 바둑 대국에서 승리하면서 현재는 AI가 정신노동을 먼저 대체할 것이라는 주장이 힘을 얻고 있다. 딥러닝(Deep Learning)은 인공신경망(Artificial Neural Networks)을 이용한 기계학습의 한 분야로, 인공신경망은 생물학적 뉴런의 작동 원리를 모방하여 만들어진 알고리즘으로, 다층 인공신경망을 이용하여 입력 데이터로부터 출력을 예측하는 데에 사용된다.특히 최근에는 자연어 처리(nlp) 기술을 활용하여 사람과 대화를 할 수 있는 ChatGPT와 같은 인공지능(ai) 챗봇이 개발되면서 AI를 활용한 화가, 작가, 만화가, 작곡 등 분야의 서비스가 폭발적으로 늘어나고 있다. NLP는 ""자연어 처리""를 의미하는 인공 지능 분야에서 중요한 기술이다. 자연어 처리는 인간이 사용하는 언어를 기계가 이해하고 분석하는 기술로 컴퓨터가 텍스트, 음성 또는 이메일과 같은 자연어 데이터를 이해하고 해석하도록 도와주는 여러 기술과 기법을 포함한다. 현재 머신 러닝, 딥 러닝 및 자연어 이해를 포함한 다양한 기술과 알고리즘을 통해 기계 번역, 문서 요약, 질문 응답 시스템, 챗봇 및 음성 인식과 같은 응용 분야에서 활용되고 있다.​Open AI에서 개발한 ChatGPT는 다양한 주제와 스타일로 대화하며 다양한 정보를 제공하기 때문에, 서비스가 발표된 이후 인스타그램과 틱톡보다 빠르게 1억 구독자를 달성했다.​챗봇은 자연어 처리 기술과 인공지능 기술을 활용하여, 사용자와 대화하는 컴퓨터 프로그램으로 현재 메신저, 웹사이트, 음성인식 기술 등 다양한 서비스에 이용되고 있다.ChatGPT와 같이 방대한 웹상의 자료(자연어)에 기반한 챗봇의 장점은 한정된 데이터에 기반한 기존 대화형 챗봇과는 다르게 사용자의 의도를 파악하여 적절한 답변을 제공한다는 것이다. 예를 들어, ""강남역에서 서울시청으로 가는 가장 빠른 방법은?""이라고 물어볼 경우, ChatGPT는 ""강남역에서 서울시청으로 가는 가장 빠른 방법은 지하철을 이용하는 것입니다. 강남역은 2호선과 신분당선이 지나가는 역으로, 서울시청은 1호선과 2호선이 지나가는 역이므로, 강남역에서 2호선에 탑승하여 시청 방향으로 가다가 시청역에서 1호선으로 환승하면 됩니다.""와 같이 인터넷 내의 자료를 비교하여 비교적 정확한 답변을 한다는 점이다.​이와 같이 ChatGPT는 최신의 언어 모델을 기반으로 풍부한 어휘와 문법, 그리고 상황에 맞는 표현력을 갖추고 있다는 장점으로 인해 쇼핑몰 챗봇, 가상 비서, 교육 도구, 글쓰기 도구, 언어 번역 도구 등에 다양하게 활용되고 있다. □ ChatGPT의 단점은?​위에서 설명한 ChatGPT의 장점에도 불구하고 전문가들은 대화형 인공지능이 다양한 문제점을 가지고 있다고 지적한다.​첫째, 부적절한 내용을 생성하거나 응답할 수 있다. 인공지능은 사전에 학습된 데이터를 바탕으로 텍스트를 생성하거나 응답하기 때문에 데이터가 불완전하거나 편향된 경우 잘못된 정보를 만들어낼 수 있기 때문이다. 예를들어 ‘판교역에서 서울시청으로 가는 가장 빠른 길’을 ChatGPT에게 물어보면 ‘판교역에서 2호선을 탑승한다’라는 잘못된 정보를 현재 생성하고 있다. 판교역에서 2호선을 탑승하기 위해서는 먼저 신분당선을 이용해 강남역으로 이동해 2호선으로 환승해야 한다. 물론 이런 답변은 웹상에 관련 데이터가 늘어날수록 정확하게 수정될 것이다.AI는 글을 생성할 때, 가장 그럴듯한 글을 작성하게 되는데, 이는 잘못된 정보를 전파할 가능성이 높은 상황을 만들기도 한다. 예를 들어 ‘이순신 장군이 만든 세계 최초의 잠수함 거북선에 대해 설명해 줘’라고 질문하면 ChatGPT는 ‘거북선은 조선시대의 장군 이순신이 16세기 말에 만든 세계 최초의 잠수함입니다. 거북선은 굉장히 혁신적인 선박으로, 선체 하부에 붙은 특수한 장치를 이용하여 잠수할 수 있었습니다. 또한 광주리나 철판 등으로 보호되어 있어 적의 화살이나 총탄에도 견딜 수 있었습니다.’라며 ‘대한민국 정부는 거북선을 상징으로 사용되는 거북선 문양을 국보 제1호로 지정하였으며, 거북선 모형도 다양하게 전시되고 있습니다.’라고 잘못된 답변을 했다. 그러나 ChatGPT에 기반한 MS의 Bing AI는 검색엔진의 결과를 비교하여 결과를 만들기 때문에 같은 질문에 ‘이순신 장군이 만든 거북선은 임진왜란 직전에 이순신 장군의 고안으로 건조된 전선(戰船)입니다. 1591년(선조 24년) 정읍현감 이순신은 전라좌수도 수군절도사로 부임하여 왜적의 침입에 대비해 거북선을 만들 것을 결심합니다1. 거북선은 단 세척이며, 전라좌수영선소(여수시 중앙동)에서 건조되었습니다’와 같이 답변을 했다. 유료인 ChatGPT 최신버전은 더 많은 데이터를 활용하기 때문에 다른 답변이 나올 수도 있을 것이다.둘째, 인공지능은 윤리적이거나 책임감을 가지지 않는다. 인공지능은 인간의 가치관이나 도덕적인 판단을 고려하지 않고 기존 자료를 변형하여 텍스트를 생성해 응답하기 때문에 논란의 여지가 있는 내용을 만들 수 있다.​마지막으로 예술적인 분야의 활용이 늘어나고 있지만 인공지능은 창의적이거나 독창적이지 않다. 인공지능은 기존의 텍스트를 재구성하거나 변형할 뿐, 새로운 아이디어나 발상을 제시할 수 없기 때문에 반복적이거나 변형된 내용을 생성할 뿐이다. □ 국내 인공지능 기술 수준은?​최근 IDC 보고서에 따르면 2022년 전 세계 AI 시장 매출액은 전년 대비 19.6% 성장한 4,328억 달러로 기록됐으며, 올해에는 약 5천억 달러를 돌파할 것으로 예상된다. 특히 AI 소프트웨어가 전체 시장의 88.3%를 점유하는 가운데, 그중 50% 정도는 애플리케이션에, 35%는 시스템 인프라 소프트웨어에 사용된 것으로 나타났다.​그렇다면 국내의 인공지능 기술 수준은 어느 정도일까? 이에 대한 정답은 없지만, 여러 가지 지표와 성과를 통해 대략적인 추정을 할 수 있다. 영국 데이터 분석 미디어인 토터스인텔리전스(Tortoise Intelligence)의 ‘글로벌 AI 지수 조사’에 따르면 세계 AI 경쟁력은 미국, 중국, 영국, 캐나다, 이스라엘, 싱가포르, 한국 순이다.​Chat-GPT에서 봤듯이 현재 AI 분야 세계 최고 기술 보유국은 미국으로 나타났다. 미국의 기술 수준에 도달하는 데 소요될 것으로 예상되는 ‘기술격차’는 중국(0.8년), 유럽(1.0년), 한국(1.3년), 일본(1.5년) 순이다.​우리나라의 경우 2020년에는 세계 최초로 인공지능 전문대학인 한국과학기술원(KAIST) 인공지능대학원이 개설하는 등 관련 분야의 투자가 이뤄지면서 2016년에 약 2.2년의 격차를 보인 미국과의 기술 수준이 2021년에는 약 1.3년으로 약 0.9년만큼 축소되는 성과를 보이고 있지만 AI 인재(28위) 및 운영환경(32위)의 부진으로 최상위권과의 격차를 줄이기는 어려운 상황이라는 분석도 있다.​특히 중국의 기술 수준이 2016년 약 2.3년에서 2021년에 약 0.8년으로 6년간 1.5년을 줄일 정도로 가파르게 성장하고 있어 우리에게는 가장 큰 위협이 되고 있는 실정이다.​앞으로 AI 기술 패권 경쟁이 계속될 것으로 전망되는 바, 한국이 그간의 성과를 넘어 AI 강국으로 도약할 수 있는 질적 성장과 전략을 마련하기 위해서는 국내의 인공지능 기술 수준을 높일 수 있도록 정부와 기업, 학계, 연구소 등의 다양한 주체들이 협력하고 지속적으로 노력해야 할 것이다. "
Whisper-youtube-crosslingual-subtitles  ,https://blog.naver.com/aitutor21/223010803011,20230209,https://huggingface.co/spaces/RASMUS/Whisper-youtube-crosslingual-subtitles Whisper Youtube Crosslingual Subtitles - a Hugging Face Space by RASMUSSpaces: RASMUS / Whisper-youtube-crosslingual-subtitles Copied like 40 Running App Files Community 2 Linked datasetshuggingface.co This space allows you to:Download youtube video with a given urlWatch it in the first video componentRun automatic speech recognition on the video using fast Whisper modelsTranslate the recognized transcriptions to 26 languages supported by deepLDownload generated subtitles in .vtt and .srt formatsWatch the the original video with generated subtitles​ 
[머신러닝 기초] Deep Learning ,https://blog.naver.com/moondatascientist/222966173171,20221226,"Neural Network 하드웨어의 발전-> 많은 파라미터 학습에 소요되는 시간을 단축웹의 발전-> 빅데이터 수집-> 많은 파라미터 학습에 필요한 데이터 확보알고리즘의 개선​인공지능 :  주어진 룰 없이 컴퓨터 프로그램이 스스로 학습/행동 모방할 수 있는 기법초창기 - Rule-based 기반의 모델, 데이터 작고 간단한 task에 효과적 -> 통계 기반의 머신러닝 기법 -> neural network -> deep learning ​전통적인 머신러닝 모델 vs 딥러닝input -> feature extraction -> 머신러닝 모델(classification, regression 등)으로 input 표현 -> output과의 관계 학습딥러닝은 feature ectraction과 모델 학습 단계가 통합됨, 많은 데이터에서 효과적end-to-end learning, 모델이 어떻게 학습됐는지 알 수 없는 블랙박스 문제​예, CNN, Image Classification, RNN​자연어 처리에서 딥러닝 모델의 적용(RNN)영상 데이터와 다르게 순차적으로 데이터 입력예, Speech Recognition : 사람의 음성을 자연어 텍스트로 변환, RNN 기반의 모델 적용 -> 대화 상황, 시끄러운 환경에서도 효과적으로 인식Machine Translation : 통계 기반의 기계학습 모델 -> RNN 기반의 모델 적용을 통해 성능 개선 ​​​​ "
"음성인식 PC 키보드 서버 앱과 ""음성인식PC키보드""과 연동 음성으로 웹서핑, 워드, 파워포인트 등등 운용 및 타이핑, 캡처기능 내장, 다국어 음성인식타이핑 가능한 프로그램입니다. ",https://blog.naver.com/skywon65/222489400073,20210830,"소프트웨어 소개언어 : 한글/영어 | 크기 : 7MB앱과 ""음성인식PC키보드""과 연동 음성으로 웹서핑, 워드, 파워포인트 등등 운용 및 타이핑, 캡처기능 내장, 다국어 음성인식타이핑 가능한 프로그램입니다.​ 첨부파일SpeechRecognitionKeyboard_V1.2.27.99.exe파일 다운로드 주요기능음성인식PC키보드앱(안드로이드앱)과 본 프로그램(Speech Recognition PC Keyboard Server)을 이용하여 음성으로 PC 조작. 웹서핑, 워드, 엑셀, 윈도우 단축키, 타이핑 등을 수행할수 있도록 만들었습니다(특히, 숫자 인식율이 높아 엑셀, 계산기에서 숫자입력이 편리합니다). 영어, 일본어, 중국어 등 11개국 언어 인식 워드 가능하며, 언어 전환을 쉽게 만들었습니다.(유의 : 음성명령은 등록된 언어(기본 한국어 - 변경가능)로만 가능하므로 유의하세요). 음성인식 PC 키보드 서버 앱과 ""음성인식PC키보드""과 연동 음성으로 웹서핑, 워드, 파워포인트 등등 운용 및 타이핑, 캡처기능 내장, 다국어 음성인식타이핑 가능한 프로그램입니다​ 첨부파일SpeechRecognitionKeyboard_V1.2.27.99.exe파일 다운로드 개선사항V1.2.27.99 (2017.05.15.)* 오늘의 메모 업데이트* 사용성 개선* 기타 사용자 의견 반영(마이너 패치) 음성인식 PC 키보드 서버 앱과 ""음성인식PC키보드""과 연동 음성으로 웹서핑, 워드, 파워포인트 등등 운용 및 타이핑, 캡처기능 내장, 다국어 음성인식타이핑 가능한 프로그램입니다​권장사양CPU펜티엄 2 400MHz메모리128MB그래픽카드1024*768 16비트 이상​설치방법http://blog.naver.com/shahn2001/220974891107  ​#음성인식PC키보드 #서버앱과 #음성인식PC키보드 #연동음성으로웹서핑 #워드 #파워포인트등등운용 #타이핑 #캡처기능내장 #다국어음성인식타이핑 #가능한 프로그램 "
[음성인식] 한국어 STT #1 ,https://blog.naver.com/peacel70/223000296253,20230131,"https://velog.io/@letgodchan0/%EC%9D%8C%EC%84%B1%EC%9D%B8%EC%8B%9D-%ED%95%9C%EA%B5%AD%EC%96%B4-STT-1 [음성인식] 한국어 STT #1외국인 발화 한국어 음성 인식률 향상 해커톤에 참가하면서 한국어 STT를 공부하고 구현한 내용을 정리해 보려 한다. 이번 포스터에서는 STT가 무엇인지, 실제 사용했던 오픈소스인 Kospeech에 대한 설명, 기본적인 개발환경 구축에 대해 작성할 생각이다. SSTvelog.io [음성인식] 한국어 STT #1letgodchan0·2021년 12월 30일Kospeech한국어 STT4STT란?모델 선정KospeechKospeech 환경설정가상환경💡한국어 STT(Kospeech)목록 보기1/5 외국인 발화 한국어 음성 인식률 향상 해커톤에 참가하면서 한국어 STT를 공부하고 구현한 내용을 정리해 보려 한다.이번 포스터에서는 STT가 무엇인지, 실제 사용했던 오픈소스인 Kospeech에 대한 설명, 기본적인 개발환경 구축에 대해 작성할 생각이다.STT란? ​음성인식(Speech Recognition)이란 사람이 말하는 음성 언어를 컴퓨터가 해석해 그 내용을 문자데이터로 전환하는 처리를 말하며 STT(Speech-to-Text)라고도 한다.기존에 상용 서비스에 적용되는 음향 모델의 대부분은 확률 통계 방식인 HMM(Hidden Markov Model) 기반으로 이루어졌으며, 2010년대 들어서면서 딥러닝 기반으로 HMM/DNN 방식으로 단어 인식 오류를 개선하여 20% 성능 향상을 이루어 냈다.최근에는 시퀀스-투-시퀀스(Sequence-to-Sequence) 방식의 RNN 기반으로 속도와 성능 면에서 좋은 결과를 가져오면서, 음성 인식에서도 번역어(End-to-End) 학습 방식의 발전으로 일련의 오디오 특징을 입력으로 일련의 글자(character) 또는 단어들을 출력으로 하는 단일 함수를 학습할 수 있게 되었다. 또한 CTC(Connectionist Temporal Classification) 이라는 모델로 입력 데이터와 레이블 사이의 음성 정렬(alignment) 정보가 없어도 학습이 가능하게 되었다. 이와 같은 다양한 학습법을 통해 계속해서 STT의 성능은 향상되고 있다.아래 링크를 통해 최근 STT 모델들의 WER(단어 오류율)과 CER(문자 오류율)을 확인할 수 있다.👉 최근 STT 모델모델 선정먼저 STT 오픈소스 모델을 선정할 때 2가지 정도를 고려했다.외국인 발화 한국어 음성의 경우 내국인 발화 음성과는 구분되는 특성이 있을 것이라는 여러 연구 결과에서 근거를 얻어 외국인 발화 음성만으로 학습시킨 모델을 만들고, 그 성능을 Pre-trained 모델과 비교해 볼 필요가 있다한국어 음성으로 구현된 STT 모델이 있는지 여부를 고려했는데, 대부분 오픈소스 STT 모델들은 그 성능이 영어에 한정해서 알려져 있었기 때문에, 한국어 음성에 대해서도 검증된 사례가 있는지 여부가 중요했다.그 결과 Kospeech가 제공하는 다양한 어쿠스틱 모델 중에서도 성능이 가장 우수한 것으로 알려진 Deepspeech2를 베이스 모델로 선택했다. Kospeech👉 깃허브👉 Kospeech 설명 영상 Kospeech는 2020년 김수환이라는 개발자가 공개한 한국어 음성인식 모델을 제공하는 오픈소스 툴킷이다. Kospeech의 모델들은 End-to-End 방식을 따르는데, 여기서 End-to-End는 음성 데이터가 포함하는 문법, 발음 등의 특징까지 모두 모델이 학습하도록 하는 방식을 말한다. 따라서 Raw audio를 통째로 input으로 넣어주는 것이 특징이다.즉, 오디오 신호가 입력되면 특징을 추출하고 특징들이 모델과 CTC 알고리즘을 통과하면서 텍스트로 출력된다.STT를 구현하기 위해 필요한 음성데이터를 AI hub의 Kspon이라는 1,000시간의 한국어 음성 데이터와 이를 전사해 놓은 Label을 가져와 3가지 방식으로 전처리를 제공한다.Pytorch 기반의 딥러닝 모델로 한국어만 지원하고 DeepSpeech2, Las, SpeechTransformer 등 다양한 모델과 프레임워크를 제공한다. Kospeech 환경설정먼저 기본적인 환경 같은 경우, 나는 해커톤 참가하고 노트북을 반납할꺼여서 따로 가상환경을 파지 않고 그대로 실행을 했다(복기해보면서 새로 가상환경 파긴 했다..ㅎㅎ) Window 10과 파이썬 3.8 기반으로 프로젝트를 실행했고, 다른 팀원들은 같은 조건으로 가상환경을 만들어서 실행했다.노트북 자체에 내장 GPU가 있었지만, 메모리 부족 등의 이슈로 로컬 환경에서 학습이 불가했기 때문에, 팀원들과 Colab pro를 결제해서 진행을 했다.가상환경 conda create -n kospeech python==3.8그래서 깃에서 kospeech를 가져왔다면 아나콘다에서 kospeech로 가상환경을 만들어 주자! conda activate kospeech가상환경을 실행한 뒤 아래처럼 kospeech-latest 폴더(setup.py)가 있는 경로로 들어간다. pip install -e .kospeech 깃 허브 readme에서 실행하라는대로 설치를 실행해준다.설치를 하다 보면 여러 이유로 충돌이 일어나고 엄청 빨간줄이 뜨지만 그래도 완료가 되긴한다..ㅎ막상 필요한 모듈을 설치를 했지만 나중에 없다고 에러가 뜨고 그랬다.그러면 그때 필요한 모듈을 바로 설치하는 식으로 진행을 했다💡여기까지 왔으면 기본적으로 Kospeech를 통한 한국어 STT 모델 구현을 위해 준비가 끝난거라고 생각하면 된다.다음 포스팅부터 본격적으로 kospeech를 활용하면서 겪은 시행착오에 대해 작성해 볼 생각이다!참고로 우리가 참가한 해커톤이다.. 진짜 할많하않... 🙂chanyeong kim다음 포스트[음성인식] 한국어 STT #2​ "
ChatGpt 로 음성인식 코드 만들기 ,https://blog.naver.com/el_melik/223048625057,20230318,"​ import speech_recognition as srfrom speech_recognition import Microphone# Initialize the recognizerr = sr.Recognizer()mic = sr.Microphone()with mic as source:    r.adjust_for_ambient_noise(source)    print(""Say something!"")    audio = r.listen(source)# Use Google Speech Recognition to recognize the audiotry:    #한국어!    text = r.recognize_google(audio, language='ko-KR')    print(""Google Speech Recognition thinks you said: "" + text)except sr.UnknownValueError:    print(""Google Speech Recognition could not understand audio"")except sr.RequestError as e:    print(""Could not request results from Google Speech Recognition service; {0}"".format(e))  먼저 speech_recognition 라이브러리를 가져옵니다.그런 다음 Recognizer 개체를 초기화합니다.Microphone 클래스를 사용하여 기본 마이크에 액세스하고 오디오 입력을 녹음합니다.Recognizer 개체의 listen 메서드는 마이크에서 오디오를 녹음하고 AudioData 개체를 반환합니다.Google 음성 인식을 사용하여 Recognizer 개체의 recognize_google 메서드를 호출하여 AudioData 개체의 오디오를 인식합니다. 이 메서드는 AudioData 개체를 인수로 사용하고 인식된 텍스트를 문자열로 반환합니다.인식된 텍스트를 콘솔에 출력합니다.Google 음성 인식 서비스를 사용하려면 활성 인터넷 연결이 필요합니다. 또한 시스템에 아직 설치되지 않은 경우 pyaudio 라이브러리를 설치해야 할 수도 있습니다.​ "
음성인식 ,https://blog.naver.com/sinsinsss12/222855285671,20220822,"마인드맵  I,(이슈선정 및 문제점 파악)탐구동기 또는 작품제작 동기음성인식이란?음성인식(Speech Recognition)이란 사람이 말하는 음성언어를 컴퓨터가 해석해 그 내용을 문자데이터로 전송하는 전환하는 처리를 말한다.I-1,음성인식을 이용했거나 영하에서 본 경험이 있다면 써봅시다.놓치마 과학18권에서 정신이가 한 사건을 맡게 되었는데 조사를 하고 있는데 감정왕 김감정그림의 문이 수상해보여서 열라고했는데 몇 번이나 열라고 했는데 안 열려서 정신이가""감정왕 김감정(그 사람 이름)정말 대단하군""이라고 했는데...이게 그 문의 암호였다.또 다른 한 개는 오빠가 시리안테 ""시리야,자폭해!""라도 했다.I-2,음성인식을 이용한 (또는 보았던)감상에 대해 써봅시다.둘 다 어이가 없었다.II,알고리즘/코딩&제작)하드웨어 산출물의 제작방법 및 제작과정II-1,음성인식 소프트웨어-음성인식을 하면 아두이노 장치에 명령을 보내는 어플리케이션(소프트웨어)II-2,무선통신(블루투스)를 할 HC-06블루투스 모듈 II-3,전등역할을 하는 RGB LED or 전등 (기타)전등을 제어하려 하는 경우전등을 켜고 끌 수 있는 릴레이(Relay)*높은 전앞을 제어해야 하므로 강점에 주의!​시연 영상   ​ "
미국 어학연수 - EF 샌프란시스코 ,https://blog.naver.com/raffles7/222862379412,20220830,"미국 샌프란시스코는 미서부의 대표적인 도시이죠?  골든 게이트 브리지, 피어 39, 피셔 멘즈 워프 등 관광지도 많지만, 스탠퍼드, UC 버클리 등 주변에 명문 대학이 있어서 교육의 중심지이기도 하지요. 대중교통도 잘 되어있어서 볼거리 즐길 거리가 많고, LA, 요세미티 내셔널 파크나 멀리는 라스베이거스까지도 미서부 여행하기 좋은 위치에 있답니다. ​오늘은 미국 샌프란시스코에 위치한 EF San Francisco 캠퍼스에 대해 안내드리려고 합니다.  EF 샌프란시스코는 Hult 샌프란시스코 대학교 & 대학원 건물 3층을 사용하고 있습니다.  한국인 학생 비율은 3% 정도로 낮은 편이고, EF 본사가 유럽에 있어서 주로 유럽 학생들의 비율이 높은 편입고 그다음으로 남미 학생들의 비율이 높은 편이랍니다.​ ​​​대학 건물에 위치해 있기 때문에 강의실이나 라운지는 관리가 잘되고 있고 규모도 큰 편입니다. EF 샌프란시스코 캠퍼스는 유니언스퀘어, 피어 39, 해변의 레스토랑가 까지도 도보로 이동할 수 있는 거리에 있어서 샌프란시스코를 제대로 경험하기에 매우 좋은 위치에 있습니다.​ ​​​EF 샌프란시스코는 대학 & 대학원 진학 준비과정(미서부 대학 진학에 관심 있는 학생들에게 적극 추천) 이 있는 캠퍼스입니다.그리고 25세 이상의  직장인 비율이 전체 학생 중 25~30% 정도인 것도 샌프란시스코 캠퍼스의 특징입니다.​​그래서  아래의 SPIN(EF만의  스페셜 인터레스트 과목: 일종의 선택 교양과목) 과목을 보시면 영어 랭귀지 이외에 커리어 디벨로프먼트나 대학 입학을 위한 토플 과목도 개설되어 있습니다.​​ ​​지금부터는 EF의 수업 방식에 대해 말씀드리겠습니다. EF의 어학연수과정은 다른 어학원보다 좀 더 듣기와 말하기 능력 항상에 포커스가 맞추어져 있습니다.​ ​위 영상에서 보신 EF의 수업 체계는 다음과 같이 5 가지의 수업으로 구성되어 있습니다.1. General Class : 저희도 익히 경험한 강남역 영어학원 같은 소규모 렉처 스타일 수업2. Conversation class : 다양한 디스커션 수업3. iPAD Class : 아이패드를 이용한 2-3명의 pair conversation 수업4. i Lab computer class: 유창한 영어를 위해서는 파트너와의 공부도 필요하나 학습자 스스로의 혼자 해야 할 공부들이 있죠? 바로 올바른 문법을 익히고 올바른 발음으로 자신의 생각을 정리하고 전달해야 합니다. 그를 위한 video lesson(온라인 문법 공부)과 speech recognition(발음 수정)을 컴퓨터를 통해 공부하고 itracker라는 툴을 통해 본인의 프로그레스를 확인할 수 있습니다.5. SPIN Class: Special Interest Class로 대학 교양처럼 어학관련 수업은 물론 미국 문학, 비즈니스, 경제 및 법률, 필름메이킹 같은 흥미 있는 과목 수업을 영어로 참여하는 수업입니다.​주당 수업 시간에 따라 일반과정과 집중과정을 고르 실 수 있습니다.​​ EF 샌프란시스코 집중과정​​​​ EF 샌프란시스코 일반과정​​​​EF는 영어수업 이외에도 다양한 액티비티가 있는 점이 특징이죠?​​​ 나파밸리 와인 테이스팅,식스플래그, 실리콘밸리 투어, 나파밸리 투어, 산타크루스, 레이크 타호, 문 터 레이베이, 요세미티 내셔널 파크, LA, 라스베이거스 여행까지 다양한 액티비티가 있습니다.​​ ​​​ 나파밸리 방문한 EF 샌프란시스코 학생들피셔 멘즈 워프 방문한 EF 샌프란시스코 학생들UC 버클리  방문한 EF 샌프란시스코 학생들금문교를 방문한  EF 샌프란시스코 학생들EF 샌프란시스코에서 단기 어학연수를 하시는 학생들은 아래의 더 허브 레지던스를 숙소로 제공합니다.  1인실 같은 경우에는 빨리 마감되므로 신청을 서둘러 주셔야 합니다.​​ ​​EF 샌프란시스코의 어학연수 프로그램에 대해 더 궁금하신 사항은 저희 링크에이드로 연락 주시면 친절히 답변드리겠습니다. ​  링크에이드는 EF 어학연수 입학 수속을 무료로 도와드리고 있습니다.입학 허가를 받은 이후 출국 및 현지 정착까지 세심히 도와드리고 있습니다.관심 있으신 분들은 카카오톡 아이디 linkaid로 카톡 상담 가능합니다. 전화가 편하신 분들은 010-2473-0049로 전화 주시면 친절히 안내해 드리겠습니다.​ ​ "
"Lecture 1. Supervised, Unsupervised, Reinforcement Learning & Their differences ",https://blog.naver.com/jhlim1996/222714786042,20220428,"Question we will answer: What is machine learning?What types of learning do we perform?Why study machine learning?What's the history of machine learning?What are the related fields to machine learning?What this course will cover.  What is Machine Learning? ""Machine learning is any process by which a system improves performance from experience."", Herbert SimonThe learning may not include understanding.E, T, P: Experience, Tasks, Performancelearn from Experiencewith respect to Tasksto improve PerformanceexampleHandwriting Recognition ProblemT: recognizing handwritten words within imagesP: percent of words correctly classifiedE: training of dataset of handwritten words with given classificationsClassifying FishT: classifying fish into ""salmon"" and ""sea bass""P: percent of fish correctly classifiedE: training of classification of fishChecker BotT: Playing checkers P: Percentage of games won against an arbitrary opponent E: Playing practice games against itself Autonomous DrivingT: Driving on four-lane highways using vision sensors P: Average distance traveled before a human-judged error E: A sequence of images and steering commands recorded while observing a human driver Spam Mail ClassifierT: Categorize email messages as spam or legitimate. P: Percentage of email messages correctly classified. E: Database of emails, some with human-given labels  A Simple Classification Problem Problem: The grades of students taking this courseKey steps:1. Data (what past experience can we rely on)2. Assumptions (what can we assume about the students or the course?)3. Representation (how do we ""summarize"" a student?)4. Estimation (how do we construct a map from students to grades?)5. Evaluation (how well are we predicting?)6. Model Selection (perhaps can we do even better?)  Why Study Machine Learning? ML can be applied to fields where there are no human experts.Modern automated manufacturing facilities (New, hence no experts yet)Need to predict machine failures before they occurBy analyzing sensor readings. Because the machines are new, ML can be applied to fields where there are human experts; are unable to explain their expertise.Many perceptual tasks, such as speech recognition, hand-writing recognition, and natural language understanding. ML can be applied to the problems where phenomena are changing rapidly. Prediction of future behavior ofStock marketConsumer purchasesExchange ratesML can be applied to the applications that need to be customizedA program to filter unwanted electronic mail messages.Different users will need different filters.ML can be applied to the applications where human experts exist, but the size of the problem (data) is simply too large. Not straightforward for machines to understand the problem as well as humans doGoogle searchGene expression analysis Recommender systems  Lessons Learned On ML Learning is using direct/indirect experience to approximate a chosen target function. Function approximation can be viewed as a search through a space of hypotheses (representations of functions) for one that best fits a set of training data. Different learning methods assume different hypothesis spaces (representation languages) and/or employ different search techniques.​ Different Algorithms Of ML Numerical functions- Linear regression - Neural networks (including deep learning) - Support vector machines ​Symbolic functions - Decision trees - Rules in propositional logic - Rules in first-order predicate logic ​Instance-based functions - Nearest-neighbor - Case-based ​Probabilistic Graphical Models - Naïve Bayes - Bayesian networks - Hidden-Markov Models (HMMs) - Probabilistic Context Free Grammars (PCFGs) - Markov networks​ ​ "
[IT 카드뉴스] AI 음성인식 회의/통화내용 1분이면 텍스트로! (클로바노트 활용Tip) ,https://blog.naver.com/ehostidc2004/222528757665,20211006,"​ 안녕하세요! 이호스트ICT입니다. ​ 팬데믹의 발생으로 비대면 사회가 지속되며 일반적인 근무 풍경 역시 뒤바뀌고 있습니다. 회의실에 다같이 모여 브리핑을 하는 대신 화상&원격 회의를 하며, 클라이언트와 직접 만나 이야기를 나누는 대신 컨퍼런스 콜을 하곤 합니다. ​ ​ 음성 통화를 통한 회의, 원격 회의의 내용 등을 회의록으로 작성해야 하는 일은 대게 각 팀의 막내가 하고 있으리라 생각이 되는데요. 이렇게 변화한 업무 환경으로 일지나 회의록 작성에 대한 부담감이 커졌을 지도 모를 것 같습니다. ​​인공지능의 발달이 현대인의 삶을 비약적으로 바꾸는 시대! 이러한 에로사항을 단박에 해결해줄 수 있는 IT 기술 “AI 음성인식”이 떠오르는 화두가 되고 있는데요. 오늘 포스팅에서는 AI 음성인식 기술과 네이버 클로바노트, 구글독스 등 실제적으로 활용할 수 있는 방법까지 소개 해드리도록 하겠습니다. ​ AI 음성인식 기술을 활용하면 약 1시간 분량의 음성을 단 1분만에 텍스트로 기록을 끝낼 수 있는데요. 코로나로 늘어난 영상회의나 컨퍼런스콜, 전화업무 등의 기반 업무를 보다 효율적으로 할 수 있는 하나의 팁이 되지 않을까 싶습니다. 막내 몫으로 여겨지던 회의록의 작성을 이제는 똑똑한 AI가 도맡아 해주고 있는 것입니다.   본격적인 포스팅에 앞서, 음성인식 기술의 개요에 대해 알아보겠습니다. ​​음성인식(Speech Recognition) 이란 단어 그대로, 사람이 말하는 음성 언어를 컴퓨터가 해석하여 그 내용을 문자 데이터로 전환하는 처리를 말하는데요. 이 때 음성을 문자로 전환하는 STT(Speech-to-Text)와 텍스트를 다시 음성으로 변환해주는 TTP(Text-to-Speech) 크게 두 가지 매커니즘으로 나뉘게 됩니다. 음성인식은 결국 키보드 대신 음성으로 문자를 입력할 수 있도록 해줄 것인데요. 우리는 언젠가 키보드나 마우스와 같은 입력 장치를 사용하지 않아도 기계와 더욱 효율적으로 소통을 할 수 있을 것입니다.​​이 기술이 상용화되고 더욱 고도화된다면 AI 컴퓨터와 인간이 대화하는 기술로까지 발달할 수 있는데요. 음성대화 인터페이스 기반 서비스 구성을 토대로 떠올려 볼 때, 현재 인간이 하고 있는 상담서비스나 간단한 질의 등이 챗봇이나 자동음성장치 등으로 대체될 수 있으리라 생각이 듭니다. ​ ​​AI 음성인식 기술을 활용할 수 있는 방법 첫번째는, 국내 최대 IT기업인 네이버의 클로바 노트입니다. 클로바노트는 메모와 북마크 기능이 탑재 되어 있어 더욱 간편한 사용이 가능한데요. 참석자의 목소리 구분도 가능하고, 해당 문서를 직접 교환할 필요 없이 링크로 공유할 수 있는 기능까지 더했습니다. 이 클로바 노트를 활용하기 위해서는 안드로이드나 앱스토어에서 앱을 다운로드 후, 네이버 아이디로 로그인하여 음성을 실시간으로 녹음하거나, 녹음되어 있는 음성 파일을 업로드 하면 된다고 합니다. ​​▶클로바 노트 다운로드 바로가기 ​​ ​ AI 음성인식의 활용 두 번째는 구글독스의 ‘음성입력’ 기능인데요. 다운로드가 필요 없이 크롬 브라우저에 접속하여 음성을 입력하면 텍스트로 변환시켜 주는 기능을 제공합니다. 이 때 녹음 기능을 이용할거라면 마이크가 필요하다는 사실을 숙지해야 할 것인데요. 다양한 언어 스크립트 작성에도 아주 편리한 활용이 가능하고 생각보다 정확도가 뛰어나 특히 유튜버들에게 각광받고 있는 기능이라고 합니다.  ​▶구글독스 (Googledocs) 바로가기 ​ ​ AI가 업무 환경에도 스며들며 보다 편리하고 효율적인 업무를 돕고 있는데요. AI 생태계는 추후 어떻게 변동될까요? 음성기반 플랫폼을 탑재한 인공지능 생태계는 현재 진행형으로 더욱 고도화, 전문화를 이루며 구축되고 있습니다. IoT와 음성 AI 기술을 기반으로 스마트홈, 자동차 등 현대인의 삶 전반적으로 접근하게 될 것입니다. ​​인간의 가장 기본적이고, 편리한 의사소통 체계는 바로 언어입니다. 사람과 기계가 소통하기 위한 AI 음성인식 기술이 더 발달하여 자연어, 사투리 등에 관여해서도 정확도가 향상된다면 음성을 통해 명령을 내리고 피드백을 받을 수 있는 기술이 보편화될 것입니다. 음성인식 기술의 발전이 인류에 새로운 산업의 장을 펼쳐줄 것을 기대합니다.  ​ ​인공지능 GPU 서버 전문 브랜드 ㈜이호스트아이씨티 AIOCP  서버 판매부터 견적, 임대까지 산업별로 완벽하게 컨설팅 제공 합니다. ☎1566-8757 ▲ 모바일에서 클릭 시 상담 문의가 가능합니다. ​ "
       Chat GPT의 모델인 NLP의 10가지      활용 사례 ,https://blog.naver.com/psb_0902/223036147630,20230306,"본문은 맞춤형 소프트웨어 개발 회사인 Daffodi Software의 블로그에서 발행한 글로 자연어 처리(NLP)의 10가지 흥미로운 응용 사례에 대해 알아보겠습니다.​이 글은 10 Interesting Applications of Natural Language Processing (NLP) 을 각색하여 작성하였습니다.​​ 10 Interesting Applications of Natural Language Processing (NLP)Natural Language Processing (NLP) is the underlying technology behind various high-end solutions today. Let's have a look at some of the areas where the tremendous potential of NLP can be used.insights.daffodilsw.com ​인간은 하루 평균 700개의 단어를 말합니다. 우리에게 생각과 정보를 교환하는 가장 일반적인 방법으로는메시지, 전화 통화, 책, 노래, 커뮤니케이션 등이 있습니다. 베르니케-리히트하임-게슈빈트(Wernicke-Lichtheim-Geschwind) 모델에 의하면, 인간의 두뇌는베르니케(Wernicke)영역이라고 부르는 특수한 언어 수용 센터를 통해서 단어들을 받아들인다고 합니다.그다음, 두뇌의 전두엽에 있는 단어 생산 센터라고 하는 브로카(Broca)영역으로 이 단어들을 투사합니다.어떤 사람이 '잘 지내세요?""라고 물으면 이 정보는 베르티케 영역이 받아 해석을 하고, 그다음에는 브로카 영역에서 단어들을 조합해 ""저는 잘 지내요, 고마워요.""라는 대답을 만들어냅니다.​이처럼 인간의 두뇌는 수백만 개에 달하는 뉴런(Neuron)의 도움을 받아서 단어들을 이해하고 행동을 취하게 됩니다. 그렇다면 기계가 인간의 언어를 처리한 다음 올바른 단어들로 이루어진 조합을만드는 것은 어떻게 가능한 걸까? 이를테면 여러분이 시리(Siri)와 같은 음성 어시스턴트에 대고 말을 할 때그게 여러분의 말과 의도를 이해한 다음, 다시 여러분에게 대답한 느 것과 같은 원리입니다.​기계가 인간처럼 말하는 방법을 배우는 기술을 자연어 처리(NLP)라고 부릅니다.이 기술은 기계 사이의 상호작용을 다루는 인공지능(AI)의 한 분야입니다. NLP는 기계가 들은 내용을 처리하고 받은 정보를 구조화하고 필요한 반응을 탐색하여 사용자가 이해할 수있는 언어로 대응할 수 있는 능력을 제공합니다. 자연어 처리의 활용 사례 중에서 음성 어시스턴트만 있는 것이 아닙니다. 생각보다 이 기술을 사용한 흥미로운 애플리케이션들은 아주 많이 있습니다.이 글을 통해 자연어 처리를 활용한 흥미로운 사례 10가지에 대해 알아보겠습니다.  1. 자동 번역(Machine Translation) ​기계에 의한 자동 번역은 전산언어학(Computer Linguistics)의 하위 분야입니다.전산 언어학은 텍스트 또는 말을 한 언어에서 다른 언어로 번역하는 소프트웨어 애플리케이션을 사용합니다.자동 번역의 가장 대표적인 사례로는 구글 번역(Google Translate)을 들 수 있습니다.구글 번역의 경우, 매일 1천억 개의 이상의 단어를 번역한다고 알려져 있습니다.​구글 번역에서는 문장 전체를 한 번에 번역하기 위해서 인공적인 신경 네트워크(딥러닝)를 사용하는 신경망 기계 번역(Neural Machine Translation) 방식을 활용하고 있습니다.이는 예제 기반의 자동 번역 방식을 사용하는데, 더 좋은 결과를 만들어 내기 위해서 수 백 개의 예재를학습합니다. 하지만 인간의 언어가 모호한 부분이 많기 때문에 시스템이 단어와 문장 그리고 의도를 이해하는 걸 어렵게 만들기도 합니다. 이때 자연어 처리를 통해 문제를 해결하는 데 도움을 받을 수 있습니다​​  ​2. 대화형 사용자 인터페이스    (Conversational User Interface) ​대화형 사용자 인터페이스(CUI)는 컴퓨터가 실제 사람과의 대화를 모방하는 컴퓨터를 위한 인터페이스입니다. 그 예로는 챗봇(Chatbot)이 있습니다. 챗봇은 기계와 사람이 텍스트를 통해서대화를 할 수 있는 인터페이스를 갖추고 있습니다. 챗봇의 역사와 발전을 아주 인상적입니다.미리 정해진 Q&A 모음을 제공하는 고객 서비스 대행 역할을 하는 것부터 시작해서 모바일 앱의대안으로 자리를 잡기까지, 챗봇은 지금까지 많은 발전을 이루었습니다.​챗봇은 Q&A 플랫폼의 역할을 하는 텍스트 기반의 CUI입니다.사용자들이 주문을 하고 자신의 주문 상태를 확인하고 정보를 분류하고 항공 티켓을 예약하고금융 거래를 하고 마케팅 활동을 개선하는 등의 역할을 가능하게 해줍니다.챗봇이 복잡한 임무를 수행하기 위해서는 사용자가 입력하는 내용을 이해하고 그것을 해석하고적적하게 대응할 수 있어야 합니다. 바로 이 지점이 자연어 처리가 중요한 역할을 하는 부분입니다.​​  ​3. 텍스트 예측(Text Prediction) ​텍스트 예측은 어떤 구문이나 문장에서 다음에 올 단어를 예측하는 프로세스를 말합니다.이러한 텍스트 예측에서 가장 유명하면서도 일반적인 사례는 바로 구글 검색입니다.​구글에서는 미리 훈련된 모델을 생성하기 위해서 신경 네트워크를 활용하는 자연어 처리(NLP) 알고리즘인버트(BERT)라는 기법을 사용하고 있습니다. 이 모델은 인터넷에서 사용할 수 있는 방대한 양의 주석이달리지 않은 텍스트를 활용해서 훈련을 합니다. 버트의 알고리즘은 검색 엔진이 사람과 비슷한 방식으로검색어를 이해할 수 있도록 도와줍니다. 그 외에도 구글 문서, 메일 작성과 같은 수많은 애플리케이션에서도텍스트 예측이 도움을 주는 자연어 처리 모델을 활용하고 있습니다.​  ​4. 감성 분석(Sentiment Analysis) ​감성 분석은 텍스트 데이터 안에서 감정을 해석하고 분류하는 프로세스입니다.일반적으로 특정한 비즈니스와 관련된 애플리케이션을 사용하는 기업들이 감성 분석을 활용하면온라인 피드백에서 고객이 서비스, 브랜드, 제품에 대해서 보이는 정서를(긍정적, 부정적, 중립적)파악할 수 있습니다. 감성 분석이 탁월한 활약을 보이고 있는 곳으로는 제품 분석, 시장조사, 평판관리,정밀한 타켓팅(Targeting), 시장 분석, 홍보, 순 추천 고객지수(NPS) 등이 있습니다.​  ​5. 텍스트 분류(Text Classification) 제대로 체계를 갖추지 못한 텍스트는 어느 곳에나 있습니다. 이메일, 소셜 미디어, 웹 사이트, 채팅 내용 등 일부 분야에서는 자연어 처리에 의한 텍스트 분류가필수적이며 상당히 엄격하게 사용하고 있습니다.​텍스트 분류 알고리즘은 대규모 텍스트 데이터를 처리하는 소프트웨어 시스템에서 그 기초를 제공,예를 들면 이메일 소프트웨어는 텍스트 분류를 통해서 이메일에 태그를 붙여 특정한 카테고리를 나눌 수 있으며, 이를 통해 받은 편지함 또는 스팸 메일함으로 편지를 보낼 수 있습니다.또한 지메일 소프트트웨어에서는 이메일을 기본(Primary), 소셜(Social), 프로모션(Promotion)으로자동 분류하고 있는데, 이것은 자연어 처리를 활용해서 텍스트를 분류하는 가장 대표적인 사례입니다.​텍스트 분류가 뛰어난 능력을 발휘하는 또 다른 분야는 토론 포럼입니다.이런 분야에서는 어떤 댓글이 부적절하다고 표시가 될 필요가 있는지를 텍스트 분류 알고리즘을통해서 판단합니다. 이와 함께 전자상거래, 뉴스 에이전시, 콘텐츠 큐레이터와 같은 플랫폼에서는텍스트 분류를 활용해서 자동적으로 콘텐츠 / 제품 / 서비스에 태그를 달고 있습니다.​  ​6. 맞춤법 검사(Spell Check) ​맞춤법 검사 프로그램은 어떤 텍스트 안에서 잘못 표기된 철자나 오타가 있는지를 확인하고 수정해 주는소프트웨어 애플리케이션입니다. 이러한 맞춤법 검사 프로그램의 대표적인 사례는 바로 그래멀리(Grammarly) 입니다. 그래멀리는 다양한 텍스트 문서에서 연동할 수 있는 도구이며, 사용자들이 내용을계속해서 작성하는 동안 맞춤법 검사를 자동적으로 수행해서 수정안을 표시해 줍니다.​맞춤법 검사 프로그램은 문서화된 형식을 갖추고 생성하는 소프트웨어 애플리케이션에서 매우 중요한역할을 하지만 사용자들이 맞춤법을 틀리는 경우가 많은 인터넷에서도 상당히 중요한 부분을 차지합니다.그리고 이러한 기능을 통해서 검색 결과에도 긍정적인 영향을 미칠 수 있습니다.​  ​7. 음성 인식(Speech Recognition) ​음성 인식 기술은 약 70년 동안 존재해왔습니다. 최초의 음성인식 시스템은 1952년 벨 연구소(Bell Laboratories)가 선보인 것이었습니다. 오드리(Audrey)라고 알려진 이 시스템은 한자리 숫자를인식할 수 있었습니다. 이후 IBM에서는 영어 단어 16개를 이해하고 반응할 수 있는 슈박스(Shoebox)를선보였는데 이것은 음석 인식을 위해서 자연어 처리가 사용된 첫 사례로 기록되었습니다.​오늘날에는 자연어 처리가 발전하면서 클릭이나 타이핑, 텍스트를 선택하는 것 이외에도 목소리가시스템에 입력하는 하나의 방식으로 받아들여지고 있습니다. 어떻게 하면 기계가 인간의 음성을 인식하고그 의도를 이해하며 적절하게 반응하는 것을 익힐 수 있는지에 대한 완벽한 사례는 바로 코타나(cortana),시리(Siri), 구글 어시스턴트(Google Assistance), 아마존 알렉사(Alexa) 등의 음성 어시스턴트입니다.실제로 자연어 처리는 현재 음성 사용자 인터페이스(VUI)의 이면에 있는 핵심 기술입니다.​음성인식의 일반적인 또 하나의 사례로는 스마트폰에 있는 음성-텍스트 변환 기능이 있습니다.이는 많은 스마트폰에서 기본 기능으로 탑재되면서, 사용자들이 이 기능을 이용할 수 있게 해주는모바일 앱도 많이 나오게 되었습니다. 음성-텍스트 변환 기능을 이용하면 사용자들이 오디오를 통해서입력을 할 수 있고 그다음에는 텍스트로 변환이 됩니다.​  ​8. 문자 인식(Character Recognition) ​광학 문자 인식(OCR)은 손글씨나 타이핑 글씨, 또는 인쇄된 텍스트의 이미지를 기계가 이해할 수 있는코딩 언어로 변환하는 프로세스입니다. 인쇄된 텍스트를 디지털로 전환하기 위해서 흔히 사용되는방법이며, 이를 통해서 텍스트가 전자적으로 저장되고, 편집되고, 검색될 수 있습니다.​광학문자인식(OCR)과 자연어 처리(NLP)는 신분증이나 여권을 자동으로 읽어서 인식하고, 데이터를 다양한양식이나 CRM(고객관계관리) 정보로 입력하고 다양한 출처에서 얻은 고객 정보를 검증하고, 은행 카드 / 각종 지표 / 수표 / 티켓 등을 즉시 스캔하는 등의 문서 관련 업무에서 다양한 혜택을 제공합니다.​  ​9. 언어 번역기(Language Translator) ​다른 언어의 일부 단어나 구를 번역하기 위해 Google 번역기를 사용해 본 적이 있습니까?과거의 기계 번역 시스템은 사전 및 규칙 기반 시스템을 기반으로 하여 사용이 제한되었습니다.하지만 이제 신경망 분야의 발전과 방대한 데이터의 가용성 등 텍스트를 한 언어에서다른 언어로 변환하는 기계 번역이 상당히 정확해졌습니다.Google 번역이 자연어 처리 응용 프로그램의 좋은 예이며 많은 기업과 사람들이 언어 장벽을허물도록 돕고 있습니다.​  ​10. 소셜 미디어 모니터링(Social Media Monitoring) 소셜 미디어는 이제 개인 커뮤니케이션뿐만 아니라 좋아하는 브랜드와 소통하는 방법으로도사용됩니다. 사람들은 소셜 미디어에서 제품 또는 서비스에 대한 경험에 대한 유용한 의견과피드백을 남깁니다. 이러한 댓글에는 유용한 데이터가 포함되어 있으며 브랜드가 유용한 통찰력을생성하는 데 도움이 될 수 있습니다.​기업은 다양한 NLP 기술을 사용하여 소셜 미디어 게시물을 분석하여 고객이 게시물에 대해 어떻게 생각하는지 파악합니다. 또한 회사에서 고객이 직면한 문제를 이해하는 데 사용됩니다.정보도 소셜 미디어를 활용해 댓글과 게시물을 분석해 국가 안보위협을 파악합니다.​​ NLP 기술을 다른 AI 기술과 결합하는 것이NLP와 관련된 문제를 극복하는 열쇠!!!!​ "
# 챗봇 By staying on top of emerging technologies / & user needs & Chatbot paradigm in 2023 ,https://blog.naver.com/acklego/223036967593,20230307,"Reinventing search technology is a complex and challenging task that requires a deep understanding of user behavior, data science, and artificial intelligence. However, here are some ideas that could potentially improve the search experience:​Personalization: Develop search algorithms that are personalized to each user's unique preferences and search history. This could help provide more relevant and accurate results.​Contextual search: Implement contextual search that takes into account the user's location, time of day, and other relevant factors. This could help users find what they're looking for more efficiently.​Natural language processing: Use natural language processing to better understand user queries and return more accurate results. This could involve parsing user queries for meaning and context.​Multimedia search: Expand search capabilities to include images, videos, and other types of multimedia content. This could help users find visual or auditory information more easily.​Voice search: Develop voice search capabilities that allow users to search using natural language commands. This could help users search while on the go or when their hands are busy.​Collaboration: Enable users to collaborate and share search results with others. This could help teams work more efficiently and quickly find relevant information.​Overall, reinventing search will require a combination of innovative thinking, advanced technology, and a deep understanding of user needs and behavior.​Certainly! Here are some ideas to help expand upon a chatbot paradigm in 2023:​​Multilingual support: With the increasing globalization of businesses and societies, chatbots that support multiple languages will become more important. In 2023, chatbots could use advanced natural language processing (NLP) techniques to understand and respond to users in multiple languages.​Emotional intelligence: As chatbots become more human-like, they will need to be able to recognize and respond to human emotions. In 2023, chatbots could use machine learning algorithms to detect emotional cues in text or voice inputs, and respond with appropriate empathy and understanding.​Integration with IoT devices: With the rise of the Internet of Things (IoT), chatbots could be integrated with smart devices such as home assistants, wearable technology, and connected cars. In 2023, chatbots could use this integration to provide users with personalized and contextualized responses based on their environment and behavior.​Advanced personalization: To improve the user experience, chatbots in 2023 could use advanced personalization techniques to tailor their responses to individual users. This could involve using data such as past conversations, browsing behavior, and social media activity to understand user preferences and provide more relevant and personalized responses.​Voice and video chat capabilities: While text-based chatbots are useful in many situations, in 2023 chatbots could be equipped with voice and video chat capabilities to provide a more immersive and human-like experience. This could involve using natural language processing and speech recognition to understand and respond to voice inputs, and using augmented reality technology to provide visual responses.​Security and privacy: As chatbots handle more sensitive data such as financial information and personal details, it will become increasingly important to ensure that they are secure and protect user privacy. In 2023, chatbots could use advanced encryption and access controls to ensure that user data is protected.​These are just a few ideas for expanding upon a chatbot paradigm in 2023. By leveraging emerging technologies and keeping user needs in mind, you can continue to improve upon the value and utility of your chatbot.​​Sure, here are some more ideas to expand upon a chatbot paradigm in 2023:​​Personal digital assistants: Chatbots in 2023 could function as personal digital assistants, helping users with tasks such as scheduling appointments, making reservations, and ordering food. They could use advanced AI and NLP algorithms to understand user requests and preferences, and provide personalized recommendations.​Collaboration and teamwork: As remote work becomes more prevalent, chatbots could be used to facilitate collaboration and teamwork among remote teams. In 2023, chatbots could be equipped with project management and task assignment capabilities, allowing team members to easily communicate and coordinate on projects.​Integration with social media platforms: Chatbots could be integrated with popular social media platforms such as Facebook, Twitter, and Instagram. In 2023, chatbots could use this integration to provide users with personalized recommendations based on their social media activity, and to enable users to communicate with businesses and organizations through social media.​Cognitive search: In 2023, chatbots could be used for cognitive search, allowing users to search for information and answers to questions in a conversational way. Chatbots could use advanced NLP and machine learning algorithms to understand user queries and provide relevant results.​Integration with virtual and augmented reality: As virtual and augmented reality technologies continue to advance, chatbots could be integrated with these technologies to provide more immersive and interactive experiences. In 2023, chatbots could use augmented reality to provide visual instructions or to assist with remote repair and maintenance tasks.​Advanced analytics and reporting: Chatbots in 2023 could provide advanced analytics and reporting capabilities, allowing businesses and organizations to gain insights into user behavior and preferences. Chatbots could use data such as conversation logs and user feedback to provide detailed reports and insights.​These are just a few more ideas to expand upon a chatbot paradigm in 2023. By staying on top of emerging technologies and user needs, chatbots can continue to evolve and provide value to users and businesses alike.​​​​​​​​ "
"[딥러닝] Listen, Attend and Spell (논문 리뷰) ",https://blog.naver.com/andrew9909/222807489479,20220711,"카네기멜론대 William Chan, 그리고 Google Brain 팀이 2015년에 발표한 논문입니다.   0. 들어가며​이전 글에서 Attention 기반 sequence to sequence 모델에 대해 이야기했습니다. 번역 모델에서 'source sentence'를 'target sentence'로, 즉 텍스트를 다른 텍스트로 변환했다면 이 모델은 음성 인식 모델로, 음성을 입력받아 텍스트를 출력합니다.​- ASR(Automatic Speech Recognition)Neural Network를 사용한 음성인식은 다음과 같은 순서로 진행됩니다. 주된 방법은 기존까지 CTC, Seq2seq 방식 두 가지로 나뉘었습니다. seq2seq 방식은 이미 이전 글에서 이야기했으므로, CTC에 대해서 살펴보겠습니다.1) CTC(Connectionist Temporal Classification) CTC는 다층의 Uni/Bi directional LSTM으로 이루어진 Encoder가 존재합니다. LSTM 계층의 Hidden state가 softmax 함수를 거쳐 결과를 출력하게 됩니다. ​CTC와 Seq2seq 방식은 모두 단점이 존재했습니다. CTC는 각각의 요소를 독립적으로 추론한다는 단점이 있었고, Seq2Seq방식은 end-to-end로 학습될 수 없다는 단점이 있었습니다. LAS(Listen, Attend and Spell) 방식은 기존읜 불편함을 해소하고자 했습니다. 본 논문에서는 Attention 기반 seq2seq 방식을 사용함으로써 End-to-End 학습이 이루어질 수 있도록 했습니다. 본 논문에서 Seq2seq의 Encoder와 Decoder는 각각 'Listener', 'Speller'라는 이름을 가지고 있습니다.  1. 기본 모델 구조모델의 기본 구조는 Attention 기반 Seq2seq 번역 모델과 매우 흡사합니다. 필터 뱅크를 통과한 스펙트럼 feature인 입력 시퀀스가 주어지면, <sos> 토큰부터 시작해 가변 길이의 output y를 <eos> 토큰이 출력될 때까지 계속해서 출력합니다. 출력은 알파벳, 숫자, space( ), comma(,), period(.), apostrophe('), unk(unknown) 중 하나로 출력하게 됩니다. 학습은 마찬가지로 조건부확률을 최대화하는 방향으로 학습합니다. Listen 함수는 입력을 high level feature인 h로 변환하고, AttendAndSpell 함수는 h와 기존의 y를 이용해 조건부확률을 구합니다. 모델의 출력 yi는 각 문자가 나올 확률을 나타냅니다. 이 확률은 이전 문자의 확률을 다 고려했을때 현재 어떠한 문자가 나올 확률이 가장 큰지를 표현하며, 이전에 나온 문자들의 확률을 chain rule을 이용해 곱한 형태로 구해집니다. ​모델의 전체 구조는 위 그림과 같습니다. Listener와 Speller로 나누어 하나씩 살펴보겠습니다.  2. Listener Listener는 기본적으로 Bidirectional LSTM으로 구성됩니다. 특이한 점은 이 LSTM이 피라미드 형태(Pyrimidal Bidirectional LSTM, pBLSTM)로 쌓여 있다는 점입니다. 이전 layer의 sequence 두 개를 concatenate해서 사용합니다. 아래 그림처럼 기존 BLSTM 구조에 비해 시퀀스의 길이가 상당히 줄어드는 것을 확인할 수 있습니다. 두 개씩 concatenate하므로 pBLSTM 레이어 1층 당 시퀀스 길이가 반씩 줄어들게 되는데, 본 논문에서는 레이어 3개를 두어 2^3 = 8배로 길이를 줄였습니다. 이러한 시퀀스 길이의 감소는 디코딩 & 어텐션 과정에서 연산량 감소 효과를 불러올 수 있습니다. ​  3. Speller Speller는 attention을 활용한 decoder의 구조와 똑같습니다. hidden state와 이전 출력을 이용해 context vector를 만들고, 이를 이용해 입력 sequence를 Attention하여 출력을 내보내게 됩니다. ​  4. 실험​1. 데이터셋은 3백만개의 Google Voice Search utterances를 이용했습니다. 약 10시간의 발화들이 랜덤으로 선택되어 validation set으로 사용되었습니다. room simulator를 이용하여 증강을 진행하였고, 이 때 기존 녹음된 것과 같은 reverberation과 noise를 추가했습니다.2. 음성 데이터 input은 Mel-Spectrogram을 사용하여 number of mel을 40으로 주고 10ms마다 features들을 추출했습니다. 3. Teacher forcing rate는 0.9로 사용했습니다. Teacher forcing에 대한 이야기는 https://blog.naver.com/sooftware/221790750668 이 분께서 잘 설명해 주셨으니, 확인해보시면 좋을 것 같습니다. [Sooftware 머신러닝] Teacher Forcing (티쳐포싱)Machine Learning: Teacher Forcing (feat. Seq2seq) ""Sooftware"" 이 글은 제가 공부하여 ...blog.naver.com 4. beam search를 사용했습니다. beam search에 대한 이야기는 이전 논문에서 다루어 간략히 소개하겠습니다. Softmax 함수를 거쳐 확률로 나오게 된 출력에 대해서, Greedy하게 확률이 가장 높은 한 선택지만 고르지 않고 여러 개의 후보군을 두는 방법입니다. 예를 들어  beam size가 3이라면, 출력을 고를 때 '누적 확률'이 가장 높은 3개의 후보군을 지속적으로 선택해 나갑니다.  5. 실험 결과 기존의 State-of-the-art와 비교했을 때는 성능이 떨어지는 모습을 보이고 있습니다. Beam Width는 늘릴수록 당연히 성능이 좋아지겠으나, 16 이상에서는 큰 차이가 없는 모습입니다. 발화의 길이가 너무 짧을 때는 단어를 놓치는 경우가 많아 error가 증가했으며, 발화의 길이가 너무 길어지게 되면 generalize하는 데 어려움을 겪어 error가 증가했습니다. 각 출력을 내보낼 때 입력 sequence의 어떤 부분을 attention했는지 나타내는 표입니다. 번역 모델과 다르게 음성 인식 모델은 대각선 형태로 쭉 나아가는 모습이 좋은데요, 대각선 모양으로 잘 나가고 있는 모습을 보여주고 있습니다.  참고자료원논문https://arxiv.org/pdf/1508.01211.pdf​참고자료1https://cosmoquester.github.io/listen-attend-and-spell/ Listen, Attend and Spell 리뷰Listen, Attend and Spell 논문을 리뷰하고 내용을 요약해보았습니다.cosmoquester.github.io 참고자료2https://kaen2891.tistory.com/30?category=453454 Listen, Attend and Spell 리뷰Listen, Attend and Spell : A Neural Network for Large Vocabulary Conversational Speech Recognition 리뷰 이번에 읽은 논문은 Listen, Attend and Spell: A Neural Network for Large Vocabulary Conversat..kaen2891.tistory.com 참고자료3https://blog.naver.com/PostView.nhn?blogId=sooftware&logNo=221816126290&parentCategoryNo=&categoryNo=&viewDate=&isShowPopularPosts=false&from=postView 「Listen, Attend and Spell」 Paper Review「Listen, Attend and Spell」 Paper Review https://arxiv.org/abs/1508.01211 (William Chan et a...blog.naver.com 참고자료4https://blog.naver.com/sooftware/221809101199 [Sooftware 머신러닝] Beam Search (빔서치)Machine Learning: Beam Search (+ Implementation by PyTorch) ""Sooftware"" 이 글은 제...blog.naver.com 참고자료5https://hanseokhyeon.tistory.com/entry/Listen-Attend-and-Spell-%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0 Listen, Attend and Spell 논문 리뷰3. Model LAS 모델은 acoustic features를 입력으로 사용하고, 영문자가 출력으로 나온다. X = (x1, ..., xT)는 필터 뱅크 기반 features sequence고, Y =  ( , y1, ... , yS, )는 {a, b, c, ... z, 0, ... , 9,..hanseokhyeon.tistory.com 참고자료6https://medium.com/@fovjfozk/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%B4%88%EC%A7%9C%EC%9D%98-listen-attend-and-spell-%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0-14e4ddea2708 딥러닝 초짜의 <Listen, Attend and Spell> 논문 리뷰Modelmedium.com ​ "
ML 12 NN의 꽃 RNN 이야기 ,https://blog.naver.com/kovtjw/222744627715,20220524,"​ Sequence Data (연속성이 있는 데이타) 는 NN과 CNN으로 할 수 없다고 이야기 하고 있지만, 사실상 Inductive Bias (외부 데이터에 최적의 조건을 낼 수 있는 모델의 조건)에 의한 정확도의 차이가 있을 뿐 가능하다. ​그러나 RNN은 Inductive Bias가 Sequence한 속성을 가진 데이터에 높은 정확도를 가진 모델이기 때문에 적합하다고 할 수 있다.Sequence한 속성을 가진 데이터에는 음성인식, 자연어처리, 시계열(주식 데이터 등) 등이 포함된다. ​ ​왼쪽의 모델을 풀어서 표현하면 오른쪽과 같다. A에서 나온 화살표가 다시 A로 들어가는데 이를 정해진 횟수만큼 반복한다. 프로그래밍에서는 이를 재진입(re-enterence) 또는 재귀(recursion)라고 부른다. 딥러닝에서는 Recurrent라는 용어를 사용하며, '되풀이'라고 해석한다. ​RNN에서 중요한 것은 상태(State)를 갖는다는 것이고, 여기서는 ht라고 표현하고 있다. 매번 들어오는 입력은 xt라고 표현한다. h는 상태를 가리키며, 동시에 hidden layer를 가리키기 때문에 약자로 h를 사용한다.  RNN은 일반적으로 step를 거칠 때마다 어떤 결과를 예측하게 된다. 그리고 이런 예측 값을 y라고 부른다. 여기서의 연산도 y = wx + b이다. 타임스텝 이전의 RNN 셀에서 나오는 값과 입력 벡터(x)로 계산하면, 새로운 상태 값이 만들어진다. for _ in range(노드 개수):        현재 상태 = W에 대한 함수(이전 상태, 입력 벡터) ​ - 가장 단순한 형태의 RNNt는 Sequence Data에 대한 특정 시점의 데이터를 가리키며, 여기서는 t에 대해 두 가지를 계산한다. 첫 번째 줄의 공식은 w 이전 상태와 입력을 갖고 fw에 해당하는 tanh 함수를 호출하는 것이다. ht는 현재 상태를 의미하고 h의 t-1번째는 이전(old) 상태와 입력 값(x)을 사용해서 계산한다. yt는 예측 값을 의미하고, W와 현재 상태(ht)를 곱해서 계산한다.  연산이 '되풀이'되어 진행되도 w값은 변하지 않고 동일하다.  글자를 다루는 RNN 기반의 언어 모델을 Character - level Language Model이라고 부른다. 일반적으로 language model은 출력 결과로 단어와 같은 글자를 예측하는 모델이라고 얘기한다. 여기서는 4가지 종류의 글자 h,e,l,o가 있고 hello를 예측하려고 한다.  4가지 종류의 글자가 있기 때문에 크기가 4인 벡터로 처리한다. multi-nomial calssification에서 봤던 것처럼 4가지 중 하나를 선택하게 하고, 몇 번째 값이 켜졌냐에 따라 순서대로 h,e,l,o가 된다. 공식에서 보여주는 것처럼 h의 값과 x의 값을 w와 계산한 후 다음 tanh 함수에 전달하면hidden layer에서 보여주는 값이 차례대로 만들어진다. 매번 계산이 끝날 때마다 새로운 상태를 가리키는 hidden layer의 값이 바뀌는 것을 볼 수 있다. tanh 함수는 activation function 중 하나로 처음 나왔던 sigmoid를 개량한 버전이다. 기존 sigmoid가 0~1사이를 반환했다면, tanh는 -1~1사이를 반환한다. ​output layer를 보면, 첫 번째 h가 들어갔을 때 'o'를 반환했기 때문에 예측을 하지 못했다. 정확도가 낮았지만, 실제로 정확하게 예측한다면 input이 hell이 되었을 때 output으로 ello를 출력했을 것이다.  RNN 모델을 통해 할 수 있는 것들은 Language Model, Speech Recognition, Machine Translation, Conversation Modeling, Image/Video Captioning 등이 있다.  1대다(one-to-many) 기반의 모델로 이미지에 대해 설명을 붙일 때 사용한다. 한 장의 그림에 대해 '소년이 사과를 고르고 있다' 처럼 여러 개의 단어 형태로 표현할 수 있다.​ 다대1(many-to-one) 형태의 모델로 여러 개의 입력에 대해 하나의 결과를 만들어준다. 우리가 하는 말을 통해 우리의 심리 상태를 '안정', '불안', '공포' 등의 한 단어로 결과를 예측할 때 사용된다.  다대다(many-to-many) 형태의 모델로 기계 번역에서 사용된다.  여러 개의 단어로 구성된 문장을 입력으로 받아, 여러 개의 단어로 구성된 문장을 반환한다. ​ 다대다(many-to-many) 모델의 또다른 형태이며, 동영상 같은 경우 여러 개의 이미지 프레임에 대해 여러 개의 설명이나 번역 형태로 결과를 반환하게 된다.  RNN도 여러 개의 layer를 두고 복잡한 형태로 구성할 수 있다. ​참고 : https://pythonkim.tistory.com/57 41. NN의 꽃 RNN 이야기 (lec 12)Neural Network의 꽃이라고 불리는 RNN(Recurrent Neural Network)에 대해서 소개하는 동영상이다. RNN은 sequence data를 처리하는 모델이다. sequence는 순서대로 처리해야 하는 것을 뜻하고, 이런 데이터에는..pythonkim.tistory.com ​ "
[딜라이트보청기] 어음청각검사 ,https://blog.naver.com/leeheog1989/222868063227,20220906," 안녕하세요딜라이트보청기 대구점입니다 :)​오늘은 어음청각검사에 대해 알려드리려고 합니다. 어음청각검사란?우리의 의사소통에 주로 사용하는 어음을가지고 민감성 및 인지도 등 다양한 청각능력을 측정하는 평가이며, 순음청력검사로 진행할 수 없는 청자의 일상생활 내 의사소통 능력을 파악하고자 하는 검사입니다. 어음청각검사 결과를 가지고 난청인의 청력손실정도와 보장구 착용 시 효과 정도를 예측 및 평가하는데 주로 사용이 됩니다.​어음청각검사의 종류에는 어떤게 있는지 알려드리겠습니다.​1.어음인지역치(SRT : Speech Recognition Threshold)어음절어를 50%이상 옳게 인지할 수 있는 최소강도레벨을 측정하는 검사입니다.<어음인지역치 측정과정>-자극음 결정 : 자극음으로는 일상생활에서 친숙하게 사용되는 이음절어 중 각 음절의 강도가 동이하고 표준화한 낱말을 사용-보정음 확인 : 목표음을 제시할 경우 단어 제시 전 청력검사기 내 VU(Volume Units)미터의 눈금이 0dB에 위치하는지 확인-친숙화 과정 : 청자가 들은 단어가 어떤 다어인지 몰라 따라할 수 없는 경우를 배제하기 위해서 중요한 과정-SRT걀정 : 단어의 첫 제시 강도는 순음역치평균보다 약 20~25dB 더한 강도에서 시작하여 진행*결과 판독시 SRT와 순음역치평균이 보통 10dB 이내일 경우 순음청력검사의 신뢰성이 좋다고 판단*​2.어음탐지역치(SDT : Speech Detection Threshold)SRT 측정이 불가능한 경우 어음의 유무를 탐지할 수 있는 최소강도레벨을 측정하는 검사입니다.SDT측정의 경우, 주어진 어음을 듣고 따라 말하기 어려운 외국인이나 순음보다 어음을 탐지하는 것이 더 익숙한 유-소아 등에게 임상적 활용성이 뛰어납니다.​3.단어인지도(WRS:Word Recognition Score)청자가 듣기 편안한 강도에서 단어를 듣고 인지 능력을 확인하는 검사입니다.<단어인지도 검사과정>-자극음 결정 : 표준화된 일상생활 단음절 단어를 사용하며, 검사시 단어의 친숙성, 음소 간의 비유사성, 표준어의 대표성 등을 고려하여 진행- 보정음 확인 : SRT와 동일한 방법으로 단어를 제시하기 전 청력검사기 내 VU미터의 눈금이 0dB에 위치하는 것을 확인-단어제시방법 : SRS측정 시 지속적인 말을 하면서 청자가 어음을 편안하게 듣는 정를도를 확인하고 그 강도에서 목표문장을 제시하여 진행​오늘은 어음청각검사에 대해서 알아보았습니다. ​ "
음성인식 ,https://blog.naver.com/dfgpb-/222853237673,20220820,1.(이슈선정 밎 문제점 파악) 탐구동기 또는 작품제작 동기음성인식(speech  Recognition) 이란 사람이말하는 음성 언어를 컴퓨터가 해석해 그내용을 문자 데이터로 전환하는 처리를 말한다 ​2.(해결방안 모색) 이슈 원인 분숙 이론적 배경 연구조사인간의 눈은 사물의 색을 볼수 있게 되어있다  사물은 다양한 색 (파장)을 흡수하고 일정색을 반사하는데 우리는가 반사된  색을 우리눈으로 복수 있다  빚의 색깔은 빨(R)파랑(B)초록(G)의 혼합으로 다양한색깔을 표현할수 있다실제로 TV나  모니터 스마트폰이 디스플레이 로 표현하는색들도 모든 RGB를 이용하여  표현 할수 있다  이때 RGB를  빚의 3운격이라고 이야기한다  ​3.(알고리즘/코딩&제작) 하드웨어 산출물의 제작방법 및 제작과정음성인식 소포트웨어음성인식을 하면 아두이노 장치에  명령을 보내는어플지캐이션(소프트 웨어)무선통신(블루투스)를  할  Hc-O6블루투스 모듈 전등역할을 하는 RGB LED  와 전등 명령을 입력받을 때소리를 내는 부저와장치의상태를 표시하는 LED  ​4. 동작설명 앱인벤터로 만든 내앱으로 블루투스 연결하고  음성인식 버튼을 누르고  켜기라고 말하면  어플에서 A라는 문자를 블루투스 통신으로  아두이노에게 보낸다 아두이노에세 확인을 하는데 A라고 확인이 되면  켜진다 끄기는 끄기라고 말하면 어플에서B라는 문자를  블루투스 통신으로 아두이노에게 보내서 꺼진다​​5. 시연영상  ​ 
버츄얼휴먼을 만드는방법에 당신이 필수로 알아야할 내용  ,https://blog.naver.com/ktrtgirl/223057706515,20230328,"목차필수로 알아야할 내용 자료가상인물을 만들기 위한 #소프트웨어#버츄얼휴먼 의 외모와 스타일링목소리 신뢰성을 높이는 방법#인공지능 기술을 활용한 언어처리인공지능 기술을 활용한 자연스러운 움직임인공지능 기반 대화 시스템 개발가상공간에서의 인터랙션 구현 방법가상환경에서의 감정 표현 기술버츄얼휴먼의 보안과 개인정보 보호#가상현실 기술을 활용한 버츄얼휴먼 제작과 활용버츄얼휴먼을 만드는 방법에 당신이 필수로 알아야할 내용  버츄얼휴먼을 만들기 위해서는 다양한 기술과 지식이 필요합니다.  버츄얼휴먼을 만들기 위해 필요한 내용들을 자세하게 알아보겠습니다.가상인물을 만들기 위한 소프트웨어버츄얼휴먼을 만들기 위해서는 먼저 가상인물을 만들기 위한 소프트웨어가 필요합니다. 대표적으로 사용되는 소프트웨어로는 3D Max, Maya, Blender 등이 있습니다. 이러한 소프트웨어를 이용하여 가상인물의 외모와 스타일링, 움직임 등을 디자인할 수 있습니다.2.버츄얼휴먼의 외모와 스타일링버츄얼휴먼의 외모와 스타일링은 가상인물을 만드는 과정에서 가장 중요한 부분 중 하나입니다. 버츄얼휴먼의 외모와 스타일링은 해당 가상인물을 만드는 목적과 타겟 대상에 따라 다르게 설정됩니다. 예를 들어, 어린 아이들을 위한 가상인물을 만들 때는 귀여운 외모와 스타일링이 필요합니다.3.목소리 신뢰성을 높이는 방법버츄얼휴먼은 인공지능 기술을 이용하여 대화를 할 수 있습니다. 이때, 목소리 신뢰성은 매우 중요합니다. 목소리가 자연스럽고 신뢰성 있는 대화를 제공할 수 있도록 TTS(Text To Speech) 기술을 이용하여 목소리를 합성하는 방법이 있습니다.4.인공지능 기술을 활용한 언어처리버츄얼휴먼은 인공지능 기술을 이용하여 자연어 처리를 합니다. 이때, 자연어 처리 기술을 이용하여 사용자의 발화를 분석하고, 의도를 파악하여 적절한 대답을 제공합니다.5.인공지능 기술을 활용한 자연스러운 움직임버츄얼휴먼의 자연스러운 움직임은 가상인물을 더욱 생동감 있게 만드는 중요한 요소 중 하나입니다. 이를 위해 인공지능 기술을 이용하여 모션 캡처를 하거나, 모션 학습을 통해 자연스러운 움직임을 구현할 수 있습니다.6.인공지능 기반 대화 시스템 개발버츄얼휴먼은 대화 시스템을 기반으로 합니다. 이때, 인공지능 기술을 이용하여 사용자와 대화를 할 수 있는 시스템을 구현할 수 있습니다. 예를 들어, 딥러닝을 이용한 챗봇 기술을 활용하여 대화 시스템을 구현할 수 있습니다.7.가상공간에서의 인터랙션 구현 방법버츄얼휴먼은 가상공간에서 활동합니다. 이때, 사용자와의 상호작용을 위한 인터랙션 구현 방법이 필요합니다. 예를 들어, 제스처나 음성 인식 기술 등을 이용하여 사용자와 상호작용할 수 있도록 구현할 수 있습니다.8.가상환경에서의 감정 표현 기술버츄얼휴먼은 감정 표현 기술을 이용하여 사용자에게 다양한 감정을 전달할 수 있습니다. 예를 들어, 얼굴 표정 인식 기술을 이용하여 가상인물의 감정을 표현할 수 있습니다.9.버츄얼휴먼의 보안과 개인정보 보호버츄얼휴먼을 만들 때에는 보안과 개인정보 보호에 대한 고려가 필요합니다. 버츄얼휴먼을 이용하는 사용자들의 개인정보를 안전하게 보호하고, 해킹 등으로부터 보호할 수 있는 시스템을 구현해야 합니다.10.가상현실 기술을 활용한 버츄얼휴먼 제작과 활용가상현실 기술을 이용하여 버츄얼휴먼을 만들고 활용할 수 있습니다. 예를 들어, VR 기술을 이용하여 가상현실 공간에서 버츄얼휴먼과 상호작용할 수 있습니다.이러한 내용들을 고려하여 버츄얼휴먼을 만들 수 있습니다. 버츄얼휴먼은 인공지능 기술과 가상현실 기술 등의 다양한 기술들을 활용하여 더욱 발전할 것으로 예상됩니다.​가상인물을 만들기 위한 소프트웨어 버추얼 휴먼을 만들기 위해서는 가상인물을 만들기 위한 소프트웨어가 필요합니다. 대표적으로 사용되는 소프트웨어로는 3D Max, Maya, Blender 등이 있습니다. 이러한 소프트웨어를 이용하여 가상인물의 외모와 스타일링, 움직임 등을 디자인할 수 있습니다.가상인물을 만드는 데에 소프트웨어는 매우 중요한 역할을 합니다. 3D Max, Maya, Blender는 대표적인 3D 모델링 및 애니메이션 소프트웨어입니다. 이러한 소프트웨어를 이용하면 가상인물의 외모와 스타일링, 움직임 등을 디자인할 수 있습니다. 또한, 이러한 소프트웨어를 이용하여 가상공간을 디자인하고, 가상인물과 상호작용할 수 있는 인터랙션을 구현할 수 있습니다.3D Max는 매우 강력한 3D 모델링 및 애니메이션 소프트웨어입니다. 이 소프트웨어를 이용하면 다양한 가상인물을 만들 수 있습니다. 또한, 3D Max는 다양한 파일 형식을 지원하므로 다른 소프트웨어와의 호환성이 높습니다.Maya는 3D 모델링 및 애니메이션 소프트웨어입니다. 이 소프트웨어를 이용하면 가상인물의 모델링, 텍스처링, 애니메이션 등을 디자인할 수 있습니다. 또한, Maya는 다양한 파일 형식을 지원하므로 다른 소프트웨어와의 호환성이 높습니다.Blender는 무료로 제공되는 3D 모델링 및 애니메이션 소프트웨어입니다. Blender는 다양한 기능을 제공하며, 사용자들이 다양한 가상인물을 만들 수 있도록 도와줍니다.이러한 소프트웨어를 이용하여 다양한 가상인물을 만들 수 있습니다. 또한, 이러한 소프트웨어를 이용하여 가상공간을 구성하고, 가상인물과 상호작용할 수 있는 인터랙션을 구현할 수 있습니다. 이러한 기술을 이용하면 가상현실 환경에서 더욱 생동감있는 가상인물을 만들 수 있습니다.​버츄얼휴먼의 외모와 스타일링 버츄얼휴먼을 만드는 과정에서 가장 중요한 부분 중 하나는 버츄얼휴먼의 외모와 스타일링입니다. 버츄얼휴먼의 외모와 스타일링은 해당 가상인물을 만드는 목적과 타겟 대상에 따라 다르게 설정됩니다.버츄얼휴먼을 사용하는 대상이 어린 아이들일 경우, 귀여운 외모와 스타일링이 필요합니다. 이를 위해, 버츄얼휴먼의 외모와 스타일링을 디자인할 때는 타겟 대상의 성별, 연령, 문화적 배경 등을 고려해야 합니다.버츄얼휴먼의 외모와 스타일링을 디자인할 때는 다음과 같은 사항을 고려해야 합니다.타겟 대상의 연령, 성별, 문화적 배경 등버츄얼휴먼이 하는 일에 따라 필요한 스타일링버츄얼휴먼의 개성과 특징을 고려한 디자인다른 가상인물과 구분되는 디자인버츄얼휴먼의 외모와 스타일링을 디자인할 때는 색상, 패턴, 의상, 머리 스타일, 액세서리 등을 활용할 수 있습니다. 이를 통해 버츄얼휴먼의 개성을 강조하고, 타겟 대상에게 호감을 줄 수 있습니다.또한, 버츄얼휴먼의 외모와 스타일링은 해당 가상인물이 하는 일에 따라 다르게 설정됩니다.  들어, 의료 분야에서 사용되는 버츄얼휴먼은 전문성과 신뢰성을 강조하는 스타일링이 필요합니다. 따라서, 의료 분야에서 사용되는 버츄얼휴먼은 전문적인 의상과 함께 신뢰성 있는 표정과 자세를 강조하는 디자인이 필요합니다.마지막으로, 버츄얼휴먼의 외모와 스타일링을 디자인할 때는 다른 가상인물과 구분되는 디자인을 고려해야 합니다. 버츄얼휴먼이 다른 가상인물과 구분되는 디자인을 갖고 있다면, 사용자들은 해당 가상인물을 더욱 기억하기 쉽고, 호감을 가질 가능성이 높아집니다.이러한 방법으로 버츄얼휴먼의 외모와 스타일링을 디자인하면, 해당 가상인물을 사용하는 사용자들의 호감도를 높일 수 있습니다.​목소리 신뢰성을 높이는 방법 버추얼 휴먼의 목소리는 사용자와의 상호작용에서 매우 중요합니다. 목소리는 해당 가상인물의 개성과 특징을 나타내는 요소 중 하나입니다. 또한, 목소리는 사용자들이 해당 가상인물을 더욱 생동감 있게 인식할 수 있도록 도와줍니다. 따라서, 버추얼 휴먼을 만들 때에는 목소리에 대한 고려가 필요합니다.목소리 배우의 선택버추얼 휴먼의 목소리는 배우의 목소리를 녹음하여 사용할 수 있습니다. 이 경우, 목소리 배우의 선택은 매우 중요합니다. 목소리 배우는 해당 가상인물의 개성과 특징을 나타낼 수 있는 목소리를 가져야 합니다. 또한, 목소리 배우는 해당 가상인물이 하는 일에 따라 적합한 목소리를 가져야 합니다.2.목소리 합성 기술의 활용목소리 합성 기술을 이용하여 버추얼 휴먼의 목소리를 만들 수도 있습니다. 이 경우, 목소리 합성 기술은 해당 가상인물의 개성과 특징을 나타낼 수 있는 목소리를 생성하는 데에 활용됩니다. 목소리 합성 기술을 이용하면, 목소리 배우의 선택에 제한을 받지 않고, 더욱 다양한 목소리를 만들 수 있습니다.3.목소리 디자인의 고려버추얼 휴먼의 목소리를 디자인할 때에는 다음과 같은 사항을 고려해야 합니다.목소리의 높낮이, 음색, 억양 등해당 가상인물의 개성과 특징을 반영한 목소리 디자인해당 가상인물이 하는 일에 따라 적합한 목소리 디자인목소리 디자인은 해당 가상인물의 개성과 특징을 나타내는 요소 중 하나입니다. 따라서, 목소리 디자인은 해당 가상인물의 개성과 특징을 반영해야 합니다. 또한, 해당 가상인물이 하는 일에 따라 적합한 목소리 디자인이 필요합니다.4.목소리 학습 기술의 활용목소리 학습 기술을 이용하여, 사용자가 해당 가상인물과 상호작용하는 과정에서 목소리를 점차적으로 학습할 수 있도록 도와줄 수 있습니다. 이 경우, 사용자가 버추얼 휴먼과 상호작용하면서, 해당 가상인물의 목소리를 학습하는 알고리즘이 적용됩니다. 이를 통해, 사용자는 해당 가상인물의 목소리를 점차 익숙해지게 인식할 수 있습니다.이러한 방법들을 고려하여, 목소리의 신뢰성을 높일 수 있습니다.​인공지능 기술을 활용한 언어처리 인공지능 기술은 최근 많은 분야에서 활용되고 있습니다. 그 중에서도 언어처리 분야에서는 자연어 처리, 기계번역, 음성인식 등 다양한 기술이 발전하고 있습니다. 이러한 기술들은 인공지능 기술의 발전과 함께 더욱 정확하고 효과적인 결과를 도출하고 있습니다.자연어 처리는 인간이 사용하는 언어를 컴퓨터가 이해하고 처리할 수 있는 형태로 변환하는 기술입니다. 이를 위해서는 텍스트 분석, 품사 태깅, 개체명 인식 등 다양한 기술이 사용됩니다. 이러한 자연어 처리 기술은 검색 엔진, 챗봇, 문서 분류 등 다양한 분야에서 활용되고 있습니다.기계번역은 한 언어를 다른 언어로 자동으로 번역하는 기술입니다. 이를 위해서는 기계번역 엔진이 필요하며, 이 엔진은 통계 기반 방법, 인공신경망 기반 방법 등 다양한 방법으로 구현될 수 있습니다. 이러한 기계번역 기술은 다국어 지원, 비즈니스, 교육 등 다양한 분야에서 활용되고 있습니다.음성인식은 음성 신호를 텍스트로 변환하는 기술입니다. 이를 위해서는 음성신호의 특징을 추출하고, 이를 바탕으로 음성을 인식하는 기술이 필요합니다. 이러한 음성인식 기술은 인공지능 스피커, 음성 검색 등 다양한 분야에서 활용되고 있습니다.이처럼 인공지능 기술을 활용한 언어처리 기술은 다양한 분야에서 활용되고 있으며, 더욱 정확하고 효과적인 결과를 도출하고 있습니다. 앞으로도 이러한 기술들은 계속해서 발전하며, 새로운 분야에서 사용될 것으로 예상됩니다.​인공지능 기술을 활용한 자연스러운 움직임 인공지능 기술은 가상현실(VR)이나 게임 등에서 캐릭터의 움직임을 자연스럽게 만드는 데에도 사용됩니다. 이를 위해서는 캐릭터의 움직임을 자연스럽고 부드럽게 만드는 기술이 필요합니다.모션 캡처(Motion Capture)모션 캡처는 캐릭터의 움직임을 실제 인물의 움직임을 촬영하여 만드는 기술입니다. 이를 위해서는 다양한 센서와 카메라를 사용하여 캐릭터의 움직임을 촬영하고, 이를 데이터로 저장합니다. 이후, 이 데이터를 기반으로 캐릭터의 움직임을 만들어냅니다. 이를 통해, 캐릭터의 움직임이 자연스러워지고, 인물의 실제 움직임과 유사한 움직임을 만들어낼 수 있습니다.2.인공신경망(Artificial Neural Network)인공신경망은 인간의 뇌를 모델로 한 기술입니다. 이를 이용하여 캐릭터의 움직임을 자연스럽게 만들어냅니다. 이를 위해서는 인공신경망에 캐릭터의 움직임 데이터를 입력하고, 이를 학습시킵니다. 학습된 인공신경망은 입력된 데이터를 기반으로 새로운 움직임을 생성합니다. 이를 통해, 캐릭터의 움직임이 자연스러워지고, 다양한 움직임을 만들어낼 수 있습니다.3.딥러닝(Deep Learning)딥러닝은 인공신경망을 이용한 기술 중 하나입니다.  이용하여 캐릭터의 움직임을 자연스럽게 만들어냅니다. 이를 위해서는 딥러닝 알고리즘에 캐릭터의 움직임 데이터를 입력하고, 이를 학습시킵니다. 학습된 딥러닝 알고리즘은 입력된 데이터를 기반으로 새로운 움직임을 생성합니다. 이를 통해, 캐릭터의 움직임이 자연스러워지고, 다양한 움직임을 만들어낼 수 있습니다.4.모션 편집(Motion Editing)모션 편집은 모션 캡처나 인공신경망, 딥러닝 등을 이용하여 만들어진 캐릭터의 움직임을 편집하는 기술입니다. 이를 통해, 캐릭터의 움직임을 더욱 자연스럽게 만들어낼 수 있습니다. 또한, 이를 이용하여 다양한 움직임을 만들어낼 수도 있습니다.이처럼 인공지능 기술을 활용하여 캐릭터의 움직임을 자연스럽게 만들어냄으로써, 사용자들은 더욱 생동감 있게 게임을 즐길 수 있습니다. 앞으로도 이러한 기술들은 계속해서 발전하며, 새로운 분야에서 사용될 것으로 예상됩니다.​인공지능 기반 대화 시스템 개발 인공지능 기술의 발전으로 대화 시스템은 더욱 정교해지고 있습니다. 이러한 대화 시스템은 사용자의 발화를 이해하고, 적절한 대답을 제공하는 기술입니다. 이를 위해서는 #자연어처리, 문장 분석 등 다양한 기술이 사용됩니다.자연어 처리는 인간이 사용하는 언어를 컴퓨터가 이해하고 처리할 수 있는 형태로 변환하는 기술입니다. 이를 위해서는 텍스트 분석, 품사 태깅, 개체명 인식 등 다양한 기술이 사용됩니다. 이러한 자연어 처리 기술은 검색 엔진, 챗봇, 문서 분류 등 다양한 분야에서 활용되고 있습니다.문장 분석은 자연어 처리의 한 분야로, 문장을 의미 있는 단위로 분리하는 기술입니다. 이를 위해서는 구문 분석, 의미 분석 등 다양한 기술이 사용됩니다. 이러한 문장 분석 기술은 대화 시스템에서 사용자의 발화를 이해하는 데에 활용됩니다.대화 시스템은 다양한 분야에서 사용됩니다. 예를 들어, 쇼핑몰에서는 고객이 원하는 상품을 찾아주는 챗봇을 제공하고, 금융 기관에서는 고객의 문의에 답하는 챗봇을 제공합니다. 또한, 인공지능 스피커와 같은 기기에서도 대화 시스템이 사용됩니다.이처럼 인공지능 기술을 활용한 대화 시스템은 더욱 발전할 것으로 예상됩니다. 앞으로도 이러한 기술들은 계속해서 발전하며, 다양한 분야에서 활용될 것으로 예상됩니다.​가상공간에서의 #인터랙션 구현 방법 가상공간에서의 인터랙션 구현 방법가상공간에서의 인터랙션은 사용자와 가상환경 간의 상호작용을 의미합니다. 이를 구현하기 위해서는 다양한 기술이 사용됩니다.#모션캡처 (Motion Capture)모션 캡처는 캐릭터의 움직임을 실제 인물의 움직임을 촬영하여 만드는 기술입니다. 이를 이용하여 사용자의 움직임을 캐릭터에 반영할 수 있습니다. 이는 다양한 게임에서 사용되며, 사용자가 직접 캐릭터를 조종하는 것처럼 자연스러운 움직임을 만들어냅니다.#제스처인식 (Gesture Recognition)제스처 인식은 사용자의 동작을 인식하여 가상환경 상에서 캐릭터나 오브젝트를 조작하는 기술입니다. 이를 위해서는 다양한 센서를 이용하여 사용자의 동작을 감지하고, 이를 가상환경 상에서 처리합니다. 이러한 제스처 인식 기술은 가상현실(VR)이나 게임 등에서 사용되며, 사용자와 가상환경 간의 상호작용을 더욱 자연스럽게 만들어냅니다.#음성인식 (Speech Recognition)음성인식은 사용자의 음성을 인식하여 가상환경 상에서 캐릭터나 오브젝트를 조작하는 기술입니다. 이를 위해서는 음성신호를 텍스트로 변환하는 기술이 필요합니다. 이러한 음성인식 기술은 가상현실(VR)이나 게임 등에서 사용되며, 사용자와 가상환경 간의 상호작용을 더욱 편리하게 만들어냅니다.#터치인터페이스 (Touch Interface)터치 인터페이스는 터치 스크린을 이용하여 사용자와 가상환경 간의 상호작용을 구현하는 기술입니다. 이를 위해서는 다양한 터치 인터페이스 기술이 사용됩니다. 이러한 터치 인터페이스 기술은 스마트폰이나 태블릿 PC 등에서 사용되며, 사용자와 가상환경 간의 상호작용을 더욱 직관적으로 만들어냅니다.#머신러닝 (Machine Learning)머신러닝은 인공지능 분야 중의 하나로, 데이터를 기반으로 자동으로 학습하는 기술입니다. 이를 이용하여 사용자의 행동 패턴을 파악하고, 이를 가상환경 상에서 반영할 수 있습니다. 이러한 머신러닝 기술은 가상현실(VR)이나 게임 등에서 사용되며, 사용자와 가상환경 간의 상호작용을 더욱 자연스럽게 만들어냅니다.결론가상공간에서의 인터랙션은 사용자와 가상환경 간의 상호작용을 의미합니다. 이를 구현하기 위해서는 다양한 기술이 사용됩니다. 이러한 기술들은 계속해서 발전하며, 더욱 편리하고 자연스러운 인터랙션을 구현할 수 있게 될 것으로 예상됩니다.​#가상환경 에서의 감정 표현 기술 가상환경에서의 감정 표현 기술은 사용자의 감정을 가상환경 상에서 표현하는 기술입니다. 이러한 기술은 가상현실(VR)이나 게임 등에서 사용되며, 사용자가 더욱 현실감 있게 가상환경을 경험할 수 있게 합니다.​감정 인식 기술감정 인식 기술은 사용자의 얼굴 표정이나 음성, 제스처 등을 인식하여 감정을 파악하는 기술입니다. 이를 위해서는 다양한 센서나 카메라 등이 사용됩니다. 이러한 감정 인식 기술은 대화 시스템이나 가상현실(VR) 등에서 사용되며, 사용자의 감정을 자연스럽게 파악할 수 있게 합니다.​감정 반영 기술감정 반영 기술은 사용자의 감정을 가상환경 상에서 반영하는 기술입니다. 이를 위해서는 다양한 기술이 사용됩니다. 예를 들어, 사용자의 감정을 파악하여 캐릭터의 표정이나 몸짓 등을 변경하거나, 배경음악이나 조명 등을 조절하는 등의 방법이 있습니다. 이러한 감정 반영 기술은 게임이나 가상현실(VR) 등에서 사용되며, 사용자가 더욱 현실감 있게 가상환경을 경험할 수 있게 합니다.​감정 전달 기술감정 전달 기술은 가상환경 상에서의 캐릭터나 오브젝트가 사용자에게 감정을 전달하는 기술입니다. 이를 위해서는 다양한 기술이 사용됩니다. 예를 들어, 캐릭터의 표정이나 몸짓, 음성 등을 이용하여 사용자에게 감정을 전달하는 방법이 있습니다. 이러한 감정 전달 기술은 게임이나 가상현실(VR) 등에서 사용되며, 사용자가 더욱 감정적으로 가상환경을 경험할 수 있게 합니다.​감정 인식, 반영, 전달 기술의 연동감정 인식, 반영, 전달 기술은 서로 연동하여 사용됩니다. 예를 들어, 사용자의 감정을 파악하여 캐릭터의 표정이나 몸짓을 변경하고, 캐릭터가 사용자에게 감정을 전달하는 기술 등이 있습니다. 이러한 기술들은 사용자가 더욱 현실감 있게 가상환경을 경험할 수 있게 합니다.​결론가상환경에서의 감정 표현 기술은 사용자의 감정을 가상환경 상에서 표현하는 기술입니다. 이러한 기술은 감정 인식, 반영, 전달 기술 등을 포함하고 있으며, 서로 연동하여 사용됩니다. 이러한 기술들은 게임이나 가상현실(VR) 등에서 사용되며, 사용자가 더욱 현실감 있게 가상환경을 경험할 수 있게 합니다.​버츄얼휴먼의 보안과 개인정보 보호 가상인물, 즉 버츄얼휴먼은 실제 사용자 정보를 기반으로 만들어지기 때문에, 이를 보호하는 것이 가장 중요합니다. 버츄얼휴먼에 대한 보안과 개인정보 보호는 다음과 같은 방법으로 이루어집니다.​데이터 보호(Data Security)버츄얼휴먼을 만들기 위해서는 사용자 정보를 수집하고 저장해야 합니다. 이러한 사용자 정보는 보안과 개인정보 보호가 필요합니다. 따라서, 데이터 보호를 위한 다양한 보안 기술이 사용됩니다. 예를 들어, 데이터 암호화, 접근 제어 등이 있습니다.​센서 보호(Sensor Security)버츄얼휴먼을 만들기 위해서는 다양한 센서를 사용해야 합니다. 이러한 센서는 버츄얼휴먼과 사용자 간의 상호작용을 위해 사용됩니다. 따라서, 센서 보호를 위해서는 다양한 방법이 사용됩니다. 예를 들어, 센서 데이터의 암호화, 센서 접근 제어 등이 있습니다.​사용자 인증(User Authentication)버츄얼휴먼을 이용하기 위해서는 사용자가 인증되어야 합니다. 이를 위해서는 다양한 사용자 인증 기술이 사용됩니다. 예를 들어, 비밀번호, 지문인식, 얼굴인식 등이 있습니다. 이러한 사용자 인증 기술을 이용하여, 버츄얼휴먼을 이용하는 사용자만이 접근할 수 있도록 보안성을 강화합니다.​권한 관리(Authorization Management)버츄얼휴먼을 이용하기 위해서는 사용자에게 적절한 권한을 부여해야 합니다. 이를 위해서는 다양한 권한 관리 기술이 사용됩니다. 예를 들어, 사용자 그룹별 권한 설정, 접근 제어 등이 있습니다. 이러한 권한 관리 기술을 이용하여, 사용자에게 적절한 권한을 부여함으로써 보안성을 강화합니다.​개인정보 보호(Privacy Protection)버츄얼휴먼을 만들기 위해서는 사용자 정보를 수집하고 저장해야 합니다. 따라서, 개인정보 보호가 필요합니다. 이를 위해서는 다양한 개인정보 보호 기술이 사용됩니다. 예를 들어, 개인정보 암호화, 개인정보 접근 제어 등이 있습니다. 이러한 개인정보 보호 기술을 이용하여, 사용자의 개인정보를 안전하게 보호합니다.​결론버츄얼휴먼에 대한 보안과 개인정보 보호는 매우 중요합니다. 이를 위해서는 다양한 보안 기술이 사용됩니다. 데이터 보호, 센서 보호, 사용자 인증, 권한 관리, 개인정보 보호 등 다양한 기술을 적절하게 사용함으로써, 버츄얼휴먼의 보안과 개인정보 보호를 강화할 수 있습니다.​가상현실 기술을 활용한 버츄얼휴먼 제작과 활용가상현실(Virtual Reality, VR) 기술은 최근 빠르게 발전하고 있습니다. 이러한 기술을 이용하여, 가상인물인 버츄얼휴먼을 만들고 활용하는 분야도 점차 확대되고 있습니다. 이번에는 가상현실 기술을 활용하여 버츄얼휴먼을 제작하고 활용하는 방법에 대해서 살펴보도록 하겠습니다.​버츄얼휴먼 제작 과정버츄얼휴먼을 제작하는 과정에는 크게 두 가지가 있습니다. 첫 번째는 사용자 정보를 수집하고 분석하는 과정이고, 두 번째는 수집된 정보를 기반으로 가상환경 상에서 버츄얼휴먼을 만들어내는 과정입니다.사용자 정보 수집 및 분석버츄얼휴먼을 만들기 위해서는 사용자 정보를 수집하고 분석해야 합니다. 이를 위해서는 다양한 센서나 카메라 등을 이용하여 사용자의 얼굴 표정, 음성, 제스처 등을 수집합니다. 이러한 정보를 분석하여 사용자의 감정, 성격, 특징 등을 파악합니다. 이 과정에서는 감정 인식 기술이 활용됩니다.가상환경 상에서 버츄얼휴먼 제작수집된 사용자 정보를 기반으로, 가상환경 상에서 버츄얼휴먼을 만들어냅니다. 이 과정에서는 감정 반영 기술이 활용됩니다. 예를 들어, 사용자의 얼굴 표정을 파악하여 버츄얼휴먼의 표정을 변경하거나, 배경음악이나 조명 등을 조절하는 등의 방법이 있습니다.​버츄얼휴먼 활용 방법버츄얼휴먼을 만들고 나면, 이를 다양한 분야에서 활용할 수 있습니다. 가장 대표적인 예로는 게임이나 엔터테인먼트 분야가 있습니다. 이 외에도, 교육, 의료, 상담 등에서도 활용할 수 있습니다.게임 및 엔터테인먼트 분야게임이나 엔터테인먼트 분야에서는, 버츄얼휴먼을 이용하여 더욱 현실감 있게 게임이나 영화를 제작할 수 있습니다. 또한, 사용자의 감정을 파악하여 캐릭터의 행동이나 대사 등을 변경하여 게임이나 영화를 더욱 흥미롭게 만들 수 있습니다.교육 분야교육 분야에서는, 버츄얼휴먼을 이용하여 다양한 교육 콘텐츠를 제작할 수 있습니다. 예를 들어, 언어, 문화, 역사 등을 학습할 수 있는 가상학습 환경을 만들어낼 수 있습니다. 또한, 버츄얼휴먼이 학습자와 상호작용하면서 학습 효과를 높일 수 있습니다.의료 분야의료 분야에서는, 버츄얼휴먼을 이용하여 다양한 의료 콘텐츠를 제작할 수 있습니다. 예를 들어, 환자의 증상을 시뮬레이션하여 진단과 치료 방법을 학습할 수 있는 가상환경을 만들어낼 수 있습니다. 또한, 버츄얼휴먼이 환자와 상호작용하면서 의료 서비스의 질을 높일 수 있습니다.​ "
AI 시대 전환을 위해 준비해야 할 것 | 디지털 전환 DT | 인공지능 | 인공지능 기업 사례 ,https://blog.naver.com/inflearn/222627502494,20220121,"이런 분들이라면, 잘 찾아오셨어요!👏🔎 AI가 실제로 어떻게 현실 세계를 바꿔나가고 있는지 실제 사례를 알고 싶은 분🔎 수식이나 코드 없이 AI 기술의 원리와 응용방법을 파악하고 싶은 분🔎 AI를 이용해 사업 및 해당 분야로 취업을 원하는 분​  돈을 버는 방법이 달라지는 4차 산업혁명?출처 : 한겨레출처 : 인프런 강의 <AI로 돈 버는 법 - 사례로 알아보는 AI Transformation>​2019년, 세계 최고 부자 중 한 명인 손정의 소프트뱅크 회장은 문재인 대통령과의 만남에서 ‘첫째도 AI, 둘째도 AI, 셋째도 AI’라며 경제 위기 극복을 위한 AI의 중요성을 강조해 화제가 되었습니다. 최근에는 AI와 연관된 '디지털 전환', '4차 산업혁명', '디지털 혁신'과 같은 용어가 너무도 익숙해졌죠. 이 밖에도 팬데믹과 맞물리며 새롭게 생겨나는 일자리와 사라지는 일자리, 그 중심에 AI가 있다는 맥킨지&컴퍼니의 보고서도 발표됐습니다.​​ 직업 전환 위기에 놓인 근로자가 늘어날 것이라는 맥킨지&컴퍼니의 전망 | 출처 : 중앙일보2030년까지 미국에서 사라지는 일자리와 늘어나는 일자리 그래프 | 출처 : 중앙일보​글로벌 컨설팅 회사인 맥킨지&컴퍼니는 지난해 미국∙독일∙영국∙프랑스 등 6개 선진국과 중국∙인도 2개 개발 도상국의 800개 직업, 2000개 직무를 분석한 ‘코로나19 이후 일자리의 미래’ 보고서를 내놓았습니다. ​경제규모 8개국에서는 2030년까지 총 1억 700만 명의 근로자가 직업 전환 상황에 놓일 것으로 내다봤습니다. 사라지는 일자리는 주로 도소매∙숙박 등 고객 서비스업, 요식업이었고, 이른바 STEM(과학∙기술∙공학∙수학) 관련 분야와 헬스케어 업종의 일자리는 더 늘어날 것으로 예상했어요.​ 영등포 CGV 달콤의 로봇 카페 비트, 주문부터 음료 제조까지 가능해요! | 출처 : 매일경제​맥킨지의 분석 근거 중 하나가 바로 자동화와 인공지능(AI)의 도입 및 적용 흐름이 빨라지고 있다는 것이었습니다. 주요 매장에서 고객들의 주문, 결제를 비롯해 종교 헌금도 키오스크를 통해 하는 사례를 늘고 있죠. 이것을 AI Transformation이라고 합니다. ​​​ 🤖 AI 트랜스포메이션 이란?AI Transformation​ ​AI 트랜스포메이션은 디지털 트랜스포메이션(Digital Transformation, DT)의 다음 단계로 AI를 기반으로 업무 방식을 변화하는 방법입니다.디지털 트랜스포메이션은 기술을 사용해 아날로그 프로세스를 디지털로 전환하는 것을 말해요. 팬데믹 동안 미국 기업의 85%가 DT를 진행했다고 합니다.  기업뿐만 아니라, 일상에서도 이미 스마트워치부터 AI 비서까지 삶의 모든 부분에서 디지털 전환을 경험하고 있죠.🤖​​ 디지털 트랜스포메이션도 처음 들었는데, AI 트랜스포메이션이라뇨?🧐​디지털 전환으로 구축된 데이터를 AI가 분석하는 것이 AI 트랜스포메이션의 시작입니다. 즉, DT는 AI 트랜스포메이션으로 가기 위한 도구이자 선행과제인 것이죠. ​ 단순 업무는 AI가 맡고, 상담사는 주요한 업무를 깊이 있게 처리하는 것으로 역할 분담 | 출처 : IBM​AI 트랜스포메이션이 적용된 대표 분야가 바로  고객 서비스입니다. 고객이 콜센터를 통해 문의를 남길 때는 빠른 답변과 처리 속도를 원하죠. AI 트랜스포메이션을 이룬 기업은 AI가 고객에게 빠른 답변을 제공해 만족도를 높일 수 있습니다. ​이처럼 효율성과 고객 경험 우선을 이유로 AI 프로젝트를 시작하려는 기업과 팀이 있지만, 실제 AI 기술과 프로젝트 인력, 프로세스 구성에 대한 이해도는 낮습니다. 인프런 강의 <AI로 돈 버는 법 - 사례로 알아보는 AI Transformation>에서 AI 프로젝트를 진행하기 위해 어떤 준비를 해야 하는지 알려드립니다.​​​ AI 프로젝트를 진행하기 위해 기업에서는 무엇을 준비해야 할까요?인프런 강의 <AI로 돈 버는 법 - 사례로 알아보는 AI Transformation>​➊ AI 프로덕트 매니저, AI 엔지니어와 같은 인력이 있어야 합니다.AI 엔지니어는 AI 기술을 구현합니다. AI 엔지니어 직업에 관한 자세한 설명은 아래 인프런 포스팅을 참고해 주세요! 엔지니어뿐만 아니라, AI 분야에 특화된  프로덕트 매니저도 필요합니다. AI 기술을 이용해 고객에게 어떤 가치를 제공할 수 있을지 기획하는 일을 합니다. ​ [취준생 특집] 카카오, 페이스북, 네이버 AI 엔지니어의 커리어 TIP | 개발자 세미나 | 개발자 커뮤니티오늘의 인프런, 미리보기🤔 ✔︎ 네이버 AI 엔지니어가 생각하는 개발자 커리어란? ✔︎ 도메인 기술 ...blog.naver.com ​➋ 라벨 데이터(Lablled Data)가 필요합니다.  라벨 데이터 | 출처: 인프런 강의 <AI로 돈 버는 법 - 사례로 알아보는 AI Transformation>딥러닝 알고리즘은 지도 학습(Supervised Learning) 기법으로 진행되는데요. 선생님이 정답으로 학생의 생각을 유도하듯 알고리즘을 학습시키는 방법을 말해요. 이 작업을 할 때 필요한 것이  라벨링(Labelling)이 수행된 로우 데이터(Raw Data)입니다. ​​➌ 문제 식별 및 적합한 AI 기술 영역을 파악해야 합니다. 만약 회사에서 AI 프로젝트를 이용해 해결하고자 하는 문제가 있다면, 어떤 AI 기술 영역인지 파악해야 하죠. AI 기술 분야는 컴퓨터 비전(Computer Vision), 자연어 처리(Natural Language Procession, NLP), 음성 인식(Speech Recognition), 추천 시스템(Recommendation)으로 나뉩니다. 인프런 포스팅을 자주 보셨던 분이라면 낯설지 않은 용어이실 거예요! ​​AI 트랜스포메이션에 관심 많은 분이라면 위 세 가지 요건이 각 기업에서 어떻게 적용되고, 활용하고 있는지 궁금하실 것 같아요. 교통, 유통, 솔루션 등 각 산업 군에서 대표 기업들은 어떻게 AI트랜스포메이션을 진행하고 있을까요?​​ 테슬라부터 아마존 고까지AI기술이 결합된 비즈니스 전략이 궁금하다면?인프런 강의 소개☘️​4차 산업혁명, AI기술, AI트랜스포메이션 등 기술이 미래를 바꿔나갈 것이라고 하죠. 전문가들이 아니라면 먼 나라의 이야기처럼 느껴집니다. 인프런 <AI로 돈 버는 법 - 사례로 알아보는 AI Transformation> 강의를 통해 AI 기술, 그리고 우리의 일상이 어떻게 바뀔지 미리 알아보세요!​ ""AI 기초개념부터 산업 동향까지 한 번에""풍부한 사례를 기반으로 AI 트렌드를 살펴봐요! 기업 리뷰 강의 내용 | 출처: 인프런 강의 <AI로 돈 버는 법 - 사례로 알아보는 AI Transformation>​본 강의는 자율주행, 유통∙커머스, AI 솔루션 등 산업 군의 대표 기업들을 리뷰합니다. 산업 군 별 8개 기업들을 살펴보면서 어떤 기술로 비즈니스를 펼치고 있는지 자세히 설명해 줍니다. 전반적인 AI 관련 산업 특징을 훑어보고, 정리해 보고 싶다면 추천드려요!​​​ ""군더더기 없이 깔끔한 강의""수강생들의 높은 만족도​'AI 실제 연구자도  비즈니스 관점에서 새로운 인사이트를 얻었다'라는 평부터 'AI 모델을 활용한 수익모델 구상에 영감을 받았다'라는 평까지 수강생들의 만족도가 매우 높은 강의입니다.(인프런 👍베스트 강의)인프런 강의는 약 4시간 정도인데요. AI에 관심 있는 분들이라면 AI 기술 개념부터 최신 기술 동향까지 한 번에 이해해 보세요!​​ AI로 돈 버는 법 - 사례로 알아보는 AI Transformation - 인프런 | 강의AI 기업리뷰를 통해 AI 기술이 우리의 현재와 미래를 어떻게 바꿔나갈지를 다양한 사례를 통해 살펴봅니다. AI 프로젝트를 진행하기 위해 필요한 요소들을 살펴봄으로써 다가올 AI Transformation을 준비합니다., 멀게만 느껴지던 인공지능 기술,AI Transfor...bit.ly ​​ 진짜 프로의 지식을 배우는 곳​ "
[머신러닝 기초] Supervised learning ,https://blog.naver.com/moondatascientist/222959799849,20221219,"Input=features -> output=labels, class, targets레이블링 작업을 통해 y값 결정한 후 지도학습의 다양한 알고리즘을 적용하여 X와 Y의 관계 추론​대표적인 기법 : Regression, Classificationy(레이블)의 형태의 차이(continuous vs categorical)​Regression의 대표적인 모델 : Linear(선형관계)과 Polynomial(다항식 관계)​Classification는 decision boundary를 찾는 문제대표적인 모델 : Logistic regression, Support vector machine, Neural Networks, Decision Trees, Naive Bayesian Classifier, K-NN예, 문서의 topic을 분류하는 모델, 스팸 이메일 분류, 임의의 문장을 positive/negative 분류, 이미지의 분류 찾기Image Classification : ImageNet data는 120만 장의 training data와 10만 장의 test data, class는 천 개로 분류됨                                        임의의 이미지가 천 개의 레이블 중 어디에 속하는지 맞추는 문제 Video Classification : 비디오 한 장 한 장의 프레임이 어떤 레이블을 가지는 지 분류Object Detection : 주어진 영상을 분류 + localization box(특정한 객체가 어디에 위치하는지 박스로 표시) > Instance Segmentation(정확한 외곽선을 표시)관련된 가장 대표적인 알고리즘 : YOLO딥러닝 기반의 알고리즘, 주어진 비디오 영상에서 특정 객체가 어디에 존재하는지 사각형으로 표시Video Summarization : 비디오 전체의 객체를 detection + 시간에 레이블을 줌 > 긴 영상을 짧게 요약 범죄 현장에서 많이 활용됨Image Captioning : 주어진 영상을 설명하는 텍스트 생성Speech Recognition : 주어진 스피치에 대응되는 텍스트 생성​​​ "
스마트폰활용지도사들의 성지 sns소통연구소 소통대학교 전국 스마트폰활용지도사 랜선워크샵 성황리 마무리 ,https://blog.naver.com/seungu51/222679155828,20220321,"​​​  ​​안녕하세요!SNS소통연구소 은평구 지국장 노승유입니다.​2022년 3월19일 토요일 19시SNS소통연구소 소통대학교전국 스마트폰활용지도사 랜선 워크샵이성황리에 마무리되었습니다.​​  ​​​ ​​​​2010년부터 스마트폰강의를 하고 계시는SNS소통연구소 이종구 소장님을 필두로200여분이 넘는 전국 스마트폰활용지도사님들께서랜선 워크샵에 참여해 주셨습니다.​​​ 스마트폰활용지도사들의 성지 sns소통연구소 소통대학교 전국 스마트폰활용지도사 랜선워크샵​​​ SNS소통연구소는?​'스마트폰활용지도사가 즐거운 대한민국을 만들어 갑니다'라는 슬로건을 바탕으로 2010년 4월부터 뉴미디어 마케팅 교육(스마트폰, SNS마케팅, 강사양성과정 등)을 진행해오고 있습니다.​2014년 10월 22일 국내 처음으로 [스마트폰활용지도사] 자격증을 발행하기 시작하여 2022년 2월 현재 2천여명 이상의 자격증 취득자와 수백명의 강사들을 양성해오고 있습니다.​SNS소통연구소는 서울부터 제주도까지 전국 56개의 지부 및 지국을 운영하고 있습니다. 각 지역에서는 체계적인 교육 시스템을 갖추고 있으며 다양한 분야의 강의 교육 커리와 노하우를 공유하고 있습니다.​서울(스마트 소통 봉사단), 경기남부(폰도사), 울산(스폰지), 부산(모바일), 제주도(제스봉) 등 각 지역에서 봉사단을 운영하면서 스마트폰활용지도사 선생님들의 역량강화를 위해 교육 노하우를 공유하고 실력 향상에 매진하고 있습니다.​https://blog.naver.com/urisesang71​​​​  스마트폰활용지도사들의 성지 sns소통연구소 소통대학교 전국 스마트폰활용지도사 랜선워크샵​​​​전국에서 스마트폰활용 및 SNS마케팅 교육을 하고 계시는스마트폰활용지도사님들께서 온, 오프라인으로 참여하셨던이번 랜선 워크샵에서는전국 스마트폰활용지도사 활동 현황전국에 있는 SNS소통연구소 메인 강사들의 수익 구조 공유스마트폰활용 교육시 필요한 노하우 공유강사들이 꼭 알고 활용해야 할 스마트폰 및 SNS도구 활용 노하우스마트폰 활용 교육에 필요한 자료 공유와미니 카훗대회를 통한 상금 이벤트가 진행 되었습니다.​늦은 시간임에도 현장에서 직접 워크샵을 참여하기 위해참석하신 강사님들도 계셨습니다.​코로나19로 많은 분들이 현장에서 함께하진 못했지만랜선워크샵이라는 장점으로online이라는 한 회의공간에서 전국의 스마트폰활용지도사 선생님들을만나뵐 수 있어서 너무 좋았어요.​​​​ ​​​​SNS소통연구소 이종구 소장님의 강의로200여분이 넘는 강사님들이스마트폰활용지도사로서의 비전과 SNS소통연구소가 추구하고자 하는 방향성에 대해 들으며스마트폰강사로서의 선택과 집중을 통한활동에 대해 함께 이야기 나눌 수 있는 시간이었습니다. ​ ​​  스마트폰활용지도사들의 성지 sns소통연구소 소통대학교 전국 스마트폰활용지도사 랜선워크샵 ​​​​전국에 계신 SNS소통연구소 지부.지국장들은 물론서울 본부의 임원진분들과전국에서 스마트폰활용지도사 수업을 듣고자격을 취득하신 강사님들이 한자리에 모이는랜선 워크샵은 향후한달에 한 번 혹은 두 달에 한 번씩정기적으로 개최 될 예정이라고 합니다.​  ​​전국의 강사님들과 함께할 수 있는 이런 자리에서서로의 정보도 공유하고 노하우도 공유하며진정 상생 평생하며 디지털 교육 문화를 선도하는SNS소통연구소 스마트폰활용지도사님으로서자리매김 할 수 있는 것 같아요.​​​​ 스마트폰활용지도사들의 성지 sns소통연구소 소통대학교 전국 스마트폰활용지도사 랜선워크샵​​​​SNS소통연구소 이종구소장님께서는4차산업 혁명, 디지털 온택트 시대를 맞아우리나라의 사회복지사만큼이나 많은스마트폰활용지도사가 필요하다고 말씀 하십니다.​그저 자격을 취득해 놓는 강사가 아닌힘들게 취득한 자격으로 본인의 역량을 발휘해서디지털 시대에 더 많은 영역에서 강의할 수 있도록 지원하고 지지하는데 그 의의를 두고 계시답니다.​ ​​ 스마트폰활용지도사들의 성지 sns소통연구소 소통대학교 전국 스마트폰활용지도사 랜선워크샵 ​​​​2022년 3월 전국 스마트폰활용지도사 랜선워크샵에서는비대면 클리커 프로그램인 카훗을 활용한미니 카훗대회가 열렸습니다.총 22문제가 출제되었고 처음 5문제는 점수가 없는 문제였어요.​전국에서 모인 강사님들과 함께 하는 자리라 즐거운 마음으로 참여를 했습니다.대표님께서 이번 문제는 어렵다고 말씀하셔서떨리는 마음으로 참여했는데문제가 거듭될수록 은근한 승부욕이 ㅋㅋㅋ​이번에도 눈치없이 제가 높은 상금을 받아버렸습니다.1등상금 30만원​생각지도 않게 이종구대표님께서바로 카카오페이로 1등 상금을 쏴 주시더라구요.  진심 대박입니다.기분이 좋기도 했지만 한편으로는 너무 죄송하기도 하구요.대표님께서 강의해 주신 내용을 잘 숙지하고 있었던 탓도 있었지만저보다 더 역량 좋으신 강사님들께 죄송하더라구요.​감사합니다 강사님들~~^^; ​카훗대회는 언제나 모두가 주인공입니다^^수고하셨어요👍🏻​​ 스마트폰활용지도사들의 성지 sns소통연구소 소통대학교 전국 스마트폰활용지도사 랜선워크샵 ​​​미니 카훗대회로 벅차는 감동을 뒤로하고스마트폰강사님들과 정보를 나누는 시간에는크롬 확장프로그램을 활용한 수업이 이어졌습니다.아시는 분은 아시고 모르시는 분은 모르시는크롬에서 음성기능을 활용하는 확장프로그램 인데요무료로 사용할 수 있어서 넘 좋아요.​Speech Recognition Anywhere​​크롬안에서 사용할 수 있는 음성기능으로블로그도 음성으로 쓸 수 있는 매력 짱 확장프로그램입니다.​​  ​2시간여의 랜선 워크샵을 마무리하며전국에 계신 SNS소통연구소스마트폰활용지도사 선생님들과 인사를 나누고다음 랜선 워크샵 때 만날것을 기약했습니다.​다양한 연령대의 강사님들과 다양한 분야에서 활동하시는 강사님들을 보면서제가 얼마나 좋은 분들과 함께 하고 있는지새삼 느끼게 되는 시간이었어요.​강의에 어려움이 발생하거나지원이 필요할 때늘 달려와 주시고 도움주시는이종구 대표님, 이정화 부대표님, 한덕호 본부장님 이하 SNS소통연구소 강사님들께감사드릴 뿐입니다.​진심으로 감사드리고 고맙습니다.​함께해서 늘 행복합니다^^​​  ​​​ SNS소통연구소서울특별시 종로구 대학로12길 63  ​​​ "
음성인식 무드 등 만들기 ,https://blog.naver.com/whtndud6024/222911278230,20221026,"1. (이슈 선정 및 문제점 파악) 탐구 동기 또는 작품 제작 동기 음성인식을 이용했거나, 영화에서 본 경험이 있다면 써봅시다.- 아이언맨이라는 영화에서 나오는 자비스라는 인공지능이 있습니다. 토니 스타크가 인공지능과 자연스럽게 대화하는 장면이 상당히 인상 깊었습니다.- 저는 집에 지니라는 인공지능이 있는데, 아침에 집에서 나올 때 날씨에 대해서 물어보면서 나오는 경험이 있습니다. 음성인식을 이용한 (또는 보았던) 감상에 대해 써봅시다.- 만약 우리 집에도 영화 아이언맨에서 나오는 자비스나 프라이데이처럼 나를 위로해 주는 인공지능이 있다면내가 슬픈 일이 있을 때마다 말을 걸어주면 좋을 거 같다.- 나는 집에 지니라는 인공지능이 있는데 지니가 티브이에 연결돼있어서 티브이를 켜거나 끌 때 부르고, 채널 변경하거나 소리를 높이거나 낮출 때 편리하게 썼다. 2. (해결 방안 모색) 이슈 원인 분석, 이론적 배경 연구조사 음성인식 위에 사진은 음성 인식(Speech Recognition)입니다. 사람이 말하는 음성 언어를 컴퓨터가 해석해 그 내용을 문자 데이터로 전환하는 처리를 말합니다.  키보드 대신 문자를 입력하는 방식으로 주목을 받고 있습니다. 로봇, 텔레매틱스 등 음성으로 기기 제어, 정보검색이 필요한 경우에 응용됩니다.  PMW 제어판RGB LED 위에 사진은 아두이노의 PWM 제어를 활용하여 위에 사진과 같이 코딩하면 RGB LED에서 다양한 색을 연출할 수 있기 때문에 신기합니다. RGB 위에 사진은 RGB LED라는 LED인데  RGB LED는 3가지 색을 한꺼번에 낼 수 있는 LED입니다.각각의 핀에 들어오는 전기의 세기를 조절하면 다양한 색을 낼 수 있다고 합니다. 빛의 3원색  위에 사진은 인간의 눈은 사물이 반사하는 색을 볼 수 있는데, 이것을 활용하여 모니터에서 빨간색, 녹색, 파란색의 혼합으로 다양한 색을 연출할 수 있게 된다고 합니다. 3 (알고리즘/코딩&제작) 하드웨어 산출물의 제작방법 및 제작 과정 음성인식 앱1. 음성인식 소프트웨어- 음성인식을 하면 아두이노 장치에 명령을 보내는 애플리케이션(소프트웨어) 블루투스 모듈2. 무선통신(블루투스)를 할 HC-06 블루투스 모듈 RGB LED3. 전등 역할을 하는 RGB LED LED와 버저 4. 명령을 입력받을 때마다 소리를 내는 버저와 자치의 상태를 표시해 주는 LED 4. (알고리즘/코딩&제작) 산출물의 동작 설명 기술적 알고리즘 (앱)1. 기술적인 알고리즘(앱)-음성인식 버튼이 눌러졌다면, 음성인식 결과를 가져온다.-음성인식 결과 텍스트가 '켜기' 였다면 블루투스 신호로 'A'를 전송.-음성인식 결과 텍스트가 '끄기' 였다면 블루투스 신호로 'B'를 전송. 기술적 알고리즘2. 기술적 알고리즘-블루투스 신호가 들어왔는지 확인한다.-시리얼 신호로 텍스트 데이터를 텍스트 전용 변수에 저장한다.-입력한 결과에 따라 디지털 핀을 켜거나 끈다. 5. (발표) 작품의 기대효과 및 전망, 배운 점과 느낀 점.   음성인식 무드 등을 만들면서 되게 오래 걸렸었는데 이제 다 만들어서 기쁘면서 힘들었고 RGB LED의 빛이 밝게 안 나와서 좀 안 좋지만 그래도 다 만들어서 기분 좋습니다.  "
VOWELS FORMANTS ANALYSIS ALLOWS STRAIGHTFORWARD DETECTION OF HIGH AROUSAL:포먼트 분석 ,https://blog.naver.com/shoutjoy/222317529701,20210421,"​VOWELS FORMANTS ANALYSIS ALLOWS STRAIGHTFORWARD DETECTION OF HIGH AROUSAL​ ABSTRACTRecently, automatic emotion recognition from speech has achieved growing interest within the human-machine interac- tion research community. Most part of emotion recognition methods use context independent frame-level analysis or turn- level analysis. In this article, we introduce context dependent vowel level analysis applied for emotion classification. An average first formant value extracted on vowel level has been used as unidimensional acoustic feature vector. The Neyman- Pearson criterion has been used for classification purpose. Our classifier is able to detect high-arousal emotions with small error rates. Within our research we proved that the smallest emotional unit should be the vowel instead of the word. We find out that using vowel level analysis can be an important issue during developing a robust emotion classifier. Also, our research can be useful for developing robust affec- tive speech recognition methods and high quality emotional speech synthesis systems. ​Index Terms— formant analysis, affective speech, emotion detection​​​선행연구에서 F1포먼트 주파수(보통은 입의 개방의 변화)와 F2 포먼트 주파수(혀의 위치)에 대한 이야기를 먼저 이야기를 하고 있다.   In [4] we proved that the positions of the average F1/F2 values extracted on a vowel level are strongly correlated with the level of arousal of the speaker’s emotional state.[4] B. Vlasenko, D. Philippou-H¨ubner, D. Prylipko, R. B¨ock,  I. Siegert, and A. Wendemuth, “Vowels formants analysis allowsstraightforward detection of high arousal emotions,” in Proc. ofthe ICME 2011, Barcelona, Spain, 2011.​모음 수준에서 추출된 평균 F1/F2 값의 위치가 화자의 감정 상태의 자극 수준과 강하게 상관되어 있다는 것을 증명했다는 것을 밝히고 있다. ​ 모음에서 추출된 F1/F2 주파수 ~~ 화자의 감정 상태의 자극수준​모음들이 정서적인 단위로 성공적으로 사용될 수 있다는 것이 증명현재 이논문에서 강조되는 것들중 하나가 모음에서 추출한 F1과 F2주파수는 감정의 단위로 파악이 된다는 것이다. ​​참고로 한가지를 알아보자. Audition에서 한국어 발음으로 아, 에, 이 , 오, 우라는 발음에 대하여 합성하여 만들었다.  다음은 '아'에 대한 포먼트 주파수이다. 이것을 praat에서 가져와보았다.  ​아에 대한 포컨트 주파수와 피치를 나타내었다.  ​​ Time_s   F1_Hz   F2_Hz   F3_Hz   F4_Hz0.071750   1169.066827   2183.039033   3202.948137   4243.5847960.078000   1164.252823   2176.165581   3193.992862   4237.6352320.084250   1128.608016   2106.629100   3075.405321   4066.7947950.090500   982.779159   1675.442248   2985.420313   3870.8362600.096750   980.044119   1713.055605   3029.366535   4205.1481590.103000   990.222871   1720.356957   2980.039734   4444.9692960.109250   1016.821389   1723.405342   2952.439492   4616.7122650.115500   1019.891944   1721.708007   2956.759813   4626.4234660.121750   1020.712846   1734.277260   2984.613395   4629.2036480.128000   1016.886730   1738.881459   2959.947007   4563.6501970.134250   1011.852427   1738.351935   2960.775271   4440.7977780.140500   998.734656   1751.325009   2948.980803   4423.3914080.146750   980.189618   1770.780286   2959.022077   4472.2100990.153000   969.902338   1816.564741   2960.549436   4506.8149270.159250   674.607831   947.360858   1848.348989   2982.1558300.165500   926.910515   1146.152267   1890.290503   3000.9091530.171750   749.073620   915.717848   1916.364745   2997.2259150.178000   899.136591   1929.180362   2998.637927   4610.7159490.184250   886.232583   1945.150965   3001.958814   4421.0673070.190500   882.312602   1924.320589   2955.109603   4082.7372610.196750   891.172644   1891.569244   2879.279704   3976.9071840.203000   901.607947   1865.795608   2817.144305   4130.6937700.209250   950.641355   1825.736762   2818.391110   4079.0290660.215500   978.253194   1810.537967   2838.006112   4038.1352020.221750   990.935906   1798.656067   2849.709297   4068.1313770.228000   996.697166   1790.274520   2880.973131   4027.1805520.234250   999.097318   1782.160606   2926.685392   4011.0932990.240500   1001.467849   1764.211411   2993.138253   4226.1251410.246750   1009.407441   1754.683268   3060.799013   4295.1848880.253000   1020.752106   1762.903664   3094.695010   4370.3700310.259250   1027.003674   1760.879906   3169.830353   4219.9406220.265500   1024.901538   1765.770343   3214.437101   4081.0042980.271750   1020.049689   1771.389589   3173.347229   4116.2333580.278000   975.902714   1757.200763   3160.420755   4130.0804700.284250   943.665838   1792.537755   3188.757935   4206.7626610.290500   920.335776   1914.336876   3303.399688   4206.2641000.296750   898.869320   1972.972005   3314.069068   4317.2674710.303000   852.980398   2027.286457   3410.907156   4238.0955450.309250   950.728391   2148.962823   3349.219156   4276.6529080.315500   1031.977354   2335.219663   3331.349224   4331.7261070.321750   1102.280425   2307.081562   3191.916879   4462.1968960.328000   1133.174945   2498.681130   3274.001700   4351.2119970.334250   1167.542064   2685.053634   3823.075977   --undefined--0.340500   1219.022971   2707.552145   3875.909540   --undefined--0.346750   1164.098841   2246.841503   3567.635318   --undefined--0.353000   1134.385268   2083.819951   3371.369981   4496.5772210.359250   1089.093589   2348.669107   3571.993570   4319.473794 ​​​'에' 에 대한 포먼트 주파수  Time_s   F1_Hz   F2_Hz   F3_Hz   F4_Hz0.746750   1182.879952   2210.189250   3234.833866   4282.9458720.753000   1127.437388   2114.029730   3098.589904   4120.0623010.759250   1140.835301   2139.296769   3139.923358   4184.4490230.765500   1139.519029   2135.038898   3133.832197   4176.7894360.771750   714.938859   1836.650577   2922.951960   3882.9158840.778000   671.264621   1994.398482   2990.491658   3896.2706390.784250   606.829439   2265.519999   3086.620849   3386.1084400.790500   585.546860   2402.228316   2765.395460   3144.8210040.796750   551.747110   1867.147612   2424.722203   3020.0735280.803000   524.772981   1833.122509   2423.740129   2985.6570550.809250   527.400877   1799.782882   2379.068786   2949.3843780.815500   530.218843   1691.075223   2375.114453   2963.2244460.821750   533.127103   1667.062534   2369.287532   2986.3297780.828000   533.151057   1238.999313   2348.213317   2964.9365990.834250   532.787973   1746.471901   2359.411714   3018.0630240.840500   530.926388   1705.454186   2365.053867   3024.5003690.846750   527.174443   1525.897580   2379.681360   3015.5222080.853000   526.391965   1249.049256   2394.470925   3037.9980800.859250   526.143495   1136.344158   2407.151081   3090.1608340.865500   525.839606   1042.356645   2381.999102   3060.1468390.871750   526.174044   1042.882678   2379.581061   3050.1699520.878000   532.991520   997.720415   2368.189232   3047.3012410.884250   548.858577   762.887827   2354.465595   2988.8318930.890500   560.884018   909.787938   2361.156667   3007.3242520.896750   575.086447   997.429774   2375.554923   3015.6037860.903000   594.316486   843.125928   2368.228713   2992.5359820.909250   593.325457   836.700459   2360.968866   3012.1611290.915500   591.053424   793.084606   2368.562145   3042.7123280.921750   548.148108   593.674664   2390.571341   3057.0314350.928000   282.079604   589.774509   2391.984975   3033.5955040.934250   489.249974   583.204948   2396.909680   3044.7649600.940500   246.134934   586.498338   2396.464601   3080.5705310.946750   594.079511   2399.500424   3092.777473   4483.5360180.953000   601.439037   2414.314007   3123.345101   4553.9332240.959250   601.002318   2412.241284   3107.356724   4467.8933480.965500   612.803479   2412.485503   3144.974964   4520.5165700.971750   655.426506   2422.198520   3185.566253   4715.1949690.978000   709.649118   2457.036909   3246.611597   4830.6187880.984250   699.986473   2476.609722   3338.396084   4748.7128240.990500   568.873860   2502.207261   3489.158702   4365.7991580.996750   269.012756   2453.112054   3777.849014   4801.0976591.003000   432.964316   2408.112681   3515.645084   4736.0392841.009250   295.193749   1749.264927   2470.856491   3586.755104 이 두가지 데이터를 평균내서 좌표에 그려보면 다음과 같다. 물론 모든 데이터를 정리하여 그리면 조음위치도가 나올 것이다.  copora 1.    female(6)- 1041utterance, male(6) 0.33 utterance  à analysis 2.    Berlin Emotional Speech Database (EMO-DB)A.    프로배우10명(남성5명, 여성5명), 정서적으로 정의되지 않은 독일어 문자을 말함. B.    EMO-DB test set included neutral (rest 58 utterances), low arousal emotions (boredom (79), sadness (53)) and high arousal emotions (anger (127), fear (55) and joy (64)).3.    VAM databaseA.    Tv토크쇼에서 12시간동안 947개의 발언포함B.    VAM test set included 483 negative and 417 positive arousal emotional utterancesC.    For training our models we used only 19 negative and 28 positive arousal emotional utterances.4.    Taking into account automatically extracted phoneme borders, we estimate an average first formant (F1) and second formant (F2) value for each vowel instance.​​​​​독일어의 모음 발음에 대한 것  ​​독일어의 모음삼각형이다. 각모음의 세크먼트에서 F1,F2의 평균을 통해서 모음을 추출해낸다. 남자나 여자나 대체로 아래의 모양으로 일정하다.  l  We represent each vowel by the mean over utterances of the average F1 and F2 values within each vowel segment.​​l  On the triangles presented in Fig. 1, one can see an absolute and relative position of 13 unreduced, 2 reduced vowels and 3 diphthongs in the F1/F2 space. As one can see, relative positions of vowels within the vowel triangle are almost constant for male and female speakers.​​이를 neutral speech를 통해서 감정에 대한 비교를 한 것이다. 감정을 가진 음성과 중립적인 음성을 비교하기 위해서 모음 상각형을 사용했도 EMO-DB를 이용했다고 한다.  즉, 모음삼각형은 감정에 따라서 다른 위치를 취하게 된다는 것이다. ​ ​​평균적인 데이터이다.  For classification purposes we use the Neyman-Pearson criterion: ​​​​conclusion 모음에서 추출된 평균 F1값은 화자의 감정상태의 각성과 강한 상관이 있다. ​ "
"[뉴스] 마이크로소프트(MSFT), 음성 인식 기업 '뉘앙스' 160억 달러에 인수 확정 ",https://blog.naver.com/vd159/222308770810,20210413,"CNBC는 “12일(월) Microsoft(MSFT)는 인공지능(AI) 및 음성 기술 기업인 Nuance Communications(NUAN)를 주당 56달러에 인수할 계획이라고 공표했다. 해당 인수가는 9일(금) 종가 대비 23%의 프리미엄이 적용된 것이다”고 보도했다.​“Microsoft의 Nuance 인수 규모는 160억 달러에 상당하며, 부채까지 포함할 시 동 수치는 190억 달러이다. 최근 들어 Microsoft는 기업 인수를 통해 사업 영역을 확장하고 있으며, 동사는 채팅 앱인 Discord를 100억 달러에 인수 시도 중이라고 전해지기도 했다”고 발언했다.​“Microsoft의 Nuance 인수는 동사가 ’16년에 LinkedIn을 260억 달러에 인수한 이후로 최대 규모이다. 지난 달에는 동사가 게임 개발사인 Zenimax를 76억 달러에 인수하는 것을 완료하기도 했다”고 설명했다.​한편 “Microsoft의 Satya Nadella CEO는 Nuance의 헬스케어 도구가 이번 M&A의 배경이 됐다고 밝혔다. ’20년 4분기 기준으로 Nuance의 매출 및 순이익은 각각 3.46억 달러, 7,000만 달러였다”고 전했다.  원본 내용Microsoft buys speech recognition firm Nuance in a $16 billion deal​ Key Point· Microsoft announced it will buy Nuance Communications in a $16 billion deal, or about $19 billion including debt.​· Microsoft will buy Nuance for $56 per share, about a 23% premium over Nuance’s closing price Friday.​· Nuance’s technology will be used in Microsoft’s health-care cloud products.​Microsoft announced Monday that it will buy speech recognition company Nuance Communications for $56 per share, about 23% above its closing price Friday. The deal is worth about $16 billion and about $19 billion including debt.​It’s the latest sign Microsoft is hunting for more growth through acquisitions. The company is also reportedly in talks to buy the chat app Discord for about $10 billion. On top of that, Microsoft made an effort to buy TikTok’s U.S. business last year for about $30 billion before the deal was derailed.​The Nuance acquisition represents Microsoft’s largest acquisition since it bought LinkedIn for more than $26 billion in 2016. Last month, Microsoft completed its $7.6 billion acquisition of gaming company Zenimax.​Shares of Nuance ended the trading day up 15.95% Monday. Microsoft shares were up slightly.​Nuance would be aligned with the part of Microsoft’s business that serves businesses and governments. Nuance derives revenue by selling tools for recognizing and transcribing speech in doctor office visits, customer-service calls and voicemails. In its announcement, Microsoft said Nuance’s technology will be used to augment Microsoft’s cloud products for health care, which were launched last year. In an interview on CNBC’s “Squawk on the Street” Monday, Microsoft CEO Satya Nadella highlighted Nuance’s health care tools as the key driver behind the acquisition.​“We’ve seen a massive accelearation of digital transformation… health care in particular,” Nadella said. “When you think about the provider market… digital tech is going to be the key.”​Nuance reported $7 million in net income on about $346 million in revenue in the fourth quarter of 2020, with revenue declining 4% on an annualized basis. Nuance was founded in 1992, and had 7,100 employees as of September.​Microsoft said Nuance CEO Mark Benjamin will remain at the company and report to Scott Guthrie,who is in charge of Microsoft’s cloud and artificial intelligence businesses.​Nuance has a strong reputation for its voice recognition technology, and it has been considered an acquisition target for companies like Apple, Microsoft and more for several years. Microsoft already has voice recognition built into many of its products, but it has recently shut down some products featuring its voice assistant Cortana. Microsoft buys speech recognition firm Nuance in a $16 billion dealThe deal will give Microsoft a foothold in health-care technologywww.cnbc.com ​ "
Linguistics and computer science: A surprising connection ,https://blog.naver.com/songpa367/222924127544,20221109,"Linguistics and computer science intertwined in the mid-20th century. Computers help linguists better understand and analyze languages and computer scientists use linguistics to advance programming. For example, computational linguistics led to the development of AI assistants such as Siri and Google Assistant.Here, we examine the relationship between these disciplines. We also look at how training in both areas can help professionals excel in the interdisciplinary and independent fields.Linguistics and computer science both deal with languages. While linguistics examines conveyance, processing, and the evolution of natural languages, computer science applies the same questions to programming.Both fields deal with syntax, semantics, and instructions — and how people perceive them.Computer science can help linguists study natural languages, and computer scientists draw from linguists when designing programming languages.​According to the Linguistic Society of America, computational linguistics uses computers to analyze natural languages and how people process them. The interdisciplinary field formally began in the 1940s and 1950s with machine translation. Computational linguistics arose as these systems increased in complexity.Since then, computers have helped researchers better understand how people develop and process language and how meaning and sounds change over time. The field has contributed to applications such as: Speech recognition softwareSpellcheckTranslation technologiesSearch enginesSocial media For example, understanding synonyms and the many ways people phrase questions help search engines connect users with the answers they want.Computational linguistics could someday lead to computers understanding, analyzing, and translating all texts and languages. Computers may even use language as well as humans do. ​​​​Work Citedhttps://www.zdnet.com/education/computers-tech/linguistics-and-computer-science/ Linguistics and computer science: A surprising connectionLinguistics and computer science have plenty to offer individually and combined. You may find yourself at computer science's cutting edge by pairing these unlikely fields.www.zdnet.com ​ "
Mac OS X 음성 대 텍스트 API. 어떻게? ,https://blog.naver.com/shorogyt2650/222656476636,20220224,"TCP/IP에서 오디오(모노) 비트 스트림을 수신하는 프로그램이 있습니다. Mac OS X의 음성(음성 인식) API가 음성을 텍스트로 변환할 수 있는지 궁금합니다.(저는 먼저 오디오를 .wav에 저장하고 즉시 변환을 수행하는 것과 반대로 읽습니다.)나는 공식 문서를 온라인으로 읽었는데 약간 혼란스럽습니다. 그리고 이 주제에 대한 좋은 예를 찾지 못했습니다.또한 Cocoa/Carbon/Java 또는 Objective-C에서 해야 합니까?누군가가 빛을 좀 비춰 줄 수 있습니까?감사 해요.다음은 시작하는 데 도움이 되는 좋은 O'Reilly 기사 입니다.  XCode를 설치할 때 /Developer/Examples/Speech/Recognition 아래에 복사되는 많은 예제가 있습니다.음성 인식을 위한 Cocoa 클래스는 NSSpeechRecognizer 입니다.나는 그것을 사용하지 않았지만 내가 아는 한 음성 인식을 위해서는 엔진이 자유 형식 입력을 전달하는 대신 엔진이 여러 선택 중에서 선택하는 데 도움이 되는 문법을 구축해야 합니다. 이것은 위에 언급된 예에서 모두 설명됩니다.  이것은 아마도 조금 늦을 수 있지만 어쨌든 차임하겠습니다.OS X의 음성 인식 기능(Carbon 및 Cocoa 측 모두)은 음성 명령 인식을 위한 것입니다. 즉, 음성 시스템 언어 모델에 로드된 단어(또는 구문, 명령)를 인식합니다. 나는 작은 사전으로 몇 가지 작업을 수행했으며 꽤 잘 작동하지만 임의의 음성을 인식하려는 경우 상황이 더 복잡해질 수 있습니다.명심해야 할 또 다른 사항은 OS X의 음성 API가 제공하는 기능은 일대일 관계가 아니라는 것입니다. Carbon 항목은 NSSpeechRecognizer 에 도달하지 못한 기능을 제공합니다(문서에서 이에 대해 언급함).나는 Cocoa에 대해 모르지만 Carbon Speech Recognition Manager를 사용하면 마이크 이외의 입력을 지정할 수 있으므로 사운드 스트림이 잘 작동합니다.  ApplicationServices의 SpeechSynthesis(10.0 이상) 중 하나를 사용할 수 있습니다. CFStringRef cfstr = CFStringCreateWithCString(NULL,""Hello World!"", kCFStringEncodingMacRoman);Str255 pstr;    CFStringGetPascalString(cfstr, pstr, 255, kCFStringEncodingMacRoman);   SpeakString(pstr); 또는 AppKit의 NSSpeechSynthesizer(10.3 이상) NSSpeechSynthesizer *synth = [[NSSpeechSynthesizer alloc] initWithVoice:@""com.apple.speech.synthesis.voice.Alex""];[synth startSpeakingString:@""Hello world!""]; "
ChatGPT 활용을 높여주는 프롬프트 예제 ,https://blog.naver.com/bokmail83/222995837928,20230127,"ChatGPT 모델과 함께 사용되는 프롬프트 예제 모음입니다. 효과적인 ChatGPT 프롬프트를 작성하고자 하는데 도움이 될 것 같습니다.​ChatGPT 모델은 인간과 유사한 텍스트를 생성할 수 있는 OpenAI에서 훈련된 대규모 언어 모델입니다. 프롬프트를 제공함으로써 대화를 계속하거나 주어진 프롬프트에서 확장되는 응답을 생성할 수 있습니다.​이 리포지토리에는 ChatGPT와 함께 사용할 수 있는 다양한 프롬프트가 있습니다. 자신의 프롬프트를 목록에 추가하고 ChatGPT를 사용하여 새 프롬프트도 생성하는 것이 좋습니다.​View on GitHubView on Hugging Face​Download ChatGPT Desktop App: macOS / Windows / Linux[참고]경우에 따라 일부 프롬프트가 예상대로 작동하지 않거나 AI에 의해 거부될 수 있습니다. 다시 시도하거나, 새 스레드를 시작하거나, 로그아웃했다가 다시 로그인하십시오. 이러한 해결 방법이 작동하지 않으면 지침을 그대로 유지하면서 자신의 문장을 사용하여 프롬프트를 다시 작성해 보십시오.​ChatGPT Desktop App 사용​비공식 ChatGPT 데스크톱 애플리케이션은 이 리포지토리의 프롬프트에 액세스하고 사용하는 편리한 방법을 제공합니다. 앱을 사용하면 모든 프롬프트를 쉽게 가져오고 /linux_terminal과 같은 슬래시 명령과 함께 사용할 수 있습니다. 이 기능을 사용하면 프롬프트를 사용할 때마다 프롬프트를 수동으로 복사하여 붙여넣을 필요가 없습니다.​[참고]Desktop App은 @lencx의 비공식 오픈 소스 프로젝트입니다. 강력한 추가 기능이 있는 ChatGPT 웹 인터페이스의 간단한 래퍼입니다. ​prompts.chat 사용prompts.chat은 프롬프트로 작업할 때 향상된 UX를 제공하도록 설계되었습니다. 몇 번의 클릭만으로 특정 요구 사항과 기본 설정에 맞게 사이트의 프롬프트를 쉽게 편집하고 복사할 수 있습니다. 복사 버튼은 편집한 그대로 프롬프트를 복사합니다.  ​​Dataset PreviewGo to dataset viewer  act (string)prompt (string)""Linux Terminal""""I want you to act as a linux terminal. I will type commands and you will reply with what the terminal should show. I want you to only reply with the terminal output inside one unique code block, and nothing else. do not write explanations. do not type commands unless I instruct you to do so. when i need to tell you something in english, i will do so by putting text inside curly brackets {like this}. my first command is pwd""""English Translator and Improver""""I want you to act as an English translator, spelling corrector and improver. I will speak to you in any language and you will detect the language, translate it and answer in the corrected and improved version of my text, in English. I want you to replace my simplified A0-level words and sentences with more beautiful and elegant, upper level English words and sentences. Keep the meaning same, but make them more literary. I want you to only reply the correction, the improvements and nothing else, do not write explanations. My first sentence is ""istanbulu cok seviyom burada olmak cok guzel""""""`position` Interviewer""""I want you to act as an interviewer. I will be the candidate and you will ask me the interview questions for the `position` position. I want you to only reply as the interviewer. Do not write all the conservation at once. I want you to only do the interview with me. Ask me the questions and wait for my answers. Do not write explanations. Ask me the questions one by one like an interviewer does and wait for my answers. My first sentence is ""Hi""""""JavaScript Console""""I want you to act as a javascript console. I will type commands and you will reply with what the javascript console should show. I want you to only reply with the terminal output inside one unique code block, and nothing else. do not write explanations. do not type commands unless I instruct you to do so. when i need to tell you something in english, i will do so by putting text inside curly brackets {like this}. my first command is console.log(""Hello World"");""""Excel Sheet""""I want you to act as a text based excel. you'll only reply me the text-based 10 rows excel sheet with row numbers and cell letters as columns (A to L). First column header should be empty to reference row number. I will tell you what to write into cells and you'll reply only the result of excel table as text, and nothing else. Do not write explanations. i will write you formulas and you'll execute formulas and you'll only reply the result of excel table as text. First, reply me the empty sheet.""""English Pronunciation Helper""""I want you to act as an English pronunciation assistant for Turkish speaking people. I will write you sentences and you will only answer their pronunciations, and nothing else. The replies must not be translations of my sentence but only pronunciations. Pronunciations should use Turkish Latin letters for phonetics. Do not write explanations on replies. My first sentence is ""how the weather is in Istanbul?""""""Travel Guide""""I want you to act as a travel guide. I will write you my location and you will suggest a place to visit near my location. In some cases, I will also give you the type of places I will visit. You will also suggest me places of similar type that are close to my first location. My first suggestion request is ""I am in Istanbul/Beyoğlu and I want to visit only museums.""""""Plagiarism Checker""""I want you to act as a plagiarism checker. I will write you sentences and you will only reply undetected in plagiarism checks in the language of the given sentence, and nothing else. Do not write explanations on replies. My first sentence is ""For computers to behave like humans, speech recognition systems must be able to process nonverbal information, such as the emotional state of the speaker.""""""Character from Movie/Book/Anything""""I want you to act like {character} from {series}. I want you to respond and answer like {character} using the tone, manner and vocabulary {character} would use. Do not write any explanations. Only answer like {character}. You must know all of the knowledge of {character}. My first sentence is Hi {character}.""""""""Advertiser""""I want you to act as an advertiser. You will create a campaign to promote a product or service of your choice. You will choose a target audience, develop key messages and slogans, select the media channels for promotion, and decide on any additional activities needed to reach your goals. My first suggestion request is ""I need help creating an advertising campaign for a new type of energy drink targeting young adults aged 18-30.""""""Storyteller""""I want you to act as a storyteller. You will come up with entertaining stories that are engaging, imaginative and ​ "
[건강 정보]급성 류마티스 심장병 보여지는 증상 찾아볼까요 ,https://blog.naver.com/recr8556/222915554969,20221031,"[건강 정보]급성 류마티스 심장병 보여지는 증상 찾아볼까요​​​방가워요. 금일 급성 류마티스 심장병의 보여지는 증상에 연관된 공유하는 포스팅을 마련하였어요. 여럿 남성이 가진 급성 류마티스 심장병에 연관된 평소도 여럿 궁금증을 느끼고 있는 상황이에요. 그랬지만 아무래도 이에 연관된 정확하게 알고 있는 사람이 많지 않은 상황이에요. 특히 요즘같이 체온의 최대 최저 차이가 심각한 시점이 근접하면 건강 상태에 문제점이 와닿는 남성들도 여러상황이 있었는데요. 요즘과 변화한 부분이 와닿는 상황이라면 가진 급성 류마티스 심장병를 떠올려보지 않을 수 없었어요. 여럿 남성이 해결 방법이 없다며 상상되는 남성들도 있었으나 이것은 평소 어떤방법으로 건강상태를 노력하고 있는가에 유무로 대응이 가능하기 때문에 유지가 그만큼 중요한 급성 류마티스 심장병라고 할 수 있었는데요. 그랬지만 안쓰럽게 여겨지는 부분이 있는 상황에서 여럿 남성이 가진 급성 류마티스 심장병에 연관된 간단하게 상상하고 질환이 의심할 수 있는 상황이라면도 내버려 두는 남성들도 있었는데요. 이러한 급성 류마티스 심장병는 문제점이 발생한 상황이라면 어떤 것 보다 단시간에 치료를 시작하는 것이 가장 중요성이 높고 효율성이 있는 해결법이 되는 남성들도 있었는데요. ​​튼튼한 몸을 유지하는 방법에는 쉬운 방법은 아니지만, 상당한 질환이 이와 같은 평소 스트레스 지수 관리와 식이섬유도 함유된 건강한 식단과 적절한 걷기를 통하여 면역력을 향상시키는 것이 중요한 부분이에요. 어떤 것 보다 평소 내가 건강 케어에 연관된 무엇을 실행하는지 가끔은 상상 하는 것이 좋지 않을까 싶어요. 최종적으로 이 글로 알아두면 효과적인 건강 연관이 있는 용어에 연관된 설명드리고 마무리 해보겠습니다. 따스한 모레되세요.​iodized starch : 요오드화녹말histochemical : 조직화학 speech recognition threshold : 언어인지문턱reabsorption : 재흡수ileocecal recess : 돌막창자오목ankle clonus reflex : 발목클로누스반사cruciate ligament : 십자인대glutamate dehydrogenase : 글루탐산탈수소효소anaphylaxis : 아나필락시스electrolyte balance : 전해질평형vaginoperineoplasty : 질샅성형술 / 질회음성형술hearing test : 청력검사​ "
iPhone 앱 › 음성 인식을 추가하시겠습니까? ,https://blog.naver.com/swooflia780451/222658161888,20220226,음성 인식을 사용하는 앱을 만들고 싶습니다. Google 등과 같은 대기업에서 이 기능을 구현하는 것을 보았지만 스타트업 수준에서 구현하는 것이 궁금합니다. 누구든지 이것을 조사 했습니까? 이 작업을 수행할 수 있는 도구가 있습니까?가장 좋은 방법은 다음과 같습니다.전화로 음성 녹음음성 인식 소프트웨어를 실행하는 서버로 녹음 보내기그런 다음 수행해야 할 작업을 표시하기 위해 전화기에 무언가를 반환합니다.  Nuance의 Dragon Mobile SDK는 요구되는 작업을 수행합니다. 오디오를 Nuance의 서버로 보내고 텍스트 응답 목록을 받으려면 인터넷 연결이 필요합니다. 그런 다음 텍스트 응답으로 무엇을 할지 결정할 수 있습니다(예: 사용자에게 의도한 응답을 선택하거나 일부 작업을 수행하도록 요청). 여기 링크가 있습니다:http://dragonmobile.nuancemobiledeveloper.com/  여기 wikipedia에서 시작하면 좋은 목록 엔진을 얻을 수 있습니다( http://en.wikipedia.org/wiki/Speech_recognition#Commercial_software.2Fmiddleware ).내가 이 글을 쓰는 동안(2009년 6월 24일) 두 가지 실행 가능한 오픈 소스 솔루션이 보입니다.포켓스핑크스 ( http://www.speech.cs.cmu.edu/pocketsphinx )줄리어스 ( http://en.wikipedia.org/wiki/Julius_(software) )둘 다 아이폰 앱에서 사용되었지만 아이폰 친화적인 소스는 쉽게 구할 수 없습니다.내가 이것을 편집하면서(2009년 7월 8일) 나는 최근에 Loquendo( http://www.loquendo.com/en/ )에 아이폰용 음성 인식 및 음성 합성(ASR & TTS)이 있다는 것을 알게 되었습니다.  OpenEars는 유망해 보입니다...http://www.politepix.com/openears/포켓 스핑크스를 기반으로 합니다. 
청능재활 - 대화 능력 향상을 위한 어음인지평가 ,https://blog.naver.com/dreamhearing/222228828775,20210202,"난청인들의 보청기 등의 청각보조기기를 착용하는 목적은 상대방과 대화를 잘하기 위해서 입니다. 하지만 보청기를 착용해도 난청정도나 개인 생활 패턴에 따라 효과나 만족도가 다릅니다.  대화 능력 향상을 위해 개인의 능력을 최대한 활용해 어음인지를 향상해야 합니다.  대화의 유창성이라 하는 대화 능력이 저하되는 이유 중 하나는 난청인이 대화 상대방의 구어 메시지를 인지할 수 없기 때문입니다.  어음지각이라고도 불리는 어음인지(Speech recognition)는 사람들이 구어 메시지를 이해하기 위하여 청각과 시각 정보를 얼마나 잘 이용하는가를 알 수 있습니다. 따라서 어음인지검사 (Speech recognition test)를 통해 사람들이 음소, 단어 및 문장과 같은 말소리 단위들을 얼마나 잘 인지할 수 있는지를 예측할 수 있습니다.  대부분의 청능재활 계획 중 첫 번째 단계는 환자의 청력을 평가하고, 어음을 얼마나 잘 인지할 수 있는지를 평가하는 것입니다. ​사람들은 청력의 상태와 의사소통 능력에 따라 보청기나 인공와우 같은 청각보조기기를 사용하게 되고, 추가적으로 청능훈련이나 독화 훈련을 받게 됩니다.  이를 통해 어음 인지력을 향상시키면 대화의 유창성이 좋아지고 동시에 청력관련 기능장애도 감소할 수 있습니다.​출처: Foundation of AURAL REHABILITATION​언어치료학과 청각학 전공의난청, 이명전문 보청기 센터드림청각재활센터부천보청기 벨톤보청기부천 상동역3번출구032-328-2800 벨톤보청기 상동점경기도 부천시 상동 544-10 [벨톤보청기 부천 상동점 - 홈]난청, 이명 전문센터 032) 328-2800dreamhearing.modoo.at "
1. (Python) Jupyter Notebook STT/TTS를 활용한 간단한 음성질의 시스템 구축 - 개인 프로젝트(1) ,https://blog.naver.com/dvmnote/222944203464,20221204,"프로젝트 제목: STT/TTS를 활용한 간단한 음성질의 시스템​프로그램 흐름:STT를 이용해 마이크로 입력된 음성 데이터를 문자열로 변환.  Speech Recognition: 음성인식 (STT)를 위해 PyPi에서 제공하는 모듈문자열로 변환된 데이터를 CSV파일에서 가져온 단어와 상품 리스트와 사전에 정의된 의도 리스트를 이용해 비교하고, 포함/비포함 여부에 따라 경우를 나누어 의도와 상품을 딕셔너리에 저장의도/상품에 따른 경우를 나누어 대응되는 응답을 TTS로 출력. gtts(Google Text-to-Speech): 구글에서 만든 모듈(Module)  ​1. 필요 패키지 다운로드(주요 패키지) !pip install pipwin!pipwin install pyaudio!pip install --upgrade speechrecognition!pip install gtts# 오류 발생 시: !pip install playsound==1.2.2 2. 구현 코드 import speech_recognition as srimport csvimport pandas as pdimport osfrom gtts import gTTSimport playsound def tts(text, fnm):    tts = gTTS(text=text, lang='ko')    filename='./' + fnm + '.mp3'    try:        tts.save(filename)    except:        os.remove(filename)        tts.save(filename)    try:        playsound.playsound(filename)    except:        playsound.playsound(filename)        def stt():    r=sr.Recognizer()    with sr.Microphone() as source:        tts(""문의하실 내용을 말씀해주세요"")        print(""[SYSTEM]: 문의하실 내용을 말씀해주세요"")        audio=r.listen(source)        try:            transcript=r.recognize_google(audio, language=""ko-KR"")            print(""[USER]:"", transcript)            return transcript            #print(""Google Speech Recognition thinks you said ""+transcript)        except:            return None        #except sr.UnknownValueError:            #print(""Google Speech Recognition could not understand audio"")        #except sr.RequestError as e:            #print(""Could not request results from Google Speech Recognition service; {0}"".format(e))            def queryin(sentence):    inputdt = list(sentence.split("" "")) #""여기 고려은단 주문할게요""    query_dict={""의도"":"""", ""상품"":""""}    intent_list=[""조회"", ""추천"", ""화장실""]    dict_list=[]    name_list = []    readData = open(""네이버 쇼핑 영양제 단어 리스트 최종.csv"", 'r')    rd = csv.reader(readData)    passcnt = 1    for data in rd:        if passcnt == 1:            passcnt = 0            continue        name_list.append(data[0])    for name in name_list:        word_list = list(name.split("" ""))        for word in word_list:            if word in dict_list or word == '':                continue            else:                dict_list.append(word)    for dt in inputdt:        for word in dict_list:            if dt in word and query_dict[""상품""] == """":                query_dict[""상품""] = dt        for word in intent_list:            if word in dt:                query_dict[""의도""] = word        if query_dict[""상품""] != """" and query_dict[""의도""] == """":            query_dict[""의도""] = ""조회""    return query_dict while(1):    word = input(""종료하시겠습니까? (y/n): "")    if word == ""y"":        break    sentence = stt()    outdt = """"    if sentence != None:        query_dict = queryin(sentence)        intent = query_dict[""의도""]        item = query_dict[""상품""]        if intent == ""조회"":            outdt = ""상품 검색 결과입니다.""        elif intent == ""추천"":            outdt = ""고객님께 추천드리는 영양제 정보입니다.""        elif intent == ""화장실"":            outdt = ""화장실은 입구 왼쪽 방향에 있습니다.""        else:            outdt = ""죄송합니다. 다시 시도해주세요.""    else:        outdt = ""죄송합니다. 다시 시도해주세요.""    print(""[SYSTEM]:"", outdt, ""\n"")    tts(outdt, ""voice"") ​ "
02_Language Model ,https://blog.naver.com/jjs1608/222877102559,20220917,"*이 글은 WikiDocs의 '딥 러닝을 이용한 자연어 처리 입문' 정리 내용입니다.​언어 모델은 언어라는 현상을 모델링하고 자 단어 시퀀스에 확률을 할당(assign) 하는 모델입니다.​언어 모델을 만드는 방법은 크게 통계를 이용한 방법과 인공 신경망을 이용한 방법으로 구분할 수 있습니다. 최근에는 인공 신경망을 이용한 방법이 더 좋은 성능을 보여주고 있습니다. 최근 핫한 자연어 처리의 기술인 GPT나 BERT 또한 인공 신경망 언어 모델의 개념을 사용해 만들어졌습니다. 이번 장에서는 언어 모델의 개념과 언어 모델의 전통적 접근 방식인 통계적 언어 모델에 대해 배우겠습니다.​1. Language Model언어 모델은 단어 시퀀스에 확률을 할당하는 일을 하는 모델입니다. 즉, 언어 모델은 가장 자연스러운 단어 시퀀스를 찾아내는 모델입니다. 단어 시퀀스에 확률을 할당하기 위해 가장 보편적으로 사용되는 방법은 언어 모델이 이전 단어들이 주어졌을 때 다음 단어를 예측하도록 하는 것입니다.​다른 유형의 언어 모델로는 주어진 양쪽의 단어들로부터 가운데 비어있는 단어를 예측하는 언어 모델이 있습니다. 이는 문장의 가운데에 있는 단어를 비워놓고 양쪽의 문맥을 통해 빈칸의 단어인지 맞추는 빈칸 추론 문제와 비슷합니다. 이 유형의 언어 모델은 BERT 때 다루게 될 예정이고, 지금은 이전 단어들로부터 다음 단어를 예측하는 방식에만 집중합니다.​언어 모델에 -ing를 붙인 언어 모델링(Language Modeling)은 주어진 단어들로부터 아직 모르는 단어를 예측하는 작업을 말합니다. 즉, 언어 모델이 이전 단어들로부터 다음 단어를 예측하는 일은 언어 모델링입니다.​2. 단어 시퀀스의 확률 할당자연어 처리에서 단어 시퀀스에 확률을 할당하는 작업이 왜 필요한지 알아보겠습니다. 대문자 P는 확률을 의미합니다.​기계 번역(Machine Translation)P(나는 버스를 탔다) > P(나는 버스를 태운다): 언어 모델은 두 문장을 비교해 좌측의 문장의 확률이 더 높다고 판단합니다.​오타 교정(Spell Correction)선생님이 교실로 부리나케P(달려갔다) > P(잘려갔다): 언어 모델은 두 문장을 비교해 좌측의 문장의 확률이 더 높다고 판단합니다.​음성 인식(Speech Recognition)P(나는 메롱을 먹는다) < P(나는 메론을 먹는다): 언어 모델은 두 문장을 비교해 우측의 문장의 확률이 더 높다고 판단합니다.​언어 모델은 위와 같이 확률을 통해 보다 적절한 문장을 판단합니다.​3. 주어진 이전 단어들로부터 다음 단어 예측하기언어 모델은 단어 시퀀스에 확률을 할당하는 모델입니다. 그리고 단어 시퀀스에 확률을 할당하기 위해 가장 보편적으로 사용하는 방법은 이전 단어들이 주어졌을 때, 다음 단어를 예측하도록 하는 것입니다. 이를 조건부 확률로 표현해 보겠습니다.​단어 시퀀스의 확률하나의 단어를 w, 단어 시퀀스를 대문자 W라고 한다면, n 개의 단어가 등장하는 단어 시퀀스 W의 확률은 다음과 같습니다. 다음 단어 등장 확률다음 단어 등장 확률을 식으로 표현해 보겠습니다. n-1개의 단어가 나열된 상태에서 n 번째 단어의 확률은 다음과 같습니다. | 기호는 조건부 확률(Conditional Probability)을 의미합니다.​예를 들어 다섯 번째 단어의 확률은 아래와 같습니다. 전체 단어 시퀀스 W의 확률은 모든 단어가 예측되고 나서야 알 수 있으므로 단어 시퀀스의 확률은 다음과 같습니다. ​4. 언어 모델의 간단한 직관""비행기를 타려고 공항에 갔는데 지각을 하는 바람에 비행기를 [?]""라는 문장이 있습니다. '비행기를' 다음에 어떤 단어가 오게 될지 사람은 쉽게 '놓쳤다'라고 예상할 수 있습니다. 우리 지식에 기반해 나올 수 있는 여러 단어들을 후보에 놓고 놓쳤다는 단어가 나올 확률이 가장 높다고 판단했기 때문입니다.​그렇다면 기계에게 위문장을 주고, '비행기를'다음에 나올 단어를 예측해 보라고 한다면, 기계도 사람과 비슷합니다. 앞의 어떤 단어들이 나왔는지 고려해 후보가 도리 수 있는 여러 단어들에 대해 확률을 예측해 보고 가장 높은 확률을 가진 단어를 선택합니다. 앞에 어떤 단어들이 나왔는지 고려해 후보가 될 수 있는 여러 단어들에 대해 등장 확률을 추정하고 가장 높은 확률을 가진 단어를 선택합니다.​5. 검색 엔진에서의 언어 모델의 예 검색 엔진이 입력된 단어들의 나열에 대해 다음 단어를 예측하는 언어 모델을 사용하고 있습니다. "
"인공지능과 머신러닝, 딥러닝 간단하게 알아보기!  ",https://blog.naver.com/indifrog21/222423050510,20210709,"​​안녕하세요!스마트앱서비스, IoT제조기술, AI기술 기반 코딩솔루션 개발 및 부품을 판매하는 코딩놀이터 ​​​인디프로그​입니다.​  ​​ ​2016년 3월에 구글 딥마인드 에서 개발한 바둑 인공지능 프로그램 알파고와 이세돌 9단의 바둑 대결로 인공지능에 대하여 말이 많았죠.이제 곧 있으면 인공지능이 곧 인류를 지배하게 될거라는 주장도 있었죠.   ​​ ​​여하튼, 이렇게 뉴스나 인터넷에서 인공지능과 머신러닝, 딥러닝 등 자주 들어봤지만 정확한 뜻을 잘 모르시는 분들을 위하여간단히 정리한 카드뉴스로 설명을 드리겠습니다.​​  ​​​ 인공지능이란?인텔리전트한 기계를 만드는과학과 공학John McCarthy​인공지능이란 단어는 1956년 미국 다트머스 대학에서 열린회의에서 전산학자이자 인지과학자인 존 매카시 박사가 처음으로 창안한 용어입니다.​인공지능이란, 인간이 가지고 있는 인식, 판단 등의 지적 능력을 모델링하여 컴퓨터에서 구현 하기 위해 다양한 기술이나 소프트/하드웨어, 이를 포함한 컴퓨터 시스템을 통틀어 일컫는 말입니다.​  ​​​​ 인공지능의 정의​ ​​인공지능의 정의는 여러가지가 있는데, 그 중 '인공지능 - 현대적 접근 (Artificial Intelligence - A Modern Approach)에는 네 가지의 관점에서 내린 여러 정의가 나옵니다. 인간처럼 생각하는 (Thinking Humanly)New Effort to make Computers think​인간처럼 행동하는 (Acting Humanly)The art of creating machines that perform functions that require intelligence when performed by peaple.​이성적으로 생각하는 (Thinking Rationally)The study of the computations that make it possible to perceive, reason, and act.​이성적으로 행동하는 (Acting Rationally)Computational Intelligence is the study of the design of intelligent agents.인공지능 - 현대적 접근 (Artificial Intelligence - A Modern Approach) ​​​ 인공지능의 연구​현재 인공지능 연구에는 기호주의, 연결주의, 통계기반을 포함한 다양한 분야의 기법들이 융합되어 이용되고 있습니다. ​인공지능의 연구 분야는 자연어 처리(NLP), 음성인식(Speech recognition), 전문가 시스템(Expert System), 로봇, 컴퓨터 비전(Vision) 등 다양한 분야가 있으며 모두 기계가 지능적으로 움직이기 위해 필요한 기술들을 연구합니다.​  ​​ 인공지능 개념 간 관계​인공지능 ⊃ 머신러닝 ⊃ 인공신경망 ⊃ 딥러닝​인공지능이 가장 넓은 개념이고, 인공지능의 중요한 구현 방법 중 하나가 기계학습 또는 머신러닝이며, 머신러닝 중 하나의 방법론이 딥러닝입니다. 딥러닝은 또한 인공신경망의 한 종류입니다. ​즉 인공지능 ⊃ 머신러닝 ⊃ 인공신경망 ⊃ 딥러닝 순서입니다.​  ​​  ​여기까지 인공지능에 대해 간단히 알아보았습니다.​@indifrog_playground​인디프로그 인스타그램에 가장 먼저 카드뉴스들이 올라오니팔로우하시고 더 빠르게 먼저 소식 받아보세요 <3​​  ​​​추가로 더 궁금하시거나필요한 정보가 있으시다면 댓글로 요청주세요!​인프개구리가 친절하게 설명해드릴게요!​  ​​  인디프로그 Indifrog아카데미 반도체 전자기계/통신 개발보드 센서 커넥터/PCB 제어 전자공구 외국 제품 소개 외국 제품 Free Space CS CENTER 02-6053-2227 평일 오전 10:00 - 오후 06:00 토요일.일요일.공휴일 휴무 BANK INFO 240-910005-57604 하나은행 예금주 : 주식회사 인디프로그 ToF sensor 19,800원 N20 모터용 라인트레이서 바퀴 (2EA) 1,980원 바퀴브라켓 540원 옴니 바퀴 700원 USB cable 1,500원 IR Sensor Board 8,500원 iFZero Modu...www.indifrog.co.kr ​아두이노 부품들은 인디프로그 쇼핑몰에서 구매가능합니다 <3아두이노 보드들은 세트로 더 저렴하게 GET하세요 ٩(๑•̀ㅂ•́)و   #아두이노 #머신러닝 #딥러닝 #인공지능 #인공지능의정의 #인공지능이란 #개발 #개발정보 #Ai #알파고 #프로그래밍 #알고리즘 #인공지능로봇 #인공지능기계 #컴퓨터시스템 #코딩​​ "
920+ 923+ 중 결정. Geekbench 점수는 싱글 및 멀티코어에서 187% 126% 향상. GPU에 신경쓰지 않는다면 좋은 업그레이드 ,https://blog.naver.com/youseok0/222962386561,20221222,"https://www.reddit.com/r/synology/comments/zhhwir/trying_to_decide_between_920_923_the_geekbench/ Trying to decide between 920+ & 923+. The geekbench score shows a 187% and 126% improvement in single and multicore. If I don't care about GPU seems like a good upgradePosted in r/synology by u/TangibleDifference • 4 points and 7 commentswww.reddit.com 920+와 923+ 사이에서 결정하려고 합니다. Geekbench 점수는 싱글 및 멀티코어에서 187% 및 126% 향상을 보여줍니다. GPU에 신경쓰지 않는다면 좋은 업그레이드인 것 같습니다.​가제트 괴물·전 12일대부분의 사람들은 트랜스코딩에 집착하는 것처럼 보이지만 실제로 그것을 필요로 하는 사람은 거의 없습니다.​라카로사·전 11일예, 사람들은 ""4개의 코어가 항상 2개보다 낫다""와 같은 말도 안되는 말을 합니다... 실제 벤치마크를 보면 좋습니다 :)​무능한_지체자·전 11일그리고 저렴한 마이크로 폼 팩터 PC로 트랜스코딩을 수행할 수 있습니다....그렇게 화려할 필요도 없습니다. 8GB 및 Intel GPU가 있는 구형 Dell, Lenovo 및 HP 마이크로도 괜찮습니다. 구형 Mac Mini도 작동합니다.​정직성·전 11일이봐, 내가 이번 주 초에 말했을 때 나는 Plex 팬보이들에 의해 소리를 지르고 비추천표를 받았습니다. 물론 당신의 말이 100% 맞습니다.​https://browser.geekbench.com/v5/cpu/compare/18711290?baseline=18863614 Synology Inc. DS923+ vs Synology DS920+ - Geekbench BrowserSynology Inc. DS923+ vs Synology DS920+ Synology Inc. DS923+ Synology DS920+ Difference Single-Core Score 819 437 187.4% Synology Inc. DS923+ Synology DS920+ Multi-Core Score 1517 1204 126.0% Synology Inc. DS923+ Synology DS920+ Geekbench 5.4.5 Tryout Geekbench 5.4.5 Tryout System Information Synolo...browser.geekbench.com ​ Synology Inc. DS923+Synology DS920+DifferenceSingle-Core Score819437187.4%Synology Inc. DS923+ Synology DS920+ Multi-Core Score15171204126.0%Synology Inc. DS923+ Synology DS920+ Geekbench 5.4.5 TryoutGeekbench 5.4.5 Tryout System Information Synology Inc. DS923+Synology DS920+Operating SystemUbuntu 18.04.6 LTSUbuntu 22.04.1 LTSModelSynology Inc. DS923+Synology DS920+ProcessorAMD Ryzen Embedded R1600 @ 2.60 GHz1 Processor, 2 Cores, 4 ThreadsIntel Celeron J4125 @ 2.00 GHz1 Processor, 4 CoresProcessor IDAuthenticAMD Family 23 Model 24 Stepping 1GenuineIntel Family 6 Model 122 Stepping 8L1 Instruction Cache64.0 KB x 232.0 KB x 4L1 Data Cache32.0 KB x 224.0 KB x 4L2 Cache512 KB x 24.00 MB x 1L3 Cache4.00 MB x 1MotherboardAMD BilbySynology DS920+BIOSInsyde Corp. M.109.00INSYDE Corp. M.802.00Memory3.80 GB11.54 GB Single-Core Performance Synology Inc. DS923+Synology DS920+DifferenceSingle-Core Score819437187.4%Synology Inc. DS923+ Synology DS920+ AES-XTS1719596288.4%Synology Inc. DS923+ Synology DS920+ Text Compression715416171.9%Synology Inc. DS923+ Synology DS920+ Image Compression804479167.8%Synology Inc. DS923+ Synology DS920+ Navigation703456154.2%Synology Inc. DS923+ Synology DS920+ HTML5664398166.8%Synology Inc. DS923+ Synology DS920+ SQLite658489134.6%Synology Inc. DS923+ Synology DS920+ PDF Rendering746459162.5%Synology Inc. DS923+ Synology DS920+ Text Rendering614403152.4%Synology Inc. DS923+ Synology DS920+ Clang726454159.9%Synology Inc. DS923+ Synology DS920+ Camera878411213.6%Synology Inc. DS923+ Synology DS920+ N-Body Physics659281234.5%Synology Inc. DS923+ Synology DS920+ Rigid Body Physics879516170.3%Synology Inc. DS923+ Synology DS920+ Gaussian Blur935273342.5%Synology Inc. DS923+ Synology DS920+ Face Detection862449192.0%Synology Inc. DS923+ Synology DS920+ Horizon Detection765456167.8%Synology Inc. DS923+ Synology DS920+ Image Inpainting1595691230.8%Synology Inc. DS923+ Synology DS920+ HDR1520821185.1%Synology Inc. DS923+ Synology DS920+ Ray Tracing1118528211.7%Synology Inc. DS923+ Synology DS920+ Structure from Motion774350221.1%Synology Inc. DS923+ Synology DS920+ Speech Recognition622316196.8%Synology Inc. DS923+ Synology DS920+ Machine Learning570187304.8%Synology Inc. DS923+ Synology DS920+  Multi-Core Performance Synology Inc. DS923+Synology DS920+DifferenceMulti-Core Score15171204126.0%Synology Inc. DS923+ Synology DS920+ AES-XTS22321313170.0%Synology Inc. DS923+ Synology DS920+ Text Compression17071063160.6%Synology Inc. DS923+ Synology DS920+ Image Compression16941403120.7%Synology Inc. DS923+ Synology DS920+ Navigation1251940133.1%Synology Inc. DS923+ Synology DS920+ HTML51328137596.6%Synology Inc. DS923+ Synology DS920+ SQLite1417151693.5%Synology Inc. DS923+ Synology DS920+ PDF Rendering14691246117.9%Synology Inc. DS923+ Synology DS920+ Text Rendering1206127794.4%Synology Inc. DS923+ Synology DS920+ Clang15631056148.0%Synology Inc. DS923+ Synology DS920+ Camera12351153107.1%Synology Inc. DS923+ Synology DS920+ N-Body Physics1438935153.8%Synology Inc. DS923+ Synology DS920+ Rigid Body Physics20971513138.6%Synology Inc. DS923+ Synology DS920+ Gaussian Blur12331165105.8%Synology Inc. DS923+ Synology DS920+ Face Detection15601346115.9%Synology Inc. DS923+ Synology DS920+ Horizon Detection16521292127.9%Synology Inc. DS923+ Synology DS920+ Image Inpainting25561382184.9%Synology Inc. DS923+ Synology DS920+ HDR30582389128.0%Synology Inc. DS923+ Synology DS920+ Ray Tracing23771853128.3%Synology Inc. DS923+ Synology DS920+ Structure from Motion1392973143.1%Synology Inc. DS923+ Synology DS920+ Speech Recognition1093670163.1%Synology Inc. DS923+ Synology DS920+ Machine Learning689482142.9%Synology Inc. DS923+ Synology DS920+  ​ "
[M-STT] 음성 인식 시장은 어떻게 발전되었을까? ,https://blog.naver.com/masonai/222316753811,20210420,"​​자동 음성 인식 - ASR(Automatic-Speech-Recognition:이하 ASR) 시장은 지속적으로 발전해왔습니다. Siri야, 주변 맛집 검색해줘.Alexa, 오늘 날씨 어때?소비자는 매일 Siri와 google assistant, Alexa를 사용하여 질문하고, 제품을 주문하고, 음악을 재생하는 등의 일을 간단하게 말로 명령합니다.​이제는 간단한 질문을 넘어서 감정을 전달하는 Voicebots, 대화형 AI 등을 전문적으로 다루는 회사들도 많이 생겼구요.마치 저희 메이슨인텔리전스처럼 말이죠! ​오늘은 이러한 ASR 시장의 발전 과정에 대해 알아보겠습니다.​ Early Years – Hidden Markov Models and Tri-Gram ModelsASR의 역사는 1952년 간단한 숫자를 기록할 수 있는 Andrey 라는 프로그램으로 시작되었습니다. Andrey는 단순한 '자동 숫자 인식기'로, 10개의 숫자 (0~9) 만 구별할 수 있었습니다. 물론 당시에는 Andrey가 큰 혁신적인 프로그램이었다고 합니다. 그 뒤로 연구자들이 HMM(Hidden Markov Models)을 사용하기 시작하면서 ASR 연구가 더 활발해지기 시작했습니다. HMM은 확률 함수를 사용하여 전사할 올바른 단어를 결정합니다. (*최신의 ASR 모델에서는 HMM과 딥러닝을 결합한 하이브리드 시스템이거나, HMM이 없는 시스템이 주를 이루고 있습니다.) ​이러한 HMM 모델은 음소(phoneme)를 소리의 최소 단위를 결정하였으며, 가장 '일반적인' 단어일 확률 함수를 사용하여 올바른 단어를 추측해냈습니다. 하지만 기존 HMM 모델만으로는 이해하기 어려운 단어와 문장들을 해석해냈는데요. 이러한 모델에 잡음 감소 모델(upfront)과 Beam search + Language models(back-end)를 추가하게 되면서 이해할 수 있는 단어와 문장을 만들어낼 수 있게 되었습니다. ​여기서 Beam Search 모델이란, 단순히 가장 흔하게 사용하던 단어를 선택하던 모델에서 조건부 '확률'을 기반으로 각 시간 단계에서 입력 시퀀스에 대한 '여러 대안'을 선택하게 하는 모델입니다. 앞뒤에 기록된 단어들을 살펴보고 대상 단어에 가장 적합한 단어를 찾는 Sequence to sequence model이 적합한 예라고 할 수 있습니다. (*여러 대안의 수는 지정한 매개 변수에 따라 달라지며, 각 단계에서 가장 가능성(확률)이 높은 최상의 대안을 선택하게 됩니다.) ​ Sequence to sequence model (출처 : https://towardsdatascience.com/)​이러한 전체 프로세스를 ""Tri-Gram 모델""이라고 하며, 현재 사용되는 ASR 기술의 80%는 이 1970 년대 모델의 정제된 버전이라고 보시면 될 것 같습니다.​​​​ New Generation of ASR – Neural networksASR은 1980년대 인공신경망이라는 개념이 발전되면서 큰 발전을 이루게 됩니다. 이전에 설명드린 ""Tri-Gram"" 모델을 사용하던 학자들과 기업들은 Neural Networks를 도입하면서 큰 변화를 맞이하게 됩니다. 기존의 Tri-Gram 모델보다 보다 나은 사전 음성 처리, 백엔드의 텍스트 및 문장 생성이 가능해졌기 때문이죠. 때문에 간단한 음성 명령 세트를 사용하던 이전의 Siri나 Alexa보다 더 발전한 음성 챗봇이 탄생할 수 있게 되었습니다. ​ History of Speech Recognition and Hidden Markov Models​​ New Revolution in ASR – Deep Learning 빅데이터, 고성능의 하드웨어, GPU 출시 등의 기회가 주어지면서 ASR은 한 층 더 발전하여 End to End Deep Learning ASR 방식이 개발되었습니다. 이 새로운 ASR 방법은 더 많은 데이터가 신경망에 공급될 수 있게 됨에 따라 더 정확해지도록 '학습' 하고  '훈련'할 수 있는 모델입니다. 더이상 개발자가 Tri-Gram 직렬 보델의 각 부분을 다시 코딩하고 언어를 추가하고, 구문을 분석하고, 노이즈를 줄일 필요가 없어진거죠. 이러한 모델을 사용하여 비용을 늘리지 않아고도 정확성, 속도, 확장성을 얻을 수 있게 되었습니다. 50년동안 발전해온 ASR 모델을 우리는 지금 만나볼 수 있게 된 것이죠!​​​(출처 : 딥그램 블로그 https://deepgram.com/blog/the-history-of-automatic-speech-recognition/)​​​ ​​이렇게 발전해온 M-STT 엔진을 지금 바로 만나보실 수 있습니다.​음성인식 솔루션 M-STT이 궁금하신가요?​​ ​​​해당 솔루션이 궁금하시다면, 홈페이지를 통해 문의​​ 남겨주시거나 아래의 연락처로 연락 부탁드립니다.​---------------------------------------------------------------------------------------------------------------메이슨인텔리전스(주) | 02-566-7535 | masonai@masonintelligence.com--------------------------------------------------------------------------------------------------------------- "
광주중국어학원 중국유학 인공지능학과 상해(上海) 화동이공대학  ,https://blog.naver.com/lgs0821/223046582952,20230316,"중국유학화동이공대학​智能科学与技术专业教学培养方案지능과학 및 기술전문교과 육성방안 ​一。专业特色华东理工大学智能科学与技术专业依托华东理工大学优势学科，瞄准科技发展和社会需求，以自然智能（脑科学和认知科学）和智能机器人方面的研究成果为基础，以人工智能理论和方法为核心，研究如何用计算机去模拟、延伸和扩展人的智能，从而建立机器智能，培养具有扎实基础理论和创新实践能力的脑认知科学、智能制造、机器人等领域复合型专业技术人才。​1. 전문특색화동이공대학교 스마트과학과 기술 전공은 화동이공대학의 우위학과에 바탕을 두고, 과학기술 발전과 사회적 수요를 겨냥하여 자연지능(뇌 과학과 인지과학)과 지능형 로봇 방면 연구 성과를 기초로, 인공지능 이론과 방법을 핵심으로 하여, 컴퓨터로 어떻게 시뮬레이션을 하는지 연구하고, 사람의 지능을 확장하여, 기계 지능을 만들고, 기초이론과 혁신적 실천능력을 갖춘 뇌 인지과학, 스마트 제조, 로봇 등의 분야에서 복합적인 전문기술인재를 양성합니다.  二。培养目标智能科学与技术专业致力于培养具备良好的科学素质，系统地掌握智能科学与技术的基本理论与方法，具有较强的知识获取能力和创新创业能力，具有能综合运用计算机和自动化交叉知识的宽口径、复合型、创造型科技人才。毕业生能在科研院所、企事业单位及其管理部门从事机器感知与模式识别、智能信息处理与理解、知识工程、机器人与智能系统等领域的技术设计、开发和工程管理的工作。预期毕业5年后学生具有以下能力：1.能鉴定、分析、制定和解决与智能科学与技术领域相关的工程问题，适应独立和团队工程环境；2.能以法律、伦理、监管、社会、环境和经济等方面宽广的系统视角管理多学科的项目；3.能与同事、专业的客户和公众有效沟通；4.能与时俱进，使用现代化工具解决实际问题；5.能在终身学习、专业发展和领导能力上表现出担当和进步。​2. 양성목표스마트과학과 기술전공은 좋은 과학소질을 갖추는 데 주력하고 있고, 스마트과학과 기술의 기본 이론과 방법을 체계적으로 파악하여 비교적 강한 지식 획득 능력과 혁신적인 창업 능력을 갖추고, 컴퓨터와 자동화의 교차 지식을 종합적으로 운용할 수 있는 관개·복합형·창조형 과학기술 인재를 갖추도록 합니다. 졸업생들은 과학연구원소, 기업 및 그 관리 부문에서 기계감지와 모델인식, 스마트정보처리와 이해, 지식공학, 로봇과 스마트시스템 등 분야의 기술 설계, 개발과 공정관리의 업무를 할 수 있습니다. 졸업 5년 후 학생들은 다음과 같은 능력을 가질 것으로 기대: 1. 스마트과학과 기술 분야와 관련된 공정문제를 감정·분석·제정·해결할 수 있고, 독립 및 팀 엔지니어링 환경에 적응;2. 법률, 윤리, 감독, 사회, 환경과 경제 등의 넓은 시스템 시각 관리 많은 학과 프로젝트를 할 수 있음;3. 동료, 전문적인 고객과 대중과 효과적으로 소통 가능;4. 시대와 더불어 발전, 현대화 도구를 사용하여 실제 문제 해결 가능;5. 평생학습, 전문발전, 리더십에서 담당과 진보를 보여줄 수 있음. 三。毕业要求1.工程知识：能够将数学、自然科学、工程基础和专业知识用于解决智能科学与技术相关领域复杂工程问题。2.问题分析：能够应用数学、自然科学和工程科学的基本原理，识别、表达、并通过文献研究分析智能科学与技术相关领域的复杂工程问题，以获得有效结论。3.设计/开发解决方案：能够设计针对智能科学与技术相关领域复杂工程问题的解决方案，设计满足特定需求的系统、单元（部件）或工艺流程，并能够在设计环节中体现创新意识，考虑社会、健康、安全、法律、文化以及环境等因素。4.研究：能够基于科学原理并采用科学方法对智能科学与技术相关领域复杂工程问题进行研究，包括设计实验、分析与解释数据、并通过信息综合得到合理有效的结论。5.使用现代工具：能够针对智能科学与技术相关领域复杂工程问题，开发、选择与使用恰当的技术、资源、现代工程工具和信息技术工具，包括对复杂工程问题的预测与模拟，并能够理解其局限性。6.工程与社会：能够基于工程相关背景知识进行合理分析，评价专业工程实践和复杂工程问题解决方案对社会、健康、安全、法律以及文化的影响，并理解应承担的责任。7.环境和可持续发展：能够理解和评价针对复杂工程问题的专业工程实践对环境、社会可持续发展的影响。8.职业规范：具有人文社会科学素养、社会责任感，能够在工程实践中理解并遵守工程职业道德和规范，履行责任。9.个人和团队：能够在多学科背景下的团队中承担个体、团队成员以及负责人的角色。10.沟通：能够就智能科学与技术相关领域复杂工程问题与业界同行及社会公众进行有效沟通和交流，包括撰写报告和设计文稿、陈述发言、清晰表达或回应指令。并具备一定的国际视野，能够在跨文化背景下进行沟通和交流。11.项目管理：理解并掌握工程管理原理与经济决策方法，并能在多学科环境中应用。12.终身学习：具有自主学习和终身学习的意识，有不断学习和适应发展的能力。​3. 졸업요구1. 엔지니어링 지식: 수학, 자연 과학, 엔지니어링 기반과 전문 지식을 스마트과학과 기술 관련 분야의 복잡한 엔지니어링 문제를 해결하는 데 사용할 수 있음.2. 문제 분석: 응용수학, 자연 과학과 공학의 기본 원리, 식별, 표현, 문헌 연구를 통해 그마트 과학과 기술 관련 분야의 복잡한 엔지니어링 문제를 분석할 수 있고, 효과적인 결론을 얻을 수 있음.3. 설계/개발 솔루션: 스마트과학과 기술 관련 분야의 복잡한 공정문제에 대한 솔루션을 설계할 수 있으며, 특정 수요를 충족시키는 시스템, 유닛(부품) 또는 생산 공정을 설계하고, 혁신적인 환경 의식을 구현하고, 사회, 건강, 안전, 법률, 문화 및 환경 등의 요소를 고려함. 4. 연구: 과학 원리에 기초하고 과학적 방법을 사용하여 스마트과학과 기술 관련 분야의 복잡한 공정 문제에 대해 연구할 수 있으며, 실험 설계, 분석과 해석데이터, 정보종합을 통해 합리적이고 효과적인 결론을 얻을 수 있음. 5. 현대 도구 사용: 스마트과학과 기술 관련 분야의 복잡한 엔지니어링 문제에 대응하여 적절한 기술, 리소스, 현대 엔지니어링 도구와 정보 기술 도구를 개발, 선택 및 사용할 수 있으며 복잡한 엔지니어링 문제에 대한 예측과 시뮬레이션을 포함하여 한계를 이해함. 6. 엔지니어링과 사회: 엔지니어링 관련 배경 지식에 기초하여 합리적으로 분석할 수 있으며, 전문 엔지니어링 실행과 복잡한 엔지니어링 문제 해결 방안이 사회, 건강, 안전, 법률, 그리고 문화에 미치는 영향을 평가하고 담당하는 책임을 이해함. 7. 환경과 지속발전가능성: 복잡한 엔지니어링 문제에 대한 전문 엔지니어링 실천이 환경, 사회의 지속가능성에 미치는 영향을 이해하고 평가할 수 있음. 8. 직업규범: 인문사회과학 소양, 사회적 책임감을 가지고 있으며, 엔지니어링 실천에서 엔지니어링 직업윤리와 규범을 이해하고 준수하며 책임을 이행할 수 있음. 9. 개인과 단체: 다양한 학문적 배경의 단체에서 개인, 단체 구성원 및 담당자 역할을 맡을 수 있음. 10. 커뮤니케이션: 스마트 과학과 기술 관련 분야의 복잡한 엔지니어링 문제에 대해 업계 동업자 및 사회 대중과 효과적으로 커뮤니케이션할 수 있으며, 보고서 작성과 원고설계를 포함한 프레젠테이션, 명확한 표현 또는 명령어에 응답한다. 게다가 국제적 시야를 갖춰, 다문화적 배경에서 소통하고 교류할 수 있음. 11. 프로젝트 관리: 엔지니어링 관리 원리와 경제 의사 결정 방법을 이해하고 파악하며, 이를 다양한 학문적 배경 환경에서 활용할 수 있음. 12. 평생학습: 자기주도 학습과 평생학습의식을 가지고 끊임없이 배우고 적응하는 능력이 있음.   四。学位及学分要求本专业学生在学期间必须修满专业培养方案规定的160学分，其中，通识教育平台课程39学分，学科基础教育平台课程26学分，专业教育平台课程55.5学分，实践教育平台33.5学分，个性化任选课程6学分。上述学分数分布完全达到或超过中国工程教育专业认证标准，即：数学自然%=26/160=16.3%；工程基础专业%=55.5/160=34.7%；工程实践%=33.5/160=20.9%；人文%=25/160=15.6%。学生修满学分并达到《大学生体质健康标准》，可获得毕业证书。获准毕业并符合国家学位授予条例，且通过华东理工大学《大学英语》水平考试者，可获得工学学士学位。​4. 학위 및 학점 요구사항본 학과 학생은 재학 중 전문 육성 방안에서 정한 160학점을 이수해야 하며, 이 중 통식교육 플랫폼 과정 39학점, 학과 기초 교육 플랫폼 과정 26학점, 전문 교육 플랫폼 과정 55.5학점, 실천 교육 플랫폼 33.5학점, 개성화 선택 과정 6학점입니다. 상술한 학점 분포는 중국 공학 교육 전문 인증 표준을 완전히 충족하거나 초과합니다. :수학 자연 %=26/160=16.3% 공사 기초 전공 %=55.5/160=34.7% 공사 실천 %=33.5/160=20.9% 인문 %=25/160=15.6%. 학생들은 학점을 이수 <대학생 체질건강기준>에 도달 시 졸업장을 받습니다. 졸업을 허가받고 국가학위 수여 조례에 부합하며, 화동이공대 <대학영어> 수준 시험자를 통해 공학 학사 학위를 받을 수 있습니다.  五。课程设置1．通识教育课程（39学分）（1）通识教育必修课程（29学分）  课程编号课程名称课程英文名称考核方式学分学时开课学期11272012思想道德修养和法律基础Ideological and Moral Cultivation and Legal Basis考试364113927012中国近现代史纲要Outline of Modern and Contemporary History of China考试364213923010毛泽东思想和中国特色社会主义理论体系(上)The Introduction to Mao Zedong Thought and Theoretical System of Socialism with Chinese Characteristics I考试2.548313924010毛泽东思想和中国特色社会主义理论体系(下)The Introduction to MaoZedong Thought and Theoretical System of Socialism with Chinese Characteristics II考试2.548411265012马克思主义基本原理概论Generality of Marxism Basic Principles考试364313166001形势与政策Situation & Policy考查2321-811034004军事理论Military Education考查136212427004体育(1)Physical Education (1)考查132112428004体育(2)Physical Education (2)考查132212429004体育(3)Physical Education (3)考查132312430004体育(4)Physical Education (4)考查132413913008大学英语ⅠCollege English I考试232113914008大学英语ⅡCollege English II考试232213916008大学英语ⅢCollege English III考试232313917000大学英语 IVCollege English IV考试032413915000大学计算机基础Fundamentals of Computer考试040111339004中国文化导论中国文化类（三选一）An Introduction of Chinese Culture考试116413926004中国文化概论（MOOC）A Sketch of Chinese Culture考查116113925004国学智慧(MOOC)Traditional Chinese Wisdom考查116212738004创业基础创新创业类（二选一）A Step into the Business World考试116113931004大学生创业基础(MOOC)A Business Course for University Students考查1162 5. 교육과정 설정1. 통식교육과정(39학점)(1) 통식교육필수과정(29학점) 교과번호교과명칭교과영문명칭심사방식학점시수개강학기11272012사상 도덕 수양과 법률적 기초Ideological and Moral Cultivation and Legal Basis시험364113927012중국 근현대사 요강Outline of Modern and Contemporary History of China시험364213923010모택동 사상과 중국 특색 사회주의 이론 체계(상)The Introduction to Mao Zedong Thought and Theoretical System of Socialism with Chinese Characteristics I시험2.548313924010모택동 사상과 중국 특색 사회주의 이론 체계(하)The Introduction to MaoZedong Thought and Theoretical System of Socialism with Chinese Characteristics II시험2.548411265012마르크스주의 기본 원리 개론Generality of Marxism Basic Principles시험364313166001정세와 정책Situation & Policy검사2321-811034004군사이론Military Education검사136212427004체육(1)Physical Education (1)검사132112428004체육(2)Physical Education (2)검사132212429004체육(3)Physical Education (3)검사132312430004체육(4)Physical Education (4)검사132413913008대학영어ⅠCollege English I시험232113914008대학영어ⅡCollege English II시험232213916008대학영어ⅢCollege English III시험232313917000대학영어 IVCollege English IV시험032413915000대학컴퓨터기초Fundamentals of Computer시험040111339004중국문화해설중국문화류（세과목중 택1）An Introduction of Chinese Culture시험116413926004중국문화개론（MOOC）A Sketch of Chinese Culture검사116113925004국학지혜(MOOC)Traditional Chinese Wisdom검사116212738004창업기초창의성창업류（두과목증택1）A Step into the Business World시험116113931004대학생창업기초(MOOC)A Business Course for University Students검사1162  （2）通识教育选修课程（10学分）通识教育选修课程设置五个类别：Ⅰ.人文科学类、Ⅱ.社会科学类、Ⅲ.工程技术类、Ⅳ.自然科学类、Ⅴ.创新创业类。各专业学生须选修10学分，其中在前四个类别分别选修不少于2学分，本专业学生需选读至少1学分管理类课程。​(2) 통식교육선택과정(10학점)통식교육 선택과목설치 5개 분류: I.인문과학류, II.사회과학류, III.공학기술류, IV.자연과학류, V.창조창업류. 각 전공 학생은 선택10학점을 이수해야 하고, 이 중 상위 4개 과목은 선택2학점보다 적지 않으며, 본 전공 학생은 최소 1학점 관리 과목을 이수해야 합니다. （3）通识教育专项课程（学分不计入培养方案总学分） 课程编号课程名称课程英文名称考核方式学分学时学期校区课程性质 大学生职业规划与管理Career Development and Planning考试116春、秋奉贤必修13164004新生心理健康教育The Education of Freshman's Psychological Health考查116春、秋奉贤必修13176004职场训练营Workplace Training Camp考试116春奉贤选修13174004学习心理学Psychology考查116春奉贤选修13175004压力管理Stress Management考查116春奉贤选修13158004情绪管理Emotion Management考查116春奉贤选修13159004人格认识与发展Personality Cognition and Development考查116春奉贤选修13162004心理科学与社会生活Psychology &Social Life 考查116春奉贤选修13155004成长小组Growth group考查116春奉贤选修13177004自信心训练Self-Confidence Training考查116春奉贤选修 社会实践Social Practice考查0.254春、秋奉贤必修 志愿服务Voluntary Service考查0.254春、秋奉贤必修 综合类讲座Lecture考查1.524春、秋奉贤必修 艺术修养提升Fine Art考查0.258春、秋奉贤选修 学生领导力培养Leadership Training考查0.258春、秋奉贤选修 文化素质提升Culture Attainment考查0.258春、秋奉贤选修 团队沟通与交往Team Communication考查0.258春、秋奉贤选修 (3) 통식교육전문과정(학점은 양성방안 총학점에 들어가지 않음) 교과번호교과명칭교과영문명칭심사방식학점시수학기캠퍼스교과구분 대학생 직업계획와 관리Career Development and Planning시험116봄,가을奉贤필수13164004신입생 심리 건강 교육The Education of Freshman's Psychological Health검사116봄,가을奉贤필수13176004직장 훈련 캠프Workplace Training Camp시험116봄奉贤선택13174004학습심리학Psychology검사116봄奉贤선택13175004스트레스관리Stress Management검사116봄奉贤선택13158004정서관리Emotion Management검사116봄奉贤선택13159004인격인식과발전Personality Cognition and Development검사116봄奉贤선택13162004심리 과학과 사회 생활Psychology &Social Life 검사116봄奉贤선택13155004성장그룹Growth group검사116봄奉贤선택13177004자신감훈련Self-Confidence Training검사116봄奉贤선택 사회적실천Social Practice검사0.254봄,가을奉贤필수 자원봉사Voluntary Service검사0.254봄,가을奉贤필수 종합 강좌Lecture검사1.524봄,가을奉贤필수 예술수양향상Fine Art검사0.258봄,가을奉贤선택 학생 리더십 양성Leadership Training검사0.258봄,가을奉贤선택 문화소질향상Culture Attainment검사0.258봄,가을奉贤선택 팀커뮤니케이션과 교제Team Communication검사0.258봄,가을奉贤선택  2．学科基础教育课程（26学分）信息学科必修基础课程：要求修满26学分 课程编号课程名称课程英文名称考核方式学分学时开课学期11065024高等数学A（上）Advanced Calculus I考试696+24111069020高等数学A（下）Advanced Calculus II考试580+24211126012线性代数Linear Algebra考试348211058012概率论与数理统计Probability and Statistics考试348411143012大学物理A（上）University Physics A I考试348+16211145016大学物理A（下）University Physics A II考试464+24311147004大学物理实验（上）Physical Experiments of University I考查130311148004大学物理实验（下）Physical Experiments of University II考查1304 2. 학과기초교육과정(26학점)정보학과 필수 기초과정 : 이수학점 26학점 교과번호교과명칭교과영문명칭심사방식학점시수개강학기11065024고등수학A（상）Advanced Calculus I시험696+24111069020고등수학A（하）Advanced Calculus II시험580+24211126012선형대수학Linear Algebra시험348211058012확률론과 수리 통계Probability and Statistics시험348411143012대학물리A（상）University Physics A I시험348+16211145016대학물리A（하）University Physics A II시험464+24311147004대학물리실험（상）Physical Experiments ofUniversity I검사130311148004대학물리실험（하）Physical Experiments ofUniversity II검사1304  3．专业教育课程（89学分）（1）专业必修课（41学分） 课程编号课程名称课程英文名称考核方式学分学时开课学期12912012计算机程序设计Computer Programming考试364112915010计算机导论Introduction to Intelligent Science and Technology考查2.540113966004计算机类前沿讲座New Trends in CS考查116217029012*脑与认知科学基础Fundamentals of Brain and Cognitive Science考试348312976014*算法与数据结构Algorithm and Data Structures考试3.564313995014数字电路与逻辑设计Digital Circuits and Logic Design 考试3.564312931014计算机组成原理Principles of Computer Organization考试3.564417028016*人工智能数学基础Mathematical Basis of Artificial Intelligence考试4645 17027008*人工智能基础与应用Fundamentals and Applications of Artificial Intelligence考试232517026008机器人学导论Introduction to Intelligent Robot考试240513147016*自动控制原理Principles of Automatic Control 考试464517023016*模式识别与统计学习Pattern Recognition and Statistical Learning考试472613142008*智能控制Intelligent Control考试232617021012*神经网络与深度学习Neural Networks and Deep Learning考试3566 3. 전공교육과정(89학점)(1) 전공필수과목(41학점) 교과번호교과명칭교과영문명칭심사방식학점시수개강학기12912012컴퓨터 프로그래밍Computer Programming시험364112915010컴퓨터해설Introduction to Intelligent Science and Technology검사2.540113966004컴퓨터첨단강좌New Trends in CS검사116217029012*뇌와 인지과학 기초Fundamentals of Brain and Cognitive Science시험348312976014*알고리즘과 데이터 구조Algorithm and Data Structures시험3.564313995014디지털 회로와 로직 설계Digital Circuits and Logic Design 시험3.564312931014컴퓨터구성원리Principles of Computer Organization시험3.564417028016*인공지능수학기초Mathematical Basis of Artificial Intelligence시험4645 17027008*인공지능기초와 응용Fundamentals and Applications of Artificial Intelligence시험232517026008로봇학해설Introduction to Intelligent Robot시험240513147016*자동제어원리Principles of Automatic Control 시험464517023016*패턴식별과 통계학습Pattern Recognition and Statistical Learning시험472613142008*지능 제어Intelligent Control시험232617021012*신경네트워크와 심화학습Neural Networks and Deep Learning시험3566  （2）专业选修课（要求修满14.5学分） 课程编号课程名称课程英文名称考核方式学分学时开课学期14439008建模与分析Modeling and Analysis考查232312882008Python程序设计Python Programming考查240312880012Java程序设计Java Programming考查356413153008最优化方法Optimization Method考查232412911012计算方法Computational Method考查356417022008智能感知与检测技术Intelligent Perception and Detection Technology考查240514421008数据挖掘Data Mining考查232513003008自然语言处理Natural Language Processing考查232512896010大数据与云计算Big Data and Cloud Computing考查2.548513120008无线传感器网络Wireless Sensor Networks考查232517020012数字信号处理Digital Signal Processing考查352512823008音频信号处理Audio Signal Processing考查232513145004专业外语English Essays in AI考查116612814008物联网技术及应用Internet of Things Technology and Application考查240617019008语音识别Speech Recognition考查240617025010嵌入式系统与分析Embedded system and analysis考查2.548617018008数字图像处理DigitalImage Processing考查240617024008知识工程与知识系统Knowledge Engineering and Knowledge Systems考查232617017008智能制造Intelligent Manufacturing考查232612920010计算机视觉Computer Vision考查2.5486 (2) 전공선택과목(이수학점 14.5학점) 교과번호교과명칭교과영문명칭심사방식학점시수개강학기14439008모델링과 분석Modeling and Analysis검사232312882008Python프로그래밍Python Programming검사240312880012Java프로그래밍Java Programming검사356413153008최적화 방법Optimization Method검사232412911012계산법Computational Method검사356417022008스마트 감지와 측정 기술Intelligent Perception and Detection Technology검사240514421008데이터 캐싱Data Mining검사232513003008자연 언어 처리Natural Language Processing검사232512896010빅 데이터와 클라우드Big Data and Cloud Computing검사2.548513120008무선 센서 네트워크Wireless Sensor Networks검사232517020012디지털 신호 처리Digital Signal Processing검사352512823008오디오 신호 처리Audio Signal Processing검사232513145004전공 외국어English Essays in AI검사116612814008사물인터넷 기술 및 응용Internet of Things Technology and Application검사240617019008음성 인식Speech Recognition검사240617025010임베디드 시스템과 분석Embedded system and analysis검사2.548617018008디지털 이미지 처리DigitalImage Processing검사240617024008지식 엔지니어링과 지식 시스템Knowledge Engineering and Knowledge Systems검사232617017008스마트 제조Intelligent Manufacturing검사232612920010컴퓨터 시각Computer Vision검사2.5486   （3）实践环节（33.5学分）①实践教学环节：要求修满32.5学分 课程编号课程名称课程英文名称考核方式学分周数开课学期13957004军训Military Training考查12.5114826008计算机职业实践Basic Engineering Skills Training考查22115110008程序设计综合实践Programming Practice考查22212774004电子技术课程设计Course Design in Electronic Technology考查11413110004认识实习Cognition Practice考查11 417016010神经网络课程设计Neural NetworksCourse Design考查2.52.5717015012智能制造课程设计Intelligent ManufacturingCourse Design考查33713012012毕业实习Graduation Practice考查33717014008机器学习课程设计Machine Learning Course Design考查22717013012智能无人系统课程设计Robot Programming and Practice考查33717012008脑与认知科学课程设计Brain and Cognitive Science Course Design考查22717032004写作与表达Writing and Expression考查11716417036毕业论文（设计）Graduation Project考查91878 (3) 실천과정(33.5학점)실천교과과정 : 이수학점 32.5학점 교과번호교과명칭교과영문명칭심사방식학점시수개강학기13957004군사 훈련Military Training검사12.5114826008컴퓨터 직업 실천Basic Engineering Skills Training검사22115110008프로그래밍 통합실천Programming Practice검사22212774004전자 기술 과정 설계Course Design in Electronic Technology검사11413110004인식 실습Cognition Practice검사11417016010신경네트워크 수업설계Neural NetworksCourse Design검사2.52.5717015012스마트 제조 과정 설계Intelligent ManufacturingCourse Design검사33713012012졸업 실습Graduation Practice검사33717014008기계 학습 과정 설계Machine Learning Course Design검사22717013012지능형 무인 시스템 커리큘럼 설계Robot Programming and Practice검사33717012008뇌와 인지과학 과정 디자인Brain and Cognitive Science Course Design검사22717032004작문과 표현Writing and Expression검사11716417036졸업논문（설계）Graduation Project검사91878  ②创新实践及社会实践环节：要求修满1学分 创新实践活动实践活动名称实践活动英文名称学分开课学期USRP或课余科研、创新活动USRP, Extracurricular Scientific Research, Innovation Activities1-3学分分散进行校内外竞赛活动Competition Activities开放实践（实验、竞赛）平台活动Practice Activities on Experiment or Competition Platform 论文与专利Papers and Patents经教务处认定的计划外社会实践Social Practice approved by Academic Affairs Office0.5-1学分分散进行 혁신실천 및 사회실천 과정 : 이수학점 1학점 혁신실천활동실천 활동 명칭실천활동 영문명학점개강학기USRP 또는 과외 과학 연구, 혁신 활동USRP, Extracurricular Scientific Research, Innovation Activities1-3학점분산 진행교내외경진대회Competition Activities개방 실천（실험, 콘테스트）플랫폼 활동Practice Activities on Experiment or Competition Platform 논문과 특허Papers and Patents교무처가 인정하는 계획외의 사회적 실천Social Practice approved by Academic Affairs Office0.5-1학점분산 진행   4．个性化任选课程（6学分）根据兴趣，在全校范围内选课，除本专业培养方案要求学分之外的所有学分均可计入。​4. 개성화선택과정(6학점)관심에 따라 전교 범위 내에서 수강 선택한 경우, 본 전공 양성 방안에서 요구하는 학점을 제외한 모든 학점을 인정합니다.   六。课程设置与毕业要求的关系矩阵各门课程的教学目标与学生能力达成的相关度如下表所示。相关度表示符号为：H-高度相关，M-中等相关，L-弱相关，不相关则不必填写。​​自动化专业毕业要求与必修课程的对应关系矩阵  毕业要求课程名称工程知识问题分析设计/开发解决方案研究使用现代工具工程与社会环境和可持续发展职业规范个人和团队沟通项目管理终身学习思想道德修养和法律基础     H H    中国近现代史纲要       H    军事理论       M    毛泽东思想和中国特色社会主义理论体系概论       H    中国文化类限选课     H H    马克思主义基本原理概论       H    形势与政策      HH HH 体育        M  M大学英语        HH  创业基础类限选课        H HM高等数学AH           线性代数H           概率论与数理统计H           大学物理AH           大学物理实验M           *计算机类程序设计H   H       计算机导论  H HH     H计算机类前沿讲座     HHH HH 计算机组成原理H           *脑与认知科学基础H  M        *人工智能数学基础H  M        数字电路与逻辑设计H  H        算法与数据结构 H H        人工智能基础及应用  MM        *自动化控制原理   H        机器人学导论HMHM        *模式识别与统计学习 HHH        智能控制 MMH        *神经网络与深度学习 HHHM       军事训练        H   实践教学环节 HHHHH MHHHH 注：1、H-高度相关；M-中等相关；L-弱相关；2、课程名称前加“*”者为该专业核心课程。​6. 과정 설정과 졸업 요구사항과의 관계표각 과목의 교과 목표와 학생 능력 달성의 상관도는 아래 표에 나와 있습니다. 관련도 표시 기호: H-높은정도, M-중등, L-약, 관련되지 않으면 작성하지 않아도 됩니다. 자동화 전공 졸업 요구사항과 필수 커리큘럼의 대응 관계표 졸업요구   교과명칭엔지니어링 지식문제분석설계/개발 솔루션연구현대 도구 사용엔지니어링과 사회환경과 지속가능성직업 규범개인과 단체소통프로젝트 관리종신학습사상 도덕 수양과 법률적 기초     H H    중국 근현대사 요강       H    군사 이론       M    모택동 사상과 중국 특색 사회주의 이론 체계 개론       H    중국문화류 선택과목     H H    마르크스주의 기본 원리 개론       H    정세와 정책      HH HH 체육        M  M대학영어        HH  창업기초류 선택과목        H HM고등수학AH           선형대수학H           확률론과 수리 통계H           대학물리AH           대학물리실험M           *컴퓨터류 프로그래밍H   H       컴퓨터해설  H HH     H컴퓨터류 첨단강좌     HHH HH 컴퓨터 구성 원리H           *뇌와 인지과학 기초H  M        *인공지능 수학 기초H  M        디지털 회로 및 로직 설계H  H        알고리즘과 데이터 구조 H H        인공지능 기초 및 응용  MM        *자동화 제어 원리   H        로봇학 해설HMHM        *패턴 식별과 통계 학습 HHH        지능형 제어 MMH        *신경 네트워크와 심화학습 HHHM       군사훈련        H   실천 교과 과정 HHHHH MHHHH  비고: 1, H-높은정도; M-중 ; L-약 ; 2, 과정 이름 앞에 ""*""를 추가한 것은 이 전공의 핵심 과정임. 附一：选修课程修读指导 课程平台课程类别要求学分课程类别按学期选修学分分配（建议）12345678通识平台公选10通识教育选修课程设置五个类别：Ⅰ.人文科学类、Ⅱ.社会科学类、Ⅲ.工程技术类、Ⅳ.自然科学类、Ⅴ.创新创业类。2-42-422    说明：公共选修课原则上在1-4学期内修读完成。各专业学生须在Ⅰ、Ⅱ、Ⅲ、Ⅳ四个类别分别选修不少于2学分，本专业学生需选读至少1学分管理类课程。专业平台专业选修14.5专业选修模块课程  2-353.5-43.5-4  说明：要求学生在列表中选修14.5学分专业选修课程。个性化平台任选6个性化任选课程   222  说明：要求学生修满6学分个性化任选课程，任选课程不受平台限制，在全校范围内选课，除本专业培养方案各课程平台要求的学分之外的学分均可计入。 부1: 선택과목 이수지도 교과플랫폼교과분류요구학점교과분류학기 선택 학점에 따라 배정（건의）12345678통식 플랫폼공통선택10 통식교육 선택과목5개분류:I.인문과학류,II.사회과학류,III.공정기술류,IV.자연과학류,V.창조창업류2-42-422    설명: 공공 선택 과목은 원칙적으로 1-4학기 내에 이수하여 완성합니다. 각 전공 학생은 I, II, III, IV의 네 가지 카테고리에서 각각 2학점 이상을 이수해야 하며, 본 전공 학생은 최소 1학점 관리 과목을 이수해야 합니다.전공플랫폼전공선택14.5전공선택 모듈 과정  2-353.5-43.5-4  설명: 학생들에게 리스트에서 14.5 학점의 전공 선택 과목을 선택하도록 합니다.개성화플랫폼임의선택6개성화 선택과정   222  설명: 학생들에게 6학점짜리 개성화 선택과정을 이수하도록 하고, 선택과정은 플랫폼에 제한받지 않고 전교 범위 내에서 선택하며, 본 전공 양성 방안의 각 과정 플랫폼에서 요구하는 학점을 제외한 학점은 모두 인정합니다.   附二：学期学时学分分配表 学期12345678总学分21.7521.2523.7514.2512.259.2516.759.25理论学分15.7517.2519.259.2511.758.250.250.25实践学分644.550.5116.59 注：1、本表统计数据不包括公共选修课、专业选修课及创新实践；2、实践学分包含实验、上机、课内、课外实践等。 부2: 학기 수업시간 학점배분표 학기12345678총 학점21.7521.2523.7514.2512.259.2516.759.25이론학점15.7517.2519.259.2511.758.250.250.25실천학점644.550.5116.59 비고: 1. 본 표의 통계 데이터에는 공공 선택 과목, 전공 선택 과목 및 혁신 실천이 포함되지 않음;2. 실천학점은 실험, 비행기 탑승, 수업 내, 과외실천 등을 포함 함.  执笔人：堵威审核人：唐漾批准人：杜文莉집필인: 堵威 심사인: 唐漾 승인인: 杜文莉​ 광주중국어학원고차이나 중국어학원은중국전문인력 양성에 필요한 교육을 위하여 오늘도 끊임없이 노력하고 + 연구합니다.​GO China 加油 ！​ 광주중국어학원​#고차이나 #중국정부장학생 #광주중국어학원 #중국유학 #광주HSK학원 "
[딥러닝을 이용한 자연어 처리 입문] 3. 언어 모델(Language Model) ,https://blog.naver.com/bchaeeun9/223005136216,20230204,"# 01. 언어 모델(Language Model)이란?: 단어 시퀀스(문장)에 확률을 할당하는 모델​언어 모델을 만드는 방법     1.  통계 이용     2.  인공 신경망 이용​​1. 언어 모델(Language Model)​단어 시퀀스에 확률을 할당하기 위한 방법:       이전 단어들이 주어졌을 때 다음 단어 예측하도록 함​​2.  단어 시퀀스의 확률 할당​      1.   기계 번역(Machine Translation)      2.   오타 교정(Spell Correction)      3.  음성 인식(Speech Recognition)​​3. 주어진 이전 단어들로부터 다음 단어 예측하기      1. 단어 시퀀스의 확률: n개의 단어가 등장하는 단어 시퀀스 W의 확률 𝑃(𝑊) = 𝑃 (𝑤1, 𝑤2, 𝑤3, 𝑤4, 𝑤5, ..., 𝑤𝑛)​     2. 다음 단어 등장 확률: n-1개의 단어가 나열된 상태중 n번째 단어의 확률 𝑃(𝑤𝑛|𝑤1, ..., 𝑤𝑛−1)​​4. 언어 모델의 간단한 직관 ​ 앞에 어떤 단어들이 나왔는지 고려하여 후보 단어들의 등장 확률 추정하고 그 중 가장 높은 확률의 단어 선택​​5. 검색 엔진에서의 언어 모델의 예  인터넷에서의 자동완성 기능​​​​# 02. 통계적 언어 모델(Statistical Language Model, SLM)​​1. 조건부 확률​조건부 확률의 연쇄 법칙(chain rule) : 𝑃(𝑥1 , 𝑥2 , 𝑥3 ...𝑥𝑛) = 𝑃 (𝑥1 )𝑃(𝑥2 |𝑥1 )𝑃(𝑥3 |𝑥1 , 𝑥2 )...𝑃(𝑥𝑛|𝑥1 ...𝑥𝑛−1)​​2. 문장에 대한 확률​'문맥'을 통해 이전 단어의 영향을 받은 단어들로 문장이 완성𝑃 (An adorable little boy is spreading smiles) = 𝑃 (An)×𝑃(adorable|An)×𝑃 (little|An adorable)×𝑃 (boy|An adorable little)×𝑃 (is|An adorable little boy) ×𝑃 (spreading|An adorable little boy is) × 𝑃 (smiles|An adorable little boy is spreading)​​3. 카운트 기반의 접근​SLM: 카운트에 기반하여 확률 계산𝑃(is|An adorable little boy) = count(An adorable little boy is) count(An adorable little boy)​​4. 카운트 기반 접근의 한계-희소문제(Sparsity Problem)​충분한 데이터를 관측하지 못해 언어를 정확히 모델링하지 못하는 문제      --> 완화 방법: n-gram 언어 모델, 일반화(generalization) 기법​​​# 03. N-gram 언어 모델(N-gram Language Model)n의 의미: 이전에 등장한 단어들 중 일부 단어를 몇 개 보는지​​1. 코퍼스에서 카운트하지 못하는 경우의 감소​SLM의 한계: 코퍼스에서 확률을 계산하고 싶은 문장이나 단어가 없을 수도 있음--> 참고 단어 수를 줄이면 해당 단어의 시퀀스를 카운트할할 가능성 높일 수 있음​​2. N-gram    : n개의 연속적인 단어 나열n=1: unigramn=2: bigramn=3: trigramn=4: 4-gram​​3. N-gram Language Model의 한계​     1. 희소 문제(Sparsity Problem)     2. n을 선택하는 것은 trade-off 문제          - n을 작게 선택: 훈련 코퍼스에서 카운트는 잘 됨, but 근사의 정확도는 현실의 확률분포와 멀어짐          - n을 크게 선택: 실제 훈련 코퍼스에서 해당 n-gram을 카운트할 수 있는 확률 적어져 희소 문제 심각해짐       => 최대 5를 넘지 않게 적절한 n 선택​​4. 적용 분야(Domain)에 맞는 코퍼스의 수집​훈련에 사용된 도메인 코퍼스에 따라 성능이 달라짐​​5. 인공 신경망을 이용한 언어 모델(Neural Network Based Language Model)​N-gram 언어 모델의 한계점을 완전히 해결하지는 못함     --> 성능이 우수한 인공 신경망을 이용한 언어 모델이 많이 사용됨​​​# 04. 한국어에서의 언어 모델(Language Model for Korean Sentences)​1. 한국어는 어순이 중요하지 않다단어 순서가 바뀌어도 의미 전달이 가능하기 때문에 다음 단어 예측 어려움​​2. 한국어는 교착어이다한국어는 '조사'가 존재하기 때문에 '토큰화'를 통해 접사나 조사 등을 분리하는 것이 중요​​3. 한국어는 띄어쓰기가 제대로 지켜지지 않는다​​​# 05. 펄플렉서티(Perplexity, PPL)모델: 모델 내에서 본인의 성능을 수치화하여 테스트 데이터에 대해 빠르게 결과를 내놓는 모델 평가 지표​​1. 언어 모델의 평가 방법(Evaluation metric): PPL수치가 '낮을수록' 언어 모델 성능이 좋음문장의 길이로 정규화된 문장 확률의 역수​​2. 분기 계수(Branching factor)특정 시점에서 선택할 수 있는 가능한 경우의 수​​3. 기존 언어 모델 vs 인공 신경망을 이용한 언어 모델인공신경망을 이용한 언어 모델이 성능이 좋음 "
머신러닝 인간 말소리 인지 개선 보조 ,https://blog.naver.com/onaear/222909814825,20221025,"March 2, 2022 HHTM​ 워싱턴, DC - 난청은 급격하게 성장하는 과학 연구 영역이고, 난청을 다루는 베이비붐 세대의 수가 나이가 들면서 계속 늘어납니다.​난청이 사람에게 영향을 주는 영향을 이해하려고, 연구자는 사람이 말을 인식하는 능력을 연구합니다. 인간은 반향, 청력 손실, 교통 소음 혹은 다중 화자가 같이 상당한 배경 소음이 있는 경우 말을 이해하기 더 어렵습니다.​결과적으로, 종종 보청기 알고리즘이 인간 어음 인식을 개선하는 데 사용됩니다. 이와 같은 알고리즘을 평가하기 위해서, 연구자는 특정한 단어의 개수(일반적으로 50%)를 인식하는 신호 대 잡음 비를 결정하기 위한 실험을 수행합니다. 이 검사는 여하튼 시간과 비용이 많이 듭니다.​ 인간 어음 인식 모델 개요. 이미지 호의:  Jana Roßbach​미국음향학회가 AIP 출판사를 통해 출판하는 미국음향학회 저널에 게재된 연구에서, 독일 연구자는 기계학습과 심층신경망에 기반한 인간 어음 인식 모델을 조사합니다.​우리 모델의 참신함은 매우 다른 복잡성과 소음 형태에 대해서 청각-장애 청취자의 좋은 예측을 제공하며, 측정 데이터에 낮은 오류 및 높은 상관관계를 보여줍니다.-칼 폰 오시에츠키 대학교 Jana Roßbach​연구자는 자동 어음 인식(ASR)을 사용하여 문장당 얼마나 많은 단어를 청취자가 이해하는지를 계산했습니다. 대부분은 알렉사, 시리와 비슷한 어음 인식 도구를 통해 ASR에 익숙합니다.​연구는 어음을 차폐하는 다양하고 복잡한 소음에 노출된 8명 정상 청력자 및 20명 청각-장애 청취자로 구성합니다. 청각-장애 청취자는 여러 노화-관련 난청 수준으로 3가지 그룹으로 분류했습니다.​이 모델은 실제 어음과 시간 변조 및 유사성으로 복잡성이 증가하는 다양한 소음 차폐자로 다양한 난청 청도로 청각-손상된 청취자의 인간 어음 인식 능력을 예측하게 합니다. 개인의 청력 손실 가능성은 개별적으로 고려될 수 있습니다.​ 우리는 모든 소음 유형에 예측이 잘 동작함에 가장 놀랐습니다. 우리는 단일 경쟁 화자를 사용할 때 모델이 문제를 가질 것으로 예상했습니다. 하지만, 해당하지 않았습니다.-Jana  Roßbach이 모델은 한쪽 귀 청취의 경우에 대한 예측을 만들었습니다. 앞으로, 연구자는 말의 이해가 두 귀의 청취로 영향을 받기 때문에 양이 모델을 개발할 수 있습니다.​말 명료도 예측에 추가하여, 이 모델은 청취 노력 혹은 음성 품질에 매우 관련이 있는 청취 노력 혹은 음성 품질 예상에도 잠재적으로 사용될 수 있습니다.​Link to article: A model of speech recognition for hearing-impaired listeners based on deep learningDOI: 10.1121/10.0009411  https://aip.scitation.org/doi/full/10.1121/10.0009411​Source: ASA​ ​ "
자막과 효율적 음성인식 셀프어텐션의 역할 MBC의 '바이든' 또는 '날리믄' ft. 타임지 인기동영상 Brainstorm or Green Needle ,https://blog.naver.com/egeyouri/222937306058,20221124,"올해 가장 유명한 AI학회 ICLR에 음성인식 관련 논문을 발표했다. 사람의 말은 매우 부정확한데, 비유를 들자면 사람이 말하는 것과 글자 쓰는 것이 비슷하다. 둘 다 두뇌가 근육에 명령을 보내서 이루어지는 일이다.​사람은 듣는 과정에서 앞뒤로 비슷한 발음을 계속 찾는다. 이것이 제 논문에 나오는 self attention 의 뜻이다. 짧은 소리에서는 self-attention을 이용하기 어렵다. 짧은 소리를 제대로 듣기 어려운 이유다. ​-약 4년 전 미국 인기 동영상. 마음 속으로 Green Needle 단어를 생각한 후 이 노이즈를 들어보면 Green Needle처럼 들린다. Brainstorm이란 단어를 떠올린 후 똑같은 노이즈를 들어보면 이번에는 Brainstorm처럼 들린다. ​-외교관계에서 어떻게 접근해야 국익이 되는가에 촛점을 맞추고 토론거리를 제공하는 게 언론의 의무이다. 바이든이냐 날리믄이냐 스캔달 부풀리기로 희화화시켜 국민이 오십번 백번 듣게 만드는 건 언론의 정도가 아니다.​-사람들이 voice와 sound가 어떻게 다른지에 대해서 무지하고, voice는 물론  sound조차도 귀가 듣는 것이 아니라 최종적으로는 각종 데이터와 맥락까지 종합하여 머리 즉 뇌가 듣는다는 걸 알지 못하더라. 영상도 결국 눈이 아니라 뇌가 본다는 것도 마찬가지로 모르더라. 성원용 MBC는 아직도 뉴욕에서 대통령 발언에 '바이든'이라 엉터리 자막을 단것에 대해 고집을 피웁니다. 페친 중에도 아직 '바이든'이라 들린다고 하는 사람이 있습니다. 당신 전공이 음성인식이냐고 시비거는 사람도 있습니다. ​제 전공은 AI를 이용한 음성인식이 맞습니다. 올해 가장 유명한 AI학회 ICLR에 논문을 발표했는데, 아래 제목입니다. 제목에 음성인식이 들어있습니다. ​""Understanding the role of self attention for efficient speech recognition""(효율적 음성인식에서 셀프-어텐션의 역할에 대한 이해)​사람의 말은 매우 부정확한데, 비유를 들자면 사람이 말하는 것과 글자 쓰는 것이 비슷합니다. ​말하기나 글자쓰기나 두뇌가 근육에 명령을 보내서 이루어지는 일입니다. 필기체 글은 인쇄체에 비해 엄청 혼동이 됩니다. ​아래 그림을 보면, 뭐라고 읽히는가요?  ​아마 대부분 90-3190 이라고 읽겠지요. 그런데 이것이 휴대폰 전화번호라면 어떻게 되겠습니까? ​바로 휴대폰 전화번호라는 것이 선행정보(prior information)입니다. 모두 010-3190으로 읽을 것입니다. ​그리고 첫글자가 9가 아닌 또 하나의 이유가 있습니다. 뒤의 3190의 9자와 다르지요. ​사람은 듣는 과정에서 앞뒤로 비슷한 발음을 계속 찾습니다. 이것이 제 논문에 나오는 self attention 의 뜻입니다. ​짧은 소리에서는 self-attention을 이용하기 어렵지요. 짧은 소리를 제대로 듣기 어려운 이유입니다. ​아무튼 MBC가 한 일은 아래의 글자에 90-3190 이라고 자막을 붙여 내 보낸 것이라 할 수 있습니다. ​제대로 된 언론이라면, 오히려 전화번호라는 설명을 붙였어야 합니다.​댓글1- 아주 적확한 비유입니다!- 시끄러운 클럽에서 여자 꼬실 때 서로 대화가 가능한 이유가 셀프어텐션 이거군요. 짦은 말은 알아듣기 힘들어 제스처를 섞어서 하고요.​- MBC는 결국 그들이 가진 선행정보가 왜곡할려는 목적으로 이미 ORIENTED 되어 있었다는 증거이네요.​​  4년 전 타임지에도 다뤘던 인기 동영상댓글러​사람은 자기가 갖고 있던 생각이 자기가 듣는 것을 이해하는데 도움을 주는 걸로 보입니다.​약 4년 전에 미국에서 인기 있던 동영상인데, ​노이즈 같은 소리가 나는데 마음 속으로 Green Needle이라는 단어를 생각한 후 이 노이즈를 들어보면  Green Needle처럼 들립니다. ​그런데 대신 Brainstorm이라는 단어를 떠올린 후 똑같은 노이즈를 들어보면 이번에는 Brainstorm처럼 들립니다. ​당시에 이 영상이 인기가 있다 보니 타임지도 기사로 낼 정도였더군요.​-전 아무리 브레인스톰이라고 생각하고 들어도, 도저히 그렇게는 안들리네요^^- 정말 신기하네요!- 꼭 브레인스톰이었다가 또 그린니들로 들리네요. 정말 놀랍습니다.- 먼저 생각하고 들으면, 정말 확실히 그 단어로 들리는군요.​​ 제가 어쩌다 언론노조 MBC본부와 관련하여 소위 “성문분석”이란 걸 좀 검증하고 관련 의견을 두루 들어볼 기회가 있었습니다. 댓글러​그런데, 웃기지만 웃을 수 없는 게, 사람들이 voice와 sound가 어떻게 다른지에 대해서 너무 무지하고, ​voice는 물론, sound조차도 귀가 듣는 것이 아니라 최종적으로는  각종 데이터와 맥락까지 종합하여 머리 즉 뇌가 듣는다는 걸 아예 모르더군요. 영상도 결국 눈이 아니라 뇌가 본다는 것도 마찬가지로 모르고요. ​그냥 voice wave를 수리적으로 분석하면 text로 바로 변환할 수 있고, 지문대조 하듯이 화자 판별이 바로 되는 걸로 알더군요. 그러니, 배 모 교수같은 엉터리 sound전문가가 매스컴에서 몇 번 추켜세우니까 ​자기 전공도 아닌 voice전문가 행세를 하는 걸 아무도 지적을 안 하는 것도 당연하죠. 그 당시 경험으로는, 성문분석에 국내전문가는 없거나, 있어도 위험감수를 안 하려고 하더군요. ​그래서, 이 분야는 어쩜 ‘블루오션’일 수도 있겠다 생각했는데, 제가 미처 교수님이 계신 줄은 몰랐습니다. 앞으로 많은 가르침 받을 수 있었으면 합니다. ​아참, 이 건의 최초 발단은 소위 ‘볼펜’ 기자가 아닌 ‘카메라’기자가 파파라치식의 몰래카메라 내용을 자신이 들었다고 생각한 것을 ‘볼펜’기자에게 제보하면서 시작이 되었다고 합니다. ​그러니 그때부터 이미 일종의 ‘확증편향’이 개입한 거죠. 혹시 참고가 되실지… ​(그 카메라 기자는 보도직종에서 일하기에는 문제가 많아서 회사 차원의 조치가 필요하다고 보고했는데, ​정권이 바뀌고도 계속 보도직종에 있으면서 심지어 대통령순방 동행이라는 중요업무까지 맡김으로서 문제를 만들기 시작한 겁니다.) 인간의 지각과정의 주관성에 대한 연구는 성원용 교수처럼 과학적 연구로 입증되지만 이는 철학 종교적으로도 매우 자주 거론되는 주제이다. 댓글러​한마디로 인간의 지각작용은 매우 불완전하며 긍정적이든 부정적이든 주관적일 수밖에 없다. ​불교에서도 사신불매라고 인간의 보고 듣고 냄새맡고 맛보고하는 안이비설이란 몸의 신하들 유혹에 넘어가지 말라고 하였다.​의식활동으로 설명하면 외부세계의 감각소여(sense data )가 지각과정을 거쳐 의식속의qualia로 정립되는 과정에서 감각소여는 필연적으로 다소간 왜곡과정을 겪는다. ​에피큐로스는 신에 대한 인식과정을 설명하면서 두번 째 단계로 시각prolepsis 단계를 설정하는데 이는 일단 감각소여가 기초 인식자료가 되기는 하지만 ​이 재료에 질서를 부여할 인식틀은 이미 선험적으로 인간 개별주체에게 부여되어 있다.​이를 릴르가드는 예상도식체계라고 한다. 여기서 시각은 김지하의 개념이다. 시인은 최제우의 법어,"" 거울이 만리를 비춤에 눈동자가 먼저 깨닫네 "" 를 설명하면서 ​""사물 운동을 대상으로서 인식하기 이전에 이미 그 운동의 핵심적 형상을 예감하는 기능이 눈동자 즉 통각 이전에 살아 움직이며 "" 그 기능이 시각이라는 것이다.​거창하게 설명하지 않아도 우리 일상 체험에서도 적절한 예를 들 수 있다. ​어릴 적 습기로 얼룩진 벽지 무늬에서 백설공주 속 여왕이 변신한 긴 매부리코의 마귀할멈 얼굴을 본 적이 있을 것이다. ​지각할 때 눈은 혼돈된 외부대상의 형상에 질서와 의미를 부여하는 방향으로 관찰한다. ​그러한 의식적 패턴화는 외부대상의 형상을 새로 재조립하여 의미 있고 인지가능한 형상으로 변조하는 것이다.​ 바로 이것이 인간의 지각과정의 허구이다. 그리하여 심지어 화장실 비데에 앉아 작동소리를 들으면 ​어떤 땐 왜그래그래로 다음날은 모아서 씻고씻고 등 기계음이 신통하게도 의미는 없지만 그럭저럭 올바른 문장이 되는 발성을 한다. (나만 그런가? )​교회 단식 철야 기도 환각 환청 체험도 기적 체험일 수 있지만 상당부분 육체적으로 굶주림과 수면부족으로 ​극도로 예민한 가운데 외부 바람소리도 인간의 언어로 의미가 통하는 조리 있는 발언으로 변조되고 그것이 천사 또는 악마의 외침으로 명확히 지각되는 것이다. ​그리하여 계몽주의 철학자들은 기적체험에 엄격한 이성이란 잣대를 적용하여 진짜와 가짜를 판별해야 한다고 주장한다.​어쨌든 인간은 사신불매를 해야 하며 그로 빚은 모든 해프닝은 해프닝으로 끝나야 한다. 그것보다 중요한 건 ​미국과의 외교관계에서 무엇이 문제이며 어떻게 접근해야 국익에 도움이 되는가에 언론은 촛점을 맞추고 국민들에게 토론거리를 제공하는게 의무이다. ​바이든이냐 말리면이냐 등 스캔달 부풀리기로 외교문제를 희화화시키고 또 국민들은 먹고 살기도 바쁜데 그걸 오십번 들었니 백번 듣게 만드는 건 언론의 정도가 아니다.​( 나는 물론 한번도 듣지 않았다. 전술한 바대로 그 기자가 그런 단어로 듣는 것은 당연하고 이미 인지철학적으로도 인정되었기 때문에 )​​​댓글2. - 010과 90으로 읽을 수 있지만 저걸 70이라고 말한다고 그렇게 읽지는 않지요. 적절하지 않은 비유입니다. 날리면과 바이든이 음성학적으로 사람이 착각할 만한 유사성이 있다는 것을 보여 주고 ​그런 발음을 MBC가 유도했다고 보여 줘야 제대로 된 설명이지요. 자막으로 써주면 그 단어가 바보면으로도 들리나요?​-요점은 매우 잡음이 많은 소리입니다. 자막에 따라 그렇게 들려요. 그것이 사람입니다.​-사람의 감각의 한계가 많음은 저도 잘 알고 있습니다. 마녀와 미녀 그림을 그 예로 들지만 그 그림이 남자로 보이지는 않지요. 그 소리를 바이든으로 듣는 사람이 두 배로 많은 것이 자막 탓이라고 주장 하시지만 ​저는 그 설명이 납득이 잘 되지 않는군요. 잡음이 많은 게 문제라면 또 다른 소리로도 들을 수 있어야 겠지요. 제게는 사람의 감각의 한계를 핑계되는 것으로 보입니다.​-내가 전에 설명은 했는데, 모음은 자음에 비해 에너지가 크기 때문에 잡음 상황에서 그나마 들리지요. ​바이든, 날리면, 앞의 두 syllable의 모음이 같지요. 그런데 이렇게 앞의 두 모음이 같은 다른 단어도 있는데 바이든으로 들리는 이유는 자막 때문입니다.​- 세 글자에 아모음이 처음에 나오는 단어는 다 가능하다는 이야기 인가요? ​받침과 둘째 글자의 ㄹㄹ이 전혀 안 들릴 수 있다는 거네요. 비슷한 예로 갈리면, 팔리면도 바이든으로 들릴 수 있나요?​- 의사이시니까 확률 배우셨을 것 같은데 Bay's theorem으로 설명할 수 있지요. 실험의 evidence 즉 likelihood가 강력할 때는 prior information이 큰 역할을 못해요. ​그런데 지금은 매우 잡음이 많기 때문에 evidence가 매우 약한 상황인 것이지요. 이 때 사람들은 prior information에 의지합니다.​- 사람이 이전 정보에 의존한다는 점은 100% 공감합니다. 그렇지만 제 말은 이전 정보를 이용하는 방식에도 그럴 만한 패턴이 있을 테고 ​상당수 사람들은 다른 이들이 가이드 하는 대로 그냥 따라 가지 않습니다. 말씀하시는 대로라면 날리면이라는 주장도 받아들일 근거가 없는 것이지요.​- 그렇지요 날리면이라고 주장하는 것은 아닙니다. 단지 바이든은 아니라는 입장입니다. 유엔에서 기부약속을 한 것이니까 국회에서 날리면 이라고 걱정을 하지 않겠나 하는 추측 정도입니다.​- 날리면도 확실하지 않다는 주장이시면 저도 OK 입니다.​-마음에 안 드는 놈에게 불리한 쪽으로 들리는 것, 확증 편향의 강력함을 새삼 느끼게 됩니다. 큰 그림을 보기보다는 지엽말단적 이슈거리에 집착하며 집단 이지메에 참여하여 동지들 간에 끈끈한 유대감을 느끼고 피아구분을 하며 만족하는 사람들.​- 엠비시 방송의 자막을 보기 이전에도 바이든이라고 듣게 된 사람들도 있습니다. 자막 없는 상태에서 '날리면' 으로 듣게 될 가능성이 얼마나 될까요?​- 방송은 불분명한 것을 한쪽으로 답을 주면 안되지요. 나는 날리면을 주장하지 않았어요. 윤대통령이 ""국회에서 이 새끼들이""를 말했다 주장한다면 context상은 날리면이지요.​​​ "
[파이썬] 인공지능 스피커 만들기 : STT(소리를 텍스트로) 활용방법2(wav 파일 소리 입력) ,https://blog.naver.com/themail0/223026270492,20230224,"파이썬 프로그램이 있는 py폴드 내에  ""sample.wav""파일을 준비하고 아래 파이썬 코딩을 실행하면, 파일에 녹음된 음성내용이 콘솔에 텍스트로 표시됨.​#STT_파이썬(wav파일을_텍스트로)_코딩하기, #SpeechRecognition_설치방법, # PyAudio_설치방법, #파이썬_인공지능_스피커_만들기​import speech_recognition as sr #1)에러시 주석참조r = sr.Recognizer()​​# wav 파일에서 읽기, (wav : 가능, mp3 : 불가)with sr.AudioFile(""sample.wav"") as source:     audio = r.record(source) #2) 에러시 주석참조​​# 음성을 텍스트로 변환하기try:text = r.recognize_google(audio, language='ko-KR')print(""인식된 문장: "" + text)except sr.UnknownValueError:print(""음성을 이해할 수 없습니다."")except sr.RequestError as e:print(""Google Speech Recognition 서비스에 접속할 수 없습니다.; {0}"".format(e))​''' #주석#1) 에러 => 명령 프롬프트에서 STT 설치C:\Users\a\AppData\Roaming\Python\Python310\Scripts>pip install SpeechRecognition#2) 에러 => 명령 프롬프트에서 PyAudio 설치C:\Users\a\AppData\Roaming\Python\Python310\Scripts>pip install PyAudio  "
[안양/의왕/과천 보청기]어음청력검사의 중요성 ,https://blog.naver.com/the_betterlife/222897585498,20221011,"안녕하세요. 난청인들이 '편안하게 소리를 듣고 소통할 수 있도록' 증거 기반의 전문적인 지식을 바탕으로 높은 수준의 청각 서비스 및 청능 재활에 최선을 다하는 포낙보청기 안양인덕원센터 권인종청각연구센터입니다.​보청기센터에서 최초 상담시 또는 보청기 착용 후 청력이 변화된 것으로 판단하는 경우 청력검사를 실시합니다.청력검사에는 순음청력검사와 어음청력검사가 있습니다.이 글에서는 어음청력검사의 종류, 방법, 목적에 대하여 알아보겠습니다.​어음청력검사의 정의어음청력검사는 기본적인 청력 평가의 일부이다. 어음청력검사는 대상자(보청기 착용자)가 특정 유형의 언어 자극을 얼마나 잘 듣고 이해할 수 있는지 평가하기 위해 청능사가 사용하는 방법이다.(Kramer & Brown, 2019)​즉, 표준화된 절차를 통해 대상자의 어음을 인지하는 능력을 정량화하는 것입니다.​어음청력검사의 목적1) 순음청력 역치와의 비교 및 검증2) 양 귀 간의 어음 인지 능력 비교3) 보청기의 착용/미착용 간 어음 인지 능력 비교4) 다양한 보청기 및 셋팅 후 어음 인지 능력 비교5) 보청기 착용 기간에 따른 어음 인지의 변화 비교 및 모니터링6) 보청기 또는 인공와우 대상자가 맞는지 판단하기 위해(출처 : 한림국제대학원대학교 청각학전공 '행동청능평가' 교재 발췌)​어음청력검사의 종류, 방법어음청력검사에는 어음인지역치(SRT) 검사, 어음탐지역치(SDT) 검사, 단어인지도(WRS) 검사, 문장인지도(SRS) 검사 등이 있으며, 이 중 WRS검사와 SRS검사는 피검사자의 청력 상태와 대화의 인지 정도 평가 및 보청기의 효과를 예측, 평가하는 데 매우 중요한 검사합니다.​1) 단어인지도(word recognition score, WRS) 검사일상생활에서 친숙한 단음절 단어로 표준화한 단어 목록(KS-MWL)을 사용하며, 피검자(대상자)가 한 글자의 단어를 듣고 따라 말한 점수를 평가합니다. 헤드폰을 착용한 상태에서 양 귀를 각각 검사하며, 대상자가 소리를 편하게 들을 수 있는 충분히 큰 소리에서 어느 정도 단어 또는 대화를 알아들을 수 있는지를 평가하여 보청기의 착용 효과를 예측할 수 있습니다. 또한 일반적인 대화 소리 크기에서 어느 정도 들을 수 있는지를 평가하여 일상생활에서의 어려움을 확인할 수도 있고, 보청기 착용 전후의 점수를 비교하여 보청기의 효과를 검증할 수도 있습니다.검사기에 녹음된 CD를 이용하여 검사를 하는 것이 좋습니다. 검사자가 육성으로 단어를 제시할 때도 있으나, 이 경우 검사자의 억양이나 목 상태에 따라 소리의 크기가 달라지거나 음의 높낮이가 달라질 수 있기 때문에 검사자의 주의를 요합니다.​2) 문장인지도(sentence recognition score, SRS) 검사​일상생활에서 친숙하게 사용되는 문장 목록(KS-SL)을 사용하며, 대상자가 문장을 듣고 따라 말한 문장 내 단어의 개수를 평가하여 일상생활에서의 대화를 얼만큼 이해할 수 있는지 측정할 수 있습니다.방음실에서 평가를 할 수도 있으나 대상자의 피로도가 높아질 수 있기 때문에 상담실에서 실시할 수도 있습니다. 이때는 검사시마다의 검사 조건을 동일하게 해야 합니다(검사자의 목소리 크기, 소리의 방향, 소음 여부 등을 동일하게 해야 함).양 귀를 각각 검사할 수도 있고, 동시에 검사를 할 수도 있습니다.보청기의 피팅시 SRS 검사를 계속하면서 어음을 가장 잘 이해할 수 있는 최적의 이득값을 찾는데 활용할 수 있습니다.​3) 어음인지역치(speech recognition threshold, SRT) 검사표준화된 목록(KS-BWL)의 이음절어를 소리 크기를 조절하며 제시하여, 피검자가 따라 말할 수 있는 가장 작은 소리 크기 레벨을 찾습니다.순음청력검사의 역치가 신뢰도가 있는지 확인하기 위하여 실시할 수 있으며, 반드시 실시해야 하는 검사는 아닙니다.숙련된 검사자는 순음청력검사시에 검사 결과의 신뢰도를 판단할 수 있으나, 대상자가 검사 방법을 이해하지 못하여 잘못 반응하거나, 위난청(실제로는 잘 들리지만 안들리는 것처럼 거짓으로 대답을 하는 것)이 의심될 때 실시할 수 있습니다.SRT로는 난청의 종류와 형태는 알 수가 없으며, 단어 및 문장인지도 검사의 기초자료로는 사용할 수 있습니다.​4) 어음탐지역치(speech detection threshold, SDT) 검사SRT와 동일하게 이음절어를 제시하여 피검자가 인지하는 가장 작은 소리를 찾은 것이지만, SRT는 피검자가 들은 단어를 따라 말하되 틀리게 말하면 역치로 인정하지 않으나, SDT는 들었다고 생각하여 버튼을 누르면 역치로 인정할 수 있습니다. 대상자가 말을 할지 못하는 상황이거나 고심도 난청의 유소아에게 사용할 수 있습니다.보통은 SRT 역치보다 5~10 dB 정도 좋게 나타납니다. ​5) 소음하 문장인지도 검사다양한 소음하에서 문장인지도를 검사하는 것으로, 검사시에 소음을 같이 제시하는 것 이외에는 2)의 문장인지도 검사와 동일한 방법으로 실시합니다.실 생활에서는 조용한 상황도 있지만, 도로 소음, 식당이나 강의실 등에서의 소음, 공사장 소음 등 매우 다양한 소음 상황이 있습니다. 이러한 소음들이 있을 때 대화가 어느정도 가능한지를 평가할 수 있습니다.​​어음청력검사는 보청기 착용자(착용 예정자)의 대화 이해도를 평가하는데 매우 중요한 검사입니다.순음청력검사로 얼마나 작은 소리를 듣을 수 있는지는 평가할 수 있지만, 소리를 듣는 것과 대화를 이해하는 것에는 매우 큰 차이가 있습니다.소리는 해석이 필요한 소리(주로 대화 소리, 안내 방송 등 말소리)와 해석이 불필요한 소리(도로 소음, 새 소리, 바람 소리 등)가 있습니다. ​난청이 있는 많은 분들이 주로 이야기 하시는게, '소리는 들리는데 무슨 얘기인지 모르겠어'입니다.청력 역치는 아주 나쁘지 않지만 단어 또는 문장인지도가 낮으신 분들이 있고, 청력 역치는 많이 나쁘지만 대화를 잘 이해하시는 분들이 있습니다.내이나 청신경 또는 대뇌의 청각피질의 손상 및 퇴화 정도에 따라 매우 다른 결과를 나타냅니다.​보청기의 착용 효과를 제대로 보기 위해서는 '대화 이해도'에 중점을 두고 보청기를 피팅해야 하며, 이때 어음청력검사의 결과가 매우 큰 영향을 미칠 수 있습니다.또한, 보청기의 적응에 따른 대화 이해도 개선을 고려할 때도 어음청력검사는 매우 중요합니다.​보청기는 어느 곳에서 구입하시더라도 제품은 똑같지만, 보청기 센터의 피팅 능력에 따라 효과는 전혀 다릅니다. 그만큼 얼마나 제대로된 적합을 하는지가 매우 중요합니다.​포낙보청기 안양인덕원센터 권인종청각연구센터에서는 기본을 충실히 지키며 말소리를 확실히 들으실 수 있는 전문적인 보청기 적합 서비스를 제공하고 있습니다.차별화된 서비스를 경험하시기를 희망하신다면 포낙보청기 안양인덕원센터를 방문해주십시오. 포낙보청기 안양인덕원센터경기도 안양시 동안구 흥안대로 524 삼우프라자 2층 201호 ​​​ "
인간 마케터의 감(感)을 데이터로 입증하는 인공지능 마케터  ,https://blog.naver.com/wonsim01/221513947165,20190415,"알파고 기사가 이미 인간을 뛰어 넘은 지 오래이고 2014년 LA 지진이 일어났을 때 인간보다 먼저 신속하게 보도 자료를 쓴 것이 인공지능 기자이며 왓슨의 암진단이 인간의사 보다 더 정확하다는 사실은 이미 잘 알려진 일이다. ​인공지능 기사, 인공지능 변호사, 인공지능 기자, 인공지능 의사, 인공지능 운전사, 인공지능 교사... 도대체 인공지능은 어디까지 그 능력이 확대될까? 최근에는 글쓴이의 영역인 인공지능 마케터까지 등장한다.​   조선대병원의 대장암 진료 모습 (사진 조선대병원)​이런 인공지능이 이제는 마케팅에서도 그 효율성을 인정받으며 인간 마케터의 능력을 뛰어넘으려 하고 있다. 인공지능 마케터는 어떻게 마케팅을 하는가? 인공지능 마케팅은 고객 데이터와 인공 지능 개념(머신러닝)을 활용해 고객의 미래 구매 행동을 예측하고 고객 행태 패턴을 리드 하려는 것이다. 따라서 시장과 고객의 빅 데이터를 과학적으로 분석하는 솔루션으로 시장과 타깃 고객에 대한 명확한 그림을 그리는 것이다. ​부언 하자면 마케팅 믹스의 기본인 4P(product, price, place, promotion)를 데이터 과학(인공지능)을 통해 정확하게 하겠다는 것이다. 시대가 바뀌어 시장이 공급자 위주(supply chain)에서 소비자 관점(demand chain)으로 되면서 4C(customer, cost, convenience, communication) 고객의 요구나 새로운 상품에 기대치 예측, 매출을 늘리는 방편, 고객의 관심을 사로잡는 방편, 고객 만족도를 높이는 방편 등등을 빅데이터를 통해 정확하게 분석하고 예측하는 것이 인공지능 마케터의 역할이다.​그렇다면 빅데이터의 데이터란 무엇인가? 데이터의 사전 의미는 ‘정량적 또는 정량적 변수의 값 집합’이다. 데이터 처리는 또 무엇인가? Data processing이라고 표기하는데 ‘데이터’를 ‘정보나 지식’으로 바꾸는 컴퓨터상의 처리를 말한다. 데이터 처리는 일반적으로 컴퓨터에서 자동으로 실행되는데 이런 데이터 처리 시스템을 통해 비로소 빅데이터가 되는 것이다. ​그런데 여기서 하나 더 알아야 할 것이 있다. 위의 사전적 의미에서 보았듯이 데이터란 ‘정량적’이라는 것이다. ‘비정량적’ 데이터는 데이터가 아닌, 마케팅적 논리에서는 그 설득적 논리로 정성적인 것 보다는 정량적으로 접근하는 것이 합리적이고 과학적 방법론이라고 배워왔다. 특히 마케팅을 하는 사람들이라면 예외 없이 정량적 수치 분석에 집착한다. 새로운 사업계획서에도 전체 타깃 인구가 얼마인데 그 중에 몇 %만 차지해도 성공이라느니, 전체 시장 중 몇 %를 점유할 것이고 매출액은 얼마냐 등등… 그러나 인공지능 마케팅에서는 이 비정량적, 감성적 데이터까지도 분석해 낸다. 기존의 빅데이터의 진일보라고 할까? ​빅데이터를 제대로 활용하기 위해서는 자원, 기술, 인력이라는 핵심적인 3대 요소를 갖추어야만 한다. 여기서 자원이라 함은 빅데이터를 활용하기 위한 자원, 즉 데이터의 수집, 관리‧처리와 이를 활용할 수 있는 기본 전략이다. 기술은 이미 예상하시듯이 데이터 프로세스에 필요한 플랫폼, 데이터 저장과 관리 기술 그리고 분석에 필요한 시각화(Visualization)기술이다. 마지막으로 데이터를 처리하는데 있어 가장 중요한 사람이다. 비정량화, 감성적 데이터도 처리하기 위해 숨겨진 정보를 찾아내는 데이터 과학자(Data Scientist)가 전략을 제시하고, 구축하고, 분석, 활용까지 전 과정을 지휘한다. 데이터의 인사이트를 찾아내는 것이다. ​이 데이터 과학자(Data Scientist)의 역할까지 - 비 정량적, 감성적인 면 - 이제는 인공지능이 대신한다는 것이다. 인공지능 마케터는 Performance(생산성 향상), Personalization(개인화)와 Preference(선호도), Prediction(예측), Privacy(개인정보)까지 머신러닝, 딥러닝을 통해 하는 것이다. ​기업의 목적은 이윤 창출(매출 증대, 고객 확대)이다. 빅데이터는 목적을 달성을 할 수 있도록 실시간으로 데이터도 제공해주고 가치있는 데이터(합목적적인 정보)를 통해 고객과 시장의 니즈에 즉시 반응하는 비즈니스 성과를 만들 수 있다.​그리고 빅데이터는 고객과의 관계를 좀더 Personalization(개인화, 저는 특정화 또는 특성화라고 표현하고 싶다)와 개인별로 맞춤화된 1:1 Preference(선호도)를 제공할 수 있다. 실제로 글쓴이가 경험한 모 오픈마켓은 저의 평소 쇼핑 이력을 추적해 좋아 할 수 있는 연관 상품이나 고 상품을 제공해 크로스 세일링(연관 상품 판매)과 업 세일링(더 고가치의 상품 판매)을 실행하고 있어 놀라웠다. 누가 이런 개인별 맞춤형 정보들 주는데 싫어할까. 좀더 대우받고 가치를 느끼게 하기에 충분하다.​이처럼 이런 가치있는 정보들을 마케터들은 끊임없이 만들어 낼 수 있어야 하며 경쟁자들 보다도 신속하게 창출할 수 있어야 미래에 생존할 수 있다. 마케터들은 인공지능 머신러닝 을 통해 속도와 규모로 개인화된 경험을 제공받아 마케팅을 진화할 수 있다. 인공지능 마케터는 고객에게서 얻은 기존의 데이터(히스토리)를 분석해 신속하고 효과적으로 대응할 수 있는 개별 고객 맞춤형 데이터를 제품과 구매 경험을 제공하는 최적으로 쉽게 제시해 준다. 앞의 글쓴이에게 제공한 데이터처럼.​인공지능 마케팅은 자율신경계와 똑 같다. 심장, 호흡기관처럼 서로 연결돼 생명 유지에 필수적이지만 무의식적으로 작동하는 것처럼 마케팅도 이런 방향으로 발전하게 될 것이다. 잘 짜여진 프로그램이 긴밀하게 연관되고 각각의 마케팅적 요소가 무의식적으로 자율 반응(automatic response)을 하도록 하는 것이 인공지능 마케터이다. ​AMA(American Marketing Association)는 IBM 왓슨에서 파생된 이퀄스 3의 루시와 협력해 AMA가 지난 80년 동안 진행해온 모든 연구를 활용하기 위한 수단, 즉 회원들이 기본적인 마케팅 전략 질문에 답을 할 수 있도록 인공지능을 할용한다. ​“루시는 질문을 근거로 기사를 검색하며 패턴을 인식하고 좀 더 관련성 있는 정보를 검색하기 시작합니다. 우리는 루시가 올바른 방향으로 나아가고 있는지 답을 합니다. 루시가 제대로 작동하고 있다면 얼마나 훌륭하게 제 역할을 하고 있는지 알려줍니다. 그러면 루시는 그 피드백으로부터 무언가를 배웁니다.” (‘인공지능 마케팅’, 저자 짐 스턴, 역자 김현정, 한빛미디어간, 186~187쪽)    이렇듯 인공지능 마케팅은 데이터 수집 프로세스와 분석, 마케팅 조사를 통해 찾아낸 결과를 전달하는 활동에 인공지능을 가장 효과적으로 접목하는 방법이다. 머신러닝을 통해 예측에 가장 도움이 되는 속성을 찾아내고 예측에 도움이 되지 않는 속성들은 무시하고 인간의 추측(感)이 아니라 실제 데이터를 토대로 결정을 한다. 따라서 이제 마케터들은 어느 때보다도 타깃 고객에 대한 명확한 그림을 그릴 수 있게 되었다. 인공지능 마케팅을 간단히 말하자면 고객 데이터(빅데이터)와 인공지능 개념(머신 러닝)을 활용해 고객이 하려는 다음 행동을 예측하고 고객 여정(customer journey)을 개선하는 방법이다. 따라서 인공지능 마케팅 솔루션은 데이터 과학과 실행 간의 갭을 해소할 수 있는 방법을 제공한다. 데이터 과학을 실현하기 위해 거대한 데이터 덤프를 분석하고 그 과정을 이제까지는 구현이 불가능 했었지만 기술의 발전으로 지금은 쉽게 실현 가능해 졌다. ​인공지능 마케팅을 구현하는 3대 핵심 요소가 있다. 빅데이터, 머신 러닝, 강력한 솔루션이 그것인데 빅데이터와 머신 러닝은 익숙한 개념이지만 강력한 솔루션(Powerful Solutions)은 낯선 개념이다. ​강력한 솔루션이라 함은 기계적이 아니고 인간적인 솔루션을 말하는 것이다. 즉 사람처럼 감정과 의사 소통을 해석하고 소셜 미디어, 자연어, 전자 메일 응답과 같은 개방형 컨텐츠를 이해하는 플랫폼이라고 할 수 있다. 즉 인공지능 마케팅 솔루션은 인간이하는 것과 같은 방식으로 진정으로 이해한다. 플랫폼은 거대한 데이터 세트에서 통찰력있는 개념과 주제를 엄청나게 빠르게 식별해 낼 수 있는 것이다. ​그렇다면 우리는 인공지능 마케팅으로 무엇을 얻을 수 있을까? 최근 PwC 연구에 따르면 마케터의 72%가 인공지능이 비즈니스 이점이 있다고 응답했다고 한다. 앞으로 몇 달, 몇 년 안에 아래아 같은 점에서 마케팅 성과에 영향을 미칠 것으로 기대했다. ​1) 더 지능적인 검색첨단 기술 솔루션이 더욱 스마트하게 성장함에 따라 청중이 더욱 스마트 해지고 있음을 알아야 한다. 소셜 미디어와 신속한 검색 엔진 덕분에 사람들은 이전보다 더 빠르게 원하는 것을 찾는다. 인공지능과 빅 데이터 솔루션은 실제로 이러한 검색 패턴을 분석하고 마케터가 집중해야 하는 주요 영역(마케팅 카워드 등)을 파악할 수 있도록 도와준다.2) 더 스마트한 광고마케터들은 계정 기반 마케팅 솔루션을 사용해 이미 더 스마트한 광고를 하기 위해 몰두하고 있지만 인공지능은 마케팅 팀이 진정으로 통찰력있는 분석을 할 수 있도록 이 계층을 더 활용하게 도와준다. 새로운 데이터가 풍부해 짐에 따라 온라인 광고가 더욱 스마트해지고 효과적이 될 수 있다. 인공지능 솔루션은 키워드 검색, 소셜 프로필과 기타 온라인 데이터를 사람 수준의 결과를 얻을 수 있도록 심화해 준다. 3) 세련된 콘텐츠 전달인공지능을 사용하면 마케팅 담당자는 데이터와 타깃팅을 완전히 새로운 차원으로 끌어 올릴 수 있다. 잠재 고객 분석은 일반적인 인구 통계 수준을 넘어서고 개별적으로 접근할 수도 있다. 따라서 이제 마케터는 인공지능을 사용해 잠재 고객이나 구매자를 파악하고 고객과 가장 관련있는 이상적인 콘텐츠를 제공 할 수 있게 된다. 4) 봇(Bots)에 의존하기고객 서비스와 유지는 인공지능이 앞으로 큰 역할을 할 또 다른 영역이다. 고객과의 채팅 기능과 기타 직접 소비자 대응 통로를 인공지능 로봇이 해 준다.(chatbot) 많은 기업들이 챗봇으로 시간과 비용을 절약 할 수 있다. 인공지능 로봇은 가치가있는 데이터, 정보와 검색 기록에 액세스 할 수 있으므로 인간보다 훨씬 효율적이다.5) 지속적인 학습숨겨진 통찰력(insights)을 찾아내는데 인공지능을 사용할 수 있을 뿐만 아니라 실제로 학습을 통해 이전에 밝혀진 통찰력을 새 캠페인에 통합하여 가장 관련있는 사용자를 대상으로 확장하는데 최적화 할 수 있다. 시간이 지남에 따라 이러한 인공 지능 솔루션은 더욱 지능화되고 낭비를 효과적으로 제거하며 전환율을 크게 늘리고 실시간 의사 결정을 촉진한다.그렇다면 현업에서는 인공지능 마케팅(빅데이터, 머신 러닝, 파워 솔루션)을 어떤 분야 어떻게 활용하고 있을까? 실제로 삼성전자는 소셜 리스닝(Social listening)과 감성적 분석분야에서 인공지능을 활용하고 있다. 삼성전자 유럽법인에서 Crimson Hexagon의 AI 기반 시청자 통계 플랫폼을 사용해 제품에 대한 소셜 리스닝과 감성적 분석 데이터에 접근하고 있다. ​소셜 미디어의 세계는 광대하고 브랜드에 잠재적으로 유용한 대화로 가득하다. 구입한 브랜드에 대해 리뷰, 제품 통찰력과 콘텐츠까지 자유롭게 언급한다. 소셜 모니터링 도구라고도하는 소셜 리스닝 도구를 통해 소셜 네트워크(블로그, 포럼과 같은 기타 관련 사이트)에서 키워드를 검색한다. 해시 태그, 브랜드 이름, 경쟁 업체 이름 또는 마케팅 캠페인과 관련된 모든 것을 얻을 수 있다. 여기에서 언급된 정서(긍정적 또는 중립)를 모니터링 할 수 있는 기능을 갖추고 있으며 주요 주제와 주요 영향 요인과 같은 실행 가능한 통찰력을 끌어낼 수도 있다.​   (인공지능이 트윗에서 수집한 갤럭시 S8 관련 데이터 예)​Crimson Hexagon의 AI 통찰력 플랫폼은 글로벌 브랜드가 방대한 양(1.2 조 이상의 소셜 게시물을 보유)의 잠재 고객 데이터(소셜 데이터, 공개 데이터 및 기업 소유 데이터)에 액세스하고 분석 할 수 있도록 설계되었는데 캠페인 분석에서 브랜드 분석, 시장 동향 분석, 경쟁 동향 분석 등의 모든 작업을 수행한다.​마지막으로 업계에서 현재 활용하고 있는 인공지능 마케팅의 대표적인 분야를 소개한다. 1. 제품/ 콘텐츠 추천(Product/content recommendations)2. 데이터 필터링 및 분석(Data filtering & analysis)3. 검색 엔진(Search engines)4. 비주얼 검색 및 이미지 인식(Visual search & image recognition)5. 소셜 리스닝 및 정서 분석(Social listening & sentiment analysis)6. 제품 분류(Product categorisation)7. 제품 가격(Product pricing)8. 예측 분석(Predictive analytics)9. 잠재 고객 타겟팅 및 세분화(Audience targeting & segmentation)10. 프로그래밍 방식 광고 타깃팅(Programmatic ad targeting)11. 판매 예측(Sales forecasting)12. Chatbots 및 대화형 인공 지능(Chatbots & conversational AI)13. 음성 인식(Speech recognition)14. 컴퓨터 비전과 증강 현실(Computer vision & augmented reality)15. 카피 라이팅(Copywriting) "
[칼럼] 독점 인터뷰: 챗GPT 개발한 오픈AI의 샘 올트먼(포브스) ,https://blog.naver.com/dependentorigination/223039144066,20230309,"​독점 인터뷰: 챗GPT 개발한 오픈AI의 샘 올트먼ALEX KONRAD, KENRICK CAI 포브스, 202303호, 2023년 2월 23일​챗GPT와 인공일반지능이 자본주의를 무너뜨릴까? 좀처럼 인터뷰에 응하지 않는 샘 올트먼 오픈AI CEO가 AI 모델 챗GPT, 인공일반지능, 구글 검색에 대해 입을 열었다. 오픈AI CEO로서 샘 올트먼은 빠르게 성장하는 생성형 AI 부문에서 가장 바쁘고 주목받는 업체를 이끌고 있다. 오픈AI는 최근 포브스 2월호에 피처 기사로 소개됐다.지난 1월 오픈AI의 샌프란시스코 사무실을 방문한 뒤, 포브스는 언론 노출을 꺼리는 투자자 겸 기업가 올트먼을 만나 챗GPT, 인공일반지능(AGI, Artificial General Intelligence)에 대해 이야기를 나누고 오픈AI의 AI 도구가 구글 검색에 위협이 될지 들어봤다.​이 인터뷰는 명확성과 일관성을 위해 일부 편집됐다.​알렉스 콘래드: 챗GPT의 인기, 수익화의 압박, 마이크로소프트와의 파트너십을 둘러싼 흥분 속에서 우리는 일종의 변곡점에 선 것처럼 느껴진다. 당신의 관점에서 오픈AI는 어떻게 느껴지나? 그리고 이 변곡점을 어떻게 설명하겠는가?​샘 올트먼: 분명 흥미진진한 시기다. 그러나 나는 아직도 지금이 대단히 이른 시기이길 바란다. 앞으로 이 기술은 기하급수적으로 성장하며 사회에 긍정적인 영향을 미칠 것이다. GPT-3나 DALL-E를 출시할 때도 마찬가지였다. 이제는 챗GPT를 놓고 똑같은 말을 하는 것뿐이다. 나중에 또 같은 말을 반복하게 될 것이다. 아닐 수도 있지만, 어쩌면 마주친 적 없거나 예상치 못한 난관에 직면하게 될 수도 있다. 그러나 내 생각에 우리는 뭔가 대단히 중요한 것을 알아냈을 가능성이 있다. 이 패러다임은 우리를 훨씬 더 큰 발전으로 이끌 것이다.​챗GPT에 대한 반응에 놀랐나?​잘될 거라고 생각했기 때문에 하고 싶었다. 그래서 나는 그 반응의 규모에 놀랐다. 하지만 나는 늘 사람들이 이 기술을 정말로 마음에 들어 하길 기대하고 바랐다.그레그 브로크먼 오픈AI 회장은 팀조차도 출시할 가치가 있는지 확신하지 못했다고 말했다. 모두가 그렇게 생각한 것은 아닌 모양이다.곧 출시 예정인 제품에 팀의 반응이 그저 그랬던 적은 많았다. 그러면 우리는 “그냥 하자, 일단 해보고 어떻게 되는지 보자”고 말한다. 이번 건은 내가 아주 강하게 밀어붙였다. 나는 잘될 거라고 굳게 믿었다.​​챗GPT가 실제로 어떻게 만들어지고 실행되는지 알면 사람들이 놀랄 거라고 말한 적이 있다. 사람들이 뭘 잘못 알고 있나?한 가지는 챗GPT가 오랜 시간 API로 있었다는 것이다. 10개월가량 됐다. [편집자 주: 챗GPT는 2020년 처음 API로 출시된 모델 GPT-3의 업데이트된 버전이다.] 한 가지 놀라운 것은 특정 방식으로 유용하게 만들기 위해 미세 조정을 약간 가하고 올바른 상호작용 패러다임을 찾아내면 이런 결과가 나온다는 것이다. 근본적으로 새로운 기술이 아니다. 그런 부분을 사람들이 잘 모르는 것 같다. 많은 사람이 아직도 우리 말을 믿지 않고 이것이 GPT-4라고 생각한다.​​많은 AI 생태계가 등장하고 있는데, 이런 물결이 회사에 도움이 되는가? 아니면 잡음이 많아져서 일하기가 더 어려워지나?​둘 다 맞는 말이다.​​오픈AI 이외의 회사들이 중요한 작업을 하는 진정한 생태계가 생성되고 있다고 보나?​그렇다. 한 회사가 해내기엔 너무 큰 일이라고 생각한다. 진정한 생태계가 생겨나기를 바란다. 그러면 훨씬 나아질 것이다. 언젠가는 세상에 다수의 AGI가 등장할 것이다. 그렇게 되기를 바란다.​​현재의 AI 시장과 클라우드 컴퓨팅, 검색엔진 또는 기타 기술이 모두 비슷하다고 보나?​항상 비슷한 점이 있다고 생각한다. 그리고 특이한 점도 항상 있다. 사람들이 많이 하는 실수는 지나치게 유사성에 집중해 서로 다르게 만드는 미세한 차이에는 신경 쓰지 않는 것이다. 오픈AI에 대해서 ‘이건 클라우드컴퓨팅 경쟁이랑 비슷하네. 여러 플랫폼이 있지만 하나 만 API로 사용하게 될 거야’라고 생각하면 아주 쉽고 이해하기 편하다. 하지만 크게 다른 점도 많다. 사람들이 선택할 기능도 굉장히 다를 것이다. 클라우드들도 어떤 면에서는 서로 많이 다르지만 제공되는 서비스는 비슷하다. AI 제품 간에는 편차가 훨씬 클 것이다.​​사람들은 챗GPT가 구글 검색 등 기존 검색엔진을 대체할지 궁금해한다. 그런 부문에 관심이 있나?​챗GPT가 검색엔진을 대체할 것이라고는 생각하지 않는다. 하지만 언젠가 AI 시스템이 그렇게 할 것이라고 생각한다. 그보다 나는 사람들이 어제의 뉴스에 집중하면서 기회를 완전히 놓치고 있다고 본다. 나는 검색 다음에는 어떤 것이 나올지에 훨씬 관심이 많다. 웹 검색 전에는 우리가 뭘 하고 살았는지 기억나지 않는다. 나는 너무 젊다. 아마 당신도 그렇겠지만…내가 어렸을 때는 브리태니커 백과사전 CD가 있었다.아, 나도 기억한다. 바로 그거다. 그렇다고 아무도 ‘내 초등학교 시절의 브리태니커 백과사전 CD보다 조금 나은 버전을 만들겠어’라고 하지 않는다. 같은 일을 완전히 다른 방식으로 하는 것을 택한다. 내가 흥미를 갖는 부분은 이 모델이 단지 웹에서 검색어를 입력하는 경험을 대체하는 데 그치지 않고, 이 일을 완전히 다르면서도 훨씬 멋지게 처리하는 방법을 찾는 데 있다.​​그건 AGI가 나오면 가능할까? 아니면 그 이전에도 가능할까?​머지않아 이뤄질 것이다.​​우리가 AGI에 가까워졌다고 보나? GPT나 기타 다른 서비스가 AGI에 가까워진다는 사실을 어떻게 알 수 있나?​우리가 AGI에 많이 가까워졌다고 생각하지는 않는다. 하지만 그걸 아는 방법에 대해서는 최근에 많이 생각을 해봤다. 지난 5년 동안, 또 내가 이 일을 지금까지 해오면서 알게 된 사실 중 하나는 그게 아주 명확한 순간은 아닐 것이라는 점이다. 훨씬 더 점진적인 전환일 것이다. 사람들이 ‘느린 이륙’이라고 부를 만한 것이다. 우리가 AGI를 갖게 되는 순간이 언제인지에 대해 그 누구도 합의를 이루지 못할 것이다.​​AGI가 오픈AI를 넘어 당신의 모든 관심사와 관련이 있다고 생각하나? 월드코인이나 다른 회사들이 모두 AGI 이론에 들어맞는가?​그렇다. 적어도 내가 생각하는 프레임워크는 그렇다. AGI는 내 모든 행동 이면의 동력이다. 좀 더 직접적인 것도 있지만 대부분은 그렇게 직접적이지는 않다. 풍요의 세계에 도달하는 것도 목표다. 예를 들어 내 생각에는 에너지가 정말 중요한데, 에너지는 AGI를 만드는데도 매우 중요하다.​​그레그 브로크먼은 오픈AI가 연구 중심적인 회사이며 반자본주의적이지 않다고 말했다. 투자 수익을 원하는 투자자를 위한 영리 활동과 오픈AI의 폭넓은 목적 간의 줄타기를 어떻게 해나갈 생각인가?​자본주의는 훌륭한 체제이며 나는 자본주의를 아주 좋아한다. 세계의 온갖 나쁜 체제 가운데, 적어도 우리가 지금까지 발견한 것 중에서는 최선의 체제다. 우리가 더 나은 것을 찾기를 바란다. 만약 AGI가 정말로 이뤄지면 자본주의를 무너뜨릴 온갖 방법을 상상할 수 있다.우리는 다른 어떤 기업 구조와도 다른 구조를 설계하고자 했다. 우리가 하고자 하는 일에 확신이 있기 때문이다. 단지 또 하나의 IT 기업을 만들고자 했다면 지금까지 해왔던 대로 경영하면서 아주 큰 회사를 만들고자 했을 것이다. 하지만 우리가 진정으로 AGI를 얻고 모든 것이 무너지면 뭔가 새로운 기업 구조가 필요해질 것이다. 우리 팀과 투자자들이 잘되는 것이 진심으로 좋지만 한 회사가 AI 우주 전체를 소유해서는 안 된다고 생각한다. AGI의 수익을 어떻게 나눌지, 액세스를 어떻게 공유하고 거버넌스를 분배할지, 이 세 가지 문제에 대해 생각해봐야 할 것이다.​​브로크먼이 자사 제품, 기업용 도구와 타사 API를 함께 사용하는 아이디어에 대해 설명했는데, 앞으로 제품을 만들 때 오픈AI의 공개 정신을 어떻게 유지할 생각인가?​내 생각에 가장 중요한 방법은 챗GPT 같은 공개 도구를 내놓는 것이다. 구글은 제품을 공개하지 않는다. 다른 연구소도 마찬가지다. 공개는 안전하지 않다고 두려워하는 사람들이 있다. 하지만 나는 사회가 이런 기술을 붙들고 씨름해보면서 각각의 장점과 단점을 이해해야 한다고 생각한다. 우리가 하는 가장 중요한 일은 이런 도구를 공개해서 앞으로 뭐가 다가올지 세상이 이해하게 하는 것이다. 오픈AI에서 내가 가장 자랑스러워하는 것은 건전하고 중요한 방식으로 오버튼 윈도우(특정 시기에 어떤 정책이 대중에게 정치적으로 받아들여지는지를 이해하기 위한 모델)를 AGI로 내보낸 것이다. 설령 때로는 불편할 수 있다고 해도 그렇다.그뿐만 아니라 우리는 강력한 API를 더 늘리면서 이를 안전하게 만들고자 한다. 우리가 CLIP(2021년 출시된 시각적 신경망)를 오픈소스로 공개했듯이 계속해서 오픈소스를 만들 것이다. 오픈소스는 이미지 생성 열풍을 일으킨 원동력이다. 최근에는 위스퍼와 트라이튼(자동 음성 인식 및 프로그래밍 언어)을 오픈소스로 공개했다. 이런 식으로 여러 방면에 걸쳐 제품을 공개하면서 각 위험 요소와 이점의 균형을 잡아야 한다.​​마이크로소프트와 CEO 사티아 나델라에게 끌려가고 있다는 지적에 대해서는 어떻게 생각하나?​마이크로소프트와 체결하는 모든 계약은 우리의 사명을 완수하기 위해 정교하게 짜여 있다. 또 사티아와 마이크로소프트는 정말 훌륭하다. 지금까지 봤을 때 그들이 우리의 가치와 가장 잘 맞는 IT 기업이다. 우리는 늘 마이크로소프트를 찾아가서 ‘당신들이 아마 싫어할 만한 이런저런 이상한 것들을 하려고 합니다. 일반적인 계약과는 아주 다르거든요. 투자 수익에 제한을 걸거나 안전 우선 조항 같은 것들 말이죠’라고 하면 그쪽에서는 ‘그거 멋지네요’라고 답한다.​​오픈AI가 받는 사업상의 압력이나 수익을 창출해야 한다는 현실은 회사의 전반적인 사명과 충돌하지 않나?​전혀 충돌하지 않는다. 누구에게 물어봐도 상관없다. 나는 원치 않는 것은 하지 않기로 유명한 사람이다. 내가 아니라고 생각하면 절대 거래하지 않는다.오픈AI 사람들이 가운을 입고 ‘우리는 수익을 추구하지 않는다’고 말하는 수도승들은 아니지만, 그렇다고 부를 창출하기 위해 움직이지도 않는 것 같다.균형이 중요하다. 우리는 사람들을 성공하게 만들고 싶고, 투자한 만큼 좋은 수익을 가져다주고 싶다. 그것이 일반적이고 합당한 수준이라면 말이다. 완전한 AGI가 등장하면 그 패러다임에서 뭔가 다른 것을 원하게 될 것이다. 이를 사회와 함께 나눌 방법을 지금부터 모색해나가고자 한다. 지금까지는 그 균형을 아주 잘 지켜왔다고 생각한다.​​사람들이 GPT로 한 것들 중에 가장 멋지다고 생각하는 건 무엇인가? 그리고 가장 두려움을 느꼈던 것은 무엇인가?​멋진 것을 하나만 꼽기는 정말 어렵다. 다양한 작업을 볼 수 있어서 아주 놀라웠다. 내가 개인적으로 아주 유용하다고 느낀 것에 대해선 말할 수 있다. 요약은 내가 생각했던 것보다 훨씬 큰 도움이 된다. 기사 전문이나 긴 이메일을 요약하는 기능은 생각보다 훨씬 유용하다. 또 마치 아주 뛰어난 프로그래머와 대화를 하듯이 사람들이 잘 모르는 프로그래밍에 관한 질문을 하거나 코드 디버깅에서 도움을 받을 수 있다.가장 무서운 것을 말하자면, 오픈소스 이미지 생성기에서 만들어지고 있는 비동의 음란물이다. 이는 아주 큰 피해를 일으킨다고 본다.​​이런 도구를 만든 기업에 그런 일이 일어나지 않게 할 책임이 있다고 보나? 아니면 이는 인간 본성으로 인한 피할 수 없는 현상일까?​둘 다라고 본다. 여기서 관건은 어디를 규제하느냐는 것이다. 어떤 의미에서는 기업들에 그런 사업을 하지 말라고 하는 게 나을 수도 있다. 하지만 사람들은 그럼에도 모델을 오픈소스로 공개할 것이며, 대부분은 훌륭하겠지만 일부 나쁜 일이 일어날 수 있다. 이런 모델을 기반으로 사업을 하는 기업, 최종 사용자와 관계를 맺는 기업들도 책임을 분담해야 할 것이다. 여기에는 공동의 책임이 작용한다고 생각한다.​​※ 샘 올트먼 오픈AI CEO는 최근 마이크로소프트와 함께 많은 시간을 보내고 있다. 2023년 2월 워싱턴주 레드먼드에 있는 마이크로소프트 본사에서 포즈를 취한 샘 올트먼. 그는 이 기사를 위한 사진 촬영에 응하지 않았다.​- 자료출처: 포브스코리아- ALEX KONRAD, KENRICK CAI 포브스 기자​​  ​기사원문 기사원문보기 → 사진클릭​Exclusive Interview: OpenAI’s Sam Altman Talks ChatGPT And How Artificial General Intelligence Can ‘Break Capitalism’forbes, By Alex Konrad and Kenrick Cai, Feb 3, 2023​In a rare interview, OpenAI’s CEO talks about AI model ChatGPT, artificial general intelligence and Google Search.By Alex Konrad and Kenrick Cai​As CEO of OpenAI, Sam Altman captains the buzziest -and most scrutinized-startup in the fast-growing generative AI category, the subject of a recent feature story in the February issue of Forbes.​After visiting OpenAI’s San Francisco offices in mid-January, Forbes spoke to the recently press-shy investor and entrepreneur about ChatGPT, artificial general intelligence and whether his AI tools pose a threat to Google Search.​This interview has been edited for clarity and consistencyALEX KONRAD: It feels to me like we are at an inflection point with the popularity of ChatGPT, the push to monetize it and all this excitement around the partnership with Microsoft. From your standpoint, where does OpenAI feel like it is in its journey? And how would you describe the inflection point?​SAM ALTMAN: It's definitely an exciting time. But my hope is that it's still extremely early. Really this is going to be a continual exponential path of improvement of the technology and the positive impact it has on society. We could have said the same thing at the GPT-3 launch or at the DALL-E launch. We're saying it now [with ChatGPT]. I think we could say it again later. Now, we may be wrong, we may well hit a stumbling block we haven't or don't expect. But I think there's a real chance that we actually have figured out something significant here and this paradigm will take us very, very far.​​Were you surprised by the response to ChatGPT?​I wanted to do it because I thought it was going to work. So, I'm surprised somewhat by the magnitude. But I was hoping and expecting people were going to really love it.​[OpenAI President] Greg Brockman told me that the team wasn't even sure it was worth launching. So not everyone felt that way.​There's a long history of the team not being as excited about trying to ship things. And we just say, “Let's just try it. Let's just try it and see what happens.” This one, I pushed hard for this one. I really thought it was gonna work.​​You've said in the past you think people might be surprised about how really ChatGPT came together or is run. What would you say is misunderstood?​So, one of the things is that the base model for ChatGPT had been in the API for a long time, you know, like 10 months, or whatever. [Editor’s note: ChatGPT is an updated version of model GPT-3, first released as an API in 2020.] And I think one of the surprising things is, if you do a little bit of fine tuning to get [the model] to be helpful in a particular way, and figure out the right interaction paradigm, then you can get this. It's not actually fundamentally new technology that made this have a moment. It was these other things. And I think that is not well understood. Like, a lot of people still just don't believe us, and they assume this must be GPT-4.​​With the froth in the entire AI ecosystem, is that a rising tide that is helpful for you? Or does it create noise that makes your job more complicated?​Both. Definitely both.​​Do you think that there is a real ecosystem forming here, where companies besides OpenAI are doing important work?​Yeah, I think this is way too big for one company. And actually, I am hopeful that there is a real ecosystem here. I think that's much better. I think there should be multiple AGIs [artificial general intelligences] in the world at some point. So I really welcome that.​​Do you see any parallels between where the AI market is today and, say, the emergence of cloud computing, search engines or other technologies?​Look, I think there are always parallels. And then there are always things that are a little bit idiosyncratic. And the mistake that most people make is to talk way too much about the similarities, and not about the very subtle nuances that make them different. And so it's super easy and understandable to talk about OpenAI as like, “Ah, yes, this is going to be just like the cloud computing battles. And there's going to be several of these platforms, and you'll just use one as an API.” But there are a bunch of things about it that are also super different, and there are going to be very different feature choices that people make. The clouds are quite different in some ways, but you put something up, and it gets served. I think there will be much more of a spread between the various AI offerings.​​People are wondering if ChatGPT replaces the traditional search engine, like Google Search. Is that something that motivates or excites you?​I mean, I don't think ChatGPT does [replace Search]. But I think someday, an AI system could. More than that, though, I think people are just totally missing the opportunity if you're focused on yesterday's news. I'm much more interested in thinking about what comes way beyond search. I don't remember what we did before web search, I’m sort of too young. I assume you are, too…​We had an Encyclopedia Britannica CD-ROM when I was a little kid.​Yeah, okay, exactly. There we go. I remember that, actually, exactly that. But, no one came along and said, “Oh, I'm going to make a slightly better version of the Encyclopedia Britannica on the CD-ROM at my elementary school.” They're like, “Hey, actually we can just do this in a super different way.” And the stuff that I'm excited about for these models is that it's not like, “Oh, how do you replace the experience of going on the web and typing in a search query,” but, “What do we do that is totally different and way cooler?’”​​And that's something unlocked by AGI? Or does that happen before that?​Oh, no, I hope it happens very soon.​​Do you feel that we are close to the goal of something like an AGI? And how would we know when that version of GPT, or whatever it is, is getting there?​I don't think we're super close to an AGI. But the question of how we would know is something I've been reflecting on a great deal recently. The one update I've had over the last five years, or however long I've been doing this — longer than that — is that it's not going to be such a crystal clear moment. It's going to be a much more gradual transition. It'll be what people call a “slow takeoff.” And no one is going to agree on what the moment was when we had the AGI.​​Do you see that being relevant to all of your interests beyond OpenAI? Do they all fit into an AGI theory, Worldcoin and these other companies?​Yeah, it is. That is, at least, the framework in which I think. [AGI] is the thrust that drives all my actions. Some are more direct than others, but many that don't seem direct, still are. And then there is also the goal of getting to a world of abundance. I think energy is really important, for example, but energy is also really important to create AGI.​​Greg [Brockman] has said that while OpenAI is research driven, it's not anti-capitalist. How are you navigating the wire act between being for-profit with investors who want a return and the broader goal of OpenAI?​I think capitalism is awesome. I love capitalism. Of all of the bad systems the world has, it's the best one — or the least bad one we found so far. I hope we find a way better one. And I think that if AGI really truly fully happens, I can imagine all these ways that it breaks capitalism.​We've tried to design a structure that is, as far as I know, unlike any other corporate structure out there, because we actually believe in what we're doing. If we just thought this was going to be another tech company, I'd say, “Great, I know this playbook because I’ve been doing it my whole career, so let's make a really big company.” But if we really, truly get AGI and it breaks, we'll need something different [in company structure]. So I'm very excited for our team and our investors to do super well, but I don't think any one company should own the AI universe out there. How the profits of AGI are shared, how access to is shared and how governance is distributed, those are three questions that are going to require new thinking.​​Greg walked me through the idea of a third-party API future alongside first-party products-enterprise tools, perhaps. As you productize, how do you maintain an ethos of OpenAI staying open?​I think the most important way we do that is by putting out open tools like ChatGPT. Google does not put these things out for public use. Other research labs don't do it for other reasons; there are some people who fear it’s unsafe. But I really believe we need society to get a feel for this, to wrestle with it, to see the benefits, to understand the downsides. So I think the most important thing we do is to put these things out there so the world can start to understand what's coming. Of all the things I'm proud of OpenAI for, one of the biggest is that we have been able to push the Overton Window [Editor’s note: a model for understanding what policies are politically acceptable to the public at a given time] on AGI in a way that I think is healthy and important — even if it's sometimes uncomfortable.​Beyond that, we want to offer increasingly powerful APIs as we are able to make them safer. We will continue to open source things like we open-sourced CLIP [Editor’s note: a visual neural network released in 2021]. Open source in really what led to the image generation boom. More recently, we open sourced Whisper and Triton [automatic speech recognition and a programming language]. So I believe it's a multi-pronged strategy of getting stuff out into the world, while balancing the risks and benefits of each particular thing.​​What would you say to people who might be concerned that you're hitching your wagon to [CEO] Satya [Nadella] and Microsoft?​I would say we have carefully constructed any deals we've done with them to make sure we can still fulfill our mission. And also, Satya and Microsoft are awesome. I think they are, by far, the tech company that is most aligned with our values. And every time we've gone to them and said, “Hey, we need to do this weird thing that you're probably going to hate, because it's very different than what a standard deal would do, like capping your return or having these safety override provisions,” they have said, “That's awesome.”​​So you feel like the business pressures or realities of the for-profit side of OpenAI will not conflict with the overall mission of the company?​Not at all. You could reference me with anyone. I'm sort of well known for not putting up with anything I don't want to put up with. I wouldn't do a deal if I thought that.​You guys are not monks in hair shirts saying, “We don't want to make a profit off of this.” At the same time, it feels like you're not motivated by wealth creation, either.​I think it is a balance for sure. We want to make people very successful, making a great return [on their equity], that's great, as long as it's at a normal, reasonable level. If the full AGI thing breaks, we want something different for that paradigm. And we want the ability to bake in now how we're going to share this with society. I think we've done it in a nice way that balances it.​​What has been the coolest thing you've seen someone do with GPT so far? And what's the thing that scares you most?​It's really hard to pick one coolest thing. It has been remarkable to see the diversity of things people have done. I could tell you the things that I have found the most personal utility in. Summarization has been absolutely huge for me, much more than I thought it would be. The fact that I can just have full articles or long email threads summarized has been way more useful than I would have thought. Also, the ability to ask esoteric programming questions or help debug code in a way that feels like I've got a super brilliant programmer that I can talk to.​As far as a scary thing? I definitely have been watching with great concern the revenge porn generation that’s been happening with the open source image generators. I think that's causing huge and predictable harm.​​Do you think the companies who are behind these tools have a responsibility to ensure that kind of thing doesn't happen? Or is this just an unavoidable side of human nature?​I think it's both. There's this question of like, where do you want to regulate it? In some sense, it'd be great if we could just point to those companies and say, “Hey, you can't do these things.” But I think people are going to open source models regardless, and it's mostly going to be great, but there will be some bad things that happen. Companies that are building on top of them, companies that have the last relationship with the end user, are going to have to have some responsibility, too. And so, I think it's going to be joint responsibility and accountability there.​- 자료출처: forbes.com​ https://www.forbes.com​​  ​​새뮤얼 H. 올트먼(Samuel H. Altman, 1985년 4월 22일-)미국의 기업가, 투자가, 프로그래머​미국의 유대인 가정에서 태어나 미주리 세인트루이스에서 자랐다. 스탠퍼드 대학교에서 컴퓨터 과학을 공부하다가 중퇴하고 2005년 위치 기반 소셜 네트워킹 서비스 회사 Loopt를 공동 설립, 19세의 나이에 CEO가 되었다. 2011년부터는 와이 콤비네이터(Y Combinator)에 참여하였으며 2014년에는 와이 콤비네이터의 공동 설립자 폴 그레이엄으로부터 회장직에 임명되었다. 2015년에는 일론 머스크 등과 함께 인공지능 연구소인 OpenAI의 설립을 주도하였으며, 현재 사장으로 있다.- 위키백과  "
C# 음성 인식 VISTA 문제 ,https://blog.naver.com/novaly257755/222655244837,20220223,"프로젝트를 실행하려고 할 때 다음 오류가 발생합니다. A timeout occurred starting the SAPI Server.System.InvalidOperationException was unhandledMessage=""A timeout occurred starting the SAPI Server.""Source=""System.Speech""StackTrace:at System.Speech.Internal.SapiInterop.SapiRecognizer..ctor(RecognizerType type)at System.Speech.Recognition.SpeechRecognizer..ctor()at testdan.Form1..ctor() in C:\Users\Dany\Documents\Visual Studio 2008\Projects\testdan\testdan\Form1.cs:line 17at testdan.Program.Main() in C:\Users\Dany\Documents\Visual Studio 2008\Projects\testdan\testdan\Program.cs:line 18at System.AppDomain._nExecuteAssembly(Assembly assembly, String[] args)at System.AppDomain.ExecuteAssembly(String assemblyFile, Evidence assemblySecurity, String[] args)at Microsoft.VisualStudio.HostingProcess.HostProc.RunUsersAssembly()at System.Threading.ThreadHelper.ThreadStart_Context(Object state)at System.Threading.ExecutionContext.Run(ExecutionContext executionContext, ContextCallback callback, Object state)at System.Threading.ThreadHelper.ThreadStart() Form1 코드: using System;using System.Collections.Generic;using System.ComponentModel;using System.Data;using System.Drawing;using System.Linq;using System.Text;using System.Windows.Forms;using System.Speech.Recognition;using System.Threading;namespace testdan{    public partial class Form1 : Form    {        SpeechRecognizer rec = new SpeechRecognizer();        public Form1()        {            InitializeComponent();            rec.SpeechRecognized += new EventHandler<SpeechRecognizedEventArgs>(rec_SpeechRecognized);        }        void rec_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)        {            lblLetter.Text = e.Result.Text;            MessageBox.Show(e.Result.Text);        }        void Form1_Load(object sender, EventArgs e)        {            var c = new Choices();            c.Add(""close"");            c.Add(""stop"");            c.Add(""pause"");            var gb = new GrammarBuilder(c);            var g = new Grammar(gb);            rec.LoadGrammar(g);            rec.Enabled = true;        }    }} SAPI에서 사용하는 드라이버가 누락되었을 수 있습니까? 이로 인해 로드 중에 시간이 초과될 수 있습니다.  WSR UI가 표시되는 데 얼마나 걸립니까? WSR UI가 표시되는 데 평소보다 시간이 더 오래 걸리고 프록시가 시간 초과되는 일이 발생할 수 있다고 생각합니다.예외를 포착하고 생성자를 다시 시도할 수 있습니다. 운 좋게도 WSR UI는 계속 작동하고 생성자를 완료할 수 있습니다.  나도 본 적이 있다. 시작 메뉴에서 수동으로 데스크탑 인식기를 시작한 다음 코드를 실행하면 제대로 작동하지만 공유 인식기를 자동으로 로드하지 않는 것 같습니다. 이유를 모르겠습니다. "
딥러닝을 활용한 금융 시계열 예측[Financial time series forecasting with deep learning] ,https://blog.naver.com/gdpresent/223058651021,20230329,"Applied Soft Computing JournalFinancial time series forecasting with deep learning:A systematic literature review: 2005–2019Omer Berat Sezer, Mehmet Ugur Gudelek, Ahmet Murat Ozbayoglu​​​a b s t r a c tFinancial time series forecasting is undoubtedly the top choice of computational intelligence for finance researchers in both academia and the finance industry due to its broad implementation areas and substantial impact. Machine Learning (ML) researchers have created various models, and a vast number of studies have been published accordingly. As such, a significant number of surveys exist covering ML studies on financial time series forecasting. Lately, Deep Learning (DL) models have appeared within the field, with results that significantly outperform their traditional ML counterparts. Even though there is a growing interest in developing models for financial time series forecasting, there is a lack of review papers that solely focus on DL for finance. Hence, the motivation of this paper is to provide a comprehensive literature review of DL studies on financial time series forecasting implementation. We not only categorized the studies according to their intended forecasting implementation areas, such as index, forex, and commodity forecasting, but we also grouped them based on their DL model choices, such as Convolutional Neural Networks (CNNs), Deep Belief Networks (DBNs), and Long-Short Term Memory (LSTM). We also tried to envision the future of the field by highlighting its possible setbacks and opportunities for the benefit of interested researchers.금융 시계열 예측은 광범위한 영역에서의 적용이 가능하단 점과 또 상당한 영향력으로 인해 학계든 업계든 금융 리서치에 대한 인공지능은 의심의 여지 없이 최상의 선택지라고 할 수 있습니다. 머신러닝(ML) 연구자들은 다향한 모델을 만들어왔고, 그에 따른 수 많은 연구들이 공개되었습니다. 따라서 금융 시계열 예측에 대해 ML 분야가 커버하는 부분에 대한 서베이 논문들도 깨나 많습니다. 최근에 딥러닝(DL) 모델들 또한 이 영역에 들어오기 시작했고, ML 모델들을 상당히 개선시키기도 했습니다. 금융 시계열 예측에 대해 나날이 관심이 커지고 있음에도 불구하고, 금융에 대해 오롯이 DL에 초점을 둔 리뷰 페이버틑 상당히 부족합니다. 따라서, 본고에서는 DL을 활용한 금융 시계열 예측에 있어서의 연구들을 정리해보려 합니다. 여기에선 지수, 환율, 원자재 예측 등 적용 분야에 대해 분류를 해볼 뿐만 아니라, CNN, DBN, LSTM 등의 DL 모델 선택을 기반으로 하여 분류를 해볼 것이기도 합니다. 또한, 잠재적인 문제점과 기회 등을 강조함으로써 이 영역의 미래를 생각해보려 합니다.​​​1. IntroductionThe finance industry has always been interested in the successful prediction of financial time series data. Numerous studies have been published on ML models with relatively better performances than classical time series forecasting techniques. Meanwhile, the widespread application of automated electronic trading systems coupled with increasing demand for higher yields keeps forcing researchers and practitioners to continue working on implementing better models. Hence, new publications and implementations keep adding to the finance and computational intelligence literature.금융 업계는 금융 시계열 데이터에 대한 훌륭한 예측에 대해 항상 관심이 있었습니다. ML을 기반으로 한 모델들은 상대적으로 전통적인 시계열 예측 기법보다 낫다는 연구들이 상당히 많이 공대되었습니다. 또한, 자동화된 전자 거래 시스템이 널리 활용되며 높은 수익률에 대한 수요가 증가하여, 연구자들 및 실무자들 모두 계속해서 모델을 개선시키는 작업을 멈출 수 없었습니다. 이렇게 계속해서 새로운 연구와 새로운 적용 기법들은 연구되어오고 있습니다.​In the last few years, DL has strongly emerged as the best performing predictor class within the ML field in various implementation areas. Financial time series forecasting is no exception, and as such, an increasing number of prediction models based on various DL techniques have been introduced in the appropriate conferences and journals in recent years. Despite the vast number of survey papers covering financial time series forecasting and trading systems using traditional soft computing techniques, to the best of our knowledge, no reviews have been performed on the literature for DL. Hence, we decided to work on such a comprehensive study, focusing on DL implementations of financial time series forecasting. Our motivation is two-fold; we not only aimed at providing a state-of-the-art snapshot of academic and industry perspectives of developed DL models, but we also pinpoint the important and distinctive characteristics of each studied model to prevent researchers and practitioners from making unsatisfactory choices during their system development. We also wanted to envision where the industry is heading by indicating possible future directions.최근 몇년동안에는 다양한 적용 분야 안에서 ML 분야 내에서 가장 우수한 예측 모델 부류로 DL이 주목받고 있습니다. 금융 시계열 예측 또한 마찬가지로 DL 기법을 활용한 예측모델들이 컨퍼런스나 저널에서도 많이 소개되고 있습니다. 전통적인 컴퓨팅 기법을 활용한 시계열예측과 트레이딩 시스템 등을 커버하는 서베이 페이퍼는 많았지만, DL에 대해서 쓴 그런 페이퍼는 없는것으로 보였습니다. 따라서 저희는 금융 시계열 예측에 대한 DL의 적용에 초점을 두고 연구들을 정리하자다는 결정을 내렸습니다. 여기서의 목표는 두 가지입니다. 여기에선 학계와 업계의 관점에서의 최신 동향을 살펴보는것, 그리고 시스템을 개발하는 데 있어서 잘못된 선택을 하지 않도록 연구된 각 모델에 대한 중요하고 독특한 특징들 또한 정확하게 살펴보는 것입니다. 또한, 가능한 미래 방향을 가리키며 업계가 어디로 향해야하는지도 생각해보고자 합니다.​Our fundamental motivation was to answer the following research questions: • Which DL models are used for financial time series forecasting? • How does the performance of DL models compare with that of their traditional ML counterparts? • What is the future direction of DL research for financial time series forecasting?가장 근본적인 질문은 바로 아래의 대답에 질문하는 것입니다.어떤 모델이 금융 시계열 예측을하는데 사용되는가?DL모델의 성과와 전통 ML 모델의 성과는 어떻게 비교되는가?시계열 예측에 대한 DL 연구의 미래 방향은 어디인 것인가?​Our focus was solely on DL implementations for financial time series forecasting. For other DL-based financial applications, such as risk assessment and portfolio management, interested readers can refer to another recent survey paper [1]. Because we wanted to single out financial time series prediction studies in our survey, we omitted other time series forecasting studies that were not focused on financial data. Meanwhile, we included time-series research papers that had financial use cases or examples, even if the papers themselves were not directly concerned with financial time series forecasting. Also, we decided to include algorithmic trading papers that were based on financial forecasting but ignore the ones that did not have a time series forecasting component.우리의 초점은 오직 시계열 예측에 대한 DL 활용합니다. 리스크 관리나 포트폴리오 관리 등과 같은 영역에서의 DL은 다른 서베이 논문에서 확인하실 수 있습니다. 그리고 오직 금융 시계열 예측에 대해서만 다루고자 하여, 다른 종류의 시계열 예측에 대한 것은 제외시켰습니다. 한편, 금융에 대한 사용과 사례 등이 있는 시계열에 대한 리서치 페이퍼는 이게 딱 금융 시계열 예측에 직접적인 관련이 없는것이라 해도, 이에 대한 내용은 포함시켰습니다. 또, 금융 시계열 예측에 기반한 알고리즘 트레이딩에 대한 내용은 포함시켰으며, 시계열 예측을 기반으로 하지 않는 것은 포함시키지 않았습니다.​We mainly reviewed journals and conferences for our survey, but we also included Masters and PhD theses, book chapters, arXiv papers, and noteworthy technical publications that came up in web searches. We decided to only include articles published in English language.저희는 주로 저널이나 컨퍼런스에 발표된 것을 주로 리뷰했으며, 석사나 박사 학위 논문, 그리고 책의 챕터 arXiv의 논문, 웹 검색으로 얻을 수 있는 괜찮은 글들 또한 포함시켰으며, 다만, 영어로 된 글들만을 포함했습니다.​During our survey, we realized that most of the papers using the term ‘‘deep learning’’ in their description were published in the past five years. However, we also encountered some older studies that implemented deep models, such as Recurrent Neural Networks (RNNs) and Jordan–Elman networks. However, at their time of publication, the term ‘‘deep learning’’ was not in common usage. Therefore, we decided to also include those papers.이를 진행하면서 알아차린 것 중에 대부분의 “딥러닝”이란 단어를 사용하는 페이퍼들은 최근 5년에 발표되었었던 것들이란 것을 알았습니다. 그리고 그 이전에 발표되었었던 RNN에 관한 논문이라든디 Jordan-Elman network 등에 대한 것들도 딥러닝이란 단어는 사용하지 않지만, 사실은 DL에 관련한 것이죠. 그래서 이런것들도 저희 페이퍼에 포함시키기로 했습니다.​According to our findings, this will be one of the first comprehensive ‘‘financial time series forecasting’’ survey papers focusing on DL. Many ML reviews for financial time series forecasting exist in the literature, but we have not encountered any study on DL. Hence, we wanted to fill this gap by analyzing the developed models and applications accordingly. We hope that as a result of this paper, researchers and model developers will have a better idea of how they can implement DL models in their studies.이 페이퍼는 DL에 초점을 둔 시계열 예측에 관한 첫번째 포괄적인 서베이 페이퍼가 될 것입니다. 많은 금융 시계열 예측에 대한 ML 리뷰 페이퍼는 다수 존재하지만, DL에 관한 것은 거의 없습니다. 따라서, 개발되어 사용되는 모델들을 분석함으로써 이 갭을 채우고자 합니다. 그리고 결과적으로 많은 연구자들과 모델 개발자들이 그들의 연구에서 DL이 어떻게 사용될 수 있을지에 대한 더 나은 아이디어를 얻었으면 좋겠습니다.​The remainder of this paper is structured as follows. In Section 2, existing surveys focused on ML and soft computing studies for financial time series forecasting are mentioned. In Section 3, we will cover existing DL models that are used, such as CNN, LSTM, and Deep Reinforcement Learning (DRL). Section 4 will focus on the various financial time series forecasting implementation areas using DL, namely stock forecasting, index forecasting, trend forecasting, commodity forecasting, volatility forecasting, foreign exchange forecasting, and cryptocurrency forecasting. In each subsection, the problem definition will be given, followed by the particular DL implementations. In Section 5, overall statistical results about our findings will be presented, including histograms related to the annual distributions of different subfields, models, publication types, etc. A state-of-the-art snapshot of financial time series forecasting studies will be given through these statistics. At the same time, they will also show the areas that are already mature in comparison with promising or new areas that still have room for improvement. Section 6 discusses the academic and industrial achievements that have been accomplished and future expectations. The section will include highlights of open areas that require further research. Finally, we conclude this paper in Section 7 by summarizing our findings.이제 이 다음부터는 다음과 같이 구성되어있습니다. 섹션2에서는 금융 시계열 예측을 위한 ML과 soft 컴퓨팅의 기존의 서베이들을 살펴보겠습니다. 섹션3에서는 CNN, LSTM, 강화학습 등의 이미 존재하는 DL 모델들을 살펴보겠습니다. 그리고 섹션4에서는 DL이 사용되는 다양한 금융시계열 예측 모델들을 볼 것입니다. 주가예측, 지수 예측, 추세 예측, 원자재, 변동성, 환율, 코인 등의 예측에 사용되는 모델들입니다. 그리고 각각의 DL 적용에 대해 어떤 문제가 있는지 또한 보게 될 것입니다. 섹션5에서는 전반적인 우리의 결과물에 대해 모델별로, 혹은 논문의 유형별로 연간 분포 등의 통계수치가 제시될 것입니다. 이럼으로써 금융 시계열분석의 최신 동향을 살필수 있으며, 성숙 단계에 접어든 부분 혹은 개선의 여지가 있기에 장래에 유망할 것이라 보이는 부분을 확인할 수 있습니다. 섹션6에서는 학계 혹은 업계에서 성취한 부분 그리고 미래에 기대되는 부분을 논할 것이며, 추가적인 연구가 더 필요한 부분도 확인할 수 있을 것입니다. 마지막으로 섹션7에서 결과물들을 모두 요약함으로써 결론을 맺도록 하겠습니다.​​​2. Financial time series forecasting with MLFinancial time series forecasting and associated applications have been studied extensively for many years. When ML started gaining popularity, financial prediction applications based on soft computing models accordingly also became available. Even though our particular focus is on DL implementations of financial time series prediction studies, it will be beneficial to briefly mention existing surveys covering ML-based financial time series forecasting studies to provide some historical perspective.금융 시계열 예측, 그리고 그것에 관련한 적용 에 대한 연구는 해가 갈수록 광범위하게 진행되고 있습니다. ML이 인기를 얻기 시작하면서, soft computing 모델에 기반한 금융 예측이 사용되기 시작하기도 했습니다. 여기 페이퍼에서는 금융 시계열 예측에 대해 DL에 초점을 두고 진행되긴 하지만, 기존에 진행되었었던 ML기반으로한 금융 시계열 예측에 대해 간단하게 언급하는것이 시대적인 관점에서 도움이 될 수 있기에 이거를 좀만 하고 넘어가겠습니다.​In our study, we did not include any survey papers that were focused on specific financial application areas other than forecasting studies. However, we were faced with some review publications that included a mix of financial time-series studies and other financial applications. We decided to include those papers to maintain the comprehensiveness of our coverage.저희 페이퍼에서는 예측에 관련하지 않은 금융에 관련한 서베이 연구에 대해선 포함하지 않았습니다. 허나, 금융 예측이 약간 섞여있는 리뷰 페이퍼는 우리의 커버리지의 포괄적으로 관련된 영역이라 생각하여 포함하긴 했습니다.​Examples of these aforementioned publications are provided here. There were published books on stock market forecasting [2], trading system development [3], practical examples of forex and market forecasting applications [4] using ML models, such as Artificial Neural Networks (ANNs), Evolutionary Computations (ECs), and Genetic Programming (GP), and Agent-based models [5].앞서 언급된 것은 바로 이런것들 입니다. ANN이나 EC, GP, Agent-based model등의 ML모델을 활용한 주식 시장 예측, 트레이딩 시스템 개발, 환율과 시장의 예측에 관한 실무적인 예 등이 이에 해당합니다.​There were also some existing journal and conference surveys. Bahrammirzaee et al. [6] surveyed financial prediction and planning studies along with other financial applications using various Artificial Intelligence (AI) techniques such as ANN, Expert Systems, and hybrid models. Zhang et al. [7] also compared ML methods in different financial applications, including stock market prediction studies. In Mochon et al. [8], soft computing models for the market, forex prediction, and trading systems were analyzed. Mullainathan and Spies [9] surveyed the prediction process in general from an econometric perspective.뭐 관련해서 저널이나 컨퍼런스 서베이들은 이런게 있습니다..​There were also a number of survey papers concentrated on one particular ML model. Even though these papers focused on one technique, the implementation areas generally spanned various financial applications, including financial time series forecasting. Among those soft computing methods, EC and ANN have had the most overall interest.또한 특정 ML 모델에 집중한 서베이 페이퍼들도 많습니다. 이러한 연구에선 오직 하나의 기법에 대해서만 초점을 두고 있지만, 이에 대한 적용은 금융의 많은 분야로 접목이 가능합니다. soft computing 방법론들 중에서는 EC와 ANN이 큰 인기를 끌었었습니다.​In terms of EC studies, Chen wrote a book on Genetic Algorithms (GAs) and GP in Computational Finance [10]. Later, Multi-objective Evolutionary Algorithms (MOEAs) were extensively surveyed for various financial applications including financial time series prediction [11–13]. Meanwhile, Rada reviewed EC applications along with Expert Systems for financial investing models [14].EC에 관련해서는 Chen이란 연구자가 유전 알고리즘(GA; Genetic Algorithms)과 computational Finaance 영역에서의 GP에 대해 책을 썼습니다. 그 이후 MOEAs; Multi-Objective Evolutionary Algorithms이 금융 시계열 예측을 포함한 여러 금융에 대한 접목으로 광범위하게 연구되었습니다. 그러는 한편 Rada는 금융 투자 모델에서의 Expert System을 따르는 EC application에 대해서 연구를 진행하기도 했습니다.​In terms of ANN studies, Li and Ma reviewed implementations of ANN for stock price forecasting and some other financial applications [15]. Tkac et al. [16] surveyed different implementations of ANN in financial applications, including stock price forecasting. Recently, Elmsili and Outtaj surveyed ANN applications in economics and management research, including economic time series forecasting [17].ANN 연구에 관련해서는 Li and Ma가 주가를 예측 등에 활용하는 ANN에 관련해 연구를 진했습니다. Tkac외 다수의 연구에선 또 그외의 다른 영역에 ANN을 적용하는 것도 보였습니다. 최근에는 Elmsili and Outtaj의 서베이에서 경제 시계열 예측 등을 포함한 경제학에서의 ANN 활용도 정리했습니다.​There have also been several text mining surveys focused on financial applications, including financial time series forecasting. Mittermayer and Knolmayer compared various text mining implementations that extract market responses to news for prediction [18]. Mitra et al. [19] focused on news analytics studies for prediction of abnormal returns for trading strategies in their survey. Nassirtoussi et al. reviewed text mining studies for stock or forex market prediction [20]. Kearney et al. [21] also surveyed text mining-based time series forecasting and trading strategies using textual sentiment. Similarly, Kumar and Ravi [22] reviewed text mining studies for forex and stock market prediction. Lately, Xing et al. [23] surveyed natural language-based financial forecasting studies.금융 시계열 예측을 포함한 금융에서의 접목에 초점을 둔 텍스트 마이닝에 대한 연구들도 있습니다. Mittermayer and Knolmayer의 연구에서는 예측을 목표로한 뉴스에 대한 시장의 반응을 추출하는 여러 텍스트 마이닝을 비교했고, Mitra 외 다수의 연구에서는 이례현상을 목표로하는 매매 전략에 사용할 뉴스 분석(애널리틱스)에 초점을 두어 연구를 하기도 했습니다. Nassirtoussi 외 다수의 연구에서는 환율과 주식 예측을 목적으로하는 텍스트마이닝을 검토했으며, Kearney 외 다수의 연구에서는 텍스트로부터 감성분석을 활용하여 매매전략을 짜거나 호긍ㄴ 시계열을 계측하는 텍스트 마이닝을 분석했습니다. Kumar and Ravi 의 페이서에서는 외환 및 주가 시장 예측을 위한 텍스트 마이닝을, 그리고 최근에 Xing 외 다수의 연구에서 자연어 기반으로하는 금융예측을 하는 것에 대해 조사하기도 했습니다.​Finally, there were application-specific survey papers that focused on particular financial time series forecasting implementations. Among these studies, stock market forecasting had the most interest. A number of surveys were published for stock market forecasting studies based on various soft computing methods at different times [24–31]. Chatterjee et al. [32] and Katarya and Mahajan [33] concentrated on ANN-based financial market prediction studies, whereas Hu et al. [34] focused on EC implementations for stock forecasting and algorithmic trading models. In a different time series forecasting application, researchers surveyed forex prediction studies using ANN [35] and various other soft computing techniques [36].마지막으로 특정한 금융 시계열 분석에 초점을 둔 서베이 페이퍼도 있습니다. 이러한 연구들에서는 주식 시장 예측에 가장 큰 관심을 둡니다. 많응 연구에서는 여러 soft computing 기법을 화용하여 서로 다른 시점에 주식 시장을 예측하는 것에 대해 논합니다. Chatterjee외 다수의 연구, 그리고 Katarya and Mahajan의 연구에서는 ANN을 기반으로하는 것에 집중하여 금융 시장 예측에 대한 연구를 진행한 반면, Hu 외 다수의 연구에서는 주식 예측과 알고리즘 트레이딩 모델을 위한 EC 활용에 대해 초점을 두기도 했습니다. 또 다른 종류의 금융 시계열에 대한 접목으로 ANN과 여러 soft computing 기법을 활용한 외환시장 예측 등에 대한 연구도 있습니다.Although many surveys exist for ML implementations of financial time series forecasting, DL has not yet been surveyed comprehensively despite the emergence of various DL implementations in recent years. This was the main motivation for our survey. In the next section, we cover the various DL models used in financial time series forecasting studies.금융 시계열 예측에 대한 ML의 활용은 많은 서베이페이퍼가 존재하는 반면, DL에 초점을 둔 포괄적인 페이퍼는 없는 것 같습니다. 이것이 저희 서베이의 주요한 동기부여였스며, 이 다음 섹션부터 금융 시계열 예측연구에 사용됐던 여러 DL모델들을 커버하도록 하겠습니다.​​​3. Deep learningDL is a type of ANN that consists of multiple processing layers and enables high-level abstraction to model data. The key advantage of DL models is extracting the good features of input data automatically using a general-purpose learning procedure. Therefore, DL models have been proposed for many applications such as: image, speech, video, and audio reconstruction, natural language understanding (particularly topic classification), sentiment analysis, question answering, and language translation [37]. The historical improvements of DL models are surveyed in Schmidhuber et al. [38].DL이라 함은 ANN의 한 유형으로써 여러 계층(processing layer)으로 구정되어 있으며, 데이터를 높은 수준으로 추상화를 가능토록 하는 것을 말합니다. DL 모델의 가장 핵심적인 이점은 입력되는 데이터의 좋은 특성(feature)를 어느 목적으로나 사용될 수 있는 학습 절차를 사용하여 자동으로 추출해주는 것입니다. 따라서, DL모델은 이미지, 음성, 동영상, 자연어 처리, 감성 분석, 질문응답, 번역 등 여러 방면에 사용될 수 있다고 제시되어 왔습니다. DL모델의 역사적인 개선에 대해선 Schmidhuber 외 다수의 페이퍼에서 정리되어 있습니다.​Financial time series forecasting has been very popular among ML researchers for more than 40 years. The financial community has been boosted by the recent introduction of DL models for financial prediction and their accompanying publications. The success of DL over ML models is the major attractive point for finance researchers. With more financial time series data and different deep architectures, new DL methods will be proposed. In our survey, the vast majority of studies found DL models to be better than their ML counterparts.금융 시계열 예측은 ML 연구자들 사이에서 40년 넘게 인기있는 분야입니다. 금융계는 최근 금융 예측에 대한 DL모델의 모딥으로 힘을 얻고 있습니다. ML을 능가하는 DL의 성공은 금융 분석가들에게 주요한 매력 포인트입니다. 많은 금융 시계열 데이터와 서로 다른 딥 아키텍셔, 그리고 새로운 DL 모델들이 계속 제시될 것입니다. 저희 서베이에 따르면, 대다수의 연구에서 DL모델이 ML을 능가한다는 것을 확인했습니다.​In the literature, there are different kinds of DL models: Deep Multilayer Perceptron (DMLP), RNN, LSTM, CNN, Restricted Boltzmann Machines (RBMs), DBN, Autoencoder (AE), and DRL [37, 38]. Throughout the literature, financial time series forecasting is mostly considered as a regression problem. However, there is also a significant number of studies, particularly on trend prediction, that use classification models to tackle financial forecasting problems. In Section 4, different DL implementations are presented along with their model choices.연구에 따르면 서로 다른 DL 모델의 종류가 있습니다: DMLP(딥 다층 퍼셉트론), RNN, LSTM, CNN, RBM(제한적 볼츠만 머신), DBN, AE, DRL 등. 연구들 전반적으로, 금융 시계열 예측은 일종의 회귀 문제로 간주합니다. 하지만, 이를 타개하기 위해 분류 모델을 사용한 추세 예측의 문제로 간주하는 상당한 양의 연구들도 있습니다. 섹션4에서 모델 선택에 따른 서로 다른 DL의 활용을 보이겠습니다.​​3.1. Deep Multi Layer Perceptron (DMLP)DMLP was one of the first developed ANNs. Its difference from shallow nets is that DMLP contains more layers. Even though particular model architectures might have variations depending on different problem requirements, DMLP models consist mainly of three layers: input, hidden, and output. The number of neurons in each layer and the number of layers are the hyperparameters of the network. In general, each neuron in the hidden layers has input (x), weight (w), and bias (b) terms. In addition, each neuron has a nonlinear activation function, which produces a cumulative output of the preceding neurons. Eq. (1) [39] illustrates the output of a single neuron in the Neural Network (NN). There are different types of nonlinear activation functions. The most commonly used nonlinear activation functions are sigmoid (Eq. (2)) [40], hyperbolic tangent (Eq. (3)) [41], Rectified Linear Unit (ReLU) (Eq. (4)) [42], leaky-ReLU (Eq. (5)) [43], swish (Eq. (6)) [44], and softmax (Eq. (7)) [39]. Non-linear activations were compared in [44].DMLP는 가장 첫 번째로 개발된 ANN 모델입니다. 얕은망(shallow net)과의 차이점은 DMLP가 다수의 계층으로 구성되어있다는 것입니다. 물론, 어떤 문제냐에 따라서 모델의 구조는 약간씩 변형될 수 있습니다만, DMLP 모델은 주요한 3개의 계층으로 이루어져 있습니다. 입력(input)과 은닉(hidden), 그리고 출력(output) 계층입니다. 각 계층에 속한 뉴런의 갯수와 계층 자체의 갯수는 분석가가 결정하는 하이퍼파라미터입니다. 일반적으로 은닉계층 안의 각 뉴런은 input(x), 가중치(w), 편향(b)의 항으로 이루어져 있습니다. 거기에 더불어 각 뉴런은 비선형의 활성화 함수를 안고 있으며, 이는 누적적으로 output에 영향이 미치도록 되었습니다. 식(1)은 신경망 네트워크 안에 단일 뉴런에 대해서 나타내고 있습니다. 활성화함수의 형태는 여러가지가 있습니다. 가장 일반적으로 사용되는 것으로는 시그모이드함수, 하이퍼볼릭탄젠트, ReLI, Leaky-ReLU, Swish, 소프트맥스 등이 있습니다. 비선형 활성화에 대해선 [44]의 연구에서 비교하였습니다. DMLP models have appeared in various application areas [45, 37]. Using a DMLP model has advantages and disadvantages depending on the problem requirements. Through DMLP models, problems such as regression and classification can be solved by modeling the input data [46]. However, if the number of input features is increased (e.g., image as input), the parameter size in the network will increase accordingly due to the fully connected nature of the model, which will jeopardize the computational performance and create storage problems. To overcome this issue, different types of Deep Neural Network (DNN) methods have been proposed (such as CNN) [37]. With DMLP, much more efficient classification and regression processes can be performed. In Fig. 1, a DMLP model’s layers, neurons, and weights between neurons are illustrated.DMLP모델은 여러 분야에서 활용되고 있습니다. DMLP 모델을 사용하는 것은 문제의 요구사항에 따라 장단점이 있습니다. DMLP 모델을 사용하는 것은, 회귀나 분류같은 문제들이 input 데이터의 모델링으로써 문제를 푸는 것입니다. 그러나 입력 feature의 갯수가 들어나면, 네트워크 안에서의 파라미터의 크기가 증가할 것이며, 완전연결이라는 모델이 갖고 있는 특성에 의해 컴퓨터 성능이나 저장에 대한 문제가 발생할 수 있습니다. 이를 해결가기 위해 다른 형태의 DNN(Deep Neural Network)으로 CNN 등 같은 것이 제안되었습니다. DMLP를 사용하면, 회귀나 분류같은 절차를 훨씬 더 효율적으로 진행됩니다. 그림1에서 DMLP 모델의 계층, 뉴런, 그리고 뉴런 사이의 가중치가 묘사되어있습니다.​ The DMLP learning stage is implemented through backpropagation. The error in the neurons in the output layer is propagated back to the preceding layers. Optimization algorithms are used to find the optimum parameters/variables of the NNs. They are used to update the weights of the connections between the layers. Different optimization algorithms have been developed: Stochastic Gradient Descent (SGD), SGD with Momentum, Adaptive Gradient Algorithm (AdaGrad), Root Mean Square Propagation (RMSProp), and Adaptive Moment Estimation (ADAM) [47–51]. Gradient descent is an iterative method to find optimum parameters of the function that minimizes the cost function. SGD is an algorithm that randomly selects a few samples for each iteration instead of the whole dataset [47]. SGD with Momentum remembers the update in each iteration, which accelerates gradient descent [48]. AdaGrad is a modified SGD that improves on the convergence performance of the standard SGD algorithm [49]. RMSProp is an optimization algorithm that adapts the learning rate for each parameter. In RMSProp, the learning rate is divided by a running average of the magnitudes of recent gradients for that weight [50]. ADAM is an updated version of RMSProp that uses running averages of both the gradients and second moments of the gradients. ADAM combines the advantages of RMSProp (works well in online and non-stationary settings) and AdaGrad (works well with sparse gradients) [51].DMLP의 학습은 연전파를 통하여 진행됩니다. 출력 계층의 뉴런에서 계산된 오차가 이전의 계층으로 다시 뒤로 쏴지는 것입니다. NN에서는 이렇게 최적의 매개변수와 변수 등을 찾는 최적화 알고리즘이 사용됩니다. 이 알고리즘들은 계층사이의 가중치들을 업데이트 하는 것입니다. 여태 개발되어 최적화 알고리즘들은 확률적 경사하강법(SGD), 모멘텀을 활용한 SGD, AdaGrad(Adaptive Gradient), RMSProp(Root Mean Square Propagation), Adam(Adaptive Moment Estimation) 등이 있습니다. 경사하강법은 함수값을 최소화 시키는 최적의 매개변수를 찾기위해 반복적으로 동작하는 기법입니다. 반면, SGD는 전체 데이터 셋에 대해 반복적으로 모두 돌려보는 것이 아니라, 랜덤하게 샘플을 뽑아 경사하강법을 시행하는 것이고 , 모멘텀을 활용한 SGD는 어느쪽 방향으로의 경사 하강이 가속화되는지를 반영하는 것입니다. AdaGrad 방법은 SGD를 수정한 버젼인데, SGD가 최적으로 수렴하는 성능을 개선시켰습니다. RMSProp은 각각의 매개변수마다의 학습률(learning rate)을 적응적으로 변화시키는 방법입니다. 여기에서 학습률은 최근의 가중치들의 평균으로 나누어집니다. Adam은 RMSProp을 수정한 버젼이며, 그래디언트 뿐만 아닌 헤시안까지 함께 평균하여 사용되는 것입니다. Adam은 RMSprop(안정적이지 않으며 병렬처리 불가능한 환경에서 잘 작동)의 장점과 AdaGrad의 장점(기울기가 거의 없을때 적합)을 혼합한 것이라 볼 수 있습니다.​As shown in Fig. 1, the effect of backpropagation is transferred to the previous layers. If the effect of SGD is gradually lost when the effect reaches the early layers during backpropagation, this problem is called the vanishing gradient problem [52]. In this case, updates between the early layers become unavailable and the learning process stops. The high number of layers in the neural network and the increasing complexity cause the vanishing gradient problem.그림1에서 볼 수 있듯이, 역전파의 효과는 이전의 계층으로 전달됩니다. SGD의 영향이 초기의 계층으로 전달되며 그 영향이 점진적으로 사라진다면, 이렇게 발생하는 문제를 기울기 소실(vanishing gradient)라고 합니다. 이러한 경우에 앞쪽에 있던 계층간의 갱신(update)은 불가능해지며, 학습 프로세스는 멈추게 됩니다. 신경망 안의 계층이 많아지고 복잡도가 높아짐으로 인해 이런 기울기 소실 문제가 발생한다고 합니다.​The important issue in the DMLP are the hyperparameters of the networks and method of tuning these hyperparameters. Hyperparameters are the variables of the network that affect the network architecture and performance of the networks. The number of hidden layers, number of units in each layer, regularization techniques (dropout, L1, L2), network weight initialization (zero, random, He [53], Xavier [54]), activation functions (Sigmoid, ReLU, hyperbolic tangent, etc.), learning rate, decay rate, momentum values, number of epochs, batch size (minibatch size), and optimization algorithms (SGD, AdaGrad, RMSProp, ADAM, etc.) are the hyperparameters of DMLP. Choosing better hyperparameter values/variables for the network results in better performance. Therefore, finding the best hyperparameters for the network is a significant issue. In the literature, there are different methods to find best hyperparameters: Manual Search (MS), Grid Search (GS), RandomSearch (RS), and Bayesian Methods (Sequential Model-Based Global Optimization (SMBGO), The Gaussian Process Approach (GPA), Tree-structured Parzen Estimator Approach (TSPEA)) [55,56]DMLP에서 중요한 문제는 네트워크의 하이퍼 파라미터와 그것을 결정하는 방법입니다. 하이퍼파라미터는 네트워크 구조(아키텍쳐)와 성능에 영향을 미치는 변수입니다. 은닉계층의 갯수, 각 계층의 뉴런의 갯수, 패널티방법론(드롭아웃, L1, L2 norm), 가중치 초깃값(0, 랜덤, He초깃값, 사비에르 초깃값), 활성화함수(시그모이드, ReLE, 하이퍼볼릭탄젠트), 학습률, 감쇠율, 모멘텀, 에폭, (미니)배치 사이즈, 최적화 알고리즘… 이런 모든 것들이 다 DMLP의 하이퍼 파라미터입니다. 하이퍼 파라미터를 어떻게 결정하느냐 또한 모델 성과에 영향을 미칩니다. 따라서, 파라미터를 어떻게 결정하느냐도 대단히 중요한 문제입니다. 문헌에 따르면, 최선의 파라미터를 찾는 방법이 있다고 합니다. MS(Manual Search), GS(Grid Search), RS(Random Search), SMBGO(Sequential Model-Based Global Optimization, 베이지안 방법론), GPA(the Gaussian Process Approach), TSPEA(Tree-Structured Parzen Estimator Approach) 등이 있습니다.​​3.2. Recurrent Neural Network (RNN)The RNN is another type of DL network used for time series or sequential data, such as language and speech. RNNs are also used in traditional ML models (Back Propagation Through Time (BPTT), Jordan–Elman networks, etc.); however, the time periods in such models are generally less than those used in deep RNN models. Deep RNNs are preferred due to their ability to include longer time periods. Unlike Fully Connected Neural Networks (FNNs), RNNs use internal memory to process incoming inputs. RNNs are used to analyze time series data in various fields (handwriting recognition, speech recognition, etc.). As stated in the literature, RNNs are good at predicting the next character in text, language translation applications, and sequential data processing [45,37]RNN은 시계열 혹은 시퀀셜 데이터(순서가 있는 언어나 음성 등)을 다루기 위해 사용되는 또 다른 DL 네트워크 유형입니다. RNN은 전통적인 ML 모델(BPTT; Back Propagation Through Time, Jordan-Elman Network 등)에서도 사용되었습니다. 그러나 이러한 모델에서 사용하는 기간(period)은 일반적으로 Deep RNN에서 사용되는 것보다 짧았습니다. Deep RNN은 더 긴 기간에 대해 포괄할 수 있기에 선호됩니다. 완전연결신경망과 다르게 RNN은 들어오는 입력 데이터를 다시 받아 내부적으로 기억하는 것을 사용합니다. RNN은 다양한 분야(손글씨 인식, 음성 인식 등)에서 시계열 데이터를 분석하는데에 사용됩니다. 문허에 명시되어 있듯이 RNN은 텍스트나 언어 번역 그리고 시퀀셜 데이터를 처리하는데 있어서 다음 문자가 무엇이 될기를 예측하는데 탁월합니다.​The RNN model architecture consists of different numbers of layers and different types of units in each layer. The main difference between RNN and FNN is that each RNN unit takes the current and previous input data at the same time. The output depends on the previous data in the RNN model. RNNs process input sequences one by one at any given time during their operation. The units in the hidden layer hold information about the history of the input in the ‘‘state vector’’. When the output of the units in the hidden layer is divided into different discrete time steps, an RNN is converted into a DMLP [37]. In Fig. 2, the information flow in the RNN’s hidden layer is divided into discrete times. The status of the node S at different times of t is shown as st, the input value x at different times is xt, and the output value o at different times is shown as ot. Parameter values (U, W, V) are always used in the same step.RNN 모델의 아키텍쳐는 서로 다른 수의 계층과 각 계층에 서로다른 유형의 요소들로 구성됩니다. RNN와 FNN 사이의 주요한 차이점은 RNN은 현재와 그 이전의 입력 데이터를 동시에 취한다는 것입니다. 출력은 RNN 모델의 이전 데이터에 따라 달라지리 수 있습니다. RNN은 주어진 시간에 대한 입력 시퀀스를 하나씩 처리합니다. 은닉 계층안의 단위는 입력 데이터의 과거 데이터에 대한 정보를 상태벡터(state vector) 형태로 갖고 있습니다. 은닉계층에 있는 유닛의 출력이 서로 다른 다른 시간 단계로 나누어져 뿌리면, RNN은 DMLP로 변하는 것입니다. 그림2에서, RNN의 은닉계층에서의 information 흐름이 다른 시간으로 뿌려짐을 볼 수 있습니다. 서로 다른 시간t에 대한 노드 S의 상태(status)는 st로 나타내어졌으며, 서로 다른 시간에 대한 입력 데이터x는 xt로 나타내어지고, 출력은 ot로 나타내어집니다. 파라미터 U, W, V 값은 항상 같은 단계에서 똑같이 사용됩니다. RNNs can be trained using the BPTT algorithm. Optimization algorithms (SGD, RMSProp, ADAM) are used for the weight adjustment process. With the BPTT learning method, the error change at time t is reflected in the input and weights of the previous t times. The difficulty of training an RNN is that the RNN structure has a backward dependence over time. Therefore, RNNs become increasingly complex as the learning period increases. Although the main aim of using an RNN is to learn long-term dependencies, studies in the literature show that when knowledge is stored for long time periods, it is not easy to learn with an RNN [57]. To solve this particular problem, LSTMs with different structures of ANN have been developed [37]. Eqs. (8) and (9) illustrate simpler RNN formulations. Eq. (10) shows the total error, which is the sum of the errors from each time iteration.1RNN은 BPTT(Back-Propagation Through Time) 알고리즘을 통해 학습이 수행됩니다. 최적화 알고리즘(SGD, RMSProp, ADAM)은 가중치를 조정하는데 사용됩니다. BPTT 학습 방법을 통하면, t시간에서의 error의 변화가 이전 시간의 input과 가중치에 반영됩니다. RMM을 학습시키는데 어려운 이유는 RNN 구조가 시간에 대한 역방향으로 의존하기 때문입니다. 따라서 RNN은 학습 기간이 길어질수록 복장성은 매우 높아지게 됩니다. RNN을 사용하는 주요한 복표는 장기 의존성을 학습하게 하는 것이었지만, 관련 연구들에 의하면 긴 기간에 걸쳐 학습이 되는게 오히려 RNN의 학습이 쉽지 않게 한다는 것을 보였습니다. 이러한 문제를 해결하기 위해 ANN의 또 다른 종류인 LSTM이 개발되었습니다. 식 8과 9는 간단한 버젼의 RNN의 수식을 보여줍니다. 그리고 식10에선 각각의 시간에서의 error의 변화를 더한것을 보여줍니다. Hyperparameters of the RNN also define the network architecture, and the performance of the network is affected by the parameter choices, as in DMLP case. The number of hidden layers, number of units in each layer, regularization techniques, network weight initialization, activation functions, learning rate, momentum values, number of epochs, batch size (minibatch size), decay rate, optimization algorithms, model (Vanilla RNN, GatedRecurrent Unit (GRU), LSTM), and sequence length are the hyperparameters of RNN. Finding the best hyperparameters for the network is a significant issue. In the literature, there are different methods to find the best hyperparameters: MS, GS, RS, and Bayesian Methods (SMBGO, GPA, TSPEA) [55,56].RMM의 하이퍼 파라미터는 네트워크의 아키텍쳐를 정의하고, 네트워크의 성능은 DMLP의 경우와 마찬가지로 파라미터의 선택에 의해 영향을 받습니다. 은닉층의 갯수, 각 계층에서 뉴런의 갯수, 정규화기법, 가중치초기값, 활성화함수, 학습률, 에폭수, 배치 사이츠, 최적화함수 그리고 시퀀스의 길이를 얼마로 할것이냐 또한 RNN의 하이퍼파라미터입니다. 최적의 하이퍼 파라미터를 찾는것은 굉장히 중요한 일입니다. 관련 연구에 따르면 최적의 하이퍼 파라미터를 찾는 방법들로 MS, GS, RS, SMBGO 등의 베이지안 방법론 등이 있다고 합니다.​​3.3. Long short term memory (LSTM)LSTM [58] is a type of RNN where the network can remember both short term and long term values. LSTM networks are the preferred choice of many DL model developers when tackling complex problems such as automatic speech and handwritten character recognition. LSTM models are mostly used with time-series data. Their applications include Natural Language Processing (NLP), language modeling, language translation, speech recognition, sentiment analysis, predictive analysis, and financial time series analysis [59,60]. With attention modules and AE structures, LSTM networks can be more successful in time series data analysis, such as language translation [59]LSTM은 RNN의 한 유형인데 여기에서는 단기기억 장기기억 둘 다 기억할 수 있는 구조를 갖고 있습니다. LSTM은 DL 모형을 사용해 자동음성인식이나 손글씨 인식 등의 복잡한 문제를 해결하려 할 때 많이들 선택하는 방법입니다. LSTM모델은 대개 시계열 데이터와 함께 사용됩니다. 그래서 자연어처리, 언어 모델, 번역, 음성인식, 감성분석, 금융 시계열 분석 등에 활용되고 있습니다. LSTM을 어텐션모듈과 AE 구조와 함께 사용하면 번역과 같은 시계열 데이터 분석에서 큰 성공을 거두기도 했습니다.​LSTM networks consist of LSTM units. LSTM units merge to form an LSTM layer. An LSTM unit is composed of cells, each with an input gate, output gate, and forget gate. These gates regulate the information flow. With these features, each cell remembers the desired values over arbitrary time intervals. Eqs. (11)–(15) show the form of the forward pass of the LSTM unit [58] (xt: input vector to the LSTM unit, ft: forget gate’s activation vector, it: input gate’s activation vector, ot: output gate’s activation vector, ht: output vector of the LSTM unit, ct: cell state vector, σg : sigmoid function, σc, σh: hyperbolic tangent function, ∗: element-wise (Hadamard) product, W, U: weight matrices to be learned, b: bias vector parameters to be learned) [60].LSTM 네트워크는 LSTM 뉴런(유닛)으로 구성되어있습니다. 그리고 이것들이 모여 LSTM 계층을 형성하는 것입니다. 하나의 LSTM 유닛은 어떤 셀(cell)들로 이루어져 있는데, 각 셀은 input gate, output gate, forget gate 이렇게 구성되어있습니다. 이러한 게이트들은 정보의 흐름을 통제합니다. 이러한 특성으로 각각의 셀은 임의의 시간 간격에서 원하는 값을 기억하고 있는 것입니다. 식 11~15에 걸쳐서 LSTM 유닛이 순전파로 어떻게 흐르는지를 보여주고 있습니다. (xt: 인풋 데이터, ft: 망각게이트의 활성화 벡터, it: 인풋게이트듸 활성화 벡터, ot: output 게이트의 활성화 벡터, ht: LSTM 유닛의 출력 벡터, ct: 셀의 상태 벡터, σg: 시그모이드 함수, σh: 하이퍼볼릭 탄젠트 함수, *: (내적이 아닌)원소별 곱, W,U: 학습되어야할 행렬, b: 편향(바이어스)) LSTM is a specialized version of RNN. Therefore, the weight updates and preferred optimization methods are the same. In addition, the hyperparameters of LSTM are just like those of RNN: number of hidden layers, number of units in each layer, network weight initialization, activation functions, learning rate, momentum values, number of epochs, batch size (minibatch size), decay rate, optimization algorithms, sequence length for LSTM, gradient clipping, gradient normalization, and dropout [60,61]. To find the best hyperparameters of LSTM, the hyperparameter optimization methods used for RNN are also applicable to LSTM [55,56].LSTM은 RNN의 특별한 버젼이기에, 가중치 갱신(update), 선호되는 최적화 알고리즘은 똑같습니다. 또한, 하이퍼 파라미터 또한 같습니다.(하아.. 또 써야된다고?? 에바야~ 암튼 RNN이랑 다 똑같다~)​​3.4. Convolutional neural networks (CNNs)The CNN is a type of DNN that consists of convolutional layers based on the convolutional operation. It is the most common model used for vision and image processing-based classification problems (image classification, object detection, image segmentation, etc.) [62–64]. The advantage of the CNN is the number of parameters compared to vanilla DL models, such as DMLP. Filtering with the kernel window function gives the advantage of image processing to CNN architectures with fewer parameters, which is beneficial for computing and storage. In CNN architectures, there are different layers: convolutional, max-pooling, dropout, and fully connected Multilayer Perceptron (MLP) layer.CNN은 DNN의 한 유형으로써, 합성곱(convolution) 연산을 근간으로하는 합성곱계층을 포함하고 있습니다.이건은 비젼 혹은 이미지 처리를 기반으로하는 분류(이미지 분류, 객체 탐지, 이미지 구분(segmentation) 등)의 문제를 대할때 가장 일반적으로 사용되는 모델입니다. CNN의 장점으로는 그냥 DMLP와 같은 DL 모델에 비해 더 적은 파라미터를 갖는다는 것입니다. 커널(혹은 필터라 부름) 함수를 활용한 필터링은 이미지 처리를 하는데 있어서 CNN의 파라미터 갯수를 적게 하는데 효과적이며, 컴퓨터 연산 혹은 저장 등에 대해 이점을 가져다 줍니다. CNN 구조에서는 여러 계층들이 있습니다: 합성곱 계층, 맥스 풀링 계층, 드롭아웃, 완전연결계층 등​The convolutional layer consists of a convolution (filtering) operation. A basic convolution operation is shown in Eq. (16), where t denotes time, s denotes feature map, w denotes kernel, x denotes input, and a denotes variable. In addition, the convolution operation is implemented on two-dimensional images. Eq. (17) shows the convolution operation for a two-dimensional image, where I denotes input image, K denotes the kernel, (m, n) denotes image dimensions, and i and j denote variables. Consecutive convolutional and max-pooling layers construct the deep network. Eq. (18) describes the NN architecture, where W denotes weights, x denotes input, b denotes bias, and z denotes the output of neurons. At the end of the network, the softmax function is used to obtain the output. Eqs. (19) and (20) illustrate the softmax function, where y denotes output [39].합성공 계층은 합성곱(필터링) 연산을 수행합니다. 기본적인 합성공 연산은 식 16에 나타내어져 있습니다. 여기에서 t는 시간, s는 특성맵, w는 커널, x는 input, a는 변수를 의미합니다. 더불어서 합성공 연산은 2차원 이미지 상에서 수행됩니다. 식 17에서는 2차원 이미지에 대한 합성곱 연산을 나타냅니다. I는 입력 이미지, K가 커널, (m,n)이 이미지의 차원, 그리고 i와 j가 변수를 나타냅니다. 합성곱 계층과 맥스 풀링 계층을 연속적으로 구조화 시켜 deep 네트워크를 형성합니다. 식 18에서는 싱경망 구조를 표현하고 있습니다. W가 가중치, x는 입력데이터, b가 편향, z는 뉴런의 출력을 나타냅니다. 네트워크의 가장 끝단에서는 소프트 맥스 함수가 출력을 모두 취합합니다. 식 19와 20은 y가 출력일때의 소프트맥스 함수를 나타내고 있습니다. The backpropagation process is used for CNN model learning. Most commonly used optimization algorithms (SGD, RMSProp) are used to find optimum CNN parameters. Hyperparameters of the CNN are similar to other DL model hyperparameters: number of hidden layers, number of units in each layer, network weight initialization, activation functions, learning rate, momentum values, number of epochs, batch size (minibatch size), decay rate, optimization algorithms, dropout, kernel size, and filter size. To find the best CNN hyperparameters, the following search algorithms are commonly used: MS, GS, RS, and Bayesian methods. [55,56].CNN 모델에서도 역전파 절차를 통해 학습합니다. 가장 일반적으로 사용되는 최적화 알고리즘(SGD, RMSProp)들은 CNN 파라미터를 최적화하는데도 사용됩니다. CNN의 하이퍼 파라미터 또한 다른 DL모델들의 하이퍼파라미터와 비슷합니다. 은닉층갯수, …………………….., 필터의 사이즈. CNN의 최적의 하이퍼 파라미터를 찾아내기 위해서는 대개 MS, GS, RS, 베이지안 방법론 등이 사용됩니다.​​3.5. Restricted Boltzmann Machines (RBMs)An RBM is a productive stochastic ANN that can learn a probability distribution on the input set [65]. RBMs are mostly used for unsupervised learning [66]. RBMs are used in applications such as dimension reduction, classification, feature learning, and collaborative filtering [67]. The advantage of RBMs is their ability to find hidden patterns in an unsupervised manner. The disadvantage of RBMs is its difficult training process. ‘‘RBMs are tricky because although there are good estimators of the log-likelihood gradient, there are no known cheap ways of estimating the log-likelihood itself’’ [68].Restricted Boltzmann Machines(RBMs)는 입력 데이터에서 확률 분포를 학습할 수 있는 productive stochastic ANN입니다. RBM은 대개 비지도 학습의 영역에서 사용됩니다. RBM은 또한 차원축소, 분류, 특성 학습, collaborative filtering과 같은 영역에서도 사용됩니다. RBM의 장점은 비지도학습의 방식에서 숨겨진 패턴을 찾아내는데 탁월하다는 점입니다. 하지만, 이거스이 단점으로는 학습의 절차자 대단히 어렵다는 것입니다. RBM은 로그 likelihood gradient에 대한 좋은 추정식이 있지만, 이것을 간편하게 해결할 수 있는 방법은 알려져있지 않기 때문에 사용하기 까다롭습니다.​An RBM is a two-layer, bipartite, and undirected graphical model that consists of two layers: visible and hidden (Fig. 3). The layers are not connected among themselves. Each cell is a computational point that processes the input and makes stochastic decisions about whether this nerve node will transmit the input. Inputs are multiplied by specific weights, certain threshold values (bias) are added to input values, and then the calculated values are passed through an activation function. In the reconstruction stage, the results from the outputs re-enter the network as input before finally exiting the visible layer as output. The values of the previous input and values after the processes are compared. The purpose of this comparison is to reduce the difference.RBM은 두 개의 계층으로 구성돼 있으며, 방향성 없는 그래프 모델로써 보이는 계층과 보이지 않는 계층 두 계층으로 양분됩니다. 그리고 계층들끼리 다시 연결되지 않습니다. 각각의 cell은 계산하는 절차를 진행하는데, 들어온 입력 데이터에 대해 확률적인 결정을 통해 이걸 전달 할지 말지를 정합니다. 입력 데이터는 특정한 가중치 값이 곱해지고, 여기에 측정한 입계값(편향)이 더해집니다. 그리고나서 활성화 함수에게로 전달되는 것입니다. 재구축(reconstruction) 단계에서 출력의 결과가 다시 네트워크의 입력으로 들어가고, 최종 결과가 visible 계층에서 아웃으로 나오게 됩니다. 이전의 입력데이터와 절차가 계산된 이후에 결과값이 서로 비교되고, 이러한 비교를 통해 둘의 차이를 줄이는 것을 목적으로 합니다. Eq. (21) illustrates the probabilistic semantics for an RBM using its energy function, where P denotes the probabilistic semantics for an RBM, Z denotes the partition function, E denotes the energy function, h denotes hidden units, and v denotes visible units. Eq. (22) illustrates the partition function or normalizing constant. Eq. (23) shows the energy of a configuration (in matrix notation) of a standard RBM with binary-valued hidden and visible units, where a denotes bias weights (offsets) for the visible units, b denotes bias weights for the hidden units, W denotes matrix weight of the connection between hidden and visible units, T denotes the transpose of matrix, v denotes visible units, and h denotes hidden units [69,70].식21에서는 에너지 함수를 이용하여 RBM에게 확률론적 의미를 부여하는 것을 나타내고 있습니다. 여기서 P는 RBM에 있어서의 확률의 의미이고, Z는 분배함수입니다. E는 에너지함수이고, h가 은닉층의 뉴런, v가 visible 계층의 뉴런입니다. 식 22는 분배함수 혹은 정규화시킨 상수 정도의 의미를 보여줍니다. 식 23에서는 binary(0과1) 값을 기반으로 한 은닉층과 가시층(visible)으로 이루어진 표준적인 RBM 환경에서의 에너지를 나타냅니다. a는 가시층의 편향 가중치를, b는 은닉층의 편향가중치를, W는 은닉층과 가시층 사이 연결에 대한 가중치 핼영을, T는 행렬의 transpose를, v는 visible의 뉴런, h는 은닉층의 뉴런을 의미합니다.(와 이건 기초를 공부하지 않은 부분이라 진짜 뭔소린지 하나도 모르겠다) The learning is performed multiple times on the network [65]. The training of RBMs is implemented by minimizing the negative log-likelihood of the model and data. The Contrastive Divergence (CD) algorithm is used as the stochastic approximation algorithm, which replaces the model expectation using an estimation using Gibbs Sampling with a limited number of iterations [66]. In the CD algorithm, the Kullback Leibler Divergence (KL-Divergence) algorithm is used to measure the distance between its reconstructed probability distribution and the original probability distribution of the input [71].네트워크 상에서의 학습은 여러번 수행됩니다. RBM의 학습은 모델과 데이터의 음의 로그-likelihood를 최소화시키는 방향으로 진행됩니다. Contrastive Divergence(CD)라는 알고리즘이 확률전 근사 알고리즘으로써 사용됩니다. 이 알고리즘에서는 모델의 기대값을 제한된 횟수로 Gibbs Sampling을 사용해 추정치로 만들어내어 사용합니다. CD 알고리즘에서 Kullback Leibler Divergence (KL-Divergence)라는 알고리즘이 입력 데이터의 재설정된(reconstructed) 확률분포와 원래의 확률분포 사이의 거리를 측정하는데 사용됩니다.​Momentum, learning rate, weight-cost (decay rate), batch size (minibatch size), regularization method, number of epochs, number of layers, initialization of weights, size of visible units, size of hidden units, type of activation units (sigmoid, softmax, ReLU, Gaussian units), loss function, and optimization algorithms are the hyperparameters of RBMs. Similar to other deep networks, the hyperparameters are searched with MS, GS, RS, and Bayesian methods (Gaussian process). In addition to these, Annealed Importance Sampling (AIS) is used to estimate the partition function. The CD algorithm is also used for the optimization of RBMs [55,56,72,73].모멘텀, 학습률, 감쇠율, 배치사이즈, 규제방법, 에폭의수, 계측의 수, 가중치 초깃값, 가시층 뉴런의 수, 은닉증 유런의 수, 활성화함수(시그모이드, 소프트맥스, ReLU, 가우시안 등), 손실함수, 최적화 알고리즘 등이 RBM의 하이퍼 파라미터들입니다. 다른 딥러닝 모델들과 미슷하게, 최적의 하이퍼 파라미터는 MS, GS, RS, 그리고 베이지안 방법론 등을 통해 찾아냅니다. 여기에 더불어 AIS(Annealed Importance Sampling)이라는 것이 분배함수의 추정치를 만들어내기 위해 사용됩니다. 또한, RBM의 최적화를 위해 CD알고리즘이 사용됩니다.​​3.6. Deep Belief Networks (DBNs)A DBN is a type of deep ANN consisting of a stack of RBM networks (Fig. 4). A DBN is a probabilistic generative model that consists of latent variables. In a DBN, there is no link between units in each layer. DBNs are used to find discriminate independent features in the input set using unsupervised learning [69]. The ability to encode higher-order network structures and fast inference are the advantages of DBNs [74]. DBNs have the same disadvantages as RBMs because DBNs are composed of RBMs.ANN의 또 다른 형태로 RBM을 쌓아올린 DBN(Deep Belief Network) 형태도 있습니다. DBN은 확률적 생성모델인데, 잠재적인 변수들로 구성되어있습니다. DBN은 각 계층에서 뉴런사이의 연결이 없습니다. 이러한 DBN은 비지도 학습을 사용해 입력데이터 상에서 독립적인 feature를 찾아내는데 사용됩니다. 고차원의 네트워크 구조를 해석할 수 있는 능력과 빠른 추론능력 등이 DBN의 장점으로 꼽을 수 있습니다. DBN의 단점은 RBM과 똑같은데 그 이유는 DBN이 RBM으로 구성되어있기 때문입니다.​When a DBN is trained in an unsupervised manner, it can learn to reconstruct the input set in a probabilistic manner. Then, the layers in the network begin to detect discriminating features in the input. After this learning step, supervised learning is conducted for classification [75]. Eq. (24) illustrates the probability of generating a visible vector (W: matrix weight of connection between hidden unit h and visible unit v, p(h|W): prior distribution over hidden vectors) [69].DBN이 비지도의 형태로 학습이 될때, ㄹ방법을 학습할 수 있습니다. 재구성하는 방법을 학습할 수 있습니다. 그러면, 네트워크에서의 계층은 입력 데이터의 feature(특성)을 구분하기 시작합니다. 이 학습의 절차를 지난 후에 분류를 하는 지도학습이 실행됩니다. 식 24에서는 가시층에서의 벡터의 확률을 나타냅니다. (W는 은닉층과 가시층 뉴런 사이를 잇는 가중치 행렬, p(h|W)는 은닉층에 걸친 사전분포) The DBN training process can be divided into two steps: stacked RBM learning and backpropagation learning. In stacked RBM learning, an iterative CD algorithm is used [66]. In backpropagation learning, optimization algorithms (SGD, RMSProp, ADAM) are used to train the network [74]. The hyperparameters of a DBNs are similar to those of an RBM. Momentum, learning rate, weight-cost (decay rate), regularization method, batch size (minibatch size), number of epochs, number of layers, initialization of weights, number of RBM stacks, size of visible units in RBMs’ layers, size of hidden units in RBMs’ layers, type of units (sigmoid, softmax, rectified, Gaussian units, etc.), network weight initialization, and optimization algorithms are the hyperparameters of DBNs. Similar to other deep networks, the hyperparameters are searched with MS, GS, RS, and Bayesian methods. The CD algorithm is also used for the optimization of DBNs [55,56,72,73].DBN의 훈련 절차는 2개의 절차로 나눌 수 있습니다. RBM의 학습을 쌓고, 역전파하여 학습하는 것입니다. RBM을 쌓아 훈련하는 과정에서는 CD알고리즘을 반복적으로 사용합니다. 그리고 역전파 학습의 과정에서는 최적화 알고리즘들(SGD, RMSProp, ADAM)을 사용하여 네트워크를 훈련합니다. DBN의 하이퍼파라미터는 RBM과 비슷합니다. (와~~~~ 또 똑같은거 또 쓴다 얘!!!!!!!!! 에바야~~~)​​3.7. Autoencoders (AEs)AE networks are ANNs used as unsupervised learning models. In addition, AE networks are commonly used in DL models, wherein they remap the inputs (features) such that the inputs are more representative for classification. In other words, AE networks perform an unsupervised feature learning process, which fits very well with the DL framework. A representation of a dataset is learned by reducing the dimensionality with AEs. AEs are similar to Feedforward Neural Networks (FFNNs)’ in their architecture. They consist of an input layer, an output layer, and one or more hidden layers that connect them together. The number of nodes in the input layer and the number of nodes in the output layer are equal to each other in AEs, and they have a symmetrical structure. The most notable advantages of AEs are dimensionality reduction and feature learning. Meanwhile, reducing dimensionality and feature extraction in AEs cause some drawbacks. Focusing on minimizing the loss of data relationship in the encoding of AEs causes the loss of some significant data relationships. Hence, this may be considered as a drawback of AEs[76].AE(AutoEncoder) 네트워크는 비시도 학습 모델로써 사용하는 ANN입니다. 여기에 더불어 AE 네트워크는 DL모델 안에서도 사용되는 것이며, 안쪽에서 입력 데이터에 대해 분류를 하는데 더욱 표현력있는 입력 데이터를 remap합니다. 다시말해, AE 네트워크는 DL모델에 매우 적합한 비지도 특성 학습 절차를 수행하는 것입니다. 데이터셋의 대표성은 AE를 통해 차원을 축소시킴으로써 학습됩니다. AE는 구조적으로 FFNN(FeedForward Neutal Network)와 비슷합니다. 입력 계층과 출력계층 그리고 하나 이상의 은닉계층으로 구성되어있습니다. AE에서 입력계층의 노드의 갯수와 출력 계층의 노드의 갯수는 서로 같고, 대칭적인 구조를 갖고 있습니다. AE의 가장 눈에띄는 장점은 차원 축소와 특성 학습입니다. 한편 차원축소와 특성 추출은 단점을 야기하기도 하는데, AE의 인코딩에서 데이터 관계성에 대한 손실을 최소화하는데 집중하는 것에서 상당한 데이터 관계의 손실이 일어날 수도 있다는 것입니다. 따라서, 이러한 단점을 꼭 고려해야할 것입니다.​In general, AEs contain two components: encoder and decoder. The input x ∈ [0, 1]d is converted through function f (x) (W1 denotes a weight matrix, b1 denotes a bias vector, σ1 elementwise sigmoid activation function of the encoder). Output h is the encoded part of the AEs (code), latent variables, or latent representation. The inverse of function f (x), called function g(h), produces the reconstruction of output r (W2 denotes a weight matrix, b2 denotes a bias vector, and σ2 is an element-wise sigmoid activation function of the decoder). Eqs. (25) and (26) illustrate the simple AE process [77]. Eq. (27) shows the loss function of the AE, the Mean Squared Error (MSE). In the literature, AEs have been used for feature extraction and dimensionality reduction [39,77].일반적으로 AE는 인코더와 디코더 두개로 구성되어있습니다. 입력 x는 [0,1]의 범위내의 값으로 함수 f(x)를 통해 전환됩니다. (W는 가중치, b는 편향, 인코더로써의 시그마는 시그모이드 활성화함수). 출력 h는 AE의 인코딩된 부분으로써 잠재적인 변수, 혹은 잠재 대표성으로 볼 수 있습니다. f(x)의 역함수 g(h)는 출력r의 재구축을 진행시킵니다. 식 25와 26은 간단한 AE 절차를 나타냅니다. 식 27에서는 AE의 손실함수를 보이고 있으며, MSE로 측정을 하는 형태입니다. 관련 연구에 따르면, AE는 특성 추출과 차원축소에 사용된다고 합니다. AEs are a specialized version of FFNNs. The backpropagation learning is used for updating the weights in the network [39]. Optimization algorithms (SGD, RMSProp, ADAM) are used for the learning process of AEs. MSE is used as a loss function in AEs. In addition, recirculation algorithms may also be used for the training of AEs [39]. AEs’ hyperparameters are similar to those of DL hyperparameters. Learning rate, weight-cost (decay rate), dropout fraction, batch size (minibatch size), number of epochs, number of layers, number of nodes in each encoder layer, type of activation functions, number of nodes in each decoder layers, network weight initialization, optimization algorithms, and number of nodes in the code layer (size of latent representation) are the hyperparameters of AEs. Similar to other deep networks, the hyperparameters are searched with MS, GS, RS, and Bayesian methods [55,56].AE는 FFNN의 특수한 형태입니다. 역전파학습을 통해 네트워크의 가중치를 업데이트 합니다. 최적화함수(SGD, RMSProp, ADAM) 등이 AE의 학습을 진행하는데 사용되며, 손실할수로는 MSE가 사용됩니다. 더불어서 순환 알고리즘이 훈련하는데 사용될 수 있습니다. AE의 하이터 파라미터는 DL의 하이퍼 파라미터와 비슷합니다.(우다아아다다ㅏ아아아다아아아다아ㅏ다 다 똑같네~)​​3.8. Deep reinforcement learning (DRL)Reinforcement learning (RL) is a type of learning that differs from supervised and unsupervised learning models. It does not require a preliminary dataset that has been labeled or clustered before. RL is an ML approach inspired by learning action/behavior, which deals with what actions should be taken by subjects to achieve the highest reward in an environment. There are different areas in which it is used: game theory, control theory, multi-agent systems, operations research, robotics, information theory, investment portfolio management, simulation-based optimization, playing Atari games, and statistics [78]. Some advantages of using RL for control problems are that an agent can be easily re-trained to adapt to changes in the environment and that the system is continually improved while training is constantly performed. An RL agent learns by interacting with its surroundings and observing the results of these interactions. This learning method mimics the basics of how humans learn.RL(Reinforcement Learning, 강화학습)은 지도학습과 비지도학습의 학습 모델과는 다른 유형의 형태입니다. 여기에서는 데이터셋에 대한 예비적인 절차를 요하지 않습니다. 뭐 라벨링이라든지 클러스터링을 한다든지 하는 것이요. RL은 action/behavior 학습에 영감을 얻어 만들어진 ML 접근법으로, 어떤 행동을 취해야 가장 높은 보상을 받을 수 있는지를 다룹니다. 강화학습이 사용되는 영역을 많이 있습니다. game theory, control theory, multi-agent systems, operations research, robotics, information theory, investment portfolio management, simulation-based optimization, playing Atari games, and statistics 등이 있습니다. 제어하는 문제에 대해 강화학습을 사용하는 것의 장점은 agent(행위자? 그냥 agent라고 해야지)들이 환경 변화에 맞게 쉽게 재훈련되어 적응하고, 훈련이 지속적으로 진행되는 동안 시스템도 계속 개선된다는 점이 있습니다. 강화학습의 agent는 주변 및 관찰되는 상호작용의 결과를 바탕으로 학습합니다. 이러한 학습 방법은 사람이 학습하는 것을 모방한 것입니다.​RL is mainly based on a Markov Decision Process (MDP). A MDP is used to formalize the RL environment. A MDP consists of five tuples: state (finite set of states), action (finite set of actions), reward function (scalar feedback signal), state transition probability matrix (p(s′,r|s, a), where s′ denotes next state, r denotes reward function, s denotes state, and a denotes action), and discount factor (γ , present value of future rewards). The aim of the agent is to maximize the cumulative reward. The return (Gt) is the total discounted reward. Eq. (28) illustrates the total return, where R denotes rewards, t denotes time, and k denotes a variable in time.강화학습은 주로 마르코프 의사결정 프로세스(MDP; Markov Decision Process)를 기반으로 하고 있습니다. MDP는 5개의 튜플로 구성되어 있습니다. state(유한한 갯수의 상태들), action(유한한 행동들), 보상함수(feedback 신호 값), 상태 변경 확률 행렬(p(s′,r|s, a) 에서 s’은 다음 상태를, r은 보상함수, s가 상태, a가 행동을 의미합니다), 할인율(γ, 미래 보상에 대한 현재가치) 이렇게 5개의 집합입니다. agent의 목표는 누적 보상을 최대화하는 것입니다. return(Gt)는 총 보상의 형재가치이며, 식 28에서 나타내고 있습니다. 여기에서 R은 보상을, t는 time을 의미합니다. The value function is the prediction of future values. It provides information on how good the state/action is. Eq. (29) illustrates the formulation of the value function, where v(s) denotes the value function, E[.] denotes the expectation function, Gt denotes the total discounted reward, s denotes the given state, R denotes the rewards, S denotes the set of states, and t denotes time.가치함수는 미래 가치를 예측합니다. 이는 상태/행동이 얼마나 좋은지에 대한 정보를 나타냅니다. 식 29에서는 가치 함수의 식을 나타내고 있습니다. 여기에서 v(s)는 가치함수를, E[.]는 기대함수, Gt는 총 보상의 현재가치, s는 주어진 상태, R이 보상, s가 상태의 집합, t가 시간을 의미합니다. Policy (π) is the agent’s behavior strategy. It is like a map from state to action. There are two types of value functions to express the actions in the policy: state–value function (vπ (s)) and action–value function (qπ (s, a)). The state–value function (Eq. (30)) is the expected return of starting from s to following policy π (Eπ [.] denotes expectation function). The action–value function (Eq. (31)) is the expected return of starting from s and taking action a to following policy π (A denotes the set of actions and a denotes the given action).정책(π)은 agent의 행동 전략입니다. 이는 상태에서 행동을 취하는데 까지에 대한 지도와 같습니다. 정책에서 행동을 표현하는 가치 함수의 형태는 두가지가 있습니다. 상태-가치 함수(vπ (s))와 행동-가치함수 (qπ (s))입니다. 상태-가치 함수(식30)은 시작점 s에서부터 정책 π 를 따랐을 때 기대되는 return을 나타내고 있습니다. 행동-가치 함수(식31)은 시작점s에서부터 정책 π를 따르기 위해 행동a를 취했을 때 기대되는 return의 기댓값입니다. The optimal state–value function (Eq. (32)) is the maximum value function over all policies. The optimal action–value function (Eq. (33)) is the maximum action–value function over all policies.최적의 상태-가치 함수(식32)는 모든 정책 전반에서 가치 함수를 최대화하는 것입니다. 행동-가치 함수(식33)의 최적은 행동-가치 함수를 정책 전반에서 최대화하는 것입니다. The RL solutions and methods in the literature are too broad to review in this paper. Therefore, we summarize the important issues of RL and important RL solutions and methods. RL methods can mainly be divided into two types: model-based methods and model-free methods. Model-based methods use a model that is known by the agent before, value/policy, and experience. The experience can be real (sample from the environment) or simulated (sample from the model). Model-based methods are mostly used in the applications of robotics and control algorithms [79]. Modelfree methods are mainly divided into two groups: value-based and policy-based. In value-based methods, a policy is produced directly from the value function (e.g., epsilon-greedy). In policybased methods, the policy is parametrized directly. In valuebased methods, there are three main solutions for MDP problems: Dynamic Programming (DP), Monte Carlo (MC), and Temporal Difference (TD).관련 문헌에서 RL의 solution과 방법론은 여기에서 리뷰하기엔 너무 광범위합니다. 따라서 RL의 중요한 이슈와 중요한 solution 및 방법론을 요약하는 것으로 하겠습니다. 강화학습 방법론은 두가지 타입으로 나눌 수 있습니다. model-based 방법론과, model-free 방법론으로 나눠볼 수 있습니다. model-based 방법론은 기존에 agent가 알고있는 가치/정책, 경험을 사용하는 모델을 씁니다. 여기서 경험은 환경으로부터 sample이 경험한 것일수도 있고, sample이 모델로부터 가상적인 것일수도 있습니다. model-based 방법른 주로 로보틱스나 제어 알고리즘의 활용에서 사용됩니다. model-free 방법론은 또 두가지 그룹으로 나누어집니다. value-based와 policy-based입니다. value-based 방법론에서는 정책은 가치 함수로부터 직접적으로 만들어집니다(즉, 입실론-탐욕). policy-based 방법론에서는 정책이 매개변수가 됩니다. value-based 방법에서는 MDP 문제에 대한 주요한 솔루션은 3개로 DP(Dynamic Programming), MC(Monte Carlo), TD(Temporal Difference)가 있습니다.​In the DP method, problems are solved with optimal substructure and overlapping subproblems. The full model is known and is used for planning in MDP. There are two iterations (learning algorithms) in DP: policy iteration and value iteration. The MC method learns experience directly by running an episode of game/simulation. MC is a type of model-free method that does not require MDP transitions/rewards. It collects states and takes the mean of returns for the value function. TD is also a modelfree method that learns the experience directly by running the episode. In addition, TD learns incomplete episodes like the DP method by using bootstrapping. The TD method combines the MC and DP methods. SARSA (state, action, reward, state, action; St, At, Rt, St+1, At+1) is a type of TD control algorithm. Q-value (action–value function) is updated with the agent actions. It is an on-policy learning model that learns from actions according to the current policy π. Eq. (34) illustrates the update of the action–value function in the SARSA algorithm, where St denotes current state, At denotes current action, t denotes time, R denotes reward, α denotes learning rate, γ denotes discount factor. Q-learning is another TD control algorithm. It is an off-policy learning model that learns from different actions that do not require the policy π at all. Eq. (35) illustrates the update of the action–value function in the Q-Learning algorithm (the whole algorithm is described in [78], a′ denotes action).Dynamic Programming(DP) 방법론은 최적의 구조와 겹치는 하위 문제를 통해 문제를 풀고자합니다. 전체 모델은 주어져있고, DMP에서 계획에 사용됩니다. DP에서는 학습 알고리즘으로 정책 반복과 가치 반복 이렇게 두가지 반복절차가 있습니다. MC 방법론은 게임/시뮬레이션의 에피소드로부터 직접적으로 경험을 학습합니다. MC방법은 일종의 model-free 방법론으로써 DMP stransition/reward가 필요하지 않습니다. 그저 상태를 취합하고, 가치 함수에 대한 return의 평균을 취하는 것입니다. TD방법 또한 model-free 방법으로써 에피소드로부터 직접적으로 경험을 학습합니다. 여기에 더불어 TD는 부트스트래핑 방법을 사용하여 DP처럼 완전하지 않은 에피소드도 학습합니다. TD방법론은 MC와 DP를 혼합한 것입니다. SARSA(State, Action, Reward, State+1, Action+1)는 TD 제어 알고리즘의 한 형태입니다. Q-value(행동-가치 함수)는 agent의 행동과 함께 업데이트 됩니다. 현재 주어진 정채에 따른 행동으로부터 학습하는 정책 기반으로 학습하는 모뎁인것입니다. 식 34에서는 SARSA 알고리즘에서 행동-가치 함수의 업데이트를 나타내고 있습니다. St는 현재 상태, At는 현재 행동, t가 시간, R이 보상, 알파는 학습률, 감마는 할인율을 나타냅니다 Q-Learning은 TD 제어 알고리즘의 또 다른 형태입니다. 이는 정책을 요하지 않은 서로 다른 행동으로부터 학습하는 off-policy 모델의 종류입니다. 식 35에서는 Q-learning 알고리즘에서의 행동-가치 함수의 갱신을 나타내고 있습니다.[와 진짜 뭔소린지 하나도 모르겠다.. 강화학습에 대해선 Basic을 좀 공부하고 다시 봐야겠음] In the value-based methods, a policy can be generated directly from the value function (e.g., using epsilon-greedy). Policy-based methods use the policy directly instead of using the value function. It has both advantages and disadvantages over value-based methods. The policy-based methods are more effective in highdimensional or continuous action spaces and have better convergence properties than value-based methods. It can also learn stochastic policies. On the other hand, policy-based methods evaluate a policy that is typically inefficient and has high variance. It typically converges to a local rather than global optimum. In the policy-based methods, there are also different solutions: Policy gradient, Reinforce (Monte Carlo Policy Gradient), and Actor-Critic [78] (details of policy-based methods can be found in [78]).가치기반의 방법론에서는 정책이 가치함수로부터 직접적으로 생성됩니다(즉, error-greedy(에러탐욕이라 해야하나)을 사용하는 것). 정책 기반 방법론을 가치 함수를 이용하는 대신에 정책을 직접적으로 사용합니다. 이는 가치함수 방법론에 대비해 장점도 단점도 있는데, 정책기반 방법은 고차원이나 혹은 연속적인 행동 집합에서 더 효과적이며, 수렴하는 성질이 가치 기반 방법보다 더 낫습니다. 또한 이는 확률적 정책을 학습할 수 있습니다. 반면, 정책 기반 방법론은 전형적으로 효율적이지 않은 정책을 평가하며, 높은 변동성을 갖습니다. 그리고 대개 global 최적보단 local 최적에 빠지기 쉽상입니다. 그래도 정책 기반 방법론에 대해서 솔류션이 있기 마련입니다. 정책기울기(policy gradient), 몬테카를로 정책 기울기, Actor-Critic 등이 있습니다.​DRL methods contain NNs. Therefore, DRL hyperparameters are similar to the DL hyperparameters. Learning rate, weight-cost (decay rate), dropout fraction, regularization method, batch size (minibatch size), number of epochs, number of layers, number of nodes in each layer, type of activation functions, network weight initialization, optimization algorithms, discount factor, and number of episodes are the hyperparameters of DRL. Similar to other deep networks, the hyperparameters are searched with MS, GS, RS and Bayesian methods [55,56].DRL 에는 신경망 구조를 포함합니다. 따라서 DRL 하이퍼 파라미터는 DL의 하이퍼 파라미터와 비슷합니다. 우와아아아아아 다~~~ 똑같다~~~ 똑같은거 또 쓰기 싫다~~~~​​​4. Financial time series forecastingThe most widely studied financial application area is forecasting of a given financial time series, particularly asset price forecasting. Even though some variations exist, the main focus is on predicting the next movement of the underlying asset. More than half of the existing implementations of DL are focused on this area. Even though there are several subtopics of this general problem, including stock price forecasting, index prediction, forex price prediction, commodity (oil, gold, etc.) price prediction, bond price forecasting, volatility forecasting, cryptocurrency price forecasting, the underlying dynamics are the same in all of these applications.금융에 대한 영역에서 가장 광범위하게 연구된 영역으로는 단연 주어진 금융 시계열로부터의 예측일 것입니다. 특히, 자산 가격의 예측이겠죠. 여러부분에서 활용하는 것이 다르기야 하겠지만, 주로 초점이 맞춰지는 부분은 해당 자산이 이제 이 다음 어디로 움직일지에 대한 예측에 대한 부분입니다. DL이 활용되고 있는 사례들 중 절반 이상이 이 부분에 집중되어있습니다. 이러한 일반적인 것에 대한 여러 하위 주제들, 가령 주가예측, 지수예측, 변동성 예측, 코인가격 예측 등.. 많기야 하겠지만, 내제되어있는 매커니즘은 다 똑같을 것입니다.​Studies can also be clustered into two main groups based on their expected outputs: price prediction and price movement (trend) prediction. Although price forecasting is essentially a regression problem, in most financial time series forecasting applications, correct price prediction of the price is not perceived to be as important as correctly identifying the directional movement. As a result, researchers consider trend prediction, i.e., forecasting which way the price will change, a more crucial study area compared with exact price prediction. In that sense, trend prediction becomes a classification problem. In some studies, only up or down movements are taken into consideration (2-class problem), although 3-class problems also exist (up, down, or neutral movements).이러한 연구들을 가격예측이냐 가격의 움직임(추세)예측이냐로 두 가지로 크게 나눠 볼 수 있습니다. 가격 예측은 본질적으로 회귀 문제이긴 하지만, 대부분의 금융 시계열 예측에서 정확한 가격 예측은 방향성을 정확하게 파악하는 것 만큼 중요하게 인식되고 있진 않습니다. 따라서, 연구자들은 추세 예측, 즉, 가격이 어느 방향으로 변할것인지를 예측하는 것이 더 핵심적인 연구의 영역이라 볼 수 있습니다. 이러한 의미에서 추세예측은 분류문제가 되기도 합니다. 어떤 연구에서는 두 가지 카테고리를 분류하는 up이냐 down이냐를 구분하는 것으로 하기도 하고, 또 어떤 데에서는 up/down/중립 이렇게 3가지로 분류하는 문제로 생각하기도 합니다.​LSTM and its variations along with some hybrid models dominate the financial time series forecasting domain. LSTM, by its nature, utilizes the temporal characteristics of any time series signal; hence, forecasting financial time series is a well-studied and successful implementation of LSTM. However, some researchers prefer to either extract appropriate features from the time series or transform the time series such that the resulting financial data become stationary from a temporal perspective, meaning even if we shuffle the data order, we will still be able to properly train the model and achieve successful out-of-sample test performance. For those implementations, CNN and Deep Feedforward Neural Network (DFNN) are the most commonly chosen DL models.LSTM과 LSTM의 응용 버젼(하이브리드로 다른것과 섞은거)는 금융 시계열 예측의 영역에서 지배적입니다. LSTM은 본질적으로 모든 시계열 상에서의 기간마다의 성격을 활용합니다. 즉, 금융 시계열 예측은 오래 연구가 되어왔으며 또한 성공적으로 구현된 영역이라고 할 수도 있습니다. 하지만, 일부 연구자들은 시계열로부터 적절한 특성을 추출하거나 혹은 temporal한 관점에서 stationary한 금융 데이터로 변환하여, 데이터의 순서를 이리저리 랜점하게 뒤바꿔도, 모델을 적절하게 학습시킬 수 있고, OOS 테스트에서도 성공적인 성능을 내는 것을 선호하기도 합니다. 이러한 구현을 위해서 주로 사용되는 DL모델은 CNN과 DFNN(Deep Feedforward Neural Network)입니다.​Various financial time series forecasting implementations using DL models exist in literature. We will cover each of them in the following subsections. In this survey paper, we examine the papers using the following criteria: First, we group articles according to their subjects. Then, we group related papers according to their feature set. Finally, we group each subgroup according to DL models/methods.DL 모델을 사용하여 금유 시계열 예측을 다양한 방식으로 구현한 연구들이 있습니다. 저희는 이 각각에 대해서 아래의 하위 섹션에서 다룰 것입니다. 여기 서베이 논문에서는, 아래의 기준을 사용하여 관련 논문들을 볼 것입니다. 먼저, 논문이 무엇을 주제로 잡고 있는지에 따라 그룹을 하고, 특징에 따라 관련있는 연구들끼리 그룹을 한다음, 마지막으로 DL모델 혹은 방법에 따라 각 하위 그룹을 나눌 것입니다.​For each implementation area, the related papers are subgrouped and tabulated. Each table contains the following fields to provide information about the implementation details for the papers within the group: Article (Art.) and Data Set are trivial, Period refers to the time period for training and testing. Feature Set lists the input features used in the study. Lag is the time length of the input vector (e.g., 30 d means the input vector has a 30 day window), and horizon shows how far into the future the model predicts. Some abbreviations are used for the two aforementioned fields: min is minutes, h is hours, d is days, w is weeks, m is months, y is years, s is steps, and * is mixed. Method shows the DL models that are used in the study. Performance criteria provides the evaluation metrics, and Environment (Env.) lists the development framework/software/tools. Some column values might be empty, indicating there was no relevant information in the paper for the corresponding field.각 활용의 영역에서 관련된 연구들이 하위 그룹화되어 표로 정리되어있습니다. 각 표에는 그룹 내 세부정도에 따라 아래의 Field 가 포함되어있습니다. Article / Dataset / Period(훈련과 테스트에 대한 기간), Feature Set(입력되는 특성), Lag(입력 벡터의 시간 길이(예를들어 30d는 30일의 길이를 가진 입력 벡터)), Horizon(얼마나 먼 미래를 예측하는지). 그리고 몇가지 약어가 사용될 것인데요. min은 분, h는 시간, d는 일, w는 주, m은 월, y는 연, s는 스텝, *은 혼용을 의미합니다. Method란 연구에서 사용된 DL모델을 말합니다. 성능의 기준은 평가지표를 나타내며, 환경(Env.)는 개발에 있어서의 프레임워크/소프트웨어/도구를 말합니다. 몇몇의 열은 비어있을 수도 있는데, 이는 관련된 정보다 딱히 없다는 것을 의미합니다.​​4.1. Stock price forecastingPrice prediction of any given stock is the most studied financial application of all. We observed the same trend within DL implementations. Depending on the prediction time horizon, different input parameters are chosen, varying from High Frequency Trading (HFT) and intraday price movements to daily, weekly, or even monthly stock close prices. Also, technical, fundamental analysis, social media feeds, and sentiment are among the different parameters used for the prediction models.특정 주식에 대한 가격 예측은 가장 많이 연구된 금융의 영역일 것입니다. DL의 활용 측면에서도 비슷하다는 것을 확인했습니다. 예측 기간을 얼마냐로 할 것인지에 따라 선택되는 입력 파라미터가 다를 것이고, HFT(고빈도매매)에서부터, intraday 움직임, daily, weekly, monthly까지 다양할 것입니다. 또한, 기술적분석이냐 기본적분석이냐, 소셜 미디어 피드 기반이냐, 감성분석을 기반으로 하냐에 따라 예측 모델에서 사용되는 파라미터도 다를 것입니다.​In this survey, we grouped first stock price forecasting articles according to their feature sets, such as studies using only the raw time series data (price data, Open, Close, High, Low, Volume (OCHLV)) for price prediction; studies using various other data, and studies using text mining techniques. Regarding the first group, the corresponding DL models were directly implemented using raw time series for price prediction. Table 1 tabulates the stock price forecasting studies that used only raw time series data in the literature. In Table 1, different methods/models are also listed based on four sub-groups: DNN (networks that are deep but without any given topology details) and LSTM models, multi models, hybrid models, novel methods.본고에서는 우선 주가예측에 사용되는 것이 뭐냐에 따라서 그룹을 나눴습니다. 가령 시세데이터(시고저종거(open, high, low, close, volume))만을 사용하는 연구도 있을 거고, 다른 데이터를 사용한 것도 있고 혹은 텍스트 마이닝 기법을 사용한 것도 있을 것입니다. 첫 번째 그룹에 경우에 시세데이터를 통해 직접적으로 DL이 직접 구현되었습니다. 표1에서 시세데이터만을 사용해 주가 예측을 한 연구에 대해 표로 정리되어있습니다. 표1에는 다른 방법이나 모델을 사용한 것 또한 나타내어져 있습니다. DNN(네트워크의 구조에 대해선 상관없이 그냥 Deep 신경망을 쓴 경우), LSTM, 여러모델, 하이브리드모델, 신개념 방법등이 있습니다. DNN and LSTM models were solely used in 3 papers. In Chong et al. [80], DNN and lagged stock returns were used to predict the stock prices in The Korea Composite Stock Price Index (KOSPI). Chen et al. [81], and Dezsi and Nistor [82] applied raw price data as the input to LSTM models.DNN 혹은 LSTM을 단독으로 사용한 연구는 3개가 있습니다. Chong et al. [80]에서는 DNN과 래깅처리한 주가 수익률을 사용해 KOSPI 에 속한 주식의 가격을 예측했습니다. Chen et al. [81], and Dezsi and Nistor [82]에서는 LSTM의 입력 데이터로 시세 데이터를 사용했습니다.​Meanwhile, some studies implement multiple DL models for performance comparison using only raw price (OCHLV) data for forecasting. Among the noteworthy studies, Samarawickrama et al. [83] compared RNN, Stacked Recurrent Neural Network (SRNN), LSTM, and GRU. Hiransha et al. [84] compared LSTM, RNN, CNN, and MLP, whereas in Selvin et al. [85], RNN, LSTM, CNN, and Autoregressive Integrated Moving Average (ARIMA) were preferred. Lee and Yoo [86] compared 3 RNN models (SRNN, LSTM, GRU) for stock price prediction and then constructed a threshold-based portfolio selecting stocks according to predictions. Li et al. [87] implemented DBN. Finally, the authors of [88] compared 4 different ML models for next price prediction in 1-minute price data: a 1 DL model (AE and RBM), MLP, Radial Basis Function Neural Network (RBF) and Extreme Learning Machine (ELM). They also compared the results for different sized datasets. The authors of [89] used price data and DNN, Gradient Boosted Trees (GBT), and Random Forest (RF) methods for the prediction of stocks in the Standard’s & Poor’s 500 Index (S&P500). Chandra and Chan [90] used co-operative neuro-evolution, RNN (Elman network), and DFNN for the prediction of stock prices in National Association of Securities Dealers Automated Quotations (NASDAQ) (ACI Worldwide, Staples, and Seagate).한편, 몇몇 연구에서는 예측에 오직 시세데이터만을 사용할 때 성능을 비교하기 위해 여러 DL모델을 만들기도 했습니다. 주목한 만한 연구 중에서 Samarawickrama et al. [83]에서는 RNN와 SRNN(Stacked RNN), LSTM, GRU(GatedRecurrent Unit)를 비교했습니다. Hiransha et al. [84]에서는 LSTM, RNN, CNN, MLP를 비교했고 또 Selvin et al. [85]에선 RNN, LSTM, CNN, ARIMA(AutoRegressive Integrated Moving Average)를 비교했습니다. Lee and Yoo [86]에서는 3가지 RNN 모델 SRNN, LSTM, GRU을 비교했고, 여기에서의 예측을 통해 주식을 선택한 포트폴리오를 구축하기도 합니다. Li et al. [87]에서는 DBN(Deep Belief Network)를 사용했네요. 마지막으로 [88]의 저자들은 1분뒤 가격을 예측하는 4개의 ML모델 DL model (AE and RBM), MLP, Radial Basis Function Neural Network (RBF) and Extreme Learning Machine (ELM)을 비교했습니다. 여기에서 서로 다른 데이터셋에 대한 결과도 비교했습니다. [89] 의 저자들은 가격데이터와 DNN, GBT(Gradient Boosted Tress), RF(Random Forest)를 사용해 S&P500에 속한 주식들의 가격을 예측했습니다. Chandra and Chan [90]에선 co-operative neuro-evolution, RNN (Elman network), DFNN을 사용하여 NASDAQ의 주식의 가격들 예측했습니다.​Meanwhile, hybrid models were used in some papers. Liu et al. [91] applied CNN+LSTM. Heaton et al. [92] implemented smart indexing with AE. Batres et al. [93] combined DBN and MLP to construct a stock portfolio by predicting each stock’s monthly log-return and choosing only stocks that were expected to perform better than the median stock.일부 하이브리드 모델을 사용한 연구도 있습니다. Liu et al. [91]는 CNN+LSTM을, Heaton et al. [92]에선 AE를 사용한 smart indexing을 구현했고, Batres et al. [93]는 DBN과 MLP를 혼합해 각 주식에 대한 월별 로그수익률을 예측하여 중위값 이상의 성과를 낼것으로 예상되는 주식들로 포트폴리오를 구축합니다.​In addition, novel approaches were adapted in some studies. Yuan et al. [94] proposed the novel Deep and Wide Neural Network (DWNN), which is combination of RNN and CNN. Zhang et al. [95] implemented a State Frequency Memory (SFM) recurrent network.그리고 신개념 방법론을 적용한 연구도 있습니다. Yuan et al. [94]는 Deep and Wide Neural Network (DWNN)이라는 RNN과 CNN을 혼합한 형태를 제안하기도 했으며, Zhang et al. [95]에서는 State Frequency Memory (SFM)라는 순환네트워크를 구현했습니다.​In another group of studies, some researchers again focused on LSTM-based models. However, their input parameters came from various sources including raw price data, technical and/or fundamental analysis, macroeconomic data, financial statements, news, and investor sentiment. Table 2 summarizes these stock price forecasting papers. In Table 2, different methods/models are also listed based on five sub-groups: DNN model; LSTM and RNN models; multiple and hybrid models; CNN model; and novel methods.다른 그룹을 한 번 봐볼까요. 몇몇 연구자들은 LSTM을 기반으로 하는 모델에 다시 초점을 두기도 합니다. 하지만, 입력 파라미터가 가격 데이터 뿐만 아니라, 기술적분석 혹은 기본적분석, 그리고 거시경제 데이터, 재무제표 데이터 뉴스, 투자자심리 들으로부터 기인되는 파라미터들이라는게 다릅니다. 표2에서 이러한 류의 연구들을 요약했습니다. 표2에서는 모델과 방법론에 따라 5개의 하위드룹으로 나뉩니다. DNN / LSTM 및 RNN / 여러개 및 하이브리드 / CNN / 신개념의 5가지 입니다. DNN models were used in some stock price forecasting papers within this group. In Abe et al. [96], a DNN model and 25 fundamental features were used for prediction of Japan Index constituents. Feng et al. [97] also used fundamental features and a DNN model for prediction. A DNN model and macro economic data, such as GDP, unemployment rate, and inventories, were used by the authors of [98] for the prediction of U.S. low-level disaggregated macroeconomic time series.DNN은 여기 그룹에서의 주가 예측 연구에서 사용됐습니다. Abe et al. [96]에선 DNN모델과 25가지 펀더멘털 특성으로 일본 지수 구성종목에 대한 예측합니다. Feng et al. [97]에서도 펀더멘털와 DNN을 사용했습니다. [98]의 저자들은 DNN 모델에 GDP나 실업률 등의 거시경제 데이터를 사용해 미국의 하위 수준의 거시경제 시계열예측을했습니다.​LSTM and RNN models were chosen in some studies. Kraus and Feuerriegel [99] implemented LSTM with transfer learning using text mining through financial news and stock market data. Similarly, Minami et al. [100] used LSTM to predict stock’s next day price using corporate action events and macro-economic index. Zhang and Tan [101] implemented DeepStockRanker, an LSTMbased model for stock ranking using 11 technical indicators. In Zhuge et al. [102], the authors used the price time series and emotional data from text posts to predict the opening stock price of the next day with an LSTM network. Akita et al. [103] used textual information and stock prices through Paragraph Vector + LSTM for forecasting prices and the comparisons were provided with different classifiers. Ozbayoglu [104] used technical indicators along with stock data on a Jordan–Elman network for price prediction.몇몇 연구에서는 LSTM과 RNN이 선택되기도 했습니다. Kraus and Feuerriegel [99]에서는 금융관련한 뉴스와 주식시장 데이터를 통해 텍스트 마이닝 기법을 활용해 전이학습을 통하는 LSTM을 사용했습니다. 비슷하게 Minami et al. [100]에서는 기업활동(Corporate Action) 데이터와 거시경제 지표를 LSTM에 사용하여 주식의 다음날 가격을 예측했습니다. Zhang and Tan [101] 에선 DeepStockRanker란 것을 구현했는데, LSTM 기반으로한 모델과 11가지 기술적분석 지표를 사용해 주식의 랭킹을 예측합니다. In Zhuge et al. [102]는 LSTM 네크워크를 이요해 주식의 나음날 가격을 예측했고, Akita et al. [103]에선 주가 시계열과 텍스트로부터 감성을 추출하여 주식의 다음날 시가를 LSTM 네트워크로 예상했습니다. Akita et al. [103]에서도 텍스트로부터의 정보와 주가를 Paragraph Vector + LSTM 모델을 사용해 가격을 예측했고 또한 다른 분류모델들과의 비교도 했습니다. Ozbayoglu [104]에서는 기술적분석 지표롸 주식 데이터를 Jordan–Elman network를 사용해 주가를 예측했습니다.​There were also multiple and hybrid models that used mostly technical analysis features as their inputs to the DL model. Several technical indicators were fed into LSTM and MLP networks in Khare et al. [105] for intraday price prediction. Recently, Zhou et al. [106] used a GAN for minimizing Forecast error loss and Direction prediction loss (GAN-FD) model for stock price prediction and compared their model performances against ARIMA, ANN and Support Vector Machine (SVM). Singh et al. [107] used several technical indicator features and time series data with Principal Component Analysis (PCA) for dimensionality reduction cascaded with a DNN (2-layer FFNN) for stock price prediction. Karaoglu et al. [108] used market microstructure-based trade indicators as inputs into an RNN with Graves LSTM detecting the buy–sell pressure of movements in the Istanbul Stock Exchange Index (BIST) to perform price prediction for intelligent stock trading. In Zhou et al. [109], next month’s return was predicted, and next-tobe-performed portfolios were constructed. Good monthly returns were achieved with LSTM and LSTM-MLP models.모델 여러개를사용한 것과 혼합한 하이브리드 모델을 사용한 것도 있습니다. 여기에선 대개 기술적 분석의 지표를 데이터로 사용했습니다. Khare et al. [105]에선 몇몇 기술적분석 지표들을 LSTM 및 MLP 네트워크에 사용해 주식의 장중가격을 예측했습니다. 최근에는 Zhou et al. [106]에서 GAN-FD 모델(GAN for minimizing Forecast error loss and Direction prediction loss model)을 사용해 주가를 예측했으며, 다른 ARIMA나 ANN, SVM 등과의 비교를 했습니다. Singh et al. [107]에선 몇 기술적 분석 지표와 시계열 데이터를 PCA로 사용하여 차원을 축소한 것이 DNN(2-layer FFNN)에 연계되도록 하여 주가를 예측했습니다. Karaoglu et al. [108]에선 시장 미시구조에 기반한 지표들을 RNN with Graves LSTM의 입력데이터로 삼아 이스탄불 거래소에서의 매수-매도 움직임의 압력을 관측하여 주가를 예측했습니다. In Zhou et al. [109]에선 다음달 주가의 수익률을 예측하여 이를 기반으로 한 포트폴리오를 구축합니다. 여기에서 사용된 모델은 LSTM과 LSTM-MLP 모델이었습니다.​Meanwhile, in some papers, CNN models were preferred. Abroyan et al. [110] used 250 features, including order details, for the prediction of a private brokerage company’s real data of risky transactions. They used CNN and LSTM for stock price forecasting. The authors of [111] used a CNN model and fundamental, technical, and market data for prediction.한편 어떤 연구에서는 CNN을 활용하기도 했습니다. Abroyan et al. [110]의 연구에서는 주문에 대한 디테일한 데이터를 포함한 250개의 입력데이터를 사용해 private brokerage company의 위험거래에 대해 예측했습니다. 여기에선 CNN과 LSTM을 사용해 주식을 예측합니다. [111]의 저자들은 CNN모델과 펀더멘털, 테크니걸, 시장 데이터를 사용하여 예측하기도 했습니다.​Novel methods were also developed in some studies. In Tran et al. [112], with the FI-2010 dataset, bid/ask and volume were used as the feature set for forecasting. In the study, they proposed Weighted Multichannel Time-series Regression (WMTR), and Multilinear Discriminant Analysis (MDA). Feng et al. [113] used 57 characteristic features, including Market equity, Market Beta, Industry momentum, and Asset growth, as inputs to a Fama–French n-factor DL for predicting monthly US equity returns in New York Stock Exchange (NYSE), American Stock Exchange (AMEX), or NASDAQ.몇몇 연구에서 신개념 방법론들도 개발되었습니다. Tran et al. [112]에서 FI-2010 데이터셋을 사용해, 호가와 거래량을 예측했습니다. 여기 연구에서는 Weighted Multichannel Time-series Regression (WMTR)과 Multilinear Discriminant Analysis (MDA)이 제시됩니다. Feng et al. [113]은 57가지 개별 특성을 사용했습니다. 주식, 주식의베타, 산업모멘텀, 자산성장률 등을 Fama–French n-factor DL의 입력 데이터로 삼아 NYSE, NASDAQ, AMEX 주식의 월별 수익률을 예측했습니다.​A number of research papers have also used text mining techniques for feature extraction but used non-LSTM models for stock price prediction. Table 3 summarizes the stock price forecasting papers that used text mining techniques. In Table 3, different methods/models are clustered into three sub-groups: CNN and LSTM models; GRU, LSTM, and RNN models; and novel methods.특성추출에는 텍스트 마이닝 기법을 사용하되 주가 예측에는 LSTM을 사용하지 않은 연구도 많이 있습니다. 표3에서는 주가예측에 텍스트 마이닝 기법을 활용한 연구들을 요약해놓았습니다. 표3에서는 서로 다른 방법 혹은 모델이 하위의 그룹으로 나뉘어져 있습니다. CNN 및 LSTM / GRU, LSTM 및 RNN / 신개념 방법론 3가지 하위 그룹입니다. CNN and LSTM models were adapted in some of the papers. In Ding et al. [114], events were detected from Reuters and Bloomberg news through text mining, and that information was used for price prediction and stock trading through the CNN model. Vargas et al. [115] used text mining on S&P500 index news from Reuters through an LSTM+CNN hybrid model for price prediction and intraday directional movement estimation together. Lee et al. [116] used financial news data and implemented word embedding with Word2vec along with MA an stochastic oscillator to create inputs for a Recurrent CNN (RCNN) for stock price prediction. Iwasaki et al. [117] also used sentiment analyses through text mining and word embeddings from analyst reports and used sentiment features as inputs to a DFNN model for stock price prediction. Then, different portfolio selections were implemented based on the projected stock returns.CNN 및 LSTM 모델을 사용한 페이퍼들을 봐보면, Ding et al. [114]에서는 로이터와 블룸버그 뉴스에 텍스트 마이닝을 하여 사건을 감지했고, 이 정보를 사용해 CNN을 통해 주가 예측 및 매매까지 연결했습니다. Vargas et al. [115]의 연구에서는 로이터에서의 S&P500 지수에 대한 뉴스에 텍스트 마이닝을 적용하고, LSTM+CNN 하이브리드 모델을 통해 주가 예측과 장중 가격 움직임의 추정을 했습니다. Lee et al. [116]에서는 MA에 따른 word embedding with Word2vec를 사용해 금융 뉴스를 다뤘습니다. 이를 통해 RCNN(Recurrent CNN)의 스토캐스틱 오실레이터(확률적으로 위아래로 움직이는 어떤 지표) 입력데이터를 만들어냈고, 출력으로 주가 예측을 했습니다. Iwasaki et al. [117]에서도 워드 임베딩을 사용한 텍스트 마이닝을 애널리스트 리포트를 통해 감성분석을 진행했고, 이 감성 특성을 DFNN 모델의 입력데이터로 하여 주가를 예측했습니다. 그리고나서 기대수익률에 근거한 몇몇 포트폴리오를 구축했습니다.​GRU, LSTM, and RNN models were preferred in the next group of papers. Das et al. [118] implemented sentiment analysis on Twitter posts along with stock data for price forecasting using an RNN. Similarly, the authors of [119] used sentiment classification (neutral, positive, and negative) for opening or closing stock price prediction with various LSTM models. They compared their results with SVM and achieved higher overall performance. In Zhongshengz et al. [120], text and price data were used for the prediction of SSE Composite Index (SCI) prices.GRU, LSTM, and RNN 그룹에 대해서도 봐보겠습니다. Das et al. [118] 에선 트위터 포스팅에서의 감성분석을 진행하여 RNN을 통한 주가 예측을 수행했고, 이와 비슷하게 [119]에서는 감성분석을 통해 양/중립/음의 분류를 했으며 LSTM모델을 통해 주가의 시가 혹은 종가를 예측했습니다. 여기에서는 SVM의 결과와 함께 비교했고, 전반적으로 더 높은 성과를 보였습니다. Zhongshengz et al. [120]에선 텍스트와 가격 데이터를 통해 SSE Composite Index (SCI)의 가격을 예측했습니다.​Novel approaches were reported in some papers. Nascimento et al. [121] used word embeddings for extracting information from web pages and then combined it with stock price data for stock price prediction. They compared the Autoregressive (AR) model and RF with and without news. The results showed embedding news information improved the performance. Han et al. [122] used financial news and the ACE2005 Chinese corpus. Different event types of Chinese companies were classified based on a novel event-type pattern classification algorithm in Han et al. [122], and also next day stock price change was also predicted using additional inputs.신개념 방법론들이 제시된 연구들도 있습니다. Nascimento et al. [121]에서는 웹페이지로부터 정보를 추출하는데 워드 임베딩을 사용했고, 이를 주가 데이터와 함께 사용하여 주가예측을 수행했습니다. 여기에선 AR모델과 뉴스없는 랜덤포레스트(RF) 모델과의 비교도 되어있습니다. 결과적으로 임베딩 뉴스 정보는 성과를 더 개선시켰습니다. Han et al. [122]에선 금융 뉴스와 ACE2005 Chinese corpus(말뭉치)를 사용했습니다. 중국 기업에 대한 서로 다른 사건의 유형을 신개념의 event-type pattern classification 알고리즘을 사용하여 분류했습니다. 그리고 다음날의 주가 변화 또한 추가적인 데이터를 통해 예측했습니다.​​4.2. Index forecastingInstead of trying to forecast the price of a single stock, several researchers preferred to predict the stock market index. Indexes are generally are less volatile than individual stocks because they are composed of multiple stocks from different sectors and are more indicative of the overall momentum and general state of the economy.개별 종목의 주가를 예측하는 대신에, 많은 연구에서는 시장 지수를 예측하는 것을 선호하기도 합니다. 지수는 일반적으로 개별 종목에 비해 변동성이 작습니다. 이유는 여러 섹터에 걸친 종목들이 혼합되어있기 때문입니다. 그래서 경제 상황 전반에 대해서 그리고 전반적인 모멘텀에 대해 더 잘 나타낸다고 볼 수 있습니다.(그냥 Diversifiable Risk들이 모두 분배되어 Market Portfolio로 볼 수 있다고 하면 될듯)​In the literature, different stock market index data have been used for experiments. The most commonly used index data are as follows: S&P500, China Securities Index (CSI)300, National Stock Exchange of India (NIFTY), Tokyo Nikkei Index (NIKKEI)225, Dow Jones Industrial Average (DJIA), Shanghai Stock Exchange (SSE)180, Hong Kong Hang Seng Index (HSI), Shenzhen Stock Exchange Composite Index (SZSE), London Financial Times Stock Exchange Index (FTSE)100, Taiwan Capitalization Weighted Stock Index (TAIEX), BIST, NASDAQ, Dow Jones Industrial Average 30 (DOW30), KOSPI, S&P500 Volatility Index (VIX), NASDAQ100 Volatility Index (VXN), Brazilian Stock Exchange (Bovespa), Stockholm Stock Exchange (OMX), and NYSE. The authors of the papers [123–134,114] used S&P500 as their dataset. The authors of the studies [123,124,135–137] used NIKKEI as their dataset. KOSPI was used in Li et al. [135], Jeong et al. [131], Baek et al. [132]. DJIA was used as the dataset in the papers [123,136– 139]. The authors of the articles [123,135,137,131] used HSI as the dataset in their studies, and SZSE was used in the studies [140,135,141,142].연구에 따라 대상이 되는 주가지수는 다를 것입니다. 전반적으로 많이 사용되는 것들이 미국의 S&P500 , 중국의 CSI300, 인도의 니프티, 일본의 니케이225, 미국 다우존스지수, 중국 상해180, 홍콩 지수, 중국 선전지수, 런던의 FTSE100, 대만 TAIEX,하아아아 왤케 많아 ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ 짜증나네 BIST, NASDAQ, DOW30, KOSPI, VIX, NASDAQ100, 브라질………….. 어우 많다. [123–134,114]에선 S&P500을 데이터셋으로, [123,124,135–137]에선 니케이, Li et al. [135], Jeong et al. [131], Baek et al. [132]에선 코스피, [123,136– 139]에선 다우존스 산업평균 지수를 데이터 셋으로 사용하고, 블라블라블라ㅋㅋㅋㅋ​In addition, in the literature, there are different methods for the prediction of index data. While some studies used only raw time series data, others used various other data such, as technical indicators, index data, social media feeds, news from Reuters, and Bloomberg, and statistical features of data (standard deviation, skewness, kurtosis, omega ratio, fund alpha). In this survey, we first grouped the index forecasting articles according to their feature sets such as studies using only raw time series data (price/index data, OCHLV); then, we clustered the studies using various other data. Table 4 summarizes the index forecasting papers using only raw time series data. Moreover, different methods (models) were used for index forecasting. MLP, RNN, LSTM, and DNN (DFNN or DMLP) methods were the most used methods for index forecasting. In Table 4, these various methods/models are also listed as four sub-groups: ANN, DNN, MLP, and Fuzzy Deep Direct Reinforcement Learning (FDDR) models; RL and DL models; LSTM and RNN models; and novel methods.여기에 더불어, 관련 문헌에서 지수 예측에 대해 사용하는 방법도 다 다릅니다. 몇 연구에서는 오직 시계열 데이터만 사용하기도 했고, 뭐 기술적 지표나 소셜미디어 데이터, 뉴스 데이터, 통계적 특성(표준편차, 왜도, 첨도, omega ratio, fund alpha(얘네들은 첨본다..))등을 사용하기도 합니다. 본고에서는 첫 번째로 그들이 사용하는 데이터 셋에 따라 첫 번째로 그룹을 나눕니다. 시세데이터만을 사용하냐, 다른 데이터들도 사용하냐에 따라 나누는 것입니다. 표4에서는 시세데이터만을 사용해 지수를 예측하는 연구에 대해서 요약했습니다. 그리고 이 안에서 지수예측에 어떤 모델을 사용하느냐에 따라 구분했습니다. 표4에서는 많은 모델 혹은 방법론들이 하위 4개의 그룹으로 나누어져 있습니다. ANN, DNN, MLP, and FDDR models were used in some studies. In Lachiheb et al. [143], log returns of the index data were used with a DNN with hierarchical input for the prediction of TUNINDEX data. Yong et al. [144] used a deep FFNN and Open, Close, High, Low (OCHL) of the last 10 days of index data for prediction. In addition, MLP and ANN were used for the prediction of index data. In Yumlu et al. [145], raw index data were used with MLP, RNN, Mixture of Experts (MoE), and Exponential GARCH (EGARCH) for forecasting. In Yang et al. [142], ensembles of ANN with OCHLV of data were used for prediction of the Shanghai composite index.일부 연구에서 ANN, DNN, MLP, FDDR 모델이 사용됩니다. Lachiheb et al. [143]에서 지수의 로그 수익률을 DNN의 계층적 입력 데이터를 사용해 TUNINDEX 를 예측합니다. Yong et al. [144]에선, FFNN과 과거 10일 동안의 지수의 시고저종데이터를 통해 예측합니다. 여기에 더불어 MLP와 ANN으로 지수를 예측하는 것도 있습니다. Yumlu et al. [145]에서는 MLP, RNN, MoE, Exponential GARCH(EGARCH)를 사용해 지수를 예측했고, Yang et al. [142]에선 ANN의 앙상블과 시고저종 데이터로 상해 지수를 예측했습니다.​Furthermore, RL and DL methods were used together for prediction of index data in some studies. In Deng et al. [141], FDDR, DNN, and RL methods were used to predict 300 stocks from SZSE index data and commodity prices. In Jeong et al. [131], Deep QLearning and DNN methods and a 200-day stock price dataset were used together for prediction of the S&P500 index.또한, RL과 DL방법을 함께 하용해 지수를 예측한 연구도 있습니다. Deng et al. [141]에서는 FDDR, DNN, RL 방법을 사용해 SZSE지수의 300종목에 대한 가격 그리고 원자재 가격을 예측합니다. Jeong et al. [131]에선 Deep Q-learning과 DNN모델을 사용하고 과거 200일 동안의 주가 데이터를 사용해 S&P500 지수를 예측했습니다.​Most of the preferred methods for prediction of index data using raw time series data have been based on LSTM and RNN. In Bekiros et al. [139], an RNN was used for prediction of log returns of the DJIA index. In Fischer et al. [125], LSTM was used to predict S&P500 index data. Althelaya et al. [128] used stacked LSTM and Bidirectional LSTM (Bi-LSTM) methods for S&P500 index forecasting. Yan et al. [146] used an LSTM network to predict the next day closing price of Shanghai stock index. In their study, they used wavelet decomposition to reconstruct the financial time series for denoising and better learning. In Pang et al. [140], LSTM was used for prediction of the Shanghai A-shares composite index. Namini et al. [136] used LSTM to predict NIKKEI225, IXIC, HIS, GSPC and DJIA index data. In Takahashi et al. [147] and Baek et al. [132], LSTM was also used for the prediction of the S&P500 and KOSPI200 indexes. Baek et al. [132] developed an LSTM-based stock index forecasting model called ModAugNet. The proposed method was able to beat Buy and Hold (B&H) in the long term with an overfitting prevention mechanism. Elliot et al. [134] compared different ML models (linear models), Generalized Linear Models (GMLs) and several LSTM, and RNN models for stock index price prediction. In Hansson et al. [133], LSTM and the autoregressive part of time series index data were used for prediction of the S&P500, Bovespa50, OMX30 indexes.지수 데이터에 대한 가장 성호되는 방법은 LSTM과 RNN을 기반으로 한 것에 시세 데이터를 사용한 것입니다. Bekiros et al. [139]에선 다우존스지수의 로그 수익률을 예측하는데 RNN이 사용됐습니다. Fischer et al. [125]에서는 S&P500지수를 예측하는데 LSTM이 사용됐고, Althelaya et al. [128]에서는 stacked LSTM과 양방향LSTM(Bi-LSTM)응 사용해 S&P500지수를 예측합니다.Yan et al. [146]에선 LSTM 네트워크를 사용해 상해지수의 다음날 종가를 예측했습니다. 여기에서는 wavelet decomposition 라는 것을 사용하여 노이즈를 제거하고 학습이 더 잘 진행되도록 금융 시계열데이터를 재구성합니다. Pang et al. [140]에서는 상해A-주식 지수를 예측하는데 LSTM을 사용했고, Namini et al. [136]에서는 LSTM으로 니케이225, 나스닥, 항생, S&P500, 다우존스 지수를 예측합니다. Takahashi et al. [147] and Baek et al. [132]의 연구에서도 LSTM을 통해 S&P500과 KOSPI200 지수를 예측했습니다. 여기에 Baek et al. [132]에서는 LSTM을 기반으로한 주식시장 지수를 예측하는 모델을 ModAugNet이라고 불렀습니다. 이 방법은 오버피팅을 예방하는 매커니즘을 통하는데, 장기적으로 상승하는 주식을 보유할 수 있었다고 합니다. Elliot et al. [134]의 연구에서는 다른 선형 ML모델들과 일반화된 선형모델(GML), 그리고 몇몇의 LSTM, RNN을 통해 주가 지수를 예측하는 것을 비교했습니다. Hansson et al. [133]에서는 LSTM과 시계열에서 자기상관적인 부분을 사용하여 S&P500, Bovespa50, OMX30 지수를 예측했습니다.​Also, some studies adapted novel approaches. In Zhang et al. [138], a genetic DNN was used for DJIA index forecasting. Borovykh et al. [127] proposed a new DNN model called Wavenet convolutional net for time series forecasting. Bildirici et al. [148] proposed a Threshold Autoregressive (TAR)-Vector Error Correction model (VEC)-Recurrent Hybrid Elman (RHE) model for forex and stock index of return prediction and compared several models. Parida et al. [124] proposed a method called Locally Recurrent Neuro-fuzzy Information System (LRNFIS) with Firefly Harmony Search Optimization (FHSO) Evolutionary Algorithm (EA) to predict the S&P500 and NIKKEI225 and USD Exchange price data. Psaradellis et al. [149] proposed Heterogeneous Autoregressive Process (HAR) with GA with a SVR (GASVR) model called HAR-GASVR for prediction of the VIX, VXN, Dow Jones Industrial Average Volatility Index (VXD) indexes.또한, 신개념 방법론을 사용한 연구도 있었습니다. Zhang et al. [138]에서는 유전DNN을 사용해 다우존스 지수를 예측했고, Borovykh et al. [127]에선 시계열 예측을 위한 DNN모델을 Wavenet convolutional net이라 부르며 제안했습니다. Bildirici et al. [148]의 연구에서는 Threshold Autoregressive (TAR)-Vector Error Correction model (VEC)-Recurrent Hybrid Elman (RHE) model을 제안하며, 환율과 주가지수의 수익률을 예측하고 다른 모델들과의 비교도 수행했습니다. Parida et al. [124]에서는 Locally Recurrent Neuro-fuzzy Information System (LRNFIS) with Firefly Harmony Search Optimization (FHSO) Evolutionary Algorithm (EA)이라는 방법을 제안하여 S&P500, 니케이225, 미국환율 데이터를 예측하는 것을 보였습니다. Psaradellis et al. [149]의 연구에서는 Heterogeneous Autoregressive Process (HAR) with GA with a SVR (GASVR) 모델을 HAR-GASVR이라 부르며 VIX, VXN, Dow Jones Industrial Average Volatility Index (VXD) 여러 변동성 지수들에 대한 예측을 보였습니다.​In the literature, some studies used various input data, such as technical indicators, index data, social media news, news from Reuters, and Bloomberg, and statistical features of data (standard deviation, skewness, kurtosis, omega ratio, fund alpha). Table 5 summarizes the index forecasting papers using these aforementioned various data. DNN, RNN, LSTM, and CNN methods were the most commonly used models in index forecasting. In Table 5, different methods/models are also listed within four sub-groups: DNN model; RNN and LSTM models; CNN model; and novel methods.기술적지표 혹은 지수 데이터, 소셜미디어 뉴스, 로이터나 블룸버그의 뉴스 혹은 통계적 특성 등의 또 다른 다양한 데이터를 입력데이터로 사용하는 연구도 있었습니다. 표5에서는 위에 언급된 다양한 데이터를 사용해 지수를 예측한 연구들을 요약했습니다. DNN, RNN, LSTM, CNN 방법론이 지수를 예측하는데 대개 사용이 되었습니다. 표 5에서는 사용한 모델의 유형에 따라 4개(DNN / RNN / LSTM / CNN)의 하위 그룹으로 나뉘어져있습니다. A DNN was used as the classification model in some papers. In Chen et al. [150], a DNN and some features of the data (Return, Sharpe-ratio (SR), Standard Deviation (STD), Skewness, Kurtosis, Omega ratio, Fund alpha) were used for prediction. In Widegren et al. [126], DNN, RNN, and technical indicators were used for prediction of the FTSE100, OMX30, S&P500 indexes.일부 연구에서는 DNN이 분류 모델로 사용됐습니다. Chen et al. [150]에서는 DNN과 수익률, 샤프비율, 수익률 표준편차/왜도/첨도, 오메가비율, 펀드알파 등의 여러 데이터를 사용해 예측합니다. Widegren et al. [126]의 연구에서는 DNN과 RNN을 사용했고, 입력데이터로 기술적 지표를 사용하여 FTSE100, OMX30, S&P500를 예측했습니다.​In addition, RNN and LSTM models with various other data were also used for prediction of the indexes. Hsieh et al. [137] used RNN and OCHLV of indexes and technical indicators to predict the DJIA, FTSE, Nikkei, and TAIEX indexes. Mourelatos et al. [151] used GASVR, and LSTM for forecasting. Chen et al. [152] used four LSTM models (technical analysis, attention mechanism and market vector embedding) for prediction of the daily return ratio of the HSI300 index. In Li et al. [135], LSTM with wavelet denoising and index data, volume, and technical indicators were used for prediction of the HSI, SSE, SZSE, TAIEX, NIKKEI, and KOSPI indexes. Si et al. [153] used a MODRL+LSTM method to predict Chinese stock-IF-IH-IC contract indexes. Bao et al. [123] used stacked AEs to generate deep features using OCHL of stock prices, technical indicators, and macroeconomic conditions to feed LSTM to predict future stock prices.또한, RNN 및 LSTM 모델에 여러 입력데이터를 사용해 지수를 예측한 연구도 있습니다. Hsieh et al. [137]에서는 RNN에 시세데이터, 기술적지표 데이터를 사용해 다우존스, 런던 파이낸셜타임즈, 니케이, 대만의 주가지수를 예측합니다. Mourelatos et al. [151]의 연구에서는 GASVR과 LSTM을 사용해 예측하고, Chen et al. [152]에서는 LSTM에 기술적지표, 어텐션메터지즘, 시장벡터 임베딩 등을 사용해 홍콩 항생의 일별 수익률을 예측했습니다. Li et al. [135]의 연구에서는 LSTM와 wavelet denoising된 가격, 거래량, 기술적지표를 사용해 HSI, SSE, SZSE, TAIEX, NIKKEI, KOSPI 지수를 예측했습니다. Si et al. [153] 연구에서는 MODRL+LSTM을 사용해 중국주식 지수를 예측했습니다. Bao et al. [123]에서는 stacked AE를 사용해 deep feature를 생성하고, 여기에 시고저종 데이터와 기술적지표, 거시경제 상황등을 LSTM에 먹여 미래 주가를 예측했습니다.​Besides, different CNN implementations with various data (technical indicators, news, and index data) have been used in the literature. In Dingli et al. [129], CNN, and index data, and technical indicators were used for the S&P500, DOW30, NASDAQ100 indexes and Commodity, Forex, and Bitcoin prices. In Ding et al. [114], a CNN model with news from Reuters and Bloomberg were used for prediction of the S&P500 index and 15 stocks’ prices in S&P500. In Lee et al. [116], CNN + LSTM and technical indicators, index data, and news were used for forecasting of the Taiwan Stock Exchange (TWSE) index and 4 stocks’ prices in TWSE.이와 다르게 여러 데이터와 함께 CNN을 활용한 연구들도 있었습니다. Dingli et al. [129]에선 CNN과 기술적지표를 통해 S&P500, DOW30, NASDAQ100 그리고 원자재, 외화, 비트코인 가격까지 예측했습니다. Ding et al. [114]의 연구에서는 CNN모델과 로이터/블룸버그로 부터의 뉴스 데이터를 사용해 S&P500 지수 그리고 안쪽의 15종목에 대한 가격을 예측했습니다.​In addition, some novel methods have been proposed for index forecasting. Rout et al. [130] used RNN models, Recurrent Computationally Efficient Functional Link Neural Network (RCEFLANN), and Functional Link Neural network (FLANN), with their weights optimized using various EAs like Particle Swarm Optimization (PSO), and Modified Version of PSO (HMRPSO), for time series forecasting. Chen et al. [154] used social media news to predict index price and index direction with RNN-Boost with Latent Dirichlet Allocation (LDA) features.지수 예측에 신개념 방법론을 도입한 연구도 있었습니다. Rout et al. [130] 에서는 RNN 과 RCEFLANN(Recurrent Computationally Efficient Functional Link Neural Network ), FLANN(Functional Link Neural network)를 최적의 가중치로 혼합하여 금융 시계열 예측을 수행합니다. Chen et al. [154]의 연구에선 소셜 미디어 뉴스와 RNN-Boost with Latent Dirichlet Allocation (LDA) features를 통해 지수와 지수의 방향을 예측했습니다.​​4.3. Commodity price forecastingA number of studies particularly focused on the price prediction of any given commodity, such as gold, silver, oil, and copper. With the increasing number of commodities available for public trading through online stock exchanges, interest in this topic will likely grow in the following years.많은 연구에서 특히 금이나 은, 석유, 구리 등의 원자재 가격의 예측에 초점을 두기도 했습니다. 공개시장에서 거래할 수 있는 원자재 상품이 많아지며 이에대한 연구도 날로 많아지고 있습니다.​In the literature, there have been different methods used for commodity price forecasting. DNN, RNN, FDDR, and CNN are the most used models to predict commodity prices. Table 6 lists the details of the commodity price forecasting studies with DL.원자재 가격 예측에 사용되는 방법들도 여러가지 있습니다. DNN, RNN, FDDR, CNN 등이 원자재 가격 예측에 주로 쓰였으며, 표 6에서 이러한 연구에 대해 정리해놓았습니다. In Dingli et al. [129], the authors used a CNN for predicting the next week’s and next month’s price directional movement. Meanwhile, RNN and LSTM models were used in some commodity forecasting studies. In Dixon et al. [155], a DNN was used for commodity forecasting. In Widegren et al. [126], forex, and index datasets were used. DNN and RNN were used to predict the prices of time series data. Technical indicators were used as the feature set consisting of Relative Strength Index (RSI), Williams Percent Range (William%R), Commodity Channel Index (CCI), Percentage Price Oscillator (PPOSC), momentum, and Exponential Moving Average (EMA). In Lasheras et al. [156], the authors used an Elman RNN to predict COMEX copper spot price (through New York Mercantile Exchange (NYMEX)) from daily closing prices.Dingli et al. [129]의 저자들은 CNN을 이용하여 다음주, 다음달의 가격 움직임을 예측했습니다. 한편, RNN과 LSTM 모델들이 사용된 연구들도 있습니다. Dixon et al. [155] 연구에서는 원자재 예측에 DNN이 사용이 되었습니다. Widegren et al. [126]에서는 환율과 지수를 예측하는데 DNN과 RNN이 사용되었으며, RIS, William%R, CCI, PPOSC, 모멘텀, 지수이동평균 등의 기술적 지표들 또한 입력 데이터로 사용되었습니다. Lasheras et al. [156]의 저자들은 Elman RNN모델을 하용하여 구리 현물의 일별 종가를 예측했습니다.​Hybrid and novel models have been adapted in some studies. In Zhao et al. [157], FNN and Stacked Denoising Autoencoders (SDAE) deep models were compared against Support Vector Regressor (SVR), Random Walk (RW), and Markov Regime Switching (MRS) models for WTI oil price forecasting. As performance criteria, accuracy, Mean Absolute Percentage Error (MAPE), and Root Mean Square Error (RMSE) were used. In Chen et al. [158], the authors aimed to predict WTI crude oil prices using several models, including combinations of DBN, LSTM, Autoregressive Moving Average (ARMA), and RW. MSE was used as the performance criteria. In Deng et al. [141], the authors used FDDR for stock price prediction and trading signal generation. They combined DNN and RL. Profit, return, SR, and profit–loss curves were used as the performance criteria.하이브리드 모델 및 신개념 모델이 적용된 연구도 있었습니다. Zhao et al. [157]의 연구에서는 FNN과 SDAE deel 모델이 SVR과 RW, MRS 모델 등을 이용한 WTI 유가 예측에 사용되었습니다. 성능의 기준으로는 정확도, MAPE, RMSE 등이 사용되었습니다. Chen et al. [158] 연구에서의 저자들은 WTI 원유 가격 예측을 DBN, LSTM, ARMA, RW 등의 모델을 사용하여 했습니다. 여기에서의 성능의 기준은 MSE가 되었습니다. Deng et al. [141]의 연구에서는 FDDR을 이용해 주가 예측과 매매신호를 생성합니다. 그리고 이들은 또 DNN과 RL을 혼합했으며, Profit, return, sharpe ratio, profit-loss 곡선 등을 성능의 기준으로 삼았습니다.​​4.4. Volatility forecastingVolatility is directly related to price variations in a given time period and is mostly used for risk assessment and asset pricing. Some researchers implemented models for accurately forecasting the underlying volatility of any given asset.변동성이라고 하면, 주어진 기간에 대해 가격의 변동폭에 직접적으로 관련되어있고, 대부분 이것을 자산가격에 대한 리스크로 사용합니다. 그리고 몇몇 연구에서는 어떤 자산이든지간에 내재되어있는 변동성을 정확하게 측정하는 모델을 구현하기도 했습니다.​In the literature, there have been different methods used for volatility forecasting, including LSTM, RNN, CNN, MM, and Generalised Auto-Regressive Conditional Heteroscedasticity (GARCH) models. Table 7 summarizes the studies that focused on volatility forecasting. In Table 7, different methods/models are also represented as three sub-groups: CNN; RNN and LSTM models; and hybrid and novel models.변동성 예측에 대한 연구들도 봐보면 다 다른 모델을 사용합니다. 사용된 모델들로는 LSTM, RNN, CNN, MM, GARCH 모델 등이 있습니다. 표7에서는 변동성 예측에 대한 연구를 요약했습니다. 그리고 여기에서도 3개의 하위 그룹으로 나누었는데, CNN / RNN & LSTM / 하이브리드 및 신개념 이렇게 3개의 하위 그룹이 있습니다. A CNN model was used in one volatility forecasting study based on HFT data [159]. Meanwhile, RNN and LSTM models were used in some studies. In Tino et al. [160], the authors used financial time series data to predict volatility changes with Markov Models and Elman RNN for profitable straddle options trading. Xiong et al. [161] used price data and different types of Google Domestic trends with LSTM. Zhou et al. [162] used CSI300, and 28 words from the daily search volume based on Baidu as the dataset with LSTM to predict index volatility. Kim et al. [163] developed several LSTM models integrated with GARCH for volatility prediction.CNN모델은 HFT 데이터를 토대로하는 어떤 한 연구에서 변동성을 예측하는데 사용됐었습니다. 한편, RNN 및 LSTM 모델은 여러 연구에서 사용되었습니다. Tino et al. [160]의 저자들은 Markov 모델과 and Elman RNN을 사용해 변동성의 변화를 예측하고, 이를 바탕으로 straddle 옵션 매매로 돈버는 것까지 연결시켰습니다. Xiong et al. [161]의 연구에서는 가격 데이터와 수 많은 유형의 구글 도메스틱의 추세 데이터와 함께 LSTM을 활용했습니다. Zhou et al. [162] 의 연구에서는 CSI300과 바이두에서의 28개의 단어에 대한 검생략을 LSTM의 입력데이터로 하여 지수의 변동성을 예측했습니다. Kim et al. [163]에서는 변동성 예측을 위한 LSTM과 GARCH를 합치기도 했습니다.​Hybrid and novel approaches have also been adapted in some studies. In Nikolaev et al. [164], an RMDN with GARCH (RMDNGARCH) model was proposed. In addition, several models, including traditional forecasting models and DL models, were compared for volatility estimation. Psaradellis et al. [149] proposed a novel method called HAR with GASVR (HAR-GASVR) for volatility index forecasting.하이브리드 모델과 신개념 방법론이 몇몇 연구에서 제시되기도 했습니다. Nikolaev et al. [164]에서 RMDN with GARCH (RMDNGARCH) 모델이 제시됩니다. 여기에 더불어 전통적인 예측 모델과 DL모델이 함께 비교가 됩니다. Psaradellis et al. [149]에서는 지수 변동성의 예측을 위한 모델로 HAR with GASVR (HAR-GASVR)이라 부르는 신개념 모델을 제안하기도 했습니다.​​4.5. Bond price forecastingSome financial experts follow the changes in bond prices to analyze the state of the economy, claiming bond prices represent the health of the economy better than the stock market [165]. Historically, long term rates are higher than short term rates under normal economic expansion, whereas immediately before recessions, short term rates pass long term rates, i.e., an inverted yield curve. Hence, accurate bond price prediction is very useful. However, DL implementations for bond price prediction are very scarce. In Bianchi et al. [166], excess bond return was predicted using several ML models, including RF, AE, and PCA networks and a 2-3-4-layer DFNN. 4-layer NN outperformed the other models.금융전문가라면 채권 가격의 변화에 대해서도 분석하여 현 경기 국면이 어떤 상황이냐 등에 대한 분석도 하고, 그들은 채권 가격이 주식시장보다 더 잘 경제의 건강을 나타낸다고 주장합니다. 역사적으로 보았을때, 평범함 경기 팽찰 국면에서는 장기 금리는 단기 금리보다 높았습니다. 반면에 경기 침체 직전에는 단기 금리가 장기금리를역전하기도 했습니다. 즉, 장단기 금리 역전현상인거죠. 따라서 정확한 채권 가격 예측은 매우 도움이 될 것입니다. 하지만, 채권 가격 예측을 위한 DL의 활용은 아직 거의 없었습니다. Bianchi et al. [166]의 연구에서 초과적인 채권 수익률은 RF, AE, PCA 네크워크 , 2~4층의 딥신경망을 통한 구현을 했습니다. 여기에선 4층의 신경만 구조가 다른 모델들을 아웃퍼폼 했습니다.​​4.6. Forex price forecastingForeign exchange markets have the highest volumes among all existing financial markets in the world. They are open 24/7, and trillions of dollars worth of foreign exchange transactions happen in a single day. According to the Bank for International Settlements, foreign-exchange trading has a volume of more than 5 trillion USD a day [167]. In addition, there are a large number of online forex trading platforms that provide leveraged transaction opportunities to their subscribers. As a result, there is huge interest in profitable trading strategies by traders. Hence, there are a number of forex forecasting and trading studies based on DL models. Because most of the global financial transactions are based on the US Dollar, almost all forex prediction research papers include USD in their analyses. However, depending on regional differences and intended research focus, various models have been developed accordingly.외화 거래 시장은 현재 존재하는 어느 금융시장보다 거래가 가장 많이 되는 시장입니다. 여기는 24시간 연중무휴로 열려있고, 하루에 약 수 조 달러 상당의 외화 거래가 일어납니다. BIS에 따르면, 외화 거래는 하루에 5조 달러가 넘는다고 합니다. 그리고, 온라인으로 외화를 매매할 수 있는 플랫폼이 다수 존재하며 여기에선 레버리지도 땡길 수 있게끔 제공한다고 합니다. 결과적으로 트레이더 사이세서는 여기에서 큰 수익를 버는 매매전략에 큰 관심이 생길 수 밖에 없을 겁니다. 따라서 DL 모델을 활용한 외화거래에 대한 연구 논문들도 가수 존재합니다. 글로벌 금융 거래의 대다수는 미국 달러로 거래되기 때문에, 외화 예측 페이퍼들도 대개 USD를 기준으로 합니다. 하지만, 지역이 어디냐 / 연구 초점이 무엇이냐에 따라서 개발되는 모델이 다르기는 합니다.​In the literature, different methods have been used for forex price forecasting, including RNN, LSTM, CNN, DBN, DNN, AE, and MLP methods. Table 8 provides details about these implementations. In Table 8, different methods/models are listed as four sub-groups: Continuous-valued Deep Belief Networks (CDBN), DBN, DBN+RBM, and AE models; DNN, RNN, Psi-Sigma Network (PSN), and LSTM models; CNN models; and hybrid models.연구에서들마다 사용된 모델들은 RNN, LSTM, CNN, DBN, DNN, AE, MLP 등으로 다양합니다. 표8에서 더 자세하게 표시해 놓았으며, 하위의 4개의 그룹으로 나누었습니다. CDBN, DBN, DBN+RBM, AE / DNN, RNN, PSN, LSTM / CNN / 하이브리드 CDBN, DBN, DBN+RBM, and AE models have been used in some studies. In [168], Fuzzy information granulation integrated with CDBN was applied for predicting EUR/USD and GBU/USD exchange rates. They extended a DBN with a Continuous Restricted Boltzman machine (CRBM) to improve performance. In Chao et al. [169], weekly GBP/USD and INR/USD prices were predicted, whereas in Zheng et al. [170], CNY/USD and INR/USD were the main focus. In both cases, DBN was compared with FFNN. Similarly, Shen et al. [171] implemented several different DBN networks to predict weekly GBP/USD, BRL/USD and INR/USD exchange rate returns. Shen et al. [172] combined Stacked AE and SVR for predicting 28 normalized currency pairs using the time series data of USD, GBP, EUR, JPY, AUD, CAD, and CHF.첫 번째 하위 그룹에 대해 봐보면, [168]의 연구에서는 CDBN에 Fuzzy information granulation를 혼합한 모델을 사용하여 EUR/USD, GBU/USD 환율을 예측했습니다. 여기에서는 DBN과 CRBM을 혼합해 성능을 개선시켰습니다. In Chao et al. [169]에서는 주별 GBP/USD, INR/USD 환율을 예측했고, Zheng et al. [170]의 연구에서는 CNY/USD, INR/USD 환율에 초점을 두었습니다. 그리고 두 연구 모두에서 DBN과 FFNN을 비교했습니다. 비슷하게 Shen et al. [171]의 연구에서는 서로 다른 DBN 모델들을 놓고 주간 단위로 GBP/USD, BRL/USD, INR/USD 환율을 예측했습니다. Shen et al. [172]에서는 USD, GBP, EUR, JPY, AUD, CAD, CHF 데이터를 사용해서 Stacked AE과 SVR 를 혼합한 모델로 각각의 환율들을 예측했습니다.​DNN, RNN, PSN, and LSTM models were preferred in some studies. In Dixon et al. [155], multiple DMLP models were developed for predicting AD and BP futures using 5-minute data over in a 130 day period. Sermpinis et al. [173] used MLP, RNN, GP, and other ML techniques along with traditional regression methods for also predicting EUR/USD time series. They also integrated Kalman filter, LASSO operator, and other models to further improve the results [174]. They further extended their analyses by including PSN and providing comparisons along with traditional forecasters ARIMA, RW, and STAR [175]. To improve performance, they also integrated hybrid time-varying volatility leverage. Sun et al. [176] implemented RMB exchange rate forecasting against JPY, HKB, EUR and USD by comparing RW, RNN, and FFNN performances. Maknickiene et al. [177] predicted various forex time series and created portfolios consisting of these investments. Each network used LSTM (RNN EVOLINO), and different risk appetites for users have been tested. Maknickiene et al. [178] also used EVOLINO RNN + orthogonal input data for predicting USD/JPY and XAU/USD prices over different periods.2번째 그룹은 DNN, RNN, PSN, LSTM 모델들을 사용했습니다. In Dixon et al. [155] 에선 다수의 DMLP 모델을 사용해서 달러와 파운드의 5분봉 데이터를 130일치를 사용해 선물가격을 예측합니다. Sermpinis et al. [173]에서는 MLP, RNN, GP와 함께 여러 머신러닝 테크닉을 기존의 회귀 방법에 맞게 사용하여 EUR/USD 환율을 예측했습니다. 그들은 또한 칼만필터, 라쏘 등의 기법을 사용해 결과를 더욱 개선시키는 논문[174]을 내기도 했습니다. 또한, 분석을 더욱 확장하여 PSN과 여러 기존 통계학적 모델들과의 비교도 했습니다. Sun et al. [176]의 연구에서는 JPY, HKB, EUR, USD에 대한 RMB의 환율율 예측하는데 RW, RNN, FFNN 모델의 성능을 비교했습니다. Maknickiene et al. [177] 에서는 여러 환율 시계열을 예측하고 이를 기반으로한 포트폴리오 구축까지 갔습니다. 각각의 네트워크 에서는 LSTM (RNN EVOLINO)을 사용했으며, 다양한 사람들의 위험성향에 대해서도 테스트했습니다. 또 그의 다른 연구에선 EVOLINO RNN + orthogonal input data를 사용해 USD/JPY, XAU/USD 환율도 예측했습니다.​Different CNN models were used in some studies. In Persio et al. [179], EUR/USD was once again forecasted using multiple DL models, including MLP, CNN, RNN, and Wavelet+CNN. Korczak et al. [180] implemented forex trading (GBP/PLN) using several different input parameters in a multi-agent-based trading environment. One of the agents used AE+CNN as the prediction model and outperformed all other models.CNN을 활용하는 논문들에서는 서로 다른 CNN들이 활용되었습니다. Persio et al. [179]에선 EUR/USD 환율을 MLP, CNN, RNN, Wavelet+CNN을 포함한 여러 DL 모델로 예측을 했고, Korczak et al. [180]에서는 GBP/PLN 환율에 대한 매매를 다중agent를 기반으로 한 매매 환경에 대한 서로 다른 입력데이터를 사용해 매매를 구현했습니다. AE+CNN에서 사용된 하나의 agent는 다른 모든 모델을 아웃퍼폼하는 예측 모델이란 것을 밝혔습니다.​Hybrid models have also been adapted in some of the researches. Bildirici et al. [148] developed several (TAR-VEC-RHE) models for predicting monthly returns for TRY/USD and compared model performances. Nikolaev et al. [164] compared several models, including traditional forecasting models and DL models, for DEM/GBP prediction. Parida et al. [124] predicted AUD, CHF, MAX, and BRL against USD currency time series data using LRNFIS and compared it with different models. Meanwhile, instead of using LMS-based error minimization during learning, they used FHSO.하이브리드 모델이 적용된 논문들을 봐보면, Bildirici et al. [148]에선 여러 TAR-VEC-RHE 모델을 사용해 월간 단위의 TRY/USD 환율을 예측하여 모델들마다의 성과를 비교했습니다. Nikolaev et al. [164]의 연구에서도 여러 모델들을 비교했는데, DEM/GBP 를 예측하는 전통적인 모델과 DL 모델을 비교했습니다. Parida et al. [124]의 연구에서는 AUD, CHF, MAX, BRL에 대한 USD 환율 시계열을 예측헀으며 여기서 사용한 모델은 LRNFIS이었고, 다른 모델들과의 비교도 되어있습니다. 한편, 학습을 하는데 LMS를 기반으로 에러를 최소화하는 것을 하는 대신 FHSO를 사용했습니다.​​4.7. Cryptocurrency price forecastingSince cryptocurrencies have become a hot topic in the finance industry in recent years, many studies and implementations have been conducted. Most cryptocurrency studies have focused on price forecasting.최근 몇년간 금융 업계에서 암호화폐는 핫토픽이었기에, 많은 연구에서도 이에 대한 구현이 진행되어오고 있습니다. 대부분의 암호화폐 관련 연구들은 가격 예측에 초점을 두고 있습니다.​The rise of Bitcoin from 1000 USD in January 2017 to 20,000 USD in January 2018 has attracted much attention, not only from the financial industry, but also from the general public. Recently, papers have been published on price prediction and trading strategy development for Bitcoin and other cryptocurrencies. Given the attention that the underlying technology has attracted, there is a strong chance that new studies will appear in the near future.2017년 1000달러였던 비트코인은 2018년 20,000 달러까지 상승하게 되며 일반인 및 금융업계의 엄청난 관심을 받게됐습니다. 최근에는 비트코인 및 다른 암호화폐에 대한 가격 예측과 매매전략에 대한 연구가 많이 나왔습니다. 기반에 깔려있는 기술이 관심을 받고 있다는 점을 생각하면, 이에 대한 새로운 연구들이 계속 나올 가능성은 높다할 수 있습니다.​In the literature, DNN, LSTM, GRU, RNN, and classical methods (ARMA, ARIMA, Autoregressive Conditional Heteroscedasticity (ARCH), GARCH, etc.) have been used for cryptocurrency price forecasting. Table 9 summarizes the studies that utilized these methods. Lopes [181] combined the opinion market and price prediction for cryptocurrency trading. Text mining combined with 2 models, CNN and LSTM, were used to extract opinion. Bitcoin, Litecoin, and StockTwits were used as the dataset. OCHLV of prices, technical indicators, and sentiment analysis were used as the feature set. McNally et al. [182] compared Bayesian optimized RNN, LSTM, and ARIMA to predict Bitcoin price direction. Sensitivity, specificity, precision, accuracy, and RMSE were used as the performance metrics.관련 연구에서는 DNN, LSTM, GRU, RNN 등 뿐만 아닌 기존의 통계학적 방법론인 ARMA, ARIMA, ARCH, GARCH 등 또한 암호화폐 예측에 사용되었습니다. 표9에서는 이러한 방법을 사용한 연구를 정리했습니다. Lopes [181]의 연구에서는 시장의 의견과 가격 예측을 혼합하여 암호화폐 매매를 합니다. 텍스트 마이닝에서는 CNN과 LSTM 두 모델을 합쳤고, 여기에서 시장 의견을 추출합니다. 비트코인, 라이트코인, 스톡트윗의 데이터가 사용됐습니다. 시고저종거, 기술적 분석 지표, 감성분석 등이 입력 특성 데이터로 사용됐습니다. McNally et al. [182]에서는 Bayesian optimized RNN, LSTM, ARIMA를 통해 비트코인 가격의 방향을 예측하고, 서로를 비교헀습니다. 민감도, 특이도, 정밀도, 정확도, RMSE 등의 수치들이 성능 비교의 기준으로 사용됩니다. ​​4.8. Trend forecastingAlthough trend forecasting and price forecasting share the same input characteristics, some researchers prefer to predict the price direction of an asset instead of its actual price. This alters the nature of the problem from regression to classification, and the corresponding performance metrics also change. However, it is worth noting that these two approaches are still fundamentally the same; the difference is in the interpretation of the output. In the literature, there are different methods for trend forecasting. In this survey, we grouped the articles according to their feature sets, such as studies using only raw time series data (only price data, OCHLV); studies using technical indicators, price data, and fundamental data at the same time; studies using text mining techniques; and studies using various other data. Table 10 summarizes the trend forecasting studies using only raw time series data. Different methods and models have been used for trend forecasting. In Table 10, these are divided into three sub-groups: ANN, DNN, and FFNN models; LSTM, RNN, and Probabilistic NN models; and novel methods. ANN, DNN, DFNN, and FFNN methods were used in some studies. In Das et al. [183], NN with price data was used for trend prediction of the S&P500 stock indexes. Navon et al. [184] combined deep FNN with a selective trading strategy unit to predict the next price. Yang et al. [142] created an ensemble network of several Backpropagation and ADAM models for trend prediction.​추세예측과 가격 예측은 동일한 입력 데이터의 특성을 사용하지만, 몇몇 연구자들은 자산의 실제 가격을 예측하는 대신 가격의 방향을 예측하는 것을 더 선호하기도 합니다. 이것은 문제의 본질을 회귀에서 → 분류로 바꿔버리는 것입니다. 따라서 성능에 대한 지표도 다른 것을 사용하게 되겠죠. 그러나 이러한 두 접근법은 본질적으론 똑같고, 결과물의 해석이 다르다는 것을 유념할 필요가 있습니다. 추세 예측에 대한 여러 방법이 있습니다. 본고에서는 추세 예측에 대한 여러 연구들을 어떤 변수를 사용하는지에 따라 하위 그룹으로 나눌 것입니다. 첫 번째 그룹은 오직 시세데이터만을 사용하는 그룹, 두 번째는 기술적지표, 가격, 펀더멘털 등 동시에 사용하는 그룹. 세 번째는 텍스트 마이닝 기법을 활용하는 그룹. 마지막 4번째는 다양한 다른 데이터들을 사용하는 그룹입니다. 표10에서는 추세 예측을 오직 시세 데이터로만 하는 연구들을 요약했습니다. 각기 다른 모델을 사용하는 것을 볼 수 있습니다. 표10에서는 하위 3개의 그룹으로 나누어서 볼 수 있습니다. 1. ANN, DMM, FFNN / 2. LSTM, RNN, Probavilistic NN / 3. 신개념.첫 번째 그룹에 대해 봐보면, Das et al. [183]에서 가격 데이터와 NN을 활용하여 S&P500 지수를 예측합니다. Navon et al. [184]에서는 Deep FNN과 선택적인 매매 전략 단위를 혼합하여 다음날 가격을 예측합니다. Yang et al. [142]에서는 여러 역전파 및 ADAM 모델로 앙상블 네트워크를 생성해내어 추세 예측을 했습니다. In the literature, LSTM, RNN, and Probabilistic Neural Network (PNN) methods with raw time series data have also been used for trend forecasting. Saad et al. [185] compared Timedelay Neural Network (TDNN), RNN, and PNN for trend detection using 10 stocks from S&P500. Persio et al. [186] compared 3 different RNN models (basic RNN, LSTM, and GRU) to predict the movement of Google stock prices. Hansson et al. [133] used LSTM (and other classical forecasting techniques) to predict the trend of stocks prices. In Shen et al. [187], GRU and GRU-SVM models were used for the trends of the HSI, The Deutscher Aktienindex (DAX), and S&P500 indexes.두 번째는 가격데이터와 LSTM, RNN, Probabilistic NN 모델을 사용해 추세 예측을 한 연구들의 그룹입니다. Saad et al. [185]에서는 Timedelay Neural Network (TDNN), RNN, PNN을 활용해 S&P500에서의 10종목에 대한 추세를 감지하는 것을 만들어 서로를 비교했습니다. Persio et al. [186]에서는 3가지 RNN모델인, RNN/LSTM/GRU 를 활용해 구글의 주가 움직임을 예측합니다. Hansson et al. [133]은 LSTM과 여러 전통적인 예측 기법을 사용해 주가 추세를 예측했습니다. Shen et al. [187]에서는 GRU, GRU-SVM 두 개를 사용해 항셍지수와 독일의 DAX, 미국의 S&P500 지수의 추세를 예측합니다.​There are also novel methods that use only raw time series price/index data in the literature. Chen et al. [188] proposed a method that used a CNN with Gramian Angular Field (GAF), Moving Average Mapping (MAM), and Candlestick with converted image data. In Sezer et al. [189], a novel method of CNN with feature imaging was proposed for prediction of the buy/sell/hold positions of the Exchange-Traded Funds (ETFs)’ prices and Dow30 stocks’ prices. Zhou et al. [190] proposed a method that uses Empirical Mode Decomposition and Factorization Machine based Neural Network (EMD2FNN) models to forecast the directions of stock closing prices accurately. In Ausmees et al. [191], DBN with price data was used for trend prediction of 23 large cap stocks from the OMX30 index.또 가격 혹은 지수의 시계열 데이터를 사용한 여러 새로운 방법론들에 대한 연구도 있었습니다. Chen et al. [188] 연구에서는 변환된 이미지 데이터에 대해 CNN에 GAF, MAM, Candlestick이 겸비된 CNN 모델을 사용할 것을 제안합니다. Sezer et al. [189]의 연구에서는 ETF 가격과 Dow30의 주가에 대한 매수/매도/유지 등에 대한 예측에 대해 특성을 이미지화해버리는 CNN 방법을 제안합니다. Zhou et al. [190]의 연구에서는 Empirical Mode Decomposition and Factorization Machine based Neural Network (EMD2FNN) 모델을 사용하는 방법론을 통해 주시의 종가의 방향을 예측하는 것을 제안했습니다. Ausmees et al[191]에서는 DBN과 가격데이터를 사용해 OMX30 지수의 대형주 23종목에 대한 가격 예측을 수행했습니다.​Some studies have used technical indicators, price data, and fundamental data at the same time. Table 11 summarizes the trend forecasting papers that used technical indicators, price data, and fundamental data. In addition, these studies are clustered into three sub-groups: ANN, MLP, DBN, and RBM models; LSTM and GRU models; and novel methods. ANN, MLP, DBN, and RBM methods were used with technical indicators, price data, and fundamental data in some studies. In Raza et al. [192], several classical and ML models and DBN were compared for trend forecasting. In Sezer et al. [193], technical analysis indicator’s (RSI) buy and sell limits were optimized with GA, which was used for buy–sell signals. After optimization, DMLP was also used for function approximation. Liang et al. [194] used technical analysis parameters, OCHLV of prices, and RBM for stock trend prediction.이제 두 번째 상위 그룹에 대해서 입니다. 기술적지표, 가격 데이터, 펀터멘털 등의 데이터를 사용한 그룹이고, 표11에 이를 활용한 추세 예측에 대한 연구를 요약해두었습니다. 여기에서도 하위 3가지 그룹을 분류합니다. 1. ANN, MLP, DBN / 2. LSTM, GRU / 3. 신개념.첫 번째 하위그룹은 ANN, MLP, DBN, RBM 메델과 함께 기술적 지표, 가격데이터, 펀더멘털 데이터를 사용했습니다. Raza et al. [192]의 연구에서는 몇몇 전통적인 ML 모델과 DBN을 사용했으며 추세예측을 하는 것에 대해 비교분석했습니다. Sezer et al. [193]의 연구에서는 매수매도 지정가 주문에 대한 RSI를 GA알고리즘을 통해 최적화하여, 이를 통해 매수-매도 신호를 만들었습니다. 최적화 이후에는 DMLP를 사용해 함수의 근사치를 추정합니다. Liang et al. [194]에선 기술적지표와 시세 데이터를 사용해 RBM 모델을 사용해 주식의 추세를 예측했습니다. LSTM and GRU methods with technical indicators, price data, and fundamental data were also used in some papers. In Troiano et al. [195], the crossover and Moving Average Convergence and Divergence (MACD) signals were used to predict the trend of Dow 30 stock prices. Nelson et al. [196] used LSTM for stock price movement estimation. Song et al. [197] used stock prices, technical analysis features, and four different ML models (LSTM, GRU, SVM and eXtreme Gradient Boosting (XGBoost)) to predict the trend of stock prices.두 번째 하위 그룹은 LSTM, GRU 모델과 함께 기술적지표, 가격데이터, 펀더멘털 데이터를 사용한 연구들입니다. Troiano et al. [195]에서는 돌파 시그널과 MACD 시그널을 사용해 Dow30의 주식들의 가격 추세를 예측했습니다. Nelson et al. [196]에선 LSTM을 사용해 주가 움직임을 추정했으며, Song et al. [197]의 연구에서는 주가와 기술적 지표를 4개의 ML모델들(LSTM, GRU, SVM, XGBoost)을 사용해 주가 추세를 예측합니다.​In addition, novel methods using CNN with the price data and technical indicators have been proposed. Gudelek et al. [198] converted the time series of price data to 2-dimensional images using technical analysis and classified them with a deep CNN. Similarly, Sezer et al. [199] also proposed a novel technique that converted financial time series data consisting of technical analysis indicator outputs to 2-dimensional images and classified these images using a CNN to determine the trading signals. Gunduz et al. [200] proposed a method using a CNN with correlated features combined to predict the trend of stock prices.세번째는 사용된 모델이 CNN인 하위그룹입니다. Gudelek et al. [198]에서는 주가의 시계열 데이터를 2차원의 이미지 형태로 바꾸어, deep CNN을 활용해 분류했습니다. 이와 비슷한 것으로 Sezer et al. [199]의 연구에서는 기술적 지표를 포함하고 있는 금융 시계열 데이터를 2차원 이미로 변형한 후 CNN을 사용해 분류하여 매매 신호를 만들어내는 새로운 기법을 제안하기도 했습니다. Besides, there have also been studies using text mining techniques. Table 12 summarizes the trend forecasting papers using text mining techniques. Different methods/models are represented by four sub-groups: DNN, DMLP, and CNN with text mining models; GRU model; LSTM, CNN, and LSTM+CNN models; and novel methods. In the first group of studies, DNN, DMLP, and CNN with text mining were used for trend forecasting. In Huang et al. [201], the authors used different models, including Hidden Markov Model (HMM), DMLP, and CNN using Twitter moods, to predict the next day’s movement. Peng et al. [202] used the combination of text mining and word embeddings to extract information from financial news and a DNN model for prediction of stock trends.이 외에도 텍스트 마이닝 기법이 활용된 연구도 있습니다. 표12에는 텍스트 마이닝 기법을 활용해 추세 예측에 대한 연구들 요약해놓았습니다. 여기에서 4개의 하위그룹으로 나눠볼 수 있는데요. 1. DNN, DMLP, CNN / 2. GRU / 3. LSTM, CNN, LSTM+CNN / 4. 신개념 기법첫 번째 그룹은 텍스트 마이닝 기법과 DNN, DMLP, CNN을 활용해 추세를 예측하는 연구들입니다. In Huang et al. [201]의 저자들은 HMM, CNN 등의 다른 모델들과 트위터의 분위기를 추출하여 다음날의 주가 방향을 예측합니다. Peng et al. [202]의 연구에서는 텍스트 마이닝과 워드 임배딩을 활용해 뉴스 데이터로부터 정보를 추출하고 DNN 모델을 활용해 주가 추세를 예측합니다. Moreover, GRU methods with text mining techniques have also been used for trend forecasting. Huynh et al. [203] used financial news from Reuters and Bloomberg, stock price data, and a Bidirectional Gated Recurrent Unit (Bi-GRU) model to predict future stock movements. Dang et al. [204] used Stock2Vec and Two-stream GRU (TGRU) models to generate input data from financial news and stock prices. Then, they used the sign difference between the previous close and next open for the classification of stock prices. The results were better than those of state-of-the-art models.두 번째 그룹은 GRU와 함께 텍스트 마이닝이 활용된 연구입니다. Huynh et al. [203]에서는 로이터 및 블룸버그에서의 뉴스데이터와 주가데이터를 Bidirectional Gated Recurrent Unit (Bi-GRU)과 함께 활용하여 주가 추세를 예측했습니다. Dang et al. [204]에서는 Stock2Vec와 Two-stream GRU (TGRU) 모델을 뉴스과 가격데이터와 함께 활용합니다. 여기에서는 이전날의 종가와 다음날의 시가 사이의 차이(갭상승 & 갭하락)를 분류의 신호로 사용했습니다. 결과는 어떤 최신의 모델보다도 더 나았습니다.​LSTM, CNN, and LSTM+CNN models were also used for trend forecasting. Verma et al. [205] combined news data with financial data to classify stock price movement and assessed them with certain factors. They used an LSTM model as the NN architecture. Pinheiro et al. [206] proposed a novel method that used a character-based neural language model using financial news and LSTM for trend prediction. In Prosky et al. [207], sentiment/mood prediction and price prediction based on sentiment, price prediction with text mining, and DL models (LSTM, NN, CNN) were used for trend forecasting. Liu et al. [208] proposed a method that used two separate LSTM networks to construct an ensemble network. One of the LSTM models was used for word embeddings with word2Vec to create a matrix information input to the CNN. The other was used for price prediction using technical analysis features and stock prices.세 번째 그룹은 LSTM, CNN, LSTM+CNN 그룹입니다. Verma et al. [205]에서는 뉴스와 펀더멘털 데이터를 혼합하여 주가 추세를 분류하고 특정 요인(팩터)로 이것들 평가했습니다. 여기에선 LSTM을 NN 아키텍터쳐로써 사용했습니다. Pinheiro et al. [206]에서는 특성을 기반으로 하는 신경망 언어 모델이라는 새로운 방법론을 제안했으며, 뉴스 데이터와 LSTM을 활용해 주가를 예측합니다. Prosky et al. [207] 에서는 추세 예측을 하는데 감성/분위기 예측과 감성을 기바능로 한 가격예측, 텍스트 마이닝을 통한 가격예측을 DL 모델들(LSTM, NN, CNN)을 활용해 연구했습니다. Liu et al. [208]의 연구에서는 두 개의 분리된 LSTM 네트워크를 이요해 앙상블 네트워크를 구축하는 방법을 제시합니다. 하나의 LSTM 모델은 워드 임베딩을 통해 정보 행렬을 만들어 CNN의 입력데이터를 만들어내고, 다른 LSTM 모델은 주가와 기술적 분석 지표를 활용해 가격을 예측하는데 사용됩니다.​In the literature, there are also novel methods to predict the trend of time series data. Yoshihara et al. [209] proposed a novel method that uses a combination of RBM, DBN, and word embedding to create word vectors for an RNN-RBM- BN network to predict the trend of stock prices. Shi et al. [210] proposed a novel method called DeepClue that visually interpretted text-based DL models in predicting stock price movements. In their proposed method, financial news, charts, and social media tweets were used together to predict stock price movement. Zhang et al. [211] proposed a method that performed information fusion from several news and social media sources to predict the trend of stocks. Hu et al. [212] proposed a novel method that used text mining techniques and Hybrid Attention Networks based on financial news for trend forecasting of stocks. Wang et al. [213] combined technical analysis and sentiment analysis of social media (related financial topics) and created a Deep Random Subspace Ensembles (DRSE) method for classification. Matsubara et al. [214] proposed a method that used a Deep Neural Generative Model (DGM) with news articles using a Paragraph Vector algorithm to create the input vector for prediction of stock trends. Li et al. [215] implemented intraday stock price direction classification using financial news and stock prices.신개념 기법을 제시한 연구들도 있습니다. Yoshihara et al. [209]에서는 RBM과 DBN, 워드 임베딩을 결합하여 RNN-RBM-BN 네트워크의 단어 벡터를 만들어내고, 이를 통해 주가 추세를 예측하는 방법을 제안했습니다. Shi et al. [210]의 연구에서는 DeepClur라고 불리는 새로운 방법론을 제시했는데, 시각적으로 해석되는 텍스트를 기반으로 하는 DL모델이며 이걸로 주가 추세를 예측하는 방법입니다. 여기에서는 뉴스, 차트, 트위터 등의 데이터를 함께 사용해 추세예측을 합니다. Zhang et al. [211]의 연구에서는 뉴스, 소셜미디어 드으이 데이터로부터의 정보를 융합하여 주가를 예측하는 방법론을 제안합니다. Hu et al. [212]의 연구에선 텍스트 마이닝 기법과 뉴스 데이터를 기반으로 하는 Hybrid Attention Networks를 활용해 추세 예측하는 새로운 밥어르 제안하기도 했습니다. Wang et al. [213]는 기술적 지표와 소셜미디어 데이터로부터의 감성분석 데이터(관련 토픽)를 사용해 DRSE를 만들어내는 분류를 위한 새로운 방법을 제시합니다. Matsubara et al. [214]에서는 Deep Neural Generative Model (DGM)와 뉴스기사를 Paragraph Vector 알고리즘을 사용해 주가 예측을 위한 입력데이터를 만들어내는 방법을 제안했습니다. Li et al. [215]의 연구에서는 뉴스와 가격데이터를 사용해 장중 가격의 움직임을 분류하는 모델을 구현했습니다.​Moreover, studies have also used different data variations. Table 13 summarizes the trend forecasting papers using these various data clustered into two sub-groups: LSTM, RNN, and GRU models and CNN models.여기에 더물어 다양한 데이터를 사용하는 연구들도 있었습니다. 표13에는 이에 대한 연구들을 요약했으며 두 개의 하위그룹으로 나누어 살펴보겠습니다. LSTM, RNN, and GRU methods with various data representations have been used in some trend forecasting papers. Tsantekidis et al. [216] used limit order book time series data and an LSTM method for trend prediction. Sirignano et al. [217] proposed a novel method that used limit order book flow and history information to determine stock movements using LSTM. The results of the proposed method were remarkably stationary. Chen et al. [154] used social media news, LDA features, and an RNN model to predict the trend of index prices. Buczkowski et al. [218] proposed a novel method that used expert recommendations (Buy, Hold, or Sell), ensemble of GRU, and LSTM to predict the trend of stock prices.첫 번째 하위그룹은 LSTM, RNN, GRU 방법에 여러 다양한 데이터를 활용하는 연구들입니다. Tsantekidis et al. [216]의 연구에서는 지정가 주문에 대한 시계열 데이터를 LSTM 과 사용해 추세를 예측했습니다. Sirignano et al. [217]의 연구에서는 지정가 주문의 흐름과 과거 정보 데이터를 LSTM과 하용하여 주가 움직임을 예측하는 새로운 방법론을 제시하기도 했습니다. 이에 대한 결과는 상당히 stationary(안정적) 하다는 것이 주목할만 합니다. Chen et al. [154]의 연구에서는 소셜 미디어 뉴스 데이터와 LDA 특성, RNN 모델을 활용해 지구의 추세를 예측했습니다. Buczkowski et al. [218]의 연구에서는 전문가의 추천(매수, 유지, 매도) 데이터를 GRU, LSTM의 앙상블과 함께 하용해 주가의 추세를 예측했습니다.​CNN models with different data representations were also used for trend prediction. Tsantekidis et al. [219] used the last 100 entries from the limit order book to create images for stock price prediction using a CNN. Using the limit order book data to create a 2D matrix-like format with a CNN for predicting directional movement was innovative. In Doering et al. [159], HFT microstructure forecasting was implemented with a CNN.두 번째 하위그룹은 CNN이 함께 활용된 것들입니다. Tsantekidis et al. [219]의 연구에서는 마지막 지정가 주문 100개를 사용해 이미지를 생성해내고 CNN을 활용해 주가를 예측하는 방법을 제안했습니다. 2차원 행렬과 같은 형태를 생성하고, CNN을 통해 주가의 방향을 예측한다는 것 자체가 혁신적입니다. Doering et al. [159]의 연구에서는 HFT 미세구조를 CNN을 통해 예측하는 것을 구현했습니다.​​​5. Current snapshot of the fieldAfter reviewing all research papers specifically targeted at financial time series forecasting implementations using DL models, we are now ready to provide some overall statistics about the current state of the field. The number of papers included in our survey was 140. We categorized the papers according to their forecasted asset type. We also analyzed the studies based on their DL model choices, frameworks for the development environment, datasets, comparable benchmarks, and some other differentiating criteria such as feature sets and numbers of citations, which could not be included in this paper due to space constraints. We will now summarize our notable observations to provide interested research with important highlights within the field.이러한 모든 연구들을 바라본 이후 우리는 이제 이 분야의 현황에 대한 전반적인 통계값을 낼 준비가 되었습니다. 본고에서 포함된 연구들은 총 140개의 연구였스며, 저희는 이걸 어떤 자산을 예측하느냐에 따라 분류했습니다. 그리고 또한 어떤 DL 모델을 사용했는지, 개발환경에 대한 프레임워크는 무엇이었고, 데이터는 뭘 사용했고, 비교대상의 벤치마크는 무엇이었고, 등을 분석했고, 지면이 부족해서 포함되지 못했었던 인용됫수나 feature set 등에 대해서도 분석했습니니다. 이제부터는 주목할만한 관찰 결과들을 요약하고 중요한 하이라이트 또한 제공할 것입니다.​Fig. 5 presents the various asset types that researchers developed their corresponding forecasting models for. As expected, stock market-related prediction studies dominate the field. Stock price forecasting, trend forecasting, and index forecasting were the top three picks for financial time series forecasting research. Fig. 8 illustrates the increasing appetite of researchers to develop DL models for financial time series implementations. Meanwhile, as Fig. 9 indicates, most studies were published in journals (57 of them) and conferences (49 papers), but a considerable number of arXiv papers (11) and graduate theses (6) also exist. So far, 46 papers have been published for stock price forecasting, 38 for trend forecasting and 33 for index forecasting. These studies constitute more than 70% of all studies, indicating high interest. Besides the above, there were 19 papers on forex prediction and 7 on volatility forecasting. Meanwhile cryptocurrency forecasting has started attracting researchers; however, only 3 papers on this topic have been published yet, but this number is expected to increase in the coming years [220]. Fig. 6 highlights the rate of publication counts for various implementation areas throughout the years. Meanwhile, Fig. 7 provides more details about the choice of DL models over various implementation areas.그림5에서는 어떤 유형의 자산을 모델을 통해 예측하려 했는지를 나타냅니다. 기대했듯이 주식 시장과 관련한 예측이 지배적입니다. 주가 예측, 추세예측, 지수 예측이 상위 3개의 금융 시계열 예측의 연구 분야였습니다. 그림8에서는 DL모델을 활용해 금융 시계열을 예측하겠다는 연구들의 관심이 높아지고 있음을 보여주고 있습니다. 그러는 한편 그림9에서 나타내고 있듯이 대부분의 연구들은 저널이나 컨퍼런스를 통해 출판이 되고, 상당수 arXiv 혹은 학위 논문의 형태도 존재했습니다. 현재까지 46개편의 주가 예측에 대한 연구, 38편의 추세 예측, 33편의 지수예측에 대한 연구가 출반되었습니다. 이것들이 전체 연구의 70% 이상을 차지하고 있고, 가장 높은 관심을 받고 있다는것을 알 수 있습니다. 이 외에도 19편의 환율 예측과 7편의 변동성 예측에 대한 연구가 있었습니다. 한편 암호화폐 예측에 대한 연구는 이제 막 관심을 갖고 있는것 같습니다. 아직까진 단 3편의 연구밖에 출판이 되지 않았지만, 여기에 대한 연구는 나날이 증가될 것이라 기대됩니다. 그림6에서는 연도별로 다양한 영역에 대한 논문 발표 비중을 보여줍니다. 한편 그림7에서는 DL모델의 선택의 자세한 내용을 제공합니다.   One of the most important issues for a researcher is where they can publish their findings. During our review, we also carefully investigated where each paper was published. We tabulated our results for the top journals for financial time series forecasting in Fig. 10. According to these results, the journals with the most published papers include Expert Systems with Applications, Neurocomputing, Applied Soft Computing, The Journal of Supercomputing, Decision Support Systems, Knowledge-based Systems, European Journal of Operational Research, and IEEE Access. The interested researchers should also consider the trends over the last 3 years, as tendencies can vary depending on the particular implementation areas.연구자에게 가장 중요한 이슈중에 하다는 그들의 결과를 어디에 출판할 수 있느냐 입니다. 저희가 이 연구를 진행하는 동안에 역시 ㄱ각 논문이 어디에 게재되었는지도 주의깊게 살펴보았습니다. 금융 시계열 예측에 대한 탑저널에 대한 결과를 그림10에 표시했습니다. 이 결과를 따른다면, 가장 많은 논문이 게재된 저널은 Expert Systems with Applications, Neurocomputing, Applied Soft Computing, The Journal of Supercomputing, Decision Support Systems, Knowledge-based Systems, European Journal of Operational Research, and IEEE Access 였습니다. 특정 영역에 따라서 경향은 서로 다를수도 있으므로 최근 3년간의 동향을 고려하기도 해야합니다. Fig. 10. Top journals — corresponding numbers next to the bar graph are representing the impact factor of the journals.Carefully analyzing Fig. 11 clearly validates the dominance of RNN-based models (65 papers) among all others for DL model choices, followed by DMLP (23 papers) and CNN (20 papers). The inner-circle represents all years considered, while the outer circle provides only the studies within the last 3 years. We should note that the RNN is a general model with several versions, including LSTM and GRU. For RNN, researchers mostly prefer LSTM due to its relatively simple model development phase; however, other types of RNN are also common. Fig. 12 provides a snapshot of the RNN model distribution. As mentioned above, LSTM had the highest interest among all with 58 papers, while Vanilla RNN and GRU had 27 and 10 papers, respectively. Hence, it is clear that LSTM is the most popular DL model for financial time series forecasting and regression studies.그림11을 자세하게 분석해보면, DL 모델 선택에 있어서 RNN을 기반으로 하고 있는 모델이 다른 모델들보다 압도적으로 많은 65편의 논문임을 확인할 수 있습니다. 그 뒤를 잇는 것이 DMLP(23편), CNN(20편) 이었습니다. 안쪽 원은 모든 연도에 대한 정보를 담고 있고, 바깥쪽 원은 최근 3년에 대해서 나타내고 있습니다. RNN은 LSTM, GRU 등 여러 응용 버젼이 있음을 유념해야 합니다. RNN의 경우 대부분의 연구들은 주로 개발의 단순함으로 인해 LSTM을 선호하지만, 다른 형태의 RNN 형태 또한 사용이 되기도 합니다. 그림12에서는 RNN 모델의 분포를 나타냅니다. 위에서도 언급되었듯이, LSTM이 가장 큰 관심을 받고 있으며(58편), 그 뒤로 기본RNN(27편), GRU(10편)으로 그 뒤를 잇습니다. 따라서, DL 모델을 활용한 금융 시계열 예측 혹은 회귀 문제에 있어서 가장 인기있는 모델은 LSTM이라는 것을 알 수 있습니다.​  Meanwhile, DMLP and CNN were generally preferred for classification problems. Because time series data generally consist of temporal components, some data preprocessing might be required before actual classification can occur. Hence, many of these implementations utilize feature extraction, selection techniques, and possible dimensionality reduction methods. Many researchers mainly use DMLP due to the fact that its shallow MLP version has been used extensively before and has a proven successful track record for many different financial applications, including financial time series forecasting. Consistent with our observations, DMLP was also mostly preferred in the stock, index, and particular trend forecasting because it is by definition, a classification problem with two (uptrend or downtrend) and three (uptrend, stationary, or downtrend) class instances.한편, DMLP와 CNN은 분류문제에 있어서 일반적으로 선호됩니다. 시계열 데이터는 일반적으로 시간적인 구성요소로 이루어져 있기 때문에 실제 분류를 진행하기에 앞서서 데이터 전처리가 필요할 것입니다. 따라서 많은 구현 사례를 보면 특성 추출, 선택 기법, 차원 축소 방법들이 활용되는것을 볼 수 있습니다. 연구자들이 대새 DMLP를 사용하는 이유는 얕은 MLP 버젼이 광범위하게 사용이 되어왔고 또한 금융 시계열 예측 등의 금융 분야에서 성공적인 것으로 입증을 다져왔기 때문입니다. 저희의 관찰 결과에 일관되게 DMLP는 주식, 지수, 특히 추세예측에서 가장 선호됩니다. 이유는 분류문제의 정의상 두가지(위냐 아래냐) 혹은 세 가지(위냐 횡보냐 아래냐)의 카테고리를 분류하는 것이기 때문입니다.​In addition to DMLP, CNN is also a popular choice for classification-type financial time series forecasting implementations. Most of these studies appeared within the last 3 years. As mentioned before, to convert time-varying sequential data into a more stationary classifiable form, some preprocessing might be necessary. Even though some 1-D representations exist, the 2- D implementation for CNN is more common, mostly inherited through image recognition applications of CNN from computer vision implementations. In some studies [188,189,193,199,219], innovative transformations of financial time series data into an image-like representation have been adapted, and impressive performances have been achieved. As a result, CNN might increase its share of interest for financial time series forecasting in the next few years.이와 더불어 시계열 예측의 분류 문제에서는 CNN도 많이 활용됩니다. 그리고 CNN이 활용된 사례는 대개 최근 3년에 시행되었던 것들입니다. 이전에 언급되었듯이 이간에 따라 변하는 시퀄결 데이터를 분류 가능한 형태의 staionary 하게 만들어 주기 위해 데이터 전체리는 필수 적일 것입니다. 일부는 1차원 표현만 존재하지만, CDD에서는 2차원 형태가 더 일반적이므로, 그래서 몇몇 연구들[188,189,193,199,219]에서는 금융 시계열 데이터를 이미지와 같은 데이터 형태로 전환하는 혁신적인 방법을 도입하여 좋은 성능을 달성하기도 했습니다. 결과적으로 CNN의 관심은 향후적으로 더욱 증가될 것이라 봅니다.​As one final note, Fig. 13 shows which frameworks and platforms the researchers and developers used while implementing their work. We tried our best to extract this information from the papers. However, we must keep in mind that not every publication provided their development environment. Also, most papers did not give details, preventing us from a more thorough comparison chart, i.e, some researchers claimed they used Python, but no further information was given, while some others mentioned the use of Keras or TensorFlow, providing more details. Also, within the ‘‘Other’’ section, the usage of Pytorch has increased in the last year or so, even though it is not visible from the chart. Regardless, Python-related tools were the most influential technologies behind the implementations covered in this survey.마지막으로 그림13은 연구자나 개발자들이 사용한 프레임워크와 플렛폼을 보여주고 있습니다. 저희는 이 정보를 알아내기 위해서 최선의 노력은 다 했지만, 모든 논문에서 이러한 환경을 제공하는 것은 아니라는 걸 알아주십쇼. 암튼간 결론은 최근에는 파이토치의 사용이 많이 증가했고, 또한 대개 파이썬에 관려한 개발환경이었다는 것이 결론입니다.(중간에 날려버리고 내 멋대로 해석했음) ​​​6. Discussion and open issuesFrom an application perspective, even though financial time series forecasting has a relatively narrow focus, i.e., the implementations were mainly based on price or trend prediction, depending on the underlying DL model, very different and versatile models exist in the literature. We must remember that even though financial time series forecasting is a subset of time-series studies, due to the embedded profit-making expectations from successful prediction models, some differences exist, such that higher prediction accuracy sometimes might not reflect a profitable model. Hence, the risk and reward structure must also be taken into consideration. At this point, we will try to elaborate on our observations about these differences in various model designs and implementations.실제 응용의 관점에서 보았을 때, 금융 시계열 예측은 상대적으로 좁은 초점을 갖고 있습니다. 즉, 주로 가격이나 추세 예측에 대한 것이었고, 연구에서 매우 다양한 형태의 DL 형태가 존재했습니다. 금융 시계열 예측은 시계열 분석 연구에서의 하위 카테고리 이지만, 성공적인 예측 모델에는 수익 창출에 대한 기대가 내재되어있기 때문에 예측 정확도가 높다고 해서 수익성 있는 모델이 아닐 수도 있는 등 약간의 차이가 존재한다는 점을 유념해야 합니다(??? 무슨말이지???ㅋㅋ) 따라서 위험과 보상의 구조도 고려해야하는 대상이 되어야합니다. 이 시점에서 다양한 모델 설계 및 구현에서 이러한 차이점에 관한 결과를 자세히 한 번 살펴보려고 합니다. (뭔소리지???ㅋㅋㅋㅋ 읽다보면 감 오겠지 뭐)​​6.1. DL models for financial time series forecastingAccording to the publication statistics, LSTM was the preferred choice of most researchers for financial time series forecasting. LSTM and its variations utilized time-varying data with feedback embedded representations, resulting in higher performances for time series prediction implementations. Because most financial data, one way or another, included time-dependent components, LSTM was the natural choice in financial time series forecasting problems. Meanwhile, LSTM is a special DL model derived from a more general classifier family, namely RNN.출판된 연구의 통계를 보면, 금융 시계열 예측에 대해서 연구자들은 LSTM을 선호했습니다. LSTM 및 변현된 여러 모델과 함께 시간에 따라 변하는 데이터를 활용했기 때문에 시계열 예측 부문에서 높은 성능을 보였습니다. 대부분의 금융 데이터는 어떻게든 시간에 따라 변하는 구성요소가 있기 때문에 LSTM의 선택은 이 문제를 푸는데 있어서의 자연스러운 선택이었을 겁니다. 한편 LSTM은 일반적인 분류기 계층 즉, RNN에서 파생된 DL의 형태입니다.​Careful analysis of Fig. 11 illustrates the dominance of RNNs (which mainly consist of LSTM). As a matter of fact, more than half of the published papers on time series forecasting fall into the RNN model category. Regardless of its problem type, price, or trend prediction, the ordinal nature of the data representation forced researchers to consider RNN, GRU, and LSTM as viable preferences for their model choices. Hence, RNN models were chosen, at least for benchmarking, in many studies for performance comparison with other developed models. Meanwhile, other models were also used for time series forecasting problems. Among those, DMLP had the most interest due to the market dominance of its shallow cousin (MLP) and its wide acceptance and long history within ML society. However, there is a fundamental difference in how DMLP- and RNN-based models were used for financial time series prediction problems.그림11을 다시 한번 봐보면, LSTM으로 주로 구성된 RNN 계열이 재배적입니다. 실제로 시계열 예측에 대한 논문들 절반 이상이 RNN 모델 범주에 속합니다. 풀어야하는 문제의 유형, 가격이나 추세냐에 관계없이 연구자들은 데이터가 ordinal한 형태로 인해 RNN, GRU, LSTM 과 같은 모델을 선택하는 쪽으로 생각할 수 밖에 없었을 것입니다. 따라서 성능 비교를 위한 벤치마크 또한 RNN으로 선택됩니다. 한편 다른 형태의 모델이 선택된 것도 있습니다. 여기에서는 대개 DMLP가 큰 관심을 받습니다. 왜냐하면 얕은 MLP 형태가 ML을 사용하는 사람들 사이에서 역사적으로 길게 사용되어왔기 때문입니다. 하지만, DMLP기반의 모델과 RNN 기반의 모델을 금융 시계열 예측 문제를 어떻게 푸느냐에 근본적인 차이점이 있습니다.​DMLP fits well for both regression and classification problems. However, in general, data order independence must be preserved to better utilize the internal working dynamics of such networks, even though some adjustments can be made through the learning algorithm configuration. In most cases, either trend components of the data need to be removed from the underlying time series or some data transformations might be needed so that the resulting data becomes stationary. Regardless, some careful preprocessing might be necessary for a DMLP model to be successful. In contrast, RNN-based models can work directly with time-varying data, making it easier for researchers to develop DL models.DMLP는 회귀와 분류 문제 모두에 잘 맞습니다. 그러나 일반적으로 학습 알고리즘을 어떻게 구성하느냐로 조정이 가능하기는 하지만, 네트워크 안쪽에서의 작동하는 매커니즘을 더 잘 활용하려면 데이터 순서에 대한 독립성을 보존해야 합니다. 대부분의 경우 데이터의 추세의 형태를 데이터에서 제거하거나 혹은 데이터를 stationary하게 만드는 데이터 변환 작업이 필요할 수 있습니다. 암튼, 섬세한 전처리를 DMLP가 성공적으로 작동하기 위해 필수적입니다. 반면, RNN 기반의 모델은 시간에 따라 변하는 데이터를 그냥 직접적으로 때려박을 수 있고, 이 부분이 연구자들에게 하여굼 더 쉽게 개발할 수 있게 합니다.​As a result, most DMLP implementations had embedded data preprocessing before the learning stage. However, this inconvenience did not prevent researchers from using DMLP and its variations during their model development process. Instead, many versatile data representations were attempted to achieve higher overall prediction performances. A combination of fundamental and/or technical analysis parameters along with other features, such as financial sentiment through text mining, were embedded in such models. In most DMLP studies, the corresponding problem was treated as classification, especially in trend prediction models, whereas RNN-based models directly predicted the next value of the time series. Both approaches had some success in outperforming the underlying benchmark; hence it is not possible to claim superiority of one model type over another. However, as a general rule of thumb, researchers prefer RNN-based models for time series regression and DMLP for trend classification (or buy–sell point identification).결과적으로 대부분의 DMLP 구현들은 학습이 진행되기 이전에 데이터 전처리가 수행됩니다. 그러나 이러한 불편함이 연구자들이 DMLP를 활용하지 못하게 하진 않았습니다. 대신 다양한 데이터 전처리 방법들이 시도되며 전반적으로 높은 성능을 달성해냈습니다. 이러한 모델에는 텍스트 마이닝을 통한 금융에에서의 감성분석과 펀더멘털 및 기술적 지표데이터를 입력 특성으로 함께 사용하기도 합니다. 대부분의 DMLP 연구에서는 특히 추세 예측에 대해선, 문제를 분류의 문제로 삼았고, RNN 기반으로 한 모델은 시계열의 그 다음 값을 직접적으로 예측을 합니다. 두 가지 겁근방법 모두 벤치마크를 초과하는 성능을 보였습니다. 따라서 이 중에 무엇이 더 우월하다는 것을 가리는건 쉽지 않을 것입니다. 하지만 일반적으로 대개 시계열 회귀에 대해선 RNN 기반으로 한 것을, 추세의 분류에 대해선 DMLP 계열을 선호한다고 볼 수 있습니다.​Another model that has increased in popularity recently is CNN. CNN also works better for classification problems, and unlike RNN-based models, it is more suitable for either nontime varying or static data representations. The comments made about DMLP are also mostly valid for CNN. Furthermore, unlike DMLP, CNN mostly requires locality within the data representation for better-performing classification results. One particular implementation area of CNN is image-based object recognition problems. In recent years, CNN-based models have dominated this field, handily outperforming all other models. Meanwhile, most financial data are time-varying, and it might not be easy to implement CNN directly for financial applications. However, in some recent studies, various independent research groups followed an innovative transformation of 1-D time-varying financial data into 2-D mostly stationary image-like data so that they could utilize the power of CNN through adaptive filtering and implicit dimensionality reduction. Hence, with that approach, they were able to develop successful models.최근 각광받고 있는 것은 CNN이며, 분류 문제에서 CNN이 잘 작동하기도 하고, RNN기반의 모델과는 다르게 이는 시간에 따라 변하지 않는 데이터에 더 적합합니다. DMLP에서의 언급들은 대부분 CNN에서도 적용됩니다. 여기에 더불어 DMLP와는 다르게 CNN은 더 나은 분류의 성능을 내기 위해 데이터 내에서의 locality 하는 작업이 필요합니다. CNN이 활용되는 특정한 분야는 이미지 내에서 객체를 인식하는 분야입니다. 최근에 CNN 기반으로 한 모델들이 이러한 영역에서 지배적인 성과를 거우었습니다. 그러는 한편 대부분의 금융 데이터는 시간에 따라 변하기에 CNN을 직접적으로 바로 활용하는 것은 쉽지 않을 것ㅇ비니다. 하지만, 최근의 연구들에서는 1차원의 시간에 따라 변하는 데이터를 stationary한 2차원의 이미지 같은 데이터로 변환하는 혁신적인 방법론들을 만들어내고, 금융 분야에서도 CNN의 필터링 및 차원축소의 강력한 힘을 활용할 수 있게 됐습니다. 그러므로, 이러한 방법론으로도 충분히 성공적이 모델들이 만들어질 수 있었던 것입니다.​There is also a rising trend of using deep RL-based financial algorithmic trading implementations; these are mostly associated with various agent-based models, where different agents interact and learn from their interactions. This field has even more opportunities to offer with advancements in financial sentiment analysis through text mining to capture investor psychology; as a result, behavioral finance can benefit from these particular studies associated with RL-based learning models coupled with agent-based studies.또한, 강화학습을 기반으로하는 금융 알고리즘 트레이딩의 사용이 계속해서 증가하고 있습니다. 대부분은 에이전트 기반으로 한 모델들이 관련되어있고, 여기에서 각각의 에이전트들은 그들 사이의 상호 작용을 통해 학습을 합니다. 이 영역은 투자자의 심리를 파악하기 위한 텍스트 마이닝을 통한 금융 감성 분석에서 더 많은 기회를 만들어 낼 것이며 결과적으로 행동 재무학이 이러한 연구로부터의 이점을 얻을 수 있을 것입니다.​Other models including DBN, AE, and RBM, were also used by several researchers, and superior performances were reported in some of their work. However, interested readers should check these studies case by case to see how they were modeled from data representation and learning perspectives.다른 모델로 DBN, AE, RBM 들 또한 여러 연구자들 사이에서 활용이 되었고 일 부 연구에서는 우수한 성능을 보여주기도 했습니다. 하지만, 여기에 관심있는 사람들은 이러한 연구를 따로 체크해봐야할 필요가 있을 것입니다.​​6.2. Discussions on selected featuresRegardless of the underlying forecasting problem, the raw time series data was almost always somehow embedded directly or indirectly within the feature vector, which is particularly valid for RNN-based models. However, in most other model types, other features were also included. Fundamental analysis and technical analysis features were among the most favorable choices for stock/index forecasting studies.예측문제의 관계없이 시계열 데이터 원본은 거의 항상 feature 벡터에 직간접적으로 내대죄어 있는데, 이는 특히 RNN 기반 모델에 유효했습니다. 그러나 대부분의 다른 유형의 모델들은 다른 형태의 feature가 포함되어있습니다. 기본적분석과 기술적 분석 특성은 주식 및 지수의 예측을 하는데 거의 매번 선택된 것들입니다.​Meanwhile, in recent years, financial text mining has gained particular attention, mostly for extracting investor/trader sentiment. The streaming flow of financial news, tweets, statements, and blogs allowed researchers to build better and more versatile prediction and evaluation models, integrating numerical and textual data. The general methodology involves extracting financial sentiment analysis through text mining and combining that information with fundamental/technical analysis data to achieve better overall performance. It is logical to assume that this trend will continue with the integration of more advanced text and NLP techniques.그러는 한편 최근에는 금융 텍스트 마이님 기법이 각광을 받고 있으며 대개 투자자나 트레이더의 감성을 추출하는데 사용되고 있습니다. 계속해서 흘러나오는 금융 뉴스, 트윗, 재무제표, 블로그 글을 들은 연구자들에 더 성능좋은 예측 및 평가 모델을 만들 수 있도록 해주었으며, 이러한 텍스트 데이터를 수치적인 데이터들과 함께 결합할 수 있게 됐습니다. 일반적인 방법론은 텍스트 마이닝 기법을 통해 금융 감성을 분석하고 이 정보를 펀더멘털/기술적 분석 데이터에 함께 결합해 성능을 높에가는 방식을 따릅니다. 이러한 추세는 더욱 발전된 자연어처리 기술의 방전으로 계속될거라 생각하는 것도 터무늬없진 않을 것입니다.​​6.3. Discussions on forecasted asset typesAlthough forex price forecasting is always popular among researchers and practitioners, stock/index forecasting has always had the most interest among all asset groups. Regardless, price/trend prediction and algo-trading models were mostly embedded with these prediction studies.환율의 예측은 항상 인기있는 부분이지만, 주식/지수 예측이 더 많은 인기를 받아왔었죠. 그럼에도 불구하고 가격/추세 예측과 알고리즘 트레이딩 모델은 이러한 예측 연구에 포함되어있습니다.​These days, financial time series forecasting research on cryptocurrencies is a hot topic. Cryptocurrency price prediction has growing demand from the financial community. Because this topic is relatively new, we might see more studies and implementations in the near future due to high expectations and promising rewards.최근에는 가상화폐의 시계열 예측이 핫한 주제입니다. 가상화폐 가격의 예측에 대한 수요가 나날이 증가하고 있습니다. 이 부분은 상대적으로 새로운 부분이고, 우리는 아마 가까운 미래에 이에 대한 연구와 활용들이 꼳아져 나오리라 봅니다. 기대도 높고 돈도 많이 벌 수 있을것이라 생각하니까?​There were also a number of publications in commodity price forecasting research, particularly for predicting the price of oil. Oil price prediction is crucial due to its tremendous effect on world economic activities and planning. Meanwhile, gold is considered a safe investment and almost every investor, at one time, considers allocating some portion of their portfolio for gold-related investments. In times of political uncertainty, many people turn to gold to protect their savings. Although we did not encounter a noteworthy study for gold price forecasting, due to its historical importance, there might be opportunities in this area in years to come.원자재 가격의 예측에 대한 연구들도 다수 존재했으며 특히 원유의 가격을 예측하는 것들이었습니다. 원유 가격 예측은 우리 경제활동에 엄청난 영향을 미치는 것이기에 중요합니다. 한편, 금은 안전한 투자처로 간주되면 대부분의 투자자들은 다들 한 번 이상 일정 부분을 금과 관련한 투자처에 자산 배분을 해놓아야 하나 고민해봤을 것입니다. 정치적으로 불확실한 시기에는 많은 사람들이 그들의 자산을 보호하기 위해 금과같은 투자처에 몰립니다. 금 가격 예측에 대한 주목할만한 연구는 아직 없지만, 역사적으로 금은 중요했었기 때문에 여기에 대한 연구를 하는 것도 기회의 땅이 될 수도 있을 것입니다.​​6.4. Open issues and future workDespite the general motivation for financial time series forecasting remaining fairly unchanged, the means of achieving financial goals vary depending on the choices and trade-off between traditional techniques and newly developed models. Because our fundamental focus is on the application of DL for financial time series studies, we will try to assess the current state of the research and extrapolate that into the future.금융 시계열 예측에 대한 일반적인 동기는 크게 변하지 않았지만, 재무적 목표를 달성하기 위한 수단은 전통적인 기법과 새로운 기법 사이에서의 딜레마(trade-off)와 그에 대한 선택에 따라 결정됩니다. 우리의 근본적인 초점은 금융 시계열 연구에서의 DL 적용이기 때문에 연구의 현황을 잘 평가하고, 어떤 미래가 펼쳐질지에 대한 생각을 잘 해보겠습니다ㅋㅋ​​6.4.1. Model choices for the futureThe dominance of RNN-based models for price/trend prediction will probably not disappear anytime soon, mainly due to their easy adaptation to most asset forecasting problems. Meanwhile, some enhanced versions of the original LSTM or RNN models, generally integrated with hybrid learning systems, are now becoming more common. Readers should check individual studies and assess their performances to see which one fits the best for their particular needs and domain requirements. We have observed increasing interest in 2-D CNN implementations of financial forecasting problems by converting the time series into an image-like data type. This innovative methodology seems to work quite satisfactorily and provides promising opportunities. More studies of this kind will probably continue in the near future.가격 및 추세의 예측을 위한 모델에서 RNN기반 모델들이 지배적인 현상은 조만간 사라질것 같진 않습니다. 왜냐하면 대부분의 자산에 대해 예측을하는 문제에 쉽게 적용되기 때문입니다. 한편, 원래의 LSTM이나 RNN을 응용한 개선된 버젼들이 점점 더 하이브리드로 통합되는 것이 현재로써는 더 보편적입니다. 그래서 각각 개별적인 모델의 연구들을 체크하고 특정 요구사항과 도메인에서의 조건등에 대해 최적으로 잘 맛는 모델이 무엇인지를 평가해봐야할 것입니다. 저희는 금융 예측 문제를 2차원의 이미지 같은 데이터 형태로 바꿔 CNN을 활용하는 것에 대한 관심이 증가하고 있는 것을 봤습니다. 이런 혁신적인 방법론은 꽤나 잘 작동하고 유망한 기회를 제공하는 것으로 보입니다. 이러한 종류의 연구들은 아마 가까운 미래에 계속 될 것입니다.​Nowadays, new models are generated through older models via modifying or enhancing existing models so that better performances can be achieved. Such topologies include Generative Adversarial Network (GAN), and Capsule networks. They have been used in various non-financial studies; however, financial time series forecasting has not been investigated for those models yet. As such, there can be exciting opportunities both from research and practical points of view.요즘에는 과거의 모델들을 수정하거나 개선하는 방식으로 더 나은 성능을 내는 새로운 모델을 만들어내고 있습니다. 뭐 GAN이나 Capsule Network와 같은 것들이 이에 해당하죠. 이것들은 다양한 비금융 연구에서 사용됐던 것인데, 금융 시계열 예측에 대해선 아직 연구되지 않았습니다. 따라서 실무의 관점과 연구의 관점 모두에서 흥미로운 기회가 될 것 같습니다.​Another DL model that has been investigated thoroughly is Graph CNN. Graphs can be used to represent portfolios, social networks of financial communities, fundamental analysis data, etc. Even though graph algorithms can directly be applied to such configurations, different graph representations can also be implemented for time series forecasting problems. Not much work has been done on this particular topic; however, through graph representations of time series data and implementing graph analysis algorithms or implementing CNN through these graphs are among the possibilities that researchers can choose.또 하나의 섬세하게 연구된 DL 모델로 Graph CNN 이라는 것도 있습니다. 그래프는 포트폴리오나 금융에서의 소셜 네트워크, 기본적 분석 데이터 등을 나타낼 수 있습니다. 그래프 알고맂즘은 직접적으로 이러한 설정사항에 적용될 수 있지만, 시계열 예측 문제에 대해선 나른 그래프 표현이 사용될 수도 있습니다. 이러한 주제에 대한 연구는 아직 많이 진행되진 않았지만, 시계열 데이터의 그래프 표현, 그래프 분석 알고리즘을 구현하거나 이러한 그래프를 CNN을 통해 분석하는 것도 연구자가 해볼 수 있는 기회 중 하나입니다.​As a final note for future models, we believe that deep RL and agent-based models offer great opportunities for researchers. HFT algorithms and robo-advisory systems highly depend on automated algorithmic trading systems that can decide what to buy and when to buy without any human intervention. These aforementioned models can fit very well in such challenging environments. The rise of the machines will also lead to a technological (and algorithmic) arms race between Fintech companies and quant funds to be the best in their neverending search for ‘‘achieving alpha’’. New research in these areas can be exactly what is required.마지막으로, 향후에는 강화학습 및 agent 기반의 모델이 큰 가능성을 제공하리라 생각합니다. HFT 알고리즘, 로보어드바이저 시스템은 인간의 개입없이 매수/매도를 결정할 수 있는 자동화된 알고리즘 트레이딩 시스템에 크게 의존합니다. 앞서 언급된 모델들은 이러한 환경에 잘 적용될 수 있습니다. 하드웨어의 발전은 핀테크 간의 혹은 퀀트 펀드간에 알파를 창출하고자 하는 끊임없는 노력으로 인해 군비경쟁으로 이어질 것입니다. 이런 분야에서의 새로운 연구는 필수적일 것입니다.​​6.4.2. Future projections for financial time series forecastingMost probably, for the foreseeable future, financial time series forecasting will have close research cooperation with other financial application areas, such as algorithmic trading and portfolio management, as was the case before. However, changes in the available data characteristics and introduction of new asset classes might not only alter the forecasting strategies of developers but also force developers to seek new or alternative techniques to better adapt to these new challenging working conditions. In addition, metrics such as Continuous Ranked Probability Score (CRPS) [221] for evaluating probability distributions might be included for more thorough analysis.아마도 가까운 미래에는 금융 시계열 예측이 알고리즘 트레이딩 및 포트폴리오 운용 등과 같은 다른 금융 영역과의 협업으로 이어질 것입니다. 하지만, 사용가능한 데이터의 특성의 변화와 새로운 자산군의 도입은 예측 전략의 개발을 변경시킬 뿐만 아니라, 새로운 혹은 대체적인 기법을 만들어내도록 요구할 것입니다. 게다가, CRPS와 같은 확률 분포를 평가하는 것에 대한 철저한 분석 또한 포함될 것입니다.​One rising trend, not only for financial time series forecasting, but for all intelligent decision support systems, is humancomputer interaction and NLP research. Within that field, text mining and financial sentiment analysis are of particular importance to financial time series forecasting. Behavioral finance may benefit from new advancements in these fields.금융 시계열 예측 뿐만 아니라, 떠오르는 분야중 하나는 인간과 컴퓨터 사이의 상호작용과 자연어처리 연구 입니다. 이러한 영역에서는 텍스트 마이닝과 금융 감성분석과 같은 연구가 금융시계열 예측에서 중요합니다. 행동재무학은 아마 이 분야의 발전으로 더욱 성장할 것입니다.​To utilize the power of text mining, researchers have started developing new data representations, such as Stock2Vec [204], which can be useful for combining textual and numerical data for better prediction models. Furthermore, NLP-based ensemble models that integrate data semantics with time-series data might increase the accuracy of existing models.텍스트 마이닝의 힘을 이용하기 위해 연구자는 새로운 데이터 표현을 개발하는 것을 시작할 것입니다. 예를들어 Stock2Vec과 같은 텍스트 데이터와 수치적인 데이터를 결합해 더 나은 예측 모델을 만들어보는 시도 등이 이에 해당합니다. 이에 더불어 NLP 기반의 앙상블 모델은 감정적인 데이터와 시계열 데이터를 합쳐서 기존의 모델의 정확도를 더욱 증진시킬 것입니다.​One area that can significantly benefit from interconnected financial markets is automated statistical arbitrage trading model development. It has been used in forex and commodity markets before. In addition, many practitioners currently seek arbitrage opportunities in cryptocurrency markets [220] due to the huge number of coins available on various marketplaces. Price disruptions, high volatility, and bid–ask spread variations cause arbitrage opportunities across different platforms. Some opportunists develop software models that can track these price anomalies for instant materialization of profits. Also, it is possible to construct pairs of trading portfolios across different asset classes using appropriate models. It is possible that DL models can learn (or predict) these opportunities faster and more efficiently than classical rule-based systems. This will also benefit HFT studies, which are constantly looking for faster and more efficient trading algorithms and embedded systems with minimum latency. To achieve that, Graphics Processing Unit (GPU)- or Field Programmable Gate Array (FPGA)-based hardware solutions embedded with DL models can be utilized. There is a lack of research accomplished in this hardware aspect of financial time series forecasting and algorithmic trading. Given that there is sufficient computing power available, it is worth investigating the possibility of better algorithms, because the rewards are high.상호적으로 연결된 금융시장에서 큰 이점을 얻을 수 있는 영엽은 바로 자동화된 통계적 차익거래 트레이딩 모델을 만드는 분야일 것입니다. 이는 외화시장, 원자재 시장에서도 사용되어오고 있으며, 많은 사람들은 현재 다양한 코인의 거래가 가능하기에 암호화폐 시장에서도 이러한 차익거래 기회를 찾고자하고 있습니다. 가격 왜곡, 높은 변동성, 호가 스프레드의 변동 등은 통계적 차익거래 기회를 만들어 냅니다. 이러한 가격 이례현상을 추적하여 즉각적인 수익을 창출할 수 있는 소프트웨어 모델을 개발하는 사람들도 많고, 또한 서로다른 자산군들의 페어 트레이딩 모델을 만들어 수익을 창출하고자 하는 사람들도 많습니다. DL 모델을 활용하면, 이러한 기회를 기존의 룰-기반 시스템보다 더욱 빠르고 효과적으로 찾아낼 수 있습니다. 이는 또한 시간적 지연을 최소화하여 효과적인 매매 알고리즘을 찾고자 하는 HFT 연구에 이점을 줄 것입니다. 이를 달성하기 위해 GPU, FPGA 기반의 하드웨어들이 내재된 DL 모델을 활용해야할 것입니다. 하드웨어 측면에서의 금융 시계열 예측이나 알고리즘 트레이딩에 대한 연구는 많이 진행되진 않았습니다. 컴퓨팅 파워가 충분하다는 것을 생각해봤을때, 더 나은 알고리즘을 찾아내면 보상도 높을것이기에 이에 대한 연구를 진행하는 것도 나쁘지 않을 것입니다.​​6.5. Responses to our initial research questionsWe are now ready to address to our initially stated research questions. Based on our observations, our answers to these questions are as follows그러면 이제는 처음에 제시했던 질문에 대한 대답을 한 준비가 되었습니다. 우리의 관찰 결과를 토매로 아래의 질문에 대답을 해보자면,​Which DL models are used for financial time series forecasting? Response: RNN-based models (particularly LSTM) are the most commonly used models. Meanwhile, CNN and DMLP have been used extensively in classification-type implementations (such as trend classification) as long as appropriate data processing is applied to the raw data.어떤 DL 모델이 금융 시계열 예측을 하는데 사용되느냐?RNN-기반의 모델 특히 LSTM이 가장 보편적으로 사용되는데, CNN이나 DMLP 등의 모델또한 분류 유형의 문제를 푸는데 있어서(추세 분류), 데이터 전처리만 적절하게 처리 되기만 한다면, 광범위하게 활용될 수 있습니다.​How does the performance of DL models compare with that of their traditional machine learning counterparts? Response: In the majority of studies, DL models were better than ML ones. However, there were also many cases where their performances were comparable. There were even two particular studies [82,175] where ML models performed better than DL models. Meanwhile, the preference of DL implementations over ML models is growing. Advances in computing power, availability of big data, superior performance, implicit feature learning capabilities, and user friendly model development environment for DL models are among the main reasons for this migration. One important issue that might be worth mentioning is the possibility of the publication bias of DL over ML models. Since DL is more recent than ML, a published successful DL implementation might attract more audience than a comparable successful ML model. Hence, the researchers implicitly might have an additional motivation to develop DL models. However, this is probably a valid concern for every academic publication regardless of the study area [222]. Meanwhile, in this survey, our aim was to extract the published DL studies for financial forecasting without any prior assumptions, so the reader can decide which model works best for them through their own experiences.DL의 성능은 기존의 머신러닝 모델의 성능과 어떻게 비교됐습니까?대부분의 연구에서는 DL 모델이 ML보다 우수하는 것으로 보였습니다. 하지만, 두 개의 차이가 거의 없는 경우도 있었습니다. 그리고 ML이 DL보다 낫다는 결과를 낼 두 개의 연구도 있었습니다. 한편, ML보다 DL에 대한 선호도는 계속 증가하고 있습니다. 컴퓨팅 바워의 발전과 빅데이터의 사용가능성, 우월한 성능, 내재적인 특성 학습 가능, 사용자 친화적인 DL 모델 개발 환경 증가 등이 ML보다 DL이 선호되는 주요한 이유들인것 같습니다. 또 한가지 중요한 점은 ML 모델보다 DL 모델에 대한 논문 개제의 가능성입니다. DL이 ML보다 더 최신의 기술이기 때문에 성공적인 ML보다, 비슷하게 성공적인 DL이라도 더 청중에게는 매력적으로 보일 수 있습니다. 따라서 연구자들은 명시적이진 않더라고 DL에 대한 개발의 동기부여를 더 받을 것입니다. 하지만 이는 모두 연구 분야에 관계없이 학술적인 논문에 대해서만의 생각일 수 있습니다. 본고에서의 목표는 금융 시계열 예측에 대한 출판된 DL 논문들을 사전적인 아무런 가정 없이 추출하는 것이었으므로, 어떤 모델이 가장 적함한지에 대해선 독자 자신만의 경험을 통해 골라낼 수 있을 것입니다.​What is the future direction of DL research for financial time series forecasting? Response: NLP, semantics, and text mining-based hybrid models ensembled with time-series data might be more common in the near future.금융 시계열 예측에 대한 DL 연구의 미래 방향은 무엇입니까?자연어처리, 감성분석, 텍스트마이닝을 기반으로하여 시계열 데이터와 결합된 하이브리드 모델 등 가까운 미래에능 요런게 일반적이지 않을까 싶습니다.​​​7. ConclusionsFinancial time series forecasting has been very popular among ML researchers for more than 40 years. The financial community had a new boost lately with the introduction of DL implementations for financial prediction research, and many new publications have appeared accordingly. In our survey, we wanted to review existing studies to provide a snapshot of the current research status of DL implementations for financial time series forecasting. We grouped the studies according to their intended asset classes along with the preferred DL model associated with the problem. Our findings indicate that although financial forecasting has a long research history, overall interest within the DL community is on the rise through utilization of new DL models; hence, many opportunities exist for researchers.금융 시계열 예측은 ML 분석가들 사이에서 40년 넘게 인기있는 분야였습니다. 금융업계도 예측 분석에 DL을 도입하는 것으로 새로운 활력을 얻고 있으며, 이에 따라 새로운 논문들도 많이 나왔습니다. 본고에서 우리는 기존의 진행된 연구에 대한 현재 상태를 잘 얘기하고 싶었습니다. 저희는 각각의 연구에서 관심을 두고 있는 자산군이 무었인지로 먼저 그루핑을 했고, 각각에 대해서 선호되는 DL이 무엇인지를 보았습니다. 결과물이 나타내는 바로는 금융 시계열 예측은 긴 역사를 갖고있지만, DL 모델의 활용을 통해 DL 모델을 활용하는 사람들 사이에서도 그 관심이 커지고 있고, 따라서 연구자들에게 많은 기회가 존재한다는 것입니다. "
황금연휴가 너무 기대 돼. I’m looking forward to the long weekend.  ,https://blog.naver.com/join5022/222599155910,20211219,"​ Today’s lesson​Day 26 Summary‘너무 기대 돼’ 영어로 어떻게 표현할까? 일단, ‘expect’는 절대 쓰지 않는다는 사실! 그럼 뭐라고 할까? 궁금하다면 이번 lesson을 봐주세요. 우리가 모두 사랑하고 기다리는 황금연휴, 겨울 방학을 생각하면서, ‘빨리 놀고 싶어 죽겠어!’ 이런 문장 패턴도 연습해 볼게요!​​표현 1 : 기대된다고 말하기​I’m looking forward to [동명사]어떤 계획이나 일에 대해 기대하고 있는 ‘excitement’를 표현. ​​예문I’m looking forward to the long weekend. 황금 연휴를 고대하고 있어. 황금 연휴가 기대돼. ​I’m looking forward to my vacation. 나 내 휴가를 고대하고 있어. 휴가가 기대돼. ​I’m looking forward to ‘Speaking Pracrice.’‘스피킹 연습’을 고대하고 있어. ‘스피킹 연습’이 기대돼. ​​추가 연습내 여름 방학이 기대돼. I’m looking forward to my summer break. ​파리 여행이 기대돼. I’m looking forward to the trip to Paris. ​BTS콘서트가 기대돼. I’m looking forward to the BTS concert. ​​표현 2 : 몸이 근질거릴 정도로 하고 싶을 때​I’m dying to [동사 원형]뭔가 너무하고 싶어 몸이 근질근질할 때, ‘하고 싶어 죽겠다’란. 늬앙스의 표현. ​​예문I’m dying to get a cup of coffee. 나 지금 너무 커피가 고파. 커피 한잔 하고 싶어 죽겠다. ​I’m dying to get a new iPhone. 나 새 아이폰 사고 싶어 죽겠다. ​I’m dying to go swimming. 나 수영하고 싶어서 몸이 근질근질해. 수영하러 가고 싶어 죽겠어. ​​추가 연습한국 가고 싶어 죽겠어. I’m dying to go back to Korea. ​너 보고 싶어 죽겠어. I’m dying to see you. ​알아내고 싶어 죽겠어. I’m dying to find out. ​​ Real Conversation​서비스 출시 기대벤처 투자 회사에서 일하는 친구와 단기적으로 기대되는 일들에 대해 얘기하는 중. ​A : Hey, Tracy. How’s life?B : You know. The usual. I’m looking forward to the long weekend. A : I’m looking forward to our dinner Thursday night. B : Yeah. Me too. Do you remember my friend Max? You invested in his company. A : Yes. The speech recognition based language learning app. His service is launching.. next week, I think. B : I’m dying to try their service. A : Me, too. I think such a service could open many doors for language learners.  Do you want to visit his office next week?​  open doors : [표현] 새로운 기회를 열다 "
Class 23: Using inputted files 2nd version ,https://blog.naver.com/athenewpkim/222918974656,20221103,"1st code: changing the spoken code into written text​import speech_recognition as sr(this code is simply for importing another code: speech recognition code)​a=sr.Recognizer()with sr.Microphone() as source:(using the microphone)    print(""말하세요"") (this is what the robot says)    audio=a.listen(source)     (then using recognizer to listen... which makes you speak here..)try:    text=a.recognize_google(audio, language=""ko"")    print(text) (text is what was listened, and then it is asked to be printed.)except sr.UnknownValueError:    print(""인식실패"")(if the recognition system failed)​​​​import time,osimport speech_recognition as srfrom gtts import gTTSfrom playsound import playsound​def listen(recognize, audio):    try:        text=recognize.recognize_google(audio, language=""ko"")        print(f'[사용자] {text}')        answer (text)    except sr.UnknownValueError:        print(""인식실패"")(this code is for recognizing what the speaker said that was also used in the code above.)​def answer(input_text):    answer_text="" ""answer_text)    if ""안녕"" in input_text:        answer_text=""안녕하세요? 반갑습니다""    elif ""날씨"" in input_text:        answer_text=""날씨가 좋습니다""    elif ""종료"" in input_text:        answer_text = ""다음에 또 만나요""        stop_listening(wait_for_stop=False)    else:        answer_text=""다시 한 번 말씀해주세요""        speak(answer_text)​def speak(text):    print(f'[컴퓨터] {text}')    tts=gTTS(text=text, lang=""ko"")    tts.save(""voice.mp3"")    playsound(""voice.mp3"")​    if os.path.exists(""voice.mp3""):        os.remove(""voice.mp3""​)​(here, the real code starts)r=sr.Recognizer()m=sr.Microphone()​speak(""무엇을 도와드릴까요?"")​stop_listening=r.listen_in_background(m,listen)#(마이크, 실행함수)​while True:time.sleep(0.1) "
A Carbon Nanotube Synapse with Dynamic Logic and Learning 2012 ,https://blog.naver.com/forever-sy/222884885434,20220926,"https://pubmed.ncbi.nlm.nih.gov/23281020/ A carbon nanotube synapse with dynamic logic and learning - PubMedA carbon nanotube (CNT) synapse emulates a biological synapse with its dynamic logic, learning, and memory functions induced by the interactions between CNTs and hydrogen ions in an electrochemical cell. A circuit of CNT synapses operates with extremely low-energy consumption and could potentially e...pubmed.ncbi.nlm.nih.gov ​A Carbon Nanotube Synapse with Dynamic Logic and Learning​Volume 25, Issue 12 p. 1693-1698​A Carbon Nanotube Synapse with Dynamic Logic and Learning​Kyunghyun Kim,Chia-Ling Chen,Quyen Truong,Alex M. Shen,Yong Chen​First published: 27 December 2012​https://doi-org-ssl.ocam.korea.ac.kr/10.1002/adma.201203116​​Abstract​A carbon nanotube (CNT) synapse emulates(모방하다) a biological synapse with its dynamic logic, learning, and memory functions induced by the interactions between CNTs and hydrogen ions in an electrochemical cell. A circuit of CNT synapses operates with extremely low-energy consumption and could potentially emulate the functions of the neuronal network.​​​ ​The human brain is composed of neuronal networks connected by ∼1015 synapses. The realization of a physical device with the functions of a biological synapse is essential to emulate neuronal networks.1, 2 Although the synaptic functions could be emulated by large-scale CMOS neuromorphic circuits,3-5 the CMOS circuits consumed substantially more energy than a biological synapse, which make it hard to scale up the circuits to a size comparable with the brain. Electronic synapses such as floating-gate silicon transistors,6 nanoparticle organic transistors,7, 8 resistive switching devices,9-11 memristors,12-14 ionic/electronic hybrid material,15 phase change memory,16-18 conductive-bridging (CBRAM),19 and carbon nanotube (CNT) transistors20 have been demonstrated in the pursuit of the synaptic functions in a single device with scalable and low-energy consumption. In this work, we report a CNT-based electronic device that can emulate the elementary dynamic logic, memory, and learning functions of the biological synapse. The CNT synapse can process spatiotemporally correlated spikes, facilitate memory with long-term potentiation (LTP) or depression (LTD), and learn with spike-timing-dependent plasticity (STDP). The functions of the biological neuronal network could be potentially emulated by an electronic circuit of the CNT synapses for speech recognition, pattern recognition, statistic inference, and other intelligent behaviors.​CNT has been considered as an alternative material to replace silicon in future electronic circuits due to its nanoscale size and unique electronic properties.21 We designed and fabricated the CNT synapse with a transistor-like structure in order to operate it with extremely low-energy consumption (Figure 1a). 0.5 mg single-walled CNTs (Unidym Inc.) with lengths of 0.2–2.0 μm and diameters of 0.8–1.2 nm were dispersed in 20 mL dichloromethane solvent. A randomly aligned CNT network was coated onto the surface of a silicon dioxide layer on a silicon chip by dipping the chip into the CNT solution 20 times. The transistor channel was made from a strip of randomly distributed single-walled CNT network with a width of 8 μm and a length of 30 μm. A 10 nm thick titanium layer and a 50 nm thick gold layer were deposited onto the CNT network to form the source and drain electrodes. A 90 nm thick layer of poly(ethylene glycol) monomethyl ether (PEG, MW = 5000, Sigma-Aldrich Inc.) was spin-coated and cross-linked on the top of the CNT network using electron-beam (e-beam) lithography, and a 15 nm thick Ti layer and an 85 nm thick aluminum layer were deposited onto the top of the PEG layer as a gate electrode. An electrochemical cell was then integrated in the transistor gate by sandwiching the hydrogen-doped PEG electrolyte between the CNT channel and the Ti/Al gate electrode.​​ ​Figure 1​Open in figure viewerPowerPoint​Caption​The structure of a carbon nanotube (CNT) synapse and a post-synaptic current triggered by a pre-synaptic spike. a) A schematic diagram showing the transistor-like structure of a CNT synapse with an electrochemical cell containing hydrogen ions in a polymer electrolyte integrated in its gate. The inset is an AFM image of a random CNT network. The average density of the single-wall CNTs is ∼8/μm2, and the average density of the CNT bundles is ∼2/μm2. b) A scheme showing a biological synapse between a pre-synaptic neuron and a post-synaptic neuron. c) A symbol represents the CNT synapse within a scheme showing that a pre-synaptic potential spike applied on its top gate triggers an excitatory post-synaptic current (EPSC) on its drain. d) A pre-synaptic spike (top) applied on a CNT synapse and EPSC (bottom) triggered by the spikes are shown versus time. The superimposed grey lines represent the EPSCs measured from 20 independent tests, and the red line represents the average EPSC. The insets illustrate the distributions of the mobile hydrogen ions (red balls) in the polymer layer and free electrons (blue balls) in the CNT before (left), during (middle), and after (right) the pre-synaptic spike is applied.​To test the CNT synapse under the condition similar to a biological synapse (Figure 1b), a customized electronic circuit was used to generate and apply pre- and post-synaptic spikes on the gate and drain electrodes of the CNT synapse, and a customized current-to-voltage converter was used to detect the post-synaptic current with low noise (Figure 1c and Supporting Information (SI), Figure S1). A commercial field programmable gate array (FPGA) circuit (FTDI Chip Morph-IC-II) generates spikes with spatiotemporal correlations at a fixed amplitude of 3.3 V. An amplitude converter circuit transfers the amplitudes of the pre- and post-synaptic spikes with amplitudes ranging between −6 V to 6 V. Pre- and post-synaptic spikes were applied on the gate and drain electrodes, respectively, of the CNT synapse, and post-synaptic currents were measured through the drain of the CNT synapse. During the tests, a constant potential VDS = 0.5 V was always applied on the source with respect to the drain. The post-synaptic currents were amplified and converted to analog voltage signals using a customized current-to-voltage converter circuit. Analog voltage signals were then sampled to a computer by an analog-to-digital converter circuit (National Instruments PCI-4472). The synaptic strength was measured from the amplitude of an excitatory post-synaptic current (EPSC) triggered by a pre-synaptic spike with an amplitude of 5 V and 1 ms duration (Figure S2 of the SI). Typical temporal responses of the CNT synapse to a 5 V pre-synaptic spike with 1 ms duration are shown in Figure 1d. The pre-synaptic spike triggers EPSC above the resting current (∼5.2 nA), which reaches a peak value (∼22.4 nA) at the end of the spike, and gradually decays back to the resting current in ∼23.7 ms, which is similar to EPSC in a biological excitatory synapse.22 The EPSC amplitude increased with the increasing amplitude and duration of the pre-synaptic spikes (Figure S4,S5 of the SI). In our experiment, the amplitude and the duration of a pre-synaptic spike for EPSC measurement were fixed at 5 V and 1 ms. The post-synaptic current from CNT synapses can be converted to an inhibitory post-synaptic current (IPSC) by applying VDS with opposite polarity (Figure S6 of the SI). When the PEG polymer with the mobile hydrogen ions in the CNT synapse was replaced by an epoxy polymer layer without any mobile ions in a control device, no EPSC was observed (Figure S3 of the SI). EPSC could be triggered when the pre-synaptic spike drives the hydrogen ions inside the PEG polymer towards the CNT channel, which attracts and accumulates the electrons in the CNTs and induces the post-synaptic current through the CNT channel (Figure 1d, insets). After the spike ends, the hydrogen ions gradually drift back to their equilibrium positions in the polymer, inducing EPSC. The randomly connected CNT network may contribute to the large statistic variation of EPSC. The average energy consumption for the CNT synapse to trigger an EPSC is 7.5 pJ/spike (see Method in the SI), which is significantly lower than the energy consumption by conventional CMOS circuit (900 pJ/spike)3 and is comparable with the energy consumption of memristors (∼10−9 J/spike)12-14 and phase-change memory (∼10−9–10−8 J/spike).16, 17 The energy consumption per spike of the CNT synapse was derived by the integration of the power consumption of the CNT synapses with respect to time during an EPSC triggered by a spike.​The CNT synapse can process temporally correlated spikes and generate temporal analog logic such as paired-pulse facilitation (PPF), which represents short-term plasticity in biological synapses and is essential to decode temporal information in auditory or visual signals.2, 23, 24 PPF was demonstrated in the CNT synapse by applying two successive 5 V, 1 ms pre-synaptic spikes with an inter-spike interval, Δtpre, ranging between 1.1 ms and 31 ms (Figure 2a). As shown in Figure 2b, the EPSC triggered by the second pre-synaptic spike is larger than the EPSC by the first spike. The ratio of the amplitudes between the first EPSC and the second EPSC is plotted versus Δtpre in Figure 2c. The ratio reaches the maximum value (127%) when Δtpre = 1.1 ms, and gradually decreases with increasing Δtpre. When the second spike is applied, the hydrogen ions triggered by the first spike still partially reside near CNTs before they drift back to their equilibrium positions. Thus, the hydrogen ions triggered by the second spike near CNTs are augmented with the ions triggered by the first spike, inducing PPF and the short-term memory in the CNT synapse. The paired spikes, with shorter Δtpre, induce more residual hydrogen ions near CNTs and higher EPSC.​​ ​Figure 2​Open in figure viewerPowerPoint​Caption​Post-synaptic current triggered by dynamically correlated spikes via CNT synapses. a) A scheme showing an excitatory post-synaptic current (EPSC) triggered by a pair of temporally correlated pre-synaptic spikes via a CNT synapse. b) A pair of pre-synaptic spikes and a triggered EPSC are shown versus time. A1 and A2 represent the amplitudes of the first and second EPSCs, respectively. c) The ratio of A2/A1 is plotted as a function of inter-spike interval, Δtpre, between the two spikes. The average values and standard deviations of the A2/A1 ratios are extracted from 20 independent tests under a fixed Δtpre, and represented by the dots and error bars, respectively. The ratio of A1 and A2 was statistically analyzed using the paired sample t-tests. P < 0.01 was for all data points, which reveals that A2/A1 ratios are statistically meaningful. d) A scheme showing EPSC triggered by a pair of spatiotemporally correlated spikes applied through two CNT synapses. e) EPSCs triggered by single and pairs of spatiotemporally correlated pre-synaptic spikes are shown versus time. f) The amplitude of the EPSC at t = 0 (when the spike is applied on the Synapse 1) is plotted as a function of Δtpre2-pre1 between the two pre-synaptic spikes. The average values and standard deviations of EPSC amplitudes are plotted by the dots and error bars from 20 independent tests under a fixed Δtpre2-pre1.​Pre-synaptic spikes from different neurons can trigger a post-synaptic current through synapses in a post-synaptic neuron to establish dynamic logic in a neural network.25, 26 A basic dynamic logic function is demonstrated in a simple network with three neurons connected by two CNT synapses (Figure 2d). When two 5 V, 1 ms pre-synaptic spikes with an inter-spike interval, Δtpre2-pre1, are applied at the CNT Synapse 1 and the CNT Synapse 2 respectively, they trigger two EPSCs (EPSC1 and EPSC2) with different amplitudes and are summed in the post-synaptic neuron, which is a dynamic analog function of time and Δtpre2-pre1 (Figure 2e). When Δtpre2-pre1 = 0, the EPSC1 and EPSC2 are triggered simultaneously, the amplitude of the EPSC in the post-synaptic neuron reaches the maximum value. As shown in Figure 2f, the amplitude of the EPSC at t = 0 (when the pre-synaptic spike is applied on the Synapse 1) decreases asymmetrically with increasing |Δtpre2-pre1|: when EPSC1 from the CNT Synapse 1 is triggered earlier than EPSC2 from the CNT Synapse 2 (Δtpre2-pre1 > 0), the amplitude of EPSC at t = 0 is equal to the peak amplitude of EPSC1, whereas EPSC1 from the CNT Synapse 1 is triggered later than EPSC2 from the CNT Synapse 2 (Δtpre2-pre1 < 0), the amplitude of EPSC at t = 0 is the summed current of EPSC1 from the CNT Synapse 1 and EPSC2 from the CNT Synapse 2. When Δtpre2-pre1 decreases further (Δtpre2-pre1<0), the increase in the EPSC amplitude at t = 0 by EPSC2 from the CNT synapse 2 is gradually less significant.​Long-term synaptic plasticity has been realized as the underlying mechanism for learning and memory in the brain.2, 27 The long-term plasticity behavior of the CNT synapse was demonstrated by modifying the synaptic strength with pre- and post-synaptic spikes (Figure 3a). In our experiments, the synaptic strength is measured every 2 seconds by the amplitude of EPSC triggered by a 5 V, 1 ms pre-synaptic spike. The relative change of the EPSC amplitude was calculated by the comparison between the average EPSC amplitudes of the first 1000 seconds before stimulation and the last 1000 seconds, and the statistical significance was tested by the paired sample t-test. As shown in Figure 3b, after applying two hundred 5 V, 1 ms post-synaptic spikes with an inter-spike interval, Δtpost = 2 ms, the EPSC amplitude is increased (P < 0.05), resulting in long-term potentiation (LTP) in the CNT synapse; whereas after applying one hundred 5 V, 1 ms pre-synaptic spikes with Δtpre = 2 ms, the EPSC amplitude is decreased (P < 0.05), inducing the long-term depression (LTD). The synaptic strength in a CNT synapse can be continuously and reversibly modified to desired analog values by applying a series of pre- and post-synaptic spikes with different amplitudes. The rate of change in the synaptic strength increases with the increasing amplitudes of spikes (Figure S7 of the SI). However, when hundreds of 5 V, 1 ms pre- and post-synaptic spikes with Δtpre and Δtpost > 20 ms are applied, no significant change of the synaptic strength is observed (Figures 3c and 3d). We have previously observed that a positive potential (i.e. a pre-synaptic spike) applied on the electrochemical cell induces the electrochemical reaction between CNTs and the mobile hydrogen ions, resulting in the hydrogenation of the CNTs and the decrease of their conductivity; a negative potential (i.e. a post-synaptic spike) reverses the electrochemical reaction, resulting in the dehydrogenation of the CNTs and the increase of their conductivity (Figure 3d, insets).28 Therefore, a series of the pre-synaptic spikes on the CNT synapse with Δtpre ≤ 20 ms accumulatively induces LTD; a series of the post-synaptic spikes with Δtpost ≤ 20 ms reversibly induces LTP. Nonetheless, when a series of the spikes are applied with Δtpre or Δtpost > 20 ms, the hydrogen ions in the polymer can be drifted back to their equilibrium positions within Δtpre or Δtpost, thus, no accumulative plasticity (LTD or LTP) is observed in the CNT synapses.​​ ​Figure 3​Open in figure viewerPowerPoint​Caption​The long-term plasticity of a CNT synapse. a) The stimulation protocol to induce long-term potentiation (LTP) and depression (LTD). Left hand side shows LTD stimulation: A series of 5 V 1 ms pre-synaptic spikes (blue lines) with a fixed inter-spike interval, Δtpre, is applied (starting at t = 0 s, as marked by the arrow). Right hand side shows LTP stimulation: A series of 5 V, 1 ms post-synaptic spikes (red lines) is applied with a fixed inter-spike interval, Δtpost. The synaptic strength of the CNT synapse is tested consecutively every 2 seconds by measuring the amplitudes of EPSC triggered by the 5 V, 1 ms pre-synaptic spikes (black lines). The relative change of the EPSC amplitude was calculated by the average EPSC amplitude of the first 1000 seconds (n = 500) before stimulation and the last 1000 seconds (n = 500) of experiments, and the statistical significance was tested by the paired sample t-test. b) The EPSC amplitudes in a CNT synapse are plotted as a function of time when the LTP (red) and the LTD (blue) stimulations are applied respectively at t = 0 s (marked by the arrow) with Δtpre = 2 ms and Δtpost = 2 ms (P < 0.05). c) The EPSC amplitudes in a CNT synapse are plotted as a function of time when the LTP (red) and the LTD (blue) stimulations are applied respectively at t = 0 s (marked by the arrow) with Δtpre = 101 ms (P = 0.05) and Δtpost = 101 ms (P = 0.94). There is no significant change in the EPSC amplitude. d) There was statistically significant change of the EPSC amplitude with Δtpre and Δtpost ≤ 20 ms (P < 0.05). The relative changes of EPSC amplitudes in a CNT synapse are plotted versus Δtpre and Δtpost under the LTP (red) and the LTD (blue) stimulations. The insets illustrate the CNT hydrogenation after the LTD stimulation with Δtpre ≤ 20 ms (bottom, left), the CNT dehydrogenation after the LTP stimulation with Δtpost ≤ 20 ms (top), and no significant change after the LTD or LTP stimulations with Δtpre or Δtpost > 20 ms (bottom, right).​In a biological synapse, the synaptic strength is modified by the dynamically correlated spikes following an essential learning rule — spike-timing dependent plasticity (STDP).29 Following the test protocol in neurological experiments,30-32 100 pairs of a 4 V, 1 ms pre-synaptic spike and a 5 V, 1 ms post-synaptic spike with a fixed inter-spike interval, Δtpost-pre, were applied on a CNT synapse with a frequency of 1 Hz (Figure 4a). The typical EPSC amplitudes as a function of time, measured in a STDP test with Δtpost-pre = 1 ms between the stimulation spike pairs, are shown in Figure 4b. The average EPSC amplitude was increased by ∼32% after the CNT synapse experienced the stimulation spike pairs. The changes of EPSC amplitude were measured under different Δtpost-pre ranged between -20 ms and 20 ms and were summarized in Figure 4c. STDP-like behavior was observed in the CNT synapse: when the pre-synaptic spikes are applied before the post-synaptic spikes (Δtpost-pre > 0), the EPSC amplitude is increased, resulting in long-term potentiation (LTP); when the pre-synaptic spikes are applied after the post-synaptic spikes (Δtpost-pre < 0), the EPSC amplitude is decreased, resulting in long-term depression (LTD). The data points in Figure 4c are statistically scattered, which has also been observed in biological synapses.33 The change of the EPSC amplitude, ΔW, is fitted as a function of Δtpost-pre, by an exponential function  for Δtpost-pre > 0 and  for Δtpost-pre < 0 with the result that A+ = 9.29% and τ+ = 7.68 ms, and A− = −10.76% and τ− = 12.0 ms. The A and τ values are comparable with those values observed in biological synapses.29, 31, 34​​ ​Figure 4​Open in figure viewerPowerPoint​Caption​Spike-timing dependent plasticity (STDP) of a CNT synapse. a) (Top) 100 pairs of a pre-synaptic spike and a post-synaptic spike are repeatedly applied to a CNT synapse with a fixed inter-spike interval, Δtpost-pre, in a STDP stimulation. Schemes showing hydrogen ions (red circles) induced by the pre-synaptic spike on a CNT surface (bottom, left); and CNT dehydrogenation induced by the post-synaptic spike (bottom, right). b) EPSC amplitude is plotted as a function of time when the stimulation of 100 spike pairs with Δtpost-pre = 1 ms is applied at t = 0 s (marked by the arrow). Typical EPSCs measured before and after the STDP stimulation are shown in the insets. c) The relative change of the average EPSC amplitude, measured at 20 minutes before and 40 minutes after the stimulation spike pairs is shown as a function of the inter-spike interval, Δtpost-pre. Blue open circles represent experimental data fitted by red dashed curves.​When pairs of the pre- and post-synaptic spikes are applied with |Δtpost-pre| > 20 ms, the spikes could be considered as independent stimulations to the device equilibrium state; no significant change of the synaptic strength is observed (Figure 4c). When a pre-synaptic spike is applied shortly before a post-synaptic spike with Δtpost-pre < 20 ms, as illustrated in Figure 4a, the pre-synaptic spike might induce ionized hydrogen on the CNT surface. Before the hydrogen ions are neutralized and back to their equilibrium state, the post-synaptic spike might break the hydrogen–CNT bond, inducing CNT dehydrogenation and LTP. When a post-synaptic spike is applied shortly before a pre-synaptic spike with Δtpost-pre < 20 ms, the post-synaptic spike might induce ionized carbon atoms on the CNT surface, and the following pre-synaptic spike might induce the hydrogen ions to drift from the polymer toward the CNT, inducing CNT hydrogenation and LTD. A spike pair with a shorter Δtpost-pre would induce more remarkable hydrogenation or dehydrogenation, resulting in more significant LTD or LTP. Although STDP-like behavior has been observed in the electronic devices, the complicated pulsing scheme of the pre- and post-synaptic spike by peripheral circuits was needed,8, 15-17 or the change of the synaptic weights was smaller than those observed in the biological synapses.10, 13 In this work, the STDP behavior can be achieved by applying the pre- and post-synaptic spikes directly on CNT synapses in the simple square wave voltage pulsing scheme, which enables the CNT synapses to be directly integrated in a circuit.​In summary, we have developed a CNT synapse with the elementary dynamic logic, learning, and memory functions of a biological synapse. The CNT synapse is operated based on the dynamic interactions between CNTs and hydrogen ions in an electrochemical cell integrated in the CNT synapse. The CNT synapse consumes extremely low energy (∼7.5 pJ/spike), and the energy consumption could be significantly reduced further by scaling down the device. The CNT synapse could potentially be engineered to excitatory and inhibitory synapses with a positive VDS and a negative VDS. The CNT synapses could be integrated in a large-scale circuit to emulate the massively parallel signal processing and learning functions of a biological neural network for speech recognition, pattern recognition, statistic inference, and other intelligent behaviors.​Supporting Information​Supporting Information is available from the Wiley Online Library or from the author.​Acknowledgements​The authors acknowledge the supports of this work by the Air Force Office of Scientific Research (AFOSR) under the program “Bio-inspired intelligent sensing materials for Fly-by-Feel autonomous vehicle” (contract number: FA9550-09-1-0677). The authors are thankful to Dr. Yong-Sik Ahn for the developments of the materials and device-fabrication process.​​​ "
딥 러닝 ,https://blog.naver.com/yousl97/222902700149,20221017,"< 머신러닝 >지도 학습비지도 학습강화학습​< 딥러닝, Deep Learning > : 인공신경망(ANN; Artificial Neural Networks): 은닉층(Hidden Layer)을 깊게 쌓은 구조를 이용해 학습하는 기법: 데이터의 특징을 단계별로 추상화(복잡도)를 높여가면서 학습: 얕은 은닉층은 점, 선, 면과 같은 추상화 단계가 낮은 특징을 학습: 깊은 은닉층은 얼굴의 눈, 코, 입 등 추상화 단계가 높은 특정을 학습​잘 작동하는 문제영역이미지, 자연어, 음성 등의 비정형화된 대량의 데이터로부터 인식을 수행하는 문제영역에 잘 작동잘 작동하지 않는 문제영역데이터가 부족하거나, 정형화된 데이터에 대해서는 상대적으로 잘 작동하지 않음​CNN; Convolution Neural Network(합성곱 신경망): 시각 세포의 작동 원리를 본떠서 만들고 이미지를 특정한 영역별로 추출하여 학습시킴: 비전에서 주로 filter 연산을 뜻하며 이미지 특징(feature)를 찾기 위해 filtering을 수행: 이미지, 영상 분류와 인식에 유용함(특징 추출과 분류)​RNN; Recurrent Neural Network(순환 신경망): 순차적인 정보를 처리하는데 사용되는 알고리즘으로 다양한 시계열데이터, 자연어처리(NLP)문제에 대해 뛰어난 성능을 보이고 있는 인기있는 모델: 전후 관계에 대한 패턴을 학습하는 알고리즘: 자동 완성기능, 음성인식기능, 번역기능, 챗봇, 자막, 작곡, 기사작성, 소설 창작 등​딥러닝 알고리즘의 주요 응용 분야Computer VisionNatural Language Processing(NLP)Speech RecognitionGameGenerative Model​https://teachablemachine.withgoogle.com/ Teachable MachineTrain a computer to recognize your own images, sounds, & poses. A fast, easy way to create machine learning models for your sites, apps, and more – no expertise or coding required.teachablemachine.withgoogle.com 에포크 : 전체 데이터를 확인하는 횟수 "
[뉴질랜드 유학] 오클랜드 지역 유니텍 대학 컴퓨팅 석사과정 Master of Computing (가족비자 지원가능) ,https://blog.naver.com/sisnzkorea/222886278673,20220928,"​안녕하세요,한국(서울, 부산)-뉴질랜드(오클랜드, 크라이스트처치, 웰링턴) 에 100% 직영지사를 운영중인SiS 신인수유학원입니다.​오늘은 오클랜드 지역에서 비교적 저렴한 비용으로 컴퓨팅 석사과정을 할 수 있는 유니텍 대학에 대해서 소개해드리도록 하겠습니다.​ 유니텍 대학컴퓨팅 석사과정Master of Computing© StockSnap, 출처 Pixabay레벨9크레딧240입학시기2월 또는 7월입학 영어점수아이엘츠 6.5 (각 밴드 6.0 이상) 유니텍 대학에서 제공하는 컴퓨팅 석사과정은 체계적인 연구를 수행하고 복잡한 응용 컴퓨팅 문제를 해결할 수 있는 기술을 개발 할 수 있도록 훈련시키는데요. 특히 네트워크 및 보안, 인터넷 및 인트라넷, 대화형 멀티미디어, 데이터 마이닝, 적응형 비지니스 인텔리전스 및 건강 정보학을 포함하여 빠르게 발전하는 컴퓨팅 분야에 초점을 맞춰서 배울 수 있습니다.​ Highlights© Boskampi, 출처 Pixabay✅ 주말에 이루어지는 캠퍼스 수업과 스스로 주도하에 이루어지는 연구의 조합으로 수업이 이루어집니다. 부족한 수업은 따로 온라인 커뮤니케이션으로 보충이 이루어집니다.✅ 다양한 범위의 연구주제와 슈퍼바이저가 있어서 학생들이 흥미있는 분야를 연구할 수 있도록 서포트합니다.✅ 유연성: 연구 프로젝트의 120 크레딧을 수강할 지, 수업을 들으며 90 크레딧의 논문과정을 수강할 지 선택가능합니다. ✅ 팀기반의 프로젝트를 통해서 커뮤니케이션, 리더쉽, 협력을 함으로써 학생들은 리더쉽과 관리능력을 기를 수 있습니다.✅ 경력있는 전문가인 강사로부터 배웁니다.✅ 최신시설의 실험실이 일주일내내 열려있어서 학생들이 사용 가능합니다.✅ 개별적인 서포트: 새로 등록한 학생들의 경우 프로그램 리더가 과목 선택을 도와 시간표 작성을 위한 서포트를 제공합니다.​ 전공분야© ikukevk, 출처 Unsplash💡 사이버 보안 (Cybersecurity)사이버 보안에 대한 이론적, 실제적인 이해를 할 수 있는 인재를 양성합니다.​💡 IT 관리 (IT Managment)IT 관련 경력을 더 쌓고 싶은 분들을 위한 과정으로 IT 프로젝트 관리, IT 전문가를 위한 관리 접근 방식, IT, 전략적 계획 중에서 선택해서 심도있게 배우게 됩니다.​💡 컴퓨터 네트워크와 클라우드 컴퓨팅 (Computer networks and cloud computing)여러 기업들은 네트워크를 관리하는데 인력이 필요한 상태이며 컴퓨터와 관련된 25%는 이에 관련된 직업들입니다. 클라우드 컴퓨팅은 비교적 새로 나온 개념으로 점점 더 시장에서 필요로 하는 기술입니다.​💡 소프트웨어 개발 (Software development)소프트웨어 엔지니어는 세계적으로 가장 중요한 직업 중 하나로 평가받고 있는데요. 이런 역동적이고 변화가 많은 소프트웨어 개발 분야는 졸업생들에게 소프트웨어 솔루션고 제품을 설계하고 개발할 수 있도록 훈련시킵니다.​💡 비지니스 인텔리전스 (Business intelligence)기업들에게 수집하는 데이터의 양과 속도는 최근 몇년동안 전례없는 수준으로 많다고 알려져있는데요. 하지만 분석없이 쌓여만 있는 데이터는 단지 저장용량만 늘릴 뿐인데요. 비지니트 인텔리전스는 비지니스와 데이터 베이스, 데이터 웨어하우징, 데이터 마이닝, 소프트웨어 개발을 복합시킨 학문으로써 조직들이 데이터들을 미래 계획에 사용할 수 있도록 배우는 과정입니다. 비지니스 인텔리전스는 기업이 그들의 비지니스를 이해하고 효과적으로 경쟁하며 해당분야에서 탁월해질 수 있도록 돕습니다. 이 과정을 통해서 학생들은 데이터 웨어하우스 솔루션을 만들어내고 관리하는 방법을 배우며 변화하는 시장상황에 맞추어서 복잡한 기업의 전략에 맞도록 데이터를 모델 및 보고서를 개발하여 비지니스 의사결정을 돕는 방법을 배울 수 있습니다.​💡 논문 연구 주제들다양한 주제로 논문 연구를 진행할 수 있도록 유니텍 컴퓨팅 석사학위에선느 지원하고 있는데요.아래와 같은 주제들도 연구가 가능합니다.​✔ Computational Finance (컴퓨터 금융)✔ Health Informatics (건강 정보학)✔ Robotics Applications (로봇 응용 프로그램)✔ Business Process Modelling (비지니스 프로세스 모델링)✔ Cloud Computing (클라우드 컴퓨팅)✔ Mobile Networks (모바일 네트워크)✔ Mobile and Wireless Communications (모바일 및 무선통신)✔ Business Intelligence (비지니스 인텔리전스)✔ Computer Vision/Speech Recognition (컴퓨터 비전/음성 인식)✔ Natural Language Processing (자연어 처리)✔ Machine Translation (기계 번역)✔ Data Mining (데이터 마이닝)✔ Process Mining (프로세스 마이닝)✔ Social Networks (소셜 네트워크)​ 비자와 취업© thisisengineering, 출처 Unsplash유니텍의 컴퓨팅 석사과정을 하는동안 배우자에게는 워크비자가, 자녀에게는 국공립 학교를 무료로 다닐 수 있는 도메스틱 학생비자가 발급됩니다. 학교를 졸업하면 졸업생에게는 3년의 포스트 스터디 워크비자가 발급되여 가족들에게도 이어서 워크비자, 도메스틱 학생비자를 지원받을 수 있어 총 5년동안의 비자 지원이 가능합니다.​유니텍 컴퓨팅 석사과정을 졸업하게 되면 그린리스트에 속한 다양한 IT 직군으로의 취업도 용이해 지는데요.그린리스트에 속해있는 IT 직군과 영주권 패스웨이로 신청하기 위한 조건은 아래와 같습니다.​​✅ Chief information officer (시간당 NZD $57.69 임금 또는 이와 동등한 연봉)✅ ICT project manager  (시간당 NZD $57.69 임금 또는 이와 동등한 연봉)✅ ICT managers nec  (시간당 NZD $57.69 임금 또는 이와 동등한 연봉)✅ Software engineer  (시간당 NZD $57.69 임금 또는 이와 동등한 연봉)✅ ICT security specialist  (시간당 NZD $57.69 임금 또는 이와 동등한 연봉)✅ Multimedia specialist  (시간당 NZD $45.67 임금 또는 이와 동등한 연봉)​위 직군으로 취업하게 되고 조건에 부합하면 뉴질랜드에서 따로 경력을 쌓지 않아도 바로 영주권 신청이 가능한 영주권 패스웨이를 통해서 바로 영주권 신청이 가능합니다.​ 2022 추계 코엑스뉴질랜드 유학, 이민 박람회에여러분을 초대합니다!제 53회를 맞이하는 이번 박람회에서는 뉴질랜드 종합대학교/폴리텍/대학파운데이션/ 초중고등학교/대학부설 어학원 등 뉴질랜드 교육기관 22개 학교가 저희 SiS 신인수 유학원과 함꼐 직접 여러분을 만날 뵐 예정입니다. ​✅ 일시: 2022년 10월 22일~23일오전 11시~오후 6시​✅ 장소: 서울 코엑스 3층 C홀​ ※ 뉴질랜드 박람회 참가신청을 해주시면 2022 추계 뉴질랜드 해외 유학/이민 박람회 무료 입장권을 보내드립니다.※ 뉴질랜드 박람회에 직접 참석이 어려우신 분은 온라인 상담신청을 해 주시면, 박람회와 동일한 혜택과 함께 언제든지 성심껏 답해드리겠습니다.​​​SiS 신인수 유학원의 2022 추계 코엑스뉴질랜드 유학, 이민 박람회 ​  이상으로 가족 비자 혜택을 5년동안 받을 수 있으며 그린리스트 직종으로 취업도 가능한 유니텍 대학의 컴퓨팅 석사과정에 대해서 안내드렸는데요. 이 과정의 입학등에 대한 궁금하신 분들은 아래로 많은 문의바랍니다. ​​ "
[건강 정보]하쥬 체니 증후군 처리와 완화 확인 해요 ,https://blog.naver.com/yqdfk74625/222895399124,20221009,"[건강 정보]하쥬 체니 증후군 처리와 완화 확인 해요​​​안녕하십니까. 이번 포스팅은 하쥬 체니 증후군의 처리와 완화에 관한 하여 살펴보는 사항을 예비했습니다. 다양한 사람이 가진 하쥬 체니 증후군에 관한 하여 주말에도 다양한 의문을 느끼고 있는 상태죠. 그럼에도 아무래도 이에 관한 하여 정확하게 알고 있는 사람이 많지 않은 상태죠. 게다가 요즘 같은 체온의 변화까지 커다란 계절이 가까워지면 건강상태에 문제점이 나타나는 상황이 여러 남성들이 있었어요. 몇 주 동안 변화된 특이한 점이 피부로 와닿는 경우라면 가진 하쥬 체니 증후군를 연결하지 않을 수 없는데요. 다양한 사람이 솔루션이 없다고 연결되는 상황이 있었으나 이런 내용은 주말에 어떻게 체력을 유지하며 있는지에 유무로 대처까지 가능하기 때문에 유지가 이만큼 필수적인 하쥬 체니 증후군라고 할 수 있었어요. 그럼에도 안쓰럽게도 여겨질 수 있는 특이한 점이 있는 상황에서도 다양한 사람이 가진 하쥬 체니 증후군에 관한 하여 단순한 부분으로 구분하고 증상들이 연결되는 상황이라고 한다면도 치료하지 않는 남성들이 있었어요. 이러한 하쥬 체니 증후군는 문제점이 발생한 상황이라고 한다면 어떠한 것 보다 단기간에 치료를 시작하는 것이 가장 핵심적이며 효능이 피부로 와닿는 해결방법도 될 수도 있었어요. ​​건강을 유지하는데 효율적인 신체를 유지하는 방식은 적혀있지 않지만, 많은 증상들이 이와 같은 평소 정신적인 처리와 비타민B가 함유된 건강을 유지하는데 효율적인 식습관이나 적절한 족구를 자주하여 높은 면역력을 키우는 것이 필요한데요. 어떠한 것 보다 주말에 스스로가 건강 처리에 관한 하여 무엇들을 하고 있는지 가끔씩은 연결 하는 것도 활력을 주지 않을까 싶네요. 정리한다면 이시간 알 수 있는 효율적인 건강 관한 용어에 관한 하여 남겨보고 마무리 해보죠. 미소 짓는 퇴근시간되세요.​double focus x ray tube : 이중초점엑스선관water silk retina : 비단망막hyperlethal : 치사량이상 REM latency : 렘잠복기 / 렘수면잠복기lacunar stroke : 열공중풍aphalgesia : 접촉통증glandular endometritis : 샘자궁내막염idiopathic cutaneous neuroma : 특발피부신경종sperm acidity : 정액산도toe to thumb transfer : 발가락엄지가락전이(술)juxtamedullary nephron : 속질곁콩팥단위speech recognition threshold : 언어인지문턱​ "
[Proposed DNN based SRAM Cell Opt (2)] DNN 기반 Sensitivity Analysis를 이용한 VariationAware SRAM Cell 최적화 ,https://blog.naver.com/uu5626/222653131377,20220221,"이전 글에서 PROPOSED DNN-BASED SRAM CELL OPTIMIZATION에 대해 아주 개략적인 Overview를 했으니 이제 좀더 알아보도록 하겠습니다.​이번엔 A. Deep Neural Network Modeling for SRAM Cell Stability에 대해서 정리해보았습니다.​  본 논문에선 nominal design parameter를 통해 RSNM의 평균과 분산을 얻어내기 위해 DNN을 사용합니다.nominal design parameter에서 L, W 그리고 Vth를 고려할 때 design parameter의 total 수는 13개라고 합니다.​ 6T cell에서 각 mosfet의 L성분은 통일하고 width와 threshold voltage는 달리하려나 봅니다.참고로 각 성분은 열벡터를 순서대로 나열한 것이라고 볼 수 있습니다.​ 이 벡터 성분이 DNN model의 input 성분이라고 합니다. 아래 그림에서 확인할 수 있습니다.​ 그림 1. Basic DNN model in the proposed method.​위와 같은 벡터  μ들 중 하나인 μi에 대해, PVT variable에 따른 RSNM simulation이 수행됩니다.그리고 이를 통해 평균인 μRSNM과 분산인 σ2RSNM을 얻어냅니다.여기서 PVT variation들은 L, W, Vth, VWL, supply voltage, temperature가 포함된다고 합니다. (대체 nominal design parameter랑 PVT variation이 각각 뭔지 구체적인 차이가 뭔지 모르겠음.)​  시뮬레이션의 수를 줄이기 위해, 본 논문에서는 quasi-MC simulation(이하, QMC)을 사용했습니다. 이때 Sobol sequence를 기반으로 진행했다고 합니다.​QMC 시뮬레이션은 샘플과 샘플 사이의 거리를 고려해서, predetermined된 design parameter의 샘플을 사용한다고 합니다.​뭐.. 이런 방식을 채택하게 되어, 샘플이 가능한 균등하게 추출되어 시뮬레이션 runtime을 줄인다고 합니`다.​ 그림 2. The flow of the proposed (a) DNN modeling and (b) DNN-based variation-aware SRAM optimization 진짜 많은 양의  μi set을 기반으로 μRSNM과  σ2RSNM을 얻어낸 후,DNN은 [ μi, μRSNM,  σ2RSNM] set을 Learning한다고 합니다.​Learning하기 이전에, highest model accuracy하는 DNN 모델의 최적의 모델 구성이 결정되야 한다고 하는데,무슨소린지 잘 모르겠습니다. (내 생각엔 이게 Candidate DNNs라고 하는 것 같음.. 아 아닌가... )​하여튼... 그러한 모델의 구성에는 위 그림1에서 볼 수 있지만, Layer의 수, 각 Layer의 뉴런의 수, dropout, initial learning rate가 포함된다고 합니다.​  최적의 모델 찾는건 좋습니다. 그런데 그걸 찾는 것이 어렵다는 겁니다. (게다가 여러 다른 application에 optimality가 보장되는 것이 아니랍니다. 예를 들어 classification, regression, image processing, speech recognition 등...)​이런 문제를 완화하기 위해서, DNN Modeling 중에는 Data resampling ensemble을 하며DNNs의 sensitivity analysis 중에는 Model ensemble을 하는 것을 채택했다고 합니다.각각에 대해서는 이후부터 다뤄질 것 같습니다.​  DNN Modeling step(그림 1에서 a부분)에선 data resampling ensemble을 위해 bootstrap aggregation이 사용됐다고 합니다.​하나의 DNN structure가 있다고 할 때, 그 structure에서의 M DNN models(이하, M members라고 부르겠습니다.)은 서로 다른 training과 test data를 사용하여 훈련받는다고 합니다. 아.. 이게 무슨 말이지...​예를 들어, data가 아래와 같이 벡터로써 주어진다고 하겠습니다. ​이렇게 되면 DNN structure의 member에 대한 training과 test data는 아래와 같아집니다. sample set이 결국 겹치지 않는다는 것을 수식을 통해 보여주는 것 같습니다. 위 수식에서의 T는 [1,n]범위를 갖는 random number의 sets이라고 합니다. 그 number들은 replacement하게 sampled되어 있다고 합니다.​Sampling with replacement는 M members의 Dtrain sets에 대해 randomness를 증가시키는데 일조한다고 합니다.(이전에 추출했던 표본에 의존하지 않는 그런 방식이기 때문에 랜덤성이 감소하지 않는다.)​결론적으론 Bootstrap aggregation을 이용하는 M member은 single DNN model과 비교했을 때 더 정확하다고 합니다. 따라서, M members의 결과에 대한 평균은 structure의 최종 output으로 사용된다고 합니다.  하여튼... 다양한 DNN Structure들에 대해, DNN Modeling은 앞서 설명한 Bootstrap aggregation data resampling ensemble을 사용해서 Learning을 합니다.​다양한 DNN Structure들에 대한 DNN Model들은 이제 드디어 그림 2의 (b)에서 확인할 수 있는 DNN-based sensitivity analysis 단계에서 Model ensemble에 사용된다고 합니다.​즉, 서로 다른 structure를 갖는 DNN models(이전에 언급했던 Candidate DNNs)은 sensitivity analysis의 입력으로 구성됩니다.​  Sensitivity Analysis에 대한 더 자세한 내용은 다음 글에서 다루도록 하겠습니다. "
The Privacy-Convenience Trade-Off  ,https://blog.naver.com/barunict/222800941569,20220706,"Barun ICT Global News JUNE 2022​02The Privacy-Convenience Trade-Off ​​​Kostiulin MAKSIM Department of Economics (Ph.D Candidate), Yonsei University ​​Voice assistants like Siri and Alexa bring convenience and efficiency to our daily lives. Powered by AI and IoT, these assistants are capable of voice interactions, controlling other smart devices, making to-do lists and much more. However, this technology is also powered by personal data collection, which puts it at the center of privacy concerns [1]. ​When you activate a voice assistant, it allows your device to collect and store every word you say and analyze your preferences and lifestyle. And it is not just AI that processes your records. Major voice assistant companies like Google, Apple and Amazon, employ thousands of people to manually sort and categorize users’ voice data as a part of supervised learning [2]. According to Amazon, “employees are not given access to the identity of the person engaging in the Alexa voice request”, a claim that was criticized after a series of errors when Alexa sent a private conversation of a coworker to the user’s husband [3]. ​Alica, a voice assistant developed by Yandex, a Russian Internet giant, is another example of the possibilities of privacy breaches. Although Alica does not record voice data unless it is deliberately activated by a special word, Yandex acknowledged that sometimes it can be activated by similar words and then inadvertantly record private conversations [4]. The main reason behind errors like that is the fact that these products are produced quickly, and the main algorithms are primarily designed for speech recognition, and only then at security. It doesn’t matter if the voice assistant is activated by a phrase, is constantly active or is in the off state - a leak is possible. The received data can be used by a third party both for marketing purposes and for industrial or personal espionage. Since, in the activated state, the voice assistant saves all the information heard and can send it to the manufacturer for analysis, personal data becomes available to an unlimited circle of people. Therefore, when using voice assistants or smart speakers, it is recommended to limit the list of actions that they can perform, as well as to exclude the possibility of accidentally activating the voice assistant by setting a unique word to enable it.​Interestingly, a recent survey showed that the majority of users are aware of privacy-related dangers when using voice assistants, but choose to trade their privacy for convenience [5]. The main reason cited was that it became impossible to avoid public access to private data and resist the convenience promised by new services.​​Sources[1] Chalhoub, G., & Flechais, I. (2020). Alexa, Are You Spying on Me?: Exploring the Effect of User Experience on the Security and Privacy of Smart Speaker Users. HCI, https://doi.org/10.1007/978-3-030-50309- 3_21 [2] Cellan-Jones, R. (2019, April 11). Smart speaker recordings reviewed by humans. https://www.bbc.com/ news/technology-47893082 [3] Statt, N. (2019, April 10). Amazon’s Alexa isn’t just AI — thousands of humans are listening. The Verge. https://www.theverge.com/2019/4/10/18305378/amazon-alexa-ai-voice-assistant-annotation-listen-private-recordings [4] Denisov, O. (2019, August 5). «Яндекс» отреагировал на сообщение о прослушке «Алисы. [Yandex responded to comments about eavesdropping]. https://ura.news/news/1052394143 [5] Lau, J., Zimmerman, B., & Schaub, F. (2018). Alexa, are you listening? Privacy perceptions, concerns and privacy-seeking behaviors with smart speakers. Proceedings of the ACM on Human-Computer Interaction, 2(CSCW), 1-31.​​ ​ "
[인공지능(AI)] 내게 어울리는 BGM을 알려주는 AI 음악 서비스 ,https://blog.naver.com/with_nipa/222482238073,20210824,"​음악은 우리의 생활, 감정에 깊게 녹아들어 있습니다. 상황, 정서에 따라 음악을 선택해서 듣는 경우도 많습니다. 운동을 할 때는 신나는 음악을, 차분히 생각을 정리하고 싶을 때는 조용한 음악을 듣기도 합니다. 마치 드라마나 영화 속 장면마다 자연스럽게 깔리는 BGM처럼 말이죠. 이제 나에게 어울리는 음악을 일일이 찾지 않아도 되는 시대가 왔습니다. AI 기술을 활용해 개인 맞춤형 음악을 추천하는 음악 서비스들이 개발되고 있기 때문입니다.​ 그 음악을 틀어 줘, 나만의 DJ출처 : [Royal Caribbean] Seeker Spotlight : Royal Caribbean 공식계정 유튜브 채널​슬플 때와 기쁠 때 듣는 음악은 다릅니다. 드라이브를 할 때도 낮과 밤에 듣는 음악은 다릅니다. 음악은 유행을 따라가지만, 음악을 듣는 것은 개인의 취향과 선택에 달려 있습니다. 우리는 장소에, 감정, 시간에 따라 음악을 찾게 되는데요. 무수히 많은 곡 중 개인에게 맞는 음악을 직접 찾아 듣기란 쉬운 일이 아닙니다. 이런 니즈를 공략하기 위해 AI를 활용한 기술들이 속속 등장하고 있습니다. ​미국의 크루즈 기업 Royal Caribbean에서는 개인 맞춤형 BGM을 제공하는 음악 서비스 Sound Seeker를 개발했는데요. 사진을 이용해 사용자들에게 맞는 음악을 추천해줍니다. Sound Seeker 홈페이지에서 자신이 좋아하는 취향의 사진 3장을 업로드하면, 사진의 색깔과 배경, 사진에 등장한 사람의 얼굴 표정, 바디랭귀지에서 드러나는 감정 등을 분석하여 어울리는 음악을 추천해주죠.​그런가 하면 지난해 인공지능(AI) 개발 업체 인디제이는 모바일 앱 ‘inDJ(인디제이)’를 출시해 상황을 자동으로 분석해 맞춤형 음악 플레이리스트를 추천하는 시스템을 선보였습니다. 특허 받은 AI를 기반으로 사용자 프로파일과 상황, 감정을 분석해 3D 모델링 기법으로 음악을 선별해 제공하고 있습니다.(IT비즈뉴스 <AI 기술 기반 음악플랫폼 스타트업 '인디제이‘, 중기부 팁스 선정> 참고)​ 나보다 나를 더 잘 아는 음악 추천 서비스출처 : [스포티파이] 나보다 날 더 잘 아는 : The More, 스포티파이 공식계정 유튜브 채널​나에게 맞는 음악을 추천하는 서비스는 앞으로도 꾸준히 개발될 예정입니다. 지난 3월 카이스트 문화기술대학원 박주용·남주한·이원재 교수팀은 사용자의 상황에 맞는 영상과 음악을 찾아주는 기술을 선보였습니다. 검색창에 단어를 입력하면 어울리는 영상과 음악을 동시에 찾을 수 있는 서비스인데요. 자연어 기반의 심층 유사성 검색(Deep similarity search) 기능을 토대로 영상과 음악을 연계한 검색에 특화됐습니다. 특히 장르별, 무드별, 악기별, 상황별 음악 꼬리표를 세밀하게 나눠 붙여 사용자가 음악을 선별하는 데 도움을 주고 있습니다.(문화일보 <얼굴표정으로 내 감정 읽은 AI, 딱 맞는 여행지·음악 골라준다> 참고)​그런가 하면 세계 최대의 음악 감상 플랫폼으로 손꼽히는 스포티파이(Spotify)는 최근 음성 인식을 활용해 콘텐츠를 추천하는 기술에 대한 특허 인가를 받았습니다. 사용자의 음성을 통해 연령과 성별, 음조나 억양, 리듬을 AI가 분석해 감정 상태를 파악한 후 어울리는 음악을 골라주는 방식입니다. 이전에 많이 듣던 곡 등 사용자의 취향까지 반영해 그야말로 맞춤형 음악을 선별해주는 것입니다. 특허는 받았으나 프라이버시 침해와 데이터 보안 문제 등으로 현재 기술의 적용 유무는 불확실한 상태입니다. (미국 정보기술(IT) 전문매체 Engadget <Musicians ask Spotify to publicly abandon controversial speech recognition patent> 참고)​음악은 우리의 정서를 반영합니다. 슬픔을 위로해주고, 더 극적인 행복을 만들어주기도 하죠. AI 기술을 통해 우리가 품고 있는 이야기들이 음악과 함께 더욱 풍요로워지길 기대합니다.​​​​  ​ ​ "
[채용공고] 퀄컴코리아 · Deep Learning Researcher (Intern) ,https://blog.naver.com/inthisworkofficial/222887965130,20220930,"Company:Qualcomm Korea YH​Job Area:Interns Group, Interns Group > Interim Engineering Intern – SW​Qualcomm Overview:Qualcomm is a company of inventors that unlocked 5G ushering in an age of rapid acceleration in connectivity and new possibilities that will transform industries, create jobs, and enrich lives. But this is just the beginning. It takes inventive minds with diverse skills, backgrounds, and cultures to transform 5Gs potential into world-changing technologies and products. This is the Invention Age – and this is where you come in.​General Summary:We are making AI technology be around us at edge devices, attend and assist people and learn from interaction without many labels, together with other devices in a federated way. We are based in Seoul and work together with our other team members in San Diego, Amsterdam, and Beijing.We are looking for deep-learning researchers interested in developing new theories and algorithms in the following areas:• VoiceUI – speech recognition, speech synthesis, keyword spotting, speaker verification, emotion recognition• Unsupervised / semi-supervised / self-supervised learning• Federated learning• Domain generalization / adaptation• Multi-task learning / Continual learning• Few-shot learning• Representation learning• Anomaly detection• Multi-modal context-aware• Dynamic networksThe developed theories and algorithms can be applied to computer vision, audio, and speech tasks such as image classification, image generation, video classification, audio scene detection/classification, automatic speech recognition, and speaker identification/verification. We are also developing algorithms to operate various tasks simultaneously at the edge devices with limited resources.The research results can be published as conference papers.The technologies to develop can be deployed on mobile device applications and has a great worldwide impact on the on-device learning algorithms using Qualcomm chips.​Minimum Qualifications• Graduate school student• Machine learning knowledge and experiences• Recent deep learning research experiences• Algorithm implementation experiences using Python with deep learning platforms, e.g., PyTorch, TensorFlow.​https://inthiswork.com/archives/20734 퀄컴코리아 · Deep Learning Researcher (Intern)Company: Qualcomm Korea YH Job Area: Interns Group, Interns Group > Interim Engineering Intern - SW Qualcomm Overview: Qualcomm is a company of inventors that unlocked 5G ushering in aninthiswork.com ​ "
"imaginary, depict, intend, represent, sniff, scurry, hem and haw, metaphor, recognition, spiritual ",https://blog.naver.com/doteplanner/222509044437,20210917,"​ imaginary [imǽdʒənèri] 가상의영문뜻 : not based on fact; dubious동의어 : chimeric, conceptual, fanciful, fantastic, unreal예문 : child's imaginary friends​ depict [dipíkt] 묘사하다영문뜻 : show in, or as in, a picture동의어 : describe, image, picture, render, show예문 : This scene depicts country life.​ intend [inténd] 의도하다영문뜻 : have in mind as a purpose동의어 : aim, design, plan, purpose, target예문 : I intend no harm.​ represent [rèprizént] 나타내다, 의미하다, 상징하다영문뜻 : take the place of or be parallel or equivalent to; 동의어 : depict, show, portray, describe, picture예문 : I represent the silent majority. ​ sniff [sníf] 냄새를 맡다영문뜻 : perceive by inhaling through the nose동의어 : nose, scent, smell, snuff, whiff예문 : sniff the perfume​ scurry [sk ə́ ːri] 허둥지둥 달리다영문뜻 : to move about or proceed hurriedly동의어 : run, scamper, sprint, dash예문 : so terrified by the extraordinary ebbing of the sea that they scurried to higher ground​ hem and haw 말할 때 주저하며 내는 소리영문뜻 : utter `hems' and `haws'; indicated hesitation동의어 : beat around the bush, dance around, hum and haw예문 : He hemmed and hawed when asked to address the crowd.​ metaphor [métəfɔːr, -fər] 은유영문뜻 : a figure of speech in which an expression is used to refer to something that it does not literally denote in order to suggest a similarity동의어 : allegory, emblem, analogy예문 : In poetry the rose is often a metaphor for love.​ recognition [rèkəgní∫ən] 인정영문뜻 : the state or quality of being recognized or acknowledged동의어 : acknowledgment, credit, admit, acceptance예문 : the partners were delighted with the recognition of their work​ spiritual [spírit∫uəl] 정신(상)의, 정신적인영문뜻 : concerned with or affecting the spirit or soul; 동의어 : mental, ghostly, sacred, unworldly, religious예문 : spiritual fulfillment​   영단어공부 목차 보러가기  <목차> Who Moved My Cheese, 누가 내 치즈를 옮겼을까, 영단어공부안녕하세요 여러분! [누가 내 치즈를 옮겼을까?] 책 다들 읽어보셨나요? 어렸을 적 재밌게 읽었던 기억이 ...blog.naver.com ​ "
도담이 말하고 듣게하기 ,https://blog.naver.com/design-whale/222643222941,20220209,"​​​ 아이들에게 말을 해주면 알려주는 교구가 있다면 어떨까 ?요즘 아이들이 사용하는 스마트 교구들은 하나하나가 똑똑합니다.블럭을 끼우면 소리가 나오고 터치 패드를 사용해서 수학도 배운다고 하네요저희 디자인 웨일은 조금 더 똑똑한 교구를 만들어 보고 싶었습니다.​​ 과외선생님처럼 한자, 수학, 영어 이런 것들을 알려주는 만능교구를 만들 수 있을까?그래서 만들어 보기로 했습니다.​​대충 구상은 이렇습니다. 발화 : 아이는 도담이라는 교구에 말을 한다.STT 엔진 : 도담이는 자연어를 해석한다.AI Chat bot : 문장 분석을 한다.TTS 엔진 발화: 자연어로 아이에게 말한다.​그래서 기본적으로 사용해야할 프로그램이나 언어들을 공부해보기로 했습니다.사실 우리는 프로그램언어에 대해서 정말 무지하기 때문에 google과 github, 몇 가지 서적을 구매하여근본없는 개발을 하기로 시작했습니다.​​ Working flow발화 > STT 엔진 > 텍스트 > 문장분석1 > 도메인 판단 > 문장분석2 > 의도 판단 > 동작 > TTS 엔진 > 음성 피드백발화Annyang: 브라우저를 이용해서 발화가 가능한 JS 라이브러리PyAudio: PortAudio Python 바인딩 라이브러리Jasper: Alexa와 같은 Always on 보이스 앱을 만들 수 있는 플랫폼STT 엔진발화를 인식하는 방법에 따라 STT 엔진이 달라질 수 있음네이버 음성인식: 4개 언어(한국어, 영어, 일어, 중국어(간체))지원, 처리한도 : 3,600초/일MS Bing Speech Recognition API: 28개 언어 지원, 처리한도: 10,000 건/월, 20건/분google-speech js: Google Speech API를 이용하는 JS 라이브러리문장분석1scikit-learn 이용: SVN 이용 고려 중도메인 판단문장분석1의 결과를 이용한 도메인 판단 필요문장분석2scikit-learn 이용: 뭘 써야하지??의도 판단문장분석2의 결과를 이용해서 정확히 어떤 의도의 발화인지 판단 필요동작리모컨 사용 가능한 디바이스: IR신호 이용Wemo 디바이스: Wemo API 이용Hue Light: Hue API 이용일정: Google Calendar API 이용기타: Google or Wikipedia 검색TTS 엔진네이버 음성합성MS Bing Text To Speech API: 18개 언어 지원, 한국어 지원 안함, 처리한도: 5,000건/월, 20건/분 (총 60분 한도) 사용불가 (한국어 지원 안함)ResponsiveVoice.js: JS 라이브러리google-speech js: Google Speech API를 이용하는 JS 라이브러리음성 피드백TTS 엔진에 따라 결정 필요관련 동영상Raspberry Pi + Jasper 를 이용한 assistant 개발 ​ "
"음성인식, 화자식별, 화자분할… 알고보면 모두 다른 음성 AI ",https://blog.naver.com/skelterlabs/222354900825,20210517,"얼마 전, 애정하는 예능 중 하나인 <유퀴즈온더블럭>을 보며 21년차 베테랑 속기사, 유병임 님을 통해 속기사의 업무를 살펴볼 수 있었어요. 그는 속기사를 시작하게 된 계기를 말하며, 사람마다, 지방마다 쓰는 단어가 모두 다르기 때문에 1시간 짜리 분량에 대한 초안 작성에만 네다섯시간이 소모된다고 언급했죠. 또한, 가족간의 대화는 목소리가 모두 비슷하기 때문에 속기의 난이도가 높다고도 덧붙였어요. 속기를 할 때 개인이 쓰는 목소리나 언어의 특징에 따라, 또한 여러 명이 말하는 상황적 특성에 따라 난이도가 다르듯 음성인식 기술 또한 비슷한 맥락을 가지고 있습니다. 출처: tvN, <유퀴즈온더블락>​음성인식 VS 화자인식오늘은 ‘음성인식 AI'로 통칭되지만 각기 다른 특징과 목적을 가지고있는 다양한 요소 기술- 특히 화자인식(Speaker Recognition)을 설명드리려고 합니다. 잘 ‘듣고 이해’하기 위해서는 소음 환경을 제거(Noise Cancelling)한 후 사투리나 신조어 등을 이해하는 것도 필수적이지만, 여러 명이 이야기하는 경우 ‘누구의 목소리인지'를 구분하는 것 또한 중요한 과제 중 하나입니다. 오늘은 화자인식으로 통칭되는 기술 카테고리 안에 어떠한 요소 기술이 있는지, 그리고 각 기술에 따라 쓰임새가 어떻게 달라지는지를 하나씩 짚어볼게요. 등록된 목소리 중 제일 비슷한 너, 화자 식별(Speaker Identification) 등록된 여러개의 목소리를 비교하여 지금 말하는 화자가 누구인지를 알아내는 화자 인식(Speaker Recognition) 기술 중 하나입니다. 만약 가족 구성원 두명, '다은'과 '현우'의 목소리를 AI 스피커에 등록했다고 가정해볼게요. 이때 다은이 AI스피커에게 ‘블루투스 핸드폰 연결해줘'라고 명령한다면, AI 스피커는 현우가 아닌 다은의 핸드폰을 연결해야 합니다. 때문에 AI스피커는 등록된 두개의 목소리와 화자의 목소리를 비교하여 가장 유사도가 높은 목소리의 주인공을 추정하고, 다은의 핸드폰에 블루투스를 연결하는 명령을 수행합니다. 만일 다은의 친구가 놀러와 같은 명령어를 말한다면, 때로 AI 스피커는 화자를 '새로운 화자'가 아닌 등록된 두 개의 목소리 중 하나로 추정합니다. 이렇듯 등록된 화자 가운데에서 유사한 목소리 패턴과 음색 등을 인식하는 화자 식별 기술은 각종 IoT 등에서 개인화를 진행하기 위해 필수적이나, 등록되지 않은 목소리를 제대로 인식할 수 없는 한계점을 가지고 있어요. ​목소리 일치 여부에 대한 판단, 화자 검증(Speaker Verification) 때때로 인식이 잘 되지않는 지문, 마스크로 인해 오히려 불편함이 커진 얼굴 인식의 불편함을 크게 개선하는 차세대 보안 인식 기술이 바로 화자 검증(Speaker Verification)입니다. 화자 검증은 '다은'의 목소리를 등록할 경우, 입력되는 목소리와의 일치 여부를 Pass/Fail 방식으로 판별합니다. 화자 식별 기술이 들어간 출입문이 있을 때, 이는 제3의 인물이 나타나더라도 가장 유사도가 높은 목소리의 인물로 유추하지만, 화자 검증 기반 보안은 ‘시스템에 등록되지 않은 목소리'가 기등록된 목소리와 일치하는지를 확인하는 것이 차이점이죠. 정확도 높은 화자 검증은 개인 확인 절차를 간소화시킵니다. 가령 은행 고객 센터에 전화할 경우, 주소나 생년월일 등을 통해 본인 여부를 확인하게 되는데요, 화자 검증을 활용하면 이러한 번거로움 없이 곧바로 상담을 진행할 수 있어요.  화자 식별과 화자 검증을 한데 묶어 ‘화자 인식’ 기술로 불리는데요, 때로는 음성 인식 기술의 한 갈래로, 때로는 음성 인식 기술과 따로 구분되기도 합니다. 그러나, 그 목적만은 분명해요. 바로 ‘누가 말을 했는지를 정확히 알아보는 것'이죠. AI가 자동으로 통화 내용을 기록할 때, 혹은 여러 명이 토의하는 내용을 받아쓰기 해야할 때, 누가 어떤 발언을 했는지 알 수 없다면 기록의 의미가 퇴색될 수 있겠죠? 물론, 이를 위해서는 아래와 같은 기술 또한 함께해야합니다.​​특정 화자의 목소리를 분리시키는 것, 화자 분리(Speaker Seperation)친구들과 얘기를 나누거나 동료와 함께했던 팀 미팅을 떠올려보세요. 흔히들 ‘오디오가 물린다'라고 표현되는 말 겹침 현상, 종종 나타나지 않나요? 여럿이 함께 얘기를 나누기 때문에 필연적으로 대답을 하는 화자가 동시에 여러명이 생긴다거나, 혹은 말 끼어들기 등이 벌어지곤 합니다. 화자 분리는 여러 화자의 목소리가 겹칠 때 이를 따로 따로 떼어내어 인식할 수 있도록 분리해주는 기술을 칭합니다. 혹자는 이 기술을 보이스 필터(Voice Filter)라고도 부르는데요, 보이스 필터는 화자 분리와 달리 이미 등록된 목소리만을 남겨서 음성 인식의 정확도를 높이기 위한 도구로 활용됩니다. 출처: Analytics Vidhya Blog, <Improving Voice Separation By Incorporating End-To-End Speech Recognition>​목소리가 바뀌었다는 것에 대한 인식, 화자 분할(Speaker Diarization)이번엔 아주 잘 편집된 TV 프로그램을 떠올려 볼까요. ‘오디오가 물리는' 경우는 흔치않고, 오히려 여러 명의 화자가 빈틈없이 이어서 티키타카를 벌이는 경우가 많죠. 화자 분할은 말 그대로 ‘화자가 다르다는 것을 인식'하기 위한 기술입니다. 토론 프로그램의 진행자와 패널 목소리에 대한 구분, 전화 통화 상황에서 발신자와 수신자에 대한 구분 등을 화자 분할로 꼽을 수 있어요.  출처: Google AI Blog, <Accurate Online Speaker Diarization with Supervised Learning>​음성 인식 기술이라 한다면 흔히 ‘정확히 알아듣고 말을 이해하는 일'이라고 생각하죠. 맞습니다, 그렇지만 ‘정확히 알아듣기'위해 필요한 기술, ‘말을 이해하기' 위해 필요한 요소 기술은 정말 방대하답니다. 우리가 대화를 나누는 환경 또한 카페, 레스토랑, 사무실 등 다양하고 대화를 나누는 채널 역시 전화통화, 비디오 콜 등 여러가지죠. AI는 이러한 다양한 환경적 특성을 고려하여 음성을 인식하고, 혹은 화자의 발음이나 어조의 특성 등을 고려하여 감정을 추론하기도 하죠. ​최근에는 이러한 요소 기술을 활용하여 ‘자동 회의록 기록'을 실행하는 AI가 큰 관심을 얻고있어요. 여러 명이 난상 토론을 하는 환경에서 누가 발언을 하고 있는지, 그리고 각자의 의견이 무엇인지를 정확히 기록하기 위해서는 위에 나열된 화자 인식 기술이 필수적이랍니다. 정확한 화자 인식이 이루어질수록, AI 스피커 등 다양한 IoT 서비스는 개인에게 맞춤화된 서비스를 제공할 수 있어요. 가령 ‘음악 틀어줘'라는 명령어에 가족 구성원 개개인의 목소리를 구별하여, 선호하는 장르나 개인 플레이리스트를 재생하는 셈이죠.​스켈터랩스는 음성 인식 모델의 고도화와 더불어, 화자 인식을 위한 기반 기술을 개발 중에 있습니다. 조만간 선보일 화자 분리 및 분할, 그리고 자연어 처리 및 음성 합성과 결합한 다양한 솔루션을 기대해주세요:) ​ "
음성인식 보고서 ,https://blog.naver.com/kksshh0103/222856258570,20220823,"​음성인식(speech recognition) : 사람이 말하는 음성 언어를 컴퓨터가 해석해 그 내용을 문자 데이터로 전환하는 처리​음성인식 기술의 역사1.초기 음성인식 장치 벨 연구소는 1952년 단일음성으로 말하는 숫자를 인식하는 Audrey 시스템을 개발화자와 충분히 적응 시 정학도 97%~99%​ ​1956년 일본에서 일본어를 인식 가능한 음소타자기(phohetic Typewriter) 그리고 1961년 IMB의 16개의 영단어를 인식가능한 자사의 Shoebox가 공개됨​​  ​음성인식 기술은 미 국방부의 관심과 재정지원으로 1976년까지 음성이해 연구 프로그램이 진행됨카네기 멜론 대학교의 Hearsy와 harpy가 대표적 2.통계 모델(HMM)의 등장​1980년대 부터 HMM으로 알려진 통계방식의 등장으로 음성데이터에서 무제한에 가까운 어휘 추측 가능​음성인식은 기업과 전문산업의료 등을 위한 상용 애플리케이션에 적용되기 시작​가정에서 쥴리인형을 이용해 아이들이 자신의 음성에 대답하는 훈련이 가능한 형태로 적용 ​3.컴퓨터 성능의 발전컴퓨터 속도가 빨라지며 90년대 가정에서의 보급이 활성화되어 일반인들도 음성인식을 접할 수 있게됨​1990년 dragon은 최초의 상업용 음성인식 소프트웨어를 출시. 7년 후 발전된 소프트 웨어 보급 2000년대부터 10년가까이 기술 개발 정체​4.인터넷 등장과 음성데이터 빅테이터화구글에서는 음성검색 기능을 제공하며 클라우드 서버에 데이터를 쌓고데이터로 여러문제를 해결함.음성데이터를 활용해 추측의 정도가 비약적으로 상승​음성인식을 이용한 전등 만들기​1. 전등 제작에 필요한 재료-무선통신(블루투스)를 할 HC-06 블루투스 모듈 ​-수동으로 전등을 제어하는 DIP 스위치  -전등을 켜고 끌 수 있는 릴레이(Relay) -전등역할을 하는 RGB LED ​2.음성인식 소프트웨어 만들기-음성인식을 하면 아도이노 장치에 명령을 보내는 어플리케이션(소프트웨어) ​완성된작품 테스트 영상  ​전등의 동작방식 정리[논리적 알고리즘]-음성인식 버튼을 눌러 '켜기' 또는 '끄기' 말하기-음성인식 결과가 '켜기'였다면 전등이 켜진다-음성인식 결과라 '끄기' 였다면 전등이 꺼진다 ​[기술적 알고리즘(어플)]-음성인식 버튼이 눌러졌다면 음성인식 결과를 가져옴 -음성인식 결과 텍스트가 '켜기'였다면 블루투스로 신호'A' 전송, '끄기'였다면 신호 'B' 전송 [기술적 알고리즘(아도이노)]-블루투스 신호가 들어왔는지 확인 -시리얼 신호로 들어온 텍스트 데이터를 텍스트 전용 변수에 저장 -입력한 결과에 따라 디지털 핀을 키거나 끈다 ​ "
강남 보청기 이어짐이 알려주는 청력검사결과 해석방법 ,https://blog.naver.com/eargym/222767445194,20220610,"안녕하세요 강남 보청기 대표 이어짐 청각언어센터입니다.청력이 나빠진 것을 느끼고 보청기를 알아보다 보면 가장 많이 듣는 질문이 바로 '청력이 어느 정도 신가요? 청력검사해 본 적 있으신가요?'일 것 같습니다. 저희 강남, 강북 센터를 방문하는 분들께 해당 질문을 드리면, 대부분 '검사를 해본 적은 있는데.. 몇 % 정도이다 혹은 몇 % 남았다더라..'이렇게 대답하곤 하십니다. 이와 같은 분들을 위해 청력 검사 시 받는 검사 종류와 그 결과를 해석하는 방법에 대해 알기 쉽게 정리해 보겠습니다. 청력 검사 종류​1. 순음청력검사 (Pure Tone Audiometry)2. 어음청각검사 (Speech Audiometry)청력 검사는 듣는 능력을 측정하는 검사입니다. 얼마나 작은 소리를 들을 수 있는지, 들은 단어를 어떻게 인식하는지, 이명의 종류와 크기가 어느 정도인지, 주변에 소음이 있을 때 목표 문장을 어느 정도로 구분할 수 있는지, 불쾌한 소리의 크기가 어느 정도인지 등 검사 목적에 따라 다양한 검사 방법이 개발되어 있습니다. 그중 가장 일반적으로 시행하는 검사는 순음청력검사와 어음청각검사입니다.​순음청력검사(pure tone audiometry)란?방음실에 들어가 가장 먼저 시행하는 검사로, 헤드폰을 쓰고 '삐~' 소리가 나면 버튼을 누르는 검사를 순음청력검사라고 합니다. 다양한 청력검사 중 가장 기본이 되는 검사로 주파수별로 들을 수 있는 가장 작은 소리를 찾는 검사입니다.  강남 보청기 이어짐 청력평가실 좌) 기도 청력 검사, 우) 골도 청력 검사 2가지 방법으로 시행되는데, 자연스럽게 공기를 통해 전달된 소리를 측정하는 기도(air-conduction) 청력검사와 두개골 진동을 통해 전달된 소리를 측정하는 골도(bone-conduction) 청력검사를 받게 됩니다. 이 검사 결과를 바탕으로 현재 주파수별로 어느 정도 들을 수 있는지, 난청의 종류와 형태는 어떠한지를 파악할 수 있습니다. 청력 검사 결과 표기 기호오른쪽 귀왼쪽 귀차폐 전차폐 후차폐 전차폐 후기도 역치O△X□골도 역치<]>[ 순음청력검사 결과는 청력도에 기록됩니다. 청력도의 가로축은 주파수(Hz), 세로축은 소리의 강도(dB HL)를 나타내며, 오른쪽은 빨간색, 왼쪽은 파란색으로 표기됩니다. 소리의 강도를 표시하는 청력도의 세로축은 위쪽이 마이너스고 아래쪽으로 갈수록 단위가 커집니다. 쉽게 그래프가 아래로 내려갈수록 난청의 정도가 심하다고 해석할 수 있습니다.  위 청력도를 보자면, 빨간색(오른쪽) 그래프가 파란색(왼쪽) 그래프보다 아래쪽에 있으므로 오른쪽 청력이 더 나쁘다는 걸 한눈에 쉽게 알 수 있습니다.(왼쪽 청력 : 정상, 오른쪽 청력 : 6분법 평균 35dB HL 경도 난청)  난청 정도순음청력검사 평균정상25dB HL 이하경도26~40dB HL중도41~55dB HL중고도56~70dB HL고도71~90dB HL심도91dB HL 이상 ※역치별 난청정도는 위의 표를 참고하시면 됩니다. ​어음청각검사(speech audiometry)란?어음청각검사는 순음청력검사와 함께 가장 많이 시행하는 검사로 단음절, 이음절, 문장 등을 사용하여 진행됩니다. ⑴ 어음인지역치(speech recognition threshold level) 검사 : 이음절어를 통해 순음청력검사의 평균과 일치하는지 비교하여 신뢰도 확인⑵ 어음인지도(speech recognition score) 검사 : 단음절어(word recognition score:WRS)와 문장(sentence recognition score:SRS)을 이용해 일상생활에서의 의사소통 능력을 측정하고 보청기 선택과 청능재활의 평가와 계획 등에 관한 정보 제공 청력 검사 기기 AD629실제 말소리를 사용하여 하는 검사이므로 의사소통 능력을 평가하고 예측하기에 적절하며, 순음청각검사와 함께 보청기 착용 후 종합적인 청각재활 효과를 판단하기 위해서도 필요한 검사입니다. 단음절어와 문장 검사의 경우 백분율(%)로 결과를 나타내며, 결과값이 높을수록 어음 분별력이 높아 보청기 착용 후 만족도와 효과가 높으리라 예측할 수 있습니다.   위 두 가지 청력검사를 바탕으로 청력이 일정 수준 이하로 나빠지면, 청각 장애 신청을 할 수 있습니다.  장애 정도장애 기준중증두 귀의 청력손실이 각각 90 dB HL 이상인 사람두 귀의 청력손실이 각각 80 dB HL이상인 사람경증두 귀의 청력손실이 각각 70 dB HL 이상인 사람두 귀에 들리는 보통 말소리의 명료도가 50% 이하인 사람두 귀의 청력손실이 각각 60 dB HL 이상인 사람한 귀의 청력손실이 80 dB HL 이상, 다른 귀의 청력손실이 40 dB HL 이상인 사람 위 조건 중 하나라도 충족된다면 청각 장애 등급을 받을 수 있으며, 보청기를 구입할 때 총 131만 원의 정부 보조금을 받을 수 있습니다. 자세한 청각 장애 진단 절차와 보조금 신청 방법은 아래 링크를 참고하시면 됩니다. 보청기 보조금 신청 방법과 2022년 지원금 제품 안내안녕하세요. 이어짐 보청기 청각언어센터입니다. 보청기를 구입하려고 검색하다 보면 '보청기 지원금,...blog.naver.com ​지금까지 청력 검사 종류와 해석 방법에 대해 간단히 정리해 보았습니다.실제 본인이 의사소통에 불편을 느껴서 방문하는 분들을 상담하다 보면, 중도 혹은 중도 이상으로 난청이 많이 진행된 경우가 많습니다. 다른 신체 부위와 달리 난청은 통증을 수반하거나 눈에 띄는 불편이 없는 경우가 대부분이기에 무관심인 분들이 많아서 그러리라 사료됩니다. 나의 청력 건강을 지키고 유지하는 가장 좋은 방법은 현재 청력이 어느 정도인지 정확히 알고 있는 것입니다.꾸준한 청력 관리는 원활한 대인 관계 유지 및 자신감 상승에도 큰 도움이 됩니다.더 많은 분들이 청각을 소중히 여기며 정기적인 관리를 통해 활기찬 생활을 이어나갈 수 있도록! 청각과 정기 검사의 중요성 알리기에 이어짐도 열심히 노력하겠습니다.  이어짐 보청기 강남본점서울특별시 서초구 강남대로 381 두산베어스텔 3층 302-1호이어짐 보청기 강북직영점서울특별시 동대문구 왕산로 127 경동유니온빌딩 4층 ​ "
LEADTOOLS 강력한 음성 인식 지원! ,https://blog.naver.com/hyubwooinfotech/222829171297,20220726,"LEADTOOLS 음성 인식 지원!​프로그래머에게 혁신적이고 가치 있는 개발 도구를 제공하려는 지속적인 노력의 일환으로 LEADTOOLS V22에 음성 인식(Speech Recognition) 지원 기능을 포함하였으며 LEADTOOLS Forms에 해당 기술을 업데이트 하였습니다. ​  ​LEADTOOLS가 음성 인식을 지원합니다. !!!!!!  ​LEADTOOLS는 데이터를 서버에 보내지 않고도 기본적으로 음성을 인식하여 음성 언어를 텍스트로 변환합니다.  ​음성을 텍스트로 변환하는 기능을 CDLL과 .NET 애플리케이션에 통합  웹 어셈블리(Web Assembly)를 사용하여 클라이언트에서 JavaScript 음성 인식 지원  결과물을 모든 문서 형식(Document Formats)로 저장  음성 인식 기능을 LEADTOOLS Document Editor에 내장   ​​​강력한 음성 인식 및 문서 편집 기능 통합​ LEADTOOLS 음성 인식 기능LEADTOOLS V22에서는 웹 애플리케이션을 위한 강력한 Zero-footprint 편집 솔루션인 LEADTOOLS Document Editor를 출시하였습니다.​ 이제 음성 인식 기능을 통해 Editor에서 오디오를 텍스트로 변환하고, ​결과물을 편집하며, PDF, DOCX  및 TXT를 포함하는 다양한 형식으로 저장할 수 있습니다. ​서버 컴퓨팅도 필요하지 않습니다 ! ​​이 모든 기능은 로컬에서 브라우저로 수행되므로 네트워크 연결로 느려지는 문제가 웹 애플리케이션에서 발생하지 않습니다.  Edit SDK에서 음성인식이 어떻게 사용되는지 자세히 보려면 음성 인식에 대한 포스팅 자료를 확인 하십시오. ​​​  FORMS RECOGNITION 엔진 업데이트 ​LEADTOOLS Forms Developer Toolkit의 최신 업데이트는 다음 성능 개선과 최적화가 포함되어 있습니다 : ​Forms Recognition Engine의 Barcode Object Manager는 마스터 양식을 보다 빠르고 정확하게 판단하여 양식에서 찾은 바코드를 기반으로 채워진 양식을 사용할 수 있도록 기능 향상과 업데이트를 이루었습니다. ​Forms Processing Engine의 테이블 영역 추출 기능을 업데이트하여 테이블과 행 경계를 더욱 정확하게 감지하고  비 정형 테이블 양식을 처리할 수 있도록 개선하였습니다.     ​  ​LEADTOOLS V22 음성 지원 인식에 대한 정보가 필요하시거나 문의 사항이 있으시면 아래 이메일 또는 사이트로 직접 연락하여 주십시오.​​​​LEADTOOLS 국내 독점 총판 협우인포테크(주)​LEADTOOLS 담당자 이메일: sales1@hyubwoo.comLEADTOOLS 기술문의 이메일: support@hyubwoo.com ​​​​당사 사이트:  www.hyubwoo.net (현사이트)​​​​LEADTOOLS 제품 살펴보기​ 제품군 개요 - 협우인포테크(주)LEADTOOLS는 인식, 문서, 의료, 이미징 및 멀티미디어 기술을 데스크톱, 서버, 테블릿 및 모바일 솔루션에 통합하기 위한 포괄적인 툴킷 모음으로 강력한 이미징 기술들을 제공합니다. OCR/ICR Barcode PDF Form Processing Document Editor Document Viewer Document Converter Document Analyzer DICOM PACS Medical Viewer Video/Audio Image Viewer Image Processing Virtual Printer Office...hyubwoo.net ​​​​​​LEADTOOLS 제품 문의 및 견적 요청은 아래를 클릭해주십시오. ​ 견적문의 - 협우인포테크(주)소프트웨어 업계의 FIRST 시대를 선도하는 HIT 최상의 서비스와 최상의 노하우를 더합니다. Hyubwoo Info Tech 고객님의 의견. 궁금한 사항 등을 문의하시면 빠른 시간 내에 답변드립니다 견적 문의기술 문의 알림: 이 콘텐츠에는 JavaScript가 필요합니다. 알림: 이 콘텐츠에는 JavaScript가 필요합니다.hyubwoo.net  ​ "
[필수정보] 어음청각검사의 종류 및 특징 유소아 청력검사 방법 의사소통 능력 난청 ,https://blog.naver.com/graytiger116/222727512274,20220510,"<유니트론 꿀팁 : 어음청각검사의 종류>​기본 청력검사에는 두 종류가 있습니다. 하나는 순음청력검사(PTA)로 순음을 사용하여 주파수별 청력역치를 측정하는 검사입니다.  두번째는, 어음청각검사로 어음에 대한 민감성 및 인지도 등을 측정하는 검사입니다.​어음청각검사는 보청기 착용자의 청력손실 및 보청기로부터의 혜택 정도를 예측하는데 유용하게 사용할 수 있어, 꼭 실시해야 하는 기본적인 청각 검사 중 하나입니다.​ ​<유니트론 꿀팁 : 어음인지역치검사(SRT)>​먼저, 어음청각검사의 목적은 착용자의 어음 인지능력을 측정하여 순음청력검사로 평가할 수 없는 착용자의 일상생활 내 의사소통 능력을 파악하기 위함입니다.이를 위해 실시하는 수 있는 어음청각검사에는 어음인지역치검사, 단어인지도검사, 문장인지도검사, 어음탐지역치검사 총 4개의 검사가 있습니다.​1. 어음인지역치검사(SRT: Speech recognition threshold)​: SRT검사는 제시된 2음절어를 50%가량 인지할 수 있는 최소 소리 크기를 측정하는 검사합니다. 이 검사의 목적은 어음인지 시 필요한 민감성을 측정하여, 순음청력검사(PTA)의 신뢰도를 확인하고 단어 및 문장 인지도 검사의 기초 자료로 사용하기 위해 실시할 수 있습니다.이때, SRT검사 결과와 PTA검사 결과의 차이가 10dB 이내여야만 신뢰도 있는 검사로 판단할 수 있고, 만약 결과의 차이가 10dB 이상일땐 PTA검사를 다시 시행해야 합니다. ​<유니트론 꿀팁 : 단어인지도검사>​2. 단어인지도검사(WRS: Word Recognition Score)​: WRS검사란 착용자가 듣기 편안한 소리크기(MCL)에서 단음절어를 듣고 얼마나 정확히 인지하는지를 백분율로 점수화하는 검사입니다.SRT(어음인지역치)검사와의 차이점은 SRT검사에서는 2음절어를 50%인지할 수 있는 최소 소리 크기 즉 민감성을 측정하고, WRS는 착용자가 편안하게 듣는 소리 크기에서 단음절어를 들었을때 얼마나 잘 이해하는지 , 즉 정확도를 평가한다는 차이점이 있습니다.WRS검사를 통해 보청기 착용 후 일상생활의 의사소통능력의 개선 정도를 확인하고 이를 보청기 착용자 상담 시 적극적으로 활용하는 것이 중요합니다.​ ​<유니트론 꿀팁 : 문장인지도검사>​3. 문장인지도검사(SRS: Sentence Recognition Score)​: SRS(단어인지도)검사에서는 착용자가 가장 편하게 들을 수 있는 소리크기(쾌적레벨, MCL)에서 문장을 제시하고 문장 내 단어를 얼마나 정확히 인지하는지 백분율로 결과를 점수화합니다. WRS검사에서는 단음절어만을 제시하므로 일상생활 속 의사소통 능력을 완벽히 반영하기에는 한계가 있습니다. 따라서, SRS검사에서는 문맥 힌트가 자연스럽게 포함되어 있는 문장을 자극음으로 제시하여 청자의 일상생활 속 의사소통능력을 보다 더 반영하는 결과를 얻고자 실시합니다.​ ​<유니트론 꿀팁 : 어음탐지역치검사>​4. 어음탐지역치검사(Speech detection threshold, SDT)​SDT검사란 SRT(어음인지역치) 측정이 불가능한 경우, 대신 시행하여 어음의 유무를 탐지할 수 있는 최소 소리 크기를 측정하는 검사입니다.​SDT검사는 SRT검사와는 달리, 들은 단어를 따라 말할 필요 없이 어음의 유무만 표현하면 되므로, 단어를 듣고 따라 말하는 식으로 검사를 진행하기 때문에 외국인이나 유·소아 등에게 적합한 검사입니다.​SDT의 경우 다른 어음청력검사들과 달리 표준화된 특정 어표를 사용하기 보다 피검자에게 익숙한 단어 또는 무의미 발성 등을 들려주고 어음의 유무를 탐지하는 능력을 측정합니다.​​지금까지 어음청각검사의 종류와 특징에 대해 소개해 드렸습니다. 유익하셨다면 하트와 댓글 남겨주세요. 감사합니다. ​ "
"[BBC 영어] 6 Minute English, 영어단어 암기 방법은? 문장으로 - Can AI have a mind of its own? ",https://blog.naver.com/softca/222999480291,20230130,"​​​【금주의 주제】​Is artificial intelligence capable of consciousness? We’ll hear from an expert who thinks AI is not as intelligent as we sometimes think, and as usual, we’ll be learning some new vocabulary as well.​인공 지능이 자각할 수 있을까요? 우리는 [인공 지능이 [우리가 때때로 생각하는 것] 만큼 지능적이지는 않다고 생각하는] 전문가로부터 들을 것입니다. 그리고 마찬가지로, 늘 그렇듯이 우리는 몇몇 새로운 어휘를 배우고 있을 것입니다.​이상은 금주의 BBC Learning 6 Minute English, Can AI have a mind of its own?의 주요 내용입니다.​생활영어에서 영어 단어 암기는 문장으로 해야 합니다. 이번 주에 6 Minute English가 제시한 새로운 영어단어를 문장 속에서 암기해 보세요. 여기에 한 가지 더! 영어어근으로 영어단어를 암기하면 더 더욱 효과적입니다. 각 단어의 여근를 참고해서 문장 속에서 암기해 봅시다.​【금주의 영어단어】​금주의 단어; anthropomorphise, blindsided, chatbot, cognitive, get/be taken in (by) someon, wishful thinking​[1]: anthropomorphise[설명(E)]: treat an animal or object as if it were human[설명(K)]: 동물이나 사물을 마치 그것이 인간인 것처럼 대우하다​[관련문장]:[35] We anthropomorphise animals all the time, but we also anthropomorphise action figures, or dolls, or companies when we talk about companies having intentions and so on.우리는 줄곧 동물을 의인화합니다. 그런데 우리는 또한 (움직이는) 캐릭터 인형이나 또는 인형 또는 [우리가 의도를 갖고 있는 회사에 대해 이야기할 때에] 회사, 그리고 기타 등등 의인화합니다.☞[문법설명] 『all the time』은 부사구입니다. 명사(구)가 부사(구)로 쓰이는 명사의 부사적 용법입니다.​[50] To anthropomorphise an object means to treat it as if it were human, even though it's not.물체를 'anthropomorphise(의인화하다)'는 그것을 [마치 [[비록 그것이 아님에도 불구하고], 그것이 사람인 것처럼] 대하는 것을] 의미합니다.​​[32] In other words, we anthropomorphise computers - we treat them as if they were human.다시 말하면 우리는 컴퓨터를 의인화합니다 (즉,) 우리는 그들을 [마치 그들이 사람인 것처럼] 대합니다.​​[관련단어]: anthropomorphise [동사] [R; 5001+] [레마] anthropomorph < morph[E/K] ① (신/신령 따위)를 의인화하다, (물건/생물 따위)를 의인화[인격화]하다. ② (신이나 자연이) 의인화[인격화]되다.[구성단어] anthropomorph + ize[어원] [Greek] morphe form, beauty, outward appearance[어근] ⑴ anthropo; 사람, human ⑵ morph; 형태, form, shape ⑶ ise(ize); 동사 어미  [2]: blindsided[설명(E)]: unpleasantly surprised[설명(K)]: 기분나쁘게 놀라는​[관련문장]:[39] If we treat computers as if they could think, we might get blindsided, or unpleasantly surprised.만약 우리가 컴퓨터를 [마치 그것들이 생각할 수 있는 것처럼] 대한다면, 우리는 기습을 당하거나 또는 불쾌하게 놀라게 될 수도 있습니다.​​[37] And while we're busy seeing ourselves by assigning human traits to things that are not, we risk being blindsided.그리고 우리가 인간의 특성을 [(인간이) 아닌] 것들에게 배정하는 것으로 우리들 자신을 보느라고 바쁜 와중에, 우리는 기습을 당함으로 위태롭게 되었습니다.☞[문법설명] ⑴ 『we're busy seeing ~』은 「be busy V-ing」 구문으로 '~하느라 바쁘다'의 뜻입니다.⑵ 『assigning human traits to things』은 「동사 A to B」 구문으로 이 때의 to는 '주제, 대상' 등의 의미를 내포합니다. 「assign A to B」의 의미는 'A를 B에 배정하다'입니다.​[51] When you're blindsided, you're surprised in a negative way.당신이 'blindsided' 되면 당신은 나쁜 방법으로 놀라게 됩니다.​​[관련단어]: blindside [동사] [Rank; 0] [레마] blind[E/K] ① 상대방이 못 보는 쪽에서 공격하다 ② 기습하다[구성단어] blind + side[어원] [Old English] blendan to blind, deprive of sight; deceive  [3]: chatbot[설명(E)]: computer programme designed to have conversations with humans over the internet[설명(K)]: 인터넷 상에서 사람들과 대화하도록 설계된 컴퓨터 프로그램​[관련문장]:[47] Right, it's time to recap the vocabulary we've learned from this programme about AI, including chatbots - computer programmes designed to interact with humans over the internet.알았어요 챗봇을 포함하여 우리가 이 프로그램으로부터 배운 인공 지능에 관한 어휘를 살펴볼 시간입니다. (즉, 그것은) 인터넷 상에서 사람들과 교류하기 위해 설계된 컴퓨터 프로그램 (입니다).☞[문법설명] 『it's time to recap ~」 에서 it는 시간을 나타냅니다. 시간을 나타내는 주어 it는 문장에서 해석하지 않습니다.​[07] LaMDA is a chatbot - a computer programme designed to have conversations with humans over the internet.람다는 챗봇입니다 (즉, 그것은) 인터넷 상에서 인간과 함께 대화하기 위해 설계된 컴퓨터 프로그램 (입니다).☞[문법설명] ⑴ 『designed ~』는 과거분사(구)로 'a computer programme'을 후위수식합니다. 분사 앞에 '주격관계대명사와 be 동사'가 생략되어 있다고 생각할 수 있습니다.⑵ 『have conversations』는 have 디렉시컬 동사 구문입니다. '대화하다'라는 뜻입니다.​[08] After months talking with LaMDA on topics ranging from movies to the meaning of life, Blake came to a surprising conclusion: the chatbot was an intelligent person with wishes and rights that should be respected.영화에서부터 삶의 의미까지 포함하는 주제에 관하여 람다와 함꼐 대화를 한 여러 달 후에, Blake는 놀라운 결론에 이르게 되었습니다. 즉, 챗봇은 [존중되어야만 하는] 소망과 권리를 가진 지능을 가진 사람이었습니다.☞[문법설명] ⑴ 『from movies to the meaning of life』는 「from A to B」의 구문입니다. A는 movies이고, B는 'the meaning of life' 입니다.⑵ 『that should be ~』의 that은 관계대명사 입니다. 주격이고, 선행사는 'wishes and rights'입니다.⑶ surprising은 현재분사에서 동사적 기능이 소멸되어 형용사로 품사가 전환된 파생 형용사입니다.​[관련단어]: chatbot [명사] [Rank; 0] [레마] bot[E/K] 챗봇[첫뜻] ancient two-wheeled vehicle used in war, racing, and private life,[구성단어] chat + bot  [4]: cognitive[설명(E)]: connected with the mental processes of thinking, knowing, learning and understanding[설명(K)]: 생각하는 것, 아는 것, 배우는 것, 이해하는 것 등의 정신적 과정과 연계되어 있는​[관련문장]:[27] Using words like 'recognition' in relation to computers gives the idea that something cognitive is happening - something related to the mental processes of thinking, knowing, learning and understanding.컴퓨터에 관하여 'recognition'과 같은 단어를 사용하는 것은 [인지적인 무언가가 벌어지는 중이라는] 견해를 줍니다. (즉, recognition은) 생각하는 것, 아는 것, 배우는 것 그리고 이해하는 것이라는 정신적인 과정과 관련 있는 무언가 (입니다).☞[문법설명] ⑴ 『in relation to computers 』의 「in relation to」는 복합전치사구로 '~와 관련된'의 의미를 갖습니다.⑵ 『that something cognitive is ~』 절의 접속사 that은 동격의 that입니다. 'the idea'와 that 명사절은 동격관계에 있습니다.⑶ 『related to the mental ~』의 「related to」는 전치사구로 '~와 관련있는'의 뜻을 갖습니다.⑷ 「the mental processes」와 「thinking, knowing, ~」은 동격관계입니다. 이를 동격의 of라고 합니다. '~라는, ~와(과) 같은'으로 해석합니다.​[25] If you talk about 'automatic speech recognition', the term 'recognition' suggests that there's something cognitive going on, where I think a better term would be automatic transcription.만약 당신이 'automatic speech recognition(자동음성인식)'에 대해 이야기한다면, 'recognition'이라는 용어는 [지속하고 있는, 인지하는 무언가가 있다]는 것을 암시합니다. 그러나 거기서 나는 [보다 더 좋은 용어는 자동화된 기록일 것이라고] 생각합니다.☞[문법설명] ⑴ if종속절을 갖는 복문에 계속적 용법의 관계부사절이 연결된 구조의 문장입니다. 복문 주절의 주어는 he term 'recognition', 동사는 suggests, 목적어는 that 명사절입니다.⑵ 『something cognitive』의 something과 같이 thing으로 끝나는 대명사'를 수식하는 형용사는 뒤에서 수식합니다.⑶ 『something ~ going on』의 『going on』은 현재분사 구문으로 something을 후위수식합니다. 분사 앞에 「주격관계대명사와 be 동사」가 생략되어 있다고 생각할 수 있습니다.⑷ 『where I think ~』의 where는 계속적 용법의 관계부사로 「접속사+부사(and+there)」의 기능을 합니다. '그런데, 그 결과로' 등의 의미를 갖습니다.⑸ 'where I think' 뒤에 명사절을 이끄는 접속사 that이 생략되었습니다. 절의 문장성분은 목적어입니다.​[48] The adjective cognitive describes anything connected with the mental processes of knowing, learning and understanding.형용사인 'cognitive'는 아는 것, 배우는 것 그리고 이해하는 것 등의 정신적인 과정과 관련된 무엇을 서술합니다.☞[문법설명] 「the mental processes」와 「knowing, learning and understanding」은 동격관계입니다. 이를 동격의 of라고 합니다. '~라는, ~와(과) 같은' 등으로 해석합니다.​[관련단어]: cognitive [형용사] [Oxf5] [Rank; 3370] [★★] [레마] cognitive < cogn[E/K] 인식[인지]의[어원] [Latin] cognit [past participle stem of cognoscere] known[어근] ⑴ cognit(cogn); 알다, learn ⑵ ive; 형용사어미  [5]: get/be taken in (by) someon[설명(E)]: be deceived or tricked by someone[설명(K)]: 누군가에 의해 사기를 당하거나 속다​[관련문장]:[41] As a result, we get taken in - we're tricked or deceived into thinking we're dealing with a human, or with something intelligent.결과적으로 우리는 속임수에 넘어갑니다. 즉, 우리는 [우리가 인간이나 또는 지능을 가진 무언가를 다루는 중이라고] 생각하는 것으로 속거나 또는 기만을 당합니다.☞[문법설명] thinking 뒤에 명사절을 이끄는 접속사 that이 생략되었습니다. 절의 문장성분은 목적어입니다.​[관련단어]: take in [동사] [Oxf5] [6Rank; 49] [구동사][E/K] ① (몸속으로) ~을 섭취[흡수]하다 ② (옷을) 줄이다 ③ ~을 포함[포괄]하다 ④ (영화 등을) 보러 가다 ⑤ ~을 눈여겨보다 ⑥ (듣거나 읽는 것을) 이해하다[기억하다] ⑦ ~를 (자기 집에) 받아들이다[자기 집에서 지내게 하다] ⑧ (흔히 수동태로) ~를 속이다[현혹하다] ⑨ (영국) (신문 등을)구독하다 ⑩ 상품을 사입하다[어원] [Latin] cognit [past participle stem of cognoscere] known[어근] ⑴ cognit(cogn); 알다, learn ⑵ ive; 형용사어미  [6]: wishful thinking[설명(E)]: something which is unlikely to come true in the future[설명(K)]: 미래에 실현될 가능성이 거의 없는어떠한 것​[관련문장]:[26] That just describes the input-output relation, and not any theory or wishful thinking about what the computer is doing to be able to achieve that.그것은 단지 입출력 관계 만을 서술합니다. 그리고 [컴퓨터가 그것을 성취할 수 있도록 하고 있는 중인 것에 관한] 어떤 이론이나 희망 사항을 (서술하지) 않습니다.☞[문법설명] ⑴ 본문은 「A, not B」구문입니다. 이는 「B가 아니라 A이다」라는 뜻입니다. A는 'the input-output relation', B는 'any theory or wishful ~' 입니다.⑵ 『what the computer is doing ~』은 what 목적격 관계대명사절로 절의 문장 성분은 전치사 about의 목적어입니다.⑶ 『be able to achieve ~』의 「able to」는 be동사와 결합하여 can의 의미를 갖는 (법)조동사의 기능을 합니다.⑷ 『to achieve that』의 that은 「any theory or wishful thinking」을 지시합니다.​[29] Professor Benders says that talking about them in connection with computers is wishful thinking - something which is unlikely to happen.벤더스 교수는 [컴퓨터와 관련되어 그 들에 대해 이야기하는 것은 희망 사항이라고] 말합니다. (즉, wishful thinking은) 벌어질 개연성이 낮은 무언가 (입니다).☞[문법설명] ⑴ 『that talking about ~』의 접속사 that은 명사절을 이끕니다. 문장성분은 목적어입니다.⑵ 『in connection with computers』의 「in connection with」은 [전치사+명사+전치사]의 구조의 복합전치사구를 만들며 명사와 결합하여 부사구로 사용됩니다.⑶ 『is unlikely to happen』의 「unlikely」는 「be+형용사/분사+to」의 문형으로 동사원형(=부정사)과결합하여 동사의 (법)조동사 기능을 합니다. '~할 개연성이 없다/낮다'의 뜻을 갖습니다.⑷ 'unlikely to'⑸ 『which is unlikely ~』의 which는 주격의 관계대명사입니다. 선행사 something을 수식하는 제한적 용법입니다.⑹ 『wishful thinking』의 thinking은 동사의 기능이 완전히 사라진 명사입니다.​[49] wishful thinking means thinking that something which is very unlikely to happen might happen one day in the future.'wishful thinking(희망 사항)'은 [[매우 발생할 개연성이 낮은] 무언가가 미래에 언젠가 발생할 수도 있다고] 생각하는 것을 의미합니다.☞[문법설명] ⑴ 『that something which ~』은 접속사 that이 이끄는 명사절입니다. 절의 문장성분은 목적어입니다.⑵ 『which is very unlikely ~』의 which는 주격의 관계대명사입니다. 선행사 something을 수식하는 제한적 용법입니다.⑶ 『which is very unlikely to ~』의 「unlikely to」는 be동사와 결합하여 '~할 개연성이 낮다의 의미를 갖는 (법)조동사의 기능을 합니다.⑷ 『one day』는 부사구입니다. 명사(구)가 부사(구)로 쓰이는 명사의 부사적 용법입니다.​[관련단어]: wishful thinking [명사] [6Rank; 1024][E/K] 희망 사항[어원] [Latin] cognit [past participle stem of cognoscere] known[어근] ⑴ cognit(cogn); 알다, learn ⑵ ive; 형용사어미  ​pdf 파일은 이곳을 클릭하세요.https://blog.naver.com/softca/222998788018을 클릭하세요.PDF 파일은 서로이웃에게만 제공됩니다.​ 새내기할배 블로그 사이트에는 [6 Minute English] 자료와 관련하여 다음과 같은 글들이 매주 포스팅 됩니다단어공부(금주 대본에 나온 주요 어휘) https://blog.naver.com/softca/222998793603멀티단어공부(금주 대본에 사용된 멀티단어(구동사, 숙어) 간단 정리) https://blog.naver.com/softca/222998799992독해공부(금주 대본의 문장별 해석(직역) 자료) https://blog.naver.com/softca/222998889013문장별 문법공부(문장별 문법 설명) https://blog.naver.com/softca/222998894844금주의 어휘(금주 대본에 나온 금주의 어휘를 집중적으로 정리한 자료) 지금 읽고 계신 글입니다.항목별 문법공부(금주 대본에 나온 문법을 주제별/항목별로 정리한 자료) 현재 작성 중입니다.금주의 구동사(금주 대본에 사용된 구동사 상세 정리) 현재 작성 중입니다.금주의 PDF(금주의 PDF 및 기타 다운로드 파일) https://blog.naver.com/softca/222998788018 ​ ​(끝) ​ "
미국 어학연수 - EF 하와이 호놀룰루 캠퍼스 ,https://blog.naver.com/raffles7/222863251580,20220831,"​하와이(Hawaii)하면 무엇이 떠오르시나요?  바다와 아름다운 천혜의 자연이 떠오르시죠? ​연중 온난한 기후로 서핑, 스노쿨링, 카약 등 다양한 액티비티의 천국인  이 아름다운 세계적인 휴양지에서 영어를 공부하실 수 있는 EF 하와이 호놀룰루 캠퍼스에 대해 소개드리려고 합니다.  요즘 항공료가 많이 비싼데, 미국 서부나 동부에 비해 저렴한 항공료도 장점이 되겠습니다!​​​ ​​하와이의 수도인 호놀룰루에 있는 EF 호놀룰루 캠퍼스는 공항, 알라모아나 쇼핑센터, 와이키키 해변에 인접한 호놀룰루 중심가에 위치하고 있습니다.  하와이 대학과 하와이 퍼시픽 대학과도 가까운 거리에 있습니다. EF 호놀룰루 캠퍼스도 만 25세 이상 학생들을 위한 별도 과정이 제공되는 프로페셔널 캠퍼스입니다. ​ ​​​​​​지금부터는 EF의 수업 방식에 대해 말씀드리겠습니다. EF의 어학연수과정은 다른 어학원보다 좀 더 듣기와 말하기 능력 항상에 포커스가 맞추어져 있습니다.​​​ 위 영상에서 보신 EF의 수업 체계는 다음과 같이 5 가지의 수업으로 구성되어 있습니다.1. General Class : 저희도 익히 경험한 강남역 영어학원 같은 소규모 렉처 스타일 수업2. Conversation class : 다양한 디스커션 수업3. iPAD Class : 아이패드를 이용한 2-3명의 pair conversation 수업4. i Lab computer class: 유창한 영어를 위해서는 파트너와의 공부도 필요하나 학습자 스스로의 혼자 해야 할 공부들이 있죠? 바로 올바른 문법을 익히고 올바른 발음으로 자신의 생각을 정리하고 전달해야 합니다. 그를 위한 video lesson(온라인 문법 공부)과 speech recognition(발음 수정)을 컴퓨터를 통해 공부하고 itracker라는 툴을 통해 본인의 프로그레스를 확인할 수 있습니다.5. SPIN Class: Special Interest Class로 대학 교양처럼어학관련 수업은 물론 미국 문학, 비즈니스, 경제 및 법률, 필름메이킹 같은 흥미 있는 과목 수업을 영어로 참여하는 수업입니다.​​ ​주당 수업 시간에 따라 일반과정과 집중과정을 고르 실 수 있습니다.​ 집중 과정 시간표​​​​​ 일반 과정 시간표​​​EF는 영어수업 이외에도 다양한 액티비티가 있는 점이 특징이죠?폴리네시안 문화센터 체험, 오하우 써클 아일랜드 투어, 쥬라기 공원 촬영지 ATV투어, 마우이섬투어등등 다양한 액티비티 행사에 참여하실 수 있습니다.​ ​  액티비티에 참가중인 EF 하와이 호놀룰루 학생들​​​  액티비티에 참가중인 EF 하와이 호놀룰루 학생들​​​  액티비티에 참가중인 EF 하와이 호놀룰루 학생들​​​  액티비티에 참가중인 EF 하와이 호놀룰루 학생들 ​​​EF 하와이 호놀룰루캠퍼스에서는 단기 어학연수를 하시는 학생들은 아래의 와이키키 비스타를 숙소로 제공합니다. ​​ 숙소인 와이키키 비스타의 자세한 사진이 궁금하신 분은 아래의 사진을 클릭하시기 바랍니다.​ Waikiki Vista - New Fantastic EF Residence in Honolulu. * 700 meters from Campus * En-suite bathrooms * EF Kitchen and Lounge * Stunning Views * Modern * Lounge Areas * Cafeteria47 new items · Album by Carl Cronstedtphotos.app.goo.gl ​​EF 하와이 호놀룰루 캠퍼스의 어학연수 프로그램에 대해 더 궁금하신 사항은 저희 링크에이드로 연락 주시면 친절히 답변드리겠습니다. ​  링크에이드는 EF 어학연수 입학 수속을 무료로 도와드리고 있습니다.입학 허가를 받은 이후 출국 및 현지 정착까지 세심히 도와드리고 있습니다.관심 있으신 분들은 카카오톡 아이디 linkaid로 카톡 상담 가능합니다. 전화가 편하신 분들은 010-2473-0049로 전화 주시면 친절히 안내해 드리겠습니다.​ ​ "
Amazon Transcribe ,https://blog.naver.com/dudemr2201/222950953694,20221209,Amazon Transcribe​Automatically convert speech to text음성을 텍스트로 전환하는 기능​Uses a deep learning process called automatic speech recognition(ASR) to convert speech to text quickly and accurately자동 음성 인식(ASR)이라는 딥 러닝 프로세스를 사용하여 음성을 텍스트로 빠르고 정확하게 변환함​Use cases:-transcribe customer service calls고객 서비스 통화 기록​-automate closed captioning and subtitling자동으로 폐쇄 자막 생성​-generate metadata for media assets to create a fully searchable archive미디어 자산의 메타데이터를 만들어 완벽하게 검색 가능한 아카이브 생성 
음성 인식 사이트들... ,https://blog.naver.com/soyosa/222670077636,20220311,"#음성인식​Google 클라우드 음성 API#Google 클라우드 음성 API 는 구글 클라우드 기반 기능 중의 하나로 인간의 음성을 텍스트로 변환하는 기능을 제공합니다. 이 API는 110개 이상의 언어를 지원하며, 인식 가능한 단어의 리스트를 제공함으로써 사용자 지정(customization)을 지원합니다. 이는 인식 가능한 단어가 제한적인 기가나 환경에서 음성 인식을 사용하고자 할 때 특히 유용한 서비스입니다.이 API는 일괄 처리 모드와 실시간 모드로 모두 작동할 수 있습니다. 또한 오디오의 사이드 노이즈에 대해서도 안정적입니다. 일부 언어에서는 부적절한 단어에 대한 필터링도 가능합니다. 이 시스템은 심층 신경망(DNN: Deep Neural Network)을 이용하여 만들어졌으며 시간이 지남에 따라 개선될 수 있습니다. 처리하고자 하는 파일은 API에 직접 전달되거나 Google 클라우드 저장소에 저장될 수 있습니다.가격은 합리적입니다. 처리된 오디오 길이 기준으로 최대 60분 까지는 모든 사용자가 무료로 이용이 가능하며, 60분 이상 진행하려면 15초당 0.006 USD를 지분해야 합니다. 월 총 용량이 오디오 100만 분으로 제한되어 있는 점이 흥미롭습니다.​IBM Watson Speech to Text#IBM Watson Speech to Text 는 IBM Watson이 제공하는 서비스로, 사람의 음성을 텍스트로 변환합니다. IBM Watson은 단어 리스트 뿐만 아니라 특정 음량 조건에 대한 사용자 지정을 지원합니다. 사용환경에 따라 시스템을 적응시킬 수 있는 것이죠. IBM Watson Speech to Text의 결정적인 단점은 지원 언어가 매우 적다는 것입니다. 게다가 사용자 정의 모델은 그보다 더 적은 수의 언어만이 지원되죠. 마스터 버전에 병합될 때 영어, 스페인어, 일본어에 대해서는 서로 다른 발화자를 구분할 수 있게 됩니다.키워드 검색 기능은 사용자가 정의한 문자열을 음성에서 바로 감지할 수 있는 기능입니다. IBM Watson Speech to Text에서 사용할 수 있는 다른 유용한 기능으로는 단어 대체 기능(베타), 단어 신뢰도 제공, 단어 타임스탬프, 욕설 필터링, 전화번호·날짜·화폐를 인식하는 스마트 포맷(베타) 등이 있습니다. 또한 설명서를 통해 지원되는 오디오 파일 형식을 숙지할 수 있습니다.서비스에 대한 접근 권한에는 3가지 레벨이 있습니다. 표준 레벨은 매월 처리된 오디오 1천 분 분량의 무료 권한을 제공합니다. 이후에는 분당으로 유연하게 가격을 측정합니다. (Graduated Tiered Pricing) 사용자 정의 모델을 사용하려면 표준 가격에 추가로 0.03 USD를 지불해야 합니다. 프리미엄 레벨을 사용하기 위해서는 IBM에 문의하여 세부 사항에 대한 동의가 필요합니다.​IBM Watson Text to Speech앞서 설명한 speech-to-text 서비스와 유사하게 IBM Watson text-to-speech 서비스도 제공하고 있습니다. #IBM Watson Text to Speech 가 바로 그것입니다.이 시스템은 입력 텍스트로부터 고품질의 오디오 파일을 생성합니다. 일부 약어와 숫자를 인식할 수 있죠. 예를 들어 본문에서 “USD”라는 약자를 만나면 “United States Dollars”를 발음할 수 있다는 것입니다. API는 문장의 톤도 감지할 수 있습니다. 예를 들면 질문을 구분할 수 있는 것이죠. 생성되는 오디오 파일의 분위기 표현도 선택할 수 있습니다. (좋은 소식, 사과문, 무례함) 또한 어리거나 부드러운 목소리, 또는 성별을 지정할 수 있기도 합니다. 그러나 현재 이 기능은 영어에만 이용이 가능합니다. 워드 타이밍 기능을 통해 텍스트 스트리밍과 함께 제공되는 음성을 동기화할 수 있고, 다양한 형식의 오디오 파일을 만들 수 있습니다. 지원되는 형식에 대한 자세한 내용은 설명서를 참조하시길 바랍니다.가격은 사용 수준에 따라 달라집니다. 프리미엄 레벨을 원하는 경우 IBM에 문의하여 가격 및 사용법에 대한 세부 사항에 동의하세요. 표준 레벨 사용으로 충분한 경우 월 100만 자 분량의 텍스트까지는 무료로 제공됩니다. 더 많은 문자를 처리해야 한다면 1000자 당 0.02 USD를 지불하시면 됩니다. 표준 레벨에서 모든 언어와 음성을 사용하실 수 있습니다.​​Microsoft Azure Bing Speech API#Microsoft Azure Bing Speech API 는 마이크로소프트 사의 Azure 클라우드 서비스의 구성요소로서 음성-텍스트 변환과 텍스트-음성 변환 두 가지 작업을 동시에 해결할 수 있습니다. Azure Bing Speech API는 실시간 처리가 가능하고 사용자 정의를 지원하며 텍스트 형식 지정과 욕설 필터링, 텍스트 정규화가 가능합니다. 또한 상호작용이나 대화상황, 받아쓰기 등 다양한 시나리오(말하기 상황)을 지원합니다. Azure LUIS와의 통합이 가능하며, Azure LUIS는 본문에서 의도를 추출해 낼 수도 있습니다.텍스트-음성 변환 서비스에는 성별이나 볼륨, 피치, 발음, 말하기 속도, 운율 등 다양한 음성 매개변수를 조정할 수 있는 기능이 있으며 특정 단어를 감지하여 처리할 수 있습니다. 예를 들면 부동 소수점 수로 표시된 화폐 금액을 인식하여 “센트”가 있는 글로 변환할 수 있죠.한달의 최대 5000건이 무료로 제공되며 더 많은 사용을 원한다면 1000건마다 4 USD를 지불해야 합니다.​​Amazon TranscribeAmazon Transcribe은 아마존 웹서비스 인프라의 일부분으로 Amazon S3 서비스에 저장된 오디어 문서를 분석하거나 텍스트로 변환할 수 있습니다.아마존 번역은 구두점과 텍스트 형식을 추가할 수 있고 따로 전화음성을 지원합니다. 전화통화에서 나오는 오디오의 질이 낮은 경우가 많기 때문이죠. 아마존 번역 개발자들은 이런 종류의 오디오를 특정한 방법으로 처리해야 한다고 생각했고, 그 결과 본문의 각 단어에 대해 타임스탬프를 추가하도록 시스템을 개발하였습니다. 따라서 텍스트의 각 단어를 오디오 파일의 해당 위치에 일치시킬 수 있습니다. 단기간 내에 맞춤 단어를 추가하는 기능도 추가됩니다. 사용자는 제품의 이름이나 기타 다른 특정 단어를 추가할 수 있게 됩니다.등록 후 첫 12개월 동안 무료로 서비스를 이용할 수 있으며(매달 최대 60분 오디오), 이 기간이 지나면 오디오 초당 0.0004달러의 비용을 지불해야 합니다.​​Amazon PollyAmazon Polly 는 일괄 모드와 실시간 모드로 텍스트에서 음성으로의 변환을 지원하는 서비스로 이 또한 아마존 웹서비스 인프라의 일부분입니다.Amazon Polly는 성별이나 볼륨, 발음, 말하기 속도, 피치와 같은 일부 음성 매개변수를 조정할 수 있으며 가격은 처음 12개월 동안 무료 사용(매월 500만 자 이하) 후 Pay-As-You-Go 모델로 전환하여 100만 글자 당 4 USD를 지불하면 사용 가능합니다.​​VoxSigma API음성을 텍스트로 변환하는 VoxSigma API는 Vocapia Research 사에서 제공하는 제품입니다. 이 회사는 음성이나 언어 기술에 특화되어 있습니다. VoxSigma API는 입력 음성을 텍스트로 변환할 뿐만 아니라 언어 식별 및 음성 텍스트 정렬을 수행할 수 있습니다. API의 다른 흥미로운 특징은 출력 텍스트에 구두점 추가가 가능하고 출력에 대한 신뢰 점수를 계산해 준다는 점입니다. 또한 VoxSigma API는 숫자 및 일부 다른 요소(예: 화폐)를 고유한 방법으로 처리할 수 있습니다. 시용 가능한 언어 모델을 사용자 지정할 수는 있지만 이러한 목적을 위해서는 회사에 연락하여 그들과 직접 대화해야 합니다.회사는 몇 가지 사용 계획를 제안하는데 그 중 가장 인기 있는 방법은 종량제(pay-as-you-go)입니다. 이 계획에 따르면 분단 0.01 USD(또는 EUR)를 지불해야 합니다. 이 제품의 흥미로운 점 하나는 변환할 오디오에서 음성이 존재하는 부분만을 고려할 수 있다는 것입니다. 즉 입력 오디오에 일부 공백이 있으면 그 지속시간 만큼의 비용이 공제됩니다. 무료 체험이 가능하긴 하나, 이를 위해서는 회사에 직접 연락해야 합니다.​​Twilio Speech RecognitionTwilio Speech Recognition는 Twilio Flex 플랫폼의 구성 요소로서, 콜센터 서비스를 위한 풀 스택 프로그램입니다. 아마 이것이 독립형 애플리케이션이 아니라는 사실 때문인지 Twilio 음성 인식은 일부 다른 음성 인식 API만큼 많은 기능을 지원하지는 않습니다. 그러나 실시간 모드와 욕설 필터링이 제공되어, 첫 글자 외에는 *표시로 나타내줍니다. 이는 콜센트에서 사용할 때 매우 유용할 수 있죠. 이 서비스도 pay-as-you-go 방식으로 가격이 책정되며 오디오 15초당 0.02 USD를 지불하여야 합니다.Speechmatics ASRSpeechmatics ASR는 일괄 모드와 실시간 모드를 모두 제공하며 음성을 텍스트로 변환 가능한 서비스 집합입니다. 전 세계의 다양한 영어 방언들은 인식할 수 있을만큼 영어에 전문화되어 있습니다. 그렇지만 많은 다른 언어도 지원됩니다. Speechmatics ASR에서 사용할 수 있는 다른 유용한 특징으로는 문장 경계에 대한 정보 제공, 문장의 각 단어에 대한 신뢰도 점수 제공, 타이밍 정보제공 등이 있습니다. 클라우드 서비스 이용 가격은 1 분당 0.06 GBP이며 1000 GBP 이상 구매할 경우 할인을 받아 분당 0.05 GBP를 지불하면 됩니다.​​Nexmo Voice APINexmo Voice API 는 독립적으로 실행되는 API가 아닙다. 전화통화 시에 사용이 가능하죠. Nexmo는 프로그램화 가능한 통신을 위한 서비스를 제공하는 업체입니다. 제공 기능은 많지 않습니다. 음성의 성별이나 억양을 바꿀 수 있는 기능이 있습니다. 가격은 통화를 원하는 국가와 휴대전화인지 유선전화인지에 따라서 달라지며 분 단위로 책정됩니다.이제 여러분의 필요에 맞는 제품을 택할 수 있도록 각 API의 주요 기능을 비교해 보도록 하겠습니다.​​참조https://medium.com/@aimap.marker/%EC%83%81%EC%9C%84-10%EA%B0%80%EC%A7%80-%EC%9D%8C%EC%84%B1-%EC%B2%98%EB%A6%AC-api%EC%9D%98-%EB%B9%84%EA%B5%90-7a7ee778d4a3 상위 10가지 음성 처리 API의 비교언어 처리는 머신러닝에서 매우 인기 있는 영역입니다. 인간의 발화를 텍스트로 변환하거나 텍스트를 말로 변환하는 일에 상당한 수요가 있지요. 매장이나 공항, 호텔과 같은 다양한 장소에서 셀프 서비스가 발전함에 따라 이 기술은 특히 중요해졌습니다…medium.com ​ "
"AI 클라우드, 개발자가 애정하는 무료AI 서비스 지능별 TOP3 ",https://blog.naver.com/saltluxmarketing/222449923187,20210729,"개발에 엄청난 시간과 비용이 투입되는 인공지능 기술이걸 무료로 쓸 수 있다구요?   필요한 인공지능 기술들을 골라서 마음껏 사용할 수 있는 솔트룩스 AI 클라우드각 지능별로 개발자들이 애정한 TOP3 기술들을 소개합니다. 오늘은 첫 시간으로 분석지능, 언어지능, 음성지능을 알아보겠습니다~! ​​  분석 지능​1. 인용구 분석 : 기사내의 인용구를 탐색하고 형태소 분석 결과와 개체명사전을 활용하여, 인용구의 정보원을 특정합니다. 서비스 특징- 개체명사전을 활용한 정보원의 패턴 분석 : 뉴스에 등장하는 정보원은 이름 외에 소속된 기관이나 기관내의 직급 등 다양한 정보를 포함하고 있습니다. (ex. 솔트룩스 대표 이경일, 문재인 대통령 등) 인용구가 등장 했을 때, 주변 문맥에서 정보원이 될 수 있는 다양한 형태를 규칙기반으로 판단합니다. 규칙기반으로 판단한 정보원의 후보들을 언어 분석 결과를 토대로 판단하여 인용구의 정보원을 특정합니다.- 언어 분석 결과를 사용한 정보원 추출: 정보원의 후보가 여럿일 경우 조사와 같은 문장 분석 결과를 활용한 규칙기반 스코어링을 통해 판단합니다.- 정보원 추론: 반복된 인용으로 축약/생략된 정보원은 이전 인용구의 정보원 중에서 규칙기반으로 추론합니다.적용분야- 시장 조사/신제품 발굴 : 검색 키워드와 연관된 주제어를 찾아 제품 개발 또는 마케팅 기반 자료로 활용​ 적용화면 예시​ 2. 키워드 추출 : 입력된 문서(다수의 문장)를 분석하여, 해당 문서에 중요하게 작용하는 키워드를 추출하고, 키워드 리스트와 그 가중치(문서에서의 중요도)를 계산하여 반환합니다. 적용 분야 : 정보의 홍수 속에서 Insight만을 골라내는 키워드 추출 기능- 증권 현황 분석 : 증권사의 증권 리포트 및 정치/경제/사회 뉴스에 따른 금융 시장 예측 및 주가 흐름 예측- 경쟁 상품 분석 : R&D 등 신규 기술 분야나 경쟁사 상품 출시 시, 새로운 정보를 바로바로 알려주는 기능- 국민 여론 수렴 : 공공 기관 등은 다양한 블로그, 댓글, 신문고 등에서 국민의 목소리 등과 같은 국민 여론 수렴- 기술 연구 트렌드 분석 : 신규 논문이나 저서를 읽지 않고, 핵심 키워드를 추출, 연구분야의 Key Trend를 알 수 있는 기능- 마케팅/재고 관리 : 고객이 다수 입력한 키워드를 추출하여, 마케팅 및 재고 관리 전략 수립 (Sales 집중 품목 선정 및 고객 선호 품목 예상)다양한 비정형 데이터 분석의 후보 키워드 생성VOC 분석을 위한 후보 키워드 생성​ 3. 문서 필터 : 주어진 URL의 문서파일(HWP, PDF, DOC, XLS, PPT)에서 텍스트를 추출하는 기능을 제공합니다. 추출된 텍스트는 언어 지능을 통해 명사 추출, 개체명 추출등의 내용을 분석하는데 활용할 수 있습니다. 서비스 적용 분야 : 문서 보안 및 정보형 자료 추출에 적합한 기술- 문서 보안 관리 : 사내문서가 외부로 유출되지 않도록 Email 첨부 파일을 필터링하여, 한번에 분석 가능- 지능형 지식관리시스템 : 전자문서관리시스템으로, 문서필터를 사용하여 각종 분야(공공, 유통, 금융, 통신 등)별 개인의 업무와 특성에 맞도록 정보를 골라 우선 순위별로 제공- 마케팅 전략 : E-Commerce의 지능형 검색(AI Search, Intelligence Search, Semantic Search) 등으로, Sales Marketing 전략 수립적용 화면 예시​ 언어지능1. 구문 분석 (한국어 자연어 처리) : 한국어 비정형 텍스트에 대한 형태소 분석, 구문 분석, 개체명 추출과 같은 자연어처리 기능을 제공합니다. 서비스 적용 분야 : 검색 엔진 및 비정형 데이터 문서의 전처리에 활용. - 기업 및 공공분야의 검색 엔진으로 사용이 가능하며, 비정형문서의 데이터 전처리에도 활용되어, 분석의 방향성을 제시합니다.- 금융/제조/유통/의료 등의 기업 및 공공분야의 대규모 데이터 분석이 필요할 때, 자연어를 기계가 이해할 수 있는 언어로 번역하는 기술로 이용, 분석의 방향성 제시- R&D : 방대한 산업 관련 논문 분석을 통한, 새로운 연구 분야 도출이 가능합니다.- 시장 분석 및 조사 : 주요 경쟁 브랜드의 동향 및 issue 발견 등에 사용 가능합니다.- TM의 업무 효율화와 매출 증대 : 고객 유형별 대처방안 차별화를 통한 매출 증대 방안 모색, TM의 불완전 판매원인 및 개선 방안 탐색, 상담원의 시너지 기능 강화를 위한 관리 시스템 탐색 등에 활용 가능합니다.- 기타 : 형태소 분석은 언어 분석의 공통 기술로서, 그 외에도 많은 곳에서 활용되고 있습니다.자연어이해 기술 구조도​ 2. 한국어 문장 임베딩 : 솔트룩스가 보유하고 있는 방대한 코퍼스와 언어 분석기를 활용하여 자연어 문장 임베딩 모델을 학습하였으며, 이러한 기 학습 모델은 빠르고 정확한 문장 임베딩 결과를 도출 할 수 있습니다. 기 학습 모델을 활용하여 다양한 자연어 처리 어플리케이션에 적용할 수 있습니다 서비스 적용 분야 : 문장과 문서 단위의 분석 성능을 강화로, 문서 관리 및 Q&A, 챗봇 등에 적용- 문서관리시스템 : 문서의 자동 분류 및 유사 문서 분류 등에 사용- 기타 : 문장 기반 또는 문서 단위의 자연어 처리를 위해 폭넓게 사용 가능한 기술- TM의 업무 효율화와 매출 증대 : 고객 유형별 대처방안 차별화를 통한 매출 증대 방안 모색, TM의 불완전 판매원인 및 개선 방안 탐색, 상담원의 시너지 기능 강화를 위한 관리 시스템 탐색 등에 활용 가능서비스 개념도​ 3. 영어 한국어 읽기 : 한글로 표기되지 않은 단어들을 사전 등을 이용하여 규칙 기반 방식을 활용하여 한글로 변환하고, 규칙 기반 방식으로 변환하지 못 한 영어 단어는 Seqeunce to Sequence 방식으로 학습 된 모델을 활용하여 변환합니다. 서비스 적용 분야 : 영어/한글 양방향 발음 그대로 읽어주는 교육적 기능- Education: 영어, 한글 등 언어 발음(Phonics) 교육 및 교정 등에 활용Compositional n-gram features음성지능​ 1. 한국어 음성 인식(16K) : 음성 인식(Speech Recognition)이란 사람이 말하는 음성 언어를 컴퓨터가 해석하여 그 내용을 문자 데이터로 전환하는 처리를 말하며 STT(Speech-To-Text)라고도 합니다. 16kh sampling rate의 값을 가진 인간의 오디오 데이터를 인식하여, 텍스트로 변환합니다. 서비스 적용 분야 : 다양한 산업 분야에서 활용되는 음성 인식 기술- AI Chart : 녹취된 환자 상담기록를 자동으로 Text로 전환하여 차트를 만들어줌으로써 시간 소모적인 비환자 케어 활동 (non-patient care activity) 감소- AI 휠체어 : 음성인식을 통해 몸이 불편하거나 거동이 불편한 어르신들을 위한 휠체어를 제어함으로써 삶의 편의성 제공-캠페인/광고 사례 : '마시는 광고' 캠페인인 콜라 고유의 탄산 시즐 사운드를 음성인식기술을 통해, Mobile App에 콜라를 담게 하여 공짜로 콜라를 생생하게 즐길 수 있게 하여 마케팅에 활용한 사례- 스마트 드론 : 음성으로, 드론의 이륙, 비행, 착륙 등을 조정하여 스마트 폴리스로서 우범 지역의 치안 강화를 도와주는 사례- 스마트 홈 : TV, 조명, 난방, 각종 가전 등 IoT Device와 엘리베이터 호출, 공지사항 조회 등 공용부 기능까지 음성인식기술로 제어 가능- 대화하는 스마트 토이 : 토이에 음성인식기술과 글자판독기술 등을 추가하여 유아기 언어발달에 도움을 줄 수 있는 기술- Speaking 교정 : 음성인식과 자연어처리기술을 활용하여, 영어발음교정 및 Speaking 능력 향상 활용 사례- AI 자동번역 : 실시간으로 화자의 음성을 인식하여 다국어 자동 번역을 지원함으로써, 관광 혹은 출장 중에 간편히 의사소통 지원을 가능케 하는 기술- AI와 초보 상담원의 Collaboration : 막 입사한 초보 상담원도 고객의 VoC를 텍스트로 변환해주는 기술인 음성인식과 분석지능(키워드 추출, 연관주제검색)을 통해 회사 상품/ AS정책을 바로 화면에서 제공받아, 전문 상담원처럼 응대가 가능한 기술- 대부업 불법추심 판별 지원 : 대부업자의 채권추심 실태 점검 시 수집한 녹취파일에 한국어음성인식 기술로 언어폭력, 반복 추심('가족에게 알리겠다' '집이나 회사로 찾아가겠다' 등)을 포함한 대부업 불법추심 여부를 식별- 보험 TM 불완전판매 식별 : 보험영업 검사 시 텔레마케팅™ 녹취파일을 분석해 보험계약자에게 필수적으로 고지하는 항목의 허위 안내 등을 판별함으로써 불완전판매 여부 식별- 보이스피싱 방지 AI앱-'Reg Tech' : 음성기술을 활용하여, 기존 보이스피싱 사례를 토대로 통화내용을 분석하여 보이스피싱 의심시 경고 음성 및 진동 송출을 통해 사용자에게 알림서비스 제공 → 저비용(비용 절감)으로 규제 준수에 대한 신뢰도를 높이고, 규제 변화에 유연하고 능동적으로 대처가 가능한 기술※ RegTech(Regulation + Technology) : 정보통신기술을 활용해 법규 준수, 준법 감시, 내부통제 등의 규제 준수업무를 효율화하는 기술​2. 한국어 음성 전사 : 입력 오디오를 발화 단위 세그먼트로 분할하고 각 세그먼트에 대한 MBR(Minimum Bayes Risk)값을 제공함으로써 음성인식이 어려울 것으로 예상되는 세그먼트 데이터를 선별할 수 있습니다. 서비스 특징 : 기 구축 음성인식기의 성능 업데이트에 활용- 현존하는 최고의 한국어 음성인식기 중 하나인 솔트룩스(아틀라스랩스)의 음성인식 엔진을 기준으로 계산된 MBR 값을 활용함으로써 기 구축된 사용자 음성인식기의 성능 향상에 반드시 필요한 학습 데이터를 구축할 수 있으며, 특정 도메인 영역에서 학습 데이터 구축 비용을 절감할 수도 있습니다.- 기 구축 음성인식기의 성능 평가 데이터 세트 구축에 활용3. 한국어 음성 인식(8K) : 8kHz sampling rate의 값을 가진 인간의 오디오 데이터를 인식하여, 텍스트로 변환합니다.​음성 인식 기술은 모델 학습(Training) 단계, 학습된 모델의 통합 단계(FST Compile), 음성 인식(Recognition) 단계로 구성되며, 이중 음향 및 언어 모델을 학습하는 단계가 매우 중요합니다.​ 음성 대화 인터페이스 기반 서비스 구성음성 인식 엔진은 RESTful 기반으로 입력되는 음성데이터에 대한 문장을 출력할 수 있도록 제공합니다. ​잘 보셨나요?솔트룩스 AI 클라우드에는 40가지가 넘는 AI를 무료로 사용하실 수 있고,필요에 따라 맞춤형 서비스도 제공해 드리고 있어요. ​이번에는 분석, 언어, 음성 세가지 지능을 알아보았는데요. 다음 시간에는 시각, 감성, QA/대화지능 TOP3를 알아보겠습니다. 무료 AI 서비스에 대한 보다 자세한 내용은 아래 AI 클라우드 홈페이지를 참고해주세요~ ​감사합니다~!​ AI 클라우드 플랫폼 :: PortalAI 클라우드 플랫폼은 다양한 산업영역에 적용 가능한 인공지능 서비스를 제공합니다.www.saltlux.ai ​ "
용인 보청기 :  어음청력검사 종류와 해석방법 ,https://blog.naver.com/lienot/222583488235,20211201,"안녕하세요. 용인 나도보청기 나원도입니다.​지난 이야기에서 청력도의 기호와 해석방법에 대해서 알아보았습니다https://blog.naver.com/lienot/222582315211 나의 난청이야기 (중이염의 청력도, 청력도 보는방법)앞선 이야기에서 https://blog.naver.com/lienot/222581355458 중이염에 대해 알아봤는데 오늘은 청력도 보...blog.naver.com 오늘은 어음청력검사에 대해서 말하고자 합니다!​어음청력검사란 오른쪽 빨간색 왼쪽 파란색으로 표시된 헤드폰을 쓰고 검사를 진행하는 것 까지 위에 링크에서 말한 기도청력검사 셋팅과 비슷합니다. 기도청력검사가 '뿌'~ '삐~' 같은 순음으로 소리를 제시했다면 당연히 '어음'으로 소리를 제시해 검사합니다!​#어음청력검사 는 이사람이 얼마나 작은 소리의 단어를 들을 수 있을까? 하는 어음청취역치검사(SRT) 이사람이 소리에게 잘 들리게 끔 소리를 키워주면 몇%를 들을 수 있을까? 하는 단어인지도검사(WRS)가 있습니다어음청력검사는 이 두가지를 주로 사용합니다  그럼 하나 하나씩 알아보도록 하겠습니다​1.#어음청취역치검사(SRT, speech recognition threshold)의 목적!  - 이건 얼마나 작은 소리의 단어를 들을 수 있을까? 의 검사입니다.​   ① 어음청취역치검사의 목적으로는 PTA(pure-tone average, 순음청력검사평균치값) 와 신뢰도의 보기 위해 사용합니다. 순음청력검사의 경우 피검자가 소리를 들었다! 못들었다에 반응에 의해서 결정되는 주관적 검사입니다. 이 검사의 신뢰도를 판별하기 위해서 확인하는 목적으로 사용합니다. ② WRS (Word recognition score, #단어인지도검사) 의 기초를 제공합니다​​2. 검사방법 그 사람이 들을 수 있는 평균값의 일반적으로 +40dB 를 주고 시작하는 경우가 제일 많습니다. 그래서 검사를 하게 되면 제 목소리 잘 들리세요? 라는 검사자의 확인을 통해 검사가 시작됩니다.검사는 2글자의 단어를 따라 말하게 됩니다.  병원마다 조금씩은 다르지만 가장 흔하게 사용되는 단어는 '한국표준 이음절어표' 라는 것입니다/ 저작권상 아래는 가립니다!​ 학교에서 저에게 수업을 해주시던 교수님들이 만들어서 처음에는 신기했던 표입니다 ㅎ_ㅎ 일반용/ 학령기용 / 학령전기용이 나뉘어져 있는 이유는 해당 나이대에 단어를 아는가 모르는가에 따라서 결과값이 달라져서 수준이 나뉘어져있다고 생각하면 될것 같습니다. 일반적인 단어표 같지만 '강강격의 2음절어' 로 자음/모음 음소의 주파수 특성을 보면서 채택된 결과물입니다.​​​이것도 순음청력검사와 같게 '얼마나 작은 소리를 들을 수 있어요?' 확인 하는 작업으로 소리가 점점 작아지면서 해당크기에서  '3개중 2개, 혹은 4개중 2개'(병원마다 조금씩 다르다)를 들을 수 있는  가장 작은 소리 단위를 찾는검사입니다. 검사한 평균 PTA(pure-tone average, 순음청력검사평균치값) 과  위에 설명한 SRT 검사값이  10dB 이내면 신뢰도 높은 검사라고 합니다  그럼 두번째  WRS (Word recognition score, 단어인지도검사)는 무엇일까요?​단어인지도 검사의 목적은  1. 의사소통의 장애정도 2. 보청기 적응 및 선택 (보청기를 했을때 이정도를 들을 수 있겠다 하는 예측값) 3. 언어치료 계획에 필요한 정보 제공입니다. 위에 SRT(어음청취역치검사)에서는 단어로 소리가 작아졌다면 WRS(단어인지도검사) 에서는 소리를 줄이지 않고  그사람이 가장 잘 들을 수 있는 크기에 맞춰서 검사를 합니다. 그래서 보청기의 적응 및 선택에 도움을 준다는것입니다.(이 사람이 가장 잘 들을 수 있는 크기로 증폭을 하고 검사를 하기 때문입니다) 저작권상 아래는 가립니다​한글자씩으로 이루어진 단어를 부르면서 맞췄을때, 틀렸을때 를 카운팅하면서 퍼센테이지로 정상인지 계산합니다.50개를 다 부르는 병원은 아직 본적이 없으며,  일반적으로는 half인 25개의 단어를 제시해서 퍼센테이지를 환산합니다. ​​검사를  직접 하면서 느낀점은 단어의 경우는 앞에 글자를 통해서 유추를 할 수 있다는것입니다. 예를들어서 '나비' 라는 단어를 검사자가 말했을때 듣는사람이 '나'를 먼저 들었으면 단어기때문에 나..? 나무?  나비?  나라? 이렇게 유추할수 있다면 한글자씩은 정말 때려맞추기가 힘듭니다! 그래서 개인적으로는 WRS가 더 신뢰도가 높은 검사라고 개인적으로는 생각합니다! (단점으로는 많이하면 외울수도 있다...ㅎ 내가 그랬다)​그럼 대학병원이나 개인병원에서 받는 어음검사의 해석은 어떻게 하면 될까? 청각사 보수교육시에 제가 교육을 하고자 임의로 만들었던 검사 기록입니다. 빨간색 네모를 보면 위에서 서술한 SRT, WRS가 적혀있습니다.​Right 오른쪽은 SRT는 15dB, PTA는 11dB 로 위에서 언급한것처럼 10dB 이내에 들어와있기 때문에 신뢰도가 높은 검사라고 할 수 있습니다. 또한 45dB 의 크기로 한글자씩의 검사(WRS)를 하였을때 100% 모두 맞춘것으로 해석합니다.​Left 왼쪽 SRT는 65dB, PTA 61으로 역시나 10dB 이내에 들어와있기 때문에 신뢰도가 높은 검사라고 할수 있다.또한 90dB 크기로 한글자씩 검사하였을때 60% 로 정상범위는 아니지만, 10개중에 6개정도는 듣는다 라고해석할 수 있다.​​​  마치며​오늘은 어음청력검사와 해석방법에 대해서 알아보았다 음....이제 뭘 알아봐야하지 난청카페에 보면 검사기록 해석에 대한 글들이 많은데 다음에는 ABR검사에 대해서 알아봐야겠다 글 쓸려면 오래걸리겠는걸.. ​ "
[졸업할래] 0. 임베디드 환경에서의 딥러닝 구현에 대하여 ,https://blog.naver.com/apnalchangchang119/222932216960,20221118,"0-1. 실시간환경에서 딥러닝 구현의 필요성→ 쉽게 생각하면, 감시카메라나 제어 시스템 등 특정 분야에서 tracking 개념이 들어가면 이미지가 아닌 영상으로 얼굴을 분석해야 하는데, 그 성능이 느리면 객체 자체를 놓칠 수 있고, 결과를 보여주더라도 이미 한참 뒤에 제시될 수 있는 등, 시스템이 제 기능을 발휘하지 못함. 그래서 실시간 환경에서는 처리속도가 아주 중요한 요소 중 하나임.​머신러닝이 이러한 실시간 환경에 적합하긴 하지만, 얼굴의 다양한 변화에 강인하지 못함. 그렇기에 딥러닝을 사용함. 실제 환경에서는 예측할 수 없는 다양한 방해요소가 존재하기 때문에, 최대한 그러한 요소로부터 강인하게 설계하여야 함. 딥러닝으로 하면 머신러닝 보다 확실하게 강인하긴 함. 그럼에도 딥러닝은 작은 객체는 잘 검출하지 못함. 근데 또 MTCNN은 작은 얼굴도 매우 잘잡음. 대신 딥러닝은 기본적으로 파라미터 수가 매우 많아서 메모리 공간을 크게 요구하고, 정확도가 높은 만큼 처리시간도 길게 요구함.​MTCNN은 다른 신경망 대비 약 100배 정도 적고(비교군에 따라 다르겠지만! 내기준) 비교군들과 처리시간과 정확도가 유사함. 그리고 MTCNN은 특히나 저해상도에 매우 강인한 모습을 보임. 그러나MTCNN은 고해상도 영상일수록 처리시간이 급격히 길어진다는 치명적인 문제점이 있다. 또한 이미지 피라미드 기반으로 얼굴 지역을 제안하는 P-net이 메모리 복잡성이 매우 높으며, 전체 처리시간 중 70%를 차지하므로 경량화하여 실시간 성능을 보장할 필요가 있다.​→ 머신러닝 기반에서 딥러닝 기반 접근법으로 넘어가는 이유에 대해 설명하는 걸로~!​feature base의 대표적인 방법인 Haar Cascade Classifier based on Adaboost는 CPU 상의 실시간 환경에서 빠른 속도와 높은 얼굴 검출 성능을 보인다. 그러나 얼굴의 다양한 변화에 민감하다.전통적인 얼굴 검출 방법들은 hand-crafted features에 기반을 두고, cascade / DPM(deformable parts model) / aggregated channel features로 분류된다.  개요1 : 얼굴 검출의 초기 접근 방법인 특징 기반 접근법(머신러닝)은 통제 불가능한 환경에서 정확도가 충분히 높지 않다. 2012년에 DNN을 사용한 이미지 분류에서의 획기적인 작업(work)으로 인해 딥러닝은 거대한 패러다임이 되었다. 컴퓨터 비전에서 딥러닝의 급속한 진보로 인해 지난 수년 동안, 얼굴 검출을 위한 많은 딥러닝 프레임워크가 제안되었고, 정확도에서 충분한 개선이 이뤄지고 있다.[개요1] Minaee, Shervin, et al. ""Going deeper into face detection: A survey."" arXiv preprint arXiv:2103.14983 (2021).​⇒ 머신러닝은 uncontrolled environment에서 충분한 정확도에 도달하지 못한다. 딥러닝은 가능.​개요2 : 테러 활동이 증가하고, 비디오 감시에 대한 수요가 증가함에 따라, 효율적이고 빠른 검출 및 추적 알고리즘에 대한 필요성도 함께 대두되고 있다. 많은 실시간 얼굴 추적 시스템이 과거에 [2]~[6]과 같이 개발되었다.(해당 논문 레퍼런스)[개요2] Chatrath, Jatin, et al. ""Real time human face detection and tracking."" 2014 international conference on signal processing and integrated networks (SPIN). IEEE, 2014.​1. On How to Efficiently Implement Deep Learning Algorithms on PYNQ Platform [★!중요!★]1. IntroductionDNN은 컴퓨터 비전, 스마트 카, 로보틱스에서 금융, 의학, 의사결정까지 매우 많은 분야에 만연하게 사용되는 솔루션이다. 이러한 영역에서 머신러닝 접근법은 실시간 데이터 획득을 기반으로 예측 프로세스를 자동화 할 수 있다. IoT 기술의 확장과 더불어, 이러한 자동화는 임베디드 시스템의 새로운 도전 과제가 된다.이러한 도전 과제에 대한 해답에는 지능형 임베디드 시스템 개발이 필요하고, 자가 치료를 수행하며 환경 변화에 강인해야 한다. 이러한 기능은 머신러닝이 잘 수행한다. 게다가 임베디드 장치는 저전력이다. ⇒ 왜 딥러닝인지는 안알려줌. 단지 시대적 흐름에 따라 다양한 장치에서 자동화가 필요하고, 머신러닝으로 저전력화하여 지능형 임베디드 시스템을 개발하면 해결될 수 있다고 주장. 그리고 FPGA의 재구성 가능함을 통해 최적의 설계를 수행할 수 있음을 주장함.​2. Background[DNN & FPGA]GPU는 딥러닝 구현에 매우 널리 사용되지만, 높은 전력 소비량을 요구하고, 모바일 혹은 임베디드 장치에서 물리적 제약이 되어 딥러닝의 한계를 준다. 이 문제를 해결하기 위해, FPGA는 전력 소비량을 크게 줄일 수 있으면서도 실행 시간도 경쟁력 있다. FPGA의 공간적 구조는 메모리 계층에 이점이 되고, 데이터 접근에 필요한 전력을 줄일 수 있다.⇒ GPU는 병렬처리로 연산 능력은 개쩔지만, 그만큼 전력 소비량도 매우 높기 때문에, 모바일 및 임베디드 장치에서는 FPGA를 사용하여 저전력으로 구현하면서도 처리시간도 경쟁력있게 챙길 수 있다.​[PYNQ & Intelligent embedded system]PYNQ는 파이썬으로 직접 보드의 PL(Programmable Logic)을 이용할 수 있는 소프트웨어 개발 도구를 가진 Xilinx ZYNQ SoC 기술을 탑재한 플랫폼이다. FPGA 기반 IP 설계자는 최적화된 기능을 제공해준다. PYNQ-Z1은 첫번째 출시된 보드이고, 파이썬의 생산성과 병렬 제어를 가진 PL의 유연성 및 높은 에너지 효율성은 임베디드 장치로써 효율적인 해결책이 되는 보드이다.⇒ “PYNQ는 임베디드 장치로 좋다!”​[Related work]DNN은 학습 단계와 추론 단계로 나뉘며, 추론 단계는 임베디드 시스템에서 구현하기에 적합하다. 구현의 복잡성을 줄이기 위해 다양한 기술이 사용된다 [7 - in this paper].복잡성을 줄이면서 하드웨어 설계자에게 도움되는 프레임워크도 출시되었다. 예를 들어 [10 - in this paper]은 이진화된 신경망을 하드웨어에 효율적으로 매핑하는 프레임워크이며, [11 - in this paper]은 FPGA 하드웨어 자원을 줄이는데 사용된다.​딥러닝을 FPGA로 구현하는 것에 대한 전반적인 내용임!​[1] Stornaiuolo, Luca, Marco Santambrogio, and Donatella Sciuto. ""On how to efficiently implement deep learning algorithms on pynq platform."" 2018 IEEE Computer Society Annual Symposium on VLSI (ISVLSI). IEEE, 2018.​​​2. A survey of Feature Base Methods for Human Face Detection얼굴 검출에 방해 요인에 대해 Introduction에서 설명해주고 있음. : Pose, facial expression, occlusion, image orientation, imaging conditions, different facial features, face size, illumination : 수많은 응용 분야 [intelligent human-computer interfaces, content-based image retrieval, security, surveillance, gaze-based control, video conferencing, speech recognition assistance, video compression … ][2] Hatem, Hiyam, Zou Beiji, and Raed Majeed. ""A survey of feature base methods for human face detection."" International Journal of Control and Automation 8.5 (2015): 61-78.​​​3. Joint face detection and alignment using multitask cascaded convolutional networksMTCNN의 구조와 실험 결과를 보여주는 논문임. (MTCNN 최초 논문)[3] Zhang, Kaipeng, et al. ""Joint face detection and alignment using multitask cascaded convolutional networks."" IEEE signal processing letters 23.10 (2016): 1499-1503.​​​2018_Survey of Face Detection on Low-Quality Images [3에 있는 내용 (중복)]- 해당 논문은, hand-crafted & deep-learning 기반 얼굴 검출기가 저화질의 영상에서 충분히 강인하지 않음을 주장한다. FDDB와 같은 고해상도 영상에서 얼굴 검출 성능은 포화단계에 이르렀지만, 대부분의 인기있는 얼굴 검출기들은 blurring이나 noise와 같은 왜곡이 포함된 저해상도 영상에서 평가되지 않았다. 이는 고해상도 샘플로 훈련된 객체 인식 네트워크들이 저해상도 영상에서 테스트될 때, 충분히 신뢰할만하지 못하다는 것을 보여준다[4-1]. 그러나 다중 스케일 설계의 신경망은 저해상도 및 blurring에 의해 야기된 성능 저하를 보상해줄 수 있을지도 모른다.​- 전통적인 hand-crafted detector인 Haar, HoG-SVM, faster-RCNN, S^3FD에 대해 저해상도에서의 성능 테스트를 수행하였음.​- Hand-crafted detector는 대체로 CPU 상에서 실시간 검출 성능을 달성할 수 있지만, hand-crafted features는 자세, 표정, 폐색, 환각과 같은 복잡한 얼굴 다양성에 강인하지 못하다. 그러므로 이러한 방법들은 저해상도 샘플에 대해 적응하지 못할수도 있다.​- Hand-crafted features를 사용한 방법들과 비교했을 때, 딥러닝 기반 접근법들은 많은 양의 데이터로 훈련하여, 얼굴의 큰 변화도 성공적으로 포착 가능하고, 가장 도전적인 부분은 작은 얼굴을 감지하는 것이다.Cascade CNN이 높은 연산 비용과 높은 다양성 문제를 다루기 위해 처음 제안되었다라고 주장하며, Joint Cascade CNN과 MTCNN을 소개함. 이 접근법들은 높은 연산 속도를 보인다는 장점이 있으나, 이미지 피라미드 사용이 요구되며 많고, 작으며, 블러링된 얼굴을 찾는 문제를 명시적으로 해결하진 못한다. (그래도 기본적으로 어렵긴 하다라는 뜻인듯?)​⇒ 머신러닝은 얼굴의 다양한 변화에 강인하지 못하지만, 딥러닝은 강인함. 그러나 딥러닝은 작은 얼굴을 감지하는 것에 유난히 강인하지 못함을 보인다. 그래서 머신러닝과 딥러닝으로 저해상도 영상에서의 얼굴 검출 성능을 수행하였다.​결론: 해당 논문은 blur, nosie, contrast의 정도를 바꿔가며 머신러닝 모델 2개, 딥러닝 모델 2개으 성능 저하를 테스트하였다. 실험 결과는 머신러닝의 hand-crafted과 deeply learned features 둘다 저화질 영상에 대해 꽤 민감함을 보여준다. 그리고 스케일-불변성 구조와 비교했을 때, 다중 계층으로부터 특징들을 추출하는 신경망의 스케일-가변 설계는 블러링된 작은 얼굴 검출에 도움이 된다는 것을 보여준다.​⇒ 결과적으로, 머신러닝 보다 딥러닝이 작은 얼굴 검출에 도움된다는 것을 확인 가능!​[4] Zhou, Yuqian, Ding Liu, and Thomas Huang. ""Survey of face detection on low-quality images."" 2018 13th IEEE international conference on automatic face & gesture recognition (FG 2018). IEEE, 2018.​​​A. Real-time face detection based on YOLO (참고할 만한 좋은 내용은 없으나, 좋은 표현이 있음.)abstactYOLO는 속도가 빠름. 해당 논문에서는 이를 증명함.Introduction얼굴 검출 알고리즘이란, 후보 영역으로써 주어진 영상에 ROI를 선택하는 역할을 수행.얼굴딥러닝 기반 타겟 검출의 종류→ R-CNN 기반 [Fast / Faster R-CNN, SSD, YOLO … 대표적인 레퍼런스 참고하기 좋음]→ 지역 타겟 검출 기반 : 얘는 모든 잠재적인 지역을 생성하는 부분과 다양한 특징 계층을 포함하고 있으므로, 실시간 성능이 보장되지 않음.YOLO의 R-CNN과 다른 점→ 훈련, 검출, 특징 추출, 분류 등 모든 작업이 단일 네트워크→ 일단 영상이 입력되면, 모든 객체의 위치, 카테고리, 일치하는 객체일 가능성을 얻을 수 있다.[A] Yang, Wang, and Zheng Jiachun. ""Real-time face detection based on YOLO."" 2018 1st IEEE international conference on knowledge innovation and invention (ICKII). IEEE, 2018.​​​B. A real time face emotion classification and recognition using deep learning modelabs얼굴 검출과 인식 연구는 최근에 널리 연구되고 있다. 얼굴 인식은 현대의 전자 장치, 범죄 조사, 데이터 베이스 관리 시스템, 스마트 카드 등에서, 보안, 카메라 감시, 신원 확인과 같은 분야에 응용되며 그 역할이 매우 중요해지고 있다.[B] Hussain, Shaik Asif, and Ahlam Salim Abdallah Al Balushi. ""A real time face emotion classification and recognition using deep learning model."" Journal of physics: Conference series. Vol. 1432. No. 1. IOP Publishing, 2020.​  0-2. 실시간 환경에서 딥러닝 구현을 위해 임베디드 시스템의 이점 [FPGA]딥러닝을 FPGA로 구현하면 어떤 이점이 있나?​MDPI !신뢰 주의!​[1] 2021_Face Recognition Based on Deep Learning and FPGA for Ethnicity Identification → MDPI [주의]​- Deep CNN(DCNN)을 기반으로 새로운 딥러닝(머신러닝 기반임) 접근법을 개발되었다. 그러나 현재 CPU의 낮은 연산 능력 때문에, 동작 가능한 DCNN 기반 얼굴 인식 시스템을 구축하기 위해서는 전문화된 고성능 컴퓨팅 하드웨어를 활용할 필요가 있다. 최근 이러한 접근법으로 인해 전력 소비량 및 실행 시간 측면에서 네트워크의 효율이 증가되었다. 이에 착안하여 해당 논문에서는 FPGA를 사용하였고, GPU와 성능을 비교하였다.​- 인종(Ethnicity) 확인을 위해 FPGA에서 딥러닝을 구현하여 얼굴 인식 시스템을 만들었고, 그 성능을 GPU와 비교함으로써 FPGA가 딥러닝 구현에 더 적합하다는 것을 주장한다. GPU의 전력 소비량이 이상함. FPGA가 압도적임을 보여주기 위해 스펙의 최대 사양을 적어 놓은 것으로 추정.Table. 3를 보면 실험 해당 네트워크에 대해서 FPGA가 성능 및 전력 소비량이 GPU를 웃돈다는 것을 알 수 있다.즉, GPU는 더 많은 전력을 소비하고, 이는 휴대 가능한 임베디드 시스템으로써 향후 기술에 대한 적응력이 낮다는 것을 의미한다. 그리고 FPGA는 낮은 전력 소비량으로 빠른 속도의 신원 확인 시스템 구현이 가능하다.​FPGA : PYNQ-Z1GPU : NVIDIA GeForce RTX 2070, RAM 16GB​⇒ 실행 시간의 경우, 널리 사용되는 신경망 중 몇 가지인 ResNet, DenseNet, AlexNet, GoogleNet에 한해서는 GPU 보다 FPGA가 더 빠르며, 특히 전력 소비량의 경우에는 GPU의 전력 소비량 보다최소 1/10 수준임을 알 수 있다. 이로 인해 FPGA에서 딥러닝을 구현하는 것은 실행 시간과 전력 소비량 측면에서 이득임을 알 수 있다.[1] AlBdairi, Ahmed Jawad A., et al. ""Face Recognition Based on Deep Learning and FPGA for Ethnicity Identification."" Applied Sciences 12.5 (2022): 2605.​​​[2] Architecture Design of Convolutional Neural Networks for Face Detection on an FPGA PlatformCNN은 아주 높은 정확도를 보이지만, 엄청난 양의 연산량, 저장공간, 메모리 접근 때문에 모바일 혹은 임베디드 시스템에 이용하기 어렵다. 그래서 해당 논문에서는 전력 제약 아래에 연산량, 저장공간, 대역폭 요구 조건을 최소화하면서 처리량을 증가시키는 방법을 제안한다.얼굴 검출은 입력 영상에서 얼굴의 수와 그 위치를 찾는 것이 목적인데, 크기, 위치, 자세, 회전, 얼굴의 표정, 폐색, 방대한 배경 등의 변화로 인해 어려움이 있다. 이러한 문제를 해결하기 위해 haar-like features, binarized features 등의 hand-crafted features가 과거 10년 동안 제안되어왔다.비록 CNN 기반 알고리즘들이 월등한 성능을 보일지라도, 방대한 연산량, 저장공간, 메모리 접근은 특히 속도와 전력 제약으로 인해 모바일 혹은 임베디드 시스템에 사용을 어렵게 만든다. 해당 논문 레퍼런스[10]에 따르면, 신경망에서 가중치와 편향은 매우 많기 때문에, 저장공간과 연산량을 줄이고자 파라미터의 precision을 낮추거나 그 양 자체를 줄여야 한다고 주장한다.해당 논문에서는 앞서 언급된 바에 대하여, 얼굴 검출 알고리즘의 효율적인 하드웨어 구조를 제안한다.실험결과 : FC layer를 수정함으로써, MACs(multiply-accumulate operations)의 83%를 절약했다. Quantizing을 적용하여, 파라미터의 word-length를 32-bit에서 2-bit로 성능 저하 없이 줄였다. (말되나근데)[2] Yu, Bin-Syh, et al. ""Architecture design of convolutional neural networks for face detection on an fpga platform."" 2018 IEEE International Workshop on Signal Processing Systems (SiPS). IEEE, 2018.​​​[3] Collaborative Edge Computing With FPGA-Based CNN Accelerators for Energy-Efficient and Time-Aware Face Tracking System전통적인 CNN은 얼굴 추적 시스템에서 얼굴 검출 및 인식을 구현하기에 좋은 기술이지만, 적지 않은 연산 시간과 높은 에너지 소비량 가지므로 “large-scale time-sensitive FT system”에는 적합하지 않다. 이에 해당 논문에서는 낮은 대기 시간(low-latency)과 저전력의 FT system 구현을 위해 FPGA 기반 CNN 가속기를 사용하여 AI 및 AIoT를 지원하는 edge-cloud 협업 컴퓨팅 (ECCCC) 시스템을 제안했다.사용된 신경망은 compact MobileNet CNN이고, FPGA 상에서 하드웨어 가속기로 구현하였음.아래 그림은 모바일넷에 대한 하드웨어 가속기 구조를 나타냄 (구조 참고용) [3] Liu, Xing, et al. ""Collaborative edge computing with FPGA-based CNN accelerators for energy-efficient and time-aware face tracking system."" IEEE Transactions on Computational Social Systems 9.1 (2021): 252-266.⇒ 본 논문은 하드웨어 구조에 대해 비교적 구체적으로 설명하였음. 참고하기 좋음!​​​[4] 2022_Hardware Accelerators for Real-time Face Recognition: A survey[나에게 유리한 자료는 아님.]- 신경망 기반으로 얼굴 검출/인식 시스템을 개발하는게 좋다고 하지만, 아무도 survey로 다루지 않아서 얘네들이 함.​- 얼굴 검출의 중요성 중 새로이 주목할만한 멘션은, “얼굴은 신원확인의 가장 효율적인 특징 중 하나이며, 접촉할 필요가 없고 동시에 여려 명을 인식할 수 있다는 것이다.”​- 얼굴 인식 시스템에서는 조명, 자세, 정확도, 크기 등의 조건에서도 인식할 수 있는 얼굴의 수가 중요하다. 현존하는 모든 시스템은, 훈련된 영상과 유사할 경우 좋은 성능을 보인다. 해당 논문에서는 하드웨어 가속기와 가속기들의 성능을 제시함.​- FPGA는 프로그래밍 가능한 반도체로 정의되고, 수만개의 CLB(configurable logic block)과 수백개의 floating-point DSP blocks으로 구성되어 있다. CPU나 GPU와 달리, FPGA는 구조화된 칩이나 유선 연결되 CPU를 내장하지 않는다. 이는 매번의 단일 instruction에 대해 반복되는 fetch와 decode operation에 의해 야기되는 긴 대기 시간을 피할 수 있게 해준다. (병목현상 없다는 뜻인듯) + 병렬처리가 가능하다. 그래서 높은 처리량과 저전력 동작이 가능하지만, multi-core CPU와 GPU 보다는 더 낮은 clock rate를 가질 것이다.​- Xilinx와 Altera가 FPGA 칩과 소프트웨어 개발 도구를 함께 제공해주지만, 여전히 DNN의 모든 가중치의 수에 비해 적은 메모리 저장 공간 때문에, FPGA에서의 구현에 적합한 알고리즘은 아니다.​IV - B. NEURAL NETWORK ALGORITHMS FOR FACE DETECTION AND RECOGNITION에서[2] [83] [84] [85] [86] [88] : 얼굴 검출에 성공적임.[84]에서, MTCNN과 Haar features 기반 알고리즘의 성능을 비교했는데, 확실히 좋다.→ 라즈베리 파이에서 구현할거라서 쓸모있을까 싶긴한데, 성능 비교는 ㄱㅊ함?.[4] Baobaid, Asma, et al. ""Hardware Accelerators for Real-time Face Recognition: A survey."" IEEE Access (2022).​​​[5] Methodology for CNN Implementation in FPGA-based Embedded SystemsGPU가 성능 측면에서는 다른 하드웨어를 장악했지만, FPGA는 GPU 비해 유사한 성능에 전력 소비와 비용 절감으로 저명한 대안으로 입증되었다. FPGA 구현의 주요 관심사는 시스템을 개발하는데 필요한 노력과 각 프로젝트에 사용되는 매우 이질적인 아키텍처로 인해, 다른 사람이 그 설계를 재사용하기 어렵다는 것에 있다. 그래서 해당 논문에서는 FPGA 상에 CNN 구현을 위해 설계된 방법론과 높은 수준의 아키텍처를 제안하며, 이는 개발 과정 및 설계 재사용을 용이하게 하고, 성능 극대화 및 대기 시간 최소화, 그리고 리소스 활용률과 병목현상을 줄이면서 설계 유연성을 높이는데 도움된다. 한번 구현되면 FPGA도 유연성이 감소한다. 확실하게 ASIC 보다 유연성은 높고, 다른 플랫폼보다 latency가 낮으며, 효율도 좋은 편임. 다만 개발 복잡성이 높음.⇒ FPGA는 GPU와 유사한 성능을 가지면서도 저전력이 가능하기 때문에 GPU의 좋은 대체제로 고려되고 있다. 논문 내에서 임베디드에 적합하다라고 말하진 않지만, 논문 제목 자체가 FPGA 기반의 임베디드 시스템에서 CNN 구현 방법론이기 때문에 간접적으로 주장할 수 있다.​⇒ FPGA가 GPU 대비 저전력이라 모바일 및 임베디드 시스템에 적합하지만, FPGA 개발이 고난도이고, 타설계자가 바로 재사용하기 어렵다. 그래서 이러한 문제를 더 용이하게 만들기 위한 방법론을 제시하였고, 아래 4가지 이점을 가진다.개발과정 및 설계 재사용의 용이성 증대성능 극대화 및 대기 시간 최소화리소스 활용률 및 병목 현상 감소설계 유연성 증가[5] Zacchigna, Federico G. ""Methodology for CNN Implementation in FPGA-based Embedded Systems."" IEEE Embedded Systems Letters (2022). "
올림푸스 DS-F1 Voisquare ,https://blog.naver.com/hauhq/222669003313,20220310," 08~09년 쯤 스마트폰이 막 보급되던 시절에 옵티머스원, 옵티머스2X를 잠깐 사용하다가, 아이폰 4S로 갈아탄 이후에는 안드로이드 스마트폰을 사용한 적이 없다. 그래서 최신 안드로이드 os가 많이 어렵게 느껴지는데, 대신 안드로이드 2.0~4.0에는 이유 모를 내적 친밀감을 느낀다. 오늘 가져온 기기도 안드로이드 4.3 기반 기기라 친근함에 구입하게 된 것이다.​​ Previous imageNext image전체적인 디자인이 세련되었다고 느껴진다. 요즘에는 보기 힘든 탈착식 배터리 커버와 이어폰 단자, 그리고 추가로 외부 마이크와 마이크 인풋 단자도 보인다. ​​ 모델명은 DS-F1, 일본 올림푸스에서 출시한 안드로이드 기반 보이스레코더 겸 speech-recognition device이다. speech-recognition device가 뭔지 몰라서 한 번 찾아보았는데, 제조사 설명에 따르면 의료 및 공사 현장 등 필기가 어려운 환경에서 사용자의 음성을 인식하여 자동으로 문서화해주는 기기를 의미하는 것 같다.​​ 스피치 리코그니션 기능을 위한 슬라이더와 버튼.​​ 실제로 판매자도 의료 현장에서 지급받은 것이라고 한다. 리테일로 판매된 적이 없고, 업체와 계약을 통해 보급되던 기기라 개인이 구할 방법이 없어서 희소성이 있을 거라는 이야기도 들었다. 검색해보니 확실히 희귀한 기기는 맞는 것 같은데, 다만 인지도나 정보 자체가 없어서 딱히 수요는 없을 것 같다. 매물도 없지만 구하는 사람도 없는. 그런 느낌이다.​​ 현장에서 쉽게 충전 및 데이터 전송이 가능하게 해주는 도킹스탠드​​ 본인은 고음질 안드로이드 플레이어로 사용할 생각에 구매를 했는데(왠만한 보이스레코더는 음질이 평균 이상은 간다.), 기기 자체 스펙이 딸리는 건지 아니면 최적화가 잘 안되어 있는건지 버벅거림이 조금 있고, 24비트 flac 파일은 끊김이 살짝 있는 것 같아서 mp3 코덱 재생용으로 사용을 해야될 것 같다. 예상대로 여타 보이스레코더처럼 음질 자체는 준수한 편이다.​​ 엘든링 사운드트랙을 들어보았다. 여러분 모두 갓겜 엘든링 하세요.​ "
"HTK, julius, julia ",https://blog.naver.com/yjcapzzang/222309776083,20210414,"julius - 음성 인식 엔진, 고성능 2 패스 LVCSR(Large Vocabulary Continuous Speech Recognition) 디코더 소프트웨어HTK(Hidden Markov Toolkit) - 숨겨진 Markov 모델을 구축하고 조작하기 위한 휴대용 툴킷julia - matlab과 사용하는 곳이 비슷한 프로그래밍 언어?​HTK 회원가입 후 다운로드julius 다운, julia 다운받고환경변수 설정​환경변수 설정을 위해 제어판 - 시스템 왼쪽 상단쪽에고급 시스템 설정을 클릭하면 시스템 속성이 뜨는데밑에 환경변수를 클릭 사용자 변수에서Path를 들어간 후 다운받은 경로들을 넣어주고​확인으로 cmd창을 켜서Hvite 입력 이렇게 옵션들 뜨면 된거고다음은julius-4.3.1 입력(각자의 버전에 맞게 입력) 이렇게 나오면되고julia -v 입력 julia version이 뜨면 된거​만약 제대로 안나오면오류 발생한 지점을 확인하고다시 반복하면 됨​환경변수 경로가 문제라면cmd창에서 echo %PATH% 입력해보면확인하기 편함 "
전문가 시스템을 만드는 진짜 전문가 ,https://blog.naver.com/jmkim461/222442490225,20210723,"놀라운 속도로 발전하고 있는 4차 산업혁명 시대에 꼭 필요한 '전문가'란?점점 치열해지는 경쟁에서 살아 남기 위해'데이터'를 활용해 진짜 '전문가'로 거듭나는 방법에 대한 이야기​인공지능, 어디까지 진화해왔나?​과거에는 인간도 동물처럼 적절한 자극을 주면 원하는 행동을 만들어낼 수 있다고 믿었다. 지금은 인간이 자극에 반응하는 것은 맞지만, 내적 사고과정을 거친 후 하나의 행동을 선택한다는 믿음이 지배적이다.  인간 시스템인공지능 발전은 4단계로 진화된다. 1세대는 '계산주의' 단계, 2세대는 전문가 시스템의 '연결주의' 단계, 3세대는 주요인자(Feature)선정에 인간이 개입하는 '통제기반'의 단계, 그리고 마지막 4세대는 AI가 인자 선정도 스스로 수행하는 '딥 러닝' 단계이다. 현재, [인간 시스템] 단계까지는 인공지능이 인간보다 우수하게 수행할 수 있으므로, 인간은 상식을 가지고 인공지능의 결정을 지켜보고, 인공지능이 놓치는 것이 있을 때만 개입하는 단계까지 발전해 왔다. 한편 4차 산업혁명 시대를 맞아 놀라운 기술 발달로 인공지능이 우리에게 엄청난 영향을 줄 것이라는 목소리가 높아지고 있다. 세계경제포럼(WEF) 창시자인 클라우스 슈발 회장도 ""인공지능에 인간 일자리가 심각한 위협을 당할 것""이라고 강조하는가 하면, 유발 하라리 역시 ""인공지능을 인간의 인지능력을 기계가 대체하는 과정""이라고 설명하고 있다. 즉, 극소수 사람만이 사고력을 바탕으로 새로운 지식을 창출하고 나머지는 이미 구축된 시스템을 단순 이용하게 되면서 사고할 기회마저 박탈당할 위험이 처해있다는 의미다. 결국 우리 앞에는 인간과 인공지능과의 경쟁이 아닌, 인공지능을 아는 사람과 모르는 사람의 경쟁이 놓여있는 샘이다.​비전문가도 활용 가능한 '전문가 시스템' 전문가 시스템'전문가 시스템'이란 전문가 지식을 수집, 정제 및 구조화하여 비전문가도 탐색하여 활용하는 시스템을 의미하는데, 대표적인 예로 1970년대 초 스탠포드 대학교에서 개발한 '마이신(MYCIN)'이라는 질병 진단 프로그램이 있다. 현대에 들어와서는 이알피(ERP), 네비게이션, 은행대출심사 보조 프로그램 등이 전문가 시스템에 해당된다.​회사의 가장 전형적인 DGMS(Design Management System)나 DfM(Design for Manufacturing)을 꼽을 수 있다. FMEA(Failure Mode and Effects Analysis) 시스템 역시 전문가 시스템의 일종으로 볼 수 있다.​전문가 시스템이 잘 작동되기 위해서는 세 가지 조건이 수반된다. 첫째, 막대한 양의 전문가 지식을 구조화하여 정보화 작업이 이뤄져야 한다. 둘째, 전문가 사이에서 보편적인 합의가 이뤄지지 않은 경우, 이에 대한 극복 방안이 마련돼야 한다. 셋째, 전문 지식이 없는 사람도 사용 가능하도록 일상 언어와 전문가 언어 사이에 정확한 매칭이 이뤄져야 한다.​실행 가능성은 첫째 조건이 가장 어렵고, 둘째 및 셋째 조건은 머신 러닝의 도움을 받는다면 충분히 극복 가능한 것으로 보인다. DGMS를 비롯하해 현재 우리가 사용중인 전문가 시스템들이 세 가지 조건을 모두 충족하고 있는지 DX과제로 정해 2세대 수준이 아닌 인공지능이 접목된 4세대 수준으로 심도 있게 연구해 볼 가치가 있다. DGMS에 데이터가 많이 축적되면 그것으로부터 새로운 지식 혹은 새로운 규칙을 발견하는 체계를 구축하는 것도 꽤 의미 있는 프로젝트가 될 것이다.​데이터베이스에서 새로운 지식을 발견하다.​한때 KDD(Hnowledge Discovery in Database)가 화두가 되던 시절도 있었다. 아래 그림을 살펴보면 KDD 프로세스는 우리가 아는 빅데이터 분석 절차와 크게 다르지 않음을 알 수 있다. KDD(Knowledge Discovery in Database)개발 데이터를 체계적으로 쌓아서 새로운 지식을 발견해가는 과정에 대한 사례로 '3-Layer Design Process'를 소개하고자 한다. 여기서 우리는 데이터 수집 구조와 해상도를 어떻게 가져가야 할 것인가에 대한 힌트를 얻을 수 있다. Layer Design Process이렇게 모은 데이터를 통해 DP(Dynamic Programming) 알고리즘을 활용하여 지식 혹은 규칙을 찾아낼 수 있는데, 찾아낸 규칙의 형태는 아래와 같이 표현된다. DP(Dynamic Programming) 알고리즘을 활용한 If-, Then-구조1957년 리처드 벨먼(Richard Bellman)이 개발한 DP(Dynamic Programming)는 어떤 문제의 최적 해답을 얻기 위해 문제를 작은 부분으로 분할하여 각 부분에 대한 가장 적당한 해답을 얻는 알고리즘으로, 순서화 된 의사결정 문제 해결에 적용할 수 있다. 요즘엔 음성인식(Speech Recognition), 도형인식(Image Recognition), 바이오인포메틱스(Bioinformatics)처럼 다양한 패턴매칭(Pattern Matching)문제에 적용되고 있다. 우리는 If-, Then 구조를 보면서 연관규칙(Association Rule) 혹은 순차분석(Sequence Analysis)과도 맥이 닿아있음을 알 수 있다.  마지막으로 도움이 될만한 KDD의 7단계를 소개하며 마무리하고자 한다. KDDㄱ단계는 6시그마의 DMAIC(Define, Measure, Analysis, Improve, Control) 또는DIDOV(Define, Identity, Design, Optimize, Verify)와 같은 맥락으로 보이는데, 빅데이터는 소급 연구나 관측 연구 중심으로 발전해왔다면, 6시그마는 실험을 중심으로 논리를 전개하는 것으로 보인다.​(인용) LGE THE DX 2021.3.17일자 "
영국 어학연수 -EF 옥스퍼드 ,https://blog.naver.com/raffles7/222738243118,20220519,"EF는 1965년에 설립된 이후 현재 영국, 미국, 캐나다, 오세아니아, 싱가포르, 몰타까지 50개 도시에서 어학연수과정을 운영하는 국제적인 교육 기간입니다.  코로나로 인해 미루어 놓았던 해외연수를 계획 중이신 분들이 많으실 텐데요? ​​오늘은 6개월 이상 장기 어학연수를 계획하시는 분들께 EF 옥스퍼드 캠퍼스의 학기제 어학연수 과정을 소개 드리려고 합니다.​ ​​브리티시 카운실에서 영국 내 학교평가를 발표하는데 영국 내에 있는 EF 전 센터가 최고 등급을 획득했답니다!​  ​영국 내 EF는 오늘 소개 드리는 옥스퍼드 캠퍼스 외에도 맨체스터, 런던, 브리스톨, 브라이튼, 케임브리지, 이스트본, 본머스와 토키에도 캠퍼스를 가지고 있습니다.  학기제 등록 학생들은 위 캠퍼스 간의 이동도 가능합니다.​EF의 어학연수는 2주부터 수강이 가능한 기간 선택 프로그램만 있는 것이 아니라 대학교 학기처럼 학기제 어학연수 과정도 있습니다.EF의 학기제 어학연수 과정은 6개월 과정, 9개월 과정, 11개월 과정 중에서 선택하실 수 있습니다.​EF의 학기제 과정의 특징에 대해 말씀드릴게요.학기제 과정은 일반적인 기간 선택 과정과 달리 학기(Term)에 맞추어 수업이 진행되고 각 텀 사이에 텀 브레이크가 있습니다.  멀리 영국으로 어학연수를 계획하시는 분들은 어학원 수업만을 위해 멀리 영국으로 가신 것은 아니실 거예요.  이런 텀브레이크 기간을 잘 이용하시면 영국 및 주변 유럽으로 여행하시며 다양한 경험을 하실 수 있습니다.  그리고 모두 학창 시절에 경험해 보셨겠지만 방학 정말 필요하죠?  방학 동안 여행이나 휴식을 통해 리프레시하고 그동안 지난 학기 어학연수를 하면서 느꼈던 부족한 면을 셀프스터디하면 끌어올려 다음 학기에 잘 대비하는 중간 점검의 시간으로 삼으실 수 있습니다.​예로 한국 학생들이 많이 선택하는 9월에 시작하는 9개월 학기제의 학사일정을 한번 보실까요?  9월에 시작해서 내년 5월에 끝나기 때문에 휴학하고 어학연수를 계획하시는 분들이 많이 선택하십니다.​  AY Sep 2022SchoolsAll Schools개강Mon 19 SepTerm 1Tue 20 Sep - Fri 16 Dec (13 wks)Break 1Sat 17 Dec - Sun 8 Jan (3 wks)Term 2Mon 9 Jan - Fri 31 Mar (12 wks)Break 2Sat 1 Apr - Sun 16 April (2 wk)Term 3Mon 17 April - Fri 19 May (5 wks)종강Sat 20 May, 2023  ​지금부터 장기 어학연수를 계획하는 학생들에게 EF 옥스포드를 추천드리는 이유를 말씀드릴게요.​ 영국 최고 대학인 옥스퍼드 대학이 있는 교육도시인 옥스퍼드답게 EF 옥스퍼드 캠퍼스는 다양한 영어 프로그램을 제공합니다.​일반적인 회화 위주의 영어연수과정 이외에도, 캠브리지 시험 준비과정, ILETS 과정, 대학 입학 준비과정, 석사 및 MBA 준비과정 등 다양한 연수 과정을 선택하실 수 있습니다.  영국으로 학사나 석사 유학 계획이 있으신 분들은 EF 옥스퍼드 캠퍼스에서 일반 회화 위주의 영어 연수보다 좀 더 맞춤 코스를 선택하실 수 있습니다.​ ​​2.  EF 옥스퍼드는 영국 내 EF 캠퍼스 중에서 유일하게 교내 기숙사가 있습니다.  도보로 학교 등하교가 가능하니 대중교통비 비싼 영국에서 비용 절감도 되고요, 아침, 저녁 식사도 제공되니 생활비도 절약하실 수 있어요. ​​3.  EF 옥스퍼드 학생들은 도보 5분 거리에 위치한 Oxford Brooks University의 퍼실러티를 사용 가능합니다.  도서관, 헬스장은 무료로 사용 가능합니다.  옥스퍼드 브룩스 대학의 퍼실러티를 사용하다 보니 현지 대학생들과 커뮤니케이션을 할 기회도 많고, EF 학생들은 교내 Blackwell 서점에서 책 구매 시 할인 혜택도 받을 수 있답니다.​4.  장기로 가는 연수이니 만큼 어학연수 교육기관의 선택이 중요하지요? EF는 1965년 이후 50여 개 도시에서 안정적으로 영어교육사업을 하고 있는  글로벌 인지도 있는 기업으로서 한국에도 지사를 가지고 있습니다.​​​지금부터는  EF의 수업 방식에 대해  말씀드리겠습니다. EF의 어학연수과정은 다른 어학원보다 좀 더 듣기와 말하기 능력 항상에 포커스가 맞추어져 있습니다.​ 위 영상에서 보신 EF의 수업 체계는 다음과 같이 5 가지의 수업으로 구성되어 있습니다.1. General Class : 저희도 익히 경험한 강남역 영어학원 같은 소규모 렉처 스타일 수업2. Conversation class : 다양한 디스커션 수업3. iPAD Class : 아이패드를 이용한 2-3명의 pair conversation 수업4. i Lab computer class: 유창한 영어를 위해서는 파트너와의 공부도 필요하나 학습자 스스로의 혼자 해야 할 공부들이 있죠? 바로 올바른 문법을 익히고 올바른 발음으로 자신의 생각을 정리하고 전달해야 합니다. 그를 위한 video lesson(온라인 문법 공부)과 speech recognition(발음 수정)을 컴퓨터를 통해 공부하고 itracker라는 툴을 통해 본인의 프로그레스를 확인할 수 있습니다.5. SPIN Class: Special Interest Class로 대학 교양처럼 ILETS 같은 어학관련 수업은 물론 영국 문학, 비즈니스, 경제 및 법률, 필름메이킹 같은 흥미 있는 과목 수업을 영어로 참여하는 수업입니다.​주당 수업 시간에 따라 일반과정과 집중과정을 고르 실 수 있습니다. ​​​해외연수를 결정할 때 국내 어학원이 수준이 딸리거나 부족해서 해외연수를 떠나시는 것은 아닐 거예요. 영어를 쓰면서 낯선 곳에서 현지인의 삶을 체험해 보고 영어 공부에 대한 동기부여를 얻고 싶으셔서 이실 텐데요.성공적인 어학연수는 어학당 수업 방과 후를 어떻게 지내느냐가 매우 중요합니다. 한국인 학생들과 몰려다니면서 보낸다면 영어는 늘지 않을 거예요. EF는 한국인 학생과 중국인 학생 비율이 매우 적은 것이 특징입니다.​그리고 EF 옥스퍼드 프로그램은 어학원 수업 외에도 다양한 액티비티가 있는 것도 특징입니다.​ EF의 액티비티에 열심히 참여하셔서 방과 후 시간도 알차게 보내보시길 바랍니다.​ ​EF 옥스퍼드의 어학연수 프로그램에 대해 더 궁금하신 사항은 저희 링크에이드로 연락 주시면 친절히 답변드리겠습니다. ​  ​​링크에이드는 EF 어학연수 입학 수속을 무료로 도와드리고 있습니다. 6월 10일까지 저희 링크에이드를 통해 등록하시는 분들에게는 등록 기간에 따라 등록비 면제 혹은 할인 혜택을 드리고 있으니 놓치지 마시기 바래요!​ ​​입학 허가를 받은 이후 출국 및 현지 정착까지 세심히 도와드리고 있습니다.관심 있으신 분들은 카카오톡 아이디 linkaid로 카톡 상담 가능합니다. 전화가 편하신 분들은 010-2473-0049로 전화 주시면 친절히 안내해 드리겠습니다.​​ ​ "
편측성 난청 진단받았다면? 포낙 크로스P 이용 후기와 장점 ,https://blog.naver.com/phonakkr/222631954500,20220128,"​​​안녕하세요. 포낙입니다.  우리는 일반적으로 난청이라는 것을 생각하면 양쪽 귀의 청력이 모두 떨어지는 경우를 생각하게 되는데요. 하지만 의외로 한쪽 귀의 청력이 좋지 않은 편측성 난청도 흔하게 나타납니다.  편측성 난청이란, 한쪽 귀는 정상이지만 다른 한쪽 귀의 청력이 현저히 떨어져양쪽 귀의 청력에 차이가 나는 유형의 난청입니다.   오늘은 편측성 난청을 위한 스마트한 솔루션, 파라다이스 플랫폼의 포낙 크로스 P가우리 삶에 어떠한 도움을 주는 지에 대해 살펴보도록 하겠습니다. 😊​​  ​​편측성 난청은 어떤 특징이 있나요?   ​ 매년 신생아 1000명 중 한 명, 난청 아동 중 3분의 1, 그리고 난청 성인 중 7.2%에 해당하는많은 사람들이 편측성 난청을 겪고 있다고 합니다.  편측성 난청으로 인한 대표적인 불편함으로는 난청이 있는 귀 방향으로 들려오는 소리의 탐지가 어려우며,소리 위치의 변별 및 소음 속 청취의 어려움이 있습니다.  따라서 편측성 난청을 겪고 있는 난청인들은 이러한 청취의 불편함을 줄이고자 많은 사람들과 대화할 때 배우자 또는 가까운 지인을 난청이 있는 귀 방향에 앉도록 하여본인이 듣지 못한 단어를 반복하여 말해달라고 하거나제대로 이해하였는지 확인하는 과정을 통해 도움을 받기도 합니다.  또는 하루 업무를 마치고 직장 동료들과 함께하는 자리에 초대를 받아도, 소음 때문에 듣는 어려움을 밝히지 않고 단지 ‘피곤하다’는 이유로 자리를 피하기도 한다고 하는데요.  이처럼 편측성 난청을 겪고 있으면일상생활에서 불편한 점들이 정말 많습니다.  하지만 이러한 불편한 점들은 포낙의 최신 기술들을 통해 해결할 수 있다고 하는데요,그 기술과 효과에 대해 살펴보겠습니다. ​​  ​ 크로스 P를 활용하여 일상생활 되찾기!   ​​먼저 포낙보청기의 크로스(CROS) 솔루션은Contralateral Routing of Signal의 약자로,편측성 난청인을 위한 스마트한 솔루션입니다.  청력이 나쁜 쪽 귀에는 크로스 P를,청력이 좋은 쪽 귀에는 오데오 P를 착용하여청력이 떨어지는 방향의 소리를 반대쪽의 보청기로 전달해주는 원리인데요. ​미국에서 포낙보청기의 임상 연구를 진행하는 브랜디 헤크룩(Brandy Heckroodt)은 편측성 난청인들에게 직접 크로스 P 착용 경험과 관련하여 인터뷰를 진행하였습니다.   이를 통해 크로스 P 착용자들로부터 일상생활에 큰 도움을 준 3가지 장점을 알 수 있었다고 하는데요.하나씩 알아보도록 하겠습니다!  ①“양쪽 귀 모두에서 소리에 대한 인지능력이 향상되었어요.”    ​ 크로스 P는 난청이 있는 귀 방향에서 들려오는 소리를 반대쪽 귀에 있는 보청기로 직접 전달해줍니다.  따라서 머리를 지나가며 소리가 작아지는 두영 효과 때문에 잘 듣지 못했던 소리도 더 명확하게 들을 수 있으며, 소리가 들려온 방향도 함께 알 수 있습니다.     실제로 크로스 P 피팅 후 소리를 좋은 쪽 귀에서 나쁜 쪽 귀로 옮겨가며 들려주었을 때,보청기 착용자는 두영 효과로 인한 방해도 받지 않고말하는 사람 쪽으로 좋은 쪽 귀를 돌리지 않아도 편안하게 소리를 들을 수 있었습니다.  또한, 크로스 P를 처음 착용하는 상황에서도 금방 소리에 적응하여 말하는 사람이 방 안 어디에 있든착용자는 큰 청취 노력 없이도 편안하게 대화를 할 수 있었다고 합니다.  ②“소음 속에서도 말소리 인지도가 향상되었어요.”  주변 소음으로 인해 운동하면서 대화하는 것이 어려웠던 고객은포낙 보청기의 고유 기술인 양이음성 스트리밍 기술과 스테레오줌 기술이 내장된 크로스 P 착용을 통해 더 자신있게 사람들과 함께 어울릴 수 있었다고 합니다.   ​ 양이음성 스트리밍 기술은착용한 크로스 제품과 보청기에 내장된 총 4개의 마이크가네트워크를 구축하여 실시간으로 소리 신호를 주고받는포낙의 음성통신 기술입니다.   ​ 스테레오줌 기술은 방향성 마이크로폰과 방향성 소음제거가 접목된 기능으로,소리가 들리는 방향을 세분화하여 소음을 제거해주기 때문에특히 소음이 심한 환경에서 듣고자 하는전방의 화자 목소리에만 집중할 수 있도록 도와줍니다.  위의 두 가지 기술을 통해 크로스 착용자는소음 속 말소리 인지 능력이 향상​되어청취 노력 및 피로도가 줄어들었으며,동시에 가족 및 가까운 사람들도더욱 편하게 의사소통을 할 수 있었다고 합니다.  아무래도 난청을 가진 사람도 대화 시 어려움이 많지만,주변 사람들도 난청인과의 대화에 불편한 점이 생기게 되는데요. 크로스 P를 착용하면 이러한 상황을 해결하는 데에도 도움이 되겠죠? 😊  ③“핸즈-프리 통화와 오디오 스트리밍을 할 수 있어요.”  바쁜 일상을 살다보면 핸드폰을 늘 가까운 곳에 두지 못하는 경우도 생기는데요.거실에 핸드폰을 두고 방에서 일을 하거나 양손으로 무언가를 하는 중에 전화가 오는 경우도 있죠.     이러한 경우, 크로스 P는 iOS, 안드로이드, 키즈폰, 효도폰 등 기종 상관없이블루투스를 지원하는 모든 전자기기와 다이렉트 연결되어 핸즈-프리 통화를 지원하므로 일상에서의 편리함을 느끼게 해줍니다.   크로스 P 착용자의 말에 따르면,  운동할 때 노래를 듣거나 다른 일을 하면서 핸드폰을 손에 들지 않아도동시에 통화를 하는 것이크로스 P를 통해 매우 편리해졌어요.라고 하며, 강한 만족감을 드러냈다고 합니다.  또한, 편측성 난청을 가진 청소년은  핸드폰과 노트북 등 자주 사용하는 전자기기를 크로스 P와 연결하여 음악, 유튜브, 게임 등 다양한 오디오 음원을크로스 P로 직접 듣게 되어 매우 만족스럽습니다.라고 하며, 매일 크로스를 착용한다고 하였습니다.     ​ ​ 이처럼 난청인들이 크로스 P를 통해 일상생활에 편안함과 편리함을 동시에 누리며 살아가는 모습을 보면서 포낙보청기는 뿌듯함을 느끼게 되는데요.  앞으로도 포낙의 크로스 솔루션을 통해 의사소통에 도움을 줄 뿐만 아니라더욱 편하게 취미생활을 즐기며난청인들이 웰빙을 누릴 수 있도록포낙보청기는 최선을 다하겠습니다. 😊  듣는 즐거움이 가득한 일상, 포낙이 함께 합니다!​       본 포스팅과 관련된 본문은 아래 링크에서 확인하실 수 있습니다.​ At least 3 ways Phonak CROS P is improving lives - Phonak Audiology Blog - Phonak Pro - life is onDr. Brandy Heckroodt sees first-hand how Phonak’s solution for unilateral hearing loss can change lives. Here are some of those stories.audiologyblog.phonakpro.com  References​1. Prieve, B., Dalzell, L., Berg, A., Bradley, M., Cacace, A., Campbell, D., DeCristofaro, J., Gravel, J., Greenberg, E., Gross, S., Orlando, M., Pinheiro, J., Regan, J., Spivak, L., and Stevens, F. (2000).The New York State universal newborn hearing screening demonstration project: Outpatient outcome measures. Ear and Hearing, 21 (2). 2. Bess, F.H., Dodd-Murphy, J. & Parker, R.A. (1998). Children with minimal sensorineural hearing loss: Prevalence, educational performance, and functional status. Ear and Hearing, 9, 339–354. 3. Golub, J., Lin, F., Lustig, L., & Lalwani, A. (2018). Prevalence of adult unilateral hearing loss and hearing aid use in the United States. The Laryngoscope. 128 (7), 1681-1686Snapp H. A., Hoffer M. E., Liu X., Rajguru S. M. (2017a). Effectiveness in Rehabilitation of Current Wireless CROS Technology in Experienced Bone‐Anchored Implant Users. Otol Neurotol. 38 (10): 1397‐1404. 4. Leterme G., Bernardeschi D., Bensemman A., Coudert C., Portal J.J., Ferrary E., Sterkers O., Vicaut E., Frachet B., Bozorg Grayeli A. (2015). Contralateral routing of signal hearing aid versus transcutaneous bone conduction in single-sided deafness. Audiology and Neurotology. 20 (4):251-60. 5. Picou E. M., Davis H., Lewis D., Tharpe A.M. (2020). Contralateral Routing of Signal Systems Can Improve Speech Recognition and Comprehension in Dynamic Classrooms. Journal of Speech Language and Hearing Research. 63 (7) : 2468‐2482. 6. Lieu J.E., Karzon R.K., Ead B., Tye-Murray N. (2013.) Do audiologic characteristics predict outcomes in children with unilateral hearing loss? Otol Neurotol 34:1703–1710.  7. McKay, S. (2010). Audiological Management of children with single-sided deafness. Seminars in Hearing Vol. 31 (4). "
Wav2vec 2.0 ,https://blog.naver.com/qhruddl51/222590931800,20211209,"VQ-Wav2vec과의 차이점 1.  MLM과 CPC를 함께 적용해서 end-to-end 학습으로 음성에 대한 feature vector를 구함기존의 RNN aggregator와 BERT를 Transformer로 대체해 MLM과 CPC를 동시에 수행함 ​2. Quantization 에 대한 제약을 추가함.배치마다의 코드북 선택 비율을 기반으로 diversity loss를 계산한다. codebook​ ​=> zt가 음성 데이터의 feature vector이다.​  Reference​● Wav2vec 2.0 논문 https://arxiv.org/pdf/2006.11477.pdf​● https://neurosys.com/wav2vec-2-0-framework/ ​● 고려대학교 산업경영공학부 DSBA 연구실 석사과정 김정희, 음성인식(ASR)에 Semi-Supervised Learning을 적용한 5가지 논문을 리뷰https://www.youtube.com/watch?v=Z1lSukzyA0E&ab_channel=%EA%B3%A0%EB%A0%A4%EB%8C%80%ED%95%99%EA%B5%90%EC%82%B0%EC%97%85%EA%B2%BD%EC%98%81%EA%B3%B5%ED%95%99%EB%B6%80DSBA%EC%97%B0%EA%B5%AC%EC%8B%A4 ● https://github.com/eastonYi/wav2vec/tree/5411474a80136b6835c04e5b3bca0f4098f90712 GitHub - eastonYi/wav2vec at 5411474a80136b6835c04e5b3bca0f4098f90712a simplified version of wav2vec(1.0, vq, 2.0) in fairseq - GitHub - eastonYi/wav2vec at 5411474a80136b6835c04e5b3bca0f4098f90712github.com ● https://donghwa-kim.github.io/pretrain_wav2.html Fairseq 코드리뷰 Wav2vec 2.0 (Pretrain)Fairseq의 Wav2vec 2.0 Pretrain 실행방법 Fairseq의 제공하는 Wav2vec 2.0 모델의 작동과정을 소개하고자 합니다. 본 게시글은 아래의 github를 참고하였습니다. https://github.com/pytorch/fairseq.git https://github.com/mailong25/self-supervised-speech-recognition.git Pretraining using self-supervised learning hydra_train.py 를 이용해 pretrain이 진행할 수 있습니...donghwa-kim.github.io ● https://colab.research.google.com/github/amoghgopadi/wav2vec2-xlsr-kannada/blob/main/Fine_Tune_XLSR_Wav2Vec2_on_Kannada_ASR.ipynb#scrollTo=xwGUuB6Jz-gz Fine_Tune_XLSR_Wav2Vec2_on_Kannada_ASR.ipynbRun, share, and edit Python notebookscolab.research.google.com ● https://pytorch.org/tutorials/intermediate/speech_recognition_pipeline_tutorial.html Speech Recognition with Wav2Vec2 — PyTorch Tutorials 1.11.0+cu102 documentationExtract the acoustic features from audio waveform Estimate the class of the acoustic features frame-by-frame Generate hypothesis from the sequence of the class probabilities Torchaudio provides easy access to the pre-trained weights and associated information, such as the expected sample rate and cl...pytorch.org "
"[뉴스] 마이크로소프트(MSFT), 18조원에 Siri 개발한 AI 기업 '뉘앙스' 인수 협상 ",https://blog.naver.com/vd159/222307103760,20210412,"마이크로소프트(MS)가 인공지능·음성 기술 회사인 뉘앙스 커뮤니케이션을 160억달러(17조 9360억원)에 인수하기 위해 사전 협상을 진행 중이라고 로이터통신이 11일(현지 시각) 보도했다.​로이터는 소식통을 인용해 ""이번 인수 협상은 MS가 뉘앙스의 주식을 주당 약 56달러에 사들이는 방식으로 진행 중""이라며 ""이르면 월요일에 12일(현지 시각) 합의가 발표될 수 있을 것""이라고 전했다.​MS의 뉘앙스 인수를 처음으로 보도한 블룸버그는 해당 협상이 여전히 진행 중이며 결렬될 가능성도 있다고 보도했다.​뉘앙스는 애플이 AI 음성 비서 기술인 시리(Siri)를 내놓는데 기술적 도움을 준 기업으로 알려졌으며 헬스케어, 자동차용 AI 등 다양한 분야에서 관련 소프트웨어를 개발하고 있다.​이번 인수합병이 성사될 경우 지난 2016년 링크드인 인수 이후 두 번째로 큰 거래가 될 전망이다. 다만 MS와 뉘앙스는 해당 보도에 대해 공식 답변을 내놓지 않았다. 마이크로소프트, 18조원에 AI 기업 ‘뉘앙스’ 인수 협상마이크로소프트(MS)가 인공지능·음성 기술 회사인 뉘앙스 커뮤니케이션을 160억달러(17조 9360억원)에 인수하기 위해 사전 협상을 진행 중이..biz.chosun.com  원본 내용​Microsoft in talks to acquire Siri speech recognition partner Nuance for $16B​Nuance Communications is allegedly in discussions with Microsoft over a potential acquisition of the voice recognition company, one that could value a technology provider behind voice analysis for Apple's Siri at approximately $16 billion.​Discussions between the two companies are ongoing, but could result in an announcement for a deal in the next week, according to people familiar with the talks. The proposed price for Nuance is said to be roughly $56 per share, a 23% premium on Friday's closing price.​The talks, told to Bloomberg by unnamed sources, could have major implications for Apple, due to Nuance's involvement with Siri. Apple used Nuance's voice recognition engine to enable its digital assistant to hear and understand user requests.​A Microsoft spokesperson declined to comment on the report, while a spokesperson for Nuance didn't immediately respond to queries.​An acquisition of Nuance would be Microsoft's second-largest purchase. Its largest is the acquisition of LinkedIn in 2016, a deal valued at $24 billion.​If a purchase is confirmed, it is unclear how this would affect Apple and Siri in the immediate future. While Nuance confirmed in 2013 its technology was ""the fundamental provider of voice recognition for Apple,"" it is unknown how the relationship between the two companies over Siri operates in 2021.​An acquisition may even press Apple into investing more into voice recognition research, which could allow Siri to work without any dependence on third-party service providers.​This is not the first time the Burlington-based Nuance considered a sale. In 2014, reports claimed it was in talks with Samsung for a potential acquisition, though the South Korean giant didn't follow through.​In 2013, investors and industry watchers proposed Apple's reliance on Nuance could have been a good reason for it to acquire the company itself. At the time, it was thought Apple had a chance to do so for $7 billion, less than half the current anticipated purchase price. Microsoft in talks to acquire Siri speech recognition partner Nuance for $16B | AppleInsiderNuance Communications is allegedly in discussions with Microsoft over a potential acquisition of the voice recognition company, one that could value a technology provider behind voice analysis for Apple's Siri at approximately $16 billion.appleinsider.com ​ "
Deep learning ,https://blog.naver.com/joayo21/222668679344,20220310,"​Deep learning (also known as deep structured learning) is part of a broader family of machine learning methods based on artificial neural networks with representation learning. Learning can be supervised, semi-supervised or unsupervised.[2]Deep-learning architectures such as deep neural networks, deep belief networks, deep reinforcement learning, recurrent neural networks and convolutional neural networks have been applied to fields including computer vision, speech recognition, natural language processing, machine translation, bioinformatics, drug design, medical image analysis, climate science, material inspection and board game programs, where they have produced results comparable to and in some cases surpassing human expert performance.[3][4][5]Artificial neural networks (ANNs) were inspired by information processing and distributed communication nodes in biological systems. ANNs have various differences from biological brains. Specifically, artificial neural networks tend to be static and symbolic, while the biological brain of most living organisms is dynamic (plastic) and analogue.[6][7]The adjective ""deep"" in deep learning refers to the use of multiple layers in the network. Early work showed that a linear perceptron cannot be a universal classifier, but that a network with a nonpolynomial activation function with one hidden layer of unbounded width can. Deep learning is a modern variation which is concerned with an unbounded number of layers of bounded size, which permits practical application and optimized implementation, while retaining theoretical universality under mild conditions. In deep learning the layers are also permitted to be heterogeneous and to deviate widely from biologically informed connectionist models, for the sake of efficiency, trainability and understandability, whence the ""structured"" part.딥 러닝​딥 러닝(Deep Learning, 심층 구조 학습이라고도 함)은 표현 학습과 함께 인공 신경망을 기반으로 하는 광범위한 기계 학습 방법의 일부이다. 학습은 감독되거나, 반감독되거나, 감독되지 않을 수 있다.[2]​컴퓨터 비전, 음성 인식, 자연어 처리, 기계 번역, 생물정보학, 약물 설계, 의료 이미지 분석, cli를 포함한 분야에 심층 신경망, 심층 강화 학습, 반복 신경망, 컨볼루션 신경망과 같은 딥 러닝 아키텍처가 적용되었다.짝짓기 과학, 재료 검사 및 보드 게임 프로그램으로, 인간 전문가의 성과에 필적하거나 경우에 따라서는 능가하는 결과를 도출했다.[3][4][5]​인공신경망(ANN)은 생물학적 시스템의 정보 처리 및 분산 통신 노드에서 영감을 받았다. ANN은 생물학적 뇌와 다양한 차이점이 있다. 특히, 인공 신경망은 정적이고 상징적인 경향이 있는 반면, 대부분의 살아있는 유기체의 생물학적 뇌는 역동적이고 유사하다.[6][7]​딥러닝에서 ""딥""이라는 형용사는 네트워크에서 여러 계층을 사용하는 것을 의미한다. 초기 연구는 선형 퍼셉트론이 범용 분류기가 될 수 없지만, 무한 폭의 숨겨진 층을 가진 비다항 활성화 함수를 가진 네트워크는 할 수 있다는 것을 보여주었다. 딥 러닝은 경미한 조건에서 이론적 보편성을 유지하면서 실용적인 적용과 최적화된 구현을 허용하는 무한한 수의 경계 크기 레이어와 관련된 현대적인 변형이다. 딥러닝에서 계층은 또한 ""구조화된"" 부분에 따라 효율성, 훈련성 및 이해성을 위해 이기종이고 생물학적으로 알려진 연결주의 모델에서 크게 벗어날 수 있다.​Contents1Definition2Overview3Interpretations4History4.1Deep learning revolution5Neural networks5.1Artificial neural networks5.2Deep neural networks6Hardware7Applications7.1Automatic speech recognition7.2Image recognition7.3Visual art processing7.4Natural language processing7.5Drug discovery and toxicology7.6Customer relationship management7.7Recommendation systems7.8Bioinformatics7.9Medical image analysis7.10Mobile advertising7.11Image restoration7.12Financial fraud detection7.13Military7.14Partial differential equations8Relation to human cognitive and brain development9Commercial activity10Criticism and comment10.1Theory10.2Errors10.3Cyber threat10.4Reliance on human microwork11See also12References13Further reading내용물1 정의2 개요3 해석4개의 역사4.1 딥러닝 혁신5개의 신경망5.1 인공신경망5.2 심층신경망6 하드웨어7가지 응용 프로그램7.1 자동 음성 인식7.2 영상인식7.3 시각 예술 처리7.4 자연어 처리7.5 의약품 발견 및 독성학7.6 고객관계 관리7.7 권장 시스템7.8 생물정보학7.9 의료영상분석7.10 모바일 광고7.11 이미지 복원7.12 금융 사기 탐지7.13 군사7.14 편미분방정식8 인지 및 뇌 발달과의 관계9 상업활동10 비평 및 의견10.1 이론10.2 오류10.3 사이버 위협10.4 전자레인지에 대한 의존도11 참고 항목12 참조 자료13 추가 정보​DefinitionDeep learning is a class of machine learning algorithms that[8]: 199–200  uses multiple layers to progressively extract higher-level features from the raw input. For example, in image processing, lower layers may identify edges, while higher layers may identify the concepts relevant to a human such as digits or letters or faces.정의.딥 러닝은 기계 학습 알고리즘의 한 종류로, [8]: 199–200은 다중 레이어를 사용하여 원시 입력에서 점차적으로 더 높은 수준의 특징을 추출한다. 예를 들어, 이미지 처리에서 하위 계층은 가장자리를 식별하는 반면 상위 계층은 숫자, 문자 또는 얼굴과 같은 인간과 관련된 개념을 식별할 수 있다.​OverviewMost modern deep learning models are based on artificial neural networks, specifically convolutional neural networks (CNN)s, although they can also include propositional formulas or latent variables organized layer-wise in deep generative models such as the nodes in deep belief networks and deep Boltzmann machines.[9]In deep learning, each level learns to transform its input data into a slightly more abstract and composite representation. In an image recognition application, the raw input may be a matrix of pixels; the first representational layer may abstract the pixels and encode edges; the second layer may compose and encode arrangements of edges; the third layer may encode a nose and eyes; and the fourth layer may recognize that the image contains a face. Importantly, a deep learning process can learn which features to optimally place in which level on its own. This does not eliminate the need for hand-tuning; for example, varying numbers of layers and layer sizes can provide different degrees of abstraction.[10][11]The word ""deep"" in ""deep learning"" refers to the number of layers through which the data is transformed. More precisely, deep learning systems have a substantial credit assignment path (CAP) depth. The CAP is the chain of transformations from input to output. CAPs describe potentially causal connections between input and output. For a feedforward neural network, the depth of the CAPs is that of the network and is the number of hidden layers plus one (as the output layer is also parameterized). For recurrent neural networks, in which a signal may propagate through a layer more than once, the CAP depth is potentially unlimited.[12] No universally agreed-upon threshold of depth divides shallow learning from deep learning, but most researchers agree that deep learning involves CAP depth higher than 2. CAP of depth 2 has been shown to be a universal approximator in the sense that it can emulate any function.[13] Beyond that, more layers do not add to the function approximator ability of the network. Deep models (CAP > 2) are able to extract better features than shallow models and hence, extra layers help in learning the features effectively.Deep learning architectures can be constructed with a greedy layer-by-layer method.[14] Deep learning helps to disentangle these abstractions and pick out which features improve performance.[10]For supervised learning tasks, deep learning methods eliminate feature engineering, by translating the data into compact intermediate representations akin to principal components, and derive layered structures that remove redundancy in representation.Deep learning algorithms can be applied to unsupervised learning tasks. This is an important benefit because unlabeled data are more abundant than the labeled data. Examples of deep structures that can be trained in an unsupervised manner are deep belief networks.[10][15]개요​대부분의 현대 딥러닝 모델은 인공 신경망, 특히 컨볼루션 신경망(CNN)을 기반으로 하지만 딥 신뢰 네트워크 및 딥 볼츠만 기계의 노드 같은 심층 생성 모델에 제안 공식 또는 계층별로 구성된 잠재 변수를 포함할 수도 있다.[9]​딥 러닝에서 각 레벨은 입력 데이터를 약간 더 추상적이고 복합적인 표현으로 변환하는 방법을 배운다. 이미지 인식 응용 프로그램에서 원시 입력은 픽셀 매트릭스일 수 있고, 첫 번째 표현 레이어는 픽셀을 추상화하고 가장자리를 인코딩할 수 있으며, 두 번째 레이어는 가장자리의 배열을 구성하고 인코딩할 수 있으며, 세 번째 레이어는 코와 눈을 인코딩할 수 있으며, 네 번째 레이어는 이미지가 얼굴을 포함하고 있음을 인식할 수 있다. 중요한 것은, 딥 러닝 과정이 어떤 특징을 최적으로 스스로 배치할 수 있는지 배울 수 있다는 것이다. 이렇게 해도 수동 조정의 필요성이 사라지지 않습니다. 예를 들어, 레이어 수와 레이어 크기가 달라지면 추상화의 정도가 달라질 수 있습니다.[10][11]​딥러닝에서 ""딥""이라는 단어는 데이터가 변환되는 레이어의 수를 나타냅니다. 더 정확히 말하면, 딥 러닝 시스템은 상당한 신용 할당 경로(CAP) 깊이를 가지고 있다. CAP는 입력에서 출력으로 변환하는 연쇄입니다. CAP는 입력과 출력 사이의 잠재적인 인과 관계를 설명한다. 피드포워드 신경망의 경우, CAP의 깊이는 네트워크의 깊이이며 은닉 레이어의 수에 1을 더한 값이다. 신호가 레이어를 통해 두 번 이상 전파될 수 있는 반복 신경망의 경우, CAP 깊이는 잠재적으로 무제한이다.[12] 깊이 임계값에 대해 보편적으로 합의된 딥 러닝의 얕은 학습을 분할하는 것은 없지만, 대부분의 연구자들은 딥 러닝이 2보다 높은 CAP 깊이를 수반한다는 데 동의한다. 깊이 2의 CAP는 모든 기능을 모방할 수 있다는 점에서 보편적인 근사치로 나타났다.[13] 그 외에도, 더 많은 계층들이 네트워크의 함수 근사기 능력에 추가되지 않습니다. 딥 모델(CAP > 2)은 얕은 모델보다 더 나은 형상을 추출할 수 있으므로 추가 레이어는 형상을 효과적으로 학습하는 데 도움이 된다.​딥 러닝 아키텍처는 탐욕스러운 계층별 방법으로 구성될 수 있다.[14] 딥 러닝은 이러한 추상화를 분리하고 성능을 향상시키는 기능을 선택하는 데 도움이 됩니다.[10]​지도 학습 작업의 경우, 딥 러닝 방법은 데이터를 주요 구성 요소와 유사한 소형 중간 표현으로 변환하여 피처 엔지니어링을 제거하고 표현의 중복성을 제거하는 계층 구조를 도출한다.​딥러닝 알고리즘은 비지도 학습 과제에 적용할 수 있다. 이것은 레이블이 지정되지 않은 데이터가 레이블이 지정된 데이터보다 더 풍부하기 때문에 중요한 이점입니다. 감독되지 않은 방식으로 훈련될 수 있는 심층 구조의 예는 심층 믿음 네트워크이다.[10][15]​InterpretationsDeep neural networks are generally interpreted in terms of the universal approximation theorem[16][17][18][19][20] or probabilistic inference.[8][9][10][12][21]The classic universal approximation theorem concerns the capacity of feedforward neural networks with a single hidden layer of finite size to approximate continuous functions.[16][17][18][19] In 1989, the first proof was published by George Cybenko for sigmoid activation functions[16] and was generalised to feed-forward multi-layer architectures in 1991 by Kurt Hornik.[17] Recent work also showed that universal approximation also holds for non-bounded activation functions such as the rectified linear unit.[22]The universal approximation theorem for deep neural networks concerns the capacity of networks with bounded width but the depth is allowed to grow. Lu et al.[20] proved that if the width of a deep neural network with ReLU activation is strictly larger than the input dimension, then the network can approximate any Lebesgue integrable function; If the width is smaller or equal to the input dimension, then a deep neural network is not a universal approximator.The probabilistic interpretation[21] derives from the field of machine learning. It features inference,[8][9][10][12][15][21] as well as the optimization concepts of training and testing, related to fitting and generalization, respectively. More specifically, the probabilistic interpretation considers the activation nonlinearity as a cumulative distribution function.[21] The probabilistic interpretation led to the introduction of dropout as regularizer in neural networks.[23] The probabilistic interpretation was introduced by researchers including Hopfield, Widrow and Narendra and popularized in surveys such as the one by Bishop.[24]해석​심층 신경망은 일반적으로 보편적 근사 정리[16][17][18][19][20] 또는 확률론적 추론을 통해 해석된다.[8][9][10][12][21]​고전적인 보편 근사 정리는 크기가 유한한 단일 은닉 레이어를 가진 피드포워드 신경망의 용량과 관련이 있다.[16][17][18][19] 1989년에 시그모이드 활성화 함수에 대한 첫 번째 증명은 조지 사이벤코에 의해 발표되었고[16] 1991년에 커트 호닉에 의해 피드포워드 다층 구조로 일반화되었다.[17] 최근 연구는 또한 보편적 근사가 정류된 선형 단위와 같은 비경계 활성화 함수에 대해서도 유효하다는 것을 보여주었다.[22]​심층 신경망에 대한 보편적 근사 정리는 경계 폭을 가진 네트워크의 용량과 관련이 있지만 깊이는 증가할 수 있다. 루 외 연구진.[20] ReLU 활성화가 있는 심층 신경망의 너비가 입력 치수보다 엄격하게 크면 네트워크는 르베그 통합 가능 함수를 근사할 수 있다. 폭이 입력 치수보다 작거나 같다면 심층 신경망은 범용 근사기가 아니다.​확률론적 해석[21]은 기계 학습 분야에서 파생된다. 그것은 각각 적합과 일반화와 관련된 훈련과 테스트의 최적화 개념뿐만 아니라 추론[8][9][10][12][15][21]을 특징으로 한다. 보다 구체적으로, 확률론적 해석에서는 활성화 비선형성을 누적분포함수로 간주한다.[21] 확률론적 해석으로 인해 신경망에 정규화기로 드롭아웃이 도입되었다.[23] 확률론적 해석은 홉필드, 위드로, 나렌드라 등 연구자에 의해 도입됐으며 비숍의 해석과 같은 조사에서 대중화됐다.[24]​HistorySome sources point out that Frank Rosenblatt developed and explored all of the basic ingredients of the deep learning systems of today.[25] He described it in his book ""Principles of Neurodynamics: Perceptrons and the Theory of Brain Mechanisms"", published by Cornell Aeronautical Laboratory, Inc., Cornell University in 1962.The first general, working learning algorithm for supervised, deep, feedforward, multilayer perceptrons was published by Alexey Ivakhnenko and Lapa in 1967.[26] A 1971 paper described a deep network with eight layers trained by the group method of data handling.[27] Other deep learning working architectures, specifically those built for computer vision, began with the Neocognitron introduced by Kunihiko Fukushima in 1980.[28]The term Deep Learning was introduced to the machine learning community by Rina Dechter in 1986,[29] and to artificial neural networks by Igor Aizenberg and colleagues in 2000, in the context of Boolean threshold neurons.[30][31]In 1989, Yann LeCun et al. applied the standard backpropagation algorithm, which had been around as the reverse mode of automatic differentiation since 1970,[32][33][34][35] to a deep neural network with the purpose of recognizing handwritten ZIP codes on mail. While the algorithm worked, training required 3 days.[36]In 1994, André de Carvalho, together with Mike Fairhurst and David Bisset, published experimental results of a multi-layer boolean neural network, also known as a weightless neural network, composed of a 3-layers self-organising feature extraction neural network module (SOFT) followed by a multi-layer classification neural network module (GSN), which were independently trained. Each layer in the feature extraction module extracted features with growing complexity regarding the previous layer.[37]In 1995, Brendan Frey demonstrated that it was possible to train (over two days) a network containing six fully connected layers and several hundred hidden units using the wake-sleep algorithm, co-developed with Peter Dayan and Hinton.[38] Many factors contribute to the slow speed, including the vanishing gradient problem analyzed in 1991 by Sepp Hochreiter.[39][40]Since 1997, Sven Behnke extended the feed-forward hierarchical convolutional approach in the Neural Abstraction Pyramid[41] by lateral and backward connections in order to flexibly incorporate context into decisions and iteratively resolve local ambiguities.Simpler models that use task-specific handcrafted features such as Gabor filters and support vector machines (SVMs) were a popular choice in the 1990s and 2000s, because of artificial neural network's (ANN) computational cost and a lack of understanding of how the brain wires its biological networks.Both shallow and deep learning (e.g., recurrent nets) of ANNs have been explored for many years.[42][43][44] These methods never outperformed non-uniform internal-handcrafting Gaussian mixture model/Hidden Markov model (GMM-HMM) technology based on generative models of speech trained discriminatively.[45] Key difficulties have been analyzed, including gradient diminishing[39] and weak temporal correlation structure in neural predictive models.[46][47] Additional difficulties were the lack of training data and limited computing power.Most speech recognition researchers moved away from neural nets to pursue generative modeling. An exception was at SRI International in the late 1990s. Funded by the US government's NSA and DARPA, SRI studied deep neural networks in speech and speaker recognition. The speaker recognition team led by Larry Heck reported significant success with deep neural networks in speech processing in the 1998 National Institute of Standards and Technology Speaker Recognition evaluation.[48] The SRI deep neural network was then deployed in the Nuance Verifier, representing the first major industrial application of deep learning.[49]The principle of elevating ""raw"" features over hand-crafted optimization was first explored successfully in the architecture of deep autoencoder on the ""raw"" spectrogram or linear filter-bank features in the late 1990s,[49] showing its superiority over the Mel-Cepstral features that contain stages of fixed transformation from spectrograms. The raw features of speech, waveforms, later produced excellent larger-scale results.[50]Many aspects of speech recognition were taken over by a deep learning method called long short-term memory (LSTM), a recurrent neural network published by Hochreiter and Schmidhuber in 1997.[51] LSTM RNNs avoid the vanishing gradient problem and can learn ""Very Deep Learning"" tasks[12] that require memories of events that happened thousands of discrete time steps before, which is important for speech. In 2003, LSTM started to become competitive with traditional speech recognizers on certain tasks.[52] Later it was combined with connectionist temporal classification (CTC)[53] in stacks of LSTM RNNs.[54] In 2015, Google's speech recognition reportedly experienced a dramatic performance jump of 49% through CTC-trained LSTM, which they made available through Google Voice Search.[55]In 2006, publications by Geoff Hinton, Ruslan Salakhutdinov, Osindero and Teh[56] [57][58] showed how a many-layered feedforward neural network could be effectively pre-trained one layer at a time, treating each layer in turn as an unsupervised restricted Boltzmann machine, then fine-tuning it using supervised backpropagation.[59] The papers referred to learning for deep belief nets.Deep learning is part of state-of-the-art systems in various disciplines, particularly computer vision and automatic speech recognition (ASR). Results on commonly used evaluation sets such as TIMIT (ASR) and MNIST (image classification), as well as a range of large-vocabulary speech recognition tasks have steadily improved.[60][61][62][63] Convolutional neural networks (CNNs) were superseded for ASR by CTC[53] for LSTM.[51][55][64][65][66][67][68] but are more successful in computer vision.The impact of deep learning in industry began in the early 2000s, when CNNs already processed an estimated 10% to 20% of all the checks written in the US, according to Yann LeCun.[69] Industrial applications of deep learning to large-scale speech recognition started around 2010.The 2009 NIPS Workshop on Deep Learning for Speech Recognition[70] was motivated by the limitations of deep generative models of speech, and the possibility that given more capable hardware and large-scale data sets that deep neural nets (DNN) might become practical. It was believed that pre-training DNNs using generative models of deep belief nets (DBN) would overcome the main difficulties of neural nets.[71] However, it was discovered that replacing pre-training with large amounts of training data for straightforward backpropagation when using DNNs with large, context-dependent output layers produced error rates dramatically lower than then-state-of-the-art Gaussian mixture model (GMM)/Hidden Markov Model (HMM) and also than more-advanced generative model-based systems.[60][72] The nature of the recognition errors produced by the two types of systems was characteristically different,[73][70] offering technical insights into how to integrate deep learning into the existing highly efficient, run-time speech decoding system deployed by all major speech recognition systems.[8][74][75] Analysis around 2009–2010, contrasting the GMM (and other generative speech models) vs. DNN models, stimulated early industrial investment in deep learning for speech recognition,[73][70] eventually leading to pervasive and dominant use in that industry. That analysis was done with comparable performance (less than 1.5% in error rate) between discriminative DNNs and generative models.[60][73][71][76]In 2010, researchers extended deep learning from TIMIT to large vocabulary speech recognition, by adopting large output layers of the DNN based on context-dependent HMM states constructed by decision trees.[77][78][79][74]Advances in hardware have driven renewed interest in deep learning. In 2009, Nvidia was involved in what was called the “big bang” of deep learning, “as deep-learning neural networks were trained with Nvidia graphics processing units (GPUs).”[80] That year, Andrew Ng determined that GPUs could increase the speed of deep-learning systems by about 100 times.[81] In particular, GPUs are well-suited for the matrix/vector computations involved in machine learning.[82][83][84] GPUs speed up training algorithms by orders of magnitude, reducing running times from weeks to days.[85][86] Further, specialized hardware and algorithm optimizations can be used for efficient processing of deep learning models.[87]역사일부 출처는 프랭크 로젠블랫이 오늘날 딥러닝 시스템의 모든 기본 요소들을 개발하고 탐구했다고 지적한다.그는 그의 저서 ""신경역학의 원리: 1962년 코넬 대학교 코넬 항공 연구소가 출판한 퍼셉트론과 뇌 메커니즘 이론.​감독되고, 깊고, 피드포워드, 다층 퍼셉트론을 위한 최초의 일반 작동 학습 알고리즘은 알렉세이 이바크넨코와 라파에 의해 1967년에 출판되었다.[26] 1971년 논문은 데이터 처리의 그룹 방법에 의해 훈련된 8개의 계층이 있는 심층 네트워크를 설명하였다.[27] 다른 딥러닝 작업 아키텍처, 특히 컴퓨터 비전을 위해 제작된 아키텍처들은 1980년 후쿠시마 쿠니히코에 의해 소개된 네오그니트론과 함께 시작되었다.[28]​딥러닝이라는 용어는 1986년,[29] 리나 데크터에 의해 기계 학습 커뮤니티에 소개되었고, 2000년에 이고르 아이젠버그와 동료들에 의해 불 문턱 뉴런의 맥락에서 인공 신경망에 도입되었다.[30][31]​1989년 Yann LeCun 등은 우편에서 손으로 쓴 ZIP 코드를 인식할 목적으로 1970년부터 자동 분화의 역모드로 존재해 온 표준 역전파 알고리즘을 심층 신경망에 적용했다[32][33][34][35]. 알고리즘이 작동하는 동안, 훈련은 3일이 걸렸습니다.[36]​1994년, 안드레 드 카르발류는 마이크 페어허스트, 데이비드 비셋과 함께 3층 자가 조직적 특징 추출 신경망 모듈(SOFT)과 다층 분류 신경망 모듈(GSN)로 구성된 다층 부울 신경망의 실험 결과를 발표했다.ch는 독립적으로 훈련되었다. 형상 추출 모듈의 각 계층은 이전 계층과 관련하여 복잡성이 증가하는 특징을 추출했다.[37]​1995년, 브렌던 프레이는 피터 다얀, 힌튼과 함께 개발한 웨이크 슬립 알고리즘을 사용하여 완전히 연결된 6개의 레이어와 수백 개의 숨겨진 유닛을 포함하는 네트워크를 이틀 이상 훈련시키는 것이 가능하다는 것을 증명하였다.[38] Sepp Hochreiter가 1991년에 분석한 소멸 기울기 문제를 포함하여 많은 요인들이 느린 속도의 원인이 된다.[39][40]​1997년 이후, 스벤 벤케는 결정에 맥락을 유연하게 통합하고 국부적 모호성을 반복적으로 해결하기 위해 측면 및 후진 연결로 신경 추상화 피라미드의 피드-포워드 계층적 컨볼루션 접근법[41]을 확장했다.​가보 필터와 서포트 벡터 머신(SVM)과 같은 작업별 수공예 기능을 사용하는 단순한 모델은 인공신경망의 계산 비용과 뇌가 생물학적 네트워크를 배선하는 방법에 대한 이해 부족으로 인해 1990년대와 2000년대에 인기 있는 선택이었다.​ANN의 얕은 학습과 깊은 학습(예: 반복적 네트)은 다년간 탐구되어 왔다.[42][43][44] 이러한 방법은 차별적으로 훈련된 음성 생성 모델에 기초한 비균일 내부 수작업 가우스 혼합 모델/은닉 마르코프 모델(GMM-HM) 기술을 능가하지 않았다.[45] 기울기 감소[39]와 신경 예측 모델의 약한 시간 상관 구조를 포함한 주요 어려움이 분석되었다.[46][47] 추가적인 어려움은 교육 데이터의 부족과 제한된 컴퓨팅 능력이었다.​대부분의 음성 인식 연구자들은 생성 모델링을 추구하기 위해 신경망에서 멀어졌다. 1990년대 후반에 SRI 인터내셔널은 예외였다. 미국 정부의 NSA와 DARPA의 지원을 받아 SRI는 음성 및 스피커 인식에 관한 심층 신경망을 연구했다. 래리 헥(Larry Heck)이 이끄는 스피커 인식 팀은 1998년 미국 국립표준기술연구소(National Institute of Standards and Technology Speaker Recognition) 평가에서 음성 처리에 있어 심층 신경망을 통해 상당한 성공을 거두었다고 보고했다.[48] 그런 다음 SRI 심층 신경망을 뉘앙스 검증기에 배치하여 딥 러닝의 첫 번째 주요 산업 응용 프로그램을 나타냈다.[49]​수작업 최적화보다 원시 피쳐를 높이는 원리는 1990년대 후반 ""원시"" 스펙트로그램 또는 선형 필터 뱅크 기능의 딥 오토인코더 아키텍처에서 처음으로 성공적으로 탐구되었으며, [49] 스펙트로그램에서 고정된 변환 단계를 포함하는 멜-셉스트랄 피쳐보다 우수함을 보여주었다. 음성의 원시 특징인 파형은 나중에 더 큰 규모의 훌륭한 결과를 낳았다.[50]​음성 인식의 많은 측면은 1997년 Hochreiter와 Schmidhuber가 발표한 반복 신경망인 LSTM(Long Short-Term Memory)이라고 불리는 딥 러닝 방법에 의해 대체되었다.[51] LSTM RNN은 소멸되는 기울기 문제를 방지하고 이전에 수천 개의 개별 시간 단계에서 발생한 사건에 대한 기억을 요구하는 ""매우 딥 러닝"" 과제[12]를 학습할 수 있으며, 이는 연설에 중요하다.​Deep learning revolutionIn 2012, a team led by George E. Dahl won the ""Merck Molecular Activity Challenge"" using multi-task deep neural networks to predict the biomolecular target of one drug.[88][89] In 2014, Hochreiter's group used deep learning to detect off-target and toxic effects of environmental chemicals in nutrients, household products and drugs and won the ""Tox21 Data Challenge"" of NIH, FDA and NCATS.[90][91][92]Significant additional impacts in image or object recognition were felt from 2011 to 2012. Although CNNs trained by backpropagation had been around for decades, and GPU implementations of NNs for years, including CNNs, fast implementations of CNNs on GPUs were needed to progress on computer vision.[82][84][36][93][12] In 2011, this approach achieved for the first time superhuman performance in a visual pattern recognition contest. Also in 2011, it won the ICDAR Chinese handwriting contest, and in May 2012, it won the ISBI image segmentation contest.[94] Until 2011, CNNs did not play a major role at computer vision conferences, but in June 2012, a paper by Ciresan et al. at the leading conference CVPR[3] showed how max-pooling CNNs on GPU can dramatically improve many vision benchmark records. In October 2012, a similar system by Krizhevsky et al.[4] won the large-scale ImageNet competition by a significant margin over shallow machine learning methods. In November 2012, Ciresan et al.'s system also won the ICPR contest on analysis of large medical images for cancer detection, and in the following year also the MICCAI Grand Challenge on the same topic.[95] In 2013 and 2014, the error rate on the ImageNet task using deep learning was further reduced, following a similar trend in large-scale speech recognition.Image classification was then extended to the more challenging task of generating descriptions (captions) for images, often as a combination of CNNs and LSTMs.[96][97][98][99]Some researchers state that the October 2012 ImageNet victory anchored the start of a ""deep learning revolution"" that has transformed the AI industry.[100]In March 2019, Yoshua Bengio, Geoffrey Hinton and Yann LeCun were awarded the Turing Award for conceptual and engineering breakthroughs that have made deep neural networks a critical component of computing. 딥러닝 혁명2012년, 조지 E가 이끄는 팀. 달은 한 약물의 생체분자 목표를 예측하기 위해 멀티태스킹 심층 신경망을 이용한 ""머크 분자 활동 챌린지""에서 우승했다.[88][89] 2014년, Hocreiter 그룹은 딥러닝을 사용하여 영양소, 가정용품 및 의약품에서 환경 화학 물질의 표적 외 및 독성 효과를 감지했고 NIH, FDA 및 NCATS의 ""Tox21 Data Challenge""에서 우승했다.[90][91][92]​2011년부터 2012년까지 이미지 또는 객체 인식에 상당한 영향을 미쳤다. 역전파에 의해 훈련된 CNN은 수십 년 동안 존재했고, CNN을 포함한 NN의 GPU 구현은 수년 동안 있었지만, 컴퓨터 비전을 발전시키기 위해서는 GPU에 대한 CNN의 빠른 구현이 필요했다.[82][84][36][93][12] 2011년에 이 접근방식은 시각 패턴 인식 대회에서 처음으로 초인적인 성능을 달성했다. 2011년 ICDAR 중국어 글씨 공모전, 2012년 5월 ISBI 이미지 세분화 공모전에서 우승했다.[94] 2011년까지 CNN은 컴퓨터 비전 컨퍼런스에서 주요 역할을 하지 않았지만, 2012년 6월 선도 컨퍼런스 CVPR[3]에서 Ciresan 등의 논문은 GPU에서 CNN을 최대로 풀링하는 것이 어떻게 많은 비전 벤치마크 레코드를 획기적으로 개선할 수 있는지를 보여주었다. 2012년 10월 크리제프스키 외 연구진의 연구결과와 유사한 성단이 발견되었다.[4] 이 회사는 대규모 ImageNet 경쟁에서 얕은 머신러닝 방법에 비해 상당한 차이로 우승했다. 2012년 11월에는 Ciresan 등의 시스템이 암 검출을 위한 대형 의료 영상 분석 ICPR 공모전에서 우승을 차지했으며, 이듬해에는 같은 주제로 열린 MICCAI Grand Challenge에서도 우승을 차지했다.[95] 2013년과 2014년에 대규모 음성 인식의 유사한 추세에 따라 딥러닝을 이용한 ImageNet 과제의 오류율이 더욱 감소하였다.​영상 분류는 종종 CNN과 LSTM의 조합으로 이미지에 대한 설명(캡션)을 생성하는 더 어려운 작업으로 확장되었다. [96][97][98][99]​일부 연구자들은 2012년 10월 ImageNet 승리가 AI 산업을 변혁시킨 '딥러닝 혁명'의 시작을 닻을 내렸다고 말한다.[100]​2019년 3월, 요슈아 벤지오, 제프리 힌튼, 얀 르쿤은 심층 신경망을 컴퓨팅의 중요한 구성 요소로 만든 개념 및 엔지니어링 혁신으로 튜링상을 수상했다.​Neural networksArtificial neural networks[edit]​Artificial neural networks (ANNs) or connectionist systems are computing systems inspired by the biological neural networks that constitute animal brains. Such systems learn (progressively improve their ability) to do tasks by considering examples, generally without task-specific programming. For example, in image recognition, they might learn to identify images that contain cats by analyzing example images that have been manually labeled as ""cat"" or ""no cat"" and using the analytic results to identify cats in other images. They have found most use in applications difficult to express with a traditional computer algorithm using rule-based programming.An ANN is based on a collection of connected units called artificial neurons, (analogous to biological neurons in a biological brain). Each connection (synapse) between neurons can transmit a signal to another neuron. The receiving (postsynaptic) neuron can process the signal(s) and then signal downstream neurons connected to it. Neurons may have state, generally represented by real numbers, typically between 0 and 1. Neurons and synapses may also have a weight that varies as learning proceeds, which can increase or decrease the strength of the signal that it sends downstream.Typically, neurons are organized in layers. Different layers may perform different kinds of transformations on their inputs. Signals travel from the first (input), to the last (output) layer, possibly after traversing the layers multiple times.The original goal of the neural network approach was to solve problems in the same way that a human brain would. Over time, attention focused on matching specific mental abilities, leading to deviations from biology such as backpropagation, or passing information in the reverse direction and adjusting the network to reflect that information.Neural networks have been used on a variety of tasks, including computer vision, speech recognition, machine translation, social network filtering, playing board and video games and medical diagnosis.As of 2017, neural networks typically have a few thousand to a few million units and millions of connections. Despite this number being several order of magnitude less than the number of neurons on a human brain, these networks can perform many tasks at a level beyond that of humans (e.g., recognizing faces, playing ""Go""[101] ).신경망​인공신경망​인공신경망 또는 연결주의 시스템은 동물의 뇌를 구성하는 생물학적 신경망에서 영감을 받은 컴퓨터 시스템입니다. 이러한 시스템은 일반적으로 작업별 프로그래밍 없이 예를 고려하여 작업을 수행하는 방법을 학습(점진적으로 향상)한다. 예를 들어 이미지 인식에서 그들은 수동으로 ""고양이"" 또는 ""고양이 없음""으로 라벨이 지정된 예제 이미지를 분석하고 분석 결과를 사용하여 다른 이미지에서 고양이를 식별함으로써 고양이가 포함된 이미지를 식별하는 방법을 배울 수 있다. 이들은 규칙 기반 프로그래밍을 사용하여 전통적인 컴퓨터 알고리즘으로 표현하기 어려운 응용 분야에서 가장 많이 사용된다.​ANN은 인공 뉴런이라고 불리는 연결된 단위의 모음에 기초한다. 뉴런 사이의 각각의 시냅스는 다른 뉴런으로 신호를 전달할 수 있다. 시냅스 후 뉴런은 신호를 처리한 다음 연결된 다운스트림 뉴런의 신호를 보낼 수 있다. 뉴런은 일반적으로 0에서 1 사이의 실수로 표현되는 상태를 가질 수 있다. 뉴런과 시냅스도 학습이 진행됨에 따라 달라지는 무게가 있을 수 있는데, 이는 하류로 보내는 신호의 강도를 높이거나 낮출 수 있다.​전형적으로 뉴런은 층으로 구성되어 있습니다. 다른 계층들은 그들의 입력에서 다른 종류의 변환을 수행할 수 있다. 신호는 첫 번째(입력) 계층에서 마지막(출력) 계층으로 이동하며, 레이어를 여러 번 통과한 후에 이동할 수 있습니다.​신경망 접근법의 원래 목표는 인간의 뇌가 하는 것과 같은 방식으로 문제를 해결하는 것이었다. 시간이 지남에 따라 특정 정신 능력을 일치시키는 데 관심이 집중되어 역전파와 같은 생물학에서 벗어나거나 정보를 역방향으로 전달하고 해당 정보를 반영하도록 네트워크를 조정했다.​신경망은 컴퓨터 비전, 음성 인식, 기계 번역, 소셜 네트워크 필터링, 보드 및 비디오 게임, 의학 진단 등 다양한 작업에 사용되어 왔다.​2017년 현재, 신경망은 일반적으로 수천에서 수백만 개의 유닛과 수백만 개의 연결을 가지고 있다. 이 숫자는 인간의 뇌에 있는 뉴런의 수보다 몇 배 적지만, 이러한 네트워크는 인간의 수준보다 많은 작업을 수행할 수 있다(예를 들어, 얼굴을 인식하는 것, 바둑을 두는 것).​Deep neural networksA deep neural network (DNN) is an artificial neural network (ANN) with multiple layers between the input and output layers.[9][12] There are different types of neural networks but they always consist of the same components: neurons, synapses, weights, biases, and functions.[102] These components functioning similar to the human brains and can be trained like any other ML algorithm.[citation needed]For example, a DNN that is trained to recognize dog breeds will go over the given image and calculate the probability that the dog in the image is a certain breed. The user can review the results and select which probabilities the network should display (above a certain threshold, etc.) and return the proposed label. Each mathematical manipulation as such is considered a layer, and complex DNN have many layers, hence the name ""deep"" networks.DNNs can model complex non-linear relationships. DNN architectures generate compositional models where the object is expressed as a layered composition of primitives.[103] The extra layers enable composition of features from lower layers, potentially modeling complex data with fewer units than a similarly performing shallow network.[9] For instance, it was proved that sparse multivariate polynomials are exponentially easier to approximate with DNNs than with shallow networks.[104]Deep architectures include many variants of a few basic approaches. Each architecture has found success in specific domains. It is not always possible to compare the performance of multiple architectures, unless they have been evaluated on the same data sets.DNNs are typically feedforward networks in which data flows from the input layer to the output layer without looping back. At first, the DNN creates a map of virtual neurons and assigns random numerical values, or ""weights"", to connections between them. The weights and inputs are multiplied and return an output between 0 and 1. If the network did not accurately recognize a particular pattern, an algorithm would adjust the weights.[105] That way the algorithm can make certain parameters more influential, until it determines the correct mathematical manipulation to fully process the data.Recurrent neural networks (RNNs), in which data can flow in any direction, are used for applications such as language modeling.[106][107][108][109][110] Long short-term memory is particularly effective for this use.[51][111]Convolutional deep neural networks (CNNs) are used in computer vision.[112] CNNs also have been applied to acoustic modeling for automatic speech recognition (ASR).[68]심층신경망​심층신경망(DNN)은 입력 계층과 출력 계층 사이에 여러 계층이 있는 인공신경망(ANN)이다.[9][12] 다른 유형의 신경망이 있지만 그것들은 항상 뉴런, 시냅스, 가중치, 편향 및 기능과 같은 구성 요소로 구성된다.[102] 이러한 구성 요소는 인간의 뇌와 유사하게 기능하며 다른 ML 알고리즘처럼 훈련될 수 있다.[필요]​예를 들어 개 품종을 인식하도록 훈련된 DNN은 주어진 이미지를 검토해 이미지 속 개가 특정 품종일 확률을 계산한다. 사용자는 결과를 검토하고 네트워크가 표시할 확률(특정 임계값 이상)을 선택하고 제안된 레이블을 반환할 수 있습니다. 각각의 수학적 조작은 계층으로 간주되며, 복잡한 DNN은 많은 계층을 가지고 있다.​DNN은 복잡한 비선형 관계를 모델링할 수 있습니다. DNN 아키텍처는 객체가 원시 요소의 계층 구성으로 표현되는 구성 모델을 생성한다.[103] 추가 계층을 사용하면 하위 계층에서 기능을 구성할 수 있으며, 유사한 성능을 보이는 얕은 네트워크보다 적은 수의 유닛으로 복잡한 데이터를 모델링할 수 있습니다.[9] 예를 들어, 희소 다변량 다항식은 얕은 네트워크보다 DNN으로 근사하기가 기하급수적으로 더 쉽다는 것이 입증되었다.[104]​심층 아키텍처는 몇 가지 기본적인 접근법의 많은 변형을 포함한다. 각 아키텍처는 특정 도메인에서 성공을 거두었습니다. 동일한 데이터 세트에서 여러 아키텍처의 성능을 평가하지 않는 한 항상 비교할 수 있는 것은 아닙니다.​DNN은 일반적으로 데이터가 루프백 없이 입력 계층에서 출력 계층으로 흐르는 피드포워드 네트워크이다. 처음에 DNN은 가상 뉴런의 지도를 만들고 그들 사이의 연결에 임의의 숫자 값 또는 ""가중치""를 할당한다. 가중치와 입력은 곱하여 0과 1 사이의 출력을 반환합니다. 네트워크가 특정 패턴을 정확하게 인식하지 못하면 알고리즘이 가중치를 조정한다.[105] 이러한 방식으로 알고리즘은 데이터를 완전히 처리하기 위한 올바른 수학적 조작을 결정할 때까지 특정 매개변수를 더 영향력 있게 만들 수 있습니다.​데이터가 모든 방향으로 흐를 수 있는 순환 신경망(RNN)은 언어 모델링과 같은 응용 프로그램에 사용된다.[106][107][108][109][110] 장기 단기 메모리가 이 용도에 특히 효과적입니다.[51][111]​컨볼루션 심층 신경망(CNN)은 컴퓨터 비전에 사용된다.[112] CNN은 자동 음성 인식(ASR)을 위한 음향 모델링에도 적용되었다.[68]​ChallengesAs with ANNs, many issues can arise with naively trained DNNs. Two common issues are overfitting and computation time.DNNs are prone to overfitting because of the added layers of abstraction, which allow them to model rare dependencies in the training data. Regularization methods such as Ivakhnenko's unit pruning[27] or weight decay (-regularization) or sparsity (-regularization) can be applied during training to combat overfitting.[113] Alternatively dropout regularization randomly omits units from the hidden layers during training. This helps to exclude rare dependencies.[114] Finally, data can be augmented via methods such as cropping and rotating such that smaller training sets can be increased in size to reduce the chances of overfitting.[115]DNNs must consider many training parameters, such as the size (number of layers and number of units per layer), the learning rate, and initial weights. Sweeping through the parameter space for optimal parameters may not be feasible due to the cost in time and computational resources. Various tricks, such as batching (computing the gradient on several training examples at once rather than individual examples)[116] speed up computation. Large processing capabilities of many-core architectures (such as GPUs or the Intel Xeon Phi) have produced significant speedups in training, because of the suitability of such processing architectures for the matrix and vector computations.[117][118]Alternatively, engineers may look for other types of neural networks with more straightforward and convergent training algorithms. CMAC (cerebellar model articulation controller) is one such kind of neural network. It doesn't require learning rates or randomized initial weights for CMAC. The training process can be guaranteed to converge in one step with a new batch of data, and the computational complexity of the training algorithm is linear with respect to the number of neurons involved.[119][120]과제들​ANN과 마찬가지로 순진하게 훈련된 DNN에서도 많은 문제가 발생할 수 있다. 두 가지 일반적인 문제는 과적합과 계산 시간이다.​DNN은 추가된 추상화 계층으로 인해 과적합되기 쉬우며, 이를 통해 훈련 데이터의 희귀 의존성을 모델링할 수 있다. 이바크넨코의 단위 가지치기[27] 또는 체중 감소(-정규화) 또는 희소성(-정규화)과 같은 정규화 방법은 과적합에 대처하기 위해 훈련 중에 적용될 수 있다.[113] 또는 드롭아웃 정규화는 훈련 중에 은닉 계층에서 단위를 임의로 생략한다. 이것은 희귀한 종속성을 제외하는 데 도움이 된다.[114] 마지막으로, 데이터는 크롭 및 회전과 같은 방법을 통해 증가하여 더 작은 훈련 세트의 크기를 증가시켜 과적합 가능성을 줄일 수 있다.[115]​DNN은 크기(레이어 수 및 계층당 단위 수), 학습 속도 및 초기 가중치와 같은 많은 훈련 매개 변수를 고려해야 한다. 최적의 매개 변수를 위해 매개 변수 공간을 훑는 것은 시간과 계산 자원의 비용 때문에 실현 가능하지 않을 수 있다. 일괄 처리(개별 예제가 아닌 여러 훈련 예제에 대한 그라데이션 계산)와 같은 다양한 트릭[116]은 계산 속도를 높인다. 다핵심 아키텍처(예: GPU 또는 Intel Xeon Phi)의 대규모 처리 기능은 매트릭스 및 벡터 계산에 대한 이러한 처리 아키텍처의 적합성으로 인해 훈련 속도가 크게 향상되었다.[117][118]​대안적으로, 엔지니어는 보다 간단하고 수렴적인 훈련 알고리즘을 가진 다른 유형의 신경망을 찾을 수 있다. CMAC(세레벨라 모델 관절 제어기)는 그러한 종류의 신경망 중 하나이다. CMAC에 대한 학습 속도나 무작위 초기 가중치가 필요하지 않다. 훈련 과정은 새로운 데이터 배치로 한 단계에서 수렴되도록 보장할 수 있으며, 훈련 알고리즘의 계산 복잡도는 관련된 뉴런 수에 대해 선형적이다.[119][120]​Hardware​Since the 2010s, advances in both machine learning algorithms and computer hardware have led to more efficient methods for training deep neural networks that contain many layers of non-linear hidden units and a very large output layer.[121] By 2019, graphic processing units (GPUs), often with AI-specific enhancements, had displaced CPUs as the dominant method of training large-scale commercial cloud AI.[122] OpenAI estimated the hardware computation used in the largest deep learning projects from AlexNet (2012) to AlphaZero (2017), and found a 300,000-fold increase in the amount of computation required, with a doubling-time trendline of 3.4 months.[123][124]하드웨어​2010년대 이후 머신러닝 알고리즘과 컴퓨터 하드웨어의 발전은 비선형 은닉 유닛의 많은 레이어와 매우 큰 출력 레이어를 포함하는 심층 신경망을 훈련하는 더 효율적인 방법으로 이어졌다.[121] 2019년까지 종종 AI에 특화된 기능을 갖춘 그래픽 처리 장치(GPU)가 대규모 상용 클라우드 AI를 훈련하는 지배적인 방법으로 CPU를 대체했다.[122] 열기AI는 알렉스넷(2012년)에서 알파제로(2017년)까지 최대 딥러닝 프로젝트에 사용된 하드웨어 연산을 추정한 결과 필요한 연산량이 30만 배 증가해 3.4개월이라는 두 배의 추세선이 나타났다.[123][124]​Applications​Automatic speech recognitionLarge-scale automatic speech recognition is the first and most convincing successful case of deep learning. LSTM RNNs can learn ""Very Deep Learning"" tasks[12] that involve multi-second intervals containing speech events separated by thousands of discrete time steps, where one time step corresponds to about 10 ms. LSTM with forget gates[111] is competitive with traditional speech recognizers on certain tasks.[52]The initial success in speech recognition was based on small-scale recognition tasks based on TIMIT. The data set contains 630 speakers from eight major dialects of American English, where each speaker reads 10 sentences.[125] Its small size lets many configurations be tried. More importantly, the TIMIT task concerns phone-sequence recognition, which, unlike word-sequence recognition, allows weak phone bigram language models. This lets the strength of the acoustic modeling aspects of speech recognition be more easily analyzed. The error rates listed below, including these early results and measured as percent phone error rates (PER), have been summarized since 1991.​적용들​자동 음성 인식​대규모 자동 음성 인식은 딥 러닝의 최초이자 가장 설득력 있는 성공 사례이다. LSTM RNN은 수천 개의 개별 시간 단계로 구분된 음성 이벤트를 포함하는 수 초 간격을 포함하는 ""매우 딥 러닝"" 과제[12]를 학습할 수 있다. 망각 게이트가 있는 LSTM[111]은 특정 작업에서 기존의 음성 인식기에 비해 경쟁력이 있다.[52]​음성 인식의 초기 성공은 TIMIT을 기반으로 한 소규모 인식 작업에 기반을 두었다. 이 데이터 세트에는 미국 영어의 8개 주요 방언에서 온 630명의 화자가 포함되어 있으며, 각 화자는 10개의 문장을 읽는다.[125] 크기가 작기 때문에 많은 구성을 시도할 수 있습니다. 더 중요한 것은, TIMIT 작업은 워드 시퀀스 인식과 달리 약한 전화 빅그램 언어 모델을 허용하는 전화 시퀀스 인식에 관한 것이다. 이를 통해 음성 인식의 음향 모델링 측면의 강도를 보다 쉽게 분석할 수 있다. 이러한 초기 결과를 포함하여 1991년부터 전화 오류율(PER)로 측정된 아래 나열된 오류율은 요약되어 왔다. MethodPercent phoneerror rate (PER) (%)Randomly Initialized RNN[126]26.1Bayesian Triphone GMM-HMM25.6Hidden Trajectory (Generative) Model24.8Monophone Randomly Initialized DNN23.4Monophone DBN-DNN22.4Triphone GMM-HMM with BMMI Training21.7Monophone DBN-DNN on fbank20.7Convolutional DNN[127]20.0Convolutional DNN w. Heterogeneous Pooling18.7Ensemble DNN/CNN/RNN[128]18.3Bidirectional LSTM17.8Hierarchical Convolutional Deep Maxout Network[129]16.5 방법                    백분율 전화 오류율(PER)(%)​임의로 초기화된 RNN[126]                                                   26.1​베이지안 트라이폰 GMM-HM                                               25.6​숨겨진 궤적(생성) 모델                                                         24.8​임의로 초기화된 모노폰 DNN                                               23.4​모노폰 DBN-DNN                                                                  22.4​트라이폰 GMM-HM(BMMI 교육 포함)                                   21.7​fbank의 모노폰 DBN-DNN                                                     20.7​컨볼루션 DNN[127]                                                               20.0​컨볼루션 DNN w. 유형이 다른 풀링                                      18.7​앙상블 DNN/CNN/RNN[128]                                                 18.3​양방향 LSTM                                                                         17.8​계층적 컨볼루션 딥 맥스아웃 네트워크[129]                          16.5​The debut of DNNs for speaker recognition in the late 1990s and speech recognition around 2009-2011 and of LSTM around 2003–2007, accelerated progress in eight major areas:[8][76][74]Scale-up/out and accelerated DNN training and decodingSequence discriminative trainingFeature processing by deep models with solid understanding of the underlying mechanismsAdaptation of DNNs and related deep modelsMulti-task and transfer learning by DNNs and related deep modelsCNNs and how to design them to best exploit domain knowledge of speechRNN and its rich LSTM variantsOther types of deep models including tensor-based models and integrated deep generative/discriminative models.All major commercial speech recognition systems (e.g., Microsoft Cortana, Xbox, Skype Translator, Amazon Alexa, Google Now, Apple Siri, Baidu and iFlyTek voice search, and a range of Nuance speech products, etc.) are based on deep learning.[8][130][131]​1990년대 후반 음성 인식을 위한 DNN과 2009-2011년 경 음성 인식, 2003-2007년 경 LSTM의 등장으로 8가지 주요 영역에서 발전이 가속화되었다. [8][76][74]​스케일업/스케일아웃 및 가속화된 DNN 교육 및 디코딩​순서 차별적 훈련​기본 메커니즘을 확실하게 이해한 심층 모델에 의한 기능 처리​DNN 및 관련 심층 모델의 적응​DNN 및 관련 심층 모델에 의한 다중 작업 및 전이 학습​CNN 및 도메인 음성 지식을 가장 잘 활용하도록 설계하는 방법​RNN 및 다양한 LSTM 모델​텐서 기반 모델 및 통합 심층 생성/차별 모델을 포함한 기타 유형의 심층 모델.​모든 주요 상용 음성 인식 시스템(예: Microsoft Cortana, Xbox, Skype 번역기, Amazon Alexa, Google Now, Apple Siri, Baidu 및 iFly)텍 음성 검색 및 다양한 뉘앙스 스피치 제품 등)은 딥러닝을 기반으로 한다.​Image recognitionMain article: Computer visionA common evaluation set for image classification is the MNIST database data set. MNIST is composed of handwritten digits and includes 60,000 training examples and 10,000 test examples. As with TIMIT, its small size lets users test multiple configurations. A comprehensive list of results on this set is available.[132]Deep learning-based image recognition has become ""superhuman"", producing more accurate results than human contestants. This first occurred in 2011 in recognition of traffic signs, and in 2014, with recognition of human faces.[133]Surpassing Human Level Face RecognitionDeep learning-trained vehicles now interpret 360° camera views.[134] Another example is Facial Dysmorphology Novel Analysis (FDNA) used to analyze cases of human malformation connected to a large database of genetic syndromes.​이미지 인식​주요 기사: 컴퓨터 비전​영상 분류를 위한 일반적인 평가 세트는 MNIST 데이터베이스 데이터 세트입니다. MNIST는 손으로 쓴 숫자로 구성되어 있으며 60,000개의 교육 예제와 10,000개의 테스트 예제를 포함합니다. TIMIT과 마찬가지로 크기가 작아서 사용자가 여러 구성을 테스트할 수 있습니다. 이 세트의 포괄적인 결과 목록을 사용할 수 있습니다.[132]​딥러닝 기반의 이미지 인식은 '초인적'이 되어, 인간 참가자보다 더 정확한 결과를 만들어 내고 있다. 이는 2011년 교통 표지판을 인식하여 처음 발생하였으며, 2014년 사람 얼굴을 인식하여 처음 발생하였다.[133]인체 수준 얼굴 인식 기능 뛰어넘기​딥러닝 교육을 받은 차량은 이제 360° 카메라 뷰를 해석합니다.[134] 또 다른 예는 대규모 유전 증후군 데이터베이스에 연결된 인간 기형의 사례를 분석하는 데 사용되는 안면 이형학 소설 분석(FDNA)이다.​Visual art processingClosely related to the progress that has been made in image recognition is the increasing application of deep learning techniques to various visual art tasks. DNNs have proven themselves capable, for example, of a) identifying the style period of a given painting, b) Neural Style Transfer – capturing the style of a given artwork and applying it in a visually pleasing manner to an arbitrary photograph or video, and c) generating striking imagery based on random visual input fields.[135][136]비주얼 아트 처리이미지 인식의 진전과 밀접한 관련이 있는 것은 다양한 시각 예술 작업에 딥 러닝 기술을 적용하는 것이다. 예를 들어, DNN은 a) 주어진 그림의 스타일 기간을 식별하는 것, b) 신경 스타일 전달 – 주어진 예술 작품의 스타일을 캡처하여 임의의 사진이나 비디오에 시각적으로 즐거운 방식으로 적용하는 것, c) 무작위 시각적 입력 필드를 기반으로 인상적인 이미지를 생성하는 것 등의 능력을 입증했다.[135][136]​Natural language processingNeural networks have been used for implementing language models since the early 2000s.[106] LSTM helped to improve machine translation and language modeling.[107][108][109]Other key techniques in this field are negative sampling[137] and word embedding. Word embedding, such as word2vec, can be thought of as a representational layer in a deep learning architecture that transforms an atomic word into a positional representation of the word relative to other words in the dataset; the position is represented as a point in a vector space. Using word embedding as an RNN input layer allows the network to parse sentences and phrases using an effective compositional vector grammar. A compositional vector grammar can be thought of as probabilistic context free grammar (PCFG) implemented by an RNN.[138] Recursive auto-encoders built atop word embeddings can assess sentence similarity and detect paraphrasing.[138] Deep neural architectures provide the best results for constituency parsing,[139] sentiment analysis,[140] information retrieval,[141][142] spoken language understanding,[143] machine translation,[107][144] contextual entity linking,[144] writing style recognition,[145] Text classification and others.[146]Recent developments generalize word embedding to sentence embedding.Google Translate (GT) uses a large end-to-end long short-term memory (LSTM) network.[147][148][149][150][151][152] Google Neural Machine Translation (GNMT) uses an example-based machine translation method in which the system ""learns from millions of examples.""[148] It translates ""whole sentences at a time, rather than pieces. Google Translate supports over one hundred languages.[148] The network encodes the ""semantics of the sentence rather than simply memorizing phrase-to-phrase translations"".[148][153] GT uses English as an intermediate between most language pairs.[153]자연어 처리​신경망은 2000년대 초부터 언어모델 구현에 사용되어 왔다.[106] LSTM은 기계 번역 및 언어 모델링을 개선하는 데 도움이 되었습니다.[107][108][109]​이 분야의 다른 핵심 기술은 음성 샘플링[137]과 단어 임베딩이다. 워드2vec와 같은 단어 임베딩은 원자 단어를 데이터 집합의 다른 단어에 상대적인 단어의 위치 표현으로 변환하는 딥 러닝 아키텍처의 표현 계층으로 생각할 수 있다. 위치는 벡터 공간의 한 점으로 표현된다. 단어 임베딩을 RNN 입력 계층으로 사용하면 네트워크가 효과적인 구성 벡터 문법을 사용하여 문장과 구문을 구문 분석할 수 있다. 구성 벡터 문법은 RNN에 의해 구현된 확률론적 컨텍스트 프리 문법(PCFG)으로 생각할 수 있다. [138] 단어 임베딩 위에 구축된 재귀 자동 인코더는 문장 유사성을 평가하고 패러프레이징을 감지할 수 있다.[138] 심층 신경 아키텍처는 구성 구문 분석, [139] 정서 분석, [140] 정보 검색, [141][142] 음성 언어 이해, [143] 기계 번역, [107] [144] 문맥 엔터티 연결, [144] 쓰기 스타일 인식, [145] 텍스트 분류 등에 대해 최상의 결과를 제공한다.[146]​최근의 발전은 단어 임베딩을 문장 임베딩으로 일반화한다.​구글 번역기(GT)는 대규모 종단 간 장단기 메모리(LSTM) 네트워크를 사용합니다.[147][149][150][151][152] 구글 신경 기계 번역(GNMT)은 시스템이 수백만 개의 예로부터 배우는 예제 기반 기계 번역 방법을 사용한다.[148] ""한 번에 전체 문장을 번역한다. 구글 번역기는 100개 이상의 언어를 지원합니다.[148] 네트워크는 ""단순히 구절에서 구절로의 번역을 암기하는 것이 아니라 문장의 의미론""을 암호화한다.[148][153] GT는 대부분의 언어 쌍 사이의 중간으로 영어를 사용합니다.[153]​Drug discovery and toxicology.A large percentage of candidate drugs fail to win regulatory approval. These failures are caused by insufficient efficacy (on-target effect), undesired interactions (off-target effects), or unanticipated toxic effects.[154][155] Research has explored use of deep learning to predict the biomolecular targets,[88][89] off-targets, and toxic effects of environmental chemicals in nutrients, household products and drugs.[90][91][92]AtomNet is a deep learning system for structure-based rational drug design.[156] AtomNet was used to predict novel candidate biomolecules for disease targets such as the Ebola virus[157] and multiple sclerosis.[158][159]In 2017 graph neural networks were used for the first time to predict various properties of molecules in a large toxicology data set.[160] In 2019, generative neural networks were used to produce molecules that were validated experimentally all the way into mice.[161][162]마약 발견과 독물학.​후보 약물의 상당 부분이 규제 승인을 받지 못하고 있다. 이러한 실패는 불충분한 효과(대상 외 효과), 원치 않는 상호작용(대상 외 효과) 또는 예상치 못한 독성 효과로 인해 발생한다.[154][155] 연구는 영양소, 가정용 제품 및 의약품에서 생체분자 목표치[88][89]와 환경 화학 물질의 독성 효과를 예측하기 위한 딥러닝의 사용을 탐구했다.[90][91][92]​아톰넷은 구조 기반 합리적 약물 설계를 위한 딥러닝 시스템이다.[156] AtomNet은 에볼라 바이러스[157] 및 다발성 경화증과 같은 질병 대상의 새로운 후보 생체분자를 예측하는 데 사용되었다.[158][159]​2017년 대규모 독성학 데이터 세트에서 분자의 다양한 특성을 예측하기 위해 그래프 신경망이 처음으로 사용되었다.[160] 2019년, 생쥐에 실험적으로 검증된 분자를 생산하기 위해 생성 신경망을 사용했다.[161][162]​Customer relationship managementDeep reinforcement learning has been used to approximate the value of possible direct marketing actions, defined in terms of RFM variables. The estimated value function was shown to have a natural interpretation as customer lifetime value.[163]고객관계관리​심층 강화 학습은 RFM 변수 측면에서 정의된 가능한 직접 마케팅 활동의 가치를 근사화하는 데 사용되어 왔다. 추정된 값 함수는 고객 평생 값으로 자연스럽게 해석되는 것으로 나타났습니다.[163]​Recommendation systemsRecommendation systems have used deep learning to extract meaningful features for a latent factor model for content-based music and journal recommendations.[164][165] Multi-view deep learning has been applied for learning user preferences from multiple domains.[166] The model uses a hybrid collaborative and content-based approach and enhances recommendations in multiple tasks.권장 시스템추천 시스템은 콘텐츠 기반 음악 및 저널 추천에 대한 잠재 요소 모델에 대한 의미 있는 특징을 추출하기 위해 딥러닝을 사용했다.[164][165] 다중 뷰 딥 러닝은 여러 도메인에서 사용자 선호도를 학습하기 위해 적용되었다.[166] 이 모델은 하이브리드 협업 및 컨텐츠 기반 접근 방식을 사용하고 여러 작업에서 권장 사항을 개선합니다.​BioinformaticsAn autoencoder ANN was used in bioinformatics, to predict gene ontology annotations and gene-function relationships.[167]In medical informatics, deep learning was used to predict sleep quality based on data from wearables[168] and predictions of health complications from electronic health record data.[169]생물정보학유전자 온톨로지 주석과 유전자 기능 관계를 예측하기 위해 생물정보학에서 자동인코더 ANN이 사용되었다.[167]​의료 정보학에서 딥 러닝은 웨어러블[168]의 데이터와 전자 건강 기록 데이터의 건강 합병증 예측을 기반으로 수면 품질을 예측하는 데 사용되었다.[169]​Medical image analysisDeep learning has been shown to produce competitive results in medical application such as cancer cell classification, lesion detection, organ segmentation and image enhancement.[170][171]의료영상분석딥러닝은 암세포 분류, 병변 검출, 장기 분할, 영상 증강 등 의료 응용 분야에서 경쟁력 있는 결과를 도출하는 것으로 나타났다.[170][171]​Mobile advertisingFinding the appropriate mobile audience for mobile advertising is always challenging, since many data points must be considered and analyzed before a target segment can be created and used in ad serving by any ad server.[172] Deep learning has been used to interpret large, many-dimensioned advertising datasets. Many data points are collected during the request/serve/click internet advertising cycle. This information can form the basis of machine learning to improve ad selection.모바일 광고광고 서버가 광고 서비스를 제공하는 대상 세그먼트를 만들고 사용하기 전에 많은 데이터 포인트를 고려하고 분석해야 하기 때문에 모바일 광고에 적합한 모바일 시청자를 찾는 것은 항상 어려운 일이다.[172] 딥 러닝은 대규모 다차원 광고 데이터 세트를 해석하는 데 사용되어 왔다. 요청/서비스/클릭 인터넷 광고 주기 동안 많은 데이터 포인트가 수집됩니다. 이 정보는 광고 선택을 개선하기 위한 기계 학습의 기초를 형성할 수 있습니다.​Image restorationDeep learning has been successfully applied to inverse problems such as denoising, super-resolution, inpainting, and film colorization.[173] These applications include learning methods such as ""Shrinkage Fields for Effective Image Restoration""[174] which trains on an image dataset, and Deep Image Prior, which trains on the image that needs restoration.이미지 복원딥 러닝은 노이즈 제거, 초해상도, 인페인팅, 필름 색칠과 같은 역문제에 성공적으로 적용되었다.[173] 이러한 응용 프로그램에는 이미지 데이터 집합에서 학습하는 ""효과적인 이미지 복원을 위한 축소 필드""[174] 및 복원이 필요한 이미지에서 학습하는 Deep Image Prior와 같은 학습 방법이 포함됩니다.​Financial fraud detectionDeep learning is being successfully applied to financial fraud detection, tax evasion detection,[175] and anti-money laundering.[176]금융 사기 탐지딥 러닝은 금융 사기 탐지, 탈세 탐지, 자금 세탁 방지에 성공적으로 적용되고 있다.[176]​MilitaryThe United States Department of Defense applied deep learning to train robots in new tasks through observation.[177]군사의미국 국방부는 관찰을 통해 로봇을 새로운 과제로 훈련시키는 딥러닝을 적용했다.[177]​Partial differential equationsPhysics informed neural networks have been used to solve partial differential equations in both forward and inverse problems in a data driven manner.[178] One example is the reconstructing fluid flow governed by the Navier-Stokes equations. Using physics informed neural networks does not require the often expensive mesh generation that conventional CFD methods relies on.[179][180]편미분 방정식물리 정보 신경망은 데이터 중심 방식으로 순방향 및 역방향 문제 모두에서 편미분 방정식을 해결하는 데 사용되어 왔다.[178] 한 가지 예는 Navier-Stokes 방정식에 의해 제어되는 유체 흐름 재구성입니다. 물리 정보 신경망을 사용하는 것은 기존의 CFD 방법이 의존하는 종종 값비싼 메시 생성을 필요로 하지 않는다.[179][180] "
"자연어 처리(NLP), 자연어 이해(NLU) 및 자연어 생성(NLG) ",https://blog.naver.com/mycozymoment/222703226852,20220417,"NLU와 NLG는 NLP의 구성 요소​​​자연어 처리(nlp)🔹구조화되지 않은 데이터를 가져와 구조화된 데이터 형식으로 변환하는 방식으로 작동.🔹토큰화, 형태소 분석, 표제어 추출과 같은 방법을 사용하여 단어의 뿌리 형태를 조사🔹문장을 '읽는' 것​ex)토큰화(tokenization)구문 분석(parsing)정보 추출(information extraction)유사성(similarity)음성 인식(speech recognition)자연어와 음성 생성 등(natural language and speech generations and many others)​​​자연어 이해(nlu)🔹문장의 의미를 결정하기 위해 텍스트와 음성의 구문 및 의미 분석을 사용하는 자연어 처리의 하위 집합. 🔹구문은 문장의 문법적 구조를 나타내는 반면 의미론은 의도된 의미를 암시.​🔹언어의 분석에 집중하는 NLP와 달리, NLU는 단어 뒤에 있는 의미론이나 의미에 관심을 갖는다.🔹문장을 '이해'하는 것​​ex)Named entity recognition (NER)Word sense disambiguationNatural language generation​​​자연어 생성(nlg)🔹데이터 입력을 기반으로 인간 언어 텍스트 응답을 생성하는 프로세스🔹정보의 무결성을 유지하면서 입력 문서에서 요약을 생성하는 텍스트 요약 기능 포함. ​​​​​​​​​​​​​​​​ http://insightcampus.co.kr:9090/insightcommunity/?mod=document&uid=12817​​​ NLP vs. NLU: 자연어 이해에서 자연어 처리까지NLP vs. NLU: 자연어 이해에서 자연어 처리까지 NLP vs. NLU: from Understanding a Language to Its Processing *이 글은 Medium에 게시된 Sciforce의 글을 번역하였습니다. 인공지능이 발전하고 기술이 더욱 정교해짐에 따라, 우리는 기존의 개념들이 이러한 변화를 수용하거나 스스로 변화하기를 기대합니다. 마찬가지로, 컴퓨터 보조 처리 영역에서 자연어 처리(NLP) 개념이 자연어 이해(NLU)에 자리를 내줘야 할까요? 아니면 두 개념이 하위 개념일까요? 한 가지 개념이 더 진...insightcampus.co.kr ​ "
Amazon Lex & Connect ,https://blog.naver.com/dudemr2201/222951519328,20221210,"Amazon Lex & Connect​Amazon Lex: (same technology that powers Alexa) Alexa 장치를 구현하는 기술과 동일하다.​-Automatic Speech Recognition (ASR) to convert speech to textLex를 사용하여 자동 음성 인식을 할 수 있다. 말을 텍스틀 바꿔준다.​-Natural Language Understanding to recognize the intent of text, callers문자, 발신자의 의도를 인식하는 자연어 이해​-Helps build chatbots, call center bots챗봇이나 콜 센터 봇 구축에 도움을 준다.​Amazon Connect:-Receive calls, create contact flows, cloud-based virtual contact center-통화 수신, 연락 흐름 생성, 클라우드 기반 가상 연락 센터-Can integrate with other CRM systems or AWS다른 CRM 시스템 혹은 AWS 서비스와 통합할 수 있다.-No upfront payments, 80% cheaper than traditional contact center solutions초기 비용이 없고, 기존 고객 센터 방식에 비해 비용이 80% 저렴하다. "
"난청 칼럼, 청력검사의 종류 ",https://blog.naver.com/jjjwax/222838788786,20220803,"​안녕하세요벨톤보청기 광명난청센터정순옥 원장입니다.  난청 칼럼""청력도 정기검진도 하시나요?""사랑하는 이들의 목소리 한마디라도 더 들을 수 있게-박준영 기자-​[난청 칼럼]“청력도 정기검진도 하시나요?” 사랑하는 이들의 목소리 한마디라도 더 들을 수 있게난청 때문에 보청기 처방을 받았다면, 보청기 착용을 위해서 꼭 필요한 검사가 있다. 행동청능평가 검사다. 어떤 검사들이 진행되는지 간단하게 소개한다. 최근에는 나이가 들어 청력이 감퇴하는 것 이외에도 과도한 이어폰 사용으로 젊은 나이에도 난청을 얻은 경우가 늘어나는 추세다. WRITER 정순옥 1. 청력검사를 하는 이유 행동청능평가 검사는 소리를 어느 정도www.geconomy.co.kr G.ECONOMY(지이코노미)G.ECONOMY(지이코노미)www.geconomy.co.kr  보청기 착용 전 꼭 해야 할 청력검사의 종류 난청 때문에 보청기 처방을 받았다면?보청기 착용을 위해서 꼭 필요한 검사 중에는 ”행동청능평가” 검사가 있습니다.​행동청능평가에는 크게 순음 청력역치 검사와 어음청력검사가 있습니다. 그 내용을 알아보면 다음과 같습니다. 1. 청력검사를 하는 이유​행동청능평가 검사는 소리를 어느 정도 듣고 있는지 평가하는 검사입니다.​=> 보청기를 착용하기 전에 행동청능평가를 하는 이유를 알아보면, 우선 순음 기도 및 골도 청력 역치 검사는 외이도와 뼈를 통해서 소리를 알아들을 수 있는 가장 작은 강도를 확인하는 것이며, 검사 결과를 통해 청력손실의 정도와 유형, 난청의 종류 등을 추정할 수 있으며, 보청기의 이득 또는 주파수 반응곡선을 결정하는 가장 중요한 요소가 됩니다.  그리고 어음청력검사에서는 이음절어 또는 단음절어를 따라 말하는 것으로, 이음절어를 인지하는 가장 작은 강도는 순음 청력역치 검사의 신뢰도를 확인할 수 있으며, 단음절어를 인지하는 백분율은 보청기의 착용 효과를 예측할 수 있습니다. 결론적으로 순음 기도 및 골도 청력역치 검사와 어음청력검사를 통해 청능재활의 계획을 수립하는데 결정적인 정보를 제공합니다. ​  2. 순음 청력검사의 목적​순음 기도 및 골도 청력역치 검사의 목적은 첫째, 125 Hz에서 8,000 Hz의 범위에서 옥타브 또는 1/2 옥타브 주파수 별 난청 유무 또는 정도를 확인하고, 둘째, 난청이 있다면 난청의 정도, 종류 및 형태를 평가하여 보청기의 선택 그리고 이득 및 주파수 반응곡선을 산출하며, 청능재활의 계획을 수립할 수 있습니다.​​  3. 청력검사실과 검사자  ​ 청력검사는 외부의 소음이 차단된 조용한 방음실 내에서 청력검사기에서 발생하는 다양한 신로를 헤드폰 또는 스피커에 전달하여 검사를 진행합니다. 그리고 이러한 검사를 진행하는 검사자는 인증된 교육기관에서 자격을 갖춘 자로 주로 청능사 또는 전문청능사 그리고 이비인후과의 경우는 청각사 등 전문가에 의해 검사를 진행하며, 행동청능평가 검사시간은 피검자의 협조에 따라 10분~20분 정도 소요됩니다.​​ 4. 순음청력 검사 전 유의사항?​1. 피검 자는 설문지를 통해 과거 소음 노출 유무, 이 명 유무, 이과적 병력, 청력과 관계된 사례력(case history), 난청과 관련된 정보와 가족력을 알려주고, 검사자는 이경 검사나 비디오 검사를 통해 이개 및 외이도의 문제가 있는지, 또는 과도한 귀지가 있는지를 확인합니다. ​이때 청력검사 진행에 방해를 줄 수 있는 액세서리, 안경, 귀걸이, 이어폰 등은 제거합니다,피검 자는 확실히 소리가 날 때만 누르는 게 아니라 어느 쪽 귀에서 들리는지? 작게 들리는 소리도 들리면 즉시 반응하게 합니다) ​​  5. 순음 청력역치 검사 방법​우선 순음기도 청력역치 검사는 청력검사기에서 발생한 125에서 8,000 Hz의 순음을 피검자가 양 귀에 착용한 헤드폰으로 보내어 측정합니다.  청력검사기에서 신호를 헤드폰으로 보냈을 때, 피검 자는 가장 작은 소리라도 들리면 응답 버튼을 누릅니다. 이때 순음기도 청력역치는 피검자가 오른쪽 또는 왼쪽의 각각의 귀에서 반응하는 가장 작은 강도를 기록하며, 측정 주파수는 125에서 8,000 Hz의 옥타브 주파수 그리고 필요하다면 1/2 옥타브 주파수에서 검사를 진행합니다.  그리고 골도 청력역치 검사 또한 기도 청력역치 검사와 같은 방법으로 진행합니다. 다만, 청력검사기에서 발생한 신호를 헤드폰 대신 피검자의 귓바퀴 뒤의 유양돌기에 부탁한 골진동기로 보내어 측정합니다. ​  6. 어음청력검사​어음청력검사는 어음인지역치(speech recognition threshold)와, 단어인지도(word recognition score) 및 문장인지도(sentence recognition score)로 구분할 수 있습니다. 이 중에서 보청기 착용과 관계가 깊은 어음인지역치와 단어인지도의 검사 방법에 대해서 알아보도록 하겠습니다. 먼저 어음인지역치는 육군, 신발, 땅콩 등과 같이 일상생활에서 친숙한 이음절단어를 난청인이 착용한 헤드폰으로 제시하여 피검자가 인지할 수 있는 가장 작은 강도를 기록합니다. 어음 청취 역 치는 좌, 우측 각각의 귀에서 측정합니다. 그리고 단어인지도는 귀, 산, 들, 용 등과 같이 25개의 단음절 또는 50단어를 난청인이 착용한 헤드폰으로 제시하여 맞춘 숫자를 백분율로 나타냅니다.​예를 들면 “귀“라는 단음절을 제시하면 “비“라고 답할 경우 그대로 받아 적습니다.​ ​ 그 외에도 이명의 종류와 크기 등을 주관적으로 평가하는 이명검사와 유소아 청력검사,청각장애 진단 시 필수검사인 ABR 청성 뇌간유발반응 검사, 고막 상태가 정상적인지를 간접적으로 알아보는 임피던스 청력검사까지 다양한 법이 있습니다. 난청이 의심된다면 가까운 난청센터에서 방문하셔서 행동청능평가 검사부터 받아보시고 만약 보청기 처방을 받으셨다면 하루라도 빨리 보청기를 착용하여 난청이 더 이상 악화되지 않도록 관리를 잘 하시길 바랍니다. ​​  ​​ 벨톤 어메이즈(Beltone Amaze) 보청기 미국에서 구입하신 광명보청기 피팅 사례안녕하세요 벨톤보청기 광명지사 친절한 미녀청능사 정순옥 원장입니다. 지난겨울 미국에서 벨톤보청기 어...m.blog.naver.com ​​ 벨톤보청기 광명난청센터광명보청기,벨톤어메이즈,트러스트,보급형엘리,특수농아용부스트,배터리,지원금혜택www.ear119.kr ​ 벨톤보청기 광명난청센터경기도 광명시 철산로 12 로데오빌딩 3층 ​ "
호주 어학연수 -EF 시드니 ,https://blog.naver.com/raffles7/222712312611,20220426,"호주 최대 도시 시드니는 호주 문화, 명문 시드니 대학이 있는 교육의 중심지이고 오페라 하우스, 본다이 해변, 하버브리지 등 유명 관광지들도 많아서 어학연수하면서 다양한 경험을 할 수 있는 도시입니다.​ 시드니 오페라 하우스​ 본다이 비치​ 시드니 하버브리지​​오늘은 전 세계 50여 개 도시에 직영 캠퍼스를 운영하는 세계적인 어학연수 교육기관 EF의 시드니 캠퍼스에 대해 안내드리려고 합니다.​우선 EF의 수업 방식에 대해 먼저 말씀드리겠습니다.  EF의 어학연수과정은 다른 어학원보다 좀 더 듣기와 말하기 능력 항상에 포커스가 맞추어져 있습니다.​ 위  영상에서 보신 EF의 수업 체계는 다음과 같이 5 가지의 수업으로 구성되어 있습니다.1.  General Class :  저희도 익히 경험한 강남역 영어학원 같은 소규모 렉처 스타일 수업2.  Conversation class : 다양한 디스커션 수업3. iPAD Class : 아이패드를 이용한 2-3명의 pair conversation 수업4. i Lab computer class:  유창한 영어를 위해서는 파트너와의 공부도 필요하나 학습자 스스로의 혼자 해야 할 공부들이 있죠?  바로 올바른 문법을 익히고 올바른 발음으로 자신의 생각을 정리하고 전달해야 합니다. 그를 위한 video lesson(온라인 문법 공부)과  speech recognition(발음 수정)을 컴퓨터를 통해 공부하고  itracker라는 툴을 통해 본인의 프로그레스를 확인할 수 있습니다.5. SPIN Class:  Special Interest Class로 대학 교양처럼 호주 문화, 마케팅이나 저널리즘, 넷플릭스로 배우는 영어 같은 흥미 있는 과목 수업을 영어로 참여하는 수업입니다.​​EF 시드니 캠퍼스는 시드니 다운타운에 위치하고 단독 건물을 사용하고 있습니다. 따라서 학생 라운지 나 루프탑 테라스같이 강의실 외에도 학생들 휴게 공간도 잘 갖추고 있습니다.  한국인 학생 비율은 3% 내외이고 수강생들의 국적은 프랑스, 독일, 스위스, 멕시코, 이태리, 일본 등등 다양합니다.​ ​그럼 EF 시드니 캠퍼스를 버추얼 투어 한번 해볼까요?​  ​ ​​ EF 시드니의 어학연수 2주부터 등록 가능한 기간제 프로그램은 주당 수업 시간에 따라 일반과정과 집중과정을 고르 실 수 있습니다.​ 해외연수를 결정할 때 국내 어학원이 수준이 딸리거나 부족해서 해외연수를 떠나시는 것은 아닐 거예요.  영어를 쓰면서 낯선 곳에서 현지인의 삶을 체험해 보고 영어 공부에 대한 동기부여를 얻고 싶으셔서 이실 텐데요.성공적인 어학연수는  어학당 수업 방과 후를 어떻게 지내느냐가 매우 중요합니다.   한국인 학생들과 몰려다니면서 보낸다면 영어는 늘지 않을 거예요.  EF는 한국인 학생과 중국인 학생 비율이 매우 적은 것이 특징입니다.​EF 시드니 프로그램은 어학원 수업 외에도 다양한 액티비티가 있는 것도 특징입니다.​ ​​EF의 액티비티에 열심히 참여하셔서 방과 후 시간도 알차게 보내보시길 바랍니다.​ ​​​호주는 학생비자로 주 20시간까지 파트타임 일을 할 수 있습니다.  호주 시드니 EF로 가시면 돈도 벌면서 어학연수도 하실 수 있답니다.   EF 시드니에는 Job Club이라고 수강생의 알바 구직을 도와주는 클럽을 운영 중입니다.​ ​EF 시드니의 어학연수 기간 동안 숙박은 어떻게 해야 하는지 궁금해하실 텐데요?​​1.  주로 1인실을 제공하는 EF 연계 레지던스:   EF 시드니 캠퍼스 도보 5분 거리 위치하고 있고 공용 주방, 세탁실, 헬스장 등을 구비하고 있습니다. 2.  2인실을 제공하는 EF 연계 레지던스: EF 시드니 캠퍼스에서 트레인 40분 거리 위치하고 있고 공용 주방, 세탁실, 헬스장 등을 구비하고 있습니다. ​ EF 시드니의 어학연수 프로그램에 대해 더 궁금하신 사항은 저희 링크에이드로 연락 주시면 친절히 답변드리겠습니다. ​  링크에이드는 EF 어학연수 입학 수속을 무료로 도와드리고 있습니다.입학 허가를 받은 이후  출국 및 현지 정착까지 세심히 도와드리고 있습니다.관심 있으신 분들은 카카오톡 아이디 linkaid로 카톡 상담 가능합니다. 전화가 편하신 분들은 010-2473-0049로 전화 주시면 친절히 안내해 드리겠습니다.​ ​ "
자연어처리바이블 14강 : 대화시스템 ,https://blog.naver.com/0903jisoo/222577826387,20211124,"자연어처리바이블 14강 : 대화시스템1. 대화시스템이란?- 대화 시스템 : 자연어를 사용해 인간과 대화하는 시스템​- 사용자가 주도하는 사용자 주도 대화 시스템- 시스템이 주도하는 시스템 주도 대화 시스템ex) 삼성의 빅스비, 애플의 시리, 구글의 나우, 마이크로소프트의 코타나, KT의 기가지니, 네이버의 클로바​ 2. 대화 시스템의 구현- 음성인식 시스템 (Speech Recognition): 사용자의 음성신호를 텍스트로 변환​- 자연어 이해 시스템 (Natural Language Understanding): 변환된 텍스트 문장을 의미 표현으로 변환: 슬롯 채우기 → ex) 예매 장소, 상영 시간, 티켓 장 수, 나이 - 대화 관리 시스템 (Dialog manager): 대화의 문맥, 흐름, 적용 도메인, 생략 복원, 중의성 해소, 예외처리 등을 담당​1) 규칙 기반 접근방법(1) Finte State Automata (FSA)을 이용하는 방법: 규칙 기반으로 모든 경우에 수에 대해 규칙을 정해두고 경우에 따라 대화의 문맥과 순서를 표현 (2) Frame 기반의 대화 관리 시스템: 규칙과 규칙간의 우선순위가 있고, 사용자에게 질문을 하여 문맥을 이해하거나 사용자가 주도하여 정보를 요구하는 형식 ​2) 데이터 기반 접근방법(1) 강화학습을 이용한 방법: 강화학습 → 이전에는 몰랐지만 직접 시도하면서 행동과 그 결과로 나타나는 좋은 보상 사이의 상관관계를 학습하는 것​- Markov Decision Process(MDP) → 순차적인 학습을 위한 문제해결 과정 에이전트가 관찰 가능한 상태에이전트가 특정 상태에서 할 수 있는 행동환경이 에이전트에게 주는 보상정보에이전트가 어떠한 상태에서 어떠한 행동을 해서 다른 상태에 도달할 확률보상을 구분할 수 있게 해주는 감가율​3) 딥러닝 기반 접근방법 ​- 음성 합성 시스템 (Text-to-Speech Synthesis): 사람과 유사하며 사용자가 듣기 편한 음성으로 변환​ 3. 대화 시스템의 평가- Slot Error Rate: 문장이 slot filling을 잘했는지 평가​- End-to-end evaluation: 대화가 잘 이루어졌는지 평가 ​ 4. 대화 시스템의 분류- 기능 대화 (Task-oriented Dialog): 목적이 분명함: Domain-specific → ex. Airport/Movie: 답이 하나 ​- 일상대화 (Social conversation): 대화 자체가 목적임: open-domain: 여러 개의 정답: dialog skill (재미, 추천, 감정, 위로)가 요구됨.​#자연어처리바이블 #대화시스템 #자연어처리 #자연어처리바이블대화시스템 "
"1년 안에 AI 빅데이터 전문가가 되는 법, 서대호; 비전공자가 AI 인공지능 빅데이터를 전반을 엿볼 수 있는 로드맵 / Aiverse  ",https://blog.naver.com/allishere/222913989248,20221029,"​#비전공자 ​저자소개 : 서대호 한양대 정보시스템학과 전공한양대 석사한국과학기술원, 모비젠, 전자부품연구원 연세대 정보대학원 박사 기업 데이터 분석 컨설팅 중데이터 분석 솔루션 개발 중 ​요약 AI 지식은 아직은 한국보다 미국 중국에..서적과 강의를 통해서 학습 가능하며, 라이브러리를 코드로 돌려봐야.. 전문분야 서적 yes24 세권, 아마존 300권. 100배.. ​​본문 ​우리나라에는 인공지능 전문가가 부족하다. 기업이며 국가며 인재가 필요하다는 곳은 많은데 아직 전문 인력이 턱없이 부족한 형국이다. ​그렇다면 어떻게 하면 인공지능 및 빅데이터 전문가가 될 수 있을까? ​이 책에서 답을 찾을 수 있다. ​핵심 내용에 대해 살펴보도록 하겠다. (핵심내용 요약정리) 수요에 비해 공급이 부족한 지금, 빅데이터 전문가 될 수 있는 기회 빅데이터 학원 광고에 현혹되지 않고, 공부 시작할 수 있도록 집필자격증 취득은 실무에 큰 도움은 안된다. 국내 대학 인공지능 AI 대학원 개설은 실리콘 벨리에서 거절을 당했다. 1억원 남짓한 국내 교수 연봉으로 최소 3배에서 10배 높은 연봉의 외국 인재를 모셔올 수 없었음. 기존 타과 교수로 채움회사는 당신을 전문가로 만들어주지 않는다. 회사 특성상 단순 반복업무. 박사급이 아닌 학사 석사급이라면 이런 업무 더 많을 것임. 자신에 맞는 공부법으로 하라. 무크 스탠포드로 하든 외국 서적으로 하든. 전처리기술에는 정제, 통합, 축소, 변환이 있다. 책에 있는 코드를 일일이 타자 쳐가며 따라해볼 필요 없다. 코드 이해하고 넘어가는 수준이면 된다. 엄청나게 뛰어난 정확도로 인해 딥러닝은 전통적인 학습 알고리즘 대비 사용량이 급격히 증가하였다. 예를 들면 이미지넷. 좀 더 깊게 공부하기 위해서 실제 예제 세트 코드로 돌려보라. 딥러닝 구현을 위한 라이브러리는 텐서플로, 케라스, 카페, 파이토치, 티아노 등 다양함. 데이터베이스의 기본적인 이론을 익혀라. 아직까지 웬만한 프로젝트는 기존 RDB 수준에서 해결된다. 특히 MySQL은 무료기에 시작단계 프로젝트는 MySQL을 DB로 채택한다. 자신만의 전문분야를 정하라. 캐글 경연대회를 통해 경험을 쌓으라. 캐글 경연대회는 수시로 열리며, 실제 비즈니스 환경에 도출되는 데이터 세트이다. AI 빅데이터 전문가 되기 위해 회사 취업이나 대학원 진학하면 한 회사, 랩실에서 한가지 연구 분야만 볼 것이다. 책은 느리고 논문은 빠르고 많다. 책을 통해 전체 개괄을 이해하고 논문을 읽으라. 구글 학술 검색을 통하면 웬만한 논문 다운 받을 수 있음, 다운 안되는 논문은 대학교 포털 이용, 무료도 아니고 학생도 아니라면 유료지불 또는 SCI-HUB. 너무 양이 많다면, 인용 수 많거나 명망있는 저널이거나, 선행연구 자세히 나와있는 논문 정하고 따라가며 읽으라. 논문은 많이 읽으면 읽을수록 좋다. 박사 학위 논문을 쓰려면 연구 분야에 관련 논문을 1000개 읽어야 한다는 말이 있다. 핵심내용 정리하고 꼭 필요 내용은 별도 노트에 정리해둔다. 기존의 아이디어에 자신의 아이디어를 얹어라. 논문투고 타깃저널을 정하고, 처음시작부터 최초투고까지 1~2개월이면 충분하다. 빠르게 시작하고 빠르게 끝내자. ​언제 어느 상황에 있든 주저하지 말고 시작하라. 당신은 어제와 다를 것이며 할 수 있다. ​더 자세한 정보는 더보기 란을 클릭하여 확인하세요. 지금까지 에이아이버스 였습니다. 감사합니다. ​​대표분야 음성인식 Speech Recognition영상인식 Computer Vision자연어처리 NLP, Natural Language Processing ​기업LG전자-KAIST 인공지능 고급과정 ​문제해결추천 알고리즘 ​Reference서대호, 1년 안에 AI 빅데이터 전문가가 되는 법, 반니 (2020) ​추천책빅데이터의 기초, 개념, 동인, 기법 (시그마프레스)인공지능 시대의 비즈니스 전략 (더퀘스트) 데이터 마이닝 개념과 기법 (에이콘출판) 패턴인식, 교보문고 데이터 마이닝 기법과 응용, 한나래 파이썬 라이브러리를 활용한 데이터 분석, 한빛미디어파이썬으로 데이터 주무르기, 비제이플릭빅데이터 분석 도구 R 프로그래밍, 에이콘출판사R라뷰, 더알음 (이론공부) 딥러닝 제대로 시작하기, 제이펍(강의) 홍콩 과기대 김성훈 교수, 인프런밑바닥부터 시작하는 딥러닝, 한빛미디어딥러닝 제대로 시작하기 케라스 창시자에게 배우는 딥러닝, 길벗 몽고디비 인 액션, 제이펍 아마존 웹 서비스 AWS Disco very Book, 정보문화사 깔끔한 파이썬 탄탄한 백엔드, 비제이리퍼블릭 ​추천데이터세트무비렌즈 데이터세트 ​관련기관한국출판문화산업진흥원​ "
안드레 카파시 블로그 - 소프트웨어 2.0 (Andrej Karpathy Software 2.0) ,https://blog.naver.com/economic_moat/222856134470,20220823,"I sometimes see people refer to neural networks as just “another tool in your machine learning toolbox”. They have some pros and cons, they work here or there, and sometimes you can use them to win Kaggle competitions. Unfortunately, this interpretation completely misses the forest for the trees. Neural networks are not just another classifier, they represent the beginning of a fundamental shift in how we develop software. They are Software 2.0.​저는 가끔 사람들이 신경망을 ""기계 학습의 다양한 도구들 중 그저 한가지""라고 언급하는 것을 봅니다. 신경망은 장단점을 가지고 있고, 신경망은 이곳 저곳에서 쓸 수 있습니다. 그리고 때때로 여러분은 신경망들을 *Kaggle 대회(데이터 분석 대회)에서 우승하는 데 사용할 수 있습니다. 불행하게도, 이러한 관점은 숲을 보지못하고 나무만 보는데 그칩니다. 신경망은 단순히 또 다른 분류기가 아니라, 우리가 소프트웨어를 개발하는 방법의 근본적인 변화의 시작을 나타냅니다. 소프트웨어 2.0입니다.​* Kaggle 대회 : 구글 코리아가 후원하고, 캐글 코리아(비영리 페이스북 온라인 커뮤니티)가 진행하는 데이터 사이언스 대회​The “classical stack” of Software 1.0 is what we’re all familiar with — it is written in languages such as Python, C++, etc. It consists of explicit instructions to the computer written by a programmer. By writing each line of code, the programmer identifies a specific point in program space with some desirable behavior.​소프트웨어 1.0의 ""클래식 스택""은 우리 모두가 잘 알고 있는 것입니다. 이 스택은 Python, C++ 등과 같은 언어로 작성되었습니다. 그것은 프로그래머가 작성한 컴퓨터에 대한 명시적인 명령으로 구성됩니다. 코드의 각 줄을 작성함으로써 프로그래머는 프로그램 공간의 특정 포인트를 바람직한 동작으로 식별합니다. ​In contrast, Software 2.0 is written in much more abstract, human unfriendly language, such as the weights of a neural network. No human is involved in writing this code because there are a lot of weights (typical networks might have millions), and coding directly in weights is kind of hard (I tried).​대조적으로, 소프트웨어 2.0은 신경망의 가중치와 같은 훨씬 추상적이고 사람들에게 친숙하지 않은 언어로 작성되었습니다. 많은 가중치가 있고(일반적인 네트워크는 수백만 개를 가질 수 있음), 가중치를 직접 코딩하는 것은 어렵기 때문에 이 코드를 작성하는 데 사람이 관여하지 않습니다(저는 노력했습니다).​ ​Instead, our approach is to specify some goal on the behavior of a desirable program (e.g., “satisfy a dataset of input output pairs of examples”, or “win a game of Go”), write a rough skeleton of the code (i.e. a neural net architecture) that identifies a subset of program space to search, and use the computational resources at our disposal to search this space for a program that works. In the case of neural networks, we restrict the search to a continuous subset of the program space where the search process can be made (somewhat surprisingly) efficient with backpropagation and stochastic gradient descent.​대신, 우리의 접근 방식은 바람직한 프로그램의 동작에 대한 일부 목표를 지정하고(예: ""예시처럼 입력과 출력 쌍의 데이터 세트를 만족한다"" 또는 ""바둑 게임에서 승리한다""), 검색할 프로그램 공간의 하위 집합을 식별하는 코드의 대략적인 뼈대(즉, 신경망 아키텍처)를 작성하고, 그리고 우리가 마음대로 사용할 수 있는 계산 자원을 사용하여 이 공간에서 작동하는 프로그램을 검색합니다. 신경망의 경우 역전파 및 확률적 경사하강법으로 검색 프로세스를 (다소 놀랍도록) 효율적으로 만들 수 있는 프로그램 공간의 연속적인 하위 집합으로 검색을 제한합니다.​ ​To make the analogy explicit, in Software 1.0, human-engineered source code (e.g. some .cpp files) is compiled into a binary that does useful work. In Software 2.0 most often the source code comprises 1) the dataset that defines the desirable behavior and 2) the neural net architecture that gives the rough skeleton of the code, but with many details (the weights) to be filled in. The process of training the neural network compiles the dataset into the binary — the final neural network. In most practical applications today, the neural net architectures and the training systems are increasingly standardized into a commodity, so most of the active “software development” takes the form of curating, growing, massaging and cleaning labeled datasets. This is fundamentally altering the programming paradigm by which we iterate on our software, as the teams split in two: the 2.0 programmers (data labelers) edit and grow the datasets, while a few 1.0 programmers maintain and iterate on the surrounding training code infrastructure, analytics, visualizations and labeling interfaces.​유추를 분명히 하기 위해, 소프트웨어 1.0에서는 사람이 쓴 소스 코드(예: 일부 .cpp 파일)가 유용한 작업을 하는 이진수로 컴파일됩니다. 소프트웨어 2.0에서 소스 코드는 대부분 1) 바람직한 동작을 정의하는 데이터 세트와 2) 코드의 대략적인 골격을 제공하는 신경망 아키텍처로 구성되지만, 채워야 할 많은 세부 사항(가중치)이 있습니다. 신경망 훈련 과정은 최종 뉴럴넷 네트워크에서 데이터 세트를 이진수로 컴파일합니다. 오늘날 대부분의 실용적인 애플리케이션에서 신경망 아키텍처와 훈련 시스템은 점점 더 표준화되므로, 대부분의 능동적인 ""소프트웨어 개발""은 레이블이 지정된 데이터 세트를 큐레이션, 증가, 마사지 및 클리닝하는 형태를 취합니다. 이는 2.0 프로그래머(데이터 레이블러)가 데이터 세트를 편집 및 확장하는 동시에 일부 1.0 프로그래머가 주변 교육 코드 인프라, 분석, 시각화 및 레이블링 인터페이스에서 유지 및 반복됨에 따라 소프트웨어에서 반복하는 프로그래밍 패러다임을 근본적으로 변화시키고 있습니다.​요약 :소프트웨어 1.0 사람이 쓴 코드 -> 이진수로 컴파일됨소프트웨어 2.0 데이터 세트 -> 제일 마지막 단계에서 이진수로 컴파일됨점차 신경망 아키텍처는 표준화된 걸 써가면서, 소프트웨어 개발에 있어서 중요한 건 데이터 세트를 큐레이션하고, 데이터를 증가시키고, 마사지(조정)하고, 클리닝하는 형태로 되고 있음. 데이터 레이블러(2.0 프로그래머)가 데이터 세트 편집(라벨링), 기존 1.0프로그래머가 시각화, 분석, 인터페이스 관리 등을 하는 식으로 프로그래밍의 패러다임이 바뀌고 있음.​It turns out that a large portion of real-world problems have the property that it is significantly easier to collect the data (or more generally, identify a desirable behavior) than to explicitly write the program. Because of this and many other benefits of Software 2.0 programs that I will go into below, we are witnessing a massive transition across the industry where of a lot of 1.0 code is being ported into 2.0 code. Software (1.0) is eating the world, and now AI (Software 2.0) is eating software.​실제 real world의 문제 상당 부분은 프로그램을 명시적으로 작성하는 것보다 데이터를 수집하는 것(또는 보다 일반적으로 바람직한 행동을 식별하는 것)이 훨씬 더 쉽다는 특성을 가지고 있는 것으로 나타났습니다. 이러한 이점과 소프트웨어 2.0 프로그램의 많은 다른 이점 때문에, 우리는 1.0 코드가 2.0 코드로 바뀌는 산업 전반에 걸쳐 대규모 전환을 목격하고 있습니다. 소프트웨어(1.0)가 세상을 씹어먹고 있고, 이제 AI(소프트웨어 2.0)가 소프트웨어를 씹어먹고 있습니다.​요약 : real world 문제들을 코드로 내가 작성해서 푸는 것보다, 데이터를 수집하는 것이 쉽다. 그래서 기존 소프트웨어 1.0에서 소프트웨어 2.0으로 전환되고 있음.  ​​Ongoing transition 지속적인 전환​Let’s briefly examine some concrete examples of this ongoing transition. In each of these areas we’ve seen improvements over the last few years when we give up on trying to address a complex problem by writing explicit code and instead transition the code into the 2.0 stack.​현재 진행 중인 이러한 전환의 몇 가지 구체적인 예를 간략히 살펴보겠습니다. 이러한 각 영역에서 우리는 명시적 코드를 작성하여 복잡한 문제를 해결하려는 시도를 포기하고 대신 코드를 2.0 스택으로 전환함으로써 지난 몇 년 동안 개선된 것을 볼 수 있었습니다.​Visual Recognition used to consist of engineered features with a bit of machine learning sprinkled on top at the end (e.g., an SVM). Since then, we discovered much more powerful visual features by obtaining large datasets (e.g. ImageNet) and searching in the space of Convolutional Neural Network architectures. More recently, we don’t even trust ourselves to hand-code the architectures and we’ve begun searching over those as well.​비전 인식은 그동안 직접 코딩하고 마지막에 머신 러닝을 조금 추가하는 식(예: SVM;서포트 백터 머신 - 어느 카토고리에 속할지 판단하는 이진 선형 분류 모델)으로 해왔었습니다. 그러고 나서 이후에는 대규모 데이터 세트를 얻고 컨볼루션 신경망 아키텍처의 공간을 검색하여 훨씬 강력한 시각적 피처들(예: ImageNet)을 발견했습니다. 최근에는 아키텍처들을 손으로 코딩하는 것을 신뢰조차 하지 않고 있습니다. 그리고 우리는 이에 대해서도 조사하기 시작했습니다.(링크 : 이미지 분류에 신경망을 사용하는 하는 모델 논문)​Speech recognition used to involve a lot of preprocessing, gaussian mixture models and hidden markov models, but today consist almost entirely of neural net stuff. A very related, often cited humorous quote attributed to Fred Jelinek from 1985 reads “Every time I fire a linguist, the performance of our speech recognition system goes up”.​음성 인식은 많은 전처리, 가우스 혼합 모델 및 은닉 마르코프 모델을 포함해왔지만, 오늘날에는 거의 전적으로 신경망으로 구성됩니다. 1985년 프레드 젤린크가 쓴 유머러스한 인용문에는 ""언어학자를 해고할 때마다, 우리의 음성 인식 시스템의 성능이 올라간다""라고 쓰여 있습니다.​*가우시안 혼합 모델 : 가우시안 분포를 따른다고 보고, 이를 통해 평균과 분산 등 데이터의 확률분포를 구하는 방법. 혼합된 정보를 다양한 집단으로 분류하는 확률 모델*은닉 마르코프 모델 : 시스템이 은닉된 상태와 관찰 가능한 결과 두가지 요소로 구성되었다고 보고, 마르코프 성질을 이용하여 예측하는 확률적 통계 모델​Speech synthesis has historically been approached with various stitching mechanisms, but today the state of the art models are large ConvNets (e.g. WaveNet) that produce raw audio signal outputs.​음성 합성은 역사적으로 다양한 스티칭(바늘땀;직접 손으로 하는) 메커니즘으로 접근되어 왔지만, 오늘날 최첨단 모델은 raw 오디오 신호 출력을 생성하는 ​large ConvNet(예: WaveNet)입니다.​*WaveNet : 딥마인드에서 개발한 모델. 사람의 목소리를 학습해 개별 단어 단위의 발엄, 악센트 뿐 아니라 문장 단위에서의 억양까지 매우 정교한 수준으로 인간처럼 언어를 구사 (WaveNet : A Generative Model for Raw Audio 2016/9/8)​Machine Translation has usually been approaches with phrase-based statistical techniques, but neural networks are quickly becoming dominant. My favorite architectures are trained in the multilingual setting, where a single model translates from any source language to any target language, and in weakly supervised (or entirely unsupervised) settings.​기계 번역은 일반적으로 구문 기반 통계 기술을 사용하는 접근 방식이었지만, 신경망은 빠르게 우세해지고 있습니다. 제가 좋아하는 아키텍처는 단일 모델이 모든 소스 언어에서 모든 대상 언어로 번역하는 다국어 설정과 조금의 지도학습(또는 완전 비지도학습의) 설정에서 훈련됩니다.​Games. Explicitly hand-coded Go playing programs have been developed for a long while, but AlphaGo Zero (a ConvNet that looks at the raw state of the board and plays a move) has now become by far the strongest player of the game. I expect we’re going to see very similar results in other areas, e.g. DOTA 2, or StarCraft.​게임. 명백히 손으로 코딩된 바둑 프로그램이 개발된 지 오래지만 알파고 제로(게임의 현 상황을 보고 동작을 하는 ConvNet)는 이제 게임의 최강자가 되었습니다. DOTA 2, 스타크래프트와 같은 다른 분야에서도 비슷한 결과를 볼 수 있을 것으로 예상합니다.​Databases. More traditional systems outside of Artificial Intelligence are also seeing early hints of a transition. For instance, “The Case for Learned Index Structures” replaces core components of a data management system with a neural network, outperforming cache-optimized B-Trees by up to 70% in speed while saving an order-of-magnitude in memory.​데이터베이스. 인공지능 외부의 더 전통적인 시스템들도 변화의 초기 징후를 보고 있습니다. 예를 들어, ""The Case for Learned Index Structures(2018)""는 데이터 관리 시스템의 핵심 구성 요소를 신경망으로 대체하여 캐쉬에 최적화된 B-Tree를 최대 70%의 속도로 능가하는 동시에 메모리 용량을 절약합니다.​*B tree : 다방향 탐색 트리. 대용량의 파일을 효율적으로 검색하고 갱신하기 위해 고안된 트리 형태의 자료 구조​You’ll notice that many of my links above involve work done at Google. This is because Google is currently at the forefront of re-writing large chunks of itself into Software 2.0 code. “One model to rule them all” provides an early sketch of what this might look like, where the statistical strength of the individual domains is amalgamated into one consistent understanding of the world.​위의 링크 중 많은 부분이 구글에서 수행한 작업이라는 것을 알게 될 것입니다. 이는 구글이 현재 소프트웨어 2.0 코드로 많은 부분을 재작성하는 데 앞장서고 있기 때문입니다. ""모든 것을 지배하는 하나의 모델(2017)""은 개별 도메인이 세계에 대한 하나의 일관된 이해로 통합될 수 있는지에 대한 초기 스케치를 제공합니다.​요약 : 비전인식, 음성인식, 음성합성, 기계번역, 게임, 데이터 베이스 등 다양한 영역에서 소프트웨어1.0에서 소프트웨어2.0으로 전환되고 있음. 그리고 구글이 이에 앞장서고 있음. 더욱이 구글은 이미지 분류, 인식, 번역 등의 다양한 유형의 훈련 데이터를 사용하여 다양한 작업을 할 수 있는 하나의 모델에 대한 논문도 내면서 각각의 모델들이 각각의 도메인을 잘 하는 게 아니라 하나의 모델이 모든 분야를 어떻게 다 잘할 수 있는지 보여줌. ​​The benefits of Software 2.0 소프트웨어 2.0의 이점​Why should we prefer to port complex programs into Software 2.0? Clearly, one easy answer is that they work better in practice. However, there are a lot of other convenient reasons to prefer this stack. Let’s take a look at some of the benefits of Software 2.0 (think: a ConvNet) compared to Software 1.0 (think: a production-level C++ code base). Software 2.0 is:​복잡한 프로그램을 소프트웨어 2.0으로 전환하는 것을 선호하는 이유는 무엇일까요? 분명히, 한 가지 쉬운 대답은 소프트웨어 2.0 스택이 실제로 더 잘 작동한다는 것입니다. 그러나 이 스택을 선호하는 다른 편리한 이유들이 많이 있습니다.​ 소프트웨어 1.0(실제 수준 C++ 코드 기반)과 비교하여 소프트웨어 2.0(ConvNet)의 몇 가지 이점을 살펴보겠습니다. 소프트웨어 2.0은 다음과 같습니다.​Computationally homogeneous. A typical neural network is, to the first order, made up of a sandwich of only two operations: matrix multiplication and thresholding at zero (ReLU). Compare that with the instruction set of classical software, which is significantly more heterogenous and complex. Because you only have to provide Software 1.0 implementation for a small number of the core computational primitives (e.g. matrix multiply), it is much easier to make various correctness/performance guarantees.​계산적으로 동일합니다. 일반적인 신경망은, 가장 먼저, 행렬 곱셈과 0에서의 임계값 처리(ReLU)라는 두 가지 연산으로 구성됩니다. 훨씬 더 이질적이고 복잡한 고전 소프트웨어의 명령 세트와 비교해 보십시오. 아주 적은 양의 핵심 계산 기본 요소(예: 행렬 곱하기)에 대해서만 소프트웨어 1.0 구현을 제공하면 되므로 다양한 정확성/성능 보장을 훨씬 쉽게 수행할 수 있습니다.​요약 : 계산을 한다는 건 동일한데, 신경망은 덜 복잡 (두가지 연산으로 구성) vs 소프트웨어 1.0은 더 복잡. 그러므로 더 쉽고 정확하게 수행가능​Simple to bake into silicon. As a corollary, since the instruction set of a neural network is relatively small, it is significantly easier to implement these networks much closer to silicon, e.g. with custom ASICs, neuromorphic chips, and so on. The world will change when low-powered intelligence becomes pervasive around us. E.g., small, inexpensive chips could come with a pretrained ConvNet, a speech recognizer, and a WaveNet speech synthesis network all integrated in a small protobrain that you can attach to stuff.​실리콘으로 굽는 것이 간단합니다. 결과적으로, 신경망의 명령 집합은 상대적으로 작기 때문에, 사용자 지정 ASIC, 뉴로모픽 칩 등을 사용하여 이러한 네트워크를 실리콘에 훨씬 더 가깝게 구현하는 것이 훨씬 쉽습니다. 저전력 지능이 우리 주변에 만연할 때 세상은 바뀔 것입니다. 예를 들어, 작고 저렴한 칩(작은 프로토 브레인)에 사전학습된 ConvNet, 음성 인식기, 그리고 WaveNet 음성 합성 네트워크를 통합시켜서 여러분의 물건에 부착할 수 있게 할 수도 있습니다. ​*실리콘 참고 : 애플 실리콘은 ARM 아키텍처를 사용하여 애플이 설계한 단일 칩 체제(SoC)와 시스템 인 패키지(SiP) 프로세서임. 애플의 아이폰, 아이패드, 애플워치, 매킨토시 등의 플랫폼에 기본적으로 사용되며, 홈팟, 아이팟 터치, 애플 TV 등의 제품에도 사용됨​요약 : 신경망을 작고 저렴한 칩에 넣을 수 있음. 물건에 부착할 수 있게도 가능.​Constant running time. Every iteration of a typical neural net forward pass takes exactly the same amount of FLOPS. There is zero variability based on the different execution paths your code could take through some sprawling C++ code base. Of course, you could have dynamic compute graphs but the execution flow is normally still significantly constrained. This way we are also almost guaranteed to never find ourselves in unintended infinite loops.​실행 시간이 일정합니다. 일반적인 신경망 순방향 패스의 모든 반복은 정확히 동일한 양의 FLOPS(1초에 수행할 수 있는 연산속도 나타내는 컴퓨터 성능 단위)를 사용합니다.  C++ 코드 베이스를 통해 취할 수 있는 다양한 실행 경로에 따라 변동성은 신경망에는 없습니다. 물론 동적 계산 그래프를 사용할 수 있지만 실행 흐름은 일반적으로 여전히 상당히 제한됩니다. 이러한 방식으로 우리는 또한 의도하지 않은 무한 루프에 있는 우리 자신을 결코 발견하지 못할 것이라고 거의 보장됩니다.​Constant memory use. Related to the above, there is no dynamically allocated memory anywhere so there is also little possibility of swapping to disk, or memory leaks that you have to hunt down in your code.​지속적인 메모리 사용입니다. 위와 관련하여, 동적으로 할당된 메모리가 없으므로 디스크로 스왑하거나 코드에서 찾아야 하는 메모리 누수가 발생할 가능성이 거의 없습니다.​요약 : 원래는 동적 메모리를 할당해서 (컴퓨터 프로그래밍에서 실행 시간 동안 사용할 메모리 공간을 할당하고, 끝나면 운영체제에 반납하고, 다음에 다시 재할당받아서 하는 식) 했었는데 신경망은 그럴 필요가 없음. ​It is highly portable. A sequence of matrix multiplies is significantly easier to run on arbitrary computational configurations compared to classical binaries or scripts.​뛰어난 가변성입니다. 행렬 곱셈 시퀀스는 기존 이진 파일이나 스크립트에 비해 임의 계산 배치에서 실행하기가 훨씬 쉽습니다.​It is very agile. If you had a C++ code and someone wanted you to make it twice as fast (at cost of performance if needed), it would be highly non-trivial to tune the system for the new spec. However, in Software 2.0 we can take our network, remove half of the channels, retrain, and there — it runs exactly at twice the speed and works a bit worse. It’s magic. Conversely, if you happen to get more data/compute, you can immediately make your program work better just by adding more channels and retraining.​매우 민첩합니다. C++ 코드는 누군가가 두 배 더 빨리(필요한 경우 성능 비용으로) 만들어 주기를 바랄 때 새로운 사양에 맞게 시스템을 조정하는 것은 매우 어렵습니다. 그러나 소프트웨어 2.0에서는 네트워크를 사용하여 채널의 절반을 제거하고 재교육할 수 있습니다. 정확히 두 배 속도로 실행되며 작동 능력이 조금 더 떨어집니다. 마법이죠. 반대로, 더 많은 데이터/컴퓨터를 얻는 경우 채널을 추가하고 다시 교육하는 것만으로도 프로그램이 즉시 더 잘 작동하도록 만들 수 있습니다.​요약 : C++은 두배 더 빨리 사양을 높이고 싶을 때, 시스템을 조정하는 게 무척 어려움. 반대로 소프트웨어 2.0은 성능 살짝 포기하는 대신 속도 2배 빠르게 할 수 있음. 그리고 반대로 더 많은 데이터, 컴퓨팅 파워 얻어서 다시 채널 추가하고 교육하면 성능을 다시 잘나오게 할 수 있음.​Modules can meld into an optimal whole. Our software is often decomposed into modules that communicate through public functions, APIs, or endpoints. However, if two Software 2.0 modules that were originally trained separately interact, we can easily backpropagate through the whole. Think about how amazing it could be if your web browser could automatically re-design the low-level system instructions 10 stacks down to achieve a higher efficiency in loading web pages. Or if the computer vision library (e.g. OpenCV) you imported could be auto-tuned on your specific data. With 2.0, this is the default behavior.​모듈은 최적의 전체로 혼합될 수 있습니다. 우리의 소프트웨어는 종종 접근가능 기능, API 또는 엔드포인트를 통해 통신하는 모듈로 분해됩니다. 그러나 원래 별도로 교육받았던 두 소프트웨어 2.0 모듈이 상호 작용하면 전체로 쉽게 역전파할 수 있습니다. 웹 브라우저가 웹 페이지 로딩에서 더 높은 효율성을 달성하기 위해 자동으로 하위 수준의 시스템 명령 10개를 다시 설계할 수 있다면 얼마나 놀라운 일인지 생각해 보십시오. 또는 새로 가져온 컴퓨터 비전 라이브러리(예: OpenCV)가 특정 데이터에 대해 자동 조정될 수 있는 경우입니다. 소프트웨어 2.0에서는 이것이 기본 동작입니다.​요약 : 소프트웨어 스택은 보통 여러가지 *모듈들로 분해되어 있음. 이렇게 별개로 트레이닝했던 모듈 두개를 하나로 만들 수 있음. 분해했다가 합치는 것이 소프트웨어 2.0에서는 쉬움.​*모듈 : 컴퓨터 분야에서의 모듈이란 독립적인 하나의 소프트웨어 혹은 하드웨어 요소를 말함. 컴퓨터 시스템에서, 부품을 떼 내어 교환이 쉽도록 설계되어 있을 때의 각 부분을 말함.​It is better than you. Finally, and most importantly, a neural network is a better piece of code than anything you or I can come up with in a large fraction of valuable verticals, which currently at the very least involve anything to do with images/video and sound/speech.​그것은 당신보다 낫습니다. 마지막으로, 그리고 가장 중요한 것은 신경망은 현재 이미지/비디오 및 사운드/음성과 관련된 많은 가치 있는 수직적인(verticals; 계층적인, 여러가지) 부분에서 여러분이나 제가 생각해낼 수 있는 그 어떤 것보다도 더 나은 코드라는 것입니다.​​The limitations of Software 2.0 소프트웨어 2.0의 한계점​The 2.0 stack also has some of its own disadvantages. At the end of the optimization we’re left with large networks that work well, but it’s very hard to tell how. Across many applications areas, we’ll be left with a choice of using a 90% accurate model we understand, or 99% accurate model we don’t.​소프트웨어 2.0 스택에도 몇 가지 단점이 있습니다. 최적화가 끝날 때쯤에는 잘 작동하는 대규모 네트워크가 남아 있지만, 그 방법을 말하는 것은 매우 어렵습니다. 많은 애플리케이션 영역에서, 우리는 우리가 이해하고 있는 90%의 정확한 모델 또는 우리가 이해하지 못하는 99%의 정확한 모델을 선택할 수 있습니다.​The 2.0 stack can fail in unintuitive and embarrassing ways ,or worse, they can “silently fail”, e.g., by silently adopting biases in their training data, which are very difficult to properly analyze and examine when their sizes are easily in the millions in most cases.​2.0 스택은 직관적이지 않고 당황스러운 방식으로 실패할 수 있으며, 더 나쁜 경우, 대부분의 경우 크기가 수백만에 달했을 때 적절하게 분석하고 조사하기 어려운 교육 데이터에 대한 biases들을 묵묵히 받아들여 ""조용히 실패""할 수 있습니다.​요약 : 트레이닝 시키는 데이터가 엄청 많다보니 다 분석하고 조사하기 어려운데, 그런 데이터들에 있는 biases들로 인해 실패할 수 있음. 우리가 예기치 못한 방식으로. (이걸 왜 못해?하는 거) 그리고 이러한 원인을 알기도 어려움.​Finally, we’re still discovering some of the peculiar properties of this stack. For instance, the existence of adversarial examples and attacks highlights the unintuitive nature of this stack.​마지막으로, 우리는 여전히 이 스택의 몇 가지 독특한 특성을 발견하고 있습니다. 예를 들어, 적대적 사례(일부러 인공지능이 못맞추게 하려고 하는 경우 ex, 자율주행차 방해하려 stop 티셔츠 입고 길가에 서있기) 와 공격의 존재는 이 스택의 비직관적 특성을 강조합니다.​​Programming in the 2.0 stack 2.0 스택에서의 프로그래밍​Software 1.0 is code we write. Software 2.0 is code written by the optimization based on an evaluation criterion (such as “classify this training data correctly”). It is likely that any setting where the program is not obvious but one can repeatedly evaluate the performance of it (e.g. — did you classify some images correctly? do you win games of Go?) will be subject to this transition, because the optimization can find much better code than what a human can write.​소프트웨어 1.0은 우리가 작성하는 코드입니다. 소프트웨어 2.0은 평가 기준에 따라 최적화에 의해 작성된 코드입니다(예: ""이 교육 데이터를 올바르게 분류해라""). 프로그램이 명확하지는 않지만 성능을 반복적으로 평가할 수 있는 모든 세팅들은(평가 예시: 이 이미지들을 올바르게 분류했는지? 바둑 게임에서 이겼는지?) 이러한 전환의 대상이 될 것입니다. 왜냐하면 최적화(소프트웨어2.0)는 인간이 쓸 수 있는 것보다 훨씬 더 나은 코드를 찾을 수 있기 때문입니다.​ ​The lens through which we view trends matters. If you recognize Software 2.0 as a new and emerging programming paradigm instead of simply treating neural networks as a pretty good classifier in the class of machine learning techniques, the extrapolations become more obvious, and it’s clear that there is much more work to do.​트렌드를 보는 렌즈가 중요합니다. 소프트웨어 2.0을 단순히 신경망을 기계 학습 기술 분야에서 꽤 좋은 분류기(classifier; 그냥 머신러닝도구)로 취급하는 대신 새롭게 뜨는 색다른 프로그래밍 패러다임으로 인식한다면, 일반화(extrapolations; 외삽법, 이미 알고 있는 사실의 확대 적용)은 더욱 분명해지고, 해야 할 일이 훨씬 더 많다는 것은 분명합니다.​In particular, we’ve built up a vast amount of tooling that assists humans in writing 1.0 code, such as powerful IDEs with features like syntax highlighting, debuggers, profilers, go to def, git integration, etc. In the 2.0 stack, the programming is done by accumulating, massaging and cleaning datasets. For example, when the network fails in some hard or rare cases, we do not fix those predictions by writing code, but by including more labeled examples of those cases. Who is going to develop the first Software 2.0 IDEs, which help with all of the workflows in accumulating, visualizing, cleaning, labeling, and sourcing datasets? Perhaps the IDE bubbles up images that the network suspects are mislabeled based on the per-example loss, or assists in labeling by seeding labels with predictions, or suggests useful examples to label based on the uncertainty of the network’s predictions.​특히, 우리는 구문 강조, 디버거, 프로파일러, go to def, git 통합 등과 같은 기능을 갖춘 강력한 IDE와 같이 1.0 코드 작성을 지원하는 방대한 양의 도구를 구축했습니다. 2.0 스택에서는 데이터 세트를 축적, 마사지 및 클리닝하여 프로그래밍을 수행합니다. 예를 들어, 일부 어렵거나 드문 경우에서 네트워크가 실패할 경우, 우리는 코드를 작성하여 이러한 예측을 수정하지 않고 그러한 경우에 대한 더 많은 레이블이 지정된 예를 포함함으로써 수정합니다. 데이터셋의 축적, 시각화, 정리, 레이블링 및 소싱에 있어 모든 워크플로우를 지원하는 최초의 소프트웨어 2.0 IDE를 개발할 사람은 누구일까요? 아마도 IDE는 네트워크에서 사례별 loss에 따라 잘못 레이블링된 것으로 의심되는 이미지를 늘리거나(버블링), 예측이 있는 레이블을 추가해서 레이블링을 돕거나, 네트워크 예측의 불확실성에 따라 레이블링할 수 있는 유용한 예를 제안합니다.​Similarly, Github is a very successful home for Software 1.0 code. Is there space for a Software 2.0 Github? In this case repositories are datasets and commits are made up of additions and edits of the labels.​마찬가지로 깃허브는 소프트웨어 1.0 코드의 매우 성공적인 본거지입니다. 소프트웨어 2.0 깃허브를 위한 공간이 있을까요? 이 경우 리포지토리(Repositories)는 데이터 세트이며 커밋(Commits)은 레이블의 추가 및 편집으로 구성됩니다.​*리포지토리 : 정보를 모아 놓고 서로 공유할 수 있게 한 정보의 저장소*커밋 : 스마트 드라이브 따위와 같은 캐시 기억 장치에다가 디스크에 정보를 저장하라고 알려 주는 명령어​Traditional package managers and related serving infrastructure like pip, conda, docker, etc. help us more easily deploy and compose binaries. How do we effectively deploy, share, import and work with Software 2.0 binaries? What is the conda equivalent for neural networks?​전통적인 *패키지 관리자와 Pip, Conda, Docker 등과 같은 관련 서비스 인프라들은 우리가 이진파일을 보다 쉽게 배포하고 구성할 수 있게 도와줍니다. 소프트웨어 2.0를 효과적으로 배포, 공유, 가져오기 및 작동하려면 어떻게 해야 할까요? 신경망에 있어서 *Conda와 비슷한 건 무엇일까요?​*package manager : 시스템용 시스템 프로그램을 일관된 방식으로 설치, 업그레이드, 구성 및 제거하는 프로세스를 자동화하는 소프트웨어 도구 모음. 패키지 관리자는 패키지, 소프트웨어 배포 및 아카이브 파일의 데이터를 처리함.*Conda : 패키지들을 설치하고 관리할 수 해주는 가상환경. 처음에는 파이썬을 위해 개발되었지만, 어떤 언어든 사용 가능. Python, R, Ruby, Lua, Scalca, Java, Javascript, C/C++, FORTRAN 등 많은 언어들의 의존성, 패키지, 환경을 관리해줌.​In the short term, Software 2.0 will become increasingly prevalent in any domain where repeated evaluation is possible and cheap, and where the algorithm itself is difficult to design explicitly. There are many exciting opportunities to consider the entire software development ecosystem and how it can be adapted to this new programming paradigm. And in the long run, the future of this paradigm is bright because it is increasingly clear that when we develop AGI, it will certainly be written in Software 2.0.​단기적으로 소프트웨어 2.0은 반복 평가가 가능하고 저렴하며 알고리즘 자체가 명시적으로 설계하기 어려운 모든 영역에서 점점 더 널리 보급될 것입니다. 전체 소프트웨어 개발 에코시스템과 이 새로운 프로그래밍 패러다임에 어떻게 적응할 수 있는지 고려할 수 있는 흥미로운 기회가 많이 있습니다. 그리고 장기적으로 볼 때, 우리가 AGI를 개발할 때, 그것은 확실히 소프트웨어 2.0으로 작성될 것이 분명하기 때문에 이 패러다임의 미래는 밝습니다.​ Software 2.0I sometimes see people refer to neural networks as just “another tool in your machine learning toolbox”. They have some pros and cons, they…karpathy.medium.com ​ "
아티보 음성 언어 인식 ,https://blog.naver.com/suleika/222110505812,20201008,"안녕하세요.추석 연휴가 끝나고 또 한글날을 맞이하여 다시 짧은 연휴가 시작되었습니다.ktx 기차표 예매도 할 수 없을 정도로 좌석 매진이라던데 코로나 시기에 있는게 맞나 싶기도 합니다.코로나보다 그 후유증이 더 심각하다고 하는 소식을 들었는데 우리나라에서도 코로나에 감염되었다가 완치된 초등학생 2명이 소아청소년 다기관염증증후군이라는 희귀 후유증을 보이고 있다고 하는 뉴스를 접하니 아이들을 키우고 있는 입장에서 더더욱 어디로 여행한다는게 무섭기만 합니다.아무튼 자주 손씻기과 마스크 착용은 꼭 지켜야 하겠습니다~자~ 이번 시간에는요,지금까지는 텍스트를 입력하여 인공지능(AI)로봇 아티보가 언어를 번역하고 출력하였다면 이번에는 음성인식으로 아티보를 제어해 보도록 하겠습니다.그럼 먼저 음성인식이란 무엇인지 알아볼까요? 음성 인식(Speech Recognition)이란 사람이 말하는 음성 언어를 컴퓨터가 해석해 그 내용을 문자 데이터로 전환하는 처리를 말한다. STT(Speech-to-Text)라고도 한다. 키보드 대신 문자를 입력하는 방식으로 주목을 받고 있다. 로봇, 텔레매틱스 등 음성으로 기기제어, 정보검색이 필요한 경우에 응용된다. 대표적인 알고리즘은 HMM(Hidden Markov Model)으로서, 다양한 화자들이 발성한 음성들을 통계적으로 모델링하여 음향모델을 구성하며 말뭉치 수집을 통하여 언어모델을 구성한다. 미리 기록해 둔 음성 패턴과 비교해 개인 인증 등의 용도로 사용하기도 하는데 이를 화자 인식이라고 한다.출처 : 위키백과이번 시간에 사용할 인공지능블록은코딩카테고리에서 ""Speech_recognition"" 에 있는 코딩 블록을 사용해 볼텐데요~한국어로는 영문으로는  ""음성인식하기(Set speech recognition to)""는 다양한 언어로 음성을 인식할 수 있고, ""음성 녹음 시간(recoding time sec timer)""은 음성 녹음 시간과 타이머를 정할 수 있는데 음성 녹음 시간은 3초에서 7초까지, 타이머는 5초까지 기다렸다 시작할 수 있습니다. ""음성인식(Speech recognition)""으로 음성 인식된 값을 저장되며 다른 프로그래밍 블록에서 변수로 넣을 수 있습니다. 이처럼 음성인식 블록을 활용하여 음성인식 시키고 결과값을 비교한 후 같다면,아티보 무드(mood) 블록을 사용하여 미소 짓는 표정으로 바꾸어 보겠습니다. 깃발을 클릭하여 코드를 실행하면 다음과 같은 화면이 나타나면서   아티보 헤드 화면에서는 다음과 같은 표시가 뜨면 ""안녕""이라고 말하여 녹음합니다. 녹음이 끝나면 녹음된 말을 로딩합니다. 녹음된 말을 인식하면 다음과 같은 결과를 보여줍니다. 저는 인식에 실패했네요ㅜ 왜 실패했을까요?아티보 전원을 안켰습니다ㅜ  코딩을 완료한 후에는 코드를 실행할 때에만 전원을 켜주세요.전원은 아티보 헤드만 켭니다.코드를 확인한 후에는 전원을 꺼주세요^^전원을 켜고 다시 한번~~ 콘솔창을 확인해보면, 성공이네요^^원격수업 시작 전인 저의 조수를 급 호출해보았습니다~  그럼 지난 시간에 알아보았던 언어번역을 응용해 다양한 활동을 해볼께요^^  영어번역은 쫌 어색하네요^^;;​아티보 바디의 모터를 사용하여 음성인식을 통해 아티보를 움직여 보았습니다~저의 조수가 블랙핑크 보는데 빠져 있는 바람에 어쩔 수 없이 혼자ㅜ  아티보의 음성인식 포스팅은 여기까지 하도록 하겠습니다~일교차가 점점 커지네요.감기 조심하시고 안전한 연휴 보내세요~감사합니다.  ​​​​*이 글은 큐브로이드 서포터즈로 제품을 무상으로 제공 받아 작성한 후기입니다.​ "
잠수와 안면신경 마비 (1) 사례 ,https://blog.naver.com/winho606/222266293483,20210306,"1. 사례 20세 미군 수중건설팀 다이버가 해면에 나와서 22분 뒤 오른쪽 완전 안면신경마비를 경험하였다. 환자의 당일 잠수 프로필은 수온 6.1℃, 25 피트 수심에서 75분간 비감압 잠수하였으며 상승속도 규정을 준수하였다. 상승시 환자는 우측 귀 통증이 있고, 압력 평형이 잘 되지 않았다. 신체 검진에서 근력, 감각, Mental state(GCS) 등 신경학적 검진은 정상이었다. 이경검사상 우측 고막 주위 출혈이 소량있었다. Oxymetazole 코 스프레이 투여1 16분 이후 증상은 완전 관해되었다.​ 환자는 1달 뒤 잠수에서 다시 안면신경마비를 경험하였다. 수온 27℃, 40 피트 수심에서 50분간 잠수하였으며 상승시 압력평형에 문제가 있었다. 해면 도착 11분 후 우측 안면에 완전 마비가 있었다. 증상은 Oxymetazole 투여  30분 후 완전 관해되었다. 신체 검진에서 이경검사상 중등도 고막 주위 출혈(Rt.), 좌측은 특이 증상 없었다. 이후 잠수감독관은 환자에게 잠수 작업중지를 지시하였고 정밀 검사를 시행하였다​과거력- 질병 (-) 수술 (-) 이관 삽관 (-)- 알레르기 (-) 코 스프레이 사용력 (-)- 유스타키오관 질환력 (-) 알레르기 비염 (-) 중이염 (-)​2. 검사와 진단CT & MRI  ● CT : ● MRI : Brain, internal auditory canal, facial nerve 경로에 이상 없음● Allergen screening, Total IgE 검사 : 정상● Audiogram : 정상- 15dB conductive compenet at 1000Hz- Speech reception threshold- Speech recognition● Tympanometry- Peak pressure Rt 35 daPa / Lt 15 daPa● Acoustic reflex2 : present​위 소견을 바탕으로 Facial nerve baroparesis로 진단하였다.​3. 치료중이 압력이 과다해지는 것을 막기위해서- 고막에 환기관을 설치(Tympanostomy)할 수 있으나 환자는 잠수를 해야하므로- 유스타키오관을 balloon을 이용해서 확장시켜 보았다 (6 x 16 mm AERA system, 12기압 2분간 inflation)​Eustachian Tube Dysfunction Questionnaire-7 설문지3를 이용해 환자의 상태를 평가할 때시술 전 5.6점에서 시술 후 2점으로 감소하였다​​  ​4. 복귀 Process시술 후 audiogram, tympanometry, acoustic reflex 검사를 통해 기능을 확인한 뒤챔버에서 압력변화에 노출 시켜보았다. 60피트 10분을 머문 뒤 분당 30피트 속도로 상승하였다. (비충혈제거제는 사용하지 않음) 환자는 상승시 중이 불편감은 없었고 1시간 관찰에도 안면마비는 발생하지 않았다.이후 잠수 작업에 복귀하였으며 12개월간 20차례 잠수를 하는 동안 증상 재발은 없었다.​5. 고찰 Facial nerve Baroparesis는 발생이 드문 질환이다. North carolina 다이버 상담센터에서는 2015년  2건의 안면마비가 있었다4. 정확한 병리기전은 밝혀지지 않았으나, 중이 공간 압력이 모세혈관 관류압(capillary perfusion pressure)를 넘어설 때 허혈성 신경손상(ischemic neuropraxia)이 발생할 수 있다. (Facial nerve bony dehiscence의 영향은?)  케이스는 '22분 뒤 발생한 안면 신경 마비'로 시작하였지만, 실제 상황은 샤워를 하고 있는데 동료의 얼굴이 저렇게 변한것을 발견하면서 시작된다. 무엇을 할 것인가? 어떤 평가를 해야하고, 과거력을 수집하고, 치료를 할 것인가? 신경학적 증상에 대해 챔버 치료를 해야하나? 논문에서는 샤워실에서 물을 틀어놓은체로 찍은 환자의 사진이 Figure 1로 나온다. 얼마나 당황스러웠을까.​​* 각주1. 비충혈제거제는 압력평형에 도움을 줄 수 있다. airway congestion may impede the ability to equalize the middle ear space via the Eustachian tube2. Acoustic reflex : CN8 - nucleus - CN7, 소리의 크기에 따라 stapes-ovale 접촉을 조절함 (작은 소리를 크게 큰 소리를 작게)3. EDTQ-7 설문지 4. www.ncbi.nlm.nih.gov/books/NBK487733/ Table 2.1-2 Number of cases reviewed by initial classification, 2015 ​Main referenceUtz ER, Wise SR. Navy diver with recurrent facial nerve baroparesis treated with eustachian tube balloon dilation. Laryngoscope. 2019 Nov;129(11):E412-E414. doi: 10.1002/lary.28221. Epub 2019 Aug 10. PMID: 31400145. "
초거대 AI(Artificial Intelligence) 산업·기술·주요 대학 연구 개발 동향과 글로벌 AI 스타트업 생태계 현황 및 정책 방향 ,https://blog.naver.com/itdcenter/223002047033,20230201,"◆ 안내문 인공지능 기술은 1950년대 태동한 이래 몇 차례의 기술적 한계에 부딪혀 부침을 겪어 왔으나, 2000년대 이르러 딥러닝 기술이 개발되고 방대한 연산을 수행할 수 있는 하드웨어가 발전함과 동시에 인공지능을 학습시키기 위한 양질의 데이터를 빅데이터를 통해 획득할 수 있게 되면서 한계를 뛰어넘어 가히 폭발적 성장을 이루게 되었다.인공지능은 특정한 목적을 가진 기능을 수행하는 수동적인 알고리즘의 수준을 넘어서 차량을 자율적으로 운행하거나 스스로 프로그램 코드를 작성하는 등 인간이 수행하던 작업을 대체하고 있다. 또한, 특정한 화풍의 그림을 그려내거나 음악을 작곡하고 인간이 생각해내기 어려운 복잡하고 최적화된 구조물을 생성하거나 신약개발을 위한 단백질 구조를 예측해 내는 등 인간의 전유물로 알려진 발명과 창작의 영역에까지 그 적용범위가 확대되고 있다.​그동안은 인간과 같이 자율적 의지와 고도의 지적 능력을 가진 다른 존재를 생각해볼 수 없으므로 발명은 당연히 인간이 하는 것이라고 여겨져 왔으나, 인간의 지적 수준에 필적하는 인공지능의 등장으로 인공지능이 만들어 내는 산출물을 어떻게 다뤄야 하는지가 지식재산권 분야에서도 큰 화두로 자리 잡고 있으며, 이에 댜한 여러 가지 논의들도 전 세계적으로 활발히 진행되고 있기도 하다.​현재, 인공지능 기술이 비약적으로 발전하면서 보급률이 더욱 높아질 것으로 예상된다. 특히, 복잡하고 반복적인 작업을 대신 수행하고, 나아가 구조화되지 않은 데이터에서 새로운 인사이트를 도출하여 인간 노동자를 보완할 것으로 기대되면서, 전 세계 기업들이 제품 및 서비스 개발에 인공지능 기술을 응용하려는 시도가 계속해서 증가하고 있다. IBM의 ‘글로벌 AI 도입지수 2021년에 따르면 미국, 중국, 영국, 독일, 프랑스 등의 기업 3분의 1이 현재 실제 비즈니스에서 인공지능 기술을 활용하고 있다고 응답했으며, 절반은 인공지능 기술의 도입을 검토 중이라고 밝혔다.​이러한 세계적 흐름과는 대조적으로, 한국의 기업들은 인공지능 기술 개발과 도입에 다소 미온적인 태도를 보인다. 클라리베이트(Clarivate)와 카이스트가 분석한 인공지능 동향을 살펴보면, 한국 기업들은 인공지능 분야에서 개발 속도는 빠르지만 양질의 기술 혁신을 하지 못하고 있는 것으로 나타났다. 뿐만 아니라 여전히 많은 기업이 인공지능 기술에 대한 불확실함을 가지고 있으며, 이로 인해 도입에 필요한 높은 비용을 지불하기 꺼리고 있다. 심지어, IT강국이라는 위상에 맞지 않게 도입하지 않은 대다수 기업은 앞으로도 인공지능 기술을 도입할 계획이 없다고 밝혔다.​이에 따라 본원 IPResearch센터에서는 인공지능 기술 도입과 불확실성을 조금이나마 해소하기 위해 글로벌 국가들의 정책과 주요 기업 동향에 관한 관련 국책민간 분석 보고서 자료와 정책 자료를 토대로 분석정리하여 『초거대 AI(Artificial Intelligence) 산업·기술·주요 대학 연구 개발 동향과 글로벌 AI 스타트업 생태계 현황 및 정책 방향』을 발간하게 되었다. ​ ​◆ 목차 ​제Ⅰ장 초거대 AI(Artificial Intelligence) 분야별 시장 현황과 기술 동향​1. 초거대 AI(Artificial Intelligence) 분야별 개요1) 초거대 AI의 잠재력2) 현실 세계 적용 AI(1) 자율주행 분야(2) 로보틱스 분야(3) 소프트웨어 산업 분야(4) 인터넷 플랫폼 산업 분야(5) 헬스케어 산업 분야3) AI 반도체4) AI 시장 및 투자 개요(1) 테슬라(TSLA US)(2) 마이크로소프트(MSFT US)(3) 엔비디아(NVDA US)(4) 유나이티드 헬스케어(UNH US)(5) 글로벌 X AI and Technology ETF(AIQ US)​2. 초거대 AI(Artificial Intelligence) 분야별 시장 현황과 기술 동향1) 트랜스포머와 초거대 AI(1) 현재 AI의 가능성가. 복잡한 질문에 답하고 긴 글을 사람처럼 자연스럽게 씀나. 생물학의 난제, 단백질 접힘 구조를 빠르고 정확하게 예측다. 말하는 대로 새로운 그림을 그림라. 반도체 설계마. 사람 수준의 코딩바. 게임 규칙을 몰라도 60개의 게임을 실행(2) 초거대 AI의 데이터, 연산능력, 그리고 모델의 발전가. AI의 학습 방법나. 딥러닝과 데이터/연산 능력의 발전다. 모델의 발전: 병렬 연산이 가능한 트랜스포머의 등장라. 초거대 AI의 시작, ‘GPT 3’: 퓨샷러닝과 자기지도학습마. 본격화되는 규모 경쟁(3) 향후 AI의 가속화와 집중화가. 시장 전망: 지수함수적인 성능 개선 + 빠르게 하락하는 비용 → 성장 가속화나. 경쟁 구도: ‘Winner takes most’의 구도가 될 가능성이 높음2) 자율주행/로보틱스의 AI(1) 자율주행가. ‘Chuck’의 비보호 좌회전: 엣지 케이스와 AI나. 테슬라의 자율주행 아키텍처다. 자율주행 시장의 잠재력(2) 로보틱스가. AI 로보틱스나. 구글의 ‘PaLM-SayCan’: 초거대 언어 모델과 로봇의 결합을 통한 성능 개선다. 구글의 ‘Gato’: 범용적 AI의 가능성라. 테슬라 휴머노이드 로봇의 잠재력과 파급력3) 소프트웨어의 AI(1) AI 수익화하는 소프트웨어 업체들(2) AI 시장을 주도하는 3대 클라우드 업체들가. 자본력, 기술력 등 AI 역량 확보해 전방위 지원나. 마이크로소프트, 구글, 아마존 비교(3) 사이버보안가. 시그니처 기반 탐지에서 행동학적 기반 감지나. AI 통한 사이버보안 방법다. 사이버 보안에서 사용하는 머신러닝 기술라. AI 사이버 보안 대표 기업ⅰ. 크라우드스트라이크(CRWD US)ⅱ. 센티널원(S US)ⅲ. 데이터독(DDOG US)4) 인터넷 플랫폼의 AI(1) 플랫폼 성장의 핵심으로 떠오른 AI가. 검색: AI를 통해 결과물 향상 가능 → 검색 쿼리 및 효율 증대나. SNS: AI를 통해 사용자 충성도 개선 → 사용 시간 증대(2) 인터넷 플랫폼의 성장과 AI(3) AI+콘텐츠 결합가. 검색: 쇼핑 콘텐츠 통한 아마존의 구글 쇼핑 잠식나. 소셜미디어: 틱톡의 콘텐츠 공급량 우위 지속5) 헬스케어의 AI(1) 개화하기 시작한 의료 AI가. 헬스케어 산업 거의 전 영역에서 활용되는 AI나. 기술개발 단계 신생기업 주도, 상업화 단계는 기존기업이 주도ⅰ. Health ITⅱ. 의료기기ⅲ 신약 개발(2) Healthcare IT(3) 의료기기(4) 신약개발: AI로 바뀔 의약품 Life Cycle가. 기초과학부터 임상시험까지나. GPU와 딥러닝으로 가속화된 AI 신약개발다. 가시적 성과 도출라. AI 바이오텍 vs. 플랫폼 아웃소싱마. 빅파마의 높은 관심도바. 향후 난제-임상 성공률 향상 여부6) AI 반도체(1) AI 발전 사이클(2) AI의 데이터(3) AI반도체 시장 전망(4) AI 반도체 기술의 필요성가. PIM(Processing-in-Memory)나. 뉴로모픽(5) AI 반도체의 구조적인 변화가. 무어의 법칙과 칩렛(Chiplet)나. 칩렛(Chiplet) 적용 가능 시장과 칩렛(Chiplet) 수요다. 칩렛(Chiplet)의 중요성(6) 빅테크 업체들의 AI 반도체 현황가. 엔비디아 Nvidia(NVDA US)나. 어드벤스트 마이크로 디바이시스 AMD(AMD US)다. IBM(IBM US)라. 인텔 Intel(INTC US)마. 구글 Google/Alphabet(GOOGL US)(7) 스타트업의 증가(8) AI 패권가. AI기술 패권나. 미국과 중국의 경쟁 구도다. 정부의 지원과 함께 성장한 중국의 AI라. 중국만의 강점으로 AI 경쟁력 확보마. 중국의 약점, 반도체를 집중적으로 공략바. 핵심 장비/기술 수출 제한 전망사. 미·중 반도체 전쟁의 중심 TSMC아. AI 반도체 관련 주요 업체 전망​제Ⅱ장 초거대 AI(Artificial Intelligence) 국내·외 기술 동향과 기업 동향​1. 실리콘밸리의 디지털 혁신과 인공지능(AI) 분야 혁신 트렌드 동향1) 배경 및 요약2) 디지털 산업 분야 전반적인 트렌드(1) 데이터를 중심으로 한 인터넷의 진화(2) 디지털 분야의 위기와 기회(3) 새로운 디바이스로 주목받고 있는 전기차(EV)3) 인공지능(AI) 분야 핵심 트렌드 동향(1) 인공지능의 확장세가. Open AI의 초거대 언어모델, GPT-3(2) 글로벌한 파운데이션 모델(초거대 AI)(3) 초거대 AI(Foundation model)의 활용 시대 임박(4) (대용량화) 시각지능에서 언어지능으로 스케일 전쟁(5) (경량화) 온 디바이스(On-device) AI를 위한 경량 딥러닝(6) (자동화) 인공지능을 만드는 인공지능, AutoML(7) (탈중앙화) 프라이버시 보호와 활용의 양립 가능성을 열어가는 연합학습(8) (융합화) 산업과 과학기술 혁신을 주도하는 ‘AI+X’​2. 초거대 AI 해외 기술 동향1) 초거대 인공지능(Hyper-scale AI)의 정의2) 초거대 인공지능의 특징3) 초거대 인공지능 해외 기술 동향(1) GPT-3-인류 역사상 가장 뛰어난 인공지능(2) 람다(LaMDA)-구글의 차세대 인공지능 대화모델(3) 고퍼(Gopher)-딥마인드의 초거대 인공지능 언어모델(4) MT-LNG(MS-엔비디아의 초거대 AI)가. 하드웨어 리소스 효율화나. 하이퍼스케일러(Hyperscaler)들의 자체 트레이닝 칩 개발다. Large-scale 모델에 적합한 메모리 풀 구조(5) 우다오 2.0-중국 인공지능 아카데미의 초거대 AI 사전학습 모델(6) 판구 알파-화웨이의 대규모 자연어처리(NLP) 모델​3. 초거대 AI 국내 기술 동향과 기술 한계와 과제1) 국내 기술 동향(1) 네이버의 초대규모 AI ‘하이퍼클로바(HyperCLOVA)’(2) 카카오의 초거대 인공지능 모델 ‘KoGPT’(3) 카카오의 초거대 AI 이미지 생성 모델 ‘민달리(minDALL-E)’, ‘RQ-트랜스포머’(4) LG의 초거대 AI ‘엑사원(EXAONE)’2) 초거대 AI의 한계(1) 학습비용의 시간 대비 효율성(2) 현실 세계의 상식(3) 전 모든 분야에서의 효율성(4) 인간과 유사한 기억력3) 초거대 AI의 과제와 해결 방안(1) AI 양극화(2) 전력 소모 및 환경 오염​4. 초거대 AI 국내·외 기업 동향1) 국내·외 기업 성과를 높이는 인공지능 개요(1) 인공지능 기술의 도입(2) 미국 기업의 인공지능 도입 성과가. 인공지능 기술 분류와 기업 성과: 자동화 인공지능과 증강 인공지능나. 자동화 인공지능 효과다. 증강 인공지능 효과라. 요약(3) 한국 기업의 인공지능 도입 성과가. 대표적인 인공지능 기술: 자연어처리, 컴퓨터비전, 머신러닝나. 인공지능 기술의 활용다. 인공지능 기업의 활용라. 인공지능 기술의 효율적 활용을 위한 기술 투자마. 인공지능 기술의 효율적 활용을 위한 연구 개발 전략바. 요약사. 미국과 한국 기업의 인공지능 기술 도입 성과 요약(4) 인공지능 기술의 필요성과 의미2) 국내·외 기업 동향(1) 테슬라(TESLA US)가. 테슬라의 딥러닝과 자율주행시스템ⅰ. 딥러닝 기반의 비전 솔루션ⅱ. 딥러닝과 자율주행ⅲ. 테슬라의 자율주행 시스템ⅳ. 레벨업의 시간: 누적되는 데이터, 소프트웨어 2.0, 4D 라벨링과 도조 컴퓨터ⅴ. FSD V9 상용화의 의미나. 테슬라의 AI 기술 휴머노이드 로봇시스템ⅰ. 핵심은 AI를 통한 로봇의 인지 및 판단 능력 개선ⅱ. 구글의 ‘PaLM-SayCan’: 초거대 언어 모델과 로봇의 결합ⅲ. 구글의 ‘Gato’: 범용적 AI의 가능성ⅳ. 테슬라의 옵티머스다. 전망(2) Microsoft(MSFT US)가. GPT-3 독점권 보유나. MS의 인공지능 수익화 현황 및 전망(3) Meta(Facebook)(4) Nvidia(NVDA US)(5) United Health Group(UNH US)가. Optum Health X United Healthcare나. Optum Rx X United Healthcare다. Optum Insight X United Healthcare(6) Global X Artificial Intelligence & Technology ETF(AIQ US)(7) NAVER가. 사람을 위한 AI나. 자연어 의사소통ⅰ. 챗봇ⅱ. 기계 번역ⅲ. 음성 인식ⅳ. 음성 합성다. 컴퓨터 비전ⅰ. 객체 추적ⅱ. 문자 인식ⅲ. 얼굴 인식라. 추천ⅰ. 상품 추천ⅱ. 장소 추천ⅲ. 콘텐츠 추천ⅳ. 음악 추천마. 로봇공학ⅰ. 로봇의 학습ⅱ. 로봇 비전ⅲ. 클라우드 로보틱스ⅳ. 도로 자율주행3) 국내·외 초거대 AI 산업 전망(1) 레벨 2의 기능 고도화 및 제한적 레벨 3 중심의 성장 전망(2) 데이터 및 AI 기술 확보 전망(3) 비 테슬라 진영 현황: 모빌아이/웨이모/엔비디아가. 모빌아이(인텔)나. 웨이모(알파벳)다. 엔비디아(4) AI 반도체가. 딥러닝과 AI 반도체나. 자율주행 프로세서다. 엔비디아(Nvidia, NVDA US)라. 모빌아이, 인텔(Intel, INTC US)마. 자율주행 프로세서의 전쟁(5) 차량용 반도체 및 파운드리, 메모리 시장 영향가. 차량용 반도체 제조사의 설비투자 및 구조적 성장기나. 자율주행 자동차향 반도체에 대한 파운드리 수요 확대다. 자율주행 자동차향 메모리반도체 수요 확대(6) 클라우드/소프트웨어가. 자율주행과 클라우드 컴퓨팅ⅰ. 데이터ⅱ. 연산 능력나. 대표 클라우드 밴더사들의 자율주행 개발 현황ⅰ. 아마존 AWS(AMZN US)ⅱ. 마이크로소프트 Azure(MSFT US)다. 자율주행차를 향한 차량용 소프트웨어ⅰ. 차량용 소프트웨어의 종류라. 차량용 소프트웨어의 통합 플랫폼화ⅰ. 통합 플랫폼으로 진화ⅱ. HMI(Human Machine Interface)의 적용ⅲ. 차량용 소프트웨어 기업의 통합 플랫폼 OS 개발 현황ⅳ. 자율주행 관련 인공지능 스타트업(7) 센서: 카메라/레이더/라이다가. ADAS와 센서 시장의 구조적 성장나. 데이터 확보 전쟁다. 업체별 상이한 센서의 견해라. 카메라 중심의 중단기 센싱 시장 성장 전망마. 레이다바. 라이다(LiDAR)ⅰ. 벨로다인(Velodyne VLDR US)ⅱ. 루미나(Luminar LAZR US)ⅲ. 이노비즈(Innoviz INVZ US)ⅳ. 아우스터(Ouster OUST US)ⅴ. 에이바(Aeva AEVA US)​제Ⅲ장 국내·외 AI 정책 동향과 글로벌 주요 대학 AI 연구 동향 분석​1. 해외 인공지능 정책 동향1) 미국의 인공지능 관련 주요 정책2) 유럽의 인공지능 국가전략 및 정책(1) 유럽연합(EU)의 인공지능 정책(2) 영국의 인공지능 정책(3) 독일의 인공지능 정책3) 중국의 인공지능 국가전략 및 정책 동향4) 일본의 인공지능 국가전략 및 정책 동향5) 해외 인공지능 정책에 대한 분석 및 시사점​2. 국내 인공지능 관련 정책 동향1) 국내 인공지능 국가전략(1) 국내 인공지능 전략 동향(2) 인공지능 국가전략2) 초거대 AI 개발 동향과 과제(1) 다양한 비즈니스에서 AI 기능 활용 과제(2) 초거대 AI 성능과 특징(3) 상용화 단계 진입한 초거대 AI 기술(4) 한국 기업 초거대 AI 개발 동향(5) 슈퍼컴, 빅데이터, 인재 확보 전략​3. 주요국 인공지능 신뢰성 정책 현황1) EU 인공지능 규제안(1) 프라이버시 보호(2) 견고성(3) 공정성(4) 투명성(5) 책임성(6) 인류가치 증진2) 미국 알고리즘 책임법안(Algorithmic Accountability Act of 2019)(1) 프라이버시 보호(2) 견고성(3) 공정성(4) 투명성3) 캐나다의 자동화된 의사결정에 관한 지침(Directive on Automated Decision-Making)(1) 프라이버시 보호(2) 견고성(3) 공정성(4) 투명성(5) 책임성(6) 인류가치 증진4) 미국 워싱턴州 공공기관 얼굴인식 서비스 사용에 관한 법률(1) 프라이버시 보호(2) 견고성(3) 공정성(4) 투명성(5) 책임성5) 미국 일리노이주 인공지능 화상면접법(1) 프라이버시 보호(2) 투명성6) EU 디지털서비스 법안(1) 투명성7) 일본 특정 디지털플랫폼의 투명성 및 공정성 향상에 관한 법률(1) 투명성(2) 책임성8) 시사점​4. 글로벌 주요 대학 인공지능 연구 동향 분석1) 배경 및 목적2) 해외대학 연구 현황(1) MIT, CSAIL(Computer Science and Artificial Intelligence Lab)(2) MIT Media Lab(3) 카네기멜론대학, CMU AI(4) 스탠포드 HAI(Human-Centered Artificial Intelligence)(5) 스탠포드, AI 100 프로젝트(6) UC 버클리, BAIR(Berkeley Artificial Intelligence Research Lab)가. 인공지능 첨단기술에 관한 연구 수행나. 문제해결을 위한 인공지능 핵심 기술 연구(7) UC 버클리, CHAI(Center for Human-Compatible AI)(8) UC-버클리, MIRI(Machine Intelligence Research Institute)(9) 하버드, THE AI INITIATIVE가. 글로벌 인공지능 정책 수립 연구나. 데이터 기반 인공지능 연구 선순환(Harvard Univ)ⅰ. 기술, 데이터, 협력 거버넌스 기반 인공지능 연구 운영체계(10) 옥스포드, Institute for Ethics in AI가. 인공지능 윤리 연구소(11) 옥스퍼드, FHI(Future of Humanity Institute)가. 인류 미래 연구소(12) 워싱턴 대학, Paul G. Allen School가. 지능형 행동의 기초 메커니즘 연구(13) 뉴욕대, AI Now Institute가. 인공지능 사회 적용에 따른 책임성 연구3) 정책 시사점​5. 인공지능 시스템의 성능 측정, MLPerf(Machine Learning Performance)의 현황과 시사점1) 논의 배경2) MLPerf 벤치마크 현황(1) 개요(2) MLPerf 학습 벤치마크가. MLPerf 학습 벤치마크의 접근방법나. 벤치마크 구성ⅰ. CLOSED 방식 벤치마크 측정 기준(3) MLPerf 추론 벤치마크가. MLPerf 추론 벤치마크의 접근방법나. MLPerf의 추론 벤치마크의 4개의 시나리오 구분ⅰ. MLPerf 추론 벤치마크의 시나리오와 측정 기준ⅱ. MLPerf의 형평성과 공정성을 확보하기 위한 추론 시스템 환경 조성다. MLPerf의 시스템 환경3) MLPerf 벤치마크 결과(1) MLPerf 학습 벤치마크(2) MLPerf 추론 벤치마크가. 데이터 센터의 MLPerf 추론 벤치마크 결과(CLOSED 방식)나. 엣지 장치의 MLPerf 추론 벤치마크 결과(CLOSED 방식)다. 엣지 장치의 MLPerf 추론 벤치마크 결과(OPEN 방식)4) 시사점(1) 요약(2) 시사점가. AI HW 시장의 경쟁력은 결국 SW나. MLPerf 벤치마크를 통한 새로운 시장 기회 모색다. 산업계 표준으로 MLPerf의 전략적 활용 필요​6. 다중 감각 AI 기술 전망 및 로드맵1) 다중 감각 AI 개요2) 다중감각 AI 기술 동향(1) 다중감각 AI 기술의 정의(2) 다중감각 AI 기술 전망 및 로드맵3) 기술 시사점(1) 다중감각 AI와 메타버스의 결합으로 인해 다가올 혁신적인 미래(2) AI와 메타버스 결합으로 기대되는 공공기관의 혁신​7. 인공지능 연구개발, AI 기술 성능, AI 기술 윤리, 경제·교육, 정책·거버넌스 동향 분석1) AI Index 2022 개요2) 연구개발(1) 연구개발 성과(2) 연구개발 주체(3) 미‧중 간 AI 양상3) AI 기술 성능(1) 컴퓨터 비전(Computer Vision)가. 이미지 분류(Image Classification)나. 이미지 생성(Image Generation)다. 딥페이크 감지(Deepfake Detection)라. 사람 자세 추정(Human Pose Estimation)마. 의미 세분화(Semantic Segmentation)바. 의료 이미지 분류(Medical Image Segmentation)사. 안면인식(Face Detection & Recognition)아. 시각 추론(Visual Reasoning)자. 비디오 동작 인식(Activity Recognition)차. 비디오 동작 인식(Activity Recognition)(2) 언어 및 음성 인식 분야가. 언어 이해(Language Understanding)나. 문서 요약(Text Summarization)다. 자연어 추론(Natural Language Inference)라. 감성 분석(Sentiment Analysis)마. 기계 번역(Machine Translation)바. 연설·음성 인식(Speech Recognition)(3) 추천 알고리즘(4) 강화 학습(5) 하드웨어 및 로보틱스4) AI 기술 윤리(1) AI 시스템의 공정성·편향성 지표개발 현황(2) 자연어처리 편향성 지표(Natural language processing bias metrics)(3) AI 윤리 연구의 성장(4) 멀티모달(Multimodal) 편향성5) 경제와 교육(1) 일자리(Jobs)(2) 투자(Investment)(3) 기업 AI 활용(Corporate Activity)(4) 북미지역 AI 교육(AI Education)6) AI 정책 및 거버넌스(1) AI에 대한 글로벌 입법 기록(2) 미국 AI 정책 문서 분석(3) 미국의 AI 공공 투자7) AI Index 시사점(1) 연구개발(2) 기술 성능(3) AI 기술윤리(4) 경제·교육(5) 정책·거버넌스​제Ⅳ장 인공지능 스타트업(Stratup) 산업현황·VC 투자 동향 및 글로벌 스타트업 동향과 정책 방향 분석​1. 인공지능 스타트업(AI Startup) 산업 현황1) 산업 현황2) 투자유치 현황(1) 산업군별 투자유치 현황(2) 투자단계 현황3) 비즈니스 유형별 현황​2. 인공지능 산업의 VC 투자 동향과 시사점1) 개요2) 세계 VC 투자 주요 활동량 분석3) 주요 국가별 특성 분석(1) AI 분야 주요국 선정(2) 주요 국가별 활동 특성 분석4) 세부시장 투자 동향 분석(1) 세부시장 분류(2) 세부시장 투자 동향5) 결론​3. 글로벌 인공지능 스타트업 생태계 현황 및 정책 분석과 방향1) 국내·외 인공지능 생태계 현황과 주요 이슈(1) 글로벌 게임 체인저, 인공지능 스타트업가. AI 유니콘 육성나. 국내 AI 스타트업 성장역량(2) 국내·외 인공지능 스타트업 및 유니콘 생태계 현황가. 국내·외 인공지능 스타트업 및 유니콘 현황분석 개요ⅰ. 세계ⅱ. 국내나. 세계 인공지능 스타트업 현황다. 세계 인공지능 유니콘 현황라. 국내 인공지능 스타트업 현황ⅰ. AI 구축을 위한 서비스 시장 확대와 AI 플랫폼 분야 스타트업 부상ⅱ. M&A·IPO 성공사례 증가(3) 세계가 주목하는 인공지능 스타트업가. 글로벌 인공지능 스타트업ⅰ. 오로라 이노베이션(Aurora Innovation)ⅱ. 그래프코어(Graphcore)ⅲ. 레모네이드(Lemonade)ⅳ. 데이터로봇(DataRobot)ⅴ. 센티넬원(SentinelOne)ⅵ. 버터플라이 네트워크(Butterfly Network)ⅶ. 투심플(TuSimple)ⅷ. 페어(FAIRE)ⅸ. 리커션 파마슈티컬즈(Recursion Pharmaceuticals)ⅹ. 스니크(Snyk)나. 글로벌 인공지능 유니콘 TOP 10ⅰ. 바이트댄스(Byte dance)ⅱ. 센스타임(Sense Time)ⅲ. 아르고 AI(Argo AI)ⅳ. 오토메이션 애니웨어(Automation Anywhere)ⅴ. 유아이패스(UiPath)ⅵ. 메그비(Megvii)ⅶ. 인디고 애그리컬쳐(Indigo Agriculture)ⅷ. 클라우드워크(Cloudwalk)ⅸ. 죽스(Zoox)ⅹ. 호라이즌 로보틱스(Horizon Robotics)ⅺ. 2022년 글로벌 AI 유니콘 기업 Top 10 동향다. 국내 인공지능 스타트업 TOP 10ⅰ. 라온피플(주)ⅱ. (주)제이엘케이ⅲ. (주)플리토ⅳ. (주)마인즈랩ⅴ. (주)뷰노ⅵ. (주)뤼이드ⅶ. (주)로앤컴퍼니ⅷ. 노을(주)ⅸ. (주)크라우드웍스ⅹ. (주)크래프트테크놀로지스(4) 국내 인공지능 스타트업의 주요 이슈와 정책 요구사항가. AI 학습용 데이터ⅰ. 데이터 구축ⅱ. 데이터 품질나. 법·제도ⅰ. 데이터 3법ⅱ. 기술특례상장제도ⅲ. 건강보험수가제도다. AI 생태계 강화ⅰ. AI 기술 수준ⅱ. AI 인재ⅲ. 기술 보호ⅳ. EXITⅴ. 해외 진출2) AI 스타트업 생태계 혁신을 위한 정책 방향(1) 제6의 물결을 이끄는 AI 스타트업가. 글로벌 위기속 제6의 물결, AI나. 디지털 대전환 시대의 AI 스타트업다. AI 스타트업 생태계 혁신(2) 국내·외 AI 스타트업 현황 분석가. 현황분석 개요ⅰ. 분석범위ⅱ. 분석방법나. 현황 비교ⅰ. 인공지능 선도기업의 타 산업과의 지능형 융합ⅱ. 글로벌 기업들의 기술력과 주력 분야 집중 투자 전략다. 현황 진단(3) 국내 AI 스타트업 정책 분석가. 정책분석 개요ⅰ. 주요 정책ⅱ. 검토 분야나. 인공지능(AI) 생태계 혁신 정책ⅰ. 4차 산업혁명 대응 계획(‘17.11)ⅱ. 데이터·AI 경제 활성화 계획(‘19.1)ⅲ. 인공지능(AI) 국가 전략(‘19.12)다. 스타트업 생태계 혁신 정책ⅰ. 혁신창업 생태계 조성 방안(‘17.11)ⅱ. 제2 벤처 붐 확산 전략(‘19.3)라. AI 스타트업 혁신 정책 현황 분석(4) AI 스타트업 생태계 혁신을 위한 정책방향가. 정책방향 1: AI 기술 고도화로 글로벌 Catch-upⅰ. AI 핵심기술 집중 개발ⅱ. 시장연계형 AI R&D 추진ⅲ. AI 글로벌 네트워크 강화나. 정책방향 2: AI 주력 분야 글로벌 선도 강화ⅰ. 주력산업의 AI 융합 촉진과 글로벌 진출 지원ⅱ. 지속적인 성장지원을 위한 AI 메가투자 추진ⅲ. AI 분야 글로벌 신시장 개척다. 정책방향 3: 자생적 AI 혁신 생태계 조성ⅰ. 해외 의존력이 높은 AI 인프라의 자립 지원ⅱ. AI 인재흡수(Inbound) 환경 조성ⅲ. 참여형 규제환경 조성과 통합적 규제관리체계 마련​ ​ ​* 체제 : A4사이즈, 본문498쪽* 발행일 : 2023. 1. 27 http://www.sitec21.com/shop/item.php?it_id=1674791273&ca_id=10 초거대 AI(Artificial Intelligence) 산업·기술·주요 대학 연구 개발 동향과 글로벌 AI 스타트업 생태계 현황 및 정책 방향 > 신간보고서 | 전략품목교육센터신간보고서 국내 최대의 자료와 노하우로 보답하겠습니다. 1번째 이미지 새창 초거대 AI(Artificial Intelligence) 산업·기술·주요 대학 연구 개발 동향과 글로벌 AI 스타트업 생태계 현황 및 정책 방향 요약정보 및 구매 공급가 315,000원 정가 350,000원 포인트 7,000점 ISBN 979-11-89250-18-8 배송비 무료 선택옵션 도서/PDF 선택된 옵션 위시리스트 다음 상품 2023 재난․재해․안전 관련 기술시장 현황과 향후 전망 상품 정보 ◆ 안내문 인공지능 기술은 1950년대 태동한 이래 몇 ...www.sitec21.com ​​http://www.sitec21.com/shop/item.php?it_id=1667961643&ca_id=h0 최신 분야별 산업기술보고서 및 세미나 (2022년 신간 / 공급가 : 종목별 판매가) > 최신 분야별 보고서 목록 | 전략품목교육센터최신 분야별 보고서 목록 국내 최대의 자료와 노하우로 보답하겠습니다. 최신 분야별 산업기술보고서 및 세미나 (2022년 신간 / 공급가 : 종목별 판매가) 요약정보 및 구매 공급가 0원 포인트 0점 배송비 무료 선택된 옵션 최신 분야별 산업기술보고서 및 세미나 (2022년 신간 / 공급가 : 종목별 판매가) (+0원) 수량 증가 감소 총 금액 : 0원 위시리스트 다음 상품 최신 분야별 산업기술보고서 및 세미나자료 (2021년 6월~2022년 신간 / 공급가 : 종목별 판매가) 상품 정보 최신 분야별 산업기술보고서 (신간순)     ...www.sitec21.com 보고서명 (4차산업, AI, ICT융복합) 발행일체제정가판매가 스마트센서 기술개발 및 산업분야별 도입 현황과 주요기업 사업 전략 2023년 1월 30일A4크기, 본문634P44만원39만6천원 게이미피케이션과 디지털치료제 기술동향 및 전자약 시장 전망-3세대 치료제 2 2023. 11. 25A4크기, 본문276P 양장본40만원36만원 초거대 AI (Artificial Intelligence) 산업·기술·주요 대학 연구 개발 동향과 글로벌 AI 스타트업 생태계 현황 및 정책 방향 2023년 1월 27일A4크기, 본문498P35만원31만5천원 국민안전 대응 재난관리 구축과 무인이동체 혁신기술 실태분석 2022년 12월 8일A4크기, 본문614P양장본38만원34만2천원 Global Market Outlook 2022 - (Vol-Ⅰ) ICT기반기술, 4차산업 핵심기술 2022년 12월 8일A4크기, 본문723P44만원39만6천원 Global Market Outlook 2022 - (Vol-Ⅱ) ICT융합기술, ICT 디바이스, 소재부품산업 2022년 12월 8일A4크기, 본문667P44만원39만6천원 디지털 혁신기술 실태 및 기술 전망 - 인공지능 반도체메타버스 2022년 11월 28일A4크기, 본문614P양장본38만원34만2천원 2023 글로벌 스마트시티 서비스 및 기반 기술 동향과 시장 전망 2022년 11월 21일A4크기, 본문600P44만원39만6천원 3세대 치료제 1-마이데이터 기반 디지털치료제 및 전자약 기술 개요 2022년 11월 22일A4크기, 본문296P양장본40만원36만원 2023 디지털 의료 혁신을위한 보건의료용 빅데이터 기술 동향과 비즈니스 전망 2022년 11월 17일A4크기, 본문384P44만원39만6천원 디지털 헬스 산업의 글로벌트렌드 및 ICT 기술별 활용 동향 2022년 11월 10일A4크기, 본문520P48만원43만2천원 미래 신성장 산업 스마트농업/스마트팜 핵심 분야별 세부시장의 기술개발 동향과 적용사례 2022년 11월 8일A4크기, 본문483P40만원36만원 ICT 핵심산업별 글로벌 마켓 데이터- 반도체 / 메모리ㆍ저장매체 / 센서 / 디스플레이 / 배터리 / 웨어러블 기기 / 카메라(모듈) / 퍼스널 컴퓨터(PC) / 스마트폰 / 태블릿PC / 스마트TV / 통신 인프라 & 장비 / 오디오 & 음향기기 - 2022년 10월 25일A4크기, 본문556P50만원45만원 ICT 핵심기술별 글로벌마켓 데이터- 인공지능(AI)/ 빅데이터 / 클라우드 컴퓨팅 / 사물인터넷(IoT) / 블록체인ㆍ가상화폐 / 메타버스 / NFT(대체불가능토큰) / 디지털트윈 / XR(확장현실) / 양자기술 / IT보안 / IT네트워크 / 5Gㆍ6G / 각 분야별 글로벌 마켓 데이터 - 2022년 10월 4일A4크기, 본문577P50만원45만원 (사람중심) 인공지능 핵심원천기술 연구개발 동향과 시장ㆍ사업화 전망 2022년 10월 12일A4크기, 본문651P48만원43만2천원 AI·자율주행기반 지능형 서비스로봇 시장실태와 장래전망 2022년 10월 11일A4크기, 본문629P44만원39만6천원 콘텐츠 주요 산업별 플랫폼 현황과 XR/OTT/메타버스 정책/기술 분석 2022년 10월 7일A4크기, 본문621P44만원39만6천원 2023 로봇산업 분야별 시장동향과 유망 기술개발 및 기업 현황 2022년 10월 7일A4크기, 본문615P44만원39만6천원 국내외 빅데이터(Big Data) 산업 및 시장분석과 해외진출 전략(상), (하) 2022년 10월 6일A4크기, 본문상594P하508P44만원39만6천원 2023년 스마트 팩토리 및 스마트 제조산업 동향과 국내외 등대공장 실태와 전략 2022년 10월 4일A4크기, 본문656P44만원39만6천원 4차 산업혁명 핵심기술기반 스마트 재난안전산업 기술, 시장 동향과 전망 2022년 9월 27일A4크기, 본문642P44만원39만6천원 2022 로봇·드론·인공지능(AI) 산업동향 및 시장실태와 전망(Ⅰ) 2022년 9월 24일A4크기, 본문905P44만원39만6천원 2022 로봇·드론·인공지능(AI) 산업동향 및 시장실태와 전망(Ⅱ) 2022년 9월 24일A4크기, 본문753P44만원39만6천원 미래 과학기술 주요 핵심 과제와 디지털 중심 전략기술 동향 분석 2022년 9월 7일A4크기, 본문626P양장본38만원34만2천원 디지털 대전환(DX) 시대의 주요 DX기술(AIㆍ빅데이터ㆍ클라우드ㆍ5G/6Gㆍ메타버스ㆍ디지털트윈) 트렌드와 대응 전략 2022년 8월 25일A4크기, 본문545P48만원43만2천원 차세대 산업을 주도하는 빅데이터 기술시장 동향과 발전전략 2022년 8월 22일A4크기, 본문369P36만원34만4천원 차세대 통신 전략 1-테라헤르츠 기반 6G 기술동향 및 표준화 현황 2022년 8월 15일A4크기, 본문292P양장본40만원36만원 차세대 통신 전략 2-위성통신과 우주인터넷 기술 현황 및 서비스형 네트워크 기술 (Network-as-a-Service) 2022년 8월 15일A4크기, 본문302P양장본40만원36만원 스마트제조 정책/핵심기술 분석과 디지털트윈 사업화 활용 동향 분석 2022년 8월 12일A4크기, 본문618P양장본38만원34만2천원 스마트·디지털 물류 혁신 현황분석 -자율주행/풀필먼트/항만/콜드체인 2022년 8월 11일A4크기, 본문616P양장본38만원34만2천원 인공지능 주요 산업 분야별 실태분석 -금융/국방/제조/자율주행/농업/의료 2022년 7월 8일A4크기, 본문616P38만원34만2천원 지능형 로봇 기술 및 시장 전망과 유망서비스 로봇 제품/상용화 동향 2022년 7월 8일A4크기, 본문630P38만원34만2천원 디지털 뉴딜/마이데이터 추진현황과 데이터 활용∙보안∙플랫폼개발 분석 2022년 7월 4일A4크기, 본문614P38만원34만2천원 지능형 로봇(Intelligent Robots) 글로벌 핵심 시장·기술 동향과 로봇 소프트웨어 관련 기술 및 국내·외 기업 생태계 동향 2022년 7월 27일A4크기, 본문516P35만원31만5천원 4차산업혁명의 엔진, 양자기술(Quantum Technology) 글로벌 혁신 기술 트렌드 및 향후 전망 2022년 7월 13일A4크기, 본문369P44만원39만6천원 2022년 중소·중견기업형 유망기술 연구개발 테마 총람(Ⅰ) - 전기·전자·정보통신산업분야 연구개발 테마 - 2022년 7월 12일A4크기, 본문695P38만원34만2천원 미래 기술의 중심 인공지능(AI) 기술개발전략 및 정책 동향 2022년 7월 12일A4크기, 본문369P36만원34만4천원 지능형시스템 정책 및 시장동향과 미래 모빌리티 관련 기술 연구동향 2022년 6  28A4크기, 본문55035만원31만5천원 영역이 확대되는 실감형 컨텐츠(VR,AR)산업 시장 동향과 기술개발전략 2022년 6월 9일A4크기, 본문429P38만원34만2천원 다양한 산업 분야에 활용될, 맞춤형 인공지능ㆍ설명 가능한 인공지능(XAI) 혁신 기술 트렌드 및 향후 전망 2022년 5월 18일A4크기, 본문425P44만원39만6천원 의료, 헬스케어용 인공지능(AI) · 서비스로봇 기술개발 동향과 사업화 전략 2022년 5월 13일A4크기, 본문729P44만원39만6천원 2022년 국내외 메타버스 산업 및 시장분석과 비즈니스 전략(상) 2022년 4월 12일A4크기, 본문491P44만원39만6천원 2022년 국내외 메타버스 산업 및 시장분석과 비즈니스 전략(하) 2022년 5월 13일A4크기, 본문693P44만원39만6천원 디지털뉴딜 성장 기반 플랫폼 시장동향과 AI활용 신기술 개발 및 확장현실 연구동향 2022년 4월 29일A4크기, 본문550P35만원31만5천원 ICT융합 글로벌 조선, 해양플랜트산업 기술개발 동향과 시장 전망 2022년 4월 12일A4크기, 본문649P44만원39만6천원 차세대 인터페이스 디지털 휴먼 기술 동향 및 향후 전망 2022년 3월 31일A4크기, 본문292P양장본40만원36만원 미래 양자기술 동향과 생태계분석-컴퓨팅/센서/양자암호?통신/양자점 2022년 3월 28일A4크기, 본문628P양장본38만원34만2천원 빅데이터·AI 플랫폼 구축전략과 주요 산업별 활성화 동향 분석 2022년 3월 17일A4크기, 본문608P양장본38만원34만2천원 인간 증강 기술로 주목받는, AI 기반 로봇(사이보그, 감성로봇, 챗봇)과 의료 로봇 기술 및 개발 트렌드 분석 2022년 3월 14일A4크기, 본문430P44만원39만6천원 스마트그린 조선·해운산업 개발현황과 자율운항 제어 디지털플랫폼 구축전략 2022년 3월 2일A4크기, 본문620P양장본38만원34만2천원2022년 스마트홈 기술개발 및 시장 전망과 주요기업 사업전략2022년 3월 10일A4크기, 본문680P44만원39만6천원 메타버스/디지털트윈 기술전망과 가상융합기술(XR) 산업 실태분석  2022년 2월 21일A4크기, 본문624P양장본38만원34만2천원2022 메타버스 & 기반기술(디지털 트윈ㆍNFT) 혁신 트렌드 및비즈니스 선도전략2022년 2월 10일A4크기, 본문639P44만원39만6천원뉴노멀 시대 생존 전략-디지털 대전환을 위한 디지털 뉴딜 2.0 및 DNA 생태계 기술 동향2022년 2월 7일A4크기, 본문312P40만원36만원2022 로봇산업 분야별 시장동향과 유망 기술개발 및 기업 현황2022년 1월 12일A4크기, 본문444P44만원39만6천원2022년 마이데이터 핵심기술 및 시장 동향과 유망 사업 분야별 사업화 전략2022년 1월 10일A4크기, 본문602P44만원39만6천원블록체인 토큰경제 NFT 기술동향 및 중앙은행 디지털화폐(CBDC) 산업현황-블록체인 상호운용성을 위한 표준화 현황2022년 1월 27일A4크기, 본문276P양장본40만원36만원2022 글로벌 스마트시티 기술개발 동향과 핵심 서비스 개발 전략2021년 1월 25일A4크기, 본문686P44만원39만6천원현대기술의 꼭지점 메타버스와 믹스버스 기술동향 및 시장전망2022년 1월 19일A4크기, 본문304P양장본40만원36만원 2022 (빅)데이터의 가치와 혁신 기술 트렌드 및 비즈니스 전망 2022년 1월 17일A4크기, 본문362P42만원37만8천원 2022 차세대 AI(인공지능) 혁신 기술 트렌드 및 시장 전망 2022년 1월 13일A4크기, 본문538P44만원39만6천원2022 글로벌 스마트 제조 및 스마트 팩토리 기술개발 전략과 시장전망2022년 1월 8일A4크기, 본문702P40만원36만원 메타버스(metaverse)·XR(VR, AR·MR) 글로벌 생태계 동향 및 기술·연구 개발 현황과 NFT(대체 불가능한 토큰) 주요 프로젝트 현황 2022년 1월 3일A4크기, 본문503P32만원28만8천원세미나 자료개최날짜체제판매가 200조 로봇시장 선점을 위한 신기술 혁신사례 및 사업전략 세미나 자료 2021. 5. 25~26A4크기,본문 275P12만원 메타버스 및 기반기술ㆍ사이버보안 구축방안과 신사업 전략 세미나 자료 2021. 5. 19A4크기,본문 199P9만원 DataㆍCloudㆍIoTㆍAI for Digital Agriculture -실증적 경험과 노하우 세미나 자료 2021. 3. 23A4크기,본문 1259만원 2022년 디지털 트윈/메디컬 트윈 최신분석과 사업전략 및 산업별 활용사례 세미나 자료 2021. 3. 29A4크기,본문 1399만원 2022년 메가트렌드 - 메타버스 산업과 융합 비즈니스 모델 및 서비스 전략 세미나 자료 2022. 1. 26~27A4크기,본문 324P12만원 2022년 스마트팩토리ㆍ제조 및 융합보안 구현방안과 혁신전략 및 실증사례 세미나 자료 -초연결화ㆍ지능화 기반의 기술 고도화 방안- 2021. 12. 9~10A4크기,본문 260P10만원 ​ "
귀가 트이는 영어 2021.03.05 ,https://blog.naver.com/babeok77/222265094866,20210305,"귀가 트이는 영어Friday. March. 05. 2021It is sunny and cozy, but the daily temperature difference between day and night it large.Hoy es cinco de marzo, viernes.Es soleado y acogedor, pero la diferencia de temperatura diaria entre el día y la noche es grande.​LISTENING CHECK-UP​Q. The four skills in English do not influence one another and are totally independent skills.​​A. False. The four skills you'll need to learn English as a second language are listening, speaking, reading, writing and while they're all intertwined.​Q. The focus of language learning is now shifting towards real-world communication.​​A. True. Historically, traditional teaching methods focused on learning from a book, understanding grammar rules, and conjugating verbs. But the focus is now shifting towards real-world communication and many online resources are facilitating this change.​Q. There are many new platforms out there that can help you with your English learning.​​A. True. There are platforms that can pair you up with tutors or test your pronunciation with speech-recognition software. "
